<doc id="37081" url="http://en.wikipedia.org/wiki?curid=37081" title="State of the Union">
State of the Union

The State of the Union is the address presented by the President of the United States to a joint session of the United States Congress, typically delivered annually. The address not only reports on the condition of the nation but also allows presidents to outline their legislative agenda (for which they need the cooperation of Congress) and their national priorities. The address fulfills rules in Article II, Section 3 of the U.S. Constitution, requiring the President to periodically give Congress information on the "state of the union" and recommend any measures that he believes are necessary and expedient. During most of the country's first century, the President primarily only submitted a written report to Congress. With the advent of radio and television, the address is now broadcast live across the country on most networks. 
Background.
The practice arises from a command given to the president in the Constitution of the United States:
He shall from time to time give to Congress information of the State of the Union and recommend to their Consideration such measures as he shall judge necessary and expedient.—Article II, Section 3 of the U.S. Constitution
Although the language of this constitution is not specific, by tradition, the President makes this report annually in late January or early February. Between 1934 and 2013 the date has been as early as January 3, and as late as February 12.
While not required to deliver a speech, every president since Woodrow Wilson, with notable exception of Herbert Hoover, has made at least one State of the Union report as a speech delivered before a joint session of Congress. Before that time, most presidents delivered the State of the Union as a written report.
Since Franklin Roosevelt, the State of the Union is given typically each January before a joint session of the United States Congress and is held in the House of Representatives chamber of the United States Capitol. When a presidential inauguration occurs in January, the date may be delayed until February.
What began as a communication between president and Congress has become a communication between the president and the people of the United States. Since the advent of radio, and then television, the speech has been broadcast live on most networks, preempting scheduled programming. To reach the largest television audience, the speech, once given during the day, is now typically given in the evening, after 9 pm ET (UTC-5).
Also, in recent decades, newly inaugurated presidents have chosen to deliver speeches to joint sessions of Congress in the early months of their presidencies, but have not officially considered them State of the Union addresses.
History.
George Washington delivered the first regular annual message before a joint session of Congress on January 8, 1790, in New York City, then the provisional U.S. capital. In 1801, Thomas Jefferson discontinued the practice of delivering the address in person, regarding it as too monarchical (similar to the Speech from the Throne). Instead, the address was written and then sent to Congress to be read by a clerk until 1913 when Woodrow Wilson re-established the practice despite some initial controversy. However, there have been exceptions to this rule. Presidents during the latter half of the 20th century have sent written State of the Union addresses. The last President to do this was Jimmy Carter in 1981.
For many years, the speech was referred to as "the President's Annual Message to Congress". The actual term "State of the Union" first emerged in 1934 when Franklin D. Roosevelt used the phrase, becoming its generally accepted name since 1947.
Prior to 1934, the annual message was delivered at the end of the calendar year, in December. The ratification of the 20th Amendment on January 23, 1933 changed the opening of Congress from early March to early January, affecting the delivery of the annual message. Since 1934, the message or address has been delivered to Congress in January or February.
The Twentieth Amendment also established January 20 as the beginning of the presidential term. In years when a new president is inaugurated, the outgoing president may deliver a final State of the Union message, but none has done so since Jimmy Carter sent a written message in 1981. In 1953 and 1961, Congress received both a written State of the Union message from the outgoing president and a separate State of the Union speech by the incoming president. Since 1989, in recognition that the responsibility of reporting the State of the Union formally belongs to the president who held office during the past year, newly inaugurated Presidents have not officially called their first speech before Congress a "State of the Union" message.
In 1936, President Roosevelt set a precedent when he delivered the address at night. Only once before—when Woodrow Wilson asked Congress to order the U.S. into World War I—had a sitting president addressed Congress at night.
Calvin Coolidge's 1923 speech was the first to be broadcast on radio. Harry S. Truman's 1947 address was the first to be broadcast on television. Lyndon B. Johnson's address in 1965 was the first delivered in the evening. Three years later, in 1968, television networks in the United States, for the first time, imposed no time limit for their coverage of a State of the Union address. Delivered by Lyndon B. Johnson, this address was followed by extensive televised commentary by, among others, Daniel Patrick Moynihan and Milton Friedman. Ronald Reagan's 1986 State of the Union Address is the only one to have been postponed. He had planned to deliver it on January 28, 1986 but postponed it for a week after learning of the Space Shuttle "Challenger" disaster and instead addressed the nation on the day's events. Bill Clinton's 1997 address was the first broadcast available live on the World Wide Web.
Delivery of the speech.
Protocol of entry into House chamber.
A formal invitation is made to the President for each State of the Union Address.
By approximately 8:30 pm, the members of the House have gathered in their seats for the joint session. Then, the Deputy Sergeant at Arms addresses the Speaker and loudly announces the Vice President and members of the Senate, who enter and take the seats assigned for them.
The Speaker, and then the Vice President, specify the members of the House and Senate, respectively, who will escort the President into the House chamber. The Deputy Sergeant at Arms addresses the Speaker again and loudly announces, in order, the Dean of the Diplomatic Corps, the Chief Justice of the United States and the Associate Justices, and the Cabinet, each of whom enters and takes their seats when called. The justices take the seats nearest to the Speaker's rostrum and adjacent to the sections reserved for the Cabinet and the members of the Joint Chiefs of Staff.
Just after 9 pm, as the President reaches the door to the chamber, the House Sergeant at Arms stands just inside the doors, facing the Speaker and waiting for the President to be ready to enter the chamber. When he is ready, the Sergeant at Arms announces his presence, loudly stating the phrase: "Mister/Madam Speaker, the President of the United States!"
As applause and cheering begins, the President slowly walks toward the Speaker's rostrum, followed by members of his Congressional escort committee. The President's approach is slowed by pausing to shake hands, hug, kiss, and autograph copies of his speech for Members of Congress. After he takes his place at the House Clerk's desk, he hands two manila envelopes previously placed on the desk and containing copies of his address to the Speaker and Vice President.
After continuing applause from the attendees has diminished, the Speaker introduces the President to the Representatives and Senators, stating: "Members of [the] Congress, I have the high privilege and [the] distinct honor of presenting to you the President of the United States." This leads to a further round of applause and, eventually, the beginning of the address by the President.
Designated survivor and other logistics.
Customarily, one cabinet member (the designated survivor) does not attend, in order to provide continuity in the line of succession in the event that a catastrophe disables the President, the Vice President, and other succeeding officers gathered in the House chamber. Additionally, since the September 11 attacks in 2001, a few members of Congress have been asked to relocate to undisclosed locations for the duration of the speech to form a rump Congress in the event of a disaster.
Both the Speaker and the Vice President sit at the Speaker's desk, behind the President for the duration of the speech. If either is unavailable, the next highest-ranking member of the respective house substitutes. Once the chamber settles down from the President's arrival, the Speaker officially presents the President to the joint session of Congress. The President then delivers the speech from the podium at the front of the House Chamber.
In the State of the Union the President traditionally outlines the administration's accomplishments over the previous year, as well as the agenda for the coming year, often in upbeat and optimistic terms. Since the 1982 address, it has also become common for the President to honor special guests sitting in the gallery, such as everyday Americans or visiting heads of state. During that 1982 address, President Ronald Reagan acknowledged Lenny Skutnik for his act of heroism following the crash of Air Florida Flight 90. Since then, the term "Lenny Skutniks" has been used to refer to individuals invited to sit in the gallery, and then cited by the President, during the State of the Union.
State of the Union speeches usually last a little over an hour, partly because of the large amounts of applause that occur from the audience throughout. The applause is often political in tone, with many portions of the speech being applauded only by members of the President's own party. As non-political officeholders, members of the Supreme Court or the Joint Chiefs of Staff rarely applaud in order to retain the appearance of political impartiality. In recent years, the presiding officers of the House and the Senate, the Speaker and the Vice President, respectively, have departed from the neutrality expected of presiding officers of deliberative bodies, as they, too, stand and applaud in response to the remarks of the President with which they agree.
For the 2011 address, Senator Mark Udall of Colorado proposed a break in tradition wherein all members of Congress sit together regardless of party, as well as the avoiding of standing; this was in response to the 2011 Tucson Shooting in which Representative Gabrielle Giffords was shot and wounded in an assassination attempt. This practice was also repeated during the 2012 address.
Opposition response.
Since 1966, the speech has been followed on television by a response or rebuttal by a member of the major political party opposing the President's party. The response is typically broadcast from a studio with no audience. In 1970, the Democratic Party put together a TV program with their speech to reply to President Nixon, as well as a televised response to Nixon's written speech in 1973. The same thing was done by Democrats for President Reagan's speeches in 1982 and 1985. The response is not always produced in a studio; in 1997, the Republicans for the first time delivered the response in front of high school students. In 2004, the Democratic Party's response was also delivered in Spanish for the first time, by New Mexico Governor Bill Richardson. In 2011, Minnesota Congresswoman Michele Bachmann also gave a televised response for the Tea Party Express, a first for a political movement.
Significance.
Although much of the pomp and ceremony behind the State of the Union address is governed by tradition rather than law, in modern times, the event is seen as one of the most important in the US political calendar. It is one of the few instances when all three branches of the US government are assembled under one roof: members of both houses of Congress constituting the legislative, the President's Cabinet constituting the executive, and the Chief Justice and Associate Justices of the Supreme Court constituting the judiciary. In addition, the military is represented by the Joint Chiefs of Staff, while foreign governments are represented by the Dean of the Diplomatic Corps. The address has also been used as an opportunity to honor the achievements of some ordinary Americans, who are typically invited by the President to sit with the First Lady.
Local versions.
Certain states have a similar annual address given by the governor. For most of them, it is called the State of the State address. In Iowa, it is called the Condition of the State Address; in Kentucky, Massachusetts, Pennsylvania, and Virginia, the speech is called the State of the Commonwealth address. The mayor of Washington, D.C. gives a State of the District address. American Samoa has a State of the Territory address given by the governor. Puerto Rico has a State Address given by the governor. Some cities or counties also have an annual State of the City Address given by the mayor, county commissioner or board chair, including Sonoma County, California; Orlando, Florida; Cincinnati, Ohio; Parma, Ohio; Detroit, Michigan; Seattle, Washington; Birmingham, Alabama; Boston, Massachusetts; Los Angeles, California; Buffalo, New York; Rochester, New York; San Antonio, Texas; McAllen, Texas; and San Diego, California. The Mayor of the Metropolitan Government of Nashville and Davidson County in Nashville, Tennessee gives a speech similar called the State of Metro Address. Some university presidents give a State of the University address at the beginning of every academic term. Private companies usually have a "State of the Corporation" or "State of the Company" address given by the respective CEO. The model has also been adopted by the European Union.

</doc>
<doc id="37084" url="http://en.wikipedia.org/wiki?curid=37084" title="Event cascade">
Event cascade

In event-driven computer programs, an event cascade occurs when an event handler causes another event to occur which also triggers an event handler. This can be a tricky source of program errors (see computer bug).

</doc>
<doc id="37085" url="http://en.wikipedia.org/wiki?curid=37085" title="Software bug">
Software bug

A software bug is an error, flaw, failure, or fault in a computer program or system that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. Most bugs arise from mistakes and errors made by people in either a program's source code or its design, or in frameworks and operating systems used by such programs, and a few are caused by compilers producing incorrect code. A program that contains a large number of bugs, and/or bugs that seriously interfere with its functionality, is said to be "buggy". Reports detailing bugs in a program are commonly known as bug reports, defect reports, fault reports, problem reports, trouble reports, change requests, and so forth.
Bugs trigger errors that can in turn have a wide variety of ripple effects, with varying levels of inconvenience to the user of the program. Some bugs have only a subtle effect on the program's functionality, and may thus lie undetected for a long time. More serious bugs may cause the program to crash or freeze. Others qualify as security bugs and might for example enable a malicious user to bypass access controls in order to obtain unauthorized privileges.
The results of bugs may be extremely serious. Bugs in the code controlling the Therac-25 radiation therapy machine were directly responsible for some patient deaths in the 1980s. In 1996, the European Space Agency's US$1 billion prototype Ariane 5 rocket had to be destroyed less than a minute after launch, due to a bug in the on-board guidance computer program. In June 1994, a Royal Air Force Chinook helicopter crashed into the Mull of Kintyre, killing 29. This was initially dismissed as pilot error, but an investigation by "Computer Weekly" uncovered sufficient evidence to convince a House of Lords inquiry that it may have been caused by a software bug in the aircraft's engine control computer.
In 2002, a study commissioned by the US Department of Commerce' National Institute of Standards and Technology concluded that "software bugs, or errors, are so prevalent and so detrimental that they cost the US economy an estimated $59 billion annually, or about 0.6 percent of the gross domestic product".
Etymology.
Use of the term "bug" to describe inexplicable defects has been a part of engineering jargon for many decades and predates computers and computer software; it may have originally been used in hardware engineering to describe mechanical malfunctions. For instance, Thomas Edison wrote the following words in a letter to an associate in 1878:
It has been just so in all of my inventions. The first step is an intuition, and comes with a burst, then difficulties arise — this thing gives out and [it is] then that "Bugs" — as such little faults and difficulties are called — show themselves and months of intense watching, study and labor are requisite before commercial success or failure is certainly reached."
Baffle Ball, the first mechanical pinball game, was advertised as being "free of bugs" in 1931. Problems with military gear during World War II were referred to as "bug"s (or glitches).
The term "bug" was used in an account by computer pioneer Grace Hopper, who publicized the cause of a malfunction in an early electromechanical computer. A typical version of the story is given by this quote:
In 1946, when Hopper was released from active duty, she joined the Harvard Faculty at the Computation Laboratory where she continued her work on the Mark II and Mark III. Operators traced an error in the Mark II to a moth trapped in a relay, coining the term "bug". This bug was carefully removed and taped to the log book. Stemming from the first bug, today we call errors or glitches in a program a "bug".
Hopper was not actually the one who found the insect, as she readily acknowledged. The date in the log book was September 9, 1947, although sometimes erroneously reported as 1945. The operators who did find it, including William "Bill" Burke, later of the Naval Weapons Laboratory, Dahlgren, Virginia, were familiar with the engineering term and, amused, kept the insect with the notation "First actual case of bug being found." Hopper loved to recount the story. This log book, complete with attached moth, is part of the collection of the Smithsonian National Museum of American History, though it is not currently on display.
The related term "debug" also appears to predate its usage in computing: the Oxford English Dictionary's etymology of the word contains an attestation from 1945, in the context of aircraft engines.
Prevalence.
In software development projects, a "mistake" or "fault" can be introduced at any stage during development. Bugs are a consequence of the nature of human factors in the programming task. They arise from oversights or mutual misunderstandings made by a software team during specification, design, coding, data entry and documentation. For example, in creating a relatively simple program to sort a list of words into alphabetical order, one's design might fail to consider what should happen when a word contains a hyphen. Perhaps, when converting the abstract design into the chosen programming language, one might inadvertently create an off-by-one error and fail to sort the last word in the list. Finally, when typing the resulting program into the computer, one might accidentally type a "<" where a ">" was intended, perhaps resulting in the words being sorted into reverse alphabetical order.
Another category of bug is called a "race condition" that can occur when programs have multiple components executing at the same time, either on the same system or across multiple systems interacting across a network. If the components interact in a different order than the developers intended, it may break the logical flow of the program. These bugs can be difficult to detect or anticipate, since they may not occur during every execution of a program.
More complex bugs can arise from unintended interactions between different parts of a computer program. This frequently occurs because computer programs can be complex — millions of lines long in some cases — often having been programmed by many people over a great length of time, so that programmers are unable to mentally track every possible way in which parts can interact.
Mistake metamorphism.
There is ongoing debate over the use of the term "bug" to describe software errors. One argument is that the word "bug" is divorced from a sense that a human being caused the problem, and instead implies that the defect arose on its own, leading to a push to abandon the term "bug" in favor of terms such as "defect", with limited success.
In software engineering, "mistake metamorphism" (from Greek "meta" = "change", "morph" = "form") refers to the evolution of a defect in the final stage of software deployment. Transformation of a "mistake" committed by an analyst in the early stages of the software development lifecycle, which leads to a "defect" in the final stage of the cycle has been called 'mistake metamorphism'.
Different stages of a "mistake" in the entire cycle may be described by using the following terms:
Prevention.
The software industry has put much effort into finding methods for preventing programmers from inadvertently introducing bugs while writing software. These include:
Debugging.
Finding and fixing bugs, or "debugging", has always been a major part of computer programming. Maurice Wilkes, an early computing pioneer, described his realization in the late 1940s that much of the rest of his life would be spent finding mistakes in his own programs. As computer programs grow more complex, bugs become more common and difficult to fix. Often programmers spend more time and effort finding and fixing bugs than writing new code. Software testers are professionals whose primary task is to find bugs, or write code to support testing. On some projects, more resources can be spent on testing than in developing the program.
Usually, the most difficult part of debugging is finding the bug in the source code. Once it is found, correcting it is usually relatively easy. Programs known as debuggers exist to help programmers locate bugs by executing code line by line, watching variable values, and other features to observe program behavior. Without a debugger, code can be added so that messages or values can be written to a console (for example with "printf" in the C programming language) or to a window or log file to trace program execution or show values.
However, even with the aid of a debugger, locating bugs is something of an art. It is not uncommon for a bug in one section of a program to cause failures in a completely different section, thus making it especially difficult to track (for example, an error in a graphics rendering routine causing a file I/O routine to fail), in an apparently unrelated part of the system.
Sometimes, a bug is not an isolated flaw, but represents an error of thinking or planning on the part of the programmer. Such "logic errors" require a section of the program to be overhauled or rewritten. As a part of Code review, stepping through the code modelling the execution process in one's head or on paper can often find these errors without ever needing to reproduce the bug as such, if it can be shown there is some faulty logic in its implementation.
But more typically, the first step in locating a bug is to reproduce it reliably. Once the bug is reproduced, the programmer can use a debugger or some other tool to monitor the execution of the program in the faulty region, and find the point at which the program went astray.
It is not always easy to reproduce bugs. Some are triggered by inputs to the program which may be difficult for the programmer to re-create. One cause of the Therac-25 radiation machine deaths was a bug (specifically, a race condition) that occurred only when the machine operator very rapidly entered a treatment plan; it took days of practice to become able to do this, so the bug did not manifest in testing or when the manufacturer attempted to duplicate it. Other bugs may disappear when the program is run with a debugger; these are heisenbugs (humorously named after the Heisenberg uncertainty principle).
Debugging is still a tedious task requiring considerable effort. Since the 1990s, particularly following the Ariane 5 Flight 501 disaster, there has been a renewed interest in the development of effective automated aids to debugging. For instance, methods of static code analysis by abstract interpretation have already made significant achievements, while still remaining much of a work in progress.
As with any creative act, sometimes a flash of inspiration will show a solution, but this is rare and, by definition, cannot be relied on.
There are also classes of bugs that have nothing to do with the code itself. If, for example, one relies on faulty documentation or hardware, the code may be written perfectly properly to what the documentation says, but the bug truly lies in the documentation or hardware, not the code. However, it is common to change the code instead of the other parts of the system, as the cost and time to change it is generally less. Embedded systems frequently have workarounds for hardware bugs, since to make a new version of a ROM is much cheaper than remanufacturing the hardware, especially if they are commodity items.
Bug management.
Bug management encompasses more than bug tracking, and there exists no industry-wide standard. Proposed changes to software – bugs as well as enhancement requests and even entire releases – are commonly tracked and managed using bug tracking systems. The items added may be called defects, tickets, issues, or, following the agile development paradigm, stories and epics. The systems allow or even require some type of categorization of each issue. Categories may be objective, subjective or a combination, such as version number, area of the software, severity and priority, as well as what type of issue it is, such as a feature request or a bug. Issue tracking systems may also be used, but are more commonly used for tracking external issues such as customer trouble reports.
It is common practice for software to be released with known bugs that are considered "non-critical" as defined by the software producer(s). While software products may, by definition, contain any number of unknown bugs, measurements during testing can provide an estimate of the number of likely bugs remaining; this becomes more reliable the longer a product is tested and developed. Most big software projects maintain two lists of "known bugs"— those known to the software team, and those to be told to users. The second list informs users about bugs that are not fixed in the current release, or not fixed at all, and a workaround may be offered.
In the case of software bugs, "severity" can be an attempt to categorize the symptom or actual behavior of the bug. Severity levels are not standardized and are decided by each software producer, if they are even used. For example, bug severity levels might be "crash or hang", "no workaround" (meaning there is no way the customer can accomplish a given task), "has workaround" (meaning there is a way for the user to recover and accomplish the task), "UI" or "visual defect" (for example, a missing image or displaced button or form element), or "documentation error". Some software publishers use more qualified severities such as "critical", "high", "low," "blocker," or "trivial". The severity of a bug may be a separate category to its priority for fixing, and the two may be quantified and managed separately. For example, if a flaw is found in an application which causes it to crash, yet the crash is so rare and takes, say, ten extremely unusual or unlikely steps to produce it, management may set its priority as "low" or even "will not fix." How the priority for fixing is used is decided internally by each software producer. Priorities are sometimes numerical and sometimes words, such "critical," "high," "low" or "deferred"; note that these can be similar or even identical to severity ratings when looking at different software producers. For example, a software company may decide that priority 1 bugs are always to be fixed for the next release, whereas "5" could mean its fix is put off – sometimes indefinitely.
A software publisher may opt not to fix a particular bug for a number of reasons, including:
The amount and type of damage a software bug can cause naturally affects decision-making, processes and policy regarding software quality. In applications such as manned space travel or automotive safety, since software flaws have the potential to cause human injury or even death, such software will have far more scrutiny and quality control than, for example, an online shopping website. In applications such as banking, where software flaws have the potential to cause serious financial damage to a bank or its customers, quality control is also more important than, say, a photo editing application. NASA's SATC managed to reduce the number of errors to fewer than 0.1 per 1000 lines of code (SLOC) but this was not felt to be feasible for projects in the business world.
A school of thought popularized by Eric S. Raymond as Linus's Law says that popular open-source software has more chance of having few or no bugs than other software, because "given enough eyeballs, all bugs are shallow". This assertion has been disputed, however: computer security specialist Elias Levy wrote that "it is easy to hide vulnerabilities in complex, little understood and undocumented source code," because, "even if people are reviewing the code, that doesn't mean they're qualified to do so."
Security vulnerabilities.
Malicious software may attempt to exploit known vulnerabilities in a system–which may or may not be bugs. Viruses are not bugs in themselves–they are typically programs that are doing precisely what they were designed to do. However, viruses are occasionally referred to as such in the popular press. In addition, it is often a security bug in a computer program that allows viruses to work in the first place.
Well-known bugs.
A number of software bugs have become well-known, usually due to their severity: examples include various space and military aircraft crashes. Possibly the most famous bug is the Year 2000 problem, also known as the Y2K bug, in which it was feared that worldwide economic collapse would happen at the start of the year 2000 as a result of computers thinking it was 1900. (In the end, no major problems occurred.)

</doc>
<doc id="37086" url="http://en.wikipedia.org/wiki?curid=37086" title="Battle of Lechfeld (955)">
Battle of Lechfeld (955)

The Battle of Lechfeld (10 August 955) was a decisive victory for Otto I the Great, King of the Germans, over the Hungarian "harka "Bulcsú and the chieftains Lél (Lehel) and Súr. It is often seen as the defining event in the repulsion of Hungarians incursions into Western Europe. Located south of Augsburg, the Lechfeld is the flood plain that lies along the Lech River. The battle appears as the second Battle of Augsburg in Hungarian historiography. It was followed by the Battle of Recknitz in October. It was important in rallying the East Frankish realm against a foreign enemy.
The first Battle of Lechfeld happened in the same area forty-five years earlier.
Sources.
The most important source is Gerhard's monograph "Vita Sancti Uodalrici", which describes the series of actions from the German point of view. Another source is the chronicler Widukind of Corvey, who provides some important details. The chronicle "Gesta Hungarorum" provides insight from the Hungarian side; however, this chronicle was only written in the 12th century.
Background.
After having put down a rebellion by his son, Liudolf, Duke of Swabia and son-in-law, Conrad, Duke of Lorraine, Otto I the Great, King of the Germans, set out to Saxony, his duchy. Upon arriving in Magdeburg he received reports of the Hungarian invasion. The Hungarians had already invaded once before during the course of the rebellion. This occurred immediately after he had put down a revolt in Franconia. Because of unrest among the Polabian Slavs on the lower Elbe, Otto had to leave most of his Saxons at home. In addition, Saxony was distant from Augsburg and its environs, and considerable time would have elapsed waiting for their arrival. The battle took place six weeks after the first report of an invasion, and historian Hans Delbrück asserts that they could not have possibly made the march in time.
The King ordered his troops to concentrate on the Danube, in the vicinity of Neuburg and Ingolstadt. He did this in order to march on the Hungarian line of communications and catch them in their rear while they were raiding northeast of Augsburg. It was also a central point of concentration for all the contingents that were assembling. Strategically, therefore, this was the best location for Otto to concentrate his forces before making the final descent upon the Hungarians.
There were other troops that had an influence on the course of the battle. On previous occasions, in 932 and 954 for example, there had been Hungarian incursions that had invaded the Germanic lands to the south of the Danube, and then retreated back to their native country via Lotharingia, to the West Frankish Kingdom and finally, through Italy. That is to say, a wide sweeping U-turn that initially started westward, then progressed to the south, and then finally to the east back to their homeland; and thus escaping retribution in Germany. The King was aware of the escape of these Hungarians on the above-mentioned occasions, and was determined to trap them. He therefore ordered his brother, Archbishop Bruno, to keep the Lotharingian forces in Lorraine. He did this with the fear that the Hungarians would follow their plan of retreat on the previous occasions. However, with a powerful enough force of knights pressing them in the front from the west, and an equally strong force of knights chasing them from the east, the Hungarians would be unable to escape.
The Bishop Ulrich defended Augsburg, a border city of Swabia, with a contingent of soldiers. Motivating them with the 23rd Psalm ("Yea, though I walk through the valley of the shadow of death"). While this defense was going on, the King was raising an army to march south.
There is no reliable source on the size of the armies and the numbers are still disputed. The most accepted view is that Otto called up about 8,000 men. The eight, 1,000-strong "legiones" (divisions) included three from Bavaria, two from Swabia, one from Franconia and one from Bohemia, under Prince Boleslav I. The eighth division, commanded by Otto, and slightly larger than the others, included Saxons, Thuringians, and the King's personal guard. The King's contingent probably included seasoned knights of Frankish origin.
According to chronicles, the Hungarian army amounted to 25–50,000 men, but a more realistic figure is 10–25,000 men.
Gerhard writes that the Hungarian forces advanced to the Iller River and placed Augsburg under siege. At this time, Augsburg did not quite touch the left bank of the river, upon which it was situated. The city was defended by Bishop Ulrich. Most probably the fiercest battle took place on August 8 at the eastern gate, which the Hungarians tried to storm in large numbers. The Bishop's men defended bravely and killed the leader of the attack, forcing the Hungarians to withdraw. The next day the Hungarians launched a wider general attack. During the battle, Berchtold of Risinesburg arrived, which heralded the approach of the German army. At the end of the day, the siege was suspended, and the Hungarians prepared for the next day's battle. Count Dietpald led soldiers to Otto's camp during the night.
Battle.
The order of march of the German army was as follows: the three Bavarian contingents, the Frankish contingent under Duke Konrad, the royal unit (the center), the two contingents of Swabians and the Bohemian contingent. The Bavarians were placed at the head of column, according to Delbrück, because they were marching through Bavarian territory and they therefore knew the territory best. All of these were mounted.
According to the chronicler Widukind of Corvey, Otto "pitched his camp in the territory of the city of Augsburg and joined there the forces of Henry I, Duke of Bavaria, who was himself lying mortally ill nearby, and by Duke Conrad with a large following of Franconian knights. Conrad's unexpected arrival encouraged the warriors so much that they wished to attack the enemy immediately."
The arrival of Conrad, the exiled Duke of Lotharingia (Lorraine), and Otto's son-in-law, was particularly heartening because he had recently thrown in his lot with the Magyars, but now returned to fight under Otto; in the ensuing battle he lost his life. A legion of Swabians was commanded by Burchard III, Duke of Swabia, who had married Hedwig, the daughter of Henry, the brother of Otto. Also among those fighting under Otto was Boleslav of Bohemia. About 3,000 Saxons were commanded by Otto himself.
The Hungarians crossed the river and immediately attacked the Bohemians, then later the Swabian legions, but retreated after a short fight. As Otto received word of the attack, he ordered Conrad to recover the baggage train, which Conrad succeeded in doing before returning to the main forces. For Otto, it became evident that this was the time to attack the Hungarians, and he did not hesitate. Despite a volley of arrows from the Hungarians, Otto's army smashed into the Hungarian line, and began to sweep over it.
The Germans were able to fight hand-to-hand with the Hungarians, giving the traditionally nomadic warriors no room to use their usual shoot-and-run tactics. Bulcsú feigned a retreat with part of his force, in an attempt to lure Otto's men into breaking their line in pursuit, but to no avail. The German line maintained formation and routed the Magyars from the field. The German forces maintained discipline and methodically pursued the Magyars for the next couple of days, rather than dispersing jubilantly, as German forces had been known to do in the past. "Some of the enemy sought refuge in nearby villages, their horses being worn out; these were surrounded and burnt to death within the walls." The captured Magyars were either executed, or sent back to their ruling prince, Taksony, missing their ears and noses. The Hungarian leaders Lél, Bulcsú and Sur, who were not Árpáds, were executed after the battle. Duke Conrad was also killed, after he loosened his mail armour in the summer heat and one arrow struck his throat. "Never was so bloody a victory gained over so savage a people," was Widukind's conclusion.
Tactical details.
Otto deployed his divisions in a single line, without reserves. From right to left the line was held by Duke Conrad's Franconians, the three Bavarian divisions, Otto's division and the two Swabian divisions. The Bohemian division defended the camp. The Hungarians mounted a rapid frontal attack in a typical horse archer swarm, raining arrows among the German knights, but this was only a feint. The main attack circled behind Otto's host and struck the camp, routing Boleslav's knights. The Hungarian flanking force then attacked the two Swabian divisions from the rear, while their compatriots attacked in front.
The Swabians were disordered by the double attack, but they did not panic. Instead, they fell back fighting toward the King's division. Otto ordered Conrad to pull his division out from the extreme right and bring it behind the German line to help the Swabians on the enveloped left flank. Conrad brilliantly executed the difficult maneuver, and his knights charged the Hungarian flanking force. Pinned between Conrad and the Swabians, these horsemen were cut to pieces. Meanwhile, Otto and the Bavarians had successfully held off the enemy frontal attack. Once Conrad disposed of the flanking force, Otto led a general advance. Conrad was killed by an arrow.
Seeing the day going against them, the Hungarians bolted for their camp. Fleeing across the river, many were caught in the shallow river bed (made up of banks of pebbles) and killed as they urged their tired horses up the steep and slippery west bank of the Lech. After the Germans stormed and plundered the Hungarian camp, the raiders set out for Hungary. They had to swing a long detour south and east, during which a number of the smaller war parties were overtaken and slaughtered by the enraged local people.
Aftermath.
On the field of battle, the German lords raised Otto on their shields in the Germanic manner and proclaimed him emperor. A few years later, on the strength of this, Otto went to Rome and had himself crowned Holy Roman Emperor by Pope John XII.
The King spent the night after the battle in Augsburg. He specifically issued the order that all river crossings were to be held. This was done so that as many of the Hungarians as possible, and specifically their leaders, could be captured and killed. This strategy was successful, as Duke Henry of Bavaria captured a number of their leaders and killed them.
The Hungarian leaders Bulcsú, Lehel and Sur were taken to Regensburg and executed.
It is disputed how this had affected Hungarian statehood. What is certain is that it was not a crushing defeat, as Otto was not able to chase the army and extend the battle to Hungarian lands. After the defeat, the Hungarians reached the end of the almost 100-year era in which they were seen as the dominating military force in Europe.
After 955, the Hungarians completely ceased all campaigns westwards. In addition, Otto did not launch any further military campaigns against the Hungarians. The Hungarian leader Fajsz was dethroned following the defeat, and was succeeded as Grand Prince of the Hungarians by Taksony.
Importance.
The battle has been viewed as a symbolic victory for the knightly cavalry, who would define European warfare in the High Middle Ages, over the nomadic, light cavalry that characterized warfare during the Early Middle Ages in Central and Eastern Europe.
Paul K. Davis writes, the "Magyar defeat ended more than 90 years of their pillaging western Europe and convinced survivors to settle down, creating the basis for the state of Hungary."

</doc>
<doc id="37087" url="http://en.wikipedia.org/wiki?curid=37087" title="Steve McConnell">
Steve McConnell

Steven C. McConnell is an author of many software engineering textbooks including "Code Complete", "Rapid Development", and "Software Estimation". He is often cited as an expert in software engineering and project management. In 1998, McConnell was named as one of the three most influential people in the software industry by "Software Development" magazine, along with Bill Gates and Linus Torvalds.
Career.
McConnell graduated with a Bachelor's degree in philosophy, minoring in computer science, at Whitman College in Walla Walla, Washington ("magna cum laude", Phi Beta Kappa), and a master's degree in software engineering from Seattle University. He then pursued a career in the desktop software industry, working at Microsoft, Boeing, the Russell Investment Group and several other Seattle area firms. At Microsoft, McConnell worked on TrueType as part of Windows 3.1. At Boeing, he worked on a Strategic Defense Initiative project.
McConnell published his first book, "Code Complete", in 1993. As of March 2007 he has been the CEO and Chief Software Engineer of Construx Software, a software engineering consulting firm he founded in 1996.
From 1996 to 1998, he was the editor of the "Best Practices" column in the IEEE Software magazine. From 1998 to 2002, he served as the editor-in-chief of the magazine. Since 1998, he has served on the Panel of Experts of the SWEBOK project. He has also served as the chair of the IEEE Computer Society's Professional Practices Committee. Since March 2006 McConnell has been a member of the executive committee for the Computer Society's .

</doc>
<doc id="37091" url="http://en.wikipedia.org/wiki?curid=37091" title="Jess (programming language)">
Jess (programming language)

Jess is a rule engine for the Java platform that was developed by Ernest Friedman-Hill of Sandia National Labs. It is a superset of the CLIPS programming language. It was first written in late 1995. The language provides rule-based programming for the automation of an expert system, and is frequently termed as an "expert system shell". In recent years, intelligent agent systems have also developed, which depend on a similar capability.
Rather than a procedural paradigm, where a single program has a loop that is activated only one time, the declarative paradigm used by Jess continuously applies a collection of rules to a collection of facts by a process called "pattern matching". Rules can modify the collection of facts, or they can execute any Java code.
The Jess rules engine utilizes the Rete algorithm, and can be utilized to create:
License.
While CLIPS is licensed as open source, Jess is not open source.
JESS is free for educational and government use but a license is required to use JESS for commercial systems.
Code examples.
Code examples:
Sample code:

</doc>
<doc id="37096" url="http://en.wikipedia.org/wiki?curid=37096" title="Floating-point unit">
Floating-point unit

A floating-point unit (FPU, colloquially a math coprocessor) is a part of a computer system specially designed to carry out operations on floating point numbers. Typical operations are addition, subtraction, multiplication, division, square root, and bitshifting. Some systems (particularly older, microcode-based architectures) can also perform various transcendental functions such as exponential or trigonometric calculations, though in most modern processors these are done with software library routines.
In general purpose computer architectures, one or more FPUs may be integrated with the central processing unit; however many embedded processors do not have hardware support for floating-point operations.
When a CPU is executing a program that calls for a floating-point operation, there are three ways to carry it out:
Some systems implemented floating point via a coprocessor rather than as an integrated unit. This could be a single integrated circuit, an entire circuit board or a cabinet. Where floating-point calculation hardware has not been provided, floating point calculations are done in software, which takes more processor time but which avoids the cost of the extra hardware. For a particular computer architecture, the floating point unit instructions may be emulated by a library of software functions; this may permit the same object code to run on systems with or without floating point hardware. Emulation can be implemented on any of several levels: in the CPU as microcode (not a common practice), as an operating system function, or in user space code. When only integer functionality is available the CORDIC floating point emulation methods are most commonly used.
In most modern computer architectures, there is some division of floating-point operations from integer operations. This division varies significantly by architecture; some, like the Intel x86 have dedicated floating-point registers, while some take it as far as independent clocking schemes.
Floating-point operations are often pipelined. In earlier superscalar architectures without general out-of-order execution, floating-point operations were sometimes pipelined separately from integer operations. Since the early and mid-1990s, many microprocessors for desktops and servers have more than one FPU.
Floating-point library.
Some floating-point hardware only supports the simplest operations - addition, subtraction, and multiplication. But even the most complex floating-point hardware has a finite number of operations it can support - for example, none of them directly support arbitrary-precision arithmetic.
When a CPU is executing a program that calls for a floating-point operation that is not directly supported by the hardware, the CPU uses a series of simpler floating-point operations. In systems without any floating-point hardware, the CPU emulates it using a series of simpler fixed-point arithmetic operations that run on the integer arithmetic logic unit.
The software that lists the necessary series of operations to emulate floating-point operations is often packaged in a floating-point library.
Integrated FPUs.
In some cases, FPUs may be specialized, and divided between simpler floating-point operations (mainly addition and multiplication) and more complicated operations, like division. In some cases, only the simple operations may be implemented in hardware or microcode, while the more complex operations are implemented as software.
In some current architectures, the FPU functionality is combined with units to perform SIMD computation; an example of this is the augmentation of the x87 instructions set with SSE instruction set in the x86-64 architecture used in newer Intel and AMD processors.
Add-on FPUs.
In the 1980s, it was common in IBM PC/compatible microcomputers for the FPU to be entirely separate from the CPU, and typically sold as an optional add-on. It would only be purchased if needed to speed up or enable math-intensive programs.
The IBM PC, XT, and most compatibles based on the 8088 or 8086 had a socket for the optional 8087 coprocessor. The AT and 80286-based systems were generally socketed for the 80287, and 80386/80386SX based machines for the 80387 and 80387SX respectively, although early ones were socketed for the 80287, since the 80387 did not exist yet. Other companies manufactured co-processors for the Intel x86 series. These included Cyrix and Weitek.
Coprocessors were available for the Motorola 68000 family, the 68881 and 68882. These were common in Motorola 68020/68030-based workstations like the Sun 3 series. They were also commonly added to higher-end models of Apple Macintosh and Commodore Amiga series, but unlike IBM PC-compatible systems, sockets for adding the coprocessor were not as common in lower end systems.
There are also add-on FPUs coprocessor units for microcontroller units (MCUs/µCs)/single-board computer (SBCs), which serve to provide floating-point arithmetic capability. These add-on FPUs are host-processor-independent, possess their own programming requirements (operations, instruction sets, etc.) and are often provided with their own integrated development environments (IDE)s.

</doc>
<doc id="37101" url="http://en.wikipedia.org/wiki?curid=37101" title="Think tank">
Think tank

A think tank (or policy institute, research institute, etc.) is an organization that performs research and advocacy concerning topics such as social policy, political strategy, economics, military, technology, and culture. Most policy institutes are non-profit organizations, which some countries such as the United States and Canada provide with tax exempt status. Other think tanks are funded by governments, advocacy groups, or businesses, or derive revenue from consulting or research work related to their projects.
The following article lists global policy institutes according to continental categories, and then sub-categories by country within those areas. These listings are not comprehensive, given that more than 6,800 think tanks exist world wide.
History.
While the term "think tank" with its present sense originated in the 1950s, such organizations date to the 19th century. The Institute for Defence and Security Studies (RUSI) was founded in 1831 in London. The Fabian Society in Britain dates from 1884.
The oldest American think tank, the Carnegie Endowment for International Peace, was founded in Washington, D.C. in 1910 by philanthropist Andrew Carnegie. Carnegie charged trustees to use the fund to "hasten the abolition of international war, the foulest blot upon our civilization." The Brookings Institution was founded shortly thereafter in 1916 by Robert S. Brookings and was conceived as a bipartisan "research center modeled on academic institutions and focused on addressing the questions of the federal government." 
After 1945, the number of policy institutes increased, as many small new ones were formed to express various issue and policy agendas. Until the 1940s, most think tanks were known only by the name of the institution. During the Second World War, think tanks were often referred to as "brain boxes" after the slang term for skull. The phrase "think tank" in wartime American slang referred to rooms where strategists discussed war planning. Later the term "think tank" was used to refer to organizations that offered military advice—such as, perhaps most notably, the RAND Corporation, founded originally in 1946 as an offshoot of Douglas Aircraft Corporation, and which became an independent corporation in 1948.
For most of the 20th century, independent public policy institutes that performed research and provided advice concerning public policy were found primarily in the United States, with a much smaller number in Canada, the UK and Western Europe. Although think tanks existed in Japan for some time, they generally lacked independence, having close associations with government ministries or corporations. There has been a veritable proliferation of "think tanks" around the world that began during the 1980s as a result of globalization, the end of the Cold War, and the emergence of transnational problems. Two-thirds of all the think tanks that exist today were established after 1970 and more than half were established since 1980.
The effect of globalization on the proliferation of think tanks is most evident in regions such as Africa, Eastern Europe, Central Asia, and parts of Southeast Asia, where there was a concerted effort by the international community to assist in the creation of independent public policy research organizations. A recent survey performed by the Foreign Policy Research Institute’s Think Tanks and Civil Societies Program underscores the significance of this effort and documents the fact that most of the think tanks in these regions have been established during the last 10 years. Presently there are more than 4,500 of these institutions around the world. Many of the more established think tanks, having been created during the Cold War, are focused on international affairs, security studies, and foreign policy.
Also see the United Nations Development Programme definition.
Types.
Think tanks vary by ideological perspectives, sources of funding, topical emphasis and prospective consumers. Some think tanks, such as the Heritage Foundation, which promotes conservative principles, and the Center for American Progress, a progressive organization, are more partisan in purpose. Others, including the Tellus Institute, which emphasizes social and environmental topics, are more issue-oriented groups. Still others, such as the Cato Institute, promote libertarian social and economic theories based on Friedrich von Hayek's idea of free markets and individual liberty.
Funding sources and the consumers intended also define the workings of think tanks. Some receive direct government assistance, while others rely on private individual or corporate donors. This will invariably affect the degree of academic freedom within each policy institute and to whom or what the institution feels beholden. Funding may also represent who or what the institution wants to influence; in the United States, for example, "Some donors want to influence votes in Congress or shape public opinion, others want to position themselves or the experts they fund for future government jobs, while others want to push specific areas of research or education."
A new trend, resulting from globalization, is collaboration between policy institutes in different countries. For instance, the Carnegie Endowment for International Peace operates offices in Washington, D.C., Beijing, Beirut, Brussels and Moscow.
The Think Tanks and Civil Societies Program (TTCSP) at the University of Pennsylvania annually rates policy institutes worldwide in a number of categories and presents its findings in the "Global Go-To Think Tanks" rating index. However, this method of the study and assessment of policy institutes has been criticised by researchers such as Enrique Mendizabal and Goran Buldioski, Director of the Think Tank Fund, assisted by the Open Society Institute.
Several authors have indicated a number of different methods of describing policy institutes in a way that takes into account regional and national variations. For example:
Alternatively, one could use some of the following criteria:
Advocacy by think tanks.
In some cases, corporate interests and political groups have found it useful to create policy institutes, advocacy organizations, and think tanks. For example, The Advancement of Sound Science Coalition was formed in the mid-1990s to dispute research finding an association between second-hand smoke and cancer. According to an internal memorandum from Philip Morris Companies referring to the United States Environmental Protection Agency (EPA), "The credibility of the EPA is defeatable, but not on the basis of ETS [environmental tobacco smoke] alone... It must be part of a larger mosaic that concentrates all the EPA's enemies against it at one time."
According to the Fairness and Accuracy in Reporting, both left-wing and right-wing policy institutes are often quoted and rarely identified as such. The result is that think tank "experts" are sometimes depicted as neutral sources without any ideological predispositions when, in fact, they represent a particular perspective. In the United States, think tank publications on education are subjected to expert review by the National Education Policy Center's "Think Twice" think tank review project.
A policy institute is often a "tank", in the intellectual sense: discussion only in a sheltered group protected from outside influence isolates the participants, subjects them to several cognitive biases (groupthink, confirmation bias) and fosters members' existing beliefs. This results in surprisingly radical and even unfeasible ideas being published. Many think tanks, however, purposefully attempt to alleviate this problem by selecting members from diverse backgrounds.
A 2014 "New York Times" report asserted that foreign governments buy influence at many United States think tanks. According to the article: "More than a dozen prominent Washington research groups have received tens of millions of dollars from foreign governments in recent years while pushing United States government officials to adopt policies that often reflect the donors’ priorities."
Functional method in Latin America.
Research done by Enrique Mendizabal shows that Latin American think tanks play various roles depending on their origins, historical development and relations to other policy actors. In this study, Orazio Bellettini from Grupo FARO suggests that they:
How a policy institute addresses these largely depends on how they work, their ideology vs. evidence credentials, and the context in which they operate (including funding opportunities, the degree and type of competition they have, their staff, etc.).
This functional method addresses the inherit challenge of defining a think tank. As Simon James said in 1998, "Discussion of think tanks...has a tendency to get bogged down in the vexed question of defining what we mean by ‘think tank’—an exercise that often degenerates into futile semantics. It is better (as in the Network Functions Approach) to describe what the organisation should do. Then the shape of the organisation should follow to allow this to happen. The following framework (based on Stephen Yeo’s description of think tanks’ mode of work) is described in Enrique Mendizabal's blog "onthinktanks":
First, policy institutes may work in or base their funding on one or more of:
Second, policy institutes may base their work or arguments on:
According to the National Institute for Research Advancement, a Japanese policy institute, think tanks are "one of the main policy actors in democratic societies ..., assuring a pluralistic, open and accountable process of policy analysis, research, decision-making and evaluation". A study in early 2009 found a total of 5,465 think tanks worldwide. Of that number, 1,777 were based in the United States and approximately 350 in Washington DC alone.
Argentina.
Argentina is home to 122 think tanks; many specializing in public policy and economics issues, Argentina ranks fifth in the number of these institutions worldwide.
Brazil.
Working on public policies, Brazil hosts, for example, Instituto Liberdade, a University-based Center at Tecnopuc inside the Pontifícia Universidade Católica do Rio Grande do Sul, located in the South Region of the country, in the city of Porto Alegre. Instituto Liberdade is among the Top 40 think tanks in Latin America and the Caribbean, according to the 2009 Global Go To Think Tanks Index a report from the University of Pennsylvania's Think Tanks and Civil Societies Program (TTCSP).
Fundação Getulio Vargas (Getulio Vargas Foundation (FGV or GV)) is a Brazilian higher education institution founded on December 20, 1944. It offers regular courses of Economics, Business Administration, Law, Social Sciences and Information technology management. Its original goal was to train people for the country's public- and private-sector management. Other courses began to be offered as the institution grew. It is considered by "Foreign Policy" magazine to be a top-5 "policymaker think-tank" worldwide. The Igarapé Institute is a respected Brazilian think tank focusing on public security and policing.
Mexico.
CIDE is one of the most important think tank institutes. The researching lines are the "public policies", "public choice", "democracy", and "economy".
CIDAC – The Center of Research for Development (Centro de Investigación para el Desarrollo, Asociación Civil) is a not-for-profit think tank that undertakes research and proposes viable policy options for Mexico's economic and democratic development. The organization seeks to promote open, pluralistic debate pursuing: the Rule of Law & Democracy, market economics, social development, and strengthening Mexico-U.S. relations.
CIEP - The Economic and Budgeting Research Center (Centro de Investigacion Economica y Presupuestaria) is a non-governmental organization which main goal is to influence the development of public economics through formal analysis and research. It is composed of experts in economic and budgetary issues, that with, technical criteria, plural ideas and no partisan agenda, pursue a well informed society that comprehends government decisions on use and allocation of public resources.
Asian think tanks.
China.
In the People's Republic of China a number of think tanks are sponsored by governmental agencies, like Development Research Center of the State Council, but still retain sufficient non-official status to be able to propose and debate ideas more freely. In January 2012, the first non-official think-tank in China, South Non-Governmental Think-Tank, was established in Guangdong province.
Hong Kong.
In Hong Kong, those early think tanks established in the late 1980s and early 1990s focused on the political development including first direct Legislative Council members election in 1991 and the political framework of "One Country, Two Systems" manifested in the Sino-British Joint Declaration. After the transfer of sovereignty to China in 1997, more and more think tanks were established by various groups of intellectuals and professionals. They have various missions and objectives including promoting civic education; undertaking research on economic social and political policies; promoting "public understanding of and participation in the political, economic, and social development of the Hong Kong Special Administrative Region".
India.
India has the fifth largest number of think tanks in the world. Many are based in New Delhi, and a few are government sponsored. A number of these work on foreign policy and security issues. There are few think tanks like Centre for Civil Society who promote liberal social and economic ideas and others like the Rakshak Foundation, who encourage students to do empirical research and gain first hand experience in public policy issues. Think tanks with a development focus are those like the National Centre for Cold-chain Development ('NCCD') which serve to bring inclusive policy change by supporting the Planning Commission and related government bodies with industry specific inputs - in this case set up at the behest of the government to direct cold chain development. Other think tanks in India could be privately organisations with voluntary contributions from mutli-disciplinary professionals and academic or industry leaders.
By way of total number, India is ranked 4th with 269 think tanks. However, no Indian think tank appeared in University of Pennsylvania's "Global 50 annual list for 2012". Indian think tanks face several challenges such as — insufficient funding, lack of skilled staff and limited support from the government. Very few think tanks can afford a heavy investment in computing infrastructure. For example, a single user licence for the TIMES suite, a popular energy modelling software, costs over Rs 10 lakh ($18,000). Since Government departments are often reluctant to share data they collect, access to quality data is difficult. Although the Right to Information Act addresses this to some extent, it is still a time-consuming process for obtaining data.
Initiatives such as National Data Sharing and Accessibility Policy (NDSAP) ( to ensure systemic and semantic consistency of data collection and data sharing), National e-Governance Plan (to automate administrative processes) and National Knowledge Network (NKN) (for data and resource sharing amongst education and research institutions), if implemented properly, should help improve the quality of work done by think tanks.
Iran.
Several organizations established in Iran since the late 1990s offer a unique blend of interdisciplinary research.
Japan.
Japan has over 100 think tanks, most of which cover not only policy research but also economy, technology and so on. Some are government related, but most of the think tanks are sponsored by the private sector.
South Korea.
In Korea, the National Research Council for Economics, Humanities and Social Sciences (NRCS) is a public institution that supported 23 related research institutes in their quest to achieve the effective management and improvement of their research environment under the Prime Minister. It was established with the objective of supporting and fostering research institutes in the area of economics and social science and systematically supervising them in their contributions to the production of high-quality national policy research and the development of a concrete knowledge industry. The NRCS was reorganized in 2005 through the merger of the Korea Council of Economic and Social Research Institutes and the Korea Research Council for Humanities and Social Sciences established separately in 1999.
The Center for Free Enterprise (Korea) is a free market think tank located in Seoul, South Korea, with pages in English and Korean.
Malaysia.
Most Malaysia think tanks are government or political party related. They focus on defence, politicsm and policies. Notable ones include the Institute of Strategic and International Studies (ISIS), Malaysian Strategic Research Centre (MSRC), International Institute of Advanced Islamic Studies (IAIS) Malaysia, Institute for Democracy and Economic Affairs (IDEAS), Maritime Institute of Malaysia (MIMA) and Penang Institute (formerly Socio-economic and Environmental Research Institute (SERI)). IDEAS is Malaysia's first policy oriented classical liberal think tank. Institute for Pioneering of Education and Economic Excellence (INSPIRE).
Pakistan.
Pakistan's think tanks mainly revolve around social policy, internal politics, foreign security issues, and regional geo-politics. Most of these are centered around the capital, Islamabad with exception of Social Policy and Development Centre (SPDC) located in Karachi. Most recently, institutes such as the National University of Sciences and Technology have embarked on creating industrial linkages to create think tanks focusing on industrial and economic growth issues.
Sustainable Development Policy Institute (SDPI) is a think-tank in policy advocacy and research particularly in the area of environment and social development . SDPI was founded in August 1992, on the recommendation of Pakistan National Conservation Strategy (NCS), also called Pakistan’s Agenda 21. The Institute acts as both a generator of original research on sustainable development issues and as a knowledge disseminator.
There are several other think tanks as well, such as those concerning the state of education in the country, which hold many former or present educators. There are also think tanks concerning human rights, women rights, labour rights, justice, city development, heritage protection, and environmental protection, all headed by the country's urban dwelling, educated elite living, most of whom have studied or worked abroad. There remains a vacuum for former high ranking Government officials and party members to contribute to the think tank and policy advocacy process in these areas.
Foundation for ‘Progress’, a non-profit approved Under Section 2(36)C of FBR is an upcoming and a leading 'Think Tank' and a 'Sustainable Development Organization', founded in 2001. Its more than five hundred affiliated scholars and experts in diverse sectors, provide public policy research, analysis, advocacy and educative services and activities. In addition it also focuses on poverty alleviation through targeting of health, education and environment sectors. These services and activities are carried out under its three core programs i.e. Leadership For Pakistan, Meeting Challenges - National and Global and the program of Building Partnerships, Promoting Sustainability.
Other think tanks concern religion and how its influence could grow in an already religious country. These are centred throughout the country and work under the umbrella of the mammoth Jamaat-e-Islami with headquarters in Lahore and has immense global influence, reach and regard among more traditional Muslims. However the Jamat is a political party, and affiliations with reputable think tanks in Pakistan are not clear.
Some notable Pakistani think tanks are the MEASAC Research Center, the Institute of Policy Studies, the Pakistan Institute of International Affairs, PakistanKaKhudaHafiz and the Corporate Advisory Council (NUST). A new online think tank "Pakistan Focus" has been formed to conduct broad based study on domestic, regional and global events impacting Pakistan.
Most are known to the general public through seminars and newspaper articles, or conducting workshops and lectures at colleges and universities.
Philippines.
The Institute for Strategic and Development Studies (ISDS Philippines) is an independent non-profit, policy research and advocacy institution that is also involved in training activities in cooperation with other training institutions at home and abroad. It was founded by a group of academics from the University of the Philippines Diliman in April 1991. It was established in response to the need for an ongoing evaluation and interpretation of the changes in national and international affairs by serious international, regional, and national analysts. It was also aimed at responding to the need to provide academics a venue for research to enrich teaching and to provide inputs to policy-making.
The PIPVTR is an independent, non-stock, non-profit, non-governmental research organization officially registered at the Securities and Exchange Commission (SEC) on 29 November 2007 as Philippine Institute for Political Violence and Terrorism Research. It was first conceptualized in September 2005 by a group of experts, academics and practitioners who see the need to establish a center in the Philippines dedicated to the study of political violence and terrorism and their implications for peace and security.
Think tanks in the Philippines could be generally categorized in terms of their linkages with the national government. Several were set up by the Philippine government for the specific purpose of providing research input into the policy-making process.
Sri Lanka.
Sri Lanka has a number of think tanks that are in the form governmental, non-governmental and corporate organizations.
The Marga Institute is probably the oldest non-governmental think tank in Sri Lanka, incorporated in April 1972. It started as a civil society initiative based on ideas generated for the establishment of a development studies institution by a group of public officers, academics and professionals. The principal activity of the Marga Institute is the study and critical evaluation of past and on-going development processes in Sri Lanka. It approaches and analyses development as a process and as a condition in which economic growth, social equity, political freedom and participation are integrally linked and must reinforce one another. Its programme of research therefore covers three main areas of development-economic, social and political. Consequently, it draws on a wide range of social science disciplines and brings a multi-disciplinary approach to its work. The Institute's outlook and conceptual framework have been non-partisan and ideologically open. This has enabled it to provide a forum in which groups with different ideological and political orientations have engaged in a rational and constructive dialogue on highly controversial national issues.
Another Sri Lankan think tank with a long history is the Institute of Policy Studies (IPS) which has acquired a unique position as an authoritative independent voice in economic policy analysis, working closely with the government, private sector, academia and civil society. It was conceived in the mid-1980s as an autonomous institution designed to promote policy-oriented economic research and to strengthen the capacity for medium-term policy analysis in Sri Lanka. It was established by an Act of Parliament in December 1988 and was formally set up as a legal entity by gazette notification in April 1990 managed by a Board of Governors and the Executive Director. From its inception, the IPS was supported through a collaborative project between the Royal Netherlands Government and the Government of Sri Lanka under four phases of operations to allow the Institute to gradually diversify its income and resource base. Operational independence from financial and administrative regulations of the government were very much part of the rationale for setting up an independent IPS. Since its inception, the IPS has functioned under the key ministries involved in economic policy making and implementation in Sri Lanka while enjoying considerable autonomy in setting and implementing its research programme.
The Centre for Poverty Analysis (CEPA) was established in May 2001 by nine Sri Lankan professionals working on poverty related issues, with the intention of institutionalising poverty impact monitoring and poverty analysis within the Sri Lankan institutional landscape. It was incorporated as a non-profit company under Section 21 of the Companies Act. The catalyst for founding CEPA came from an assessment of the need for special emphasis on applied policy relevant advice. CEPA sought to fill this void by providing practice oriented services that were grounded in sound empirical research was oriented towards the market for policy making in the development field. In the past decade CEPA has succeeded in developing a body of knowledge on poverty and has developed fresh and challenging perspectives on poverty; explored alternative dimensions; conducted in-depth analyses of specific poverty conditions; contributed to a better understanding of poverty in Sri Lanka and formulated policy initiatives to alleviate the multi-dimensional facets of poverty.
Verité Research is perhaps the best recognised interdisciplinary think tank in the country, providing strategic analysis and advice for governments and the private sector in Asia. Founded by Dr. Nishan de Mel, a former senior Sri Lankan policy maker, the organisation's clients include multinational firms, multilateral agencies, diplomatic missions, government agencies and civil society actors. It features research divisions in economics, politics, law and media.
Recently, the Ministry of External Affairs of Sri Lanka (formerly known as the Ministry of Foreign Affairs) established a "Foreign Policy Think Tank" to facilitate the professional advancement of the country's foreign policy and the conduct of its external affairs. The structure of government Think Tanks in Sri Lanka are structured with the help of many academics and intellectuals affiliated with the government.
The Lakshman Kadirgamar Institute of International Relations and Strategic Studies is a regionally acclaimed policy-studies institute that is often referred to as a think tank. The Institute of Policy Studies in Colombo is another policy planning related think tank. The International Center for Ethnic Studies (located both in Colombo and Kandy) is another research-related think tank. There are several other focus-research institutes throughout the country that may be referred to as Think Tanks, such as the Marga Institute of Sri Lanka.
Other think tanks in Sri Lanka include the International Centre for Ethnic Studies, the Centre for Policy Alternatives the Islamic Think Tank and the Sri Lanka Think Tank – UK. Many private and government universities in Sri Lanka have research-related think tanks.
European think tanks.
Armenia.
According to the Global Go Think Thank Report 2012, there are around 14 think tanks in Armenia of which the largest part is located in Yerevan. The Economic Development and Research Center (EDRC), International Center for Human Development (ICHD) and the Armenian International Policy Research Group are among the most active and well known think tanks in the country.
Albania.
Traditionally in Albania, there are not many think tanks. In the last period, one of the main active is the Albania Centre for Energy Regulation & Conservation - ACERC.
Belgium.
Brussels hosts most of the European Institutions, hence a large number of international think tanks are based there. Among them there are, Bruegel, the Centre for European Policy Studies (CEPS), Centre for the New Europe (CNE), the European Centre of International Political Economy (ECIPE), the European Policy Centre (EPC), "The European Institute for International Law and International Relations" (EIILIR), the Friends of Europe, the Global Governance Institute (GGI), Libera! (think tank), the Lisbon Council, Sport and Citizenship, and ThinkYoung. Although headquartered in the Netherlands the European Centre for Development Policy Management (ECDPM) also maintains an office in Brussels. The Centre for Economic Policy Research (CEPR), though based in London, is a network of researchers based throughout Europe that contributes actively to European policy debates.
Bulgaria.
Bulgaria has a number of think tanks providing expertise and shaping policies. Most active are:
Croatia.
In Croatia, Innovation Institute is a NGO with a mission to develop innovative potential by promoting creativity, innovativeness, unorthodox thinking and questioning existing dogmas. It aims at changing the culture from being focused on redistribution of value to being focused on value creation. The key target segments include: managers, entrepreneurs, policy makers, academics and students.
Finland.
Finland has several small think tanks that provide expertise in very specific fields. Vasemmistofoorumi researches the future of leftism, OK Do is a socially-minded design thinking organization, Demos Helsinki is a think tank that researches future society and Culture Crisis Management is political artists' think tank.
In addition to specific independent think tanks, the largest political parties have their own think tank organizations. This is mainly due to support granted by state for such activity. The corporate world has focused their efforts to central representative organization Confederation of Finnish Industries, which acts as think tank in addition to negotiating salaries with workers unions. Furthermore, there is the Finnish Business and Policy Forum ("Elinkeinoelämän valtuuskunta", EVA). Agricultural and regional interests, associated with The Central Union of Agricultural Producers and Forest Owners ("Maa- ja metsätaloustuottajain Keskusliitto", MTK) and the Centre Party, are researched by Pellervo Economic Research ("Pellervon taloustutkimus", PTT). The Central Organisation of Finnish Trade Unions ("Suomen Ammattiliittojen Keskusjärjestö", SAK) and the Social Democratic Party are associated with the Labour Institute for Economic Research ("Palkansaajien tutkimuslaitos", PT). Each of these organizations often release forecasts concerning the national economy.
France.
The French Institute of International Relations (IFRI) was founded in 1979 and is the third oldest think tank of western Europe, after Chatham House (UK, 1920) and the Stockholm International Peace Research Institute (Sweden, 1960). The primary goals of IFRI are to develop applied research in the field of public policy related to international issues, and foster interactive and constructive dialogue between researchers, professionals, and opinion leaders. France also hosts the European Union Institute for Security Studies (EUISS), a Paris-based agency of the European Union and think tank researching security issues of relevance for the EU. There are also a number of pro-business think tanks, notably the Paris-based Fondation Concorde. The foundation focuses on increasing the competitiveness of French SME's and aims to revive entrepreneurship in France.
On the left, the main think tanks in France are the Fondation Jean Jaures, which is organizationally linked to the French Socialist Party, and Terra Nova. Terra Nova is an independent left-leaning think tank, although it is nevertheless considered to be close to the Socialists. It works on producing reports and analyses of current public policy issues from a progressive point of view, and contributing to the intellectual renewal of social democracy.
Germany.
In Germany all of the major parties are loosely associated with research foundations that play some role in shaping policy, but generally from the more disinterested role of providing research to support policymakers than explicitly proposing policy. These include the Friedrich-Ebert-Stiftung (Social Democratic Party-aligned), the Konrad-Adenauer-Stiftung (Christian Democratic Union-aligned), the Hanns-Seidel-Stiftung (Christian Social Union-aligned), the Heinrich-Böll-Stiftung (aligned with the Greens), Friedrich Naumann Foundation (Free Democratic Party-aligned) and the Rosa Luxemburg Foundation (aligned with Die Linke).
The German Institute for International and Security Affairs is a prominent example of a German foreign policy think tank. Atlantic Community think tank is an example of independent, non-partisan and non-profit organization set up as a joint project of Atlantische Initiative e.V. and Atlantic Initiative U.S. The Institute for Media and Communication Policy is the leading think tank in the realm of media.
Greece.
In Greece there are many think tanks, also called research organisations or institutes.
Ireland.
The Economic and Social Research Institute (ESRI) is an independent research institute in Dublin, Ireland. Its research focuses on Ireland's economic and social development to inform policy-making and societal understanding.
The Institute of International and European Affairs (IIEA) is Ireland’s leading think tank on European and International affairs
The Iona Institute is a conservative, Catholic think tank.
Tasc (Think tank for Action on Social Change) is an Irish left wing think tank.
Latvia.
While think tanks are not widespread in Latvia, as opposed to single issue advocacy organizations, there are several noticeable institutions in the Latvian think tank landscape:
There are several think tanks that are established and operate under the auspices of Universities. Such as:
Montenegro.
Recently, the Ministry of Science established a Think Tank "Montenegro Future Forum MFF", with the coordination of Science Council Government Montenegro and the support of Global Think Tank Millennium Project- Washington US - Montenegro Node -, to facilitate the country's interdisciplinary expertise and decision making policy . The Montenegro Future Forum is structured in 9 science categories, with the help of many academics and intellectuals affiliated with the public organizations . Here are categories of MFF : 1. Economy, tourism and sustainable development ; 2. Law and institutional framework ; 3. National identity ; 4. ICT ; 5. Energy, new materials, innovation ; 6. Health ; 7. Youth and sport ; 8. Agriculture ; 9. Science in practice.
MFF is led by Board consisted of : Sanja Vlahovic, Minister of Science, chairman, Andjelko Lojpur, Science Council, member, Mileta Golubovic, MFF coordinator, Milan Maric, Millennium Project, member.
There is also WEB project TeleMontenegro, initiated by Millennium project, established and operate under the auspices of Ministry of Science, with aim to promote and collaborate with MFF and other creative science resources in Montenegro and abroad, to prevent "brain drain" and provide "brain gain" in Montenegro.
Netherlands.
All major political parties in the Netherlands have state-sponsored research foundations that play a role in shaping policy. The Dutch government also has its own think tank: the Scientific Council for Government Policy.
One of the oldest think tanks in the Netherlands is the Transnational Institute in Amsterdam (1974). Maastricht in the Netherlands is also the headquarters of the European Centre for Development Policy Management (ECDPM) a "think and do tank" with a Europe and Africa focus. Amsterdam is the host of the European Student Thinkthank which is a truly pan-European thinkthank with ambassadors all over Europe.
Poland.
There is a large pool of think-tanks in Poland; none of them stand out however. The oldest state-sponsored think tank is The Western Institute in Poznań (Polish: Instytut Zachodni, German West-Institut, French: L'Institut Occidental).The second oldest is the Polish Institute of International Affairs (PISM) established in 1947. The other most important state-sponsored think tank is the Centre for Eastern Studies (OSW), which specialises in the countries neighbouring Poland and in the Baltic Sea region, the Balkans, the Turkey, the Caucasus and Central Asia. Among the private think tanks the most important are: the Center for Social and Economic Research (CASE) and Institute for Structural Research (IBS) on economic policy, The Casimir Pulaski Foundation on foreign policy, demosEUROPA on EU affairs, the Institute of Public Affairs (ISP) on social policy, the Center for International Affairs (CSM) and The Sobieski Institute.
Portugal.
Founded in 1970, the SEDES is one of the oldest Portuguese civic associations and think tanks. Contraditório think tank was founded in 2008. Contraditório is a non-profit, independent and non-partisan think tank.
Romania.
Romania's largest think tank is the Romanian Academic Society (SAR), which was founded in 1996.
The Institute for Public Policy (IPP) is a think-tank established in 2001 with the aim to support the development of democratic processes in Romania through in-depth research, comprehensive debates and non-partisan public policy analysis. Its mission is to contribute to a better process of public policy formulation in Romania. From its very inception, the Institute adhered to high professional standards and to promote concrete, objective and data-supported policy measures, with the aim to contribute to a consolidation of the democratic system in Romania by promoting the idea of public policy designed in accordance with world know standards. IPP developed and consolidated a recognized expertise in the fields of public administration reform (public services reform, modernization of the civil service body, fiscal decentralization), political parties finance, analysis of electoral systems and processes, health reform, public procurement, anti corruption policies. This was achieved by working with specialized personnel and by permanent collaboration with experts in the aforementioned fields. Since 2004, IPP is a member organization of the Policy Association for an Open Society (PASOS) network, together with other similar organizations from 22 countries. IPP motto is It's all about thinking.
Russia.
According to the Foreign Policy Research Institute, Russia has 112 think tanks, while Russian think tanks claimed four of the top ten spots in 2011’s "Top Thirty Think Tanks in Central and Eastern Europe".
Notable Russian think tanks include:
Serbia.
Serbia's best known think thank is the Foundation for the Advancement of Economics - FREN, founded in 2005 by the Belgrade University’s Faculty of Economics. Thanks to the quality and relevance of its research, FREN has established itself as one of the leading economic think tanks in Serbia. FREN's team comprises a network of over 30 associates who regularly and systematically monitor economic trends in Serbia, conduct in-depth research and encourage and facilitate the exchange of information and availability of economic data.
Slovakia.
Besides the international think tanks present in the surrounding countries as well (with Open Society Foundations being the most notable one) Slovakia has a host of its own think tanks as well. Some of the think tanks in Slovakia focus on public policy issues, such as Institute of Public Affairs ("Inštitút pre verejné otázky" or "IVO" in Slovak) or Central European Labour Studies Institute ("Stredoeurópsky inštitút pre výskum práce" or "CELSI" in Slovak). Others specialize on human rights issues such as minority protection, for example Forum Minority Research Institute ("Fórum Kisebbségkutató Intézet" or "Fórum Intézet" in Hungarian and "Fórum inštitút pre výskum menšín" or "Fórum inštitút" in Slovak). Since some of the Slovak think tanks are perceived to be associated with right-wing and liberal parties of Slovakia (with the perception being particularly strong among Slovak nationalists), findings and proposals made by these organizations are generally resented or ignored by left-wing supporters and nationalists., k.h.
Spain.
In Spain, think tanks are progressively raising their public profile. There are now at least 30 think tanks in the country.
One of the most influential Spanish think tanks is the Elcano Royal Institute, created in 2001 following the example of the Royal Institute of International Affairs (Chatham House) in the UK, although it is closely linked to (and receives funding from) the government in power. More independent but clearly to the left of the political spectrum are the Centro de Investigaciones de Relaciones Internacionales y Desarrollo (CIDOB) founded in 1973; and the Fundación para las Relaciones Internacionales y el Diálogo Exterior (FRIDE) established in 1999 by Diego Hidalgo and main driving force behind projects such as the Club de Madrid, a group of democratic former heads of state and government, the Foreign Policy Spanish Edition and DARA (international organization). Former Prime Minister José Maria Aznar presides over the Fundación para el Analisis y los Estudios Sociales (FAES), a policy institute that is associated with the conservative Popular Party (PP). Also linked to the PP is the Grupo de Estudios Estratégicos (GEES), which is known for its defense- and security-related research and analysis. For its part, the Fundación Alternativas is independent but close to left-wing ideas. The Socialist Partido Socialista Obrero Español (PSOE) created Fundación Ideas in 2009 and dissolved it in January 2014. Also in 2009, the centrist Union, Progress and Democracy (UPyD) created Fundación Progreso y Democracia (FPyD). More specialized think tanks has also emerged in Spain during the past 10 years, like the Future Trends Forum from Bankinter Foundation, a unique think tank in Europe, focused on detecting social, economic, scientific and technological trends and analyzing their possible application and impact on current business models.
Sweden.
The two biggest think tanks in Sweden is the right oriented Timbro and left oriented Agora, now changed to Arena Idé.
Others are Sektor3, SNS, FORES, Arbetarrörelsens Tankesmedja (sociodemocratic oriented), Civitas (Christian democratic oriented), Institute for Security and Development Policy, DNV(Den Nya Välfärden, no party connection) and Cogito (Green oriented). The public service radio channel P3 also hosts the programme Tankesmedjan.
Switzerland.
The first think tank of Switzerland is the liberal oriented (Avenir Suisse). Fouded in 1999 by the big ten companies of Switzerland.
Other think tanks include:
Turkey.
Turkish think tanks are relatively new. Many of them are sister organizations of a political party, University or a company. University think tanks are not typical think tanks. Most Turkish think tanks provide research and ideas, yet they play less important roles in policy making when compared with American think tanks. There are at least 20 think tanks in the country. There are number of Think Tank organizations both independent and supported by government. Turksam, Tasam and the journal of Turkish weekly are the leading information sources.
The oldest and most influential think tank organization in Turkey is ESAM (The Center for Economic and Social Research - Ekonomik ve Sosyal Araştırmalar Merkezi) which is established in 1969 and centrally based in Ankara. There are also branch offices of ESAM in İstanbul, Bursa, Konya and other cities. ESAM has strong international relationships especially with Muslim countries and societies. Ideologically it performs policies, produce ideas and manage projects in parallel to Milli Görüş and have also influential effects on political parties and international strategies. The founder Leader of Milli Görüş movement Prof. Dr. Necmettin ERBAKAN was very concerned with activities and brainstorming events of ESAM. In The Republic of Turkey, 2 of the presidents, 4 of the prime ministers, various ministers, many members of the parliament, a lot of mayors and bureaucrats had been member of ESAM. Currently the General Chairman of ESAM is a famous veteran politician Recai KUTAN (who is older Minister for two different ministries, older main opposition party leader, and founder General Chairman of Saadet Party).
Turkish Economic and Social Studies Foundation (TESEV) is another leading think-thank. Established in 1994, TESEV is an independent non-governmental think-tank, analyzing social, political and economic policy issues facing Turkey. Some of the most remarkable of TESEV’s work have been on the issues of Islam and democracy, combating corruption, state reform, and transparency and accountability. TESEV serve as a bridge between academic research and policy-making process in Turkey. Its core program areas are democratization, good governance, and foreign policy.
Education Reform Initiative (ERI) was launched within Sabancı University in 2003 with the aim of improving education policy and decision-making through research, advocacy, and training. ERI mobilizes a wide range of stakeholders in participatory education policy processes in pursuit of its mission of “quality education for all.”
Other influential Turkish think tanks are the International Strategic Research Organisation (USAK), SETA, BİLGESAM etc.
List of think-tanks in Turkey
Ukraine.
Razumkov Centre.
Razumkov Centre is a non-governmental think tank founded in 1994. It carries out research of public policy in the following spheres:
Razumkov Centre united experts in the fields of economy, energy, law, political sciences, international relations, military security, land relations, sociology, history and philosophy. The Centre has about 35 full-time employees, and over 100 persons work on contractual basis. The Ukrainian-wide public opinion polls of Razumkov Centre Sociological Service are carried out by over 300 interviewers.
Analytical materials of Razumkov Centre are:
In 2004, on the International Economic Forum in Krynica (Poland) Razumkov Centre was named the best non-governmental organisation of Eastern Europe.
The Razumkov Centre is listed among top-25 think tanks of the Central and Eastern Europe.
The average Centre’s yearly budget is approximately $600,000.
United Kingdom.
In Britain, think tanks play a similar role to the United States, attempting to shape policy, and indeed there is some cooperation between British and American think tanks. For example, the London-based think tank Chatham House and the Council on Foreign Relations were both conceived at the Paris Peace Conference, 1919 and have remained sister organisations.
The Bow Group, founded in 1951, is the oldest centre-right think tank and many of its members have gone on to serve as Members of Parliament or Members of the European Parliament. Past chairmen have included Conservative Party leader Michael Howard, Margaret Thatcher's longest-serving Cabinet Minister Geoffrey Howe, Chancellor of the Exchequer Norman Lamont and former British Telecom chairman Christopher Bland.
CIVITAS, Demos, the Institute for Public Policy Research, Policy Exchange and "Reform" are five of the most significant think-tanks of the United Kingdom.
Think tanks in the United States.
As the classification is most often used today, the oldest American think tank is the Carnegie Endowment for International Peace, founded in 1910. The Institute for Government Research, which later merged with two organizations to form the Brookings Institution, was formed in 1916. Other early twentieth century organizations now classified as think tanks include the Hoover Institution (1919), The Twentieth Century Fund (1919, and now known as the Century Foundation), the National Bureau of Economic Research (1920), the Council on Foreign Relations (1921), and the Social Science Research Council (1923). The Great Depression and its aftermath spawned several economic policy organizations, such as the National Planning Association (1934), the Tax Foundation (1937), and the Committee for Economic Development (1943).
In collaboration with the Douglas Aircraft Company, the Air Force set up the RAND Corporation in 1946 to develop weapons technology and strategic defense analysis.
In 1971 Lewis F. Powell Jr. urged conservatives to retake command of public discourse by "financing think tanks, reshaping mass media and seeking influence in universities and the judiciary."
More recently, progressive and liberal think tanks have been established, most notably the Center for American Progress. The organization has close ties to U.S. President Barack Obama and other prominent Democrats.
Think tanks help shape both foreign and domestic policy. They receive funding from private donors, and members of private organizations. By 2013, the largest 21 think tanks in the US spent more than $1 billion per year. Think tanks may feel more free to propose and debate controversial ideas than people within government. The progressive media watchgroup Fairness and Accuracy in Reporting (FAIR) has identified the top 25 think tanks by media citations, noting that from 2006 to 2007 the number of citations declined 17%. The FAIR report reveals the ideological breakdown of the citations: 37% conservative, 47% centrist, and 16% liberal. Their data show that the most-cited think tank was the Brookings Institution, followed by the Council on Foreign Relations, the American Enterprise Institute, the Heritage Foundation, and the Center for Strategic and International Studies.
Government.
Government think tanks are also important in the United States, particularly in the security and defense field. These include the Institute for National Strategic Studies, Institute for Homeland Security Studies, and the Center for Technology and National Security Policy, at the National Defense University; the Center for Naval Warfare Studies at the Naval War College and the Strategic Studies Institute at the U.S. Army War College.
The government funds, wholly or in part, activities at approximately 30 Federally Funded Research and Development Centers (FFRDCs). FFRDCs, are unique independent nonprofit entities sponsored and funded by the U.S. government to meet specific long-term technical needs that cannot be met by any other single organization. FFRDCs typically assist government agencies with scientific research and analysis, systems development, and systems acquisition. They bring together the expertise and outlook of government, industry, and academia to solve complex technical problems. These FFRDCs include the RAND Corporation, the MITRE Corporation, the Institute for Defense Analyses, the Aerospace Corporation, the MIT Lincoln Laboratory, and other organizations supporting various departments within the U.S. Government.
Similar to the above quasi-governmental organizations are Federal Advisory Committees. These groups, sometimes referred to as commissions, are a form of think tank dedicated to advising the US Presidents or the Executive branch of government. They typically focus on a specific issue and as such, might be considered similar to special interest groups. However, unlike special interest groups these committees have come under some oversight regulation and are required to make formal records available to the public. Approximately 1,000 these advisory committees are described in the FACA searchable database.
Other countries.
Azerbaijan.
According to research done by the University of Pennsylvania, there are a total of 12 think tanks in Azerbaijan.
The Center for Economic and Social Development, or CESD; in Azeri, Azerbaijan, İqtisadi və Sosial İnkişaf Mərkəzi (İSİM) is an Azeri think tank, non-profit organization, NGO based in Baku, Azerbaijan. The Center was established in 2005.
CESD focuses on policy advocacy and reform, and is involved with policy research and capacity building. CESD employs leading researchers prominent in their fields and enjoys a broad regional and international network. CESD has been set up to promote research into domestic and regional economic and social issues, advocacy towards reforms and capacity building for the purpose to positively impact the policy making and improve the participation.
CESD ranked as one of the top think tanks in the world by the University of Pennsylvania. According to the University of Pennsylvania rankings – a result of surveys from 1500 scholars and peer review evaluation – the Center for Economic and Social Development (CESD) is one of the top 25 think tanks in Central and Eastern Europe, including CIS. CESD is the only think tank from the Caucasus and Central Asia included in the top think tanks rankings.CESD is also ranked as one of the top 25 domestic economic policy thinks tanks in the world. Only CESD (ranked 19) and the Center for Economic and Social Research (CASE), (Poland, ranked 21) were included in the list from Central and Eastern Europe and CIS countries.
The Economic Research Center (ERC) is a policy-research oriented non-profit think tank established in 1999 with a mission to facilitate sustainable economic development and good governance in the new public management system of Azerbaijan. It seeks to do this by building favorable interactions between the public, private and civil society and working with different networks both in local (EITI NGO Coalition, National Budget Group, Public Coalition Against Poverty, etc.) and international levels (PWYP, IBP, ENTO, ALDA, PASOS, WTO NGO Network etc.). 
Australia.
Most Australian think tanks are based at universities – for example, the Melbourne Institute – or are government funded – for example, the Productivity Commission or the CSIRO.
There are also about 20–30 "independent" Australian think tanks, which are funded by private sources. The best-known of these think tanks play a much more limited role in Australian public and business policy making than in the United States. However, in the past decade the number of think tanks has increased substantially. Prominent Australian conservative think tanks include The Centre for Independent Studies, the Sydney Institute and the Institute of Public Affairs. Prominent leftist Australian think tanks include the McKell Institute, Per Capita, the Australia Institute, Lowy Institute and the Centre for Policy Development. In recent years regionally based independent and non-partisan think tanks have emerged. Some such as the Illawarra's i-eat-drink-think engage in discussion, research and advocacy within a broader civics framework. Commercial think tanks like the Gartner Group, Access Economics, The Helmsman Institute, and others provide additional insight which complements not for profit organisations such as CEDA, the Australian Strategic Policy Institute, and the Australian Institute of Company Directors to provide more targeted policy in defence, program governance, corporate governance and similar.
Listed in alphabetical order, think tanks based in Australia include:
Canada.
Canada has many think tanks (listed in no particular order). Each has their specific areas of interest with some overlaps:
Note: The Canadian Policy Research Networks (CPRN) is a Canadian think-tank that has disbanded.
Ghana.
Ghana's first president, Dr. Kwame Nkrumah, set up various state-supported think tanks in the 1960s. By the 1990s, a variety of policy research centers sprang up in Africa set up by academics who sought to influence public policy in Ghana.
One such think tank was The Institute of Economic Affairs, Ghana, which was founded in 1989 when the country was ruled by the Provisional National Defence Council. The IEA undertakes and publishes research on a range of economic and governance issues confronting Ghana and Sub-Saharan Africa. It has also been involved in bringing political parties together to engage in dialogue. In particular it has organised Presidential debates every election year since the Ghanaian presidential election, 1996.
Some of the active think tanks in Ghana include:
Israel.
There are many think tank teams in Israel:
Jamaica.
The Planning Institute of Jamaica is an agency of the Office of the Prime Minister that is "committed to leading the process of policy formulation on economic and social issues and external co-operation management to achieve sustainable development."
Uzbekistan.
Key projects: Preparation of the National human development report for Uzbekistan, Sociological "portrait" of the Uzbek businessman, Preparation of an analytical report on export procedures optimization in Uzbekistan,
various industry and marketing researches in Uzbekistan, Tajikistan, and Turkmenistan.

</doc>
<doc id="37104" url="http://en.wikipedia.org/wiki?curid=37104" title="Emissions trading">
Emissions trading

Emissions trading or cap and trade ("cap" meaning "a legal limit on the quantity of a certain type of chemical an economy can emit each year") is a market-based approach used to control pollution by providing economic incentives for achieving reductions in the emissions of pollutants. Various countries have adopted emission trading systems as one of the strategies for mitigating climate-change by addressing international greenhouse-gas emission.
A central authority (usually a governmental body) sets a limit or "cap" on the amount of a pollutant that may be emitted. The limit or cap is allocated and/or sold by the central authority to firms in the form of emissions permits which represent the right to emit or discharge a specific volume of the specified pollutant. Permits (and possibly also derivatives of permits) can then be traded on secondary markets. For example, the EU ETS trades primarily in "European Union Allowances" ("EUAs"), the Californian scheme in California Carbon Allowances, the New Zealand scheme in New Zealand Units and the Australian scheme in Australian Units. Firms are required to hold a number of permits (or allowances or "carbon credits") equivalent to their emissions. The total number of permits cannot exceed the cap, limiting total emissions to that level. Firms that need to increase their volume of emissions must buy permits from those who require fewer permits.
Some schemes allow the trading of foreign emissions units. Liable entities participating in the EU ETS can use a few different emissions unit types defined under the Kyoto Protocol, although the use of units "imported" from activities outside the EU remains subject to quantitative and qualitative limits.
The transfer of permits is referred to as a "trade". In effect, the buyer is paying a charge for polluting, while the seller gains a reward for having reduced emissions. Thus, in theory, those who can reduce emissions most cheaply will do so, achieving the pollution reduction at the lowest cost to society.
There are active trading programs in several air pollutants. For greenhouse gases the largest is the European Union Emission Trading Scheme, whose purpose is to avoid dangerous climate change. "Cap and trade provides the private sector with the flexibility required to reduce emissions while stimulating technological innovation and economic growth." The United States has an national market to reduce acid rain and several regional markets in nitrogen oxides. Markets for other pollutants tend to be smaller and more localized.
The National Emission trading System includes the European Union, Switzerland, New Zealand, South Korea and Kazakhstan.
The European Union ETS with all 15 member states is the oldest system. Phase 1 of the ETS launched in 2005. 
The Swiss ETS was launched in 2008 as an alternative option for complying with the national CO2 levy on heating, industrial process, and transport fuels. The ETS became mandatory for large firms on 28 February 2013. 
The New Zealand (NZ) ETS was launched in 2008 as a scheme covering only the forestry sector. In July 2010, it was aimed to expand to cover also stationary energy, fishing, industrial processes and the liquid fossil-fuels sectors. South Korea's cabinet approved and adopted rules in November 2012 for a mandatory ETS after legislation received bipartisan support in the country’s unicameral National Assembly 
The Republic of Kazakhstan ETS was launched 2013. The scheme covers plants in the manufacturing, energy, mining, metallurgy, chemicals, agriculture and transport industries which emit more than 20 000 tons of CO2 per year.
The International Emission trading program,the Kyoto Protocol program, provides for trading across nations. This program, under the United Nations Framework Convention on Climate Change, launched phase 1 in 2007. This program invests in low carbon technology to reduce emissions; participants buy permits from one another, or buy carbon offsets from projects in developing countries under the Clean Development Mechanism (CDM).
Examples of successful cap-and-trade programs include the nationwide Acid Rain Program and the regional NOx Budget Trading Program in the Northeast. Additionally, EPA issued the Clean Air Interstate Rule (CAIR).
SO2 emissions from Acid Rain Program sources have fallen from 17.3 million tons in 1980 to about 7.6 million tons in 2008, a decrease in emissions of 56 percent. Ozone season NOx emissions decreased by 43 percent between 2003 and 2008, even while energy demand remained essentially flat during the same period. CAIR will result in $85 billion to $100 billion in health benefits and nearly $2 billion in visibility benefits per year by 2015 and will substantially reduce premature mortality in the eastern United States. A recent EPA analysis shows that implementation of the Acid Rain Program is expected to reduce between 20,000 and 50,000 incidences of premature mortality annually due to reductions of ambient PM2.5 concentrations, and between 430 and 2,000 incidences annually due to reductions of ground-level ozone. NOx reductions due to the NOx Budget Trading Program have led to improvements in ozone and PM2.5, saving an estimated 580 to 1,800 lives in 2008.
Pollution as an externality.
By definition, an externality is an activity of one entity that affects the welfare of another entity that is not a party to a market transaction related to that activity. Pollution is the prime example most economists think of when discussing externalities. There are various different ways to address these from a public economics perspective, including emissions fees, command and control regulation and cap-and-trade.
Overview.
Described in its simplest form, an emissions trading system will consist of a number of participants, each of whom will have a cap, or limit, on their total emissions over a specified period of time. The cap will have been set by reference to the total emissions for a particular participant over a period of time, often referred to as its ‘baseline’, which is used as a reference point for future emissions reductions. Within a classic ‘cap and trade’ scheme, participants take on caps, or targets, requiring them to reduce their emissions and, in return, receive allowances equal to their individual caps. Participants can choose either to meet their cap by reducing their own emissions; to reduce their emissions below their cap and, perhaps, sell the excess allowances; or to let their emissions remain above their cap, and buy allowances from other participants. All that matters is that, when it comes to demonstrating compliance, every single participant holds allowances at least equal in number to its quantity of emissions. The result should then be that the total quantity of emissions will have been reduced to the sum of all the capped levels.
The overall goal of an emissions trading plan is to minimize the cost of meeting a set emissions target or cap. The cap is an enforceable limit on emissions that is usually lowered over time—aiming towards a national emissions reduction target. The government sets an overall cap on emissions and creates allowances, or limited authorizations to emit, up to the level of the cap. Sources are free to buy or sell allowances or “bank” them to use in future years. In some systems, a proportion of all traded permits must be retired periodically, causing a net reduction in emissions over time. In many cap-and-trade systems, organizations which do not pollute (and therefore have no obligations) may also participate in trading. Thus environmental groups may purchase and retire emission permits and hence drive up the price of the remaining permits according to the law of demand. Corporations can also prematurely retire allowances by donating them to a nonprofit entity and then be eligible for a tax deduction.
"International trade can offer a range of positive and negative incentives to promote international cooperation on climate change (robust evidence, medium agreement). Three issues are key to developing constructive relationships between international trade and climate agreements: how existing trade policies and rules can be modified to be more climate friendly; whether border adjustment measures (BAMs) or other trade measures can be effective in meeting the goals of international climate agreements; whether the UNFCCC, World Trade Organization (WTO), hybrid of the two, or a new institution is the best forum for a trade-and-climate architecture."
Definitions.
According to the Environmental Defense Fund, cap and trade is the most environmentally and economically sensible approach to controlling greenhouse gas emissions, the primary driver of global warming.
The "cap" sets a limit on emissions, which is lowered over time to reduce the amount of pollutants released into the atmosphere.
The "trade" creates a market for carbon allowances, helping companies innovate in order to meet, or come in under, their allocated limit. The less they emit, the less they pay, so it is in their economic incentive to pollute less.
The economics literature provides the following definitions of cap and trade emissions trading schemes.
"A cap-and-trade system constrains the aggregate emissions of regulated sources by creating a limited number of tradable emission allowances, which emission sources must secure and surrender in number equal to their emissions."
"In an emissions trading or cap-and-trade scheme, a limit on access to a resource (the cap) is defined and then allocated among users in the form of permits. Compliance is established by comparing actual emissions with permits surrendered including any permits traded within the cap."
"Under a tradable permit system, an allowable overall level of pollution is established and allocated among firms in the form of permits. Firms that keep their emission levels below their allotted level may sell their surplus permits to other firms or use them to offset excess emissions in other parts of their facilities."
Market-based and least-cost.
Economy-wide pricing of carbon is the centre piece of any policy designed to reduce emissions at the lowest possible costs.
 Ross Garnaut, lead author of the Garnaut Climate Change Review 
Economists have urged the use of "market-based" instruments such as emissions trading to address environmental problems instead of prescriptive "command and control" regulation. Command and control regulation is criticized for being excessively rigid, insensitive to geographical and technological differences, and inefficient. However, emissions trading requires a cap to effectively reduce emissions, and the cap is a government regulatory mechanism. After a cap has been set by a government political process, individual companies are free to choose how or if they will reduce their emissions. Failure to report emissions and surrender emission permits is often punishable by a further government regulatory mechanism, such as a fine that increases costs of production. Firms will choose the least-cost way to comply with the pollution regulation, which will lead to reductions where the least expensive solutions exist, while allowing emissions that are more expensive to reduce.
Under an emissions trading system, emissions sources must meet a set of emissions target, but will have flexibility with regard to how they meet the target. An individual facility may purchase emissions reduction credits or allowances from other sources, sell credits or allowances, implement cost effective internal emissions reductions, or use a combination of both. This flexibility allows firms to use the most affordable compliance strategy, given their internal marginal abatement costs and the market price of allowances or emissions reductions or credits. In theory, a firm’s individual decisions should then lead to an economically efficient allocation of reductions and lower compliance costs for individual firms and for the programme overall, relative to more traditional command and control mechanisms.
Emission markets.
For emissions trading where greenhouse gases are regulated, one emissions permit or allowance is considered equivalent to one metric ton of carbon dioxide (CO2) emissions. Other names for emissions permits are carbon credits, Kyoto units, assigned amount units, and Certified Emission Reduction units (CER). These permits or units can be sold privately or in the international market at the prevailing market price. These trade and settle internationally and hence allow allowances to be transferred between countries. Each international transfer is validated by the United Nations Framework Convention on Climate Change (UNFCCC). Each transfer of ownership within the European Union is additionally validated by the European Commission.
Emissions trading programmes such as the European Union Emissions Trading System (EU ETS) complement the country-to-country trading provided for in the Kyoto Protocol by permitting private party trading of emissions permits. Under such programmes – which are generally co-ordinated with the national emissions targets provided within the framework of the Kyoto Protocol – a national or international authority allocates emissions permits to individual companies based on established criteria, with a view to meeting national and/or regional Kyoto targets at the lowest overall economic cost.
Trading exchanges have been established to provide a spot market in permits, as well as futures and options market to help discover a market price and maintain liquidity. Carbon prices are normally quoted in euros per tonne of carbon dioxide or its equivalent (CO2e). Other greenhouse gases can also be traded, but are quoted as standard multiples of carbon dioxide with respect to their global warming potential. These features reduce the quota's financial impact on business, while ensuring that the quotas are met at a national and international level.
Currently there are six exchanges trading in UNFCCC related carbon credits: the Chicago Climate Exchange (until 2010), European Climate Exchange, NASDAQ OMX Commodities Europe, PowerNext, Commodity Exchange Bratislava and the European Energy Exchange. NASDAQ OMX Commodities Europe listed a contract to trade offsets generated by a CDM carbon project called Certified Emission Reductions. Many companies now engage in emissions abatement, offsetting, and sequestration programs to generate credits that can be sold on one of the exchanges. At least one private electronic market has been established in 2008: CantorCO2e. Carbon credits at Commodity Exchange Bratislava are traded at special platform - Carbon place.
Trading in emission permits is one of the fastest-growing segments in financial services in the City of London with a market estimated to be worth about €30 billion in 2007. Louis Redshaw, head of environmental markets at Barclays Capital, predicts that "Carbon will be the world's biggest commodity market, and it could become the world's biggest market overall."
History.
The international community began the long process towards building effective international and domestic measures to tackle GHG(Carbon dioxide, methane, nitrous oxide, hydroflurocarbons, perfluorocarbons and sulphur hexafluoride.) emissions in response to the increasing certainty that global warming is happening and the uncertainty over its likely consequences. That process began in Rio in 1992, when 160 countries agreed the UN Framework Convention on Climate Change (UNFCCC). The UNFCCC is, as its title suggests, simply a framework; the necessary detail was left to be settled by the Conference of Parties (CoP) to the UNFCCC.
The efficiency of what later was to be called the "cap-and-trade" approach to air pollution abatement was first demonstrated in a series of micro-economic computer simulation studies between 1967 and 1970 for the National Air Pollution Control Administration (predecessor to the United States Environmental Protection Agency's Office of Air and Radiation) by Ellison Burton and William Sanjour. These studies used mathematical models of several cities and their emission sources in order to compare the cost and effectiveness of various control strategies. Each abatement strategy was compared with the "least cost solution" produced by a computer optimization program to identify the least costly combination of source reductions in order to achieve a given abatement goal. In each case it was found that the least cost solution was dramatically less costly than the same amount of pollution reduction produced by any conventional abatement strategy. Burton and later Sanjour along with Edward H. Pechan continued improving and advancing these computer models at the newly created U.S. Environmental Protection Agency. The agency introduced the concept of computer modeling with least cost abatement strategies (i.e. emissions trading) in its 1972 annual report to Congress on the cost of clean air. This led to the concept of "cap and trade" as a means of achieving the "least cost solution" for a given level of abatement.
The development of emissions trading over the course of its history can be divided into four phases:
In the United States, the "acid rain"-related emission trading system was principally conceived by C. Boyden Gray, a G.H.W. Bush administration attorney. Gray worked with the Environmental Defense Fund (EDF), who worked with the EPA to write the bill that became law as part of the Clean Air Act of 1990. The new emissions cap on NOx and SO2 gases took effect in 1995, and according to "Smithsonian" magazine, those acid rain emissions dropped 3 million tons that year. In 1997, the CoP agreed, in what has been described as a watershed in international environmental treaty making, the Kyoto Protocol where 38 developed countries(Annex 1 countries.) committed themselves to targets and timetables for the reduction of GHGs. These targets for developed countries are often referred to as Assigned Amounts.
One important economic reality recognised by many of the countries that signed the Kyoto Protocol is that, if countries have to solely rely on their own domestic measures, the resulting inflexible limitations on GHG growth could entail very large costs, perhaps running into many trillions of dollars globally. As a result, international mechanisms which would allow developed countries flexibility to meet their targets were included in the Kyoto Protocol. The purpose of these mechanisms is to allow the parties to find the most economic ways to achieve their targets. These international mechanisms are outlined under Kyoto Protocol.
On April 17, 2009, the Environmental Protection Agency (EPA) formally announced that it had found that greenhouse gas (GHG) poses a threat to public health and the environment (EPA 2009a). This announcement was significant because it gives the executive branch the authority to impose carbon regulations on carbon-emitting entities.
A carbon cap-and-trade system is to be introduced nationwide in China in 2016 (China's National Development and Reform Commission proposed that an absolute cap be placed on emission by 2016.)
Public relevance.
Ozone is produced naturally in the stratosphere. It blocks UV radiation from reaching the Earth’s surface where it can harm people and eco-systems. Overexposure to UV radiation can cause a range of health effects, including skin damage (skin cancers and premature aging), eye damage (including cataracts), and suppression of the immune system. Researchers believe that overexposure to UV radiation is contributing to an increase in melanoma, the most fatal of all skin cancers. 
Reduction of chemical (chlorofluorocarbons (CFCs), hydrochlorofluorcarbons (HCFCs), carbon tetrachloride and methyl chloroform emissions will prevent or decelerate the rate of the ozone layer's thinning. Improvements have been seen since 1998 over most of the world, and it appears to be recovering because of reduced emissions of ozone-depleting substances. The Antarctic ozone is projected to return to pre- 1980 levels by 2060 to 2075.
An estimated 30–40% of the carbon dioxide emission in the atmosphere dissolves into oceans. It decreases pH in oceans, rivers and lakes. This also causes decreasing oxygen levels, killing off algae and marine ecosystems . In the oceans, CO2 concentrations have reached 450 parts-per-million (ppm) and above. Ocean acidification causes disruption of the calcification of marine organisms and the resultant risk of fundamentally altering marine food webs, the following guard rail should be obeyed: the pH of near surface waters should not drop more than 0.2 units below the pre-industrial average value in any larger ocean region.
Members of the InterAcademy Panel expect emission to decrease to less than 50% of the 1990 level. Reducing the buildup of CO2 in the atmosphere is the only practicable solution to mitigating ocean acidification.
To meet this target United Nations Framework Convention on Climate Change (UNFCCC) would require substantial emission reductions in anthropogenic.
Public opinion.
In the United States, most polling shows large support for emissions trading (often referred to as cap-and-trade). This majority support can be seen in polls conducted by Washington Post/ABC News, Zogby International and Yale University. A new Washington Post-ABC poll reveals that majorities of the American people believe in climate change, are concerned about it, are willing to change their lifestyles and pay more to address it, and want the federal government to regulate greenhouse gases. They are, however, ambivalent on cap-and-trade.
More than three-quarters of respondents, 77.0%, reported they “strongly support” (51.0%) or “somewhat support” (26.0%) the EPA’s decision to regulate carbon emissions. While 68.6% of respondents reported being “very willing” (23.0%) or “somewhat willing” (45.6%), another 26.8% reported being “somewhat unwilling” (8.8%) or “not at all willing” (18.0%) to pay higher prices for “Green” energy sources to support funding for programs that reduce the effect of global warming.
According to PolitiFact, it is a misconception that emissions trading is unpopular in the United States because of earlier polls from Zogby International and Rasmussen which misleadingly include "new taxes" in the questions (taxes aren't part of emissions trading) or high energy cost estimates.
Comparison of cap and trade with other methods of emission reduction.
Cap and trade, offsets created through a baseline("The point of comparison, often the historical emissions from a designated past year, against which emission reduction goals are measured.") and credit("Credits can be distributed by the government for emission reductions achieved by offset projects or by achieving environmental performance beyond a regulatory standard.") approach, and a carbon tax are all market-based approaches that put a price on carbon and other greenhouse gases, and provide an economic incentive to reduce emissions, beginning with the lowest-cost opportunities.
The textbook emissions trading program can be called a "cap and trade" approach in which an aggregate cap on all sources is established and these sources are then allowed to trade emissions permits amongst themselves to determine which sources actually emit the total pollution load. An alternative approach with important differences is a baseline and credit program.
In a baseline and credit program polluters that are not under an aggregate cap can create permits or credits, usually called offsets, by reducing their emissions below a baseline level of emissions. Such credits can be purchased by polluters that have a regulatory limit.
Cap and trade versus carbon tax and other methods.
Cap and trade versus carbon tax.
Regulation by cap-and-trade emissions trading can be compared to emissions fees or environmental tax approaches under a number of possible criteria. Carbon Tax is a surcharge on the carbon content of fossil fuels that aims to discourage their use and thereby reduce carbon dioxide emissions, or a direct tax on CO2 emissions.
Many commentators draw a sharp contrast between cap and trade and an alternative way to put a price on pollution: a carbon tax. In fact, cap and trade and carbon taxes are overlapping sets of policy designs. Like cap and trade, carbon taxes can have a range of scopes, points of regulation, and price schedules. And they can be fair or unfair, depending on how the revenue is used.
A comprehensive, upstream, auctioned cap-and-trade system is very similar to a comprehensive, upstream carbon tax. The main difference is what’s certain and what’s uncertain. Under a carbon tax, elected officials set the price of carbon, and the market determines the quantity emitted; in auctioned cap and trade, elected officials set the quantity of carbon emitted, and the market sets the price.
Responsiveness to inflation: In the case of inflation, cap-and-trade is at an advantage over emissions fees because it adjusts to the new prices automatically and no legislative or regulatory action is needed.
Responsiveness to cost changes: It is difficult to tell which is better between cap-and-trade and emissions fees; therefore, it might be a better option to combine the two resulting in the creation of a safety valve price (a price set by the government at which polluters can purchase additional permits beyond the cap).
Responsiveness to recessions: This point is closely related to responsiveness to cost changes, because recessions cause a drop in demand. Under cap and trade, the emissions cost automatically decreases, so a cap-and-trade scheme adds another automatic stabilizer to the economy - in effect, a type of automatic fiscal stimulus. However, if the emissions price drops to a low level, efforts to reduce emissions will also be reduced. Assuming that a government is competently able to stimulate the economy regardless of the cap-and-trade scheme, an excessively low price represents a missed opportunity to cut emissions faster than planned, so adding a price floor (or equivalently, switching to a tax temporarily) might be better - especially when there is great urgency about cutting emissions, as with greenhouse gas emissions. A price floor would also provide a degree of certainty and stability for investment in emissions reductions: recent experiences from the UK have shown that nuclear power operators are reluctant to invest on "un-subsidised" terms unless there is a guaranteed price floor for carbon (which the EU emissions trading scheme does not presently provide).
Responsiveness to uncertainty: As with cost changes, in a world of uncertainty, it is not clear whether emissions fees or cap-and-trade systems are more efficient—it basically depends on how fast the marginal social benefits of reducing pollution fall with the amount of cleanup (e.g., whether inelastic or elastic marginal social benefit schedule).
Summary:
Carbon Tax is a tax on every ton of carbon emitted. Under a cap and trade, each emitter then has a quota on how much the organization is allowed to emit. The magnitude of the tax will therefore depend on how sensitive the supply of emissions is to the price. The permit of cap and trade will depend on the carbon market. Full auctioned equivalent emissions caps can in principle generate the same revenues as the carbon tax. A similar upstream cap and trade system could be implemented. An upstream carbon tax might be the simplest to administer. Setting up a complex cap and trade arrangement that is comprehensive has high institutional needs.
Cap-and-trade versus command-and-control regulation.
Command and Control is a system of regulation that prescribes emission limits and compliance methods on a facility-by-facility or source-by-source basis and that has been the traditional approach to reducing air pollution.
Unlike emissions fees and cap and trade, which are incentive-based regulations, command-and-control regulations take a variety of forms and are much less flexible. An example of this is a performance standard which sets an emissions goal for each polluter that is fixed and, therefore, the burden of reducing pollution cannot be shifted to the firms that can achieve it more cheaply. As a result, performance standards are unlikely to be as cost effective as cap-and-trade emissions trading. Firms would charge for a higher cost for a product and a proportion of such higher cost will be passed through to the end consumers.
Economics of international emissions trading.
It is possible for a country to reduce emissions using a Command-Control approach, such as regulation, direct and indirect taxes. The cost of that approach differs between countries because the Marginal Abatement Cost Curve (MAC) — the cost of eliminating an additional unit of pollution — differs by country. It might cost China $2 to eliminate a ton of CO2, but it would probably cost Norway or the U.S. much more. International emissions-trading markets were created precisely to exploit differing MACs.
Example.
Emissions trading through "Gains from Trade" can be more beneficial for both the buyer and the seller than a simple emissions capping scheme.
Consider two European countries, such as Germany and Sweden. Each can either reduce all the required amount of emissions by itself or it can choose to buy or sell in the market.
For this example let us assume that Germany can abate its CO2 at a much cheaper cost than Sweden, i.e. MACS > MACG where the MAC curve of Sweden is steeper (higher slope) than that of Germany, and RReq
is the total amount of emissions that need to be reduced by a country.
On the left side of the graph is the MAC curve for Germany. RReq is the amount of required reductions for Germany, but at RReq the MACG curve has not intersected the market emissions permit price of CO2 (market permit price = P = λ).
Thus, given the market price of CO2 allowances, Germany has potential to profit if it abates more emissions than required.
On the right side is the MAC curve for Sweden. RReq is the amount of required reductions for Sweden, but the MACS curve already intersects the market price of CO2 permits before RReq has been reached. Thus, given the market price of CO2 permits, Sweden has potential to make a cost saving if it abates fewer emissions than required internally, and instead abates them elsewhere.
In this example, Sweden would abate emissions until its MACS intersects with P (at R*), but this would only reduce a fraction of Sweden's total required abatement.
After that it could buy emissions credits from Germany for the price "P" (per unit). The internal cost of Sweden's own abatement, combined with the permits it buys in the market from Germany, adds up to the total required reductions (RReq) for Sweden. Thus Sweden can make a saving from buying permits in the market (Δ d-e-f). This represents the "Gains from Trade", the amount of additional expense that Sweden would otherwise have to spend if it abated all of its required emissions by itself without trading.
Germany made a profit on its additional emissions abatement, above what was required: it met the regulations by abating all of the emissions that was required of it (RReq). Additionally, Germany sold its surplus permits to Sweden, and was paid "P" for every unit it abated, while spending less than "P". Its total revenue is the area of the graph (RReq 1 2 R*), its total abatement cost is area (RReq 3 2 R*), and so its net benefit from selling emission permits is the area (Δ 1-2-3) i.e. Gains from Trade
The two R* (on both graphs) represent the efficient allocations that arise from trading.
If the total cost for reducing a particular amount of emissions in the "Command Control" scenario is called "X", then to reduce the same amount of combined pollution in Sweden and Germany, the total abatement cost would be less in the "Emissions Trading" scenario i.e. (X — Δ 123 - Δ def).
The example above applies not just at the national level: it applies just as well between two companies in different countries, or between two subsidiaries within the same company.
Applying the economic theory.
The nature of the pollutant plays a very important role when policy-makers decide which framework should be used to control pollution.
CO2 acts globally, thus its impact on the environment is generally similar wherever in the globe it is released. So the location of the originator of the emissions does not really matter from an environmental standpoint.
The policy framework should be different for regional pollutants (e.g. SO2 and NOx, and also mercury) because the impact exerted by these pollutants may not be the same in all locations. The same amount of a regional pollutant can exert a very high impact in some locations and a low impact in other locations, so it does actually matter where the pollutant is released. This is known as the "Hot Spot" problem.
A Lagrange framework is commonly used to determine the least cost of achieving an objective, in this case the total reduction in emissions required in a year. In some cases it is possible to use the Lagrange optimization framework to determine the required reductions for each country (based on their MAC) so that the total cost of reduction is minimized. In such a scenario, the Lagrange multiplier represents the market allowance price (P) of a pollutant, such as the current market price of emission permits in Europe and the USA.
Countries face the permit market price that exists in the market that day, so they are able to make individual decisions that would minimize their costs while at the same time achieving regulatory compliance. This is also another version of the Equi-Marginal Principle, commonly used in economics to choose the most economically efficient decision.
Prices versus quantities, and the safety valve.
There has been longstanding debate on the relative merits of "price" versus "quantity" instruments to achieve emission reductions.
An emission cap and permit trading system is a "quantity" instrument because it fixes the overall emission level (quantity) and allows the price to vary. Uncertainty in future supply and demand conditions (market volatility) coupled with a fixed number of pollution permits creates an uncertainty in the future price of pollution permits, and the industry must accordingly bear the cost of adapting to these volatile market conditions. The burden of a volatile market thus lies with the industry rather than the controlling agency, which is generally more efficient. However, under volatile market conditions, the ability of the controlling agency to alter the caps will translate into an ability to pick "winners and losers" and thus presents an opportunity for corruption.
In contrast, an emission tax is a "price" instrument because it fixes the price while the emission level is allowed to vary according to economic activity. A major drawback of an emission tax is that the environmental outcome (e.g. a limit on the amount of emissions) is not guaranteed. On one hand, a tax will remove capital from the industry, suppressing possibly useful economic activity, but conversely, the polluter will not need to hedge as much against future uncertainty since the amount of tax will track with profits. The burden of a volatile market will be borne by the controlling (taxing) agency rather than the industry itself, which is generally less efficient. An advantage is that, given a uniform tax rate and a volatile market, the taxing entity will not be in a position to pick "winners and losers" and the opportunity for corruption will be less.
Assuming no corruption and assuming that the controlling agency and the industry are equally efficient at adapting to volatile market conditions, the best choice depends on the sensitivity of the costs of emission reduction, compared to the sensitivity of the benefits (i.e., climate damage avoided by a reduction) when the level of emission control is varied.
Because there is high uncertainty in the compliance costs of firms, some argue that the optimum choice is the price mechanism. However, the burden of uncertainty cannot be eliminated, and in this case it is shifted to the taxing agency itself.
Some scientists have warned of a threshold in atmospheric concentrations of carbon dioxide beyond which a run-away warming effect could take place, with a large possibility of causing irreversible damage. If this is a conceivable risk then a quantity instrument could be a better choice because the quantity of emissions may be capped with a higher degree of certainty. However, this may not be true if this risk exists but cannot be attached to a known level of greenhouse gas (GHG) concentration or a known emission pathway.
A third option, known as a "safety valve", is a hybrid of the price and quantity instruments. The system is essentially an emission cap and permit trading system but the maximum (or minimum) permit price is capped. Emitters have the choice of either obtaining permits in the marketplace or purchasing them from the government at a specified trigger price (which could be adjusted over time). The system is sometimes recommended as a way of overcoming the fundamental disadvantages of both systems by giving governments the flexibility to adjust the system as new information comes to light. It can be shown that by setting the trigger price high enough, or the number of permits low enough, the safety valve can be used to mimic either a pure quantity or pure price mechanism.
All three methods are being used as policy instruments to control greenhouse gas emissions: the EU-ETS is a "quantity" system using the cap and trading system to meet targets set by National Allocation Plans; Denmark has a price system using a carbon tax (World Bank, 2010, p. 218), while China uses the CO2 market price for funding of its Clean Development Mechanism projects, but imposes a "safety valve" of a minimum price per tonne of CO2.
Carbon leakage.
Carbon leakage is the effect that regulation of emissions in one country/sector has on the emissions in other countries/sectors that are not subject to the same regulation (Barker "et al.", 2007). There is no consensus over the magnitude of long-term carbon leakage (Goldemberg "et al.", 1996, p. 31).
In the Kyoto Protocol, Annex I countries are subject to caps on emissions, but non-Annex I countries are not. Barker "et al.". (2007) assessed the literature on leakage. The leakage rate is defined as the increase in CO2 emissions outside of the countries taking domestic mitigation action, divided by the reduction in emissions of countries taking domestic mitigation action. Accordingly, a leakage rate greater than 100% would mean that domestic actions to reduce emissions had had the effect of increasing emissions in other countries to a greater extent, i.e., domestic mitigation action had actually led to an increase in global emissions.
Estimates of leakage rates for action under the Kyoto Protocol ranged from 5 to 20% as a result of a loss in price competitiveness, but these leakage rates were viewed as being very uncertain. For energy-intensive industries, the beneficial effects of Annex I actions through technological development were viewed as possibly being substantial. This beneficial effect, however, had not been reliably quantified. On the empirical evidence they assessed, Barker "et al." (2007) concluded that the competitive losses of then-current mitigation actions, e.g., the EU ETS, were not significant.
Under the EU ETS rules is used to determine the volumes of free allocation of emission permits to industrial installations.
Trade.
To understand carbon trading, it is important to understand the products that are being traded. The primary product in carbon markets is the trading of GHG emission allowances. Under a cap and trade system, permits are issued to various entities for the right to emit GHG emissions that meet emission reduction requirement caps.
One of the controversies about carbon mitigation policy thus arises about how to "level the playing field" with border adjustments. One component of the American Clean Energy and Security Act, for example, along with several other energy bills put before Congress, calls for carbon surcharges on goods imported from countries without cap-and-trade programs. Even aside from issues of compliance with the General Agreement on Tariffs and Trade, such border adjustments presume that the producing countries bear responsibility for the carbon emissions.
A general perception among developing countries is that discussion of climate change in trade negotiations could lead to "green protectionism" by high-income countries (World Bank, 2010, p. 251). Tariffs on imports ("virtual carbon") consistent with a carbon price of $50 per ton of CO2 could be significant for developing countries. World Bank (2010) commented that introducing border tariffs could lead to a proliferation of trade measures where the competitive playing field is viewed as being uneven. Tariffs could also be a burden on low-income countries that have contributed very little to the problem of climate change.
Trading systems.
Kyoto Protocol.
As the Intergovernmental Panel on Climate Change (IPCC) reports came in over the years, they shed abundant light on the true state of global warming and they gave support to the environmental effort to address this unprecedented problem. However, the same discussions that started decades back had never ceased and the crusade for a tangible solution to global climate change had gone on all the while. In 1997 the Kyoto Protocol was adopted.
The Kyoto Protocol is a 1997 international treaty that came into force in 2005. In the treaty, most developed nations agreed to legally binding targets for their emissions of the six major greenhouse gases. Emission quotas (known as "Assigned amounts") were agreed by each participating 'Annex I' country, with the intention of reducing the overall emissions by 5.2% from their 1990 levels by the end of 2012. The United States is the only industrialized nation under Annex I that has not ratified the treaty, and is therefore not bound by it. The IPCC has projected that the financial effect of compliance through trading within the Kyoto commitment period will be limited at between 0.1-1.1% of GDP among trading countries.
The agreement was intended to result in industrialized countries' emissions declining in aggregate by 5.2 percent below 1990 levels by the year of 2012. Despite the failure of the United States and Australia to ratify the protocol, the agreement became effective in 2005, once the requirement that 55 Annex I (predominantly industrialized) countries, jointly accounting for 55 percent of 1990 Annex I emissions, ratify the agreement was met.
The Protocol defines several mechanisms ("flexible mechanisms") that are designed to allow Annex I countries to meet their emission reduction commitments (caps) with reduced economic impact (IPCC, 2007).
Under Article 3.3 of the Kyoto Protocol, Annex I Parties may use GHG removals, from afforestation and reforestation (forest sinks) and deforestation (sources) since 1990, to meet their emission reduction commitments.
Annex I Parties may also use International Emissions Trading (IET). Under the treaty, for the 5-year compliance period from 2008 until 2012, nations that emit less than their quota will be able to sell assigned amount units (each AAU 
representing an allowance to emit one metric tonne of CO2) to nations that exceed their quotas. It is also possible for Annex I countries to sponsor carbon projects that reduce greenhouse gas emissions in other countries. These projects generate tradable carbon credits that can be used by Annex I countries in meeting their caps. The project-based Kyoto Mechanisms are the Clean Development Mechanism (CDM) and Joint Implementation (JI). There are four such international flexible mechanisms, or Kyoto Mechanism written in the Kyoto Protocol.
Article 17 if the Protocol authorizes Annex 1 countries that have agreed to the emissions limitations to take part in emissions trading with other Annex 1 Countries.
Article 4 authorizes such parties to implement their limitations jointly, as the member states of the EU have chosen to do.
Article 6 provides that such Annex 1 countries may take part in joint initiatives (JIs) in return for emissions reduction units (ERUs) to be used against their Assigned Amounts.
Art 12 provides for a mechanism known as the clean development mechanism (CDM), under which Annex 1 countries may invest in emissions limitation projects in developing countries and use certified emissions reductions (CERs) generated against their own Assigned Amounts.
The CDM covers projects taking place in non-Annex I countries, while JI covers projects taking place in Annex I countries. CDM projects are supposed to contribute to sustainable development in developing countries, and also generate "real" and "additional" emission savings, i.e., savings that only occur thanks to the CDM project in question (Carbon Trust, 2009, p. 14). Whether or not these emission savings are genuine is, however, difficult to prove (World Bank, 2010, pp. 265–267).
Australia.
In 2003 the New South Wales (NSW) state government unilaterally established the NSW Greenhouse Gas Abatement Scheme to reduce emissions by requiring electricity generators and large consumers to purchase NSW Greenhouse Abatement Certificates (NGACs). This has prompted the rollout of free energy-efficient compact fluorescent lightbulbs and other energy-efficiency measures, funded by the credits. This scheme has been criticised by the Centre for Energy and Environmental Markets (CEEM) of the UNSW because of its lack of effectiveness in reducing emissions, its lack of transparency and its lack of verification of the additionality of emission reductions.
Both the incumbent Howard Coalition government and the Rudd Labor opposition promised to implement an emissions trading scheme (ETS) before the 2007 federal election. Labor won the election, with the new government proceeding to implement an ETS. The government introduced the Carbon Pollution Reduction Scheme, which the Liberals supported with Malcolm Turnbull as leader. Tony Abbott questioned an ETS, saying the best way to reduce emissions is with a "simple tax". Shortly before the carbon vote, Abbott defeated Turnbull in a leadership challenge, and from there on the Liberals opposed the ETS. This left the government unable to secure passage of the bill and it was subsequently withdrawn.
Julia Gillard defeated Rudd in a leadership challenge and promised not to introduce a carbon tax, but would look to legislate a price on carbon when taking the government to the 2010 election. In the first hung parliament result in 70 years, the government required the support of crossbenchers including the Greens. One requirement for Greens support was a carbon price, which Gillard proceeded with in forming a minority government. A fixed carbon price would proceed to a floating-price ETS within a few years under the plan. The fixed price leant itself to characterisation as a carbon tax and when the government proposed the Clean Energy Bill in February 2011, the opposition claimed it to be a broken election promise.
The bill was passed by the Lower House in October 2011 and the Upper House in November 2011. The Liberal Party vowed to overturn the bill if elected.
The Liberal/National coalition government elected in September 2013 has promised to reverse the climate legislation of the previous government. In July 2014, the carbon tax was repealed as well as the Emissions Trading Scheme (ETS) that was to start in 2015.
New Zealand.
The New Zealand Emissions Trading Scheme (NZ ETS) is a partial-coverage all-free allocation uncapped highly internationally linked emissions trading scheme. The NZ ETS was first legislated in the Climate Change Response (Emissions Trading) Amendment Act 2008 in September 2008 under the Fifth Labour Government of New Zealand and then amended in November 2009 and in November 2012 by the Fifth National Government of New Zealand.
The NZ ETS covers forestry (a net sink), energy (43.4% of total 2010 emissions), industry (6.7% of total 2010 emissions) and waste (2.8% of total 2010 emissions) but not pastoral agriculture (47% of 2010 total emissions). Participants in the NZ ETS must surrender one emission unit (either an international 'Kyoto' unit or a New Zealand-issued unit) for every two tonnes of carbon dioxide equivalent emissions reported or they may choose to buy NZ units from the government at a fixed price of NZ$25.
Individual sectors of the economy have different entry dates when their obligations to report emissions and surrender emission units take effect. Forestry, which contributed net removals of 17.5 Mts of CO2e in 2010 (19% of NZ's 2008 emissions,) entered the NZ ETS on 1 January 2008. The stationary energy, industrial processes and liquid fossil fuel sectors entered the NZ ETS on 1 July 2010. The waste sector (landfill operators) entered on 1 January 2013. Methane and nitrous oxide emissions from pastoral agriculture are not included in the NZ ETS. (From November 2009, agriculture was to enter the NZ ETS on 1 January 2015)
The NZ ETS is highly linked to international carbon markets as it allows the importing of most of the Kyoto Protocol emission units. However, as of June 2015, the scheme will effectively transition into a domestic scheme, with restricted access to international Kyoto units (CERs, ERUs and RMUs). It also creates a specific domestic unit; the 'New Zealand Unit' (NZU), which is issued by free allocation to emitters, with no auctions intended in the short term. Free allocation of NZUs varies between sectors. The commercial fishery sector (who are not participants) have a free allocation of units on a historic basis. Owners of pre-1990 forests have received a fixed free allocation of units. Free allocation to emissions-intensive industry, is provided on an output-intensity basis. For this sector, there is no set limit on the number of units that may be allocated. The number of units allocated to eligible emitters is based on the average emissions per unit of output within a defined 'activity'. Bertram and Terry (2010, p 16) state that as the NZ ETS does not 'cap' emissions, the NZ ETS is not a cap and trade scheme as understood in the economics literature.
Some stakeholders have criticized the New Zealand Emissions Trading Scheme for its generous free allocations of emission units and the lack of a carbon price signal (the Parliamentary Commissioner for the Environment), and for being ineffective in reducing emissions (Greenpeace Aotearoa New Zealand).
The NZ ETS was reviewed in late 2011 by an independent panel, which reported to the Government and public in September 2011.
European Union.
The European Union Emission Trading Scheme (or EU ETS) is the largest multi-national, greenhouse gas emissions trading scheme in the world. It is one of the EU's central policy instruments to meet their cap set in the Kyoto Protocol (Jones "et al."., 2007, p. 64).
After voluntary trials in the UK and Denmark, Phase I commenced operation in January 2005 with all 15 member states of the European Union participating. The program caps the amount of carbon dioxide that can be emitted from large installations with a net heat supply in excess of 20 MW, such as power plants and carbon intensive factories and covers almost half (46%) of the EU's Carbon Dioxide emissions. Phase I permits participants to trade amongst themselves and in validated credits from the developing world through Kyoto's Clean Development Mechanism.Credits are gained by helping and investmenting in clean technologies and low-carbon solutions. It is also generated by certain types of emission-saving projects around the world to cover a proportion of their emissions.
During Phases I and II, allowances for emissions have typically been given free to firms, which has resulted in them getting windfall profits (CCC, 2008, p. 149). Ellerman and Buchner (2008) (referenced by Grubb "et al."., 2009, p. 11) suggested that during its first two years in operation, the EU ETS turned an expected increase in emissions of 1-2 percent per year into a small absolute decline. Grubb "et al.". (2009, p. 11) suggested that a reasonable estimate for the emissions cut achieved during its first two years of operation was 50-100 MtCO2 per year, or 2.5-5 percent.
A number of design flaws have limited the effectiveness of scheme (Jones "et al."., 2007, p. 64). In the initial 2005-07 period, emission caps were not tight enough to drive a significant reduction in emissions (CCC, 2008, p. 149). The total allocation of allowances turned out to exceed actual emissions. This drove the carbon price down to zero in 2007. This oversupply was caused because the allocation of allowances by the EU was based on emissions data from the European Environmental Agency in Copenhagen, which uses a horizontal activity based emissions definition similar to the United Nations, the EU ETS Transaction log in Brussels however uses a vertical installation based emissions measurement system. This caused an oversupply of 200 million tonnes (10% of market) in the EU ETS in the first phase and collapsing prices.
Phase II saw some tightening, but the use of JI and CDM offsets was allowed, with the result that no reductions in the EU will be required to meet the Phase II cap (CCC, 2008, pp. 145, 149). For Phase II, the cap is expected to result in an emissions reduction in 2010 of about 2.4% compared to expected emissions without the cap (business-as-usual emissions) (Jones "et al."., 2007, p. 64). For Phase III (2013–20), the European Commission has proposed a number of changes, including:
In January 2008, Norway, Iceland, and Liechtenstein joined the European Union Emissions Trading System (EU ETS), according to a publication from the European Commission. The Norwegian Ministry of the Environment has also released its draft National Allocation Plan which provides a carbon cap-and-trade of 15 million metric tonnes of CO2, 8 million of which are set to be auctioned. According to the OECD Economic Survey of Norway 2010, the nation "has announced a target for 2008-12 10% below its commitment under the Kyoto Protocol and a 30% cut compared with 1990 by 2020." "In 2012, EU-15 emissions stood 15.1% below their base year level. Based on figures for 2012 by the European Environment Agency, EU-15 emissions averaged 11.8% below base-year levels during the 2008-2012 period. This means the EU-15 over-achieved its first Kyoto target by a wide margin."
Tokyo, Japan.
The Japanese city of Tokyo is like a country in its own right in terms of its energy consumption and GDP. Tokyo consumes as much energy as "entire countries in Northern Europe, and its production matches the GNP of the world's 16th largest country". Originally, Japan had a voluntary emissions reductions system that had been in place for some years, but was not effective. Japan has its own emission reduction policy but not a nationwide cap and trade program.
This climate strategy is enforced and overseen by the Tokyo Metropolitan Government (TMG). The scheme launched in April 2010, covers the top 1,400 emitters in the metropolitan area. The first phase, which is alike to Japan's scheme, runs up to 2015, these organizations will have to cut their carbon emissions by 6% or 8% (depending on the type of organization); those who fail to operate within their emission caps will from 2011 on be required to purchase emission allowances to cover any excess emissions, or alternatively, invest in renewable energy certificates or offset credits issued by smaller businesses or branch offices. Firms whom fail to comply will face fines of up to 500,000 yen plus an amount of credits to equal the emissions 1.3 times the amount they failed to reduce during the first phase of the scheme. In the second phase (FY2015-FY2019), the target is expected to increase to 15%-17%. In its fourth year of operation, emissions were reduced by 23% compared to base-year emissions. The long term aim is to cut the metropolis' carbon emissions by 25% from 2000 levels by 2020. These emission limits can be met by using technologies such as solar panels and advanced fuel-saving devices. 
United States.
An early example of an emission trading system has been the SO2 trading system under the framework of the Acid Rain Program of the 1990 Clean Air Act in the U.S. Under the program, which is essentially a cap-and-trade emissions trading system, SO2 emissions were reduced by 50% from 1980 levels by 2007. Some experts argue that the cap-and-trade system of SO2 emissions reduction has reduced the cost of controlling acid rain by as much as 80% versus source-by-source reduction. The SO2 program was challenged in 2004, which set in motion a series of events that led to the 2011 Cross-State Air Pollution Rule (CSAPR). Under the CSAPR, the national SO2 trading program was replaced by four separate trading groups for SO2 and NOx.
In 1997, the State of Illinois adopted a trading program for volatile organic compounds in most of the Chicago area, called the Emissions Reduction Market System. Beginning in 2000, over 100 major sources of pollution in eight Illinois counties began trading pollution credits.
In 2003, New York State proposed and attained commitments from nine Northeast states to form a cap-and-trade carbon dioxide emissions program for power generators, called the Regional Greenhouse Gas Initiative (RGGI). This program launched on January 1, 2009 with the aim to reduce the carbon "budget" of each state's electricity generation sector to 10% below their 2009 allowances by 2018.
Also in 2003, U.S. corporations were able to trade CO2 emission allowances on the Chicago Climate Exchange under a voluntary scheme. In August 2007, the Exchange announced a mechanism to create emission offsets for projects within the United States that cleanly destroy ozone-depleting substances.
Also in 2003, the Environmental Protection Agency (EPA) began to administer the NOx Budget Trading Program (NBP)under the NOx State Implementation Plan (also known as the "NOx SIP Call") The NOx Budget Trading Program was a market-based cap and trade program created to reduce emissions of nitrogen oxides (NOx) from power plants and other large combustion sources in the eastern United States. NOx is a prime ingredient in the formation of ground-level ozone (smog), a pervasive air pollution problem in many areas of the eastern United States. The NBP was designed to reduce NOx emissions during the warm summer months, referred to as the ozone season, when ground-level ozone concentrations are highest. In March 2008, EPA again strengthened the 8-hour ozone standard to 0.075 parts per million (ppm) from its previous 0.008 ppm.
In 2006, the California Legislature passed the California Global Warming Solutions Act, AB-32, which was signed into law by Governor Arnold Schwarzenegger. Thus far, flexible mechanisms in the form of project based offsets have been suggested for three main project types. The project types include: manure management, forestry, and destruction of ozone-depleted substances. However, a recent ruling from Judge Ernest H. Goldsmith of San Francisco's Superior Court states that the rules governing California's cap-and-trade system were adopted without a proper analysis of alternative methods to reduce greenhouse gas emissions. The tentative ruling, issued on January 24, 2011, argues that the California Air Resources Board violated state environmental law by failing to consider such alternatives. If the decision is made final, the state would not be allowed to implement its proposed cap-and-trade system until the California Air Resources Board fully complies with the California Environmental Quality Act.
In February 2007, five U.S. states and four Canadian provinces joined together to create the Western Climate Initiative (WCI), a regional greenhouse gas emissions trading system. In July 2010, a meeting took place to further outline the cap-and-trade system. In November 2011, Arizona, Montana, New Mexico, Oregon, Utah and Washington withdrew from the WCI.
On November 17, 2008 President-elect Barack Obama clarified, in a talk recorded for YouTube, his intentions for the US to enter a cap-and-trade system to limit global warming.
The 2010 United States federal budget proposes to support clean energy development with a 10-year investment of US $15 billion per year, generated from the sale of greenhouse gas (GHG) emissions credits. Under the proposed cap-and-trade program, all GHG emissions credits would be auctioned off, generating an estimated $78.7 billion in additional revenue in FY 2012, steadily increasing to $83 billion by FY 2019.
The American Clean Energy and Security Act (H.R. 2454), a greenhouse gas cap-and-trade bill, was passed on June 26, 2009, in the House of Representatives by a vote of 219-212. The bill originated in the House Energy and Commerce Committee and was introduced by Representatives Henry A. Waxman and Edward J. Markey. Although cap and trade also gained a significant foothold in the Senate via the efforts of Republican Lindsey Graham, Independent Democrat Joe Lieberman, and Democrat John Kerry, the Legislation died in the Senate.
South Korea.
South Korea's national emissions trading scheme officially launched on 1 January 2015, covering 525 entities from 23 sectors. With a three-year cap of 1.8687 billion tCO2e, it now forms the second largest carbon market in the world following the EU ETS.This amounts to roughly two thirds of the country's emissions. The Korean emissions trading scheme is part of the Republic of Korea's efforts to reduce greenhouse gas emissions by 30% compared to the business-as-usual scenario by 2020.
China.
In November 2011, China approved pilot tests of carbon trading in seven provinces and cities – Beijing, Chongqing, Shanghai, Shenzhen, Tianjin as well as Guangdong Province and Hubei Province, with different prices in each region. The pilot is intended to test the waters and provide valuable lessons for the design of a national system in the near future. Their successes or failures will therefore have far reaching implications for carbon market development in China in terms of trust in a national carbon trading market. Some of the pilot regions can start trading as early as 2013/2014. National trading is expected to start 2016.
India.
Trading is set to begin in 2014 after a three-year rollout period. It is a mandatory energy efficiency trading scheme covering eight sectors responsible for 54 per cent of India’s industrial energy consumption. India has pledged a 20 to 25 per cent reduction in emissions intensity from 2005 levels by 2020. Under the scheme, annual efficiency targets will be allocated to firms. Tradable energy-saving permits will be issued depending on the amount of energy saved during a target year.
Canada.
"The Montreal Protocol, aimed at protecting the stratospheric ozone layer, has also achieved significant reductions in global GHG emissions". "The Montreal Protocol set limits on emissions of ozone-depleting gases that are also potent GHGs, such as chlorofluorocarbons (CFCs) and hydrochlorofluorocarbons (HCFCs). Substitutes for those ozone-depleting gases (such as hydrofluorocarbons (HFCs), which are not ozone-depleting) may also be potent GHGs. Lessons learned from the Montreal Protocol, for example, the effect of financial and technological transfers on broadening participation in an international environmental agreement, could be of value to the design of future international climate change agreements."
Renewable energy certificates.
Renewable Energy Certificates (occasionally referred to as or "green tags" [citation required]), are a largely unrelated form of market-based instruments that are used to achieve renewable energy targets, which may be environmentally motivated (like emissions reduction targets), but may also be motivated by other aims, such as energy security or industrial policy.
Carbon market.
Carbon emissions trading is emissions trading specifically for carbon dioxide (calculated in tonnes of carbon dioxide equivalent or tCO2e) and currently makes up the bulk of emissions trading. It is one of the ways countries can meet their obligations under the Kyoto Protocol to reduce carbon emissions and thereby mitigate global warming.
Market trend.
Trading can be done directly between buyers and sellers, through several organised exchanges or through the many intermediaries active in the carbon market. The price of allowances is determined by supply and demand. As many as 40 million allowances have been traded per day. In 2012, 7.9 billion allowances were traded with a total value of €56 billion. Carbon emissions trading declined in 2013, and is expected to decline in 2014.
According to the World Bank's Carbon Finance Unit, 374 million metric tonnes of carbon dioxide equivalent (tCO2e) were exchanged through projects in 2005, a 240% increase relative to 2004 (110 mtCO2e) which was itself a 41% increase relative to 2003 (78 mtCO2e).
Global carbon markets have shrunk in value by 60% since 2011, but are expected to rise again in 2014.
In terms of dollars, the World Bank has estimated that the size of the carbon market was 11 billion USD in 2005, 30 billion USD in 2006, and 64 billion in 2007.
The Marrakesh Accords of the Kyoto protocol defined the international trading mechanisms and registries needed to support trading between countries, with allowance trading ("sources can buy or sell allowances on the open market. Because the total number of allowances is limited by the cap, emission reductions are assured.") now occurring between European countries and Asian countries. However, while the USA as a nation did not ratify the Protocol, many of its states are now developing cap-and-trade systems and are looking at ways to link their emissions trading systems together, nationally and internationally, to seek out the lowest costs and improve liquidity of the market. However, these states also wish to preserve their individual integrity and unique features. For example, in contrast to the other Kyoto-compliant systems, some states propose other types of greenhouse gas sources, different measurement methods, setting a maximum on the price of allowances, or restricting access to CDM projects. Creating instruments that are not truly fungible would introduce instability and make pricing difficult.
Various proposals are being investigated to see how these systems might be linked across markets, with the International Carbon Action Partnership (ICAP) as an international body to help co-ordinate this.
Business reaction.
In 2008, Barclays Capital predicted that the new carbon market would be worth $70 billion worldwide that year. The voluntary offset market, by comparison, is projected to grow to about $4bn by 2010.
23 multinational corporations came together in the G8 Climate Change Roundtable, a business group formed at the January 2005 World Economic Forum. The group included Ford, Toyota, British Airways, BP and Unilever. On June 9, 2005 the Group published a statement stating that there was a need to act on climate change and stressing the importance of market-based solutions. It called on governments to establish "clear, transparent, and consistent price signals" through "creation of a long-term policy framework" that would include all major producers of greenhouse gases. By December 2007 this had grown to encompass 150 global businesses.
Business in the UK have come out strongly in support of emissions trading as a key tool to mitigate climate change, supported by NGOs. However, not all businesses favor a trading approach. On December 11, 2008, Rex Tillerson, the CEO of Exxonmobil, said a carbon tax is "a more direct, more transparent and more effective approach" than a cap-and-trade program, which he said, "inevitably introduces unnecessary cost and complexity". He also said that he hoped that the revenues from a carbon tax would be used to lower other taxes so as to be revenue neutral.
The International Air Transport Association, whose 230 member airlines comprise 93% of all international traffic, position is that trading should be based on "benchmarking", setting emissions levels based on industry averages, rather than "grandfathering", which would use individual companies’ previous emissions levels to set their future permit allowances. They argue grandfathering "would penalise airlines that took early action to modernise their fleets, while a benchmarking approach, if designed properly, would reward more efficient operations".
Measuring, reporting, verification (MRV).
MRV system holds a role of producing credible information under emissions trading scheme. MRV system verifies accuracy of data on emissions (they are consistently monitored(M),reported(R) to the regulators, and verified(V)).
An emissions trading system requires measurements at the level of operator or installation. These measurements are then reported to a regulator. For greenhouse gases all trading countries maintain an inventory of emissions at national and installation level; in addition, the trading groups within North America maintain inventories at the state level through The Climate Registry. For trading between regions these inventories must be consistent, with equivalent units and measurement techniques.
In some industrial processes emissions can be physically measured by inserting sensors and flowmeters in chimneys and stacks, but many types of activity rely on theoretical calculations for measurement. Depending on local legislation, these measurements may require additional checks and verification by government or third party auditors, prior or post submission to the local regulator.
Enforcement.
Another significant, yet troublesome aspect is enforcement. Without effective MRV and enforcement the value of allowances is diminished. Enforcement can be done using several means, including fines or sanctioning those that have exceeded their allowances. Concerns include the cost of MRV and enforcement and the risk that facilities may be tempted to mislead rather than make real reductions or make up their shortfall by purchasing allowances or offsets from another entity. The net effect of a corrupt reporting system or poorly managed or financed regulator may be a discount on emission costs, and a (hidden) increase in actual emissions.
According to Nordhaus (2007, p. 27), strict enforcement of the Kyoto Protocol is likely to be observed in those countries and industries covered by the EU ETS. Ellerman and Buchner (2007, p. 71) commented on the European Commission's (EC's) role in enforcing scarcity of permits within the EU ETS. This was done by the EC's reviewing the total number of permits that member states proposed that their industries be allocated. Based on institutional and enforcement considerations, Kruger "et al." (2007, pp. 130–131) suggested that emissions trading within developing countries might not be a realistic goal in the near-term. Burniaux "et al.". (2008, p. 56) argued that due to the difficulty in enforcing international rules against sovereign states, development of the carbon market would require negotiation and consensus-building.
Criticism.
Emissions trading has been criticised for a variety of reasons.
In the popular science magazine "New Scientist", Lohmann (2006) argued that trading pollution allowances should be avoided as a climate change policy. Lohmann gave several reasons for this view. First, global warming will require more radical change than the modest changes driven by previous pollution trading schemes such as the US SO2 market. Global warming requires "nothing less than a reorganisation of society and technology that will leave most remaining fossil fuels safely underground." Carbon trading schemes have tended to reward the heaviest polluters with 'windfall profits' when they are granted enough carbon credits to match historic production. Carbon trading encourages business-as-usual as expensive long-term structural changes will not be made if there is a cheaper source of carbon credits. Cheap "offset" carbon credits are frequently available from the less developed countries, where they may be generated by local polluters at the expense of local communities.
Lohmann (2006b) supported conventional regulation, green taxes, and energy policies that are "justice-based" and "community-driven." According to Carbon Trade Watch (2009), carbon trading has had a "disastrous track record." The effectiveness of the EU ETS was criticized, and it was argued that the CDM had routinely favoured "environmentally ineffective and socially unjust projects."
Annie Leonard provided a critical view on carbon emissions trading in her 2009 documentary "The Story of Cap and Trade". This documentary emphasized three factors: unjust financial advantages to major pollutors resulting from free permits, an ineffectiveness of the system caused by cheating in connection with carbon offsets and a distraction from the search for other solutions.
Offsets.
Forest campaigner Jutta Kill (2006) of European environmental group FERN argued that offsets for emission reductions were not substitute for actual cuts in emissions. Kill stated that "[carbon] in trees is temporary: Trees can easily release carbon into the atmosphere through fire, disease, climatic changes, natural decay and timber harvesting."
Supply of permits.
Regulatory agencies run the risk of issuing too many emission credits, which can result in a very low price on emission permits (CCC, 2008, p. 140). This reduces the incentive that permit-liable firms have to cut back their emissions. On the other hand, issuing too few permits can result in an excessively high permit price (Hepburn, 2006, p. 239). This is one of the arguments in favour of a hybrid instrument, that has a price-floor, i.e., a minimum permit price, and a price-ceiling, i.e., a limit on the permit price. A price-ceiling (safety value) does, however, remove the certainty of a particular quantity limit of emissions (Bashmakov "et al."., 2001).
Incentives.
Emissions trading can result in perverse incentives. If, for example, polluting firms are given emission permits for free ("grandfathering"), this may create a reason for them not to cut their emissions. This is because a firm making large cuts in emissions would then potentially be granted fewer emission permits in the future (IMF, 2008, pp. 25–26). This perverse incentive can be alleviated if permits are auctioned, i.e., sold to polluters, rather than giving them the permits for free (Hepburn, 2006, pp. 236–237).
On the other hand, allocating permits can be used as a measure to protect domestic firms who are internationally exposed to competition (p. 237). This happens when domestic firms compete against other firms that are not subject to the same regulation. This argument in favour of allocation of permits has been used in the EU ETS, where industries that have been judged to be internationally exposed, e.g., cement and steel production, have been given permits for free (4CMR, 2008).
Auctioning.
Auctioning is a method for distributing emission allowances in a cap-and-trade system whereby allowances are sold to the highest bidder. This method of distribution may be combined with other forms of allowance distribution.
The revenues from auctioning go to the government. These revenues could, for example, be used for research and development of sustainable technology. Alternatively, revenues could be used to cut distortionary taxes, thus improving the efficiency of the overall cap policy (Fisher "et al."., 1996, p. 417).
Distributional effects.
The Congressional Budget Office (CBO, 2009) examined the potential effects of the American Clean Energy and Security Act on US households. This Act relies heavily on the free allocation of permits. The Bill was found to protect low-income consumers, but it was recommended that the Bill be changed to be more efficient. It was suggested that the Bill be changed to reduce welfare provisions for corporations, and more resources be made available for consumer relief.

</doc>
<doc id="37105" url="http://en.wikipedia.org/wiki?curid=37105" title="Metin Kaçan">
Metin Kaçan

Metin Kaçan (15 November 1961 – 6 January 2013) was a Turkish author, who is best known for his novel "Ağır Roman" ("Cholera Street"), which was translated into German (Kaçan 2003), and a movie ("Ağır Roman"), directed by Mustafa Altıoklar (1999), was based on it.
Literary career.
Besides "Ağır Roman", Kaçan is also the author of novel "Fındık Sekiz", a collection of short stories, "A ship to the Islands" ("Adalara Vapur", Kaçan 2002), and a book written in a mixed style between prose and poetry, entitled "The Tiger at Withdrawal" ("Harman Kaplan", Kaçan 1999).
Much of Kaçan's writings deals with life in Istanbul, in particular its poor quarter Dolapdere (not far from Taksim Square). To Dolapdere, he sarcastically gave the name "Cholera" ("Kolera" in Turkish) in "Ağır Roman", thereby recalling both its shabbiness and the fact that the great Polish poet Adam Mickiewicz died there from the cholera in 1855. Mickiewicz Museum at Dolapdere, still open to visitors today, figures in "Ağır Roman". The title of this novel plays ingeniously with the polysemy of the Turkish word "Roman", which means both "gypsy" and "novel". Also, together with the adjective "ağır", which means "heavy" or "slow" in Turkish, "Roman" is the designation for a special kind of street music, played by some of the novel's protagonists.
"Ağır Roman" tells the tragic story of a young hero, who grows up in "Cholera" quarter but finally fails and commits suicide. His failure parallels the failure of the quarter itself, whose ancient structures as well as its multi-ethnic and multi-religious composition disintegrate.
"Fındık Sekiz" tells a story about two cars, that appear sometimes as personified figures, and that take the semi-autobiographical protagonist Meto on a mystical journey. At the same time, Meto's conflict with a woman, who manages to have him thrown into prison through fraudulent statements, is related, which might reflect some of Kaçan's own experiences.
Kaçan's style is heavily imbued with Turkish slang. This choice gives his writings a non-conformistic, frequently vulgar, but overall extremely vivid and creative tone, which has been hailed, among others, by Yıldız Ecevit. Other characteristics of his writing are the personification of natural phenomena
and inanimate items such as cars (in particular in "Fındık Sekiz"), autobiographical details (Kaçan grew up in Dolapdere), the blurring of the limitations of poetry and prose, and references to mysticism, in particular Muslim mysticism (Sufism). His best-selling novel, "Ağır Roman" was translated into French by Actes Sud in 2010.
Rape conviction.
In 1995, Kaçan was arrested for torturing and raping his ex-girlfriend. He was released on bail pending the outcome of the trial, which ended five years later with a prison sentence of eight years and nine months. The appeals court upheld Kaçan's conviction. In 2006, he was caught by the police in his hometown while attending the funeral of a relative and sent to prison to serve his sentence. After spending close to four years in prison, the remainder of his sentence was commuted.
Death.
Kaçan committed suicide by jumping from the Bosphorus Bridge on 6 January 2013. On that day, he took a cab in Esenler on the European side of Istanbul and requested to be driven to Üsküdar on the Asian side. He asked the driver to stop on the bridge so that he could take photographs. He got out of the vehicle, ran to the edge of the bridge, and threw himself off. His brother confirmed the suicide on 8 January. His body washed ashore on 18 January, twelve days after his disappearance, on the coast of Marmara Sea at Beylikdüzü. Kaçan was 51 years old.

</doc>
<doc id="37107" url="http://en.wikipedia.org/wiki?curid=37107" title="United Nations Foundation">
United Nations Foundation

The United Nations Foundation was launched in 1998 with a $1 billion gift from Ted Turner to support the United Nations causes. The creation of the Foundation was intended to encourage other donors to also support the UN in its activities. The main issue areas that the Foundation addresses are child health, climate change & energy, sustainable development, technology, women, girls, and population, and supporting the United Nations. Some of the biggest global campaigns that the UN Foundation has been involved in include Nothing But Nets, the Measles & Rubella Initiative, the , Girl Up, Shot@Life, and the Better World Campaign, among others.
United Nations Foundation's original purpose was to build support for United Nations causes and to make sure that the United States honors its commitments to the United Nations. Since its beginning, the United Nations Foundation and the Better World Campaign have provided grants in order to support the UN goals worldwide. The United Nations Foundation serves as the largest source of private funding to the United Nations. In conjunction with the UN, they established the United Nations Fund for International Partnerships was established to serve as the UN counterpart to the Foundation.
The United Nations Foundation has collected more than $1.2 billion from other places, including from other foundations, corporations, NGOs, and individuals. The Foundation also works with UN partners in order to provide policy recommendations and project proposals. The Foundation and its sister organization, The Better World Campaign, have helped raise awareness of and support for the UN among global policy makers and the public. The UN Foundation's current budgetary breakdown is $115.7 million going to program services, $7.3 million to fundraising, and $11.8 million going to management and overhead.
Turner's choice of the UN for his donation.
Ted Turner, who in 1996 was worth $3.2 billion due to his Time Warner enterprise, decided to make a $1 billion contribution to the UN because he had previously donated to similar causes, and felt strongly about the issues the UN were participating in. Before donating to the UN, Turner was a proponent for the protection of the environment, especially in combating global warming. Turner believed that his $100 million per year donation over the course of 10 years would make a difference in the direction of the United Nations, and that he could use this donation to encourage other wealthy members of society to make financial contributions to the work of the UN.
Leadership.
The UN Foundation is led by President and chief executive officer Kathy Calvin. She previously served as the President of AOL Time Warner prior to becoming the CEO of the UN Foundation. Timothy E. Wirth, a former United States Congressman, U.S. Senator, and the first Undersecretary for Global Affairs in U.S. President Bill Clinton's administration, previously served as the Foundation's President from 1998 to 2013. Ted Turner serves as the chairman of the board. Other notable board members include Queen Rania Al-Abdullah of Jordan, former UN Secretary-General Kofi Annan, Gro Harlem Brundtland, and Muhammad Yunus among others.
Background of Foundation's involvement with the UN.
When the UN Foundation started, it desired to assist the UN with a wide variety of issues, and bring attention to particular modern problems. The Foundation wished to help then Secretary-General Kofi Annan with the problem of reforming the UN. They also desired to bolster UN programs that were viewed as successful, including children's health, population control, environment issues, and land-mines. One of their priorities was also to work with the private sector to raise more money for the UN. They also had the intention of raising awareness of the UN and its programs amongst the American population. They have had a close relationship with the UN and its leadership from the beginning in order to set goals and provide funding for particular programs.
Specific campaigns.
Global health.
One of the global issues that the UN Foundation focuses on is women and children's health. They work closely with private sector partners and UN agencies in order to address a variety of children's health issues. One of their biggest campaigns is working to reduce the number of deaths from measles. The Measles & Rubella Initiative, as it called, is a partnership between the UN Foundation, the American Red Cross, UNICEF, the Centers for Disease Control, and the World Health Organization in order to provide measles vaccinations to children across the African continent. During the first year of this campaign, vaccines were distributed across 8 African countries, which vaccinated over 21 million children. This campaign not only focuses on vaccinating children, but also putting into place health infrastructure, and promoting better access to health-care across the continent. In ten years, the Measles Initiative has protected more than 1 billion children from measles.
The UN Foundation also runs the Nothing But Nets Campaign, which is targeted at reducing malaria across the African continent. This campaign originally started when "Sports Illustrated" writer Patrick Reilley published an article asking his readers to donate money to a campaign to buy mosquito nets for those in Africa suffering from malaria. With support from the UN Foundation, Reilley's project got off the ground, and has to-date provided over 7 million nets across Africa.
The Global Polio Eradication Initiative is a partnership that includes the UN Foundation, Rotary International, the Bill and Melinda Gates Foundation, UNICEF, the U.S. Centers for Disease Control and Prevention and the World Health Organization. The initiative is dedicated to globally eradicating polio through vaccinations and has protected 2 billion children from polio.
The mHealth Alliance was formed by the UN Foundation, the Rockefeller Foundation and the Vodafone Foundation to facilitate global innovation and ensure maximum impact in the field of mobile health (mHealth). The Alliance acts as an umbrella organization to complement, draw together and expand upon the mHealth initiatives of multiple organizations around the world to address global health needs. Since 2006, the UN Foundation-Vodafone Foundation Technology Partnership has been working with the World Health Organization, DataDyne.org and country Ministries of Health to support the development of a sustainable and scalable technology solution to quickly collect vital health data.
The UN Foundation’s Shot@Life campaign educates, connects and empowers Americans to champion vaccines as one of the most cost-effective ways to save the lives of children in developing countries. The campaign encourages Americans to learn about, advocate for, and donate vaccines to decrease vaccine-preventable childhood deaths.
Every Woman Every Child was launched by UN Secretary-General Ban Ki-moon during the United Nations Millennium Development Goals Summit in 2010 and aims to save the lives of 16 million women and children 2015. It is a global effort to mobilize and intensify international and national action by governments, multilaterals, the private sector and civil society to address major health challenges facing women and children around the world.
Energy and Climate.
One of the largest global issues that the UN Foundation is involved in is climate change and energy. The UN Foundation priorities include tackling the global climate challenge, improving energy efficiency and achieving universal energy access. The Foundation's Climate Change and Sustainable Energy Program works with partners in the NGO sector, the UN, governments, and private corporations to come up with solutions and provide funding to programs related to this issue. One of their campaigns in relation to climate change and energy is to provide communities around the world with renewable energy technologies in order to promote sustainability globally.
The calls for private sector and national commitments and attracts global attention to the importance of energy for development and poverty alleviation. The initiative has been called for by UN Secretary-General Ban Ki-moon and, as part of the initiative, the UN Foundation has launched a global . The Network aims to bring together practitioners from the private sector and civil society working on the delivery of energy services and solutions related to electrification in a range of developing country contexts to develop a more integrated approach to energy access planning and execution.
The is an initiative led by the UN Foundation and supports large-scale adoption of clean and safe household cooking solutions as a way to save lives, improve livelihoods, empower women, and reduce climate change missions. The alliance works with public, private, and non-profit partners to overcome market barriers that hamper the production, deployment, and use of clean cookstoves and fuels in the developing world. It works to develop standards for cleaner stoves, increase public and policymaker awareness of the health and environmental benefits of improved stoves.
The UN Foundation’s International Bioenergy and Sustainability Initiative advances environmentally and economically sustainable strategies for harnessing biomass energy in ways that minimize competition with food products. The initiatives seeks to identify, share and promote best practices for the sustainable use of bioenergy with government leaders, UN agencies, scientific experts, and nongovernmental organizations.
Girls, women & population.
The UN Foundation launched the campaign in September 2010. This “for girls, by girls” campaign channels the energy and enthusiasm of American girls into a powerful force for change for girls globally. Through Girl Up’s support, girls in developing countries have the opportunity to become educated, healthy, safe, counted, and positioned to be the next generation of leaders.
The works to achieve universal access to reproductive health care leading to healthier women, stronger families, and more stable, prosperous communities. The UN Foundation is committed to achieving universal access to reproductive health care by 2015 which is also a Millennium Development Goal target.
UN advocacy in the U.S..
The UN Foundation has an extremely close relationship with the UN, as it provides it with large amounts of money to fund programs. The UN Foundation, through its sponsorship and funding of the Better World Campaign, works to better the relationship between the United States and the United Nations. The Better World Campaign advocates at Congress in order to ensure that the US fulfills its financial obligations to the United Nations, and that it repays its debt. The Better World Campaign allows the Foundation to have a direct channel to the US legislative and administrative powers. The UN Foundation also works with the UN to develop new websites, create the UN Radio Service, engage online media outlets and share the UN’s point of view through blogs to highlight UN peacekeeping, UN reform, climate change, HIV/AIDS and women’s empowerment.
Global Entrepreneurs Council.
The United Nations Foundation recently announced a new which is made up of 10 emerging entrepreneurs under the age of 40 who will work with the UN to address global issues. The United Nations Foundation has brought together some of the brightest entrepreneurs under 45 through the Global Entrepreneurs Council who will work with the UN to address global issues. Council members represent various industries including, corporate, creative community, and media and have a proven track record of entrepreneurship, philanthropy, and advocacy on global issues. They include the former marketing director of Facebook, Randi Zuckerberg and the former publisher of Variety Magazine, Brian Gott, among others.

</doc>
<doc id="37120" url="http://en.wikipedia.org/wiki?curid=37120" title="Nigger">
Nigger

Nigger is a noun in the English language. The word originated as a neutral term referring to black people, as a variation of the Spanish/Portuguese noun "negro", a descendant of the Latin adjective "niger" ("color black"). Often used disparagingly, by the mid 20th century, particularly in the United States, its usage had become unambiguously pejorative, a common ethnic slur usually directed at black people.
Etymology and history.
The variants "neger" and "negar", derive from the Spanish and Portuguese word "negro" (black), and from the now-pejorative French "nègre" (negro). Etymologically, "negro", "noir", "nègre", and "nigger" ultimately derive from "nigrum", the stem of the Latin "niger" (black) (pronounced [ˈniɡer] which, in every other grammatical case, grammatical gender, and grammatical number besides nominative masculine singular, is "nigr-", the "r" is trilled).
In the Colonial America of 1619, John Rolfe used "negars" in describing the African slaves shipped to the Virginia colony. Later American English spellings, "neger" and "neggar", prevailed in a northern colony, New York under the Dutch, and in metropolitan Philadelphia's Moravian and Pennsylvania Dutch communities; the African Burial Ground in New York City originally was known by the Dutch name "Begraafplaats van de Neger" (Cemetery of the Negro); an early US occurrence of "neger" in Rhode Island, dates from 1625. An alternative word for African Americans was the English word, "Black", used by Thomas Jefferson in his Notes on the State of Virginia. Among Anglophones, the word "nigger" was not always considered derogatory, because it then denoted "black-skinned", a common Anglophone usage. Nineteenth-century English (language) literature features usages of "nigger" without racist connotation, e.g. the Joseph Conrad novella "The Nigger of the 'Narcissus"' (1897). Moreover, Charles Dickens and Mark Twain created characters who used the word as contemporary usage. Twain, in the autobiographic book "Life on the Mississippi" (1883), used the term within quotes, indicating reported usage, but used the term "negro" when speaking in his own narrative persona.
During the fur trade of the early 1800s to the late 1840s in the Western United States, the word was spelled "niggur", and is often recorded in literature of the time. George Fredrick Ruxton often included the word as part of the "mountain man" lexicon, and did not indicate that the word was pejorative at the time. "Niggur" was evidently similar to the modern use of dude, or guy. This passage from Ruxton's Life in the Far West illustrates a common use of the word in spoken form—the speaker here referring to himself: "Travler, marm, this niggur's no travler; I ar' a trapper, marm, a mountain-man, wagh!" It was not used as a term exclusively for blacks among mountain men during this period, as Indians, Mexicans, and Frenchmen and Anglos alike could be a "niggur". Linguistically, in developing American English, in the early editions of "A Compendious Dictionary of the English Language" (1806), lexicographer Noah Webster suggested the "neger" new spelling in place of "negro".
By the 1900s, "nigger" had become a pejorative word. In its stead, the term "colored" became the mainstream alternative to "negro" and its derived terms. Abolitionists in Boston, Massachusetts, posted warnings to the "Colored People of Boston and vicinity". Writing in 1904, journalist Clifton Johnson documented the "opprobrious" character of the word "nigger", emphasizing that it was chosen in the South precisely because it was more offensive than "colored." Established as mainstream American English usage, the word "colored" features in the organizational title of the National Association for the Advancement of Colored People, reflecting the members' racial identity preference at the 1909 foundation. In the Southern United States, the local American English dialect changes the pronunciation of "negro" to "nigra".
By the late 1960s, the social change achieved by groups in the United States such as the Civil Rights Movement (1955–68), had legitimized the racial identity word "black" as mainstream American English usage to denote black-skinned Americans of African ancestry. In the 1990s, "Black" was displaced in favor of the compound blanket term "African American". Moreover, as a compound word, "African American" resembles the vogue word "Afro-American", an early-1970s popular usage. Currently, some black Americans continue to use the word "nigger", often spelled as "nigga" and "niggah", without irony, either to neutralize the word's impact or as a sign of solidarity.
Usages.
British.
In the United Kingdom and the Anglophone world, "nigger" denoted the dark-skinned (non-white) African and Asian (i.e., from India or nearby) peoples colonized into the British Empire, and "dark-skinned foreigners" in general.
In "A Dictionary of Modern English Usage" (1926), H. W. Fowler states that applying the word "nigger" to "others than full or partial negroes" is "felt as an insult by the person described, & betrays in the speaker, if not deliberate insolence, at least a very arrogant inhumanity"; but the second edition (1965) states: "N. has been described as 'the term that carries with it all the obloquy and contempt and rejection which whites have inflicted on blacks.'".
Victorian writer Rudyard Kipling used it in 'How the Leopard Got His Spots' and 'A Counting-Out Song' to illustrate the usage of the day. Likewise, P. G. Wodehouse used the phrase "Nigger minstrels" in "Thank You, Jeeves" (1934), the first Jeeves–Bertie novel, in admiration of their artistry and musical tradition. See also below under "Literary".
As recently as the 1950s, it may have been acceptable British usage to say "niggers" when referring to black people, notable in mainstream usages such as "Nigger Boy" brand candy cigarettes, and the color "nigger brown" or simply "nigger" (dark brown); however, by the 1970s the term was generally recognized as racist, offensive and potentially illegal along with the unambiguously offensive "nig-nog", and "golliwog". Agatha Christie's book " Ten Little Niggers" was first published in London in 1939 and continued to appear under that title until the early 1980s, when it became "And Then There Were None".
North American.
Cultural.
Addressing the use of "nigger" by black people, Cornel West said in 2007, "There's a certain rhythmic seduction to the word. If you speak in a sentence, and you have to say "cat", "companion", or "friend", as opposed to "nigger", then the rhythmic presentation is off. That rhythmic language is a form of historical memory for black people... When Richard Pryor came back from Africa, and decided to stop using the word onstage, he would sometimes start to slip up, because he was so used to speaking that way. It was the right word at the moment to keep the rhythm together in his sentence making."
Contemporarily, the implied racism of the word "nigger" has rendered its usages social taboo. In the US, magazines and newspapers often do not use it, instead printing "family-friendly" censored versions, usually "n*gg*r", "n**ger", "n——", and "the N-word"; however, historians and social activists, such as Dick Gregory, criticize the euphemisms and their usage as intellectually dishonest, because using the euphemism "the N-word" instead of "nigger" robs younger generations of Americans of the full history of black people in America.
Political.
In explaining his refusal to be conscripted to fight the Vietnam War (1965–75), professional boxer Muhammad Ali said, "No Vietcong ever called me nigger"; later, his modified answer was the title "No Vietnamese Ever Called Me Nigger" (1968) of a documentary about the front-line lot of the U.S. Army Black soldier in combat in Vietnam. An Ali biographer reports that, when interviewed by Robert Lipsyte in 1966, the boxer actually said, "I ain't got no quarrel with them Viet Cong". The word can be invoked politically for effect. When Detroit mayor Kwame Kilpatrick came under intense scrutiny for his personal conduct in 2008, he deviated from an address to city council, saying, "In the past 30 days, I've been called a nigger more than any time in my entire life." Opponents accused him of "playing the Race Card" to save his political life.
On February 28, 2007, the New York City Council symbolically banned, with a formal resolution, the use of the word "nigger"; however, there is no penalty for using it. The New York City resolution also requests excluding from Grammy Award consideration every song whose lyrics contain the word "nigger", however Ron Roecker, vice president of communication for the Recording Academy doubts that it will have any effect on actual nominations.
Sport.
In the first half of the twentieth century, before Major League Baseball was racially integrated, dark-skinned and dark-complexion players were nicknamed "Nig"; examples are: Johnny Beazley (1941–49), Joe Berry (1921–22), Bobby Bragan (1940–48), Nig Clarke (1905–20), Nig Cuppy (1892–1901), Nig Fuller (1902), Johnny Grabowski (1923–31), Nig Lipscomb (1937), Charlie Niebergall (1921–24), Nig Perrine (1907), and Frank Smith (1904–15). The 1930s movie "The Bowery" with George Raft and Wallace Beery includes a sports-bar in New York City named "Nigger Joe's".
Nature.
In some parts of the U.S., including the Midwest, Brazil nuts are known (increasingly less often) as "nigger toes". Similarly, cormorants have been known as "nigger geese".
Denotational extension.
The denotations of "nigger" also comprehend non-black/non-white and other disadvantaged people; the U.S. politician Ron Dellums said, "... it's time for somebody to lead all of America's niggers". Jerry Farber's 1967 protest, The Student as Nigger invoked the word as a metaphor for the victims of an authoritarian society. In 1969, in the UK, in the course of being interviewed by a "Nova" magazine reporter, artist Yoko Ono said, "... woman is the nigger of the world"; three years later, her husband, John Lennon, published the song "Woman Is the Nigger of the World" (1972)—about the worldwide phenomenon of discrimination against women–which was socially and politically controversial to US sensibilities. In 1978 singer Patti Smith used the word in "Rock N Roll Nigger". In 1979 singer Elvis Costello used the phrase "white nigger" in "Oliver's Army", a song describing the experiences of working-class soldiers in the British military forces on the "murder mile" (a term used to describe Belfast during The Troubles), where "white nigger" was a common British pejorative for Irish Catholics. Later, the producers of the British talent show "Stars in Their Eyes" forced a contestant to censor one of its lines, changing "... all it takes is one itchy trigger – One more widow, one less white nigger" to "... one less white figure". In his autobiography "White Niggers of America: The Precocious Autobiography of a Quebec "Terrorist"" (1968), Pierre Vallières, a Front de libération du Québec leader refers to the oppression of the Québécois people in North America.
In his memoir, "All Souls," Michael Patrick MacDonald describes how many white residents of the Old Colony housing project in South Boston used this meaning to degrade the people considered to be of lower status, whether white or black.
Of course, no one considered himself a nigger. It was always something you called someone who could be considered anything less than you. I soon found out there were a few black families living in Old Colony. They'd lived there for years and everyone said that they were okay, that they weren't niggers but just black. It felt good to all of us to not be as bad as the hopeless people in D Street or, God forbid, the ones in Columbia Point, who were both black and niggers. But now I was jealous of the kids in Old Harbor Project down the road, which seemed like a step up from Old Colony...
Other languages.
Many other languages have words that sound the same as 'nigger' (are homophonic), but do not mean the same, and have ethnic slurs dissimilar to 'nigger' but meaning the same.
Some examples of how other languages refer to a black person in a neutral and in a pejorative way:
Literary.
Historically, "nigger" is controversial in literature because of its usage as both a racist insult and a common noun. The white photographer and writer, Carl Van Vechten, a supporter of the Harlem Renaissance (1920s–30s), provoked controversy in the black community with the title of his novel "Nigger Heaven" (1926), wherein the usage increased sales; of the controversy, Langston Hughes wrote:
No book could possibly be as bad as "Nigger Heaven" has been painted. And no book has ever been better advertised by those who wished to damn it. Because it was declared obscene, everybody wanted to read it, and I'll venture to say that more Negroes bought it than ever purchased a book by a Negro author. Then, as now, the use of the word "nigger" by a white was a flashpoint for debates about the relationship between black culture and its white patrons.
In the US, the recurrent reading curricula controversy about the vocabulary of the novel "Adventures of Huckleberry Finn" (1885) by Mark Twain about the slave South, risks censorship because of 215 (counted) occurrences of the word "nigger", most refer to Jim, Huckleberry's escaped-slave raft-mate. Twain's advocates note that the novel is composed in then-contemporary vernacular usage, not racist stereotype, because Jim, the black man, is a sympathetic character in the nineteenth-century "Adventures of Huckleberry Finn". The book was re-published in 2010 with edits removing "the 'N' word" as reported in "Time" online. The "Adventures of Huckleberry Finn" has been the subject of controversy in Arizona, where a parent group's attempt to have it removed from a required reading list was struck down by the court.
Moreover, unlike the literary escaped slave Jim, antebellum slaves used the artifice of self-deprecation (known as "Uncle Toms"), in pandering to societal racist assumptions about the black man's low intelligence, by advantageously using the word "nigger" to escape the violence inherent to slavery. Implicit to "Uncle Tomming" was the unspoken reminder to white folk that a presumably inferior and sub-human person could not, reasonably, be held responsible for poorly realized work, a kitchen fire, or any such catastrophic offense. The artificial self-deprecation deflected responsibility, in hope of escaping the violent wraths of overseer and master. Using "nigger" as a self-referential identity term also was a way of avoiding white suspicion, of encountering an intelligent slave, and so put whites at their ease. In context, a slave who referred to himself, or another black man, as a "nigger" presumed the master's "perceiving" him as a slave who has accepted his societally sub-ordinate role as private property, thus, not (potentially) subversive of the authority of the master's white supremacy.
Other late-nineteenth- and early twentieth-century British literary usages suggest neutral usage. The popular Victorian era entertainment, the Gilbert and Sullivan operetta "The Mikado" (1885) twice uses the word "nigger". In the song ', the executioner, Ko-ko, sings of executing the "nigger serenader and the others of his race", personified by black-faced singers singing minstrel songs. In the song ', the Mikado sings of the punishment for older women who dye their hair or wear corsets, to be "Blacked like a nigger/With permanent walnut juice." Both lyrics are usually changed for modern performances.
In Joseph Conrad's "The Nigger of the 'Narcissus"' (1897) the main character is a black man from the West Indies; the book was published in America as "The Children of the Sea". "Ten Little Niggers" (1939) was the original British title of Agatha Christie's novel "And Then There Were None", which has also been known by the alternative title "Ten Little Indians". The word is used in some of the Swallows and Amazons series (1930s) of children's books by Arthur Ransome, e.g. "Look like niggers to me" in The Big Six.
The Reverend W. V. Awdry's "The Railway Series" (1945–72) story "Henry's Sneeze", originally described soot-covered boys with the phrase "as black as niggers". In 1972, after complaints, the description was edited to "as black as soot", in the subsequent editions. Rev. Awdry is known for Thomas the Tank Engine (1946).
"How the Leopard Got His Spots", in "Just So Stories" (1902), by Rudyard Kipling, tells of an Ethiopian man and a leopard, both originally sand-colored, deciding to camouflage themselves with painted spots, for hunting in tropical forest. The story originally included a scene wherein the leopard (now spotted) asks the Ethiopian man why he does not want spots. In contemporary editions of "How the Leopard Got His Spots", the Ethiopian's original reply: "Oh, plain black's best for a nigger", has been edited to, "Oh, plain black's best for me." Again, Kipling uses the word in "A Counting-Out Song" ("Land and Sea Tales for Scouts and Guides", 1923), the rhyme reads: "Eenie Meenie Mainee, Mo! Catch a nigger by the toe!"
In short story, "The Basement Room" (1935), by Graham Greene, the (sympathetic) servant character, Baines, tells the admiring boy, son of his employer, of his African British colony service, "You wouldn't believe it now, but I've had forty niggers under me, doing what I told them to". Replying to the boy's question: "Did you ever shoot a nigger?" Bains answers: "I never had any call to shoot. Of course I carried a gun. But you didn't need to treat them bad, that just made them stupid. Why, I loved some of those dammed niggers." The cinematic version of "The Basement Room" short story, "The Fallen Idol" (1948), directed by Carol Reed, replaced novelist Greene's "niggers" usage with "natives". Flannery O'Connor's 1955 short story uses a black lawn jockey as a representative symbol in "The Artificial Nigger".
Popular culture.
In the US and the UK, the word "nigger" featured in branding and packaging consumer products, e.g. "Nigger Hair Tobacco" and "Niggerhead Oysters", Brazil nuts were called "nigger toes", et cetera. As racism became unacceptable in mainstream culture, the tobacco brand became "Bigger Hare" and the canned goods brand became "Negro Head".
Cinema.
The movie "Blazing Saddles" (1974) used "nigger" to ridicule US racism. In "The Kentucky Fried Movie" (1977), the sequence titled "Danger Seekers" features a stuntman effecting the dangerous stunt of shouting "Niggers!" at a group of black people, then fleeing when they chased him.
The movie "Full Metal Jacket" (1987) depicts black and white U.S. Marines enduring boot camp and later fighting together in Vietnam. "Nigger" is used by soldiers of both races in jokes and as expressions of bravado ("put a nigger behind the trigger", says the black Corporal "Eightball"), with racial differences among the men seen as secondary to their shared exposure to the dangers of combat: Gunnery Sergeant Hartman (R. Lee Ermey) says, "There is no racial bigotry here. I do not look down on niggers, kikes, wops or greasers. Here you are all equally worthless."
"Die Hard with a Vengeance" (1995) featured a scene where villain Simon Peter Gruber (Jeremy Irons) required NYPD Lt. John McClane (Bruce Willis) to wear a sandwich board reading "I hate niggers" while standing on a street corner in predominantly-black Harlem, resulting in McClane meeting Zeus Carver (Samuel L. Jackson) as Carver rescued McClane from being attacked by neighborhood toughs.
Nigger was the name given to a black Labrador dog that belonged to British Royal Air Force Wing Commander Guy Gibson during World War 2. In 1943, Gibson led the successful Operation Chastise attack on dams in Germany. The dog's name was used as a single codeword whose transmission conveyed that the Möhne dam had been breached. In the 1955 film "The Dam Busters" about the raid, the dog was portrayed in several scenes; his name and the codeword were mentioned several times. Some of the scenes in which the dog's name is uttered were later shown in the 1982 film "Pink Floyd – The Wall".
In 1999, the British television network ITV broadcast a censored version with each of the twelve utterances of "Nigger" deleted. Replying to complaints against its censorship, ITV blamed the regional broadcaster, London Weekend Television, which, in turn, blamed a junior employee as the unauthorised censor. In June 2001, when ITV re-broadcast the censored version of "The Dam Busters", the Index on Censorship criticised it as "unnecessary and ridiculous" censorship breaking the continuity of the film and the story. In January 2012 the film was shown uncensored on ITV4, but with a warning at the start that the film contained racial terms from the historical period which some people could find offensive. Versions of the film edited for US television have the dog's name altered to "Trigger".
In a remake of "The Dam Busters" by Peter Jackson announced in 2008, Stephen Fry, the writer of the screenplay, said there was "no question in America that you could ever have a dog called the N-word". In the remake the dog's name is "Digger".
American director Quentin Tarantino has been criticized by some critics for the heavy usage of the word "nigger" in his movies, especially in "Jackie Brown", where the word is used 38 times and "Django Unchained", used 110 times.
Literature.
In 1897, Joseph Conrad penned a novella titled "The Nigger of the 'Narcissus"', whose titular character, James Wait, is a West Indian black sailor on board the merchant ship "Narcissus" sailing from Bombay to London. In the United States, the novel was first published with the title "The Children of the Sea: A Tale of the Forecastle", at the insistence by the publisher, Dodd, Mead and Company, that no one would buy or read a book with the word nigger in its title, not because the word was deemed offensive but that a book about a black man would not sell. In 2009, WordBridge Publishing published a new edition titled "The N-Word of the Narcissus", which also excised the word nigger from the text. According to the publisher, the point was to get rid of the offensive word, which may have led readers to avoid the book, and make it more accessible. Though praised in some quarters, many others denounced the change as censorship. The author Carl Van Vechten took the opposite view to Conrad's publishers when he advised the British novelist Ronald Firbank to change the title of his 1924 novel "Sorrow in Sunlight" to "Prancing Nigger" for the American market, and it became very successful there under that title.
Mark Twain's novel "Adventures of Huckleberry Finn" has long been the subject of controversy for its racial content, including its use of the word "nigger" as applied to the escaped slave character Jim. "Huckleberry Finn" was the fifth most challenged book during the 1990s, according to the American Library Association. In 2011, a new edition of the book published by NewSouth Books replaced the word "nigger" throughout the book with the word "slave" and also removed the word "injun". The change was spearheaded by Twain scholar Alan Gribben in the hope of "countering the 'pre-emptive censorship'" that results from the book's being removed from school curricula over language concerns. The changes sparked outrage from critics and scholars.
Music.
The Bohemian composer Antonín Dvořák wrote the String Quartet No. 12 in 1893 during his time in the United States. For its presumed association with African-American music, the quartet was referred to until the 1950s with nicknames such as "Negro Quartet" and "Nigger Quartet" before being called the "American Quartet".
Responding to accusations of racism after referring to "niggers" in the lyrics of the Guns N' Roses song, "One in a Million", Axl Rose stated "I was pissed off about some black people that were trying to rob me. I wanted to insult those particular black people. I didn't want to support racism."
The folk song "Oh! Susanna" by Stephen Foster had originally been written in four verses. The second verse describes an industrial accident which "kill’d five hundred Nigger" by electrocution.
The country music artist David Allan Coe used the racial terms "redneck", "white trash", and "nigger" in the songs "If That Ain't Country, I'll Kiss Your Ass" and "Nigger Fucker". In the 1960s, record producer J. D. "Jay" Miller published pro-racial segregation music with the "Reb Rebel" label featuring racist songs by Johnny Rebel and others, demeaning black Americans and the Civil Rights movement.
Contemporarily, rap groups such as N.W.A. (Niggaz with Attitudes), re-popularized the usage in their songs. One of the earliest uses of the word in hip hop was in the song "New York New York" by Grandmaster Flash and the Furious Five in 1983.
Theatre.
The musical "Show Boat" (from 1927 until 1946) features the word "nigger" as originally integral to the lyrics of "Ol' Man River" and "Cotton Blossom"; although deleted from the cinema versions, it is included in the 1988 EMI recording of the original score. Musical theatre historian Miles Kreuger and conductor John McGlinn propose that the word was not an insult, but a blunt illustration of how white people then perceived black people.
Comedy.
Some comedians have broached the subject, almost invariably in the form of social commentary. This was perhaps most famously done by stand-up comedian Chris Rock in his Niggas vs. Black People routine.
Translations.
"Nigger" or "nigger brown" were used in Britain as standard colour names, in the same way as "lime green". This may have been included in some language translation sources.
"Nigger-brown" colored furniture.
In April 2007, a dark brown leather sofa set, sold by Vanaik Furniture and Mattress Store in Toronto, Canada, was labelled as "Nigger-brown" color. Investigation determined that the Chinese manufacturer used an outdated version of Kingsoft's Chinese-to-English translation software for writing the tags; it translated the Chinese "dark-brown" characters to "Nigger-brown", and neither the Canadian supplier nor the store owner had noticed the incorrectly translated tag; subsequently, Kingsoft corrected its translation software.
"Nigger brown" pants.
In 2012, a typosquatting website called abercrombie-and-fitchoutlet.com, purporting to be of the clothing branch Abercrombie & Fitch and based in China, offered "nigger brown pants" for sale as the result of a faulty Chinese-to-English translator. This went viral on social media after people mistakenly believed that Abercrombie & Fitch were selling the product. A similar translation mistakes was made in 2014, involving a Chinese typosquatting counterfeit site purporting to be the clothing branch Ralph Lauren.
Derivations.
Place names.
The word "nigger" features in official place-names, such as "Nigger Bill Canyon", "Nigger Hollow", and "Niggertown Marsh". In 1967, the United States Board on Geographic Names changed the word "nigger" to "Negro" in 143 place names. First changed to "Negrohead Mountain", a peak above Santa Monica, California was renamed on (February 2010) to Ballard Mountain in honor of John Ballard, a black pioneer who settled the area in the 19th century. "Nigger Head Mountain", at Burnet, Texas, was so named because the forest atop it resembled a black man's hair. In 1966, the US First Lady, Lady Bird Johnson, denounced the racist name, asking the U.S. Board on Geographic Names and the U.S. Forest Service to rename it, becoming "Colored Mountain" in 1968; and in West Texas, "Dead Nigger Creek" was renamed "Dead Negro Draw". "Nigger Nate Grade", near Temecula, California, named for Nate Harrison, an ex-slave and settler, was renamed "Nathan Harrison Grade Road" in 1955, at the request of the NAACP.
In northwestern North America, particularly in Canada and the US, there are places which feature many uses of the word "nigger". At Penticton, British Columbia, Canada, "Niggertoe Mountain" was renamed Mount Nkwala. The place-name derived from a 1908 Christmas story about three black men who died in a blizzard; the next day, the bodies of two were found at the foot of the mountain. A point on the Lower Mississippi River, in West Baton Rouge Parish, named "Free Nigger Point" until the late twentieth century, first was renamed "Free Negro Point", but currently is named "Wilkinson Point". "Nigger Head Rock", protruding from a cliff above Highway 421, north of Pennington Gap, Virginia, was renamed "Great Stone Face" in the 1970s.
Derivatives.
"The N-word" euphemism.
Notable usage
The prosecutor [Christopher Darden], his voice trembling, added that the "N-word" was so vile that he would not utter it. "It's the filthiest, dirtiest, nastiest word in the English language."
”
— Kenneth B. Noble, January 14, 1995 "The New York Times"
The euphemism "the N-word" became mainstream American English usage during the racially contentious murder trial of ex-football player O. J. Simpson in 1995.
Key prosecution witness Detective Mark Fuhrman, of the Los Angeles Police Department (LAPD) – who denied using racist language on duty – impeached himself with his prolific use of "nigger" in tape recordings about his police work. The recordings, by screenplay writer Laura McKinney, were from a 1985 research session wherein the detective assisted her with a screenplay about LAPD policewomen. Fuhrman excused his use of the word saying he used "nigger" in the context of his "bad cop" persona. Linguistically, the popular press reporting and discussing Fuhrman's testimony substituted "the N-word" in place of "nigger".
Homophones.
"Niger" (Latin for "black") occurs in Latinate scientific nomenclature and is the root word for some homophones of "nigger"; sellers of niger seed (used as bird feed), sometimes use the name "Nyjer" seed. The classical Latin pronunciation /ˈniɡeɾ/ sounds like the English /ˈnɪɡər/, occurring in biologic and anatomic names, such as "Hyoscamus niger" (black henbane), and even for animals that are not in fact black, such as "Sciurus niger" (fox squirrel).
"Nigra" is the Latin feminine form of "niger" (black), used in biologic and anatomic names such as substantia nigra (black substance).
The word "niggardly" (miserly) is etymologically unrelated to "nigger", derived from the Old Norse word "nig" (stingy) and the Middle English word "nigon". In the US, this word has been misinterpreted as related to "nigger" and taken as offensive. In January 1999, David Howard, a white Washington, D.C. city employee, was compelled to resign after using "niggardly"—in a financial context—while speaking with black colleagues, who took umbrage. After reviewing the misunderstanding, Mayor Anthony Williams offered to reinstate Howard to his former position. Howard refused reinstatement but took a job elsewhere in the mayor's government.
The portmanteau word "wigger" (white + nigger) denotes a white person emulating "street black behavior", hoping acceptance to the hip hop, thug, and gangsta sub-cultures.
Intragroup versus intergroup usage.
Black listeners often react to the term differently, depending on whether it is used by white speakers or by black speakers. In the former case, it is regularly understood as an insult; in the latter, it may carry notes of in-group disparagement, or even be understood as neutral or affectionate, a possible instance of reappropriation.
Among the black community, the slur "nigger" is almost always rendered as "nigga", representing the pronunciation of the word in African American Vernacular English. This usage has been popularized by the rap and hip-hop music cultures and is used as part of an in-group lexicon and speech. It is not necessarily derogatory and, when used among black people, the word is often used to mean "homie" or "friend".
Acceptance of intra-group usage of the word "nigga" is still debated, although it has established a foothold amongst younger generations. The NAACP denounces the use of both "nigga" and "nigger". Mixed-race usage of "nigga" is still considered taboo, particularly if the speaker is white. However, trends indicate that usage of the term in intragroup settings is increasing even amongst white youth due to the popularity of rap and hip hop culture.
According to Arthur K. Spears (Diverse Issues in Higher Education, 2006)
In many African-American neighborhoods, nigga is simply the most common term used to refer to any male, of any race or ethnicity. Increasingly, the term has been applied to any person, male or female. "Where y'all niggas goin?" is said with no self-consciousness or animosity to a group of women, for the routine purpose of obtaining information. The point: Nigga is evaluatively neutral in terms of its inherent meaning; it may express positive, neutral or negative attitudes;
While Kevin Cato observes:
For instance, a show on Black Entertainment Television, a cable network aimed at a black audience, described the word nigger as a "term of endearment." "In the African American community, the word nigga (not nigger) brings out feelings of pride" (Davis 1). Here the word evokes a sense of community and oneness among black people. Many teens I interviewed felt that the word had no power when used amongst friends, but when used among white people the word took on a completely different meaning. In fact, comedian Alex Thomas on BET stated, "I still better not hear no white boy say that to me... I hear a white boy say that to me, it means 'White boy, you gonna get your ass beat.'"

</doc>
<doc id="37123" url="http://en.wikipedia.org/wiki?curid=37123" title="Theories of political behavior">
Theories of political behavior

Theories of political behavior, as an aspect of political science, attempt to quantify and explain the influences that define a person's political views, ideology, and levels of political participation. Theorists who have had an influence on this field include Karl Deutsch and Theodor Adorno.
Long-term influences on political orientation.
There are three main sources of influence that shape political orientation which creates long-term effects. Generally, the primary influence originates from family. As stated previously, children will often adopt their parents' ideological values. Some theorists have argued that family tends to be the strongest, most influential force which exists over the lifetime; one essay has credited the majority of the student activism of the 1930s to the influence of parents.
Secondly, teachers and other educational authority figures have a significant impact on political orientation. From as early as age 4 up until 18, children spend about 25% of their time involved in educational processes. Post-secondary education significantly raises the impact of political awareness and orientation; an October 2004 study of 1,202 college undergraduates across the United States showed that 87% of college students were registered to vote, compared to a national average of 64% of American adults. A study at Santa Clara University also showed that 84% of students there were registered to vote. Also consider that childhood and adolescent stages of personal growth have the highest level of impressionability.
Thirdly, peers also affect political orientation. Friends often, but not necessarily, have the advantage of being part of the same generation, which collectively develops a unique set of societal issues; Eric L. Dey has argued that "socialisation is the process through which individuals acquire knowledge, habits, and value orientations that will be useful in the future." The ability to relate on this common level is where the means to shape ideological growth.
Short-term influences on political orientation.
Short-term factors also affect voting behavior; the media and the impact of individual election issues are among these factors. These factors differ from the long-term factors as they are often short-lived. However, they can be just as crucial in modifying political orientation. The ways in which these two sources are interpreted often relies on the individuals specific political ideology formed by the long-term factors.
Most political scientists agree that the mass media have a profound impact on voting behavior. One author asserts that "few would argue with the notion that the institutions of the mass media are important to contemporary politics ... in the transition to liberal democratic politics in the Soviet Union and Eastern Europe the media was a key battleground."
Second, there are election issues. These include campaign issues, debates and commercials. Election years and political campaigns can shift certain political behaviors based on the candidates involved, which have different degrees of effectiveness in influencing voters.
The influence of social groups on political outcomes.
Recently, some political scientists have been interested in many studies which aimed to analyze the relation between the behavior of social groups and the political outcomes. Some of the social groups included in their studies have been age demographics, gender, and ethnic groups.
For example, in U.S. politics, the effect of ethnic groups and gender has a great influence on the political outcomes.
Latin Americans have a profound social impact on the political outcome of their vote and are emerging as a strong up-and-coming political force. The most noticeable increase in Latin American voting was in the 2000 presidential election, although the votes did not share a socially common political view at that time. In the 2006 election, the Latin American vote aided tremendously in the election of Florida Senator Mel Martinez, although in the 2004 presidential election, about 44% of Latin Americans voted for Republican President George W. Bush. Latin Americans have been seen to be showing an increasing trend in the issues on which they vote for, causing them to become more united when faced with political views. Currently illegal immigration has been claiming most attention and Latin Americans, although not completely unanimous, are concerned with the education, employment and deportation of illegal immigrants in the United States.
Over seven decades ago, women earned the right to vote and since then they have been making a difference in the outcomes of political election. Given that the right to be politically active has granted them the opportunity to expand their knowledge and influence in current affairs, they are now considered one of the main components in the country's decision-making in both politics and economy. According to The American Political Science Association, over the past 2004 presidential election, the women's vote may have well decided the outcome of the race. Susan Carroll, the author of "Women Voters and the Gender Gap", states that the increase of women influence on political behaviors is due to four main categories: women outnumber men among voters; significant efforts are underway to increase registration and turnout among women; a gender gap is evident in the 2004 election as it has been in every presidential election since 1980; and women constitute a disproportionately large share of the undecided voters who will make their decision late in the campaign.
Biology and political science.
Interdisciplinary studies in biology and political science aim to identify correlates of political behavior with biological aspects, for example the linkage of biology and political orientation, but also with other aspects like partisanship and voting behavior. This field of study is sometimes called biopolitics, although the term has other meanings. 
The study of possible genetic bases of political behavior has grown since the 1980s. The term genopolitics was coined by political scientist James Fowler in the early-2000s to describe research into identifying specific transporter/receptor genes responsible for ideological orientation beyond the sociopsychological realm of political socialisation.

</doc>
<doc id="37124" url="http://en.wikipedia.org/wiki?curid=37124" title="Austin Powers (character)">
Austin Powers (character)

Sir Augustine Danger "Austin" Powers, KBE, is a fictional character from the Austin Powers series of films, and is created and portrayed by Mike Myers. He is the protagonist of "" (1997), "The Spy Who Shagged Me" (1999) and "Austin Powers in Goldmember" (2002). He is a womanizing, hard-partying British spy embodying the Swinging London mod culture and hippie culture of the 1960s who, with his nemesis Dr. Evil, was frozen in a cryogenics experiment. The series' humor follows his attempts to adjust to the modern world as he continues to try to save it from terrorism.
Personality.
Austin Powers was a character seen as a parody of James Bond and being influenced by Harry Palmer and characters played by Peter Sellers. The character of Austin Powers represents an archetype of 1960s Swinging London, with his advocacy for free love, his use of obscure impressions and his clothing style (including crushed velvet suits and Beatle boots).
Development.
Myers, Matthew Sweet and Susanna Hoffs formed the faux British 1960s band Ming Tea after Myers' "Saturday Night Live" stint in the early 1990s. The band members all performed under pseudonyms with 1960s personas. Myers adopted the pseudonym and character of Austin Powers. This group made a number of live club and television performances in character. Myers' then wife Robin Ruzan encouraged him to write a film based on Austin Powers.
Obituaries of Simon Dee (1935–2009), the radio and television presenter, stated that his "Sixties grooviness" made him the inspiration for the character. Mike Myers has claimed his father was the inspiration behind Austin Powers.
Other media.
Video Games: Austin Powers, Austin Powers Pinball, Austin Powers: Welcome to my Underground Lair, Austin Powers: Oh Behave!, and Austin Powers: Operation Trivia.
In popular culture.
In 2010, he was voted #23 in "Entertainment Weekly"'s list "The 100 greatest characters of the last 20 years."

</doc>
<doc id="37125" url="http://en.wikipedia.org/wiki?curid=37125" title="Australian Security Intelligence Organisation">
Australian Security Intelligence Organisation

The Australian Security Intelligence Organisation (ASIO; ) is the national security service of Australia, which is responsible for the protection of the country and its citizens from espionage, sabotage, acts of foreign interference, politically motivated violence, attacks on the Australian defence system, and terrorism.
ASIO is comparable with the British Security Service (MI5) and the American Federal Bureau of Investigation (FBI). As with MI5 and FBI officers, ASIO officers have no police powers of arrest and are unarmed.
 Generally ASIO operations requiring police powers are co-ordinated with the Australian Federal Police and/or with State and Territory police forces.
 However, under the "National Security Legislation Amendment Bill 2014" passed by the Parliament of Australia, ASIO officers are exempt from prosecution for a wide range of illegal activities in the course of conducting "operations". This means they may carry arms, detain people, or carry out a wide variety of other ordinarily illegal acts in the course of operations.
ASIO Central Office is in Canberra, with a local office being located in each mainland state and territory capital. A new AUD $630 million Central Office, named the Ben Chifley Building, was officially opened by the Prime Minister Kevin Rudd on 23 July 2013.
Command, control and organisation.
ASIO is a statutory body under the "Australian Security Intelligence Organisation Act 1979" and is responsible to the Parliament of Australia through the Attorney-General. The Organisation also reports to the Parliamentary Joint Committee on Intelligence and Security, and is subject to independent review by the Inspector-General of Intelligence and Security. The head of ASIO is the Director-General of Security, who oversees the strategic management of ASIO within guidelines issued by the Attorney-General. The current Director-General is Duncan Lewis, who assumed office in September 2014.
In 2013, ASIO had a staff of around 1,740 personnel. The identity of ASIO officers, apart from the Director-General, remain an official secret. While ASIO is an equal opportunity employer, there has been some media comment of the Organisation's apparent difficulty in attracting people from a Muslim or Middle Eastern background. Furthermore, ASIO has undergone a period of rapid growth with some 70 per cent of the Organisation's officers having joined since 2002, leading to what Paul O'Sullivan, Director-General of Security from 2005 to 2009, called 'an experience gap'.
Powers and accountability.
Special investigative powers.
The special investigative powers available to ASIO officers under warrant signed by the Attorney-General include:
The Director-General also has the power to independently issue a warrant should a serious security situation arise and a warrant requested of the Attorney-General has not yet been granted.
An ASIO officer may also, without warrant, ask an operator of an aircraft or vessel questions about the aircraft or vessel, its cargo, crew, passengers, stores or voyage; and to produce supporting documents relating to these questions.
Special terrorism investigative powers.
When investigating terrorism, the Director-General may also seek a warrant from an independent judicial authority to allow:
The Director-General is not empowered to independently issue a warrant in relation to the investigation of terrorism.
Immunity from prosecution.
While The Act does not define any activities specifically to be legal, that is, to grant immunity for any specific crime, it does provide exceptions that will not be granted immunity. Section 35k (1) defines these activities as not being immune from liability for special intelligence conduct during special intelligence operations. That is to say, an ASIO operative would be deemed to have committed a crime if they were to participate in any of the following activities under any circumstances:
Collection of foreign intelligence.
ASIO also has the power to collect foreign intelligence within Australia at the request of the Minister for Foreign Affairs or the Minister for Defence. Known as Joint Intelligence Operations, and usually conducted in concert with the Australian Secret Intelligence Service the purpose of these operations is the gathering of security intelligence on and from foreign officials, organisations or companies.
Accountability.
Because of the nature of its work, ASIO does not make details of its activities public and law prevents the identities of ASIO officers from being disclosed. ASIO and the Commonwealth Government say that operational measures ensuring the legality of ASIO operations have been established.
ASIO briefs the Attorney General on all major issues affecting security and he/she is also informed of operations when considering granting warrants enabling the special investigative powers of ASIO. Furthermore, the Attorney-General issues guidelines with respect to the conduct of ASIO investigations relating to politically motivated violence and its functions of obtaining intelligence relevant to security.
ASIO reports to several governmental and parliamentary committees dealing with security, legislative and financial matters. This includes the Parliamentary Joint Committee on Intelligence and Security. A classified annual report is also provided to the government, an unclassified edited version of which is tabled in Federal Parliament.
The Office of the Inspector-General of Intelligence and Security was established in 1986 to provide additional oversight of Australia’s security and intelligence agencies. The Inspector-General has complete access to all ASIO records and has a range of inquisitorial powers.
Relationships with foreign agencies and services.
Australia’s intelligence and security agencies maintain close working relationships with the foreign and domestic intelligence and security agencies of other nations. As of 22 October 2008, ASIO has established liaison relationships with 311 authorities in 120 countries.
History.
Establishment and 'The Case'.
Following the conclusion of World War II, the joint United States-UK Venona project uncovered sensitive British and Australian government data being transmitted through Soviet diplomatic channels. Officers from MI5 were dispatched to Australia to assist local investigations. The leak was eventually tracked to a spy ring operating from the Soviet Embassy in Canberra. Consequently, allied Western governments expressed disaffection with the state of security in Australia.
Subsequently, on 16 March 1949, Prime Minister Ben Chifley issued a Directive for the Establishment and Maintenance of a Security Service, appointing South Australian Supreme Court Justice Geoffrey Reed as the first Director-General of Security. In August 1949, Justice Reed advised the Prime Minister that he had decided to name the service the 'Australian Security Intelligence Organization' ["sic"] (the spelling was amended in 1999 to bring it into line with the Australian standard form 'organisation'). The new service was to be modelled on the Security Service of the United Kingdom and an MI5 liaison team (including probable Soviet double agent Sir Roger Hollis) was attached to the fledgling ASIO during the early 1950s. Historian Robert Manne describes this early relationship as “special, almost filial” and continues “ASIO’s trust in the British counter-intelligence service appears to have been near-perfect”. One of the foundation directors of ASIO, Robert Frederick Bird Wake, in his son's biography "No Ribbons or Medals" about his father's work as a counter espionage officer, is credited with getting " the show" started in 1949. Wake worked closely with the then director general Judge Geoffry Reed. During World War Two Reed conducted an inquiry into Wake's performance as a security officer and found that he was competent and innocent of the charges laid by the Army's commander-in-chief, General Thomas Blamey. This was the start of a relationship between Reed and Wake that lasted for more than 10 years. Wake was seen as the operational head of ASIO.
When the Labor Government was defeated, the new prime minister, Robert Menzies, appointed the deputy director of Military Intelligence, Charles Spry, as the director. Wake resigned shortly after Spry's appointment.
The operation to crack the Soviet spy ring in Canberra consumed much of the resources of ASIO during the 1950s. This operation became internally known as "The Case". Among the prime suspects of the investigations were Wally Clayton, a prominent member of the Australian Communist Party, and two diplomats with the Department of External Affairs, Jim Hill and Ian Milner. However, no charges resulted from the investigations, because Australia did not have any laws against peacetime espionage at the time.
On 6 July 1950 the Charter of the Australian Security Intelligence Organization was defined by the directive of Prime Minister Menzies, following the appointment of Colonel Spry as the new Director-General. ASIO was converted to a statutory body on 13 December 1956 through the "Australian Security Intelligence Organisation Act 1956" (repealed by the "Australian Security Intelligence Organisation Act 1979", the current legislation as amended to 2007).
The Petrov Affair.
5 February 1951 saw the arrival in Sydney of Vladimir Mikhaylovich Petrov, Third Secretary of the Soviet Embassy. An ASIO field officer identified Petrov as a possible 'legal', an agent of the Soviet Ministry of State Security (MGB, a forerunner to the KGB) operating under diplomatic immunity. The Organisation began gently cultivating Petrov through another agent, Dr. Michael Bialoguski, with the eventual goal of orchestrating his defection. Ultimately, Petrov was accused by the Soviet Ambassador of several lapses in judgement that would have led to his imprisonment and probable execution upon his return to the Soviet Union. Petrov feared for his life and grabbed the defection life-line thrown him by ASIO.
The actual defection occurred on 3 April 1954. Petrov was spirited to a safe house by ASIO officers, but his disappearance and the seeming reluctance of Australian authorities to search for him made the Soviets increasingly suspicious. Fearing a defection by Petrov, MVD officers dramatically escorted his wife Evdokia to a waiting aeroplane in Sydney. There was doubt as to whether she was leaving by choice or through coercion and so Australian authorities initially did not act to prevent her being bundled into the plane. However, ASIO was in communication with the pilot and learned through relayed conversations with a flight attendant that if Evdokia spoke to her husband she might consider seeking asylum in Australia.
An opportunity to allow her to speak with her husband came when the Director-General of Security, Charles Spry, was informed that the MVD agents had broken Australian law by carrying firearms on an airliner in Australian airspace and so could be detained. When the aeroplane landed in Darwin for refuelling, the Soviet party and other passengers were asked to leave the plane. Police, acting on ASIO orders, quickly disarmed and restrained the two MVD officers and Evdokia was taken into the terminal to speak to her husband via telephone. After speaking to him, she became convinced he was alive and speaking freely and asked the Administrator of the Northern Territory for political asylum.
The affair sparked controversy in Australia when circumstantial links were noted between the leader of the Australian Labor Party and the Communist Party of Australia (and hence to the Soviet spy ring). H.V. Evatt, the leader of the Labor Party at the time, accused Prime Minister Robert Menzies of arranging the Petrov defection to discredit him. The accusations lead to a disastrous split in the Labor party.
Petrov was able to provide information on the structure of the Soviet intelligence apparatus in the mid-1950s, information that was highly valuable to the United States. It was by obtaining this information that the Organisation's reputation in the eyes of the United States was greatly enhanced.
In fact, when Brigadier Spry retired, the Deputy Director of the CIA sent the following tribute:
The Cold War.
ASIO's counter-intelligence successes continued throughout the Cold War. Following an elaborate investigation between 1961 and 1963, ASIO recommended the ejection of the First Secretary of the Soviet Embassy, Ivan Skripov, and his declaration as persona non grata. Skripov had been refining an Australian woman as an agent for Soviet intelligence; however, she was in fact an agent of ASIO.
In April 1983, ASIO uncovered more Soviet attempts at espionage and Valery Ivanov, who also held the post of First Secretary at the Soviet Embassy, was declared persona non grata. He was ejected from Australia on the grounds that he had performed duties in violation of his diplomatic status.
Penetration by the KGB.
These successes were marred, however, by the penetration of ASIO by a KGB mole in the 1970s. Due to the close defence and intelligence ties between Australia and the United States, ASIO became a backdoor to American intelligence. Upon realising ASIO was compromised, the United States pulled back on the information it shared with Australia.
Following a strenuous internal audit and a joint Federal Police investigation, George Sadil was accused of being the mole. Sadil had been a Russian interpreter with ASIO for some 25 years and highly classified documents were discovered in his place of residence. Federal Police arrested Sadil in June 1993 and charged him under the Crimes Act 1914 with several espionage and official secrets related offences. However, parts of the case against him collapsed the following year.
Sadil was committed to trial in March 1994, but the Director of Public Prosecutions decided not to proceed with the more serious espionage-related charges after reviewing the evidence against him. Sadil's profile did not match that of the mole and investigators were unable to establish any kind of money trail between him and the KGB.
Sadil pleaded guilty in December 1994 to thirteen charges of "removing ASIO documents contrary to his duty", and was sentenced to three months imprisonment. He was subsequently released on a 12 month good behaviour bond. It is believed that another ASIO officer, now retired, is suspected of being the mole but no prosecution attempts have been made.
In November 2004, former KGB Major-General Oleg Kalugin confirmed to the Australian Broadcasting Corporation's "Four Corners" programme that the KGB had in fact infiltrated ASIO in the late 1970s and early 1980s.
Sydney 2000 Olympic Games.
ASIO began planning for the 2000 Olympic and Paralympic Games, held in Sydney, as early as 1995. A specific Olympics Coordination Branch was created in 1997, and began recruiting staff with “specialised skills" the following year. In 1998, ASIO “strengthened information collection and analytical systems, monitored changes in the security environment more broadly, improved its communications technology and provided other agencies with strategic security intelligence assessments to assist their Olympics security planning.”
The Olympics Coordination Branch also began planning for the Federal Olympic Security Intelligence Centre (FOSIC) in 1998. FOSIC was to “provide security intelligence advice and threat assessments to State and Commonwealth authorities during the Sydney 2000 Games.”
Royal commissions, inquiries and reviews.
Royal Commission on Intelligence and Security, 1974–77.
On 21 August 1974, Prime Minister Gough Whitlam announced the establishment of the Royal Commission on Intelligence and Security to inquire into Australia’s intelligence agencies. Justice Robert Hope of the Supreme Court of New South Wales was appointed as Royal Commissioner.
In 1977 the Commission confirmed the need for Australia’s own security and intelligence agency and made many recommendations on improving the analytical capability and financial accountability of ASIO. It also advocated increased ministerial control, designated the conducting of security assessments for access to classified information to ASIO, and urged greater cooperation with police and foreign intelligence services. Also as a result of the Commission the jurisdiction of ASIO investigation was expanded to include sabotage and terrorism, and ASIO was given lawful authority to open mail, enter premises, use listening devices and intercept telegrams and telex under warrant.
Protective Security Review, 1978–79.
Following the Sydney Hilton bombing of 1978, the government commissioned Justice Hope with conducting a review into national protective security arrangements and into co-operation between Federal and State authorities in regards to security. In the report concluded in 1979, Justice Hope designated ASIO as the agency responsible for national threat assessments in terrorism and politically motivated violence. He also recommended that relations between ASIO and State and Territory police forces be regulated by arrangements between governments.
Royal Commission on Australian Security and Intelligence Agencies, 1983–84.
Following the publicity surrounding the expulsion of Valery Ivanov, First Secretary at the Soviet Embassy in Canberra, the Government established a Royal Commission to review the activities of Australian Security and Intelligence Agencies. Justice Hope was again Royal Commissioner.
Justice Hope completed his report in December 1984. His recommendations included that:
Justice Hope also recommended that amendments to the ASIO Act provide that “"it is not the purpose of the Act that the right of lawful advocacy, protest or dissent should be affected or that exercising those rights should, by themselves, constitute activity prejudicial to security"”.
Post-Cold War review, 1992.
In early 1992, Prime Minister Paul Keating commissioned a review “"of the overall impact of changes in international circumstances on the roles and priorities of the Australian intelligence agencies"”. In the Prime Minister’s statement of 21 July 1992, Mr Keating said:
The resource reductions mentioned were a cut of 60 staff and a $3.81 million budget decrease.
Inquiry into National Security, 1993.
Following the trial of George Sadil over the ASIO mole scandal and from concern about the implications of material having been removed from ASIO without authority, the Prime Minister announced the appointment of Mr Michael Cook AO (former head of the Office of National Assessments) to inquire into various aspects of national security. The review was completed in 1994.
Parliamentary Joint Committee inquiries.
The Parliamentary Joint Committee completed several reviews and inquiries into ASIO during the 1990s. The first concerned the security assessment process. Another was held in September into “"The nature, scope and appropriateness of the way in which ASIO reports to the Australian public on its activities".” The Committee concluded that “"the total package of information available to the Australian community about ASIO's operations exceeds that available to citizens in other countries about their domestic intelligence agencies".” Pursuant to this, recommendations were made regarding the ASIO website and other publicly accessible information.
Criticisms, controversies and conspiracies.
Opposition to the political left.
ASIO has been accused of executing an agenda against the Left of politics since its inception. In the 1960s, ASIO was also accused of neglecting its proper duties because of this supposed preoccupation with targeting the Left. Like other Western domestic security agencies, ASIO actively monitored protesters against the Vietnam War, Labor politicians and various writers, artists and actors who tended towards the Left. Other claims go further, alleging that the Organisation compiled a list of some 10,000 suspected Communist sympathisers who would be interned should the Cold War escalate.
Raids on ASIO Central Office, 1973.
Further accusations against ASIO were raised by the Attorney-General following a series of bombings from 1963 to 1970 on the consulate of Communist Yugoslavia in Australia by Croatian far-right militia. Attorney-General Lionel Murphy alleged that ASIO had withheld information on the group which could have led to preventative measures taken against further bomb attacks (however, Murphy was a member of the recently sworn-in Labor government, which still held a deep-seated suspicion of ASIO).
On 15 March 1973, Murphy and the Commonwealth Police raided the ASIO offices in Melbourne. While some claim the raid was disastrous, serving little purpose other than to shake-up both ASIO and the Whitlam government, the findings of such investigations were not published.
The Sydney Hilton bombing allegations of conspiracy, 1978.
On 13 February 1978, the Sydney Hilton Hotel was bombed, one of the few domestic terrorist incidents on Australian soil. The Hotel was the location for the Commonwealth Heads of Government Meeting (CHOGM). Three people in the street were killed – two council workers and a policeman – and several others injured. Former police officer Terry Griffiths, who was injured in the explosion, provided some evidence that suggested ASIO might have orchestrated the bombing or been aware of the possibility and allowed it to proceed. In 1985, the Director-General of Security issued a specific denial of the allegation. In 1991 the New South Wales parliament unanimously called for a joint State-Federal inquiry into the bombing. However, the Federal government vetoed any inquiry.
Anti-terrorism bungle, 2001.
A few weeks after the 11 September 2001 attacks on the United States, mistakes led ASIO to incorrectly raid the home of Bilal Daye and his wife. It has been revealed that the search warrant was for a different address. The couple subsequently sought damages and the embarrassing incident was settled out of court in late 2005, with all material relating to the case being declared strictly confidential.
Kim Beazley-Ratih Hardjono investigation, 2004.
In June 2004, Kim Beazley was accused of having a "special relationship" with Ratih Hardjono when he was defence minister. Hardjono was allegedly accused of "inappropriately" photographing a secure Australian Defence facility, working with the embassy ID, and having a close working relationship with her uncle, a senior officer in BAKIN (Indonesian Intelligence). In July, journalist Greg Sheridan contacted the then head of ASIO, Dennis Richardson, and discussed a classified operational investigation. Later in July members of the Attorney General's department were still investigating the original allegation, making Richardson's comments premature and inaccurate. The whole episode was a salient reminder to politicians in Canberra of the British experience of 'agents of influence' and honeypots. Ratih Hardjono was married to Bruce Grant in the 1990s.
Detention and removal of Scott Parkin, 2005.
In September 2005, the visa of American citizen, Scott Parkin, was cancelled after Director-General of Security, Paul O'Sullivan, issued an adverse security assessment of the visiting peace activist. Parkin was detained in Melbourne and held in custody for five days before being escorted under guard to Los Angeles, where he was informed that he was required to pay the Australian Government A$11,700 for the cost of his detention and removal. Parkin is challenging the adverse security assessment in the Federal Court in a joint civil action with two Iraqi refugees, Mohammed Sagar and Muhammad Faisal, who faced indefinite detention on the island of Nauru after also receiving adverse security assessments in 2005.
Prior to his removal, Parkin had given talks on the role of U.S. military contractor Halliburton in the Iraq war and led a small protest outside the Sydney headquarters of Halliburton subsidiary KBR. The Attorney-General at that time, Philip Ruddock, refused to explain the reasons for Parkin's removal, leading to speculation that ASIO had acted under pressure from the United States. This was denied by O'Sullivan before a Senate committee, where he gave evidence that ASIO based its assessment only on Parkin's activities in Australia. O'Sullivan refused to answer questions before a later Senate committee hearing after his legal counsel told the Federal Court that ASIO did not necessarily base its assessment solely on Parkin's activities in Australia.
Kidnap and false imprisonment of Izhar ul-Haque, 2007.
On 12 November 2007, the Supreme Court of New South Wales dismissed charges brought against a young medical student, Izhar ul-Haque. ASIO and the Australian Federal Police had investigated ul-Haque for allegedly training with Lashkar-e-Toiba in Pakistan, a declared terrorist organisation under the "Security Legislation Amendment (Terrorism) Act 2002". However, the case against the medical student collapsed when it was revealed that ASIO officers had engaged in improper conduct during the investigation. Justice Michael Adams determined that because ul-Haque was falsely led to believe that he was legally compelled to comply with the ASIO officers, the conduct of at least one of the investigating ASIO officers constituted false imprisonment and kidnap at common law, and therefore key evidence against ul-Haque was inadmissible.
Archival material.
Under the Archives Act 1983, ASIO files can be released to the public after 30 years unless they fall into any of 16 exemption categories (as itemised in section 33 of the Archives Act).

</doc>
<doc id="37126" url="http://en.wikipedia.org/wiki?curid=37126" title="Australian Secret Intelligence Service">
Australian Secret Intelligence Service

The Australian Secret Intelligence Service (ASIS; ) is the national intelligence agency of Australia, which is responsible for collecting foreign intelligence, undertaking counter-intelligence activities and cooperation with other intelligence agencies overseas. ASIS is comparable with the with the British Secret Intelligence Service (MI6) and the American Central Intelligence Agency (CIA).
According to its website, the mission of ASIS is to:
"Protect and promote Australia's vital interests through the provision of unique foreign intelligence services as directed by Government."
ASIS is part of the Department of Foreign Affairs and Trade portfolio and is housed within DFAT's headquarters in Canberra. Its current Director-General is Nick Warner.
History.
On 13 May 1952, in a meeting of the Executive Council, Prime Minister Robert Menzies established ASIS by the executive power of the Commonwealth under "s 61" of the Constitution, appointing Alfred Deakin Brookes as head. The existence of ASIS remained secret even within the Government for a period of twenty years.
Its Charter of 15 December 1954 described ASIS's role as "to obtain and distribute secret intelligence, and to plan for and conduct special operations as may be required". ASIS was expressly required to "operate outside Australian territory." A Ministerial Directive of 15 August 1958 indicated that its special operations role included conducting "special political action." It also indicated that the organisation would come under the control and supervision of the Minister for External Affairs rather than the Minister for Defence. At the time, ASIS was substantially modeled on the United Kingdom Secret Intelligence Service, also known as MI6. ASIS was at one time referred to as MO9.
On 1 November 1972, ASIS was sensationally exposed by "The Daily Telegraph" which ran an exposé regarding recruitment of ASIS agents from Australian universities for espionage activities in Asia. Soon after "The Australian Financial Review" published a more in-depth piece on the Australian Intelligence Community (ASIO, ASIS, the Joint Intelligence Organisation (JIO) [now the Defence Intelligence Organisation (DIO)], the Defence Signals Division (DSD) [formerly the Defence Signals Directorate, now the Australian Signals Directorate] and the Office of National Assessments (ONA)). It stated that "[t]he ASIS role is to collect and disseminate facts only. It is not supposed to be in the analytical or policy advising business though this is clearly difficult to avoid at times." The Ministerial Statement of 1977 stated that the "main function" of ASIS was to "obtain, by such means and subject to such conditions as are prescribed by the Government, foreign intelligence for the purpose of the protection or promotion of Australia or its interests."
On 25 October 1977, then Prime Minister Malcolm Fraser declared the existence of ASIS and its functions following a recommendation by the first of the Hope Royal Commissions (see below).
In 1992 two reports were prepared on ASIS by officers within the Department of Prime Minister and Cabinet and Office of National Assessments for the Secretaries Committee on Intelligence and Security (SCIS) and the Security Committee of Cabinet (SCOC). The Richardson Report in June examined the roles and relationships of the collection agencies (ASIO, ASIS and DSD) in the post cold war era. The Hollway Report in December examined shortfalls in Australia's foreign intelligence collection. Both reports endorsed the structure and roles of the organisations and commended the performance of ASIS.
Royal Commissions examining ASIS.
Three Royal Commissions have examined, among other things, ASIS and its operations: in 1974 and 1983 (the Hope Royal Commissions), and in 1994 (the Samuels and Codd Royal Commission).
First Hope Royal Commission.
On 21 August 1974, the Whitlam Government appointed Justice Robert Hope to conduct a Royal Commission into the structure of security and intelligence services, the nature and scope of the intelligence required and the machinery for ministerial control, direction and coordination of the security services. The Hope Royal Commission delivered eight reports, four of which were tabled in Parliament on 5 May 1977 and 25 October 1977. Aside from the observation that ASIS was 'singularly well run and well managed', the report(s) on ASIS were not released. Results from the other reports included the Australian Security Intelligence Organisation Act 1979 and the establishment of the Office of National Assessments (ONA) and the passage of the Office of National Assessments Act 1977.
Second Hope Royal Commission.
On 17 May 1983 the Hawke Government reappointed Justice Hope to conduct a second Royal Commission into ASIS, ASIO, ONA, DSD and JIO (now DIO). The inquiry was to examine progress in implementing the previous recommendations; arrangements for developing policies, assessing priorities and coordinating activities among the organisations; ministerial and parliamentary accountability; complaints procedures; financial oversight and the agencies' compliance with the law. As with the first Hope Royal Commission, the reports on ASIS and DSD, which included draft legislation on ASIS, were not made public.
Samuels and Codd Royal Commission.
In response to a "Four Corners" program aired on 21 February 1994, on 23 February 1994, the Minister for Foreign Affairs Gareth Evans announced a 'root and branch' review of ASIS. The Government appointed Justice Gordon Samuels and Mike Codd to inquire into the effectiveness and suitability of existing arrangements for control and accountability, organisation and management, protection of sources and methods, and resolution of grievances and complaints. The Royal Commission reported in March 1995.
"Four Corners" reporter Ross Coulthart made allegations regarding intelligence held by ASIS on Australians. He claimed that 'ASIS secretly holds tens of thousands of files on Australian citizens, a database completely outside privacy laws'. This allegation was investigated and denied by Samuels and Codd (see below), but the Minister did acknowledge that ASIS maintained files. The Minister said: 'ASIS does have some files, as one would expect in an organisation of that nature, even though its brief extends to activities outside the country rather than inside. They are essentially of an administrative nature.'
However, Samuels and Codd did find that certain grievances of the former officers were well founded. They appeared to support the officers' concerns regarding the grievance procedures:
"Bearing in mind the context in which the members of ASIS work, it is not surprising that there should develop a culture which sets great store by faithfulness and stoicism and tends to elevate conformity to undue heights and to regard the exercise of authority rather than consultation as the managerial norm."
However, Samuels and Codd observed that the information published in the "Four Corners" program was 'skewed towards the false', that 'the level of factual accuracy about operational matters was not high', and, quoting an aphorism, that 'what was disturbing was not true and what was true was not disturbing'. They concluded that the disclosure of the information was unnecessary and unjustifiable and had damaged the reputation of ASIS and Australia overseas. They rejected any suggestion that ASIS was unaccountable or 'out of control'. They said, 'its operational management is well structured and its tactical decisions are thoroughly considered and, in major instances, subject to external approval'. They recommended that complaints regarding ASIS operations continue to be handled by the Inspector-General of Intelligence and Security (IGIS) but that staff grievances be handled by the Administrative Appeals Tribunal.
In addition to their recommendations, Samuels and Codd put forward draft legislation to provide a statutory basis for ASIS and to protect various information from disclosure. The Samuels and Codd Bill, like the bulk of the reports, was not made public.
Controversies.
The Sheraton Hotel incident.
On 30 November 1983, ASIS garnered unwanted negative attention when a training operation held at the Sheraton Hotel, now the Mercure (Spring Street), in Melbourne went wrong. The exercise was to be a mock surveillance and hostage rescue of foreign intelligence officers. It involved junior officers who had undergone three weeks prior training and who were given considerable leeway in planning and executing the operation.
The mock hostage rescue was staged on the 10th floor of the hotel without the permission of the hotel's owner or staff. When ASIS officers were refused entry into a hotel room, they broke down the door with sledgehammers. The hotel manager, Nick Rice, was notified of a disturbance on the 10th floor by a hotel guest. When he went to investigate, he was forced back into the lift by an ASIS officer who rode the lift down to the ground floor and forcibly ejected Rice into the lobby. Believing a robbery was in progress, Rice called the police. When the lift started returning to the ground floor, ASIS officers emerged wearing masks and openly brandishing 9mm Browning pistols and Heckler & Koch MP5 submachine guns, two of them with silencers. They forced their way through the lobby to the kitchen, where two getaway cars were waiting outside the kitchen door. Police stopped one of the cars and arrested the occupants, who refused to produce any form of identification.
Within two days the Minister for Foreign Affairs Bill Hayden announced that 'an immediate and full investigation' would be conducted under the auspices of the second Hope Royal Commission on Australian Security and Intelligence Agencies, which was still in progress. A report was prepared and tabled by February 1984. It described the exercise as being 'poorly planned, poorly supervised and poorly run' and recommended that measures be taken in training to improve planning and eliminate adverse impacts on the public.
The Victoria Police conducted their own investigation but were frustrated because the Director General of ASIS, John Ryan, refused to cooperate. Bill Hayden offered to provide the real names of the seven officers involved in confidence. Premier of Victoria John Cain told Hayden that "as far as the police were concerned, there was no such thing as information in confidence".
Following the incident, "The Sunday Age" disclosed the names, or the assumed names, of five of the officers involved. The journalist noted that 'according to legal advice taken by "The Sunday Age" there is no provision that prevents the naming of an ASIS agent'. While not included within the public version of the report, the Royal Commission headed by Mr Justice Hope did prepare an appendix which would appear to have dealt with the possible security and foreign relations consequences of disclosure of participants' names by "The Sunday Age". Subsequently, in "A v Hayden", the High Court held that the Commonwealth owed no enforceable duty to ASIS officers to maintain confidentiality of their names or activities.
At the time of the Sheraton Hotel incident, the extant Ministerial Directive permitted ASIS to undertake 'covert action', including 'special operations' which, roughly described, comprised 'unorthodox, possibly para-military activity, designed to be used in case of war or some other crisis'. Following the incident and the recommendations of the Royal Commission, the covert action function was apparently abolished. The functions of ASIS can be found in section 6 of the Intelligence Services Act, as can those functions which are proscribed by the act.
Ultimately, in executing the operation, the trainees were found to have used considerable force, menacing a number of the staff and guests with weapons and physically assaulting the hotel manager. Hope found Ryan to be at fault for authorising the training operation in a public place using concealed weapons. Ryan resigned in February 1984. Hope said it was not part of his Terms of Reference to make findings or recommendations on whether any individual had committed any offence. However he did note that the individuals could potentially be prosecuted by the State of Victoria with a long list of criminal offences, including possession of firearms without a licence, possession of prohibited implements (including machine guns, silencers and housebreaking tools), aggravated burglary in possession of a firearm, common assault, wilful damage to property, possession of a disguise without lawful excuse and numerous motor vehicle offences. More than a year after the raid, the Victorian Director of Public Prosecutions concluded that while certain offences had been committed, including criminal damage and assault with a weapon, there was insufficient evidence to charge any person with a specific offence.
Victorian Holdings Ltd, the company managing the hotel, subsequently took legal action against the Commonwealth on behalf of itself and 14 hotel staff. The matter was settled out of court with the hotel being offered $300,000 in damages. The total payout to the hotel and staff was $365,400.
Involvement in Papua New Guinea.
Between 1989 and 1991 ASIS came under scrutiny following allegations relating to its role and activities in Papua New Guinea. It was alleged that ASIS had been involved in training Papua New Guinean troops to suppress independence movements in Irian Jaya and Bougainville. (In 1997 it was alleged that ASIS and DSD had failed to collect, or the Government had failed to act upon, intelligence regarding the role and presence of Sandline contractors in relation to the independence movement in Bougainville.)
"Four Corners" program.
Towards the end of 1993 ASIS became the subject of media attention after allegations were made by former ASIS officers that ASIS was unaccountable and out of control. One newspaper alleged that 'ASIS regularly flouted laws, kept dossiers on Australian citizens ... and hounded agents out of the service with little explanation'. In particular it alleged that agents were being targeted in a purge by being threatened with criminal charges relating to their official conduct, reflecting a pattern which suggested to some that ASIS or a senior ASIS officer had been 'turned' by a foreign intelligence service.
On 21 February 1994 "Four Corners" ran a program which aired the key allegations. Two former ASIS officers made claims regarding cultural and operational tensions between ASIS and the Department of Foreign Affairs and Trade (DFAT). They claimed that embassy staff had maliciously or negligently compromised activities involving the running of foreign informants and agents and the defection of foreign agents to Australia. They claimed that their grievances were ignored and that they were 'deserted in the field' and made scapegoats by ASIS.
The officers and the reporter Ross Coulthart also made brief claims regarding operational activities and priorities. The officers personally claimed that ASIS advice had been ignored by DFAT. The reporter repeated claims regarding ASIS operations aimed at destabilising the Aquino Government in the Philippines. He also made claims regarding ASIS assistance to MI6 in the Falkland conflict, in Hong Kong and in Kuwait for the benefit of British interests (including commercial interests) and potentially to the detriment of Australian interests.
The bulk of the personal statements by the officers concerned their private grievances. They raised two issues of public interest regarding the effect of secrecy on the operation of grievance procedures and the extent to which the Minister for Foreign Affairs and Trade was aware of or in control of ASIS operations. The reporter directly raised the issue of the appropriateness of ASIS operations particularly with respect to priority setting in overseas postings and operations, cooperation with foreign intelligence services, and the privacy of Australian persons and organisations. By implication, the program queried the extent to which ASIS is or should be accountable to the Minister, to Government and to Parliament.
The following day, the Shadow Minister for Foreign Affairs called for an independent judicial inquiry into the allegations. He expressed particular concern about the nature of ASIS cooperation with foreign agencies and the defects in ASIS grievance procedures. He later called for the inquiry to examine the 'poisoned relationship between ASIS and DFAT'. The Democrats spokeswoman called for a standing parliamentary committee.
Two days after the program aired, the Samuels and Codd Royal Commission was formed by Minister for Foreign Affairs Gareth Evans.
Alleged management and staffing problems.
In 2005, "The Bulletin" ran an article based on allegations by serving ASIS officers that alluded to gross mismanagement of intelligence operations, staff assignments, and taskings, particularly with respect to the war on terrorism.
The unnamed officers pointed out various problems within the agency that were plaguing the organisation's ability to collect vital and timely intelligence, such as the pitting of "..."young mostly white university educated agents with limited language skills and little knowledge of Islam against poor, zealous extremists intent on becoming suicide bombers".", the "inappropriate" assignment of "..."young female IOs" (Intelligence Officers) "against Islamic targets"...", poor staff retention rates, and general lack of officers possessing meaningful field experience.
The officers also cite a lack of proper support given to IOs tasked against terrorist targets, and the doctoring of intelligence by ASIS management, as also contributing to the lack of progress of the agency in the war on terrorism.
Legislative changes affecting ASIS.
Intelligence Services Act 2001.
ASIS was created as a result of an Executive Order in 1952, and as such, had no legislative basis. On 27 June 2001, the Intelligence Services Act 2001 (ISA) was introduced into Parliament by then Minister for Foreign Affairs Alexander Downer, which proposed significant changes to the Australian Intelligence Community (AIC). The Act was passed by Parliament on 29 October 2001.
The Intelligence Services Act 2001 converted ASIS into a statutory body, headed by the Director General. It set out the function of ASIS and the limit on that function. 
Use of weapons was prohibited by ASIS (except for self-defence). Conduct of violent or para-military operations was also curtailed.
The act authorised the minister responsible for ASIS to issue directions to the agency.
Ministerial authorisation is required for intelligence collection activities involving Australians but limited the circumstances in which this could be done.
It required the minister to make rules regulating the communication and retention of intelligence information concerning Australian persons. The Act provided for the establishment of a parliamentary oversight committee, the Parliamentary Joint Committee on ASIO, ASIS and DSD.
Intelligence Services Amendment Act 2004.
On 15 October 2003, the "Intelligence Services Amendment Bill 2003" was introduced into Parliament by Foreign Minister Alexander Downer, as an amendment to the original Intelligence Services Act 2001 (ISA). The Bill sought to amend the original ISA to allow ASIS to be involved in the planning and undertaking of paramilitary or violent activities "by others", and provide, train with, and use weapons and self-defence techniques "in certain circumstances" (that is where the overseeing minister deems the circumstances suitable). The Bill allowed ASIS to work with other organisations (such as the CIA or MI6) in paramilitary operations, provided ASIS staff and agents were not personally involved in carrying it out. Passed on 1 April 2004, five and a half months after it was introduced, the legislation enables ASIS Intelligence Officers to carry a firearm, but only for protection. These officers will also receive weapons handling training, and new generation interrogation training.
Credit.
A large portion of the history of ASIS was adapted from the Parliament of Australia of Intelligence Services Act 2001

</doc>
<doc id="37127" url="http://en.wikipedia.org/wiki?curid=37127" title="Australian Signals Directorate">
Australian Signals Directorate

Australian Signals Directorate (ASD) (formerly: Defence Signals Directorate (DSD)) is an Australian government intelligence agency responsible for signals intelligence (SIGINT) and information security (INFOSEC). ASD was established in 1947.
Overview.
According to its website, ASD has two principal functions:
Based in Canberra, at the Defence Headquarters at Russell Offices it operates monitoring facilities at Kojarena, Western Australia and at Shoal Bay, Northern Territory, which are believed to be part of the ECHELON system. It may also play a role in Pine Gap.
Under the 1948 UKUSA agreement, ASD's intelligence is shared with its foreign partner agencies:
Electronic warfare operators in the Royal Australian Corps of Signals work closely with the Australian Signals Directorate.
7 Signal Regiment (Electronic Warfare) at Borneo Barracks, Cabarlah, Queensland is associated with ASD.
Facilities.
The ASD operates at least three receiving stations:
These stations contribute signals intelligence for many Australian Government bodies, as well as the wider UKUSA partners. The ASD is also presumed to maintain a workforce at Pine Gap in central Australia.
In addition, it has been reported that many Australian embassies and overseas missions also house small facilities which provide a flow of signals intelligence to ASD.
Naming.
The Directorate has operated under a number of different names since its founding:
See also.
Other Countries

</doc>
<doc id="37129" url="http://en.wikipedia.org/wiki?curid=37129" title="Darkthrone">
Darkthrone

Darkthrone is an influential Norwegian metal band. They formed in 1986 as a death metal band under the name Black Death. In 1991, the band embraced a black metal style influenced by Bathory and Celtic Frost and became one of the leading bands in the Norwegian black metal scene. Their first three black metal albums—"A Blaze in the Northern Sky", "Under a Funeral Moon" and "Transilvanian Hunger" (sometimes dubbed the "Unholy Trinity")—are considered the peak of the band's career and to be among the most influential albums in the genre. For most of this time, Darkthrone has been a duo of Nocturno Culto and Fenriz, who have sought to remain outside the music mainstream. Since 2006, their work has strayed from the traditional black metal style and incorporated more elements of traditional heavy metal, speed metal and punk rock, being likened to Motörhead.
History.
Death metal years: 1986–1990.
The band that would become Darkthrone formed in late 1986 in Kolbotn, a small suburb of Oslo. They were a death metal band by the name of Black Death whose members were Gylve Nagell, Ivar Enger, and Anders Risberget. Main inspirations were Venom, Celtic Frost, Slayer and Cryptic Slaughter. In fall 1987, the band changed their name to Darkthrone and were joined by Dag Nilsen. Ted Skjellum joined in spring of 1988. During 1988 and 1989, the band independently released four demo tapes: "Land of Frost", "A New Dimension", "Thulcandra", and "Cromlech".
They were subsequently signed to the independent record label Peaceville Records with a four-album contract. In 1990, they recorded their first studio album, "Soulside Journey". Because of a small recording budget, the band could not afford the kind of studio they wanted but, thanks to the members of Nihilist and Entombed, they were able to record their album at Sunlight Studios. Although mainly death metal in style, there were some elements of black metal present in terms of artwork and songwriting.
Immediately following the release of this album, the band continued writing and recording new material, recording every new song on tape until it was a full album. These demos were entirely instrumental but they demonstrated the band's gradual shift towards black metal. In 1997, they would be released on the compilation album "Goatlord".
Early black metal years: 1991–1994.
During 1991, Darkthrone adopted the aesthetic style that would come to represent the black metal scene, wearing corpse paint and working under pseudonyms. Gylve Nagell became "Fenriz", Ted Skjellum became "Nocturno Culto" and Ivar Enger became "Zephyrous". In August 1991, they recorded their second album, which was released at the beginning of 1992 and titled "A Blaze in the Northern Sky". The album contained Darkthrone's first black metal recordings, and Peaceville Records was originally skeptical about releasing it due to Darkthrone's extreme diversion from their original death metal style. After the album was recorded, bassist Dag Nilsen left the band, and is merely credited as "session bass" with no picture on the album.
The band's third album, "Under a Funeral Moon", was recorded in the summer of 1992 and released in early 1993. It marked Darkthrone's total conversion to the black metal style, and is considered a landmark for the development of the genre as a whole. This album also marked the last album on which guitarist Zephyrous would perform.
It was followed by their fourth album, "Transilvanian Hunger", which was released in February 1994. This was Darkthrone's first album to have just two members, Nocturno Culto and Fenriz. The band would remain a duo from this point onwards. "Transilvanian Hunger" was characterized by a very "raw" or "low fidelity" recording style and musical simplicity. The album's release caused some controversy: some of its lyrics were written by the infamous Norwegian black metal musician Varg Vikernes, and its booklet contained the phrase "Norsk Arisk Black Metal", which translates into English as "Norwegian Aryan Black Metal".
With Moonfog Records: 1995–2004.
Darkthrone moved to another independent record label, Moonfog Productions, for subsequent releases. The label was run by Satyr of the black metal band Satyricon.
Their fifth album, "Panzerfaust", was released in 1995. The album was received well, although its production, which is similar to that of "Transilvanian Hunger", encountered some criticisms. Their sixth album, "Total Death", was released during 1996 and is notable for featuring lyrics written by four other black metal musicians, and none at all written by the group's main lyricist Fenriz.
During the years 1993–1995, drummer Fenriz was involved with numerous side projects. This included his solo ambient project Neptune Towers, recording an album with Satyr as the duo Storm, and playing bass on Dødheimsgard's debut album. Also he began playing drums for Valhall again, after having been one of the founding members in 1988 but leaving in 1990 to concentrate on Darkthrone.
In 1999, Darkthrone released the album "Ravishing Grimness", and in 2001 their following album, "Plaguewielder". While "Transilvanian Hunger" and "Panzerfaust" had songs written solely by Fenriz, these two albums had songs mostly written by Nocturno Culto and were both recorded in Ronny Le Tekrøe's studio at Toten, Norway. This explains the somewhat "clearer" sound on those records.
In the last years of the 90s, two Darkthrone tribute albums were released: "Darkthrone Holy Darkthrone" in 1998 and "The Next Thousand Years Are Ours" in 1999. The band also released "Preparing for War", a compilation of songs from 1988–1994. In 2002, the intro of their song "Kathaarian Life Code" appeared in the last scene of the film "Demonlover".
In 2003, the band released the album "Hate Them". Although this record and their next contain electronic introductions, they remain true to Darkthrone's early black metal style. "Sardonic Wrath" was released in 2004. It was the band's last album with Moonfog Productions and their last to be recorded solely in the black metal style. This album was nominated for Norway's Alarm Awards; however, the album's entry was withdrawn at the band's request. Their next releases would feature strong crust punk traits.
Change in direction: 2005–present.
In 2005, Darkthrone confirmed that they had returned to Peaceville Records, after leaving the record label in 1994. They had also started up their own record label, Tyrant Syndicate Productions, to release their future albums. To celebrate their return, Peaceville re-issued the "Preparing for War" compilation with a bonus CD of demos and a DVD of live performances. Darkthrone's first four albums were also re-released with video interviews about each of them.
Darkthrone released their eleventh album, "The Cult Is Alive", during early 2006. The album represented a shift in the band's style as the music incorporated crust punk traits. While Darkthrone's black metal roots were still evident, their shift from the genre's typical sound was more noticeable. "The Cult Is Alive" was the first Darkthrone album to appear on the album chart in Norway, debuting at number 22.
In July 2007, the band released the EP "NWOBHM" (an acronym for "New Wave of Black Heavy Metal", a take-off on the original "New Wave of British Heavy Metal") as a preview for their next album. In September that year, Darkthrone released the album "F.O.A.D." (an acronym for "Fuck Off and Die"). The phrase was used by many thrash metal and punk bands during the 1980s. While the music partially continued the punk-oriented style that was introduced on "The Cult Is Alive", this time the band focused more on traditional heavy metal.
Also during 2007, Nocturno Culto completed and released "The Misanthrope", a film about black metal and life in Norway. It includes some of his own solo recordings. In October 2008, "Dark Thrones and Black Flags" was released, using much the same style as the previous album. In 2010, the band released the album "Circle the Wagons", which again mixed crust punk along with speed metal and traditional heavy metal.
In late 2010, Peaceville acquired the rights to the band's Moonfog albums and re-issued "Panzerfaust" as a two-disc set and on vinyl. The re-issue of "Total Death" was set for March 14, 2011. In July 2012, Darkthrone announced a new album, titled "The Underground Resistance"; it was released on February 25, 2013.

</doc>
<doc id="37131" url="http://en.wikipedia.org/wiki?curid=37131" title="Burzum">
Burzum

Burzum (; ]) is the musical project of Norwegian musician and writer Varg Vikernes. Vikernes began making music in 1988, but it was not until 1991 that he recorded his first demos as Burzum. The word "burzum" means "darkness" in the Black Speech, a fictional language crafted by J. R. R. Tolkien. The project became a part of the early Norwegian black metal scene and one of the most influential acts in black metal. Vikernes recorded the first four Burzum albums between January 1992 and March 1993. However, the releases were spread out, with many months between the recording and the release of each album. In May 1994, Vikernes was sentenced to 21 years in prison for the murder of Mayhem guitarist Øystein 'Euronymous' Aarseth and the arson of three churches.
While imprisoned, Vikernes recorded two dark ambient albums using only synthesizers, as he did not have access to drums, guitar or bass. Since his release from prison in 2009, he has recorded three further black metal albums and another ambient/electronic album.
Although Vikernes is known for his political views, he does not use Burzum to promote those views. Burzum has never played live and Vikernes says he has no intention of doing so.
History.
Early years (1988–1992).
Vikernes began making music in 1988 with the band Kalashnikov. The following year, the name was changed to Uruk-Hai, after the creatures from J. R. R. Tolkien's "The Lord of the Rings". In 1990 and 1991, Vikernes played guitar for the death metal band Old Funeral, which also consisted of members who would later form the band Immortal. He appears on the Old Funeral EP "Devoured Carcass". Vikernes left Old Funeral in 1991 to concentrate on creating his own musical visions. He had a short lived project called Satanel, along Abbath Doom Occulta. He then began a solo project under the name Burzum. The word "burzum" means "darkness" in the Black Speech; a language crafted by Tolkien. Soon after recording two demo tapes, he became part of the Norwegian black metal scene. With his demo tapes, he had attracted attention from Øystein "Euronymous" Aarseth of Mayhem, who had just recently formed Deathlike Silence Productions. Aarseth then signed Burzum to the label, and shortly after, Vikernes ―under the pseudonym of Count Grishnackh―, began to record Burzum’s self-titled debut album. According to Vikernes' autobiography on his website, he had intended to record the album in the worst recording quality possible (due to this being a typical trademark of the early Norwegian black metal scene), though still make it sound acceptable. Burzum's eponymous debut album was released in 1992, being the second album released on Deathlike Silence Productions. The song "War" from this album had a guest appearance from Euronymous, playing a guitar solo "just for fun", according to Vikernes.
Vikernes has stated that he had never played any live shows with Burzum, though at one point was interested in it, so Samoth of Emperor joined the band as their bassist, though only appearing on the "Aske" EP. Additionally, Erik Lancelot was hired to be the band's drummer, though did not record on any Burzum material, and along with Samoth did not play a live show. Vikernes had by then lost his interest in playing live concerts, and stated that he "didn't even need session musicians anymore". Therefore, Samoth and Lancelot had parted ways with Burzum. "Det som engang var" was released as Burzum's second album in 1993, recorded in 1992.
Imprisonment (1993–2009).
15 May 1994 saw the release of "Hvis lyset tar oss", a new album of previously recorded material from 1992. Burzum remained as a solo project until 1994, when Vikernes was arrested for the murder of Euronymous and the burnings of several churches in Norway. During his time in prison, Vikernes released his next album, titled "Filosofem", on 1 January 1996. Recorded in March 1993, "Filosofem" was the last recording Vikernes made before his imprisonment. "Burzum / Aske", a compilation comprising the "Burzum" album and "Aske" EP, was released in 1995. While imprisoned, Vikernes managed to record two other albums in a dark ambient style. They were released as "Dauði Baldrs" (1997) and "Hliðskjálf" (1999).
In 1998, all Burzum albums released up to that point were re-released as vinyl picture discs in a special box set called "1992–1997"; however the "Filosofem" album didn't contain "Rundtgåing av den transcendentale egenhetens støtte" due to its length. The regular vinyl issue of "Filosofem" on Misanthropy had tracks 1-4 plus "Decrepitude II" on side 1 and "Rundtgåing av den transcendentale egenhetens støtte" on side 2.
Post-imprisonment (2009–present).
Soon after being released, Vikernes started writing new tracks (nine metal tracks and an ambient intro and outro) for an upcoming Burzum album. According to Vikernes' recounts, several record companies were interested in releasing his first album in eleven years. He stated about the new album, "I want to take my time, and make it the way I want it. It will be metal, and the fans can expect genuine Burzum."
The album was going to be originally titled "Den hvite guden" ("The White God"), but he later decided to change it to "Belus", which was released by Byelobog Productions ("byelobog" is the transliteration of "белобог" in Slavic languages, meaning "white god") on 8 March 2010. It was also announced that a movie would be released in 2010, based on Varg Vikernes' life in the early 1990s. The movie would mainly draw inspiration from the book "Lords of Chaos", with the film being of the same name. Vikernes expressed his contempt towards both the movie and the book upon which it is based.
A second new album of original Burzum material, "Fallen", was released on 7 March 2011, followed by a compilation album, "From the Depths of Darkness", containing re-recordings of tracks from Burzum's self-titled album and "Det som engang var", on 28 November 2011. A third new studio album of original material, titled "Umskiptar", was released in May 2012. "Sôl austan, Mâni vestan" ("East of the Sun, West of the Moon"), Burzum's first electronic album since 1999, was released in May 2013. On April 27, 2013, a song was posted on the official YouTube channel of Vikernes, titled "Back to the Shadows." In a blog post, Vikernes stated that "Back to the Shadows" will be the last metal track released by Burzum.
The album "The Ways of Yore", was released on June 2, 2014.
Musical style.
Burzum's music features characteristics common in black metal, including distorted, tremolo-picked guitar riffs, harsh vocals and the use of double bass blast beat techniques in the drumming. Earlier Burzum albums feature very low production quality, which has improved in albums created after Vikernes' release from prison. Burzum's early music shows heavy Tolkien influence; for example, the name "Count Grishnackh" is taken from an orc character called Grishnákh in Tolkien's works. The choice of the name for the project reflects both this influence and the desire for anonymity: "Burzum" is a word of the Black Speech of Mordor meaning "darkness" (though Vikernes views what Christians consider "darkness" as "light"), and is one of those found on the Ring-inscription of the One Ring (the final part of the Ring inscription being "...agh burzum-ishi krimpatul", or "...and in the 'darkness" bind them").
Musically, Burzum has progressed from black metal to classical-influenced ambient music characterised by minimalist tendencies and dark atmospheres. Vikernes' music is characterised by hypnotic repetition and simple yet profound song structures; this trademark sound has been present on Burzum's black metal and electronic albums alike. Vikernes has described Burzum as a kind of "spell" or recreation of an imaginary world tied in with Pagan history. Each album, he claims, was designed as a kind of "spell" in itself, with each beginning song intending to make the listener more susceptible to "magic", the following songs to inspire a "trance-like state of mind", and the last song to carry the listener into a "world of fantasy" (dreams, for the listener would fall asleep—Burzum was supposed to have been evening music). Vikernes claims the intent to create this fantasy world came from dissatisfaction with the real world. He has stated the "message" of Burzum can be found in the lyrics of the first song of the first album ("Feeble Screams from Forests Unknown").

</doc>
<doc id="37135" url="http://en.wikipedia.org/wiki?curid=37135" title="Barbecue">
Barbecue

Barbecue (also barbeque, BBQ and barby/barbies) is a cooking method and apparatus. While there is a vast degree of variation and overlap in terminology and method surrounding this form of cooking, the generally accepted difference between barbecuing and grilling is in the cooking time and type of heat used. Grilling is generally done quickly over moderate-to-high direct heat with little smoke, while barbecuing is done slowly over low indirect heat and the food is flavored by the smoking process.
The term as a noun can refer to the meat or to the cooking apparatus itself (the "barbecue grill" or simply "barbecue"). The term 'barbecued' is used as an adjective and refers to foods cooked by this method. The term is also used as a verb for the act of cooking food in this manner. Barbecuing is usually done in an outdoor environment by smoking the meat over wood or charcoal. Restaurant barbecue may be cooked in large brick or metal ovens specifically designed for that purpose. Barbecuing has numerous regional variations in many parts of the world.
Etymology.
Etymologists believe that "barbecue" derives from the word "barabicu" found in the language of the Taíno people of Caribbean and the Timucua of Florida, and entered European languages in the form "barbacoa". Specifically, the "Oxford English Dictionary" traces the word back to Haiti, that translates as a "framework of sticks set upon posts". Gonzalo Fernández De Oviedo y Valdés, a Spanish explorer, was the first to use the word "barbecoa" in print in Spain in 1526 in the "Diccionario de la Lengua Española (2nd Edition) of the Real Academia Española." After Columbus landed in the Americas in 1492, the Spaniards had seemed to have found native Haitians roasting animal meat over a grill consisting of a wooden framework resting on sticks and a fire made underneath, that flames and smoke would rise and envelop the animal meat, giving it a certain flavor. Strangely enough, the same framework was used as a means of protection against the wild that may attack during middle of the night while at sleep.
Traditional "barbacoa" involves digging a hole in the ground and placing some meat (usually a whole goat) with a pot underneath it, so that the juices can make a hearty broth. It is then covered with maguey leaves and coal and set alight. The cooking process takes a few hours. Olaudah Equiano, an African abolitionist, described this method of roasting alligators among the "Mosquito People" (Miskito people) on his journeys to Cabo Gracias a Dios in his narrative, "The Interesting Narrative of the Life of Olaudah Equiano".
It has been suggested that both the word and cooking technique migrated out of the Caribbean and into other languages and cultures, with the word (barbacoa) moving from Caribbean dialects into Spanish, then Portuguese, French, and English. The "Oxford English Dictionary" cites the first recorded use of the word in the English language as a verb in 1661, in Edmund Hickeringill's "Jamaca Viewed": "Some are slain, And their flesh forthwith Barbacu'd and eat." The word "barbecue" also appears as a verb in the published writings of John Lederer, following his travels in the American southeast in 1672. The first known use of the word as a noun was in 1697 by the British buccaneer William Dampier. In his "New Voyage Round the World", Dampier writes: "And lay there all night, upon our Borbecu's, or frames of Sticks, raised about 3 ft from the Ground".
Samuel Johnson's 1756 dictionary gave the following definitions:
While the standard modern English spelling of the word is "barbecue," local variations like "barbeque" and truncations such as "bar-b-q" or "bbq" may also be found. The spelling "barbeque" is given in Merriam-Webster and the Oxford English Dictionary as a variant.
In the southeastern United States, the word "barbecue" is used predominantly as a noun referring to roast pork, while in the southwestern states cuts of beef are often cooked.
Styles.
In British usage, "barbecuing" refers to a fast cooking process directly over high heat, while "grilling" refers to cooking under a source of direct, high heat — known in the United States and Canada as "broiling". In American English usage, however, "grilling" refers to a fast process over high heat, while "barbecuing" refers to a slow process using indirect heat or hot smoke (very similar to some forms of roasting). For example, in a typical U.S. home grill, food is cooked on a grate directly over hot charcoal, while in a U.S. barbecue, the coals are dispersed to the sides or at significant distance from the grate. Its South American versions are the southern Brazilian churrasco and the Argentine asado.
Alternatively, an apparatus called a smoker with a separate fire box may be used. Hot smoke is drawn past the meat by convection for very slow cooking. This is essentially how barbecue is cooked in most U.S. barbecue restaurants, but nevertheless, many consider this to be a distinct cooking process called hot smoking.
Barbecuing is a pervasive tradition in much of the world. Almost all competition grillers use charcoal, most often in large, custom designed brick or steel grills. They can range from a few 55 gallon oil drums sawed lengthwise on their sides to make a lid and grill base, to large, vehicle sized grills made of brick, weighing nearly a ton.
U.S. South and Midwest.
In the southern United States, barbecue initially revolved around the cooking of pork. During the 19th century, pigs were a low-maintenance food source as they could be released to forage for themselves in forests and woodlands. When food or meat supplies were low, these semi-wild pigs could then be caught and eaten.
It was the Spanish who first introduced the pig into America and to the Native Americans. The Americans, in turn, introduced the Spanish to the concept of true slow cooking with smoke. The Spanish colonists came to South Carolina in the early 16th century and settled at Mission Santa Elena (now Parris Island). It was in that early American colony that Europeans first learned to prepare and to eat "real" barbecue.
According to estimates, prior to the American Civil War, Southerners ate around five pounds of pork for every one pound of beef they consumed. Because of the effort to capture and cook these wild hogs, pig slaughtering became a time for celebration, and the neighborhood would be invited to share in the largesse. In Cajun culture, these are called boucheries. These feasts are sometimes called 'pig pickin's.' The traditional Southern barbecue grew out of these gatherings.
Each Southern locale has its own particular variety of barbecue, particularly concerning the sauce. North Carolina sauces vary by region; eastern North Carolina uses a vinegar-based sauce, the center of the state enjoys Lexington-style barbecue, which uses a combination of ketchup and vinegar as their base, and western North Carolina uses a heavier ketchup base. Lexington boasts of being "The Barbecue Capital of the World" and it has more than one BBQ restaurant per 1,000 residents. South Carolina is the only state that traditionally includes all four recognized barbecue sauces, including mustard-based, vinegar-based, and light and heavy tomato-based. Memphis barbecue is best known for tomato- and vinegar-based sauces. In some Memphis establishments and in Kentucky, meat is rubbed with dry seasoning (dry rubs) and smoked over hickory wood without sauce. The finished barbecue is then served with barbecue sauce on the side.
The barbecue of Alabama, Georgia, and Tennessee is almost always pork served with a sweet tomato-based sauce. However, several regional variations exist as well. Alabama is particularly known for its distinctive white sauce, a mayonnaise- and vinegar-based sauce, originating in northern Alabama, used predominantly on chicken and pork. A popular item in North Carolina and Memphis is the pulled pork sandwich served on a bun and often topped with coleslaw. Pulled pork is prepared by shredding the pork after it has been barbecued.
Kansas City-style barbecue is characterized by its use of different types of meat (including pulled pork, pork ribs, burnt ends, smoked sausage, beef brisket, beef ribs, smoked/grilled chicken, smoked turkey, and sometimes fish), a variety attributable to Kansas City's history as a center for meat packing in the United States. Hickory is the primary wood used for smoking in KC, while the sauces are typically tomato based with sweet, spicy, and tangy flavor profiles. Burnt ends, the flavorful pieces of meat cut from the ends of a smoked beef or pork brisket, are popular in many Kansas City-area barbecue restaurants.
Pit-beef prevails in Maryland and is often enjoyed at large outdoor "bull roasts", which are common for club or association fundraising events. Maryland-style pit-beef is not the product of barbecue cookery in the strictest sense, as there is no smoking of the meat involved; rather, it involves grilling the meat over a high heat. The meat is typically served rare, with a strong horseradish sauce as the preferred condiment.
The state of Kentucky, particularly Western Kentucky, is unusual in its barbecue cooking, in that the preferred meat is mutton. This kind of mutton barbecue is often used in communal events in Kentucky, such as political rallies, county fairs and church fund-raising events.
In the midwest, Chicago-style is popular and involves seasoning the meat with a dry rub, searing over a hot grill and a long slow cook in an oven. The meat, typically ribs, is then finished with a sweet-tangy sauce.
Events and gatherings.
The word "barbecue" is also used to refer to a social gathering where food is served, usually outdoors in the late afternoon or evening. In the southern United States, outdoor gatherings are not typically called "barbecues" unless barbecue itself will actually be on the menu, instead generally favoring the word "cookouts". The device used for cooking at a barbecue is commonly referred to as a "barbecue", "barbecue grill", or "grill". In North Carolina, however, "barbecue" is a noun primarily referring to the food and never used by native North Carolinians to describe the act of cooking or the device on which the meat is cooked.
Other barbecue competitions are held in virtually every state in the United States during the warmer months, usually beginning in April and going through September. These events feature keen competitions between teams of cooks and are divided into separate competitions for the best pork, beef and poultry barbecue and for the best barbecue sauces.
Techniques.
Barbecuing encompasses four or five distinct types of cooking techniques. The original technique is cooking using smoke at lower temperatures (usually around 240–280 °F or 115–145 °C) and significantly longer cooking times (several hours), known as "smoking". Another technique is "baking", utilizing a masonry oven or any other type of baking oven, which uses convection to cook meats and starches with moderate temperatures for an average cooking time (about an hour plus a few extra minutes). Yet another technique is "braising", which combines direct dry heat charbroiling on a ribbed surface with a broth-filled pot for moist heat, cooking at various speeds throughout the duration (starting fast, slowing down, then speeding up again, lasting for a few hours). 
By contrast, grilling is done over direct dry heat, usually over a hot fire (i.e., over 500 °F) for a short time (minutes). Grilling may be done over wood, charcoal, gas (natural gas or propane), or electricity. The time difference between barbecuing and grilling is due to the temperature difference: at the low temperatures used for barbecuing, it takes many hours for the meat to reach the desired internal temperature. 
Smoking.
Smoking is the process of flavoring, cooking, or preserving food by exposing it to smoke from burning or smoldering material, most often wood. Meats and fish are the most common smoked foods, though cheeses, vegetables, nuts and ingredients used to make beverages such as beer, or smoked beer are also smoked.
Roasting.
The masonry oven is similar to a smoke pit in that it allows for an open flame, but cooks much faster, and uses convection to cook. Barbecue-baking can also be done in traditional stove-ovens. It can be used to cook not only meats, but breads and other starches, and even various casseroles and desserts. It uses both direct and indirect heat to surround the food with hot air to cook, and can be basted much the same as grilled foods.
Braising.
It is possible to braise meats and vegetables in a pot on top of a grill. A gas or electric charbroil grill would be the best choices for what is known as "barbecue-braising", or combining dry heat charbroil-grilling directly on a ribbed surface and braising in a broth-filled pot for moist heat. To braise, put a pot on top of the grill, cover it, and let it simmer for a few hours. There are two advantages to barbecue-braising: the first is that this method now allows for browning the meat directly on the grill before the braising, and the second is that it also allows for glazing the meat with sauce and finishing it directly over the fire after the braising, effectively cooking the meat three times, which results in a soft textured product that falls right off the bone. This method of barbecue has a varying duration (depending on whether a slow cooker or pressure cooker is used), and is generally slower than regular grilling or baking, but faster than pit-smoking.
Other uses.
The term "barbecue" is also used to designate a flavor added to foodstuffs, the most prominent of which are potato chips.

</doc>
<doc id="37137" url="http://en.wikipedia.org/wiki?curid=37137" title="Arabidopsis">
Arabidopsis

Arabidopsis (rockcress) is a genus in the family Brassicaceae. They are small flowering plants related to cabbage and mustard. This genus is of great interest since it contains thale cress ("Arabidopsis thaliana"), one of the model organisms used for studying plant biology and the first plant to have its entire genome sequenced. Changes in thale cress are easily observed, making it a very useful model.
Status.
Currently, the genus "Arabidopsis" has nine species and a further eight subspecies recognised. This delimitation is quite recent and is based on morphological and molecular phylogenies by O'Kane and Al-Shehbaz (1997, 2003) and others.
Their findings confirm the species formerly included in "Arabidopsis" made it polyphyletic. The most recent reclassification moves two species previously placed in "Cardaminopsis" and "Hylandra" and three species of "Arabis" into "Arabidopsis", but excludes 50 that have been moved into the new genera "Beringia, Crucihimalaya, Ianhedgea, Olimarabidopsis", and "Pseudoarabidopsis".
All of the species in "Arabidopsis" are indigenous to Europe, while two of the species have broad ranges also extending into North America and Asia.
In the last two decades, "Arabidopsis thaliana" has gained much interest from the scientific community as a model organism for research on numerous aspects of plant biology. (TAIR) is a curated online information source for "Arabidopsis thaliana" genetic and molecular biology research, and is an online compilation of invited chapters on "Arabidopsis thaliana" biology. In Europe, the model organism resource centre for "Arabidopsis thaliana" germplasm, bioinformatics and molecular biology resources (including GeneChips) is the Nottingham Arabidopsis Stock Centre – NASC whilst in North America germplasm services are provided by the Arabidopsis Biological Resource Center, (ABRC) based at the Ohio State University. The ordering system for ABRC was incorporated into (TAIR) database in June 2001 whilst NASC has always (since 1991) hosted its own ordering system and genome browser.
"A. thaliana" in partial "in vitro" conditions.
Recently, "A. thaliana" tissues have been cultivated in microfluidic devices. Plant-on-chip devices show promise for future research in understanding the mechanism of sexual reproduction in "A. thaliana".
Cytogenetics.
Cytogenetic analysis has shown the haploid chromosome number (n) is variable and can be 5, 8 and 13.
"A. thaliana" is n=5 and the DNA sequencing of this species was completed in 2001.
"A. suecica" is n=13 (5+8) and is an amphidiploid species originated through hybridization between "A. thaliana" and diploid "A. arenosa". 
"A. neglecta" is n=8, as are the various subspecies of "A. halleri".
Various subspecies of "A. lyrata" and "A. arenosa" can be either 2n (diploid) or 4n (tetraploid). 
As of 2005, "A. cebennensis", "A. croatica" and "A. pedemontana" have not been investigated cytologically.
Reclassified species.
The following species previously placed in "Arabidopsis" are not currently considered part of the genus.

</doc>
<doc id="37138" url="http://en.wikipedia.org/wiki?curid=37138" title="Arabidopsis thaliana">
Arabidopsis thaliana

Arabidopsis thaliana ( thale cress, mouse-ear cress or arabidopsis) is a small flowering plant native to Eurasia. A. thaliana is edible by humans and, as with other mustard greens, is used in salads or sautéed, like many species in the Brassicacea. Considered a weed, it is found by roadsides and in disturbed lands. A winter annual with a relatively short life cycle, "Arabidopsis" is a popular model organism in plant biology and genetics. For a complex multicellular eukaryote, "Arabidopsis thaliana" has a relatively small genome of approximately 135 megabase pairs (Mbp). It was long thought to have the smallest genome of all flowering plants, but the smallest flowering plants' genomes are now considered to belong to plants in the genus "Genlisea", order Lamiales, with "Genlisea tuberosa", a carnivorous plant, showing a genome size of approximately 61 Mbp. "Arabidopsis thaliana" was the first plant to have its genome sequenced, and is a popular tool for understanding the molecular biology of many plant traits, including flower development and light sensing.
Discovery and name origin.
The plant was first described in 1577 in the Harz Mountains by (1542–1583), a physician from Nordhausen, Thüringen, Germany, who called it "Pilosella siliquosa". In 1753, Carl Linnaeus renamed the plant "Arabis thaliana" in honor of Thal. In 1842, the German botanist Gustav Heynhold erected the new genus "Arabidopsis" and placed the plant in that genus. The genus name, "Arabidopsis", comes from Greek, meaning "resembling "Arabis"" (the genus in which Linnaeus had initially placed it).
Habitat, morphology, and life cycle.
"Arabidopsis" is native to Europe, Asia, and northwestern Africa. It also appears to be native in tropical afroalpine ecosystems. It is an annual (rarely biennial) plant, usually growing to 20–25 cm tall. The leaves form a rosette at the base of the plant, with a few leaves also on the flowering stem. The basal leaves are green to slightly purplish in color, 1.5–5 cm long and 2–10 mm broad, with an entire to coarsely serrated margin; the stem leaves are smaller and unstalked, usually with an entire margin. Leaves are covered with small, unicellular hairs (called trichomes). The flowers are 3 mm in diameter, arranged in a corymb; their structure is that of the typical Brassicaceae. The fruit is a siliqua 5–20 mm long, containing 20–30 seeds. Roots are simple in structure, with a single primary root that grows vertically downward, later producing smaller lateral roots. These roots form interactions with rhizosphere bacteria such as "Bacillus megaterium".
"Arabidopsis" can complete its entire lifecycle in six weeks. The central stem that produces flowers grows after about three weeks, and the flowers naturally self-pollinate. In the lab, "Arabidopsis" may be grown in Petri plates, pots, or hydroponics, under fluorescent lights or in a greenhouse.
Distribution.
Australia: common garden weed in Katoomba, NSW. Ireland: Said to be occasional in Ireland, however other references note it as recorded from throughout Ireland. It is frequently found in Belfast.
Use as a model organism.
Botanists and biologists began to research "A. thaliana" in the early 1900s, and the first systematic collection of its mutations was performed around 1945. It is now widely used for studying plant sciences, including genetics, evolution, population genetics, and plant development. It plays the role in plant biology that mice and fruit flies ("Drosophila") play in animal biology. Although "A. thaliana" has little direct significance for agriculture, it has several traits that make it a useful model for understanding the genetic, cellular, and molecular biology of flowering plants.
The small size of its genome, and the fact that it is diploid, makes "Arabidopsis thaliana" useful for genetic mapping and sequencing — with about 157 mega base pairs and five chromosomes, "Arabidopsis" has one of the smallest genomes among plants. It was the first plant genome to be sequenced, completed in 2000 by the Arabidopsis Genome Initiative. The most up-to-date version of the "A. thaliana" genome is maintained by the Arabidopsis Information Resource (TAIR). Much work has been done to assign functions to its 27,000 genes and the 35,000 proteins they encode. Post-genomic research, such as metabolomics, has also provided useful insights to the metabolism of this species and how environmental perturbations can affect metabolic processes.
The plant's small size and rapid lifecycle are also advantageous for research. Having specialized as a spring ephemeral, it has been used to found several laboratory strains that take about six weeks from germination to mature seed. The small size of the plant is convenient for cultivation in a small space, and it produces many seeds. Further, the selfing nature of this plant assists genetic experiments. Also, as an individual plant can produce several thousand seeds; each of the above criteria leads to "A. thaliana" being valued as a genetic model organism.
Plant transformation in "Arabidopsis "is routine, using "Agrobacterium tumefaciens" to transfer DNA to the plant genome. The current protocol, termed "floral-dip", involves simply dipping a flower into a solution containing "Agrobacterium", the DNA of interest, and a detergent. This method avoids the need for tissue culture or plant regeneration.
The "Arabidopsis" gene knockout collections are a unique resource for plant biology made possible by the availability of high-throughput transformation and funding for genomics resources. The site of T-DNA insertions has been determined for over 300,000 independent transgenic lines, with the information and seeds accessible through online . Through these collections, insertional mutants are available for most genes in "Arabidopsis".
The plant is well suited for light microscopy analysis. Young seedlings on the whole, and their roots in particular, are relatively translucent. This, together with their small size, facilitates live cell imaging using both fluorescence and confocal laser scanning microscopy. By wet-mounting seedlings in water or in culture media, plants may be imaged uninvasively, obviating the need for fixation and sectioning and allowing time-lapse measurements. Fluorescent protein constructs can be introduced through transformation. The developmental stage of each cell can be inferred from its location in the plant or by using fluorescent protein markers, allowing detailed developmental analysis.
 and are curated sources for diverse "Arabidopsis" genetic and molecular biology information, and also provide numerous links, for example, to that store the results of hundreds of genome-wide gene expression profile experiments. Seed and DNA stocks can be obtained from the Nottingham Arabidopsis Stock Centre or the .
History of research.
The first mutant in "Arabidopsis" was documented in 1873 by Alexander Braun, describing a double flower phenotype (the mutated gene was likely "Agamous", cloned and characterized in 1990). However, not until 1943 did Friedrich Laibach (who had published the chromosome number in 1907) propose arabidopsis as a model organism. His student, Erna Reinholz, published her thesis on arabidopsis in 1945, describing the first collection of arabidopsis mutants that they generated using X-ray mutagenesis. Laibach continued his important contributions to arabidopsis research by collecting a large number of ecotypes. With the help of Albert Kranz, these were organised into the current ecotype collection of 750 natural accessions of "A. thaliana" from around the world.
In the 1950s and 1960s, John Langridge and George Rédei played an important role in establishing arabidopsis as a useful organism for biological laboratory experiments. Rédei wrote several scholarly reviews instrumental in introducing the model to the scientific community. The start of the arabidopsis research community dates to a newsletter called Arabidopsis Information Service (AIS), established in 1964. The first International Arabidopsis Conference was held in 1965, in Göttingen, Germany.
In the 1980s, arabidopsis started to become widely used in plant research laboratories around the world. It was one of several candidates that included maize, petunia, and tobacco. The latter two were attractive, since they were easily transformable with the then-current technologies, while maize was a well-established genetic model for plant biology. The breakthrough year for arabidopsis as the preferred model plant came in 1986, when T-DNA-mediated transformation was first published, and this coincided with the first gene to be cloned and published in Arabidopsis.
Characterized ecotypes and mutant lines of arabidopsis serve as experimental material in laboratory studies. The most commonly used background lines are L"er", or Landsberg erecta, and Col, or Columbia. Other background lines less-often cited in the scientific literature are Ws, or Wassilewskija, C24, Cvi, or Cape Verde Islands, Nossen, etc. (see for ex.) Series of mutants, named L"er"-x, Col-x, have been obtained and characterized; in general, mutant lines are available through stock centers, of which best-known are the Nottingham Arabidopsis Stock Center-NASC and the Arabidopsis Biological Resource Center-ABRC in Ohio, USA.
The Col or Columbia ecotype was selected, as an agronomically performant line, by Rédei, within a (nonirradiated) population of seeds named Landsberg he received from Laibach. Columbia, named for the location of Rédei's former institution the University of Missouri-Columbia, is the ecotype sequenced in the Arabidopsis Genome Initiative. The L"er" or Landsberg erecta line was selected by Rédei from within a Landsberg population on which he had performed some X-ray mutagenesis experiments. As the L"er" collection of mutants is derived from this initial line, L"er"-0 does not correspond to the Landsberg ecotype, which is named La-0.
Research.
Flower development.
"Arabidopsis "has been extensively studied as a model for flower development. The developing flower has four basic organs: sepals, petals, stamens, and carpels (which go on to form pistils). These organs are arranged in a series of whorls: four sepals on the outer whorl, followed by four petals inside this, six stamens, and a central carpel region. Homeotic mutations in arabidopsis result in the change of one organ to another — in the case of the "Agamous" mutation, for example, stamens become petals and carpels are replaced with a new flower, resulting in a recursively repeated sepal-petal-petal pattern.
Observations of homeotic mutations led to the formulation of the ABC model of flower development by E. Coen and E. Meyerowitz. According to this model, floral organ identity genes are divided into three classes: class A genes (which affect sepals and petals), class B genes (which affect petals and stamens), and class C genes (which affect stamens and carpels). These genes code for transcription factors that combine to cause tissue specification in their respective regions during development. Although developed through study of arabidopsis flowers, this model is generally applicable to other flowering plants.
Light sensing.
The photoreceptors phytochromes A, B, C, D, and E mediate red light-based phototropic response. Understanding the function of these receptors has helped plant biologists understand the signalling cascades that regulate photoperiodism, germination, de-etiolation, and shade avoidance in plants.
The UVR8 protein detects UV-B light and mediates response to this DNA damaging wavelength.
Arabidopsis was used extensively in the study of the genetic basis of phototropism, chloroplast alignment, and stomatal aperture and other blue light-influenced processes. These traits respond to blue light, which is perceived by the phototropin light receptors. Arabidopsis has also been important in understanding the functions of another blue light receptor, cryptochrome, which is especially important for light entrainment to control the plants' circadian rhythms.
Light response was even found in roots, which were thought not to be particularly sensitive to light. While gravitropic response of arabidopsis root organs is their predominant tropic response, specimens treated with mutagens and selected for the absence of gravitropic action showed negative phototropic response to blue or white light, and positive response to red light, indicating that the roots also show positive phototropism.
Light emitting.
In 2000, Dr. Janet Braam of Rice University genetically engineered "Arabidopsis" to glow in the dark when touched. The effect was visible to ultrasensitive cameras.
In 2013, a crowd funding project on Kickstarter called the Glowing Plant project offered to deliver seeds of genetically engineered "glow in the dark" Arabidopsis to its backers. The plants are expected to give off a dim glow.
Non-Mendelian inheritance.
In 2005, scientists at Purdue University proposed that "Arabidopsis "possessed an alternative to previously known mechanisms of DNA repair, which one scientist called a "parallel path of inheritance". It was observed in mutations of the "HOTHEAD" gene. Plants mutant in this gene exhibit organ fusion, and pollen can germinate on all plant surfaces, not just the stigma. After spending over a year eliminating simpler explanations, it was indicated that the plants "cached" versions of their ancestors' genes going back at least four generations, and used these records as templates to correct the "HOTHEAD" mutation and other single nucleotide polymorphisms. The initial hypothesis proposed the record may be RNA-based Since then, alternative models have been proposed which would explain the phenotype without requiring a new model of inheritance. More recently, the whole phenomenon is being challenged as a being a simple artifact of pollen contamination. "When Jacobsen took great pains to isolate the plants, he couldn't reproduce the [reversion] phenomenon", notes Steven Henikoff. In response to the new finding, Lolle and Pruitt agree that Peng et al. did observe cross-pollination, but note that some of their own data, such as double reversions of both mutant genes to the regular form, cannot be explained by cross-pollination.
Plant–pathogen interactions.
It is important to understand how plants achieve resistance to protect the world's food production, as well as the agriculture industry. Many model systems have been developed to better understand interactions between plants and bacterial, fungal, oomycete, viral, and nematode pathogens. "Arabidopsis thaliana" has been successfully implemented in the study of the subdicipline of plant pathology, that is, the interaction between plants and disease-causing pathogens.
The use of arabidopsis has led to many breakthroughs in the advancement of knowledge of how plants manifest plant disease resistance. The reason most plants are resistant to most pathogens is through nonhost resistance. This is, not all pathogens will infect all plants. An example where arabidopsis was used to determine the genes responsible for nonhost resistance is "Blumeria graminis", the causal agent of powdery mildew of grasses. Arabidopsis mutants were developed using the mutagen ethyl methanesulfonate and screened to determine which mutants had increased infection by "B. graminis". The mutants with higher infection rates are referred to as PEN mutants due to the ability of "B. graminis" to penetrate arabidopsis to begin the disease process. The PEN genes were later mapped to identify the genes responsible for nonhost resistance to "B. graminis".
 In general, when a plant is exposed to a pathogen, or nonpathogenic microbe, there is an initial response, known as PAMP-triggered immunity (PTI), because the plant detects conserved motifs known as Pathogen-associated molecular patterns (PAMPs). These PAMPs are detected by specialized receptors in the host known as pattern recognition receptors (PRRs) on the plant cell surface.
The best-characterized PRR in "A. thaliana" is FLS2 (Flagellin-Sensing2), which recognizes bacterial flagellin, a specialized organelle used by microorganisms for the purpose of motility, as well as the ligand flg22, which comprises the 22 amino acids recognized by FLS2. Discovery of FLS2 was facilitated by the identification of an "A. thaliana" ecotype, Ws-0, that was unable to detect flg22, leading to the identification of the gene encoding FLS2.
A second PRR, EF-Tu receptor (EFR), identified in "A. thaliana", recognizes the bacterial EF-Tu protein, the prokaryotic elongation factor used in protein synthesis, as well as the laboratory-used ligand elf18. Using "Agrobacterium"-mediated transformation, a technique that takes advantage of the natural process by which "Agrobacterium" transfers genes into host plants, the EFR gene was transformed into "Nicotiana benthamiana", tobacco plant that does not recognize EF-Tu, thereby permitting recognition of bacterial EF-Tu thereby confirming EFR as the receptor of EF-Tu.
Both FLS2 and EFR use similar signal transduction pathways to initiate PTI. "A. thaliana" has been instrumental in dissecting these pathways to better understand the regulation of immune responses, the most notable one being the mitogen-activated protein kinase (MAP kinase) cascade. Downstream responses of PTI include callose deposition, the oxidative burst, and transcription of defense-related genes.
PTI is able to combat pathogens in a nonspecific manner. A stronger and more specific response in plants is that of effector-triggered immunity (ETI). ETI is dependent upon the recognition of pathogen effectors, proteins secreted by the pathogen that alter functions in the host, by plant resistance genes (R-genes), often described as a gene-for-gene relationship. This recognition may occur directly or indirectly via a guardee protein in a hypothesis known as the guard hypothesis. The first R-gene cloned in "A. thaliana" was RPS2 (resistance to "Pseudomonas syringe" 2), which is responsible for recognition of the effector avrRpt2. The bacterial effector avrRpt2 is delivered into "A. thaliana" via the Type III secretion system of "P. syringae pv tomato" strain DC3000. Recognition of avrRpt2 by RPS2 occurs via the guardee protein RIN4, which is cleaved . Recognition of a pathogen effector leads to a dramatic immune response known as the hypersensitive response, in which the infected plant cells undergo cell death to prevent the spread of the pathogen.
Systemic acquired resistance (SAR) is another example of resistance that is better understood in plants because of research done in "A. thaliana". Benzothiadiazol (BTH), a salicylic acid (SA) analog, has been used historically as an antifungal compound in crop plants. BTH, as well as SA, has been shown to induce SAR in plants. The initiation of the SAR pathway was first demonstrated in "A. thaliana" in which increased SA levels are recognized by nonexpresser of PR genes 1 (NPR1) due to redox change in the cytosol, resulting in the reduction of NPR1. NPR1, which usually exists in a multiplex (oligomeric) state, becomes monomeric (a single unit) upon reduction. When NPR1 becomes monomeric, it translocates to the nucleus, were it interacts with many TGA transcription factors, and is able to induce pathogen-related genes such as PR1.
Meiosis.
RAD51-like proteins have been identified in eukaryotes from yeast to vertebrates. These RAD51-like proteins catalyze key steps in recombinational repair of DNA damages. Recombinational repair is particularly important for removing double-strand damages during meiosis. In "A. thaliana" a mutant defective in a gene "rad51" homologue, "xrcc3", is hypersensitive to mitomycin C, a DNA interstrand crosslinking agent implying a deficiency in repair of these double-strand damages in somatic cells. The "xrcc3" mutants are also deficient in meiotic recombination and are sterile, indicating that "xrcc3" also plays an essential role in meiosis. Thus it is likely that in "A. thaliana xrcc3"-mediated recombinational repair of DNA damage is active in somatic cells and essential during meiosis.
Self-pollination.
"A. thaliana" is a predominantly self-pollinating plant with an outcrossing rate estimated at less than 0.3%. An analysis of the genome-wide pattern of linkage disequilibrium suggested that self-pollination evolved roughly a million years ago or more. Meioses that lead to self-pollination are unlikely to produce significant beneficial genetic variability. However, these meioses can provide the adaptive benefit of recombinational repair of DNA damages during formation of germ cells at each generation. Such a benefit may have been sufficient to allow the long-term persistence of meioses even when followed by self-fertilization. A physical mechanism for self-pollination in Arabidopsis is through pre-anthesis autogamy, such that fertilisation takes place largely before flower opening.
Multigenerational.
Ongoing research on "Arabidopsis thaliana" is being performed on the International Space Station by the European Space Agency. The goals are to study the growth and reproduction of plants from seed to seed in microgravity.
"Arabidopsis thaliana" in a microfluidic device.
Plant on a chip is a device in which Arabidopsis thaliana tissues could be cultured in semi in vitro conditions. Plant-on-chip devices are expected to play greater role in understanding pollen tube guidance and the mechanism of sexual reproduction in Arabidopsis thaliana.

</doc>
<doc id="37139" url="http://en.wikipedia.org/wiki?curid=37139" title="Long Island Rail Road">
Long Island Rail Road

The Long Island Rail Road (reporting mark LI), legally known as the Long Island Rail Road Company and often abbreviated as the LIRR, is a commuter rail system in southeastern New York, stretching from Manhattan to the eastern tip of Suffolk County on Long Island. With over 334,000 daily passengers, it is the busiest commuter railroad in North America. It is also one of the few commuter systems in the world that runs 24 hours a day, 7 days a week, year-round. It is publicly owned by the Metropolitan Transportation Authority, as MTA Long Island Rail Road. The current LIRR logo combines the circular MTA logo with the text "Long Island Rail Road", and appears on the sides of trains. The LIRR is one of two commuter rail systems owned by the MTA, the other being Metro-North Railroad. Established in 1834 and having operated continuously since then, it is the oldest U.S. railroad still operating under its original name and charter.
There are 124 stations, and more than 700 mi of track, on its two lines to the two forks of the island and eight major branches, with the passenger railroad system totaling 319 mi of route.
The LIRR is the only commuter passenger railroad in the United States to operate 24/7 with significant off peak, weekend, and holiday service.
History.
The Long Island Rail Road Company was chartered in 1834 to provide a daily service between New York and Boston via a ferry connection between its Greenport, New York, terminal on Long Island's North Fork and Stonington, Connecticut. This service was superseded in 1849 by the land route through Connecticut that became part of the New York, New Haven and Hartford Railroad. The LIRR refocused its attentions towards serving Long Island, in competition with other railroads on the island. In the 1870s railroad president Conrad Poppenhusen and his successor Austin Corbin acquired all the railroads and consolidated them into the LIRR.
The LIRR was unprofitable for much of its history. In 1900, the Pennsylvania Railroad (PRR) bought a controlling interest as part of its plan for direct access to Manhattan which began on September 8, 1910. The wealthy PRR subsidized the LIRR during the first half of the new century, allowing expansion and modernization.
After the Second World War the downturn in the railroad industry and dwindling profits caused the PRR to stop subsidizing the LIRR, and the LIRR went into receivership in 1949. The State of New York, realizing how important the railroad was to the future of Long Island, began to subsidize the railroad in the 1950s and 1960s. In 1966, New York State bought the railroad's controlling stock from the PRR and put it under the newly formed Metropolitan Commuter Transportation Authority (renamed Metropolitan Transportation Authority in 1968). With MTA subsidies the LIRR modernized further, continuing to be the busiest commuter railroad in the United States.
The LIRR is one of the few railroads that has survived as an intact company from its original charter to the present day.
Major stations.
The LIRR operates out of three western terminals, in Manhattan, Brooklyn, and Queens. Jamaica Station in central Queens is the hub of all railroad activities. Expansion of the system into Grand Central Terminal is anticipated over the next few years. Major stations include:
Passenger lines and services.
The Long Island Rail Road system is made up of eleven passenger branches. Three main trunk lines, the Main Line, Montauk Branch, and Atlantic Branch, spin off eight smaller branches. For scheduling and advertising purposes some of these branches are further divided into sections such as the case with the Montauk Branch, which is known as the Babylon Branch service in the electrified portion of the line between Jamaica and Babylon, while the diesel service beyond Babylon to Montauk is referred to as the Montauk Branch service. All branches except the Port Washington Branch pass through Jamaica; the trackage west of Jamaica (except to Port Washington) is known as the City Terminal Zone. The City Terminal Zone includes portions of the Main Line and Atlantic and Montauk Branches as well as the Amtrak-owned East River Tunnels to Penn Station. The passenger lines are:
Former branches.
The railroad has dropped a number of branches due to lack of ridership over the years. Part of the Rockaway Beach Branch became part of the IND Rockaway Line of the New York City Subway, while others were downgraded to freight branches, and the rest abandoned entirely.
Additional services.
In addition to its daily commuter patronage, the LIRR also offers the following services:
Fare structure.
Like Metro-North Railroad and New Jersey Transit, the Long Island Rail Road fare system is based on the distance a passenger travels, as opposed to the New York City Subway, which has a flat rate throughout the system. The railroad is broken up into eight numbered fare zones. Zone 1 includes all of the City Terminal Zone. Zone 3 includes Jamaica and all stations east of Jamaica within the boundaries of New York City, except Far Rockaway and Belmont Park. Zones 4 and 7 include all the stations in Nassau County and Far Rockaway. Zones 9, 10, 12, and 14 includes all the stations in Suffolk County. Each zone contains many stations, and the same fare applies for travel between any station in the origin zone and any station in the destination zone.
Peak fares are charged during the week on trains that arrive at western terminals between 6 AM and 10 AM, and for trains that depart from western terminals between 4 PM and 8 PM. Any passenger holding an off peak ticket on a peak train is required to pay a step up fee. Passengers can buy tickets from ticket agents or ticket vending machines (TVMs) or on the train from conductors, but will incur an on-board penalty fee for doing so. This fee is waived for customers boarding at a station without a ticket office or ticket machine, senior citizens, people with disabilities or Medicare customers.
There are several types of tickets: one way, round trip, peak, off-peak, AM peak or off-peak senior/citizen disabled, peak child, and off-peak child. On off-peak trains, passengers can buy a family ticket for children who are accompanied by an 18 year old for $0.75 if bought from the station agent or TVM, $1.00 on the train. Senior citizen/disabled passengers traveling during the morning peak hours are required to pay the AM peak senior citizen/disabled rate. This rate is not charged during PM peak hours.
Commuters can also buy a peak or off-peak ten trip ride, a weekly unlimited or an unlimited monthly pass. Monthly passes are good on any train regardless of the time of day, within the fare zones specified on the pass.
On weekends, the railroad offers a special reduced-fare CityTicket, introduced in 2004, for passengers who travel within Zones 1 and 3 (i.e. within New York City). CityTickets can only be bought from ticket agents or machines and used on the day of purchase. They are not valid for travel to Far Rockaway because it is in Zone 4 and the Far Rockaway Branch passes through Nassau County. It is also not valid for travel to the Belmont Park station, which is only open for special events. All passengers going to Belmont Park must buy a special ticket to go from Jamaica to Belmont Park (or vice versa), as weekly and monthly passes are not accepted at Belmont Park.
During the summer the railroad offers special summer package ticket deals to places such as Long Beach, Jones Beach, the Hamptons, Montauk, and Greenport. Passengers traveling to the Hamptons and Montauk on the "Cannonball" can reserve a seat in the all-reserved Parlor Cars.
Train operations.
The LIRR is relatively isolated from the rest of the national rail system. It connects with other railroads in just two locations:
All movements on the LIRR are under the control of the Movement Bureau in Jamaica, which gives orders to the towers that control a specific portion of the railroad. Movements in Amtrak territory are controlled by Penn Station Control Center or PSCC, run jointly by the LIRR and Amtrak. The PSCC controls as far east as Harold interlocking, in the Sunnyside area of Queens. The PSCC replaced several towers. The Jamaica Control Center (new in the third quarter of 2010) controls from there east through the Jamaica terminal by direct control of interlockings. This replaced several towers in Jamaica including Jay and Hall towers at the west and east ends of Jamaica station respectively. East of there, lineside towers control the various switches and signals under the direction of the dispatchers in Jamaica.
Nearly all the lines and all passenger rolling stock is equipped for cab signalling, which displays the block signal governing movement of trains in the cab. All passenger rolling stock is equipped with Automatic Speed Control (ASC), which enforces the speed limit dictated by the cab signal if the engineer fails to comply with it, by a penalty brake application. This feature greatly enhances safety.
On many of the lines, there are no intermediate wayside signals between the interlockings: operation is solely by cab signal. Wayside signals remain at interlockings.
Power transmission.
The LIRR's electrified lines are powered by 750 V DC third rail with the contact shoe running along the top of the rail, similar to the New York City Subway and PATH trains.
Equipment.
The LIRR's electric fleet consists of 836 M7 and 170 M3 electric multiple unit cars in married pairs, meaning each car needs the other one to operate, with each car containing its own engineer's cab. The trainsets typically range up to 12 cars long. In September 2013, MTA announced that the LIRR would procure new M9 railcars from Kawasaki starting in 2016. They will replace the M3s, and expand the railroad's electric fleet.
The LIRR also uses 134 C3 Multilevel coachess powered by 23 DE30AC diesel-electric locomotives and 21 DM30AC dual-mode locomotives. They are used mostly on non-electrified territories, including the Port Jefferson, Oyster Bay, Montauk, and Greenport Branches.
Named trains.
For most of its history LIRR has served commuters, but it had many named trains, some with all-first class seating, parlor cars, and full bar service. Few of them lasted past World War II, but some names were revived during the 1950s and 1960s as the railroad expanded its east end parlor car service with luxury coaches and Pullman cars from railroads that were discontinuing their passenger trains.
Freight service.
The LIRR and other railroads that became part of the system have always had freight service, though this has diminished. The process of shedding freight service accelerated with the acquisition of the railroad by New York State.
In recent years there has been some appreciation of the need for better railroad freight service in New York City and on Long Island. Both areas are primarily served by trucking for freight haulage, an irony in a region with the most extensive rail transit service in the Americas as well as the worst traffic conditions. Proposals for a Cross-Harbor Rail Tunnel for freight have languished more than a century.
In May 1997, freight service was franchised on a 20-year term to the New York and Atlantic Railway (NYAR), a short line railroad owned by the Anacostia and Pacific Company. It has its own equipment and crews, but uses the rail facilities of the LIRR. To the east, freight service operates to the end of the West Hempstead Branch, to Huntington on the Port Jefferson Branch, to Bridgehampton on the Montauk Branch, and to Riverhead on the Main Line. On the western end it provides service on the surviving freight-only tracks of the LIRR: the Bay Ridge and Bushwick branches; the "Lower Montauk" between Jamaica and Long Island City; and to an interchange connection at Fresh Pond Junction in Queens with the CSX, Canadian Pacific, and Providence and Worcester railroads.
Freight branches.
Some non-electrified lines are used only for freight:
Passenger issues.
The LIRR has a long history of rocky relations with its passengers, especially daily commuters. Various commuter advocacy groups have been formed to try to represent those interests, in addition to the state mandated LIRR Commuters Council.
One criticism of the LIRR is that it has not improved service to the "east end" of Long Island as the twin forks continue to grow in popularity as a year round tourist and residential destination. Demand is evidenced by flourishing for-profit bus services such as the Hampton Jitney and the Hampton Luxury Liner and the early formative stages of a new East End Transportation Authority. Local politicians have joined the public outcry for the LIRR to either improve the frequency of east end services, or turn the operation over to a local transportation authority.
Critics claim that the on-time performance (OTP) calculated by the LIRR is manipulated to be artificially high. Because the LIRR does not release any raw timing data nor does it have independent (non-MTA) audits it is impossible to verify this claim, or the accuracy of the current On Time Performance measurement. The "percentage" measure is used by many other US passenger railroads but the criticism over accuracy is specific to the LIRR. As defined by the LIRR, a train is "on time" if it arrives at a station within 5 minutes and 59 seconds of the scheduled time. The criterion was 4 minutes and 59 seconds until the LIRR changed it because of a bug in their computer systems. Critics believe the OTP measure does not reflect what commuters experience on a daily basis. The LIRR publishes the current OTP in a monthly booklet called TrainTalk. TrainTalk was previously known as "Keeping Track."
A more accurate way to measure delays and OTP has been proposed. Called the "Passenger Hours Delayed" index it can measure total person-hours of a specific delay. This would be useful in comparing performance of specific days or incidents, day-to-day (or week-to-week) periods, something the current measure cannot do. This 'PHD' index measure is used by some transportation research organizations and would be more meaningful to commuters. s of 2015[ [update]] it has not been adopted. The two methods are not mutually exclusive and could be kept and published simultaneously.
2007 ridership was 86.1 million, up 4.9% over 2006. The all time highest ridership was in 1929, when 119 million passengers rode 1.89 billion passenger miles.
Law enforcement.
The LIRR Police Department, founded in 1863, was absorbed along with the Metro-North Railroad Police to form the Metropolitan Transportation Authority Police (MTA Police) in 1998.
Pension and disability fraud scandal.
A "New York Times" investigation in 2008 showed that 25% of LIRR employees who had retired since 2000 filed for disability payments from the federal Railroad Retirement Board and 97% of them were approved to receive disability pension. The total collected was more than $250,000,000 over eight years. As a result, Railroad Retirement agents from Chicago inspected the Long Island office of the Railroad Retirement Board on September 23, 2008. New York Governor David Paterson issued a statement calling for Congress to conduct a full review of the board's mission and daily activities. Officials at the board's headquarters responded to the investigation stating that all occupational disability annuities were issued in accordance with applicable laws.
On November 17, 2008, a former LIRR pension manager was arrested and charged with official misconduct for performing outside work without permission. However, these charges were all dismissed for "no merit" by Supreme Court Judge Kase on December 11, 2009 on the grounds that the prosecution had misled the grand jury in the indictment.
A report produced in September 2009 by the Government Accountability Office stated that the rate at which retirees were rewarded disability claims was above the norm for the industry in general and indicated "troubling" practices that may indicate fraud, such as the use of a very small group of physicians in making the diagnosis.
Another series of arrests on October 27, 2011 included two doctors and a former union official.
According to court documents, from 1998 through 2011, 79% of LIRR retirees obtained federal disability when they retired. On August 6, 2013, a doctor and two consultants were found guilty in connection with the accusations and sentenced to prison.
External links.
class="navbox collapsible autocollapse"
!colspan="48"

</doc>
<doc id="37141" url="http://en.wikipedia.org/wiki?curid=37141" title="Triskaidekaphobia">
Triskaidekaphobia

Triskaidekaphobia (from Greek "tris" meaning "3", "kai" meaning "and", "deka" meaning "10" and "phobos" meaning "fear" or "morbid fear") is fear of the number 13 and avoidance to use it; it is a superstition and related to the specific fear of the 13th person at the Last Supper being Judas, who betrayed Jesus Christ and ultimately hanged himself. It is also a reason for the fear of Friday the 13th, called "paraskevidekatriaphobia" (from Παρασκευή "Paraskevi", Greek for Friday) or "friggatriskaidekaphobia" (after Frigg, the Norse goddess after whom Friday is named in English).
The term was first used by Isador Coriat in "Abnormal Psychology".
Origins.
There is a myth that the earliest reference to thirteen being unlucky or evil is from the Babylonian Code of Hammurabi (circa 1780 BCE), where the thirteenth law is omitted. In fact, the original Code of Hammurabi has no numeration. The translation by L.W. King (1910), edited by Richard Hooker, omitted one article:
If the seller have gone to (his) fate (i. e., have died), the purchaser shall recover damages in said case fivefold from the estate of the seller.
Other translations of the Code of Hammurabi, for example the translation by Robert Francis Harper, include the 13th article.
Some Christian traditions have it that at the Last Supper, Judas, the disciple who betrayed Jesus, was the 13th to sit at the table. However, the Bible itself says nothing about the order at which the Apostles sat. Also, the number 13 is not uniformly bad in the Judeo-Christian tradition. For example, the attributes of God (also called the Thirteen Attributes of Mercy) are enumerated in the Torah (Exodus 34:6–7). Some modern Christian churches also use 13 attributes of God in sermons.
Triskaidekaphobia may have also affected the Vikings: It is believed that Loki was the 13th god in the Norse pantheon—more specifically, Loki was believed to have engineered the murder of Balder and was the 13th guest to arrive at the funeral. This is perhaps related to the superstition that if 13 people gather, one of them will die in the following year. However, the oldest source of this myth, "Lokasenna", has far more than 13 guests (17 of the guests are mentioned by name) so this example should not be taken too seriously. Another Norse tradition involves the myth of Norna-Gest: When the uninvited norns showed up at his birthday celebration (thus increasing the number of guests from ten to thirteen), they cursed the infant by magically binding his lifespan to that of a mystic candle they presented to him.
Events related to unlucky 13.
Apollo 13 launched on April 11, 1970 at 13:13:00 CST and experienced an oxygen tank explosion on April 13 at 21:07:53 CST. It later returned safely to earth on April 17. The Space Shuttle Columbia disaster occurred on the 113th flight of the Space Shuttle. Princess Diana's death happened at the 13th pillar of the Pont de l'Alma tunnel.
On Friday, October 13, 1307, the arrest of the Knights Templar was ordered by Philip IV of France. While the number 13 was considered unlucky, Friday the 13th was not considered unlucky at the time. The idea that their arrest was related to Friday the 13th was coined early in the 21st century and popularized by the novel "The Da Vinci Code".
In 1881 an influential group of New Yorkers led by US Civil War veteran Captain William Fowler came together to put an end to this and other superstitions. They formed a dinner cabaret club, which they called the Thirteen Club. At the first meeting, on Friday, January 13, 1881, at 8:13 p.m., thirteen people sat down to dine in Room 13 of the venue. The guests walked under a ladder to enter the room and were seated among piles of spilled salt. Many Thirteen Clubs sprang up all over North America over the next 40 years. Their activities were regularly reported in leading newspapers, and their numbers included five future US presidents, from Chester A. Arthur to Theodore Roosevelt. Thirteen Clubs had various imitators, but they all gradually faded from interest.
The number 13 also has a biblical significance. As the children of Israel approached the promised land after being led out of Egyptian bondage one of the first cities the Israelites had to conquer was Jericho. God instructed the Israel army to march around the walls of Jericho once a day for six days and then march around the walls seven times on the seventh day for a total of 13 trips around the city. After obeying all that God had commanded the circular walls fell down flat outwardly and the city was destroyed. These events are recorded in the 6th book of the bible or Joshua 6:3-4.
Vehicle registration plates in the Republic of Ireland are such that the first two digits represent the year of registration of the vehicle (i.e., 11 is a 2011 registered car, 12 is 2012, and so on). In 2012, there were concerns among members of the Society of the Irish Motor Industry (SIMI) that the prospect of having "13" registered vehicles might discourage motorists from buying new cars because of superstition surrounding the number thirteen, and that car sales and the motor industry (which was already ailing) would suffer as a result. The government, in consultation with SIMI, introduced a system whereby 2013 registered vehicles would have their registration plates' age identifier string modified to read "131" for vehicles registered in the first six months of 2013 and "132" for those registered in the latter six months of the year. The main reason for this however, is to increase the number of car sales in the latter months of the year. Even though 70% of new cars are bought during the first four months of the year, some consumers believe that it doesn't accurately reflect the real age of a new car, since cars bought in January will most likely have been manufactured the previous year, while those bought later in the year will be actually made in the same year. This system continued after 2013, with vehicles registered in the first half of 2014 labelled "141" rather than "14".
Lucky 13.
In some regions 13 is considered a lucky number. For example, 13 is lucky in Italy except in some contexts, such as sitting at the dinner table. Colgate University was started by 13 men with $13 and 13 prayers, so 13 is considered a lucky number. Friday the 13th is the luckiest day at Colgate.
Several Venezuelan sportspeople have chosen 13 as squad number, most notably Dave Concepción, Omar Vizquel, Oswaldo Guillén and Pastor Maldonado.

</doc>
<doc id="37142" url="http://en.wikipedia.org/wiki?curid=37142" title="Zeno of Citium">
Zeno of Citium

Zeno of Citium (; Greek: Ζήνων ὁ Κιτιεύς, "Zēnōn ho Kitieus"; c. 334 – c. 262 BC) was a Greek thinker from Citium (Κίτιον, "Kition"), Cyprus. He was possibly of Phoenician descent. Zeno was the founder of the Stoic school of philosophy, which he taught in Athens from about 300 BC. Based on the moral ideas of the Cynics, Stoicism laid great emphasis on goodness and peace of mind gained from living a life of Virtue in accordance with Nature. It proved very successful, and flourished as the dominant philosophy from the Hellenistic period through to the Roman era.
Life.
Zeno was born c. 334 BC,[a] in Citium in Cyprus. Most of the details known about his life come from the anecdotes preserved by Diogenes Laërtius in his "Lives and Opinions of Eminent Philosophers". Diogenes relates a legend that Zeno was a merchant; after surviving a shipwreck, Zeno wandered into a bookshop in Athens and was attracted to some writings about Socrates. He asked the librarian how to find such a man. In response, the librarian pointed to Crates of Thebes, the most famous Cynic living at that time in Greece.
Zeno is described as a haggard, tanned person, living a spare, ascetic life. This coincides with the influences of Cynic teaching, and was, at least in part, continued in his Stoic philosophy. From the day Zeno became Crates’s pupil, he showed a strong bent for philosophy, though with too much native modesty to assimilate Cynic shamelessness. Hence Crates, desirous of curing this defect in him, gave him a potful of lentil-soup to carry through the Ceramicus; and when he saw that Zeno was ashamed and tried to keep it out of sight, Crates broke the pot with a blow of his staff. As Zeno began to run off in embarrassment with the lentil-soup flowing down his legs, Crates chided “Why run away, my little Phoenician?”, “nothing terrible has befallen you.”
Apart from Crates, Zeno studied under the philosophers of the Megarian school, including Stilpo, and the dialecticians Diodorus Cronus, and Philo. He is also said to have studied Platonist philosophy under the direction of Xenocrates, and Polemo.
Zeno began teaching in the colonnade in the Agora of Athens known as the Stoa Poikile (Greek Στοὰ Ποικίλη) in 301 BC. His disciples were initially called Zenonians, but eventually they came to be known as Stoics, a name previously applied to poets who congregated in the Stoa Poikile.
Among the admirers of Zeno was king Antigonus II Gonatas of Macedonia, who, whenever he came to Athens, would visit Zeno. Zeno is said to have declined an invitation to visit Antigonus in Macedonia, although their supposed correspondence preserved by Laërtius is undoubtably the invention of a later rhetorician. Zeno instead sent his friend and disciple Persaeus, who had lived with Zeno in his house. Among Zeno's other pupils there were Aristo of Chios, Sphaerus, and Cleanthes who succeeded Zeno as the head ("scholarch") of the Stoic school in Athens.
Zeno is said to have declined Athenian citizenship when it was offered to him, fearing that he would appear unfaithful to his native land, where he was highly esteemed. We are also told that Zeno was of an earnest, if not gloomy disposition; that he preferred the company of the few to the many; that he was fond of burying himself in investigations; and that he had a dislike to verbose and elaborate speeches. Diogenes Laërtius has preserved many clever and witty remarks by Zeno, the veracity of which cannot be ascertained.
Zeno died around 262 BC.[a] Laërtius reports about his death: 
During his lifetime, Zeno received appreciation for his philosophical and pedagogical teachings. Among other things, Zeno was honored with the golden crown, and a tomb was built in honor of his moral influence on the youth of his era.
The crater Zeno on the Moon is named in his honor.
Philosophy.
Following the ideas of the Academics, Zeno divided philosophy into three parts: Logic (a very wide subject including rhetoric, grammar, and the theories of perception and thought); Physics (not just science, but the divine nature of the universe as well); and Ethics, the end goal of which was to achieve happiness through the right way of living according to Nature. Because Zeno's ideas were built upon by Chrysippus and other Stoics, it can be difficult to determine, in some areas, precisely what he thought, but his general views can be outlined:
Logic.
In his treatment of Logic, Zeno was influenced by Stilpo and the other Megarians. Zeno urged the need to lay down a basis for Logic because the wise person must know how to avoid deception. Cicero accused Zeno of being inferior to his philosophical predecessors in his treatment of Logic, and it seems true that a more exact treatment of the subject was laid down by his successors, including Chrysippus. Zeno divided true conceptions into the comprehensible and the incomprehensible, permitting for free-will the power of assent ("sunkatathesis"/συγκατάθεσις) in distinguishing between sense impressions. Zeno said that there were four stages in the process leading to true knowledge, which he illustrated with the example of the flat, extended hand, and the gradual closing of the fist:
Zeno stretched out his fingers, and showed the palm of his hand, – "Perception," – he said, – "is a thing like this."- 
Then, when he had closed his fingers a little, – "Assent is like this." – Afterwards, when he had completely closed his hand, and showed his fist, that, he said, was Comprehension. From which simile he also gave that state a new name, calling it "katalepsis" (κατάληψις). But when he brought his left hand against his right, and with it took a firm and tight hold of his fist: – "Knowledge" – he said, was of that character; and that was what none but a wise person possessed.
Physics.
The Universe, in Zeno's view, is God: a divine reasoning entity, where all the parts belong to the whole. Into this pantheistic system he incorporated the physics of Heraclitus; the Universe contains a divine artisan-fire, which foresees everything, and extending throughout the Universe, must produce everything:
Zeno, then, defines nature by saying that it is artistically working fire, which advances by fixed methods to creation. For he maintains that it is the main function of art to create and produce, and that what the hand accomplishes in the productions of the arts we employ, is accomplished much more artistically by nature, that is, as I said, by artistically working fire, which is the master of the other arts.
This divine fire, or aether, is the basis for all activity in the Universe, operating on otherwise passive matter, which neither increases nor diminishes itself. The primary substance in the Universe comes from fire, passes through the stage of air, and then becomes water: the thicker portion becoming earth, and the thinner portion becoming air again, and then rarefying back into fire. Individual souls are part of the same fire as the world-soul of the Universe. Following Heraclitus, Zeno adopted the view that the Universe underwent regular cycles of formation and destruction.
The Nature of the Universe is such that it accomplishes what is right and prevents the opposite, and is identified with unconditional Fate, while allowing it the free-will attributed to it.
Ethics.
Like the Cynics, Zeno recognised a single, sole and simple good, which is the only goal to strive for. "Happiness is a good flow of life," said Zeno, and this can only be achieved through the use of right Reason coinciding with the Universal Reason ("Logos"), which governs everything. A bad feeling ("pathos") "is a disturbance of the mind repugnant to Reason, and against Nature." This consistency of soul, out of which morally good actions spring, is Virtue, true good can only consist in Virtue.
Zeno deviated from the Cynics in saying that things that are morally indifferent could nevertheless have value. Things have a relative value in proportion to how they aid the natural instinct for self-preservation. That which is to be preferred is a "fitting action" ("kathêkon"/καθῆκον), a designation Zeno first introduced. Self-preservation, and the things that contribute towards it, has only a conditional value; it does not aid happiness, which depends only on moral actions.
Just as Virtue can only exist within the dominion of Reason, so Vice can only exist with the rejection of Reason. Virtue is absolutely opposed to Vice, the two cannot exist in the same thing together, and cannot be increased or decreased; no one moral action is more virtuous than another. All actions are either good or bad, since impulses and desires rest upon free consent, and hence even passive mental states or emotions that are not guided by reason are immoral, and produce immoral actions. Zeno distinguished four negative emotions: desire, fear, pleasure and pain ("epithumia, phobos, hêdonê, lupê" / ἐπιθυμία, φόβος, ἡδονή, λύπη), and he was probably responsible for distinguishing the three corresponding positive emotions: will, caution, and joy ("boulêsis, eulabeia, chara" / βούλησις, εὐλάβεια, χαρά), with no corresponding rational equivalent for pain. All errors must be rooted out, not merely set aside, and replaced with right Reason.
Works.
None of Zeno's writings have survived except as fragmentary quotations preserved by later writers. However, the titles of many of Zeno's writings are known and are as follows:
The most famous of these works was Zeno's "Republic", a work written in conscious imitation of (or opposition to) Plato. Although it has not survived, more is known about it than any of his other works. It outlined Zeno's vision of the ideal Stoic society built on egalitarian principles.
Notes.
a. ^ The dates for Zeno's life are controversial. According to Apollodorus, as quoted by Philodemus, Zeno died in Arrheneides' archonship (262/1 BC). According to Persaeus (Diogenes Laërtius vii. 28), Zeno lived for 72 years. His date of birth is thus 334/3 BC. A plausible chronology for his life is as follows: He was born 334/3 BC, and came to Athens in 312/11 BC at the age of 22 (Diogenes Laërtius, vii. 28). He studied philosophy for about 10 years (Diogenes Laërtius, vii. 2); opened his own school during Clearchus' archonship in 301/0 BC (Philodemus, "On the Stoics", col. 4); and was the head of the school for 39 years and 3 months (Philodemus, "On the Stoics", col. 4), and died 262/1 BC. For more information see William Scott Ferguson, (1911), "Hellenistic Athens: An Historical Essay", page 185.; and Tiziano Dorandi, "Chronology" in K. Algra et al., (2005), "The Cambridge History of Hellenistic Philosophy", page 38.
</dl>
Further reading.
</dl>

</doc>
<doc id="37143" url="http://en.wikipedia.org/wiki?curid=37143" title="Chrysippus">
Chrysippus

Chrysippus of Soli (Greek: Χρύσιππος ὁ Σολεύς, "Chrysippos ho Soleus"; 279 – c. 206 BC) was a Greek Stoic philosopher. He was a native of Soli, Cilicia, but moved to Athens as a young man, where he became a pupil of Cleanthes in the Stoic school. When Cleanthes died, around 230 BC, Chrysippus became the third head of the school. A prolific writer, Chrysippus expanded the fundamental doctrines of Zeno of Citium, the founder of the school, which earned him the title of Second Founder of Stoicism.
Chrysippus excelled in logic, the theory of knowledge, ethics and physics. He created an original system of propositional logic in order to better understand the workings of the universe and role of humanity within it. He adhered to a deterministic view of fate, but nevertheless sought a role for personal freedom in thought and action. Ethics, he taught, depended on understanding the nature of the universe, and he taught a therapy of extirpating the unruly passions which depress and crush the soul. He initiated the success of Stoicism as one of the most influential philosophical movements for centuries in the Greek and Roman world.
Life.
Chrysippus was the son of Apollonius of Tarsus but was himself born at Soli, Cilicia. He was slight in stature and is reputed to have trained as a long-distance runner. While still young, he lost his substantial inherited property when it was confiscated to the king's treasury. Chrysippus moved to Athens, where he became the disciple of Cleanthes, who was then the head ("scholarch") of the Stoic school. It is said too that he attended the courses of Arcesilaus and his successor Lacydes, in the Platonic Academy.
Chrysippus threw himself eagerly into the study of the Stoic system. His reputation for learning among his contemporaries was considerable. He was noted for intellectual audacity and self-confidence and his reliance on his own ability was shown, among other things, in the request he is supposed to have made to Cleanthes: "Give me the principles, and I will find the proofs myself." He succeeded Cleanthes as head of the Stoic school when Cleanthes died, in around 230 BC.
Chrysippus was a prolific writer. He is said to rarely have gone without writing 500 lines a day and he composed more than 705 works. His desire to be comprehensive meant that he would take both sides of an argument and his opponents accused him of filling his books with the quotations of others. He was considered diffuse and obscure in his utterances and careless in his style, but his abilities were highly regarded, and he came to be seen as a preeminent authority for the school.
He died during the 143rd Olympiad (208–204 BC) at the age of 73. Diogenes Laërtius gives two different accounts of his death. In the first account, Chrysippus was seized with dizziness having drunk undiluted wine at a feast, and died soon after. In the second account, he was watching a donkey eat some figs and cried out: "Now give the donkey a drink of pure wine to wash down the figs", whereupon he died in a fit of laughter. His nephew Aristocreon erected a statue in his honour in the Kerameikos. Chrysippus was succeeded as head of the Stoic school by his pupil Zeno of Tarsus.
Of his written works, none have survived except as fragments embedded in the works of later authors like Cicero, Seneca, Galen, Plutarch, and others. Further fragments of two works by Chrysippus are preserved among the charred papyrus remains discovered at the Villa of the Papyri at Herculaneum. These are "Logical Questions" and "On Providence". A third work discovered there may also be by him.
Philosophy.
Chrysippus had a long and successful career of resisting the attacks of the Academy and hoped not simply to defend Stoicism against the assaults of the past, but also against all possible attack in the future. He took the doctrines of Zeno and Cleanthes and crystallized them into what became the definitive system of Stoicism. He elaborated the physical doctrines of the Stoics and their theory of knowledge and he created much of their formal logic. In short, Chrysippus made the Stoic system what it was. It was said that "without Chrysippus, there would have been no Stoa".
Logic.
Chrysippus wrote much on the subject of logic and created a system of propositional logic. Aristotle's term logic had been concerned with the interrelations of terms such as "Socrates" or "man" ("all men are mortal, Socrates is a man, so Socrates is mortal"). Stoic logic, on the other hand, was concerned with the interrelations of propositions such as "it is day" ("if it is day, it is light: but it is day: so it is light"). Though the earlier Megarian dialecticians – Diodorus Cronus and Philo – had worked in this field and the pupils of Aristotle – Theophrastus and Eudemus – had investigated hypothetical syllogisms, it was Chrysippus who developed these principles into a coherent system of propositional logic.
Propositions.
Chrysippus defined a proposition as "that which is capable of being denied or affirmed as it is in itself" and gave examples of propositions such as "it is day" and "Dion is walking." He distinguished between simple and non-simple propositions, which in modern terminology are known as atomic and molecular propositions. A simple proposition is an elementary statement such as "it is day." Simple propositions are linked together to form non-simple propositions by the use of logical connectives. Chrysippus enumerated five kinds of molecular propositions according to the connective used:
Thus several types of molecular propositions, familiar to modern logic, were listed by Chrysippus, including the conjunction, the disjunction, and the conditional, and Chrysippus studied their criteria of truth closely.
Conditional propositions.
The first logicians to debate conditional statements were Diodorus Cronus and his pupil Philo. Writing five-hundred years later, Sextus Empiricus refers to a debate between Diodorus and Philo. Philo regarded all conditionals as true except those which with a correct antecedent had an incorrect consequent, and this meant a proposition such as "if it is day, then I am talking," is true unless it is day and I fall silent. But Diodorus argued that a true conditional is one in which the antecedent clause could never lead to an untrue conclusion – thus, because the proposition "if it is day, then I am talking" can be false, it is invalid. However, paradoxical propositions were still possible such as "if atomic elements of things do not exist, atomic elements exists." Chrysippus adopted a much stricter view regarding conditional propositions, which made such paradoxes impossible: to him, a conditional is true if denial of the consequent is logically incompatible with the antecedent. This corresponds to the modern-day strict conditional.
Syllogistic.
Chrysippus developed a syllogistic or system of deduction in which he made use of five types of basic arguments or argument forms called indemonstrable syllogisms, which played the role of axioms, and four inference rules, called "themata" by means of which complex syllogisms could be reduced to these axioms. The forms of the five indemonstrables were:
Of the four inference rules, only two survived. One, the so-called first "thema", was a rule of antilogism. The other, the third "thema", was a cut rule by which chain syllogisms could be reduced to simple syllogisms. The purpose of Stoic syllogistic was not merely to create a formal system. It was also understood as the study of the operations of reason, the divine reason ("logos") which governs the universe, of which human beings are a part. The goal was to find valid rules of inference and forms of proof to help people find their way in life.
Other logical work.
Chrysippus analyzed speech and the handling of names and terms. He also devoted much effort in refuting fallacies and paradoxes. According to Diogenes Laërtius, Chrysippus wrote twelve works in 23 books on the Liar paradox; seven works in 17 books on amphiboly; and another nine works in 26 books on other conundrums. In all, 28 works or 66 books were given over to puzzles or paradoxes. 
Chrysippus is the first Stoic for whom the third of the four Stoic categories, i.e. the category "somehow disposed" is attested. In the surviving evidence, Chrysippus frequently makes use of the categories of "substance" and "quality", but makes little use of the other two Stoic categories ("somehow disposed" and "somehow disposed in relation to something"). It is not clear whether the categories had any special significance for Chrysippus, and a clear doctrine of categories may be the work of later Stoics.
Later reception.
Chrysippus came to be renowned as one of the foremost logicians of ancient Greece. When Clement of Alexandria wanted to mention one who was master among logicians, as Homer was master among poets, it was Chrysippus, not Aristotle, he chose. Diogenes Laërtius wrote: "If the gods use dialectic, they would use none other than that of Chrysippus." The logical work by Chrysippus came to be neglected and forgotten. Aristotle's logic prevailed, partly because it was seen as more practical, and partly because it was taken up by the Neoplatonists. As recently as the 19th century, Stoic logic was treated with contempt, a barren formulaic system, which was merely clothing the logic of Aristotle with new terminology. It was not until the 20th century, with the advances in logic, and the modern propositional calculus, that it became clear that Stoic logic constituted a significant achievement.
Epistemology.
For the Stoics, truth is distinguished from error by the sage who possesses right reason. Chrysippus's theory of knowledge was empirical. The senses transmit messages from the external world, and their reports are controlled not by referring them to innate ideas, but by comparing them to previous reports stored in the mind. Zeno had defined impressions of sense as "an impression in the soul" and this was interpreted literally by Cleanthes, who compared the impression on the soul to the impression made by a seal on wax. Chrysippus preferred to regard it as an alteration or change in the soul; that is, the soul receives a modification from every external object that acts upon it, just as the air receives countless strokes when many people are speaking at once.
In the receipt of an impression, the soul is purely passive and the impression reveals not only its own existence, but that also of its cause—just as light displays itself and the elements that are in it. The power to name the object resides in the understanding. First must come the impression, and the understanding—having the power of utterance—expresses in speech the affection it receives from the object. True presentations are distinguished from those that are false by the use of memory, classification and comparison. If the sense organ and the mind are healthy—and provided that an external object can be really seen or heard—the presentation, due to its clearness and distinctness, has the power to extort the assent that always lies in our power, to give or to withhold. In a context in which people are understood to be rational beings, reason is developed out of these notions.
Physics.
Chrysippus insisted on the organic unity of the universe, as well as the correlation and mutual interdependence of all of its parts. He said, "the universe is its own soul and its own controlling mind." Following Zeno, Chrysippus determined fiery breath or aether to be the primitive substance of the universe. Objects are made up of inert formless matter and an informing soul, "pneuma", provides form to the undifferentiated matter. The "pneuma" pervades all of substance and maintains the unity of the universe and constitutes the soul—the incorporeal and, in many conceptions, immortal essence of a person or living thing—of the human being.
The classical elements change into one another by a process of condensation and rarefaction. Fire first becomes solidified into air; then air into water; and lastly, water into earth. The process of dissolution takes place in the reverse order: earth being rarefied into water, water into air and air into fire.
The human soul was divided by Chrysippus into eight faculties: the five senses, the power of reproduction, the power of speech, and the "ruling part" that is located in the chest rather than the head. Individual souls are perishable; but, according to the view originated by Chrysippus, the souls of wise people survive longer after their death. No individual soul can, however, survive beyond the periodic conflagration, when the universe is renewed.
Fate.
For Chrysippus, all things happen according to fate: what seems to be accidental has always some hidden cause. The unity of the world consists in the chain-like dependence of cause upon cause. Nothing can take place without a sufficient cause. According to Chrysippus, every proposition is either true or false, and this must apply to future events as well:
If any motion exists without a cause, then not every proposition will be either true or false. For that which has not efficient causes is neither true nor false. But every proposition is either true or false. Therefore, there is no motion without a cause. And if this is so, then all effects owe their existence to prior causes. And if this is so, all things happen by fate. It follows therefore that whatever happens, happens by fate.
The Stoic view of fate is entirely based on a view of the universe as a whole. Individual things and persons only come into consideration as dependent parts of this whole. Everything is, in every respect, determined by this relation, and is consequently subject to the general order of the world.
If his opponents objected that, if everything is determined by destiny, there is no individual responsibility, since what has been once foreordained must happen, come what may, Chrysippus replied that there is a distinction to be made between simple and complex predestination. Becoming ill may be fated whatever happens but, if a person's recovery is linked to consulting a doctor, then consulting the doctor is fated to occur together with that person's recovery, and this becomes a complex fact. All human actions – in fact, our destiny – are decided by our relation to things, or as Chrysippus put it, events are "co-fated" to occur:
The non-destruction of one's coat, he says, is not fated simply, but co-fated with its being taken care of, and someone's being saved from his enemies is co-fated with his fleeing those enemies; and having children is co-fated with being willing to lie with a woman. ... For many things cannot occur without our being willing and indeed contributing a most strenuous eagerness and zeal for these things, since, he says, it was fated for these things to occur in conjunction with this personal effort. ... But it will be in our power, he says, with what is in our power being included in fate.
Thus our actions are predetermined, and are causally related to the overarching network of fate, but nevertheless the moral responsibility of how we respond to impressions remains our own. The one all-determining power is active everywhere, working in each particular being according to its nature, whether in rational or irrational creatures or in inorganic objects. Every action is brought about by the co-operation of causes depending on the nature of things and the character of the agent. Our actions would only be involuntary if they were produced by external causes alone, without any co-operation, on the part of our wills, with external causes. Virtue and vice are set down as things in our power, for which, consequently, we are responsible. Moral responsibility depends only on freedom of the will, and what emanates from our will is our own, no matter whether it is possible for us to act differently or not. This rather subtle position which attempts to reconcile determinism with human responsibility is known as soft-determinism, or compatibilism.
Divination.
Chrysippus also argued for the existence of fate based on divination, which he thought there was good evidence for. It would not be possible for diviners to predict the future if the future itself was accidental. Omens and portents, he believed, are the natural symptoms of certain occurrences. There must be countless indications of the course of providence, for the most part unobserved, the meaning of only a few having become known to humanity. To those who argued that divination was superfluous as all events are foreordained, he replied that both divination and our behaviour under the warnings which it affords are included in the chain of causation.
God.
The Stoics believed that the universe is God, and Chrysippus affirmed that "the universe itself is God and the universal outpouring of its soul." It is the guiding principle of the universe, "operating in mind and reason, together with the common nature of things and the totality which embraces all existence." Based on these beliefs, physicist and philosopher Max Bernhard Weinstein identified Chrysippus as a Pandeist.
Chrysippus sought to prove the existence of God, making use of a teleological argument:
If there is anything that humanity cannot produce, the being who produces it is better than humanity. But humanity cannot produce the things that are in the universe – the heavenly bodies, etc. The being, therefore, who produces them is superior to humanity. But who is there that is superior to humanity, except God? Therefore, God exists.
Chrysippus spoke of God and gods interchangeably. He interpreted the gods of traditional Greek religion by viewing them as different aspects of the one reality. Cicero tells us that "he further maintained that aether is that which people call Zeus, and that the air which permeates the seas is Poseidon, and that the earth is what is known by the name of Demeter, and he treated in similar style the names of the other gods." In addition, the universe exists for the benefit of the universal god:
We should infer in the case of a beautiful dwelling-place that it was built for its owners and not for mice; we ought, therefore, in the same way to regard the universe as the dwelling-place of the gods.
Theodicy.
In response to the question of how evil could exist in a good universe, Chrysippus replied "evil cannot be removed, nor is it well that it should be removed." Firstly, he argued, following Plato, that it was impossible for good to exist without evil, for justice could not be known without injustice, courage without cowardice, temperance without intemperance or wisdom without foolishness. Secondly, apparent evils exist as a consequent of nature's goodness, thus it was necessary for the human skull to be made from small and thin bones for reasons of utility, but this superior utility meant that the skull is vulnerable to blows. Thirdly, evils are distributed according to the rational will of Zeus, either to punish the wicked or because they are important to the world-order as a whole. Thus evil is good under disguise, and is ultimately conducive to the best. Chrysippus compared evil to the coarse jest in the comedy; for, just as the jest, though offensive in itself, improves the piece as a whole, "so too you may criticize evil regarded by itself, yet allow that, taken with all else, it has its use."
Mathematics.
Chrysippus regarded bodies, surfaces, lines, places, the void and time as all being infinitely divisible. He determined one of the principal features of the infinite set: since a man and a finger have an infinite number of parts as do the universe and a man, it cannot be said that a man has more parts than his finger, nor that the universe has more parts than a man.
Chrysippus also responded to a problem first posed by Democritus. If a cone is divided by a plane parallel to its base, are the surfaces of the segments equal or unequal? If they are equal, then the cone becomes a cylinder; if they are unequal, then the surface of the cone must be stepped. The reply of Chrysippus was that the surfaces are both equal and unequal. Chrysippus was, in effect, negating the law of excluded middle with respect to the equal and unequal, and thus he may have anticipated an important principle of modern infinitesimal calculus, namely, the limit and the process of convergence towards a limit.
Chrysippus was notable for claiming that "one" is a number. One was not always considered a number by the ancient Greeks since they viewed one as that by which things are measured. Aristotle in his "Metaphysics" wrote, "... a measure is not the things measured, but the measure or the One is the beginning of number." Chrysippus asserted that one had "magnitude one" (Greek: πλῆθος ἕν), although this was not generally accepted by the Greeks, and Iamblichus wrote that "magnitude one" was a contradiction in terms.
Ethics.
Chrysippus taught that ethics depended on physics. In his "Physical Theses", he stated: "for there is no other or more appropriate way of approaching the subject of good and evil on the virtues or happiness than from the nature of all things and the administration of the universe." The goal of life, said Chrysippus, is to live in accordance with one's experience of the actual course of nature. A person's individual nature is part of the nature of the whole universe, and thus life should be lived in accordance with one's own human nature as well as that of the universe. Human nature is ethical, and humanity is akin to the Divine, emanating from the primal fire or aether, which, though material, is the embodiment of reason; and people should conduct themselves accordingly. People have freedom, and this freedom consists in emancipation from irrational desires (lust, riches, position in life, domination, etc.) and in subjecting the will to reason. Chrysippus laid the greatest stress on the worth and dignity of the individual, and on the power of will.
The Stoics admitted between the good and the bad a third class of things – the indifferent ("adiaphora"). Of things morally indifferent, the best includes health, and riches, and honour, and the worst includes sickness and poverty. Chrysippus accepted that it was normal in ordinary usage to refer to the preferred indifferent things as "good", but the wise person, said Chrysippus, uses such things without requiring them. Practice and habit are necessary to render virtue perfect in the individual – in other words, there is such a thing as moral progress, and character has to be built up.
The Stoics sought to be free of the unruly emotions, which they regarded as being contrary to nature. The passions or emotions ("pathe") are the disturbing element in right judgment. Chrysippus wrote a whole book concerning the therapy of the emotions. The passions are like diseases which depress and crush the soul, thus he sought to eradicate them ("apatheia"). Wrong judgements turn into passions when they gather an impetus of their own, just as, when one has started running, it is difficult to stop. One cannot hope to eradicate the emotions when one is in the heat of love or anger: this can only be done when one is calm. Therefore one should prepare in advance, and deal with the emotions in the mind as if they were present. By applying reason to emotions such as greed, pride, or lust, one can understand the harm which they cause.

</doc>
<doc id="37145" url="http://en.wikipedia.org/wiki?curid=37145" title="Lucretius">
Lucretius

Titus Lucretius Carus (; 99 BC – c. 55 BC) was a Roman poet and philosopher. His only known work is the epic philosophical poem "De rerum natura" about the tenets and philosophy of Epicureanism, and which is usually translated into English as "On the Nature of Things".
Very little is known about Lucretius's life; the only certain fact is that he was either a friend or client of Gaius Memmius, to whom the poem was addressed and dedicated.
The "De rerum natura" was a considerable influence on the Augustan poets, particularly Virgil (in his "Aeneid" and "Georgics", and to a lesser extent on the "Satires" and "Eclogues") and Horace. The work virtually disappeared during the Middle Ages but was rediscovered in 1417 in a monastery in Germany by Poggio Bracciolini, and it played an important role both in the development of atomism (Lucretius was an important influence on Pierre Gassendi) and the efforts of various figures of the Enlightenment era to construct a new Christian humanism.
Life.
And now, good Memmius, receptive ears And keen intelligence detached from cares I pray you bring to true philosophy
"De Rerum Natura" (tr. Melville) 1.50
If I must speak, my noble Memmius, As nature's majesty now known demands
"De Rerum Natura" (tr. Melville) 5.6
Virtually nothing is known about the life of Lucretius. He was probably a member of the aristocratic "gens Lucretia", and his work shows an intimate knowledge of the luxurious lifestyle in Rome. Lucretius's love of the countryside invites speculation that he inhabited family-owned rural estates, as did many wealthy Roman families, and he certainly was expensively educated with a mastery of Latin, Greek, literature, and philosophy. Jerome tells how he was driven mad by a love potion and wrote his poetry between fits of insanity, eventually committing suicide in middle age; but modern scholarship suggests this account was probably an invention. In a letter by Cicero to his brother Quintus in February 54 BC, Cicero said : "The poems of Lucretius are as you write: they exhibit many flashes of genius, and yet show great mastership." By this time, both Cicero and his brother had read "De Rerum Natura", and so might have many other Romans. A literary evaluation of Lucretius's work, however, reveals some repetition and a sudden end to Book 6 during a description of the plague at Athens. The poem appears to have been published without a final revision, possibly due to its author's death. If this is true, Lucretius must have been dead by 54 BC.
In the work of another author in late Republican Rome, Virgil writes in the second book of his "Georgics", apparently referring to Lucretius, "Happy is he who has discovered the causes of things and has cast beneath his feet all fears, unavoidable fate, and the din of the devouring Underworld."
A brief biographical note is found in Aelius Donatus's "Life of Virgil", which seems to be derived from an earlier work by Suetonius. The note reads: "The first years of his life Virgil spent in Cremona until the assumption of his "toga virilis" on his 17th birthday (when the same two men held the consulate as when he was born), and it so happened that on the very same day Lucretius the poet passed away." However, although Lucretius certainly lived and died around the time that Virgil and Cicero flourished, the information in this particular testimony is internally inconsistent: If Virgil was born in 70 BC, his 17th birthday would be in 53. The two consuls of 70 BC, Pompey and Crassus, stood together as consuls again in 55, not 53.
There is insufficient basis for a confident assertion of the date of Lucretius's birth or death in other sources. Another yet briefer note is found in the "Chronicon" of Donatus's pupil, Jerome. Writing four centuries after Lucretius's death, he enters under the 171st Olympiad the following line: "Titus Lucretius the poet is born. Later he was driven mad by a love potion, and when, during the intervals of his insanity, he had written a number of books, which were later emended by Cicero, he killed himself by his own hand in the 44th year of his life." The claim that he was driven mad by a love potion, although defended by such scholars as Reale and Catan, often is dismissed as the result of historical confusion, or anti-Epicurean slander. Jerome's image of Lucretius as a lovesick, mad poet continued to have significant influence on modern scholarship until quite recently, although it now is accepted that such a report is inaccurate. Similarly, the statement that Cicero emended (Latin: "emendavit") the work prior to publication is doubtful. The exact date of his birth varies by manuscript; in most it is recorded under 94 BC, but in others under 93 or 96. Lucretius (a materialist writer) and Jerome (a Christian priest) wrote for opposing purposes, and whether or not Jerome attempted to disparage Lucretius's work as the work of a madman is an open question.
It is impossible to estimate the credibility of the accounts of Donatus and Jerome since they wrote long after the poet's death, moreover the latter author belonged to a theological tradition explicitly hostile to Epicureanism, and the sources of their comments are unknown. If 55 BC is Lucretius's most likely year of death, however, and if Jerome is accurate about Lucretius's age (43) when he died, it can then be concluded he was born in 99 or 98 BC. Less specific estimates place the birth of Lucretius in the 90s BC and death in the 50s BC, in agreement with the poem's many allusions to the tumultuous state of political affairs in Rome and its civil strife.
"De rerum natura".
His poem "De rerum natura" (usually translated as "On the Nature of Things" or "On the Nature of the Universe") transmits the ideas of Epicureanism, which includes Atomism, and psychology. Lucretius was the first writer to introduce Roman readers to Epicurean philosophy. The poem, written in some 7,400 dactylic hexameters, is divided into six untitled books, and explores Epicurean physics through richly poetic language and metaphors. Lucretius presents the principles of atomism; the nature of the mind and soul; explanations of sensation and thought; the development of the world and its phenomena; and explains a variety of celestial and terrestrial phenomena. The universe described in the poem operates according to these physical principles, guided by "fortuna", "chance", and not the divine intervention of the traditional Roman deities.
Bibliography.
Editions
Commentary

</doc>
<doc id="37146" url="http://en.wikipedia.org/wiki?curid=37146" title="New Sweden">
New Sweden

New Sweden (Swedish: "Nya Sverige", Finnish: "Uusi Ruotsi", Latin: "Nova Svecia") was a Swedish colony along the lower reaches of Delaware River in North America from 1638 to 1655 in the present-day American Mid-Atlantic states of Delaware, New Jersey, and Pennsylvania. Fort Christina, now in Wilmington, Delaware, was the first settlement. Along with Swedes and Finns, a number of the settlers were Dutch. New Sweden was conquered by the Dutch in 1655, during the Second Northern War, and incorporated into New Netherland.
History.
By the middle of the 17th century the Realm of Sweden had reached its greatest territorial extent and was one of the great powers of Europe. Sweden then included Finland and Estonia along with parts of modern Russia, Poland, Germany and Latvia, under King Gustavus Adolphus and later Christina, Queen of Sweden. The Swedes sought to expand their influence by creating an agricultural (tobacco) and fur-trading colony to bypass French and English merchants.
The Swedish West India Company began with a mandate to establish colonies between Florida and Newfoundland for the purposes of trade, particularly concentrated in the Delaware River. Its chartered included Swedish, Dutch, and German stockholders and led by directors of the New Sweden Company, including Samuel Blommaert. The Company sponsored 11 expeditions made up of 14 separate voyages (two did not survive) to Delaware between 1638 and 1655.
The first Swedish expedition to North America embarked from the port of Gothenburg in late 1637. It was organized and overseen by Clas Fleming, a Swedish Admiral from Finland. A Dutchman, Samuel Blommaert, assisted the fitting-out and appointed Peter Minuit (the former Governor of New Amsterdam) to lead the expedition. The members of the expedition, aboard the ships "Fogel Grip" and "Kalmar Nyckel", sailed into Delaware Bay, which lay within the territory claimed by the Dutch, passing Cape May and Cape Henlopen in late March 1638, and anchored at a rocky point on the Minquas Kill that is known today as Swedes' Landing on March 29, 1638. They built a fort on the present site of the city of Wilmington, which they named Fort Christina, after Queen Christina of Sweden.
In the following years, 600 Swedes and Finns, the latter group mainly Forest Finns from central Sweden, and also a number of Dutchmen and Germans in Swedish service, settled in the area. Peter Minuit was to become the first governor of the newly established colony of New Sweden. Having been the Director of the Dutch West India Company, and the predecessor of then-Director William Kieft, Minuit knew the status of the lands on either side of the Delaware River at that time. He knew that the Dutch had established deeds for the lands east of the river (New Jersey), but not for the lands to the west (Maryland, Delaware, and Pennsylvania).
Minuit made good on his appointment by landing on the west bank of the river and gathered the sachems of the local Delaware tribe. Sachems of the Susquehannocks were also present. They held a conclave in his cabin on the "Kalmar Nyckel", and he persuaded the sachems to sign deeds he had prepared for the purpose to solve any issue with the Dutch. The Swedes claimed the section of land purchased included the land on the west side of the South River from just below the Schuylkill, in other words, today's Philadelphia, Pennsylvania, southeastern Pennsylvania, Delaware, and coastal Maryland. The Delaware sachem Mattahoon, who was one of the participants, later stated that only as much land as was contained within an area marked by "six trees" was purchased and that the rest of the land occupied by the Swedes was stolen.
Director Willem Kieft objected to the landing of the Swedes, but Minuit ignored him, since he knew that the Dutch were militarily impotent at the moment. Minuit finished Fort Christina during 1638, then departed for Stockholm for a second group. He made a side trip to the Caribbean to pick up a shipment of tobacco for resale in Europe to make the voyage profitable. Minuit died on this voyage during a hurricane at St. Christopher in the Caribbean.
The official duties of the first governor of New Sweden were carried out by Lieutenant (promoted to Captain) Måns Nilsson Kling, until a new governor was chosen and brought from Sweden two years later.
Under Johan Björnsson Printz, governor from 1643 to 1653, the company expanded along the river from Fort Christina, establishing Fort Nya Elfsborg on the east bank of the Delaware near present-day Salem, New Jersey and Fort Nya Gothenborg on Tinicum Island (to the immediate southwest of today's Philadelphia), where he also built his manor house, The Printzhof. The Swedish colony prospered at first. In 1644, New Sweden supported the Susquehannocks in their victory in a war against the English in the Province of Maryland. In May 1654, the Dutch Fort Casimir was captured by soldiers from the New Sweden colony led by governor Johan Risingh. Fort Casimir was renamed Fort Trinity (in Swedish, "Trefaldigheten").
Soon after Sweden opened the Second Northern War in the Baltic by attacking the Polish-Lithuanian Commonwealth, the Dutch moved to take advantage and an armed squadron of ships under the direction of Director-General Peter Stuyvesant seized New Sweden. The Dutch moved an army to the Delaware River in the summer of 1655, easily capturing Fort Trinity and Fort Christina. The Swedish settlement was incorporated into Dutch New Netherland on September 15, 1655. At first the Swedish and Finnish settlers continued to enjoy local autonomy. They kept their own militia, religion, court, and lands.
This status lasted officially until the English conquest of the New Netherland colony was launched on June 24, 1664. The Duke of York sold the area that is today New Jersey to John Berkeley and George Carteret for a proprietary colony, separate from the projected New York. The actual invasion started on August 29, 1664, with the capture of New Amsterdam. The invasion ended with the capture of Fort Casimir (New Castle, Delaware) in October 1664. The invasion was conducted at the start of the Second Anglo-Dutch War.
The status continued unofficially until these lands were included in William Penn's charter for Pennsylvania, on August 24, 1682. During this later period some immigration and expansion continued. The first settlement at Wicaco, a Swedish settlers' log blockhouse located below Society Hill, was built in present-day Philadelphia in 1669. It was later used as a church until about 1700, when Gloria Dei (Old Swedes') Church of Philadelphia was built on the site.
Hoarkill, New Amstel, and Upland.
The start of the Third Anglo-Dutch War resulted in the recapture of New Netherland by the Dutch in August 1673. The Dutch restored the status that pre-dated the English invasion, and codified it in the establishment of three counties in what had been New Sweden. They were Hoarkill County, which today is Sussex County, Delaware; New Amstel County, which is today New Castle County, Delaware; and Upland County, which was later partitioned between New Castle County, Delaware and the new Colony of Pennsylvania. The three counties were created on September 12, 1673, the first two on the west shore of the Delaware River, and the third on both sides of the river.
The signing of the Treaty of Westminster of 1674 ended the Dutch effort, and required them to return all of New Netherland to the English, including the three counties they created. That handover took place on June 29, 1674.
After taking stock, the English declared on November 11, 1674, that settlements on the west side of the Delaware River and Delaware Bay (in present day Delaware and Pennsylvania) were to be dependent on the Colony of New York, including the three Counties. This declaration was followed on November 11 by a new declaration that renamed New Amstel as New Castle. The other counties retained their Dutch names for the duration.
The next step in the assimilation of New Sweden into New York was the extension of the Duke's laws into the region. This took place on September 22, 1676. This was followed by the partitioning of the Counties to conform to the borders of Pennsylvania and Delaware.
The first move was to partition Upland between Delaware and Pennsylvania, with most of the Delaware portion going to New Castle County. This was accomplished on November 12, 1678. The remainder of Upland continued in place under the same name.
On June 21, 1680, New Castle and Hoarkill Counties were partitioned to produce St. Jones County.
On March 4, 1681, what had been the colony of New Sweden was formally partitioned into the colonies of Delaware and Pennsylvania. The border was established 12 miles north of New Castle, and the northern limit of Pennsylvania was set at 42 degrees north latitude. The eastern limit was the current border with New Jersey at the Delaware River, while the western limit was undefined. Pennsylvania immediately started to reorganize the lands of the former New Sweden within the limits of Pennsylvania. In June 1681, Upland ceased to exist as the result of the reorganization of the Colony of Pennsylvania, with the Upland government becoming the government of Chester County, Pennsylvania.
On August 24, 1682, the Duke of York transferred the western Delaware River region, including modern-day Delaware, to William Penn, thus transferring Deale and St. Jones from New York to Delaware. St. Jones County was renamed as Kent County; Deale County was renamed Sussex County; New Castle County retained its name.
Significance and legacy.
The historian H. Arnold Barton has suggested that the greatest significance of New Sweden was the strong and long-lasting interest in North America that the colony generated in Sweden. Major Swedish immigration to the United States did not occur until the late 19th century, however. From 1870 to 1910, over one million Swedes arrived, settling particularly in Minnesota and other states of the Upper Midwest (see Swedish American).
Traces of New Sweden persist in the lower Delaware Valley to this day, including Holy Trinity Church in Wilmington, Gloria Dei Church and St. James Kingsessing Church in Philadelphia, Trinity Episcopal Church in Swedesboro, New Jersey, and Christ Church (est. 1760) in Swedesburg, Pennsylvania, all commonly known as "Old Swedes' Church". Christiana, Delaware, is one of the few settlements in the area with a Swedish name. Swedesford Road is still found in Chester and Montgomery Counties, Pennsylvania, although Swedesford has long since become Norristown. The American Swedish Historical Museum, located in FDR Park in South Philadelphia, houses many exhibits, documents and artifacts from the New Sweden colony.
Perhaps the greatest contribution of New Sweden to the development of the New World is one that is the traditional Finnish forest house building technique. The colonists brought with them the log cabin, which became such an icon of the American frontier that it is thought of as an American structure. The C. A. Nothnagle Log House on Swedesboro-Paulsboro Road in Gibbstown, New Jersey, is one of the oldest surviving log houses in the United States.
Finnish influence.
The colonists came from all over the Swedish realm. The percentage of Finns in New Sweden grew especially towards the end of the colonization, comprising 22% of the population during Swedish rule, but rising to about 50% after the colony came under Dutch rule. The year 1664 saw the arrival of a contingent of 140 Finns. In 1655, when the ship "Mercurius" sailed to the colony, 92 of the 106 passengers were listed as Finns. Memory of the early Finnish settlement lived on in place names near the Delaware River such as Finland (Marcus Hook), Torne, Lapland, Finns Point and Mullica Hill and Mullica River.
A portion of these Finns were known as Forest Finns, people of Finnish descent living in the forest areas of Central Sweden. The Forest Finns had moved from Savonia in Eastern Finland to Dalarna, Bergslagen and other provinces in central Sweden during the late-16th and early-to-mid-17th centuries. Their relocation had started as part of an effort by Swedish king Gustav Vasa, to expand agriculture to these uninhabited parts of the country. The Finns in Savonia traditionally farmed with a slash-and-burn method which was better suited to pioneering agriculture in vast forest areas. This was also the farming method used by the Native Americans of Delaware.

</doc>
<doc id="37147" url="http://en.wikipedia.org/wiki?curid=37147" title="Tilburg">
Tilburg

Tilburg ], often referred to as Tilly, is a landlocked municipality and a city in the Netherlands, located in the southern province of Noord-Brabant.
Tilburg municipality also includes the villages of Berkel-Enschot and Udenhout. With a population of , it is the second largest city of Noord-Brabant, and the sixth largest city of the Netherlands. Its metropolitan area, which includes Goirle, consists of inhabitants.
Tilburg University is located in Tilburg, as are Avans University of Applied Sciences and Fontys University of Applied Sciences.
Tilburg is known for its 10-day-long funfair, the largest in the Benelux, held in July each year. The Monday during the funfair is called "Roze Maandag" (Pink Monday), and is primarily gay-oriented, though also enjoyed by many heterosexuals.
There are three railway stations within the municipality: Tilburg, Tilburg Universiteit and Tilburg Reeshof. The 75-hectare "Spoorzone" area around Tilburg Central station used to be owned by Dutch Railways as one of its national train maintenance yards. It has recently been purchased by the city and is currently being transformed into a happening urban zone, which should become an integral part of the inner city.
History.
Little is known about the beginnings of Tilburg. The name "Tilburg" first appeared in documents dating from 709 AD but after that there was no mention for several centuries. In the later Middle Ages, Tilburg referred to a region rather than a particular town or village; its population was largely in a couple of hamlets, one of which was known as "Eastern Tilburg" ("Oost-Tilburg"), which was later reflected in the name of "Oisterwijk" ("Eastern Quarter"). This village centred around a small (probably wooden) castle or "Motteburcht" on an equally small hill, which became derelict and was torn down after a few centuries at most. Of this first "Tilburg Castle", nothing remained c. 2000, except for a few remnants of its moat in the suburbs of Oisterwijk. In the 14th century, Tilburg was proclaimed a manor; together with Goirle, it acquired the title of "The Manor of Tilburg and Goirle".
Successively, the manorial rights fell into the hands of several lords of noble lineage. They derived their income from taxes, fines and interest paid by the villagers.
In the 15th century, one of the lords of Tilburg, Jan van Haestrecht, built Tilburg Castle. "That stone chamber at Hasselt" is mentioned in several historical documents. In 1858, however, the castle was pulled down to make way for a factory, but the name lives on, in the city arms and logo. A replica of the foundations of the castle was restored in ca. 1995 in its original location, after the factory was demolished. In 1803, Goirle was separated from Tilburg and on 18 April 1809, Tilburg was granted city status. In that year, it had about 9,000 inhabitants. In 2009 Tilburg hosted several festivities in celebration of 200 years as a city. Because of the 4 apples.
Wool capital of the Netherlands.
Tilburg grew around one of the so-called "herd places", triangular plots where a number of roads (usually sand roads) met. These herd places were collective pasturelands for flocks of sheep. Their shape is still reflected in the layout of many places in Tilburg. Many districts, including Korvel, Oerle, Broekhoven, Hasselt, Heikant, De Schans, and Heuvel, bear the names of these old hamlets. The poor farmers living in these hamlets soon decided not to sell the wool from their sheep but to weave it themselves, and for a long time, much of the space inside their small houses was occupied by a loom—by the 17th century these numbered about 300. Enterprising people saw their chance. As so-called "drapers" they supplied the weavers with the raw materials for their "home working", and the first Tilburg "mill houses" came into existence. From then on, the wool industry underwent rapid growth, and in 1881 Tilburg had as many as 145 woollen mills. Home weaving continued, however, until the early 20th century. Woollen textiles from Tilburg were known far and wide. After the World War II Tilburg retained its place as wool capital of the Netherlands, but in the 1960s the industry collapsed and by the 1980s the number of wool mills could be counted on the fingers of one hand. Present-day Tilburg industry consists of a wide variety of enterprises. The main economic sector has become transport and logistics with a variety of industry as a close second.
Urban Renewal.
At the same time as the wool industry collapsed, Cees Becht was the mayor of Tilburg. While he was in office, many buildings were destroyed, including some very precious monuments. The neighbourhood Koningswei (King's Meadows) was demolished and replaced by Koningsplein (King's Square). The old neighbourhood was some kind of slum and had to be replaced by newer development. The newer development, however, wasn't as successful as was expected, and the square feels abandoned most of the year.
Considered even worse was the demolition of the old city hall. This classicistic-styled building was a national-registered monument, but even that didn't prevent Becht's plans to demolish it to build the nine-storey, modern-day, black complex. A part of the empty area was used to build the system of the inner "Cityring".
Another building that was demolished was the old railway station, which was replaced due to "Hoogspoor" (literally: high rails), a project bringing the railway on viaducts to reduce traffic congestion in the years around 1960. The century-old station building was replaced by the modern one.
Because of all of this and some more parts of Tilburg, Cees Becht gained the dubious nickname "Cees the Sloper" (Cees the demolisher)
Modern history.
In the 1980s, many locations, formerly occupied by wool factories had been filled with small-scale housing projects. This mostly happened when Henk Letschert was mayor of Tilburg.
The "Heuvel", one of the important squares, had its own lime tree until 27 April 1994, being chopped for a bicycle parking basement. The cut-down led to many protests, because the tree was still healthy. After the Pieter Vreedeplein reconstruction, plans were made to plant a descendant of the original lime tree. Three were placed, only one of them survived. The last living tree was moved to another location again, but died shortly after. As of 23 November 2011, no more descendants have been placed. The current one is just another lime tree.
In the 1990s Tilburg developed a modern skyline. Because of new policy three buildings were build, which are considered skyscrapers in the Netherlands. These are the Interpolis headquarters, the Westpoint Tower and StadsHeer. The Westpoint Tower has an altitude of 143,1 meters and was shortly the tallest residential tower in The Netherlands, until the Montevideo in Rotterdam surpassed it. De 'StadsHeer' is the third one and is part of the 'Haestrechtkwartier' (Haestrecht quarter). The residential tower is criticized for its cubic balconies taped onto the building. Therefore, the tower is nicknamed "De Vogelkooikes" (Bird cages).
King William II.
King William II (1792–1849) always bore a warm heart towards Tilburg. "Here I can breathe freely and I feel happy", he once said about the town, and he acted accordingly. King William II always supported Tilburg—he provided money to improve the sheep breeding, built new farms and founded a cavalry barracks on the St. Joseph Street, now a monumental building of the City Archives. Although the King was always made welcome by the manufacturers he had befriended, he needed his own residence in Tilburg, and commissioned the construction of a palace, which would function as his country residence. Construction started in 1847 and was completed just days before William II died, in 1849. It is now part of Tilburg City Hall. In 1987 an obelisk was erected nearby, in memory of King William II. It replaced the old "needle" dating from 1874, which was removed from the street in 1968. After its restoration, William II's statue has got a place again in the heart of the city, where he felt happy among its inhabitants. The local football club Willem II Tilburg was named after the king.
Topography.
Tilburg Centrum.
Tilburg Centrum is the downtown of Tilburg, and is situated between (clockwise) the Spoorlaan, Heuvelring, Paleisring, Schouwburgring and Noordhoekring, which is the same as the order of the one-way roads around the district. The district has 6.331 inhabitants, and most of the shops, hotels, restaurants and cafes of the city. In 2008, the reconstructed Pieter Vreedeplein was opened to public, creating more shopping area in the city which had not the shopping possibilities as similar-sized cities in the Netherlands. Two smaller cinemas were replaced by a bigger one on the Pieter Vreedeplein in 2007. Despite being called "Centrum", the district is lying far southeast of the geographical center. The district is connected by Tilburg railway station.
Oud-Noord.
Oud-Noord is situated north of the railway that crosses Tilburg, and between the "Ringbanen" (Ring roads around the city center). The district has 31.649 inhabitants. Contemporary arts museum "De Pont" is located within the district. When the railway marshalling yard belonging to the Nederlandse Spoorwegen became obsolete, a considerable stretch of the railway across the city, the "Spoorzone", became an urban renewal project. New premises for two courses run by Fontys University of Applied Sciences will be located here, as will Tilburg's new central library, replacing the library in Koningsplein The railway yard is the largest area, though more areas along the railway will be reconstructed.
Oud-Zuid.
Oud-Zuid is a district south, and also west and east of downtown Tilburg. The district has 38.659 inhabitants. As of 2012, all the 'skyscrapers' of Tilburg, higher than 100 m are located within the district. The "Hart van Brabantlaan" is almost surrounded by high buildings like Westpoint Tower and the StadsHeer as a small part of the urban renewal. This area along the railway is partly located in "Oud-Zuid". Many important locations in Tilburg are located within the district, just out of the center, such as 013 music venue and the Schouwburg built in 1961. Also the Koningsplein with the main library and the Piushaven are located within the district. Old herd places include Korvel, Broekhoven and Oerle.
Noord.
Tilburg-Noord is located north of the Wilhelminakanaal. The district has 22.763 inhabitants. Tilburg-Noord is built in the period 1966–1974. Therefore, it has many apartment buildings up to 16 floors, drive-in houses, green strips and industrial development. The streets in this district are mostly named after musicians from the renaissance up to pop artists from the 1960s. The main shopping center is Wagnerplein, while there's also the Verdiplein in Stokhasselt. The one at the Tartinistraat became defunct. Before the district was built, it mainly was agricultural area with some villages. The most notable was Heikant, which is still the name of the biggest neighbourhood. Its former village square including the old church is still present. The northernmost part of the district is still agricultural with some forests. In this agricultural area, the blessed Peter Donders was born, there still stands a chapel and a procession park.
Oost.
Tilburg-oost consists of primarily industrial development. Residential neighbourhoods are in a small strip east of the Ringbaan Oost rather than the whole district, however, is not considered as a part of the city center. The district only has 770 inhabitants.
Zuid.
Tilburg-Zuid is located between the A58 motorway and the Ringbaan Zuid, and is the southernmost district. Tilburg-zuid has 14.836 inhabitants. The district contains two neighbourhoods and many businesses. The football club Willem II is located within the district, as well as the ice-skating rink with a speed skating rink, the Ireen Wüst IJsbaan, is located here. Next to the Ice Skating rink, there's a Multiplex cinema with twelve screens and there's a large campus where the most courses of Fontys in Tilburg are located. The district contains a hospital (St. Elisabeth hospital) and has one of the largest parks of Tilburg, the Leijpark.
West.
Tilburg-west was mostly built after WWII, and has 29.611 inhabitants. The district with its neighbourhoods consist mostly of small brick houses and apartment buildings, except for Zorgvlied, which contains more expensive, free-standing houses. The Westermarkt is the largest shopping center out of the inner city. Many higher educational buildings are standing here, like as the Tilburg University and Avans Hogeschool. Another place of many schools is along the Reitse Hoevenstraat with multiple secondary schools. The district is connected by train with the Tilburg Universiteit railway station and has one of the two hospitals in Tilburg (TweeSteden ziekenhuis). The largest mosque of Tilburg, the Turkish Süleymaniye-Mosque built in 2001, stands in the southeastern corner of the district. West is surrounded by forests like "Wandelbos" and the "Oude Warande", located west of the university.
Reeshof.
The Reeshof is the westernmost district and the most recent expansion of the city of Tilburg proper. and has a population of 42.696 inhabitants. Because of this, the Reeshof became the largest district of Tilburg. The first houses were completed in 1980, in the neighbourhood Gesworen Hoek. As of 2012, the last neighbourhood (Koolhoven Buiten) is under construction. The district is connected by Tilburg Reeshof railway station and multiple roads that encircle the district plus the industrial development Vossenberg north of the Wilhelminakanaal.
The Donge runs through the district, including the nature development with some Highland cattle grazing between the fences protecting the surrounding neighbourhoods. This small-scale nature project is called the Dongevallei, which literally means Donge Valley in English.
Demographics.
Ethnic makeup.
As of 1-1-2011, the population of Tilburg consisted of 206,234 persons. According to the Tilburg city council, the city will reach a population of 217,000 inhabitants the year 2025. Of these, 23,3% or some 47,964 people are of foreign descent. People are classified as being of foreign descent when they were born outside of the Netherlands, or when at least one of their parents was born outside of the Netherlands.
Religions.
The Tilburg agglomeration has the following religious makeup as of 2003:
Geography.
Climate.
Tilburg experiences an oceanic climate (Köppen climate classification "Cfb") similar to almost all of the Netherlands. The most notable fact from Western Brabant is that there are more thunderstorms than anywhere else in the Netherlands, up to 31 days a year.
Economy.
The economy was concentrated on wool industry for centuries, however, since the 1960s, Tilburg made more progress in having different kinds of industries, supported by the government to prevent the city from poverty after the decline of wool industry. Chemical company IFF has a factory in Tilburg. In the 1980s, the Japanese company Fujifilm came in Tilburg. Insurance companies like Interpolis and CZ are headquartered in Tilburg, as well as transportation/distribution industries situated in Tilburg for being the geographical center of the Benelux-countries. Iris Ohyama has its European offices in Tilburg.
Education.
Tilburg University.
Higher education is of significant importance, with Tilburg University attracting scholars from all over the world. It has a student population of about 13,000 students, about 8 per cent of whom are international students. This percentage has steadily increased over the past years. TiU offers both Dutch-taught and English-taught programmes.
The institution has gained a reputation in both research and education. In the field of economics, the Faculty of Economics and Business Administration ranked #1 in Europe for the second consecutive time in 2007 according to the Journal of the European Economic Association with regard to publications in top journals.
In 2007 the Executive MBA program at the university's TiasNimbas Business School ranked # 11 in the world according to the Financial Times. In the field of law, Tilburg University was ranked #1 in the Netherlands for the last three years according to Elsevier Magazine.
Culture and recreation.
Tilburg is a pilot city of the Council of Europe and the EU Intercultural cities programme.
Beverages.
"Schrobbelèr" is a local liquour. It has an alcoholic percentage of 21.5%, slightly lower than most bitters and has a relatively sweet flavour. The drink is sold in a stone jar and is drunk cold from own glass, a high and tiny chalice glass, larger than a Jägermeister glass 
The drink originated in 1973 when Tilburgian entrepreneur Jan Wassing started experimenting with a drink with lower alcoholic percentage that was appropriate for his stomach. The result was successful. The drink is distilled now at Loven industrial area in Tilburg by the Eindhoven company Schrobbeler Ltd, without the è on the last vowel. The drink is especially drank at Carnival. The name is derived from the profession of 'Schrobbelaar', in the textile industry in Tilburg. The profession was unskilled and had a low wage.
Another known drink from Tilburg is Peerke's Nat, which has a higher alcoholic percentage than Schrobbelèr (25%) and is introduced at the beatification of Peter Donders (locally named Peerke). The drink is sold in bottles of 70 centiliters.
The Koningshoeven Brewery brews trappist beer and is the only brewery in the Netherlands who does so. It was founded in 1884.
Open air art.
Tilburg has some notable art in the city, mostly supported by KORT (Kunst in Open Ruimte Tilburg, Dutch for Art in Open Space Tilburg). The most notorious example is the turning house on the Hasseltrotonde, a roundabout, mostly being criticised for being 'no art' and 'waste of money'. However, the house was erected in 2008. Except for being responsible for newer, modern art, KORT also gives information about older works of art, like the Willem II statue on the Heuvel.
Festival city, music.
The city of Tilburg hosts many festivals, such as Incubate, "Festival Mundial" (world culture), Stranger Than Paranoia (jazz), Tilburg Students Festival, and Roadburn Festival. "013" is a modern pop-centre where artists like Nick Cave, Jerry Lee Lewis, and GZA performed. Paradox is a club for experimental jazz and improvised music. Fontys University of Applied Sciences started a pop-academy in the beginning of the 21st century, and students often perform on local stages.
Museums.
Tilburg has an outstanding museum of Modern Art – De Pont Foundation. There is a large textile museum, offering not only a historical view in its former factory, but also a laboratorium for design, production and development of textile as a material. Another museum is Noordbrabants Natuurmuseum.
Parks and forests.
A lot of parks and forests provide people from Tilburg area for recreation. The Leijpark and the Reeshofpark are the largest among the parks in Tilburg. The Leijpark is famous for Festival Mundial and lies next to the St. Elisabeth hospital and a monastery, the Cenakel. The Reeshofpark is created in the late 1990s, including some restaurants opened in 2011. Some older parks include the Wilhelminapark in Oud-Noord, is built on the square of the former herd place Veldhoven. Tilburg offers, in comparison to other top-ten cities in the Netherlands the most forest area. In the municipality, Tilburg has the Wandelbos, a forest south of the similarly named neighbourhood in Tilburg-West, the Oude Warande, the Kaaistoep, a forest of 4.5km2, and partially, Huis Ter Heide in the northwest of Tilburg, a 6.5km2-sized natural redevelopment area. Out of the municipality, there's a national park called Loonse en Drunense Duinen which includes dunes of drift sand from the west coast.
Sports.
The local football team is Willem II, named in remembrance of King William II.
Tilburg Ten Miles is an annual road running competition held in Tilburg.
Students sports like rowing and hockey are popular as well. Tilburg hosts three field hockey clubs that play in top national leagues.
Tilburg has an ice skating rink, including the speed skating rink Ireen Wüst IJsbaan of 400m. Within the speed skating rink there's an ice hockey field. The Hockey team Tilburg Trappers also does well in the Eredivisie (Dutch Premier League).
Transport.
Tilburg has three railway stations: Tilburg (Centraal), Tilburg Universiteit and Tilburg Reeshof. The third of them, Station Tilburg Reeshof was built to connect the then-latest district of Tilburg, the Reeshof. Intercity trains only stop at Tilburg (centraal). The name of Tilburg Universiteit Station was from its construction in 1968 to December 2010 Tilburg West, however, after 40 years, it was not the westernmost station anymore. A fourth railway station is planned for Berkel-Enschot, also in the municipality of Tilburg and more getting absorbed by Tilburg. In the past, until 1938, Berkel-Enschot had already its own train station. Udenhout, lying further northeast in the municipality, also had its train station until 1938. Both stations are on the line to 's-Hertogenbosch.
The Tilburg city- and local buses are operated by Veolia Transport Nederland. The city experimented from 2005 to 2008 with free public transport for children and 55+-people. Before Veolia took over the bus network, it was operated by BBA (abbreviation for Brabants(ch)e Buurtspoorwegen en Autobussen).
Tilburg has an extensive bicycle path network called Sternet-Routes. The first bicycle path of this network was built between the city center and the university in 1975. From the mid-1990s, multiple bicycle paths (rather than lanes along the road) have been built. Since most of these have been paved by tiles, there is an increasing call for asphalt-paved paths. For this network of bicycle paths, there are built some new tunnels; under the railway that crosses the city.
Tilburg is, at variance from other Dutch cities of a similar size, connected by only one national motorway, the A58 / E312 (to Breda and Eindhoven). An outer beltway, consisting of two provincial 2x2-roads and the A58, has been finished in May 2012. Although the outer beltway is fully navigable, the Burgemeester Bechtweg, which was built initially as a two-lane (one per direction) road, will be finished in 2013. Two other routes are of considerable importance for Tilburg: the A261/N261 to Waalwijk and the A65/N65 to 's-Hertogenbosch. Neither is a complete motorway, and both experience bottlenecks. Various plans exist to build both to higher standards, with the N261 improved in 2015.
International relations.
Twin towns — Sister cities.
Tilburg is twinned with:

</doc>
<doc id="37149" url="http://en.wikipedia.org/wiki?curid=37149" title="Cranial nerves">
Cranial nerves

Cranial nerves are nerves emerging directly from the brain, which is in contrast to spinal nerves (which emerge from various segments of the spinal cord). Cranial nerves exchange information between the brain and parts of the body, primarily to and from regions of the head and neck.
Spinal nerves emerge regularly from the spinal cord with the nerve closest to the head emerging in the space above the first cervical vertebra with the nerve C1, with the cranial nerves filling corresponding roles above this level. Each cranial nerve is paired and is present on both sides. Depending on definition there are in humans twelve or thirteen cranial nerves pairs, which are assigned Roman numerals I–XII, and zero assigned to cranial nerve zero, (or the "terminal nerve"). Their number is based on the order in which they emerge from the brain, front to back (brainstem).
The terminal nerves, olfactory nerves (I) and optic nerves (II) emerge from the cerebrum or forebrain, and the remaining ten pairs arise from the brainstem.
The cranial nerves are considered components of the peripheral nervous system (PNS), although on a structural level the olfactory, optic and terminal nerves are more accurately considered part of the central nervous system (CNS).
Anatomy.
Traditionally, humans are considered to have twelve pairs of cranial nerves, which are numbered I–XII. They are the following: the olfactory nerve (I), the optic nerve (II),oculomotor nerve (III), trochlear nerve (IV), trigeminal nerve (V), abducens nerve (VI), facial nerve (VII), vestibulocochlear nerve (VIII), glossopharyngeal nerve (IX), vagus nerve (X), accessory nerve (XI), and hypoglossal nerve (XII). There may be a thirteenth cranial nerve, the terminal nerve, which is very small and may or may not be functional in humans
Terminology.
Cranial nerves are generally named according to their structure or function. For example, the olfactory nerve (I) supplies smell, and the facial nerve (VII) supplies motor innervation to the face. As Latin was the "lingua franca" of the study of Anatomy when the nerves were first documented, recorded, and discussed, many nerves maintain Latin or Greek names, including the trochlear nerve (IV), named according to its structure, as it supplies a muscle that attaches to a pulley (Greek: trochlea), the trigeminal nerve (V) named according to its three heads (Latin: "tri-geminus" meaning triplets), and the vagus nerve (X), named for its wandering course (Latin: "vagus").
Cranial nerves are numbered based on their rostral-caudal (front-back) position, as, when viewing the brain from below. If the brain is carefully removed from the skull the nerves are typically visible in their numeric order.
Cranial nerves have paths within and outside of the skull. The paths are within the skull are called "intracranial" and the paths outside the skull are called "extracranial". There are many holes in the skull called "foramina" by which the nerves pass to exit the skull. All cranial nerves are "paired", which means that they occur on both the right and left sides. If a nerve supplies a muscle, skin, or has another function on the same side of the body as where it originates, this is called an "ipsilateral" course. If the course is opposite to the nucleus of the nerve, this is known as a "contralateral" course.
Intracranial course.
Nuclei.
The cell bodies of many of the cells of most of the cranial nerves are contained in one or more nuclei in the brainstem. These nuclei are important relative to cranial nerve dysfunction because damage to these nuclei such as from a stroke or trauma can mimic damage to one or more branches of a cranial nerve. In terms of specific cranial nerve nuclei, the midbrain of the brainstem has the nuclei of the oculomotor nerve (III) and trochlear nerve (IV); the pons has the nuclei of the trigeminal nerve (V), abducens nerve (VI), facial nerve (VII) and vestibulocochlear nerve (VIII); and the medulla has the nuclei of the glossopharyngeal nerve (IX), vagus nerve (X), accessory nerve (XI) and hypoglossal nerve (XII). The fibers of these cranial nerves exit the brainstem from these nuclei. 
Ganglia.
Some of the cranial nerves have sensory or parasympathetic ganglia or, collections of cell bodies of neurons, which are located outside of the brain (but can be inside or outside of the skull). 
The sensory ganglia are directly correspondent to dorsal root ganglia of spinal nerves and are known as cranial sensory ganglia. Sensory ganglia exist for nerves with sensory function: V, VII, VIII, IX, X. There are also parasympathetic ganglia, which are part of the autonomic nervous system for cranial nerves III, VII, IX and X. 
Exiting the skull and Extracranial Course.
After emerging from the brain, the cranial nerves travel within the skull, and some must leave this bony compartment in order to reach their destinations. Often the nerves pass through holes in the skull, called foramina, as they travel to their destinations. Other nerves pass through bony canals, longer pathways enclosed by bone. These foramina and canals may contain more than one cranial nerve, and may also contain additional blood vessels.
Extracranial course (images).
The following images show the cranial nerves schematically showing their respective exits from the CNS or brain-stem (not including the optic nerve, which, being part of the CNS, does not leave it), and their path, as well as conceptual innervation targets.
Function.
The cranial nerves provide motor and sensory innervation mainly to the structures within the head and neck. The sensory innervation includes both "general" sensation such temperature and touch, and "special" innervation such as such as taste, vision, smell, balance and hearing 
The vagus nerve (X) provides sensory and autonomic (parasympatheic) motor innervation to structures in the neck and also to the abdominal organs (though not pelvic), and thoracic organs. 
Smell (I).
The olfactory nerve (I) conveys the sense of smell.
Damage to the olfactory nerve (I) can cause an inability to smell (anosmia), a distortion in the sense of smell (parosmia), or a distortion or lack of taste. If there is suspicion of a change in the sense of smell, each nostril is tested with substances of known odors such as coffee or soap. Intensely smelling substances, for example ammonia, may lead to the activation of pain receptors (nociceptors) of the trigeminal nerve that are located in the nasal cavity.
Vision (II).
The optic nerve (II) transmits visual information.
Damage to the optic nerve (II) affects vision. Vision is affected depending on the location of the lesion. A person may not be able to see things on their left or right side (homonymous hemianopsia), or may have difficulty seeing things on their outer visual fields (bitemporal hemianopsia) if the optic chiasm is involved.:82 Vision may be tested using a number of different tests, examining the visual field, or by examining the retina with an ophthalmoscope, using a process known as funduscopy. Visual field testing may be used to pin-point structural lesions in optic nerve, or further along the visual pathways.
Eye movement (III, IV, VI).
The oculomotor nerve (III), trochlear nerve (IV) and abducens nerve (VI) coordinate eye movement.
Damage to nerves III, IV, or VI may affect the movement of the eyeball (globe). Both or one eye may be affected; in either case double vision (diplopia) will likely occur because the movements of the eyes are no longer synchronized. Nerves III, IV and VI are tested by observing how the eye follows an object in different directions. This object may be a finger or a pin, and may be moved at different directions to test for pursuit velocity. If the eyes do not work together, the most likely cause is damage to a specific cranial nerve or nuclei.
Damage to the oculomotor nerve (III) can cause double vision (diplopia) with lateral strabismus, also eyelid drooping (ptosis) and pupil dilation (mydriasis).:84 Lesions may also lead to inability to open the eye due to paralysis of the levator palpebrae muscle. Individuals suffering from lesion to the oculomotor nerve may compensate by tilting their heads to alleviate symptoms due to paralysis of one or more of the eye muscles it controls.
Damage to the trochlear nerve (IV) can also cause diplopia with the eye adducted and elevated.:84 The result will be an eye which can not move downwards or inwards properly (especially downwards when in an inward position). This is due to impairment in the superior oblique muscle, which is innervated by the trochlear nerve.
Damage to the abducens nerve (VI) can also result in diplopia.:84 This is due to impairment in the lateral rectus muscle, which is innervated by the abducens nerve.
Facial sensation (V).
The trigeeminal nerve (V) provides sensation to the skin of the face and also controls the muscles of mastication (chewing). Conditions affecting the trigeminal nerve (V) include trigeminal neuralgia, cluster headache, and trigeminal zoster.
Trigeminal neuralgia occurs later in life, from middle age onwards, most often after an age of 60, and is a condition typically associated with very strong pain distributed over the area innervated by the maxillary or mandibular nerve divisions of this nerve (V2 and V3).
Facial expression (VII).
Lesions of the facial nerve (VII) may manifest as facial palsy. This is where a person is unable to move the muscles on one or both sides of their face. If only the peripheral nerve itself is affected, this may cause Bell's palsy. Palsy that occurs is on the same side of the affected nerve. Central facial palsy will manifest in a similar fashion. If the nerve is damaged only on one side, a person will still be able to raise the eyebrows and crease the forehead on that side. That is because the frontalis muscle is innervated by both the left and the right cranial nerve. The effect is most often unilateral, and indicates contralateral damage or engagement of the cerebrum.
Hearing and balance (VIII).
The vestibulocochlear nerve (VIII) splits into the vestibular and cochlear nerve. The vestibular part is responsible for innervating the vestibules and semicircular canal of the inner ear; this structure transmits information about balance, and is an important component of the vestibuloocular reflex, which keeps the head stable and allows the eyes to track moving objects. The cochlear nerve transmits information from the cochlea, allowing sound to be heard.
When damaged, the vestibular nerve may give rise to the sensation of spinning and dizziness, and may cause "rotatory nystagmus". Function of the vestibular nerve may be tested through caloric stimulation. Damage to the vestibulocochlear nerve can also present as repetitive and involuntary eye movements (nystagmus), particularly when looking in a horizontal plane. The cochlear nerve will cause partial or complete deafness in the affected ear.
Oral sensation, taste, and salivation (IX).
The glossopharyngeal nerve (IX) is almost exclusively sensory and supplies five afferent nuclei of the brainstem, providing sensory innervation to the oropharynx and back of the tongue. The glossopharyngeal nerve also provides parasympathetic innervation to the parotid gland (though the submandibular and sublingual glands are innervated by the facial nerve).
Unilateral absence of a gag reflex suggests a lesion of CN IX, and perhaps CN X.
Vagus nerve (X).
Loss of function of the vagus nerve (X) will lead to a loss of parasympathetic innervation to a very large number of structures. Major effects of damage to the vagus nerve may include a rise in blood pressure and heart rate. Isolated dysfunction of only the vagus nerve is rare, but can be diagnosed by a hoarse voice, due to dysfunction of the superior laryngeal nerve.
Testing of function may be performed by assessing ability to drink liquids. Choking on either saliva or liquids may indicate neurological damage to the vagus nerve (X). Damage to this nerve may result in difficulties swallowing.
Shoulder elevation and head-turning (XI).
Damage to the accessory nerve (XI) may lead to contralateral weakness in the trapezius. This can be tested by asking the subject to raise their shoulders or shrug, upon which the scapula will move out into a winged position if the nerve is damaged. Weakness or an inability to elevate the scapula may be present, since the levator scapulae is alone in providing this function. There may also be weakness present of the sternocleidomastoid muscle, but as it received cortical innervation from the ipsilateral side, any damage will give rise to ipsilateral weakness.
Tongue movement (XII).
The hypoglossal nerve (XII) is unique in that it is innervated bilaterally from both hemispheres motor cortex. Damage to the nerve at lower motor neuron level may lead to fasciculations or atrophy of the musculature of the tongue. The fasciculations of the tongue are sometimes said to look like a "bag of worms". Upper motor neuron damage will not lead to atrophy or fasciculations, but only weakness of the innervated muscles.
When the nerve is damaged, it will lead to unilateral weakness and the tongue, when extended, will move towards the weaker or damaged side, as shown in the image.
Clinical significance.
Examination.
Doctors, neurologists and other medical professionals may conduct a cranial nerve examination as part of a neurological examination to examine the cranial nerves. This is a highly formalised series of steps involving specific tests for each nerve, testing the function of the olfactory nerve (I) first, and progressing sequentially for each nerve. Knowledge of cranial nerve function is an important, as it may indicate which portion of the brainstem is damaged. It is of clinical importance to know the path and origin of the cranial nerves, both intracranially as well as extracranially.
A cranial nerve exam starts with observation of the patient, as some cranial nerve lesions may affect the symmetry of the eyes or face. The eyes are examined and the visual acuity is tested through reading a Snellen chart. The visual fields are tested for nerve lesions or nystagmus via a task to perform specific eye movements . The sensation of the face is tested, and patients are asked to perform different facial movements, such as puffing out of the cheeks. Hearing is checked by voice and tuning forks. The patient's uvula is examined. After performing a shrug and head turn, the patient's tongue function assessed by various tongue movements.
Damage.
Compression.
Nerves may be compressed because of increased intercranial pressure, a mass effect of an intracerebral haemorrhage, or tumour that presses against the nerves. The cranial nerves are often the first structures to be affected by different forms of brain injury, such as hemorrhaging or tumors, partly because they are sensitive to compression. Mononeuropathy of a cranial nerve may sometimes be the first symptom of an intracranial or skull base cancer.
An increase in intercranial pressure may lead to swelling of the optic nerves (II) and compression of the surrounding veins and capillaries, causing papilloedema. A glioma, such as an optic glioma, may also impact on the optic nerve (II). A pituitary tumour may compress the optic tracts or the optic chiasm of the optic nerve (II), leading to visual loss. A pituitary tumour may also extend into the cavernous sinus, compressing the oculuomotor nerve (III), trochlear nerve (IV) and abducens nerve (VI), leading to double-vision and strabismus. These nerves may also be affected by herniation of the temporal lobes of the brain through the falx cerebri.
The cause for trigeminal neuralgia, in which one side of the face is exquisitely tender, is thought to be compression of the nerve by the superior cerebellar artery, one of the arteries supplying the cerebellum. An acoustic neuroma, particularly at the junction between the pons and medulla, may compress the facial nerve (VII) and vestibulocochlear nerve (VIII), leading to hearing and sensory loss on the affected side.
Stroke.
Occlusion of blood vessels that supply the nerves or their nuclei, an ischemic stroke, may cause specific signs and symptoms that can localise where the occlusion occurred. If there is a stroke of the midbrain, pons or medulla, various cranial nerves may be damaged, resulting in dysfunction and symptoms of . Thrombosis, such as a 
cavernous sinus thrombosis, refers to a thrombus affecting the venous drainage from the cavernous sinus, affects the optic (II), oculomotor (III), trochlear (IV), opthalamic branch of the trigeminal nerve (V1) and the abducens nerve (VI).
Inflammation.
Inflammation can be a result of infection, such as viral causes like reactivated herpes simplex virus, or can occur spontaneously. Inflammation of the facial nerve (VII) may result in Bell's palsy.
Multiple sclerosis, an inflammatory process resulting in a loss of the myelin sheathes which surround the cranial nerves, may cause a variety of shifting symptoms affecting multiple cranial nerves. Inflammation may also affect other cranial nerves. Other rarer inflammatory causes affecting the function of multiple cranial nerves include sarcoidosis, miliary tuberculosis, and inflammation of arteries, such as granulomatosis with polyangiitis.
Other.
Trauma to the skull, disease of bone, such as Paget's disease, and injury to nerves during surgery are other causes of nerve damage.
History.
The cranial nerves were originally given their numerals by Galen millennia ago, in the rostro-caudal (or anterio-posterior) order still employed today.
Terminal nerve controversy.
The terminal nerve, often called cranial nerve zero, CN 0 (or cranial nerve "nulla" or "N", since there is no Roman numeral for zero), has been largely neglected from textbooks, even though it was first clearly identified over a century ago. It was first shown to be present in the shark, but its presence in humans (and other mammals) remained somewhat controversial. More recent studies have shown the nerve to be quite distinct in human fetuses and infants, and has also regularly been seen in the adult brain. The nerve axons are unmyelinated and arise from ganglia. The terminal nerve has also been shown to release luteinising hormone. Another study has shown the terminal nerve to be a microscopic plexus of unmyelinated fibres in the frontal lobes. It was concluded in the study, confirming earlier findings by light microscope, that this nerve is a common finding in the human brain.
Other animals.
Cranial nerves are also present in other vertebrates. Other amniotes (non-amphibian tetrapods) have cranial nerves similar to those of humans. In anamniotes (fishes and amphibians), the accessory nerve (XI) and hypoglossal nerve (XII) do not exist, with the accessory nerve (XI) being an integral part of the vagus nerve (X); the hypoglossal nerve (XII) is represented by a variable number of spinal nerves emerging from vertebral segments fused into the occiput. These two nerves only became discrete nerves in the ancestors of amniotes (non-amphibian tetrapods).

</doc>
<doc id="37150" url="http://en.wikipedia.org/wiki?curid=37150" title="White cane">
White cane

A white cane is used by many people who are blind or visually impaired. Its primary uses are as a mobility tool and as a courtesy to others, but there are at least five varieties, each serving a slightly different need.
Types.
Mobility canes are often made from aluminium, graphite-reinforced plastic or other fibre-reinforced plastic, and can come with a wide variety of tips depending upon user preference.
White canes can be either collapsible or straight, with both versions having pros and cons. The National Federation of the Blind in the United States affirms that the lightness and greater length of the straight canes allows greater mobility and safety, though collapsible canes can be stored with more ease, giving them advantage in crowded areas such as classrooms and public events.
History.
Blind people have used canes as mobility tools for centuries, but it was not until after World War I that the white cane was introduced.
In 1921 James Biggs, a photographer from Bristol who became blind after an accident and was uncomfortable with the amount of traffic around his home, painted his walking stick white to be more easily visible.
In 1931 in France, Guilly d'Herbemont launched a national white stick movement for blind people. On February 7, 1931, Guilly d'Herbemont symbolically gave the first two white canes to blind people, in the presence of several French ministers. 5,000 more white canes were later sent to blind French veterans from World War I and blind civilians.
In the United States, the introduction of the white cane is attributed to George A. Bonham of the Lions Clubs International. In 1930, a Lions Club member watched as a man who was blind attempted to cross the street with a black cane that was barely visible to motorists against the dark pavement. The Lions decided to paint the cane white to make it more visible. In 1931, Lions Clubs International began a program promoting the use of white canes for people who are blind.
The first special white cane ordinance was passed in December 1930 in Peoria, Illinois granting blind pedestrians protections and the right-of-way while carrying a white cane. 
The long cane was improved upon by World War II veterans rehabilitation specialist, Richard E. Hoover, at Valley Forge Army Hospital. In 1944, he took the Lions Club white cane (originally made of wood) and went around the hospital blindfolded for a week. During this time he developed what is now the standard method of "long cane" training or the Hoover Method. He is now called the "Father of the Lightweight Long Cane Technique." The basic technique is to swing the cane from the center of the body back and forth before the feet. The cane should be swept before the rear foot as the person steps. Before he taught other rehabilitators, or "orientors," his new technique he had a special commission to have light weight, long white canes made for the veterans of the European fronts.
On October 6, 1964, a joint resolution of the Congress, HR 753, was signed into law authorizing the President of the United States to proclaim October 15 of each year as "White Cane Safety Day". President Lyndon Johnson was the first to make this proclamation. 
Legislation about canes.
While the white cane is commonly accepted as a "symbol of blindness", different countries still have different rules concerning what constitutes a "cane for the blind".
In the United Kingdom, the white cane indicates that the individual has a visual impairment; with two red bands added it indicates that the user is deafblind.
In the United States, laws vary from state to state, but in all cases, those carrying white canes are afforded the right-of-way when crossing a road. They are afforded the right to use their cane in any public place as well. In some cases, it is illegal for a non-blind person to use a white cane with the intent of being given right-of-way.
In November 2002, Argentina passed a law recognizing the use of green canes by people with low vision, stating that the nation would "Adopt from this law, the use of a green cane in the whole of Argentina as a means of orientation and mobility for people with low vision. It will have the same characteristics in weight, length, elastic grip and fluorescent ring as do white canes used by the blind."
Comparison to guide dogs.
While a guide dog, the other major mobility aid for blind people, can interact more with the user and the environment, making them more useful in certain locations, white canes are alternatives for reasons of price, care, and in case of some people, allergies. Despite the high profile of guide dogs, however, most blind people still use canes at least sometimes, and many still use canes entirely.
Children and canes.
In many countries, including the UK, a cane is not generally introduced to a child until they are between 7 and 10 years old. However, more recently canes have been started to be introduced as soon as a child learns to walk to aid development with great success.
Joseph Cutter and Lilli Nielsen, pioneers in research on the development of blind and multiple-handicapped children, have begun to introduce new research on mobility in blind infants in children. Cutter's book, "Independent Movement and Travel in Blind Children", recommends a cane to be introduced as early as possible, so that the blind child learns to use it and move around naturally and organically, the same way a sighted child learns to walk. A longer cane, between nose and chin height, is recommended to compensate for a child's more immature grasp and tendency to hold the handle of the cane by the side instead of out in front. Mature cane technique should not be expected from a child, and style and technique can be refined as the child gets older.

</doc>
<doc id="37151" url="http://en.wikipedia.org/wiki?curid=37151" title="Association for the Taxation of Financial Transactions and for Citizens' Action">
Association for the Taxation of Financial Transactions and for Citizens' Action

The Association pour la Taxation des Transactions financières et pour l'Action Citoyenne (Association for the Taxation of financial Transactions and Citizen's Action, ATTAC) is an activist organization originally created for promoting the establishment of a tax on foreign exchange transactions.
Background.
Originally called "Action for a Tobin Tax to Assist the Citizen", ATTAC was a single-issue movement demanding the introduction of the so-called Tobin tax on currency speculation. ATTAC now devotes itself to a wide range of issues related to globalisation, monitoring the decisions of the World Trade Organization (WTO), the Organisation for Economic Co-operation and Development (OECD) and the International Monetary Fund (IMF). ATTAC attends the meetings of the G8 with the goal of influencing policymakers' decisions. Attac recently criticised Germany for what it called the criminalisation of anti-G8 groups.
At the founding, ATTAC had specific statutory objectives based on the promotion of the Tobin tax. For example, ATTAC Luxembourg specifies in article 1 of its statutes that it "aims to produce and communicate information, and to promote and carry out activities of all kinds for the recapture, by the citizens, of the power that the financial sector has on all aspects of political, economic, social and cultural life throughout the world. Such means include the taxation of transactions in foreign exchange markets (Tobin tax)."
ATTAC claims not to an anti-globalization movement, but it criticises the neoliberal ideology that it sees as dominating economic globalisation. It supports globalisation policies that they characterise as sustainable and socially just. One of ATTAC's slogans is "The World is not for sale", denouncing the "merchandisation" of society. Another slogan is "Another world is possible" pointing to an alternative globalization where people and not profit is in focus.
James Tobin opposing ATTAC.
Attac was originally founded to promote the Tobin tax by the Keynesian economist James Tobin. Tobin himself has accused Attac for misusing his name and said that he has nothing in common with Attac and is a supporter of free trade — "everything that these movements are attacking. They're misusing my name." 
Organisational history.
In December 1998, Ignacio Ramonet wrote in "Le Monde diplomatique" an editorial in which he advocated the establishment of the Tobin tax and the creation of an organisation to pressure governments around the world to introduce the tax. ATTAC was created on June 3, 1998, during a constitutive assembly in France. While it was founded in France it now exists in over forty countries around the world. In France, politicians from the left are members of the association. In Luxembourg, Francois Bausch of the left Green party is the founding politician in the association's initial member list.
ATTAC functions on a principle of decentralisation: local associations organise meetings, conferences, and compose documents that become counter-arguments to the perceived neoliberal discourse. ATTAC aims to formalise the possibility of an alternative to the neoliberal society that is currently required of globalisation. ATTAC aspires to be a movement of popular education.
Views on Attac and its members in different countries.
Finland.
Communist Juhani Lohikoski, previously a chairman of Communist Youth League and Socialist League, served as the chairman of Finnish Attac for two terms (2002 - 2004). Yrjö Hakanen, pro-Soviet chairman of the Communist Party of Finland, was a member of the board and a member of the founding committee. In March 2002 Aimo Kairamo, the long-time chief editor of the party organ of the Social Democrat Party, resigned from Attac and recommended the same decision for other social democrats because of the left-wing minority communists' leading positions. Soon also the social democrat foreign minister Erkki Tuomioja considered to follow Kairamo's example.
Sweden.
Researcher Malin Gawell covers the birth and development of Attac Sweden in her doctoral thesis on activist entrepreneurship. She suggests that Attac in Sweden was formed by people seeking a new way of organising with flat hierarchy, and with the strongly sensed need of making a change as the driving force.
From another perspective, Sydsvenskan newspaper suggested that the downturn of memberships in Swedish Attac after the hype in the beginning of 2001 may be due to its views on trade policies.
Issues and activities.
The main issues covered by ATTAC today are:
In France, ATTAC associates with many other left-wing causes.
Nestlégate.
In the year 2008 Attac Switzerland was hit by a scandal which was later called Nestlégate by the local media. Between the years 2003 and 2005, the Swiss multinational food and beverage company Nestlé, engaged the external Security company Securitas AG, to spy on the Swiss Attac branch. Nestlé started the monitoring, when Attac Switzerland decided to work on a critical book about Nestlé.
Due to Nestlégate, Attac Switzerland filed a lawsuit against Nestlé which was decided in favour of Attac in January 2013, as the personal rights of the observed were violated. They received a compensation for damages of 3'000 Swiss francs each, which has an equivalent of about 3'230 USD at the date of the proclamation of sentence.

</doc>
<doc id="37153" url="http://en.wikipedia.org/wiki?curid=37153" title="Supercomputer">
Supercomputer

A supercomputer is a computer with a very high-level computational capacity. As of 2015, there are supercomputers which could perform up-to quadrillions of floating point operations per second.
Supercomputers were introduced in the 1960s, made initially, and for decades primarily, by Seymour Cray at Control Data Corporation (CDC), Cray Research and subsequent companies bearing his name or monogram. While the supercomputers of the 1970s used only a few processors, in the 1990s machines with thousands of processors began to appear and, by the end of the 20th century, massively parallel supercomputers with tens of thousands of "off-the-shelf" processors were the norm. s of 2014[ [update]], China's Tianhe-2 supercomputer is the fastest in the world at 33.86 petaFLOPS (PFLOPS), or 33.86 quadrillion floating point operations per second.
Systems with massive numbers of processors generally take one of two paths: In one approach (e.g., in distributed computing), a large number of discrete computers (e.g., laptops) distributed across a network (e.g., the Internet) devote some or all of their time to solving a common problem; each individual computer (client) receives and completes many small tasks, reporting the results to a central server which integrates the task results from all the clients into the overall solution. In another approach, a large number of dedicated processors are placed in close proximity to each other (e.g. in a computer cluster); this saves considerable time moving data around and makes it possible for the processors to work together (rather than on separate tasks), for example in mesh and hypercube architectures.
The use of multi-core processors combined with centralization is an emerging trend; one can think of this as a small cluster (the multicore processor in a smartphone, tablet, laptop, etc.) that both depends upon and contributes to the cloud.
Supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields, including quantum mechanics, weather forecasting, climate research, oil and gas exploration, molecular modeling (computing the structures and properties of chemical compounds, biological macromolecules, polymers, and crystals), and physical simulations (such as simulations of the early moments of the universe, airplane and spacecraft aerodynamics, the detonation of nuclear weapons, and nuclear fusion). Throughout their history, they have been essential in the field of cryptanalysis.
History.
The history of supercomputing goes back to the 1960s, with the Atlas at the University of Manchester and a series of computers at Control Data Corporation (CDC), designed by Seymour Cray. These used innovative designs and parallelism to achieve superior computational peak performance.
The Atlas was a joint venture between Ferranti and the Manchester University and was designed to operate at processing speeds approaching one microsecond per instruction, about one million instructions per second. The first Atlas was officially commissioned on 7 December 1962 as one of the world's first supercomputers – considered to be the most powerful computer in the world at that time by a considerable margin, and equivalent to four IBM 7094s.
The CDC 6600, released in 1964, was designed by Cray to be the fastest in the world by a large margin. Cray switched from germanium to silicon transistors, which he ran very fast, solving the overheating problem by introducing refrigeration. Given that the 6600 outran all computers of the time by about 10 times, it was dubbed a "supercomputer" and defined the supercomputing market when one hundred computers were sold at $8 million each.
Cray left CDC in 1972 to form his own company, Cray Research. Four years after leaving CDC, Cray delivered the 80 MHz Cray 1 in 1976, and it became one of the most successful supercomputers in history. The Cray-2 released in 1985 was an 8 processor liquid cooled computer and Fluorinert was pumped through it as it operated. It performed at 1.9 gigaflops and was the world's fastest until 1990.
While the supercomputers of the 1980s used only a few processors, in the 1990s, machines with thousands of processors began to appear both in the United States and Japan, setting new computational performance records. Fujitsu's Numerical Wind Tunnel supercomputer used 166 vector processors to gain the top spot in 1994 with a peak speed of 1.7 gigaFLOPS (GFLOPS) per processor. The Hitachi SR2201 obtained a peak performance of 600 GFLOPS in 1996 by using 2048 processors connected via a fast three-dimensional crossbar network. The Intel Paragon could have 1000 to 4000 Intel i860 processors in various configurations, and was ranked the fastest in the world in 1993. The Paragon was a MIMD machine which connected processors via a high speed two dimensional mesh, allowing processes to execute on separate nodes; communicating via the Message Passing Interface.
Hardware and architecture.
Approaches to supercomputer architecture have taken dramatic turns since the earliest systems were introduced in the 1960s. Early supercomputer architectures pioneered by Seymour Cray relied on compact innovative designs and local parallelism to achieve superior computational peak performance. However, in time the demand for increased computational power ushered in the age of massively parallel systems.
While the supercomputers of the 1970s used only a few processors, in the 1990s, machines with thousands of processors began to appear and by the end of the 20th century, massively parallel supercomputers with tens of thousands of "off-the-shelf" processors were the norm. Supercomputers of the 21st century can use over 100,000 processors (some being graphic units) connected by fast connections. The Connection Machine CM-5 supercomputer is a massively parallel processing computer capable of many billions of arithmetic operations per second.
Throughout the decades, the management of heat density has remained a key issue for most centralized supercomputers. The large amount of heat generated by a system may also have other effects, e.g. reducing the lifetime of other system components. There have been diverse approaches to heat management, from pumping Fluorinert through the system, to a hybrid liquid-air cooling system or air cooling with normal air conditioning temperatures.
Systems with a massive number of processors generally take one of two paths. In the grid computing approach, the processing power of a large number of computers, organised as distributed, diverse administrative domains, is opportunistically used whenever a computer is available. In another approach, a large number of processors are used in close proximity to each other, e.g. in a computer cluster. In such a centralized massively parallel system the speed and flexibility of the interconnect becomes very important and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects. The use of multi-core processors combined with centralization is an emerging direction, e.g. as in the Cyclops64 system.
As the price, performance and energy efficiency of general purpose graphic processors (GPGPUs) have improved, a number of petaflop supercomputers such as Tianhe-I and Nebulae have started to rely on them. However, other systems such as the K computer continue to use conventional processors such as SPARC-based designs and the overall applicability of GPGPUs in general-purpose high-performance computing applications has been the subject of debate, in that while a GPGPU may be tuned to score well on specific benchmarks, its overall applicability to everyday algorithms may be limited unless significant effort is spent to tune the application towards it. However, GPUs are gaining ground and in 2012 the Jaguar supercomputer was transformed into Titan by retrofitting CPUs with GPUs.
High performance computers have an expected life cycle of about three years.
A number of "special-purpose" systems have been designed, dedicated to a single problem. This allows the use of specially programmed FPGA chips or even custom VLSI chips, allowing better price/performance ratios by sacrificing generality. Examples of special-purpose supercomputers include Belle, Deep Blue, and Hydra, for playing chess, Gravity Pipe for astrophysics, MDGRAPE-3 for protein structure computation
molecular dynamics and Deep Crack, for breaking the DES cipher.
Energy usage and heat management.
A typical supercomputer consumes large amounts of electrical power, almost all of which is converted into heat, requiring cooling. For example, Tianhe-1A consumes 4.04 megawatts of electricity. The cost to power and cool the system can be significant, e.g. 4 MW at $0.10/kWh is $400 an hour or about $3.5 million per year.
Heat management is a major issue in complex electronic devices, and affects powerful computer systems in various ways. The thermal design power and CPU power dissipation issues in supercomputing surpass those of traditional computer cooling technologies. The supercomputing awards for green computing reflect this issue.
The packing of thousands of processors together inevitably generates significant amounts of heat density that need to be dealt with. The Cray 2 was liquid cooled, and used a Fluorinert "cooling waterfall" which was forced through the modules under pressure. However, the submerged liquid cooling approach was not practical for the multi-cabinet systems based on off-the-shelf processors, and in System X a special cooling system that combined air conditioning with liquid cooling was developed in conjunction with the Liebert company.
In the Blue Gene system, IBM deliberately used low power processors to deal with heat density.
On the other hand, the IBM Power 775, released in 2011, has closely packed elements that require water cooling. The IBM Aquasar system, on the other hand uses "hot water cooling" to achieve energy efficiency, the water being used to heat buildings as well.
The energy efficiency of computer systems is generally measured in terms of "FLOPS per Watt". In 2008, IBM's Roadrunner operated at 3,76 MFLOPS/W. In November 2010, the Blue Gene/Q reached 1,684 MFLOPS/W. In June 2011 the top 2 spots on the Green 500 list were occupied by Blue Gene machines in New York (one achieving 2097 MFLOPS/W) with the DEGIMA cluster in Nagasaki placing third with 1375 MFLOPS/W.
Because copper wires can transfer energy into a supercomputer with much higher power densities than forced air or circulating refrigerants can remove waste heat,
the ability of the cooling systems to remove waste heat is a limiting factor.
s of 2015[ [update]], many existing supercomputers have more infrastructure capacity than the actual peak demand of the machine – people conservatively designed the power and cooling infrastructure to handle more than the theoretical peak electrical power consumed by the supercomputer. Designs for future supercomputers are power-limited – the thermal design power of the supercomputer as a whole, the amount that the power and cooling infrastructure can handle, is somewhat more than the expected normal power consumption, but less than the theoretical peak power consumption of the electronic hardware.
Software and system management.
Operating systems.
Since the end of the 20th century, supercomputer operating systems have undergone major transformations, based on the changes in supercomputer architecture. While early operating systems were custom tailored to each supercomputer to gain speed, the trend has been to move away from in-house operating systems to the adaptation of generic software such as Linux.
Since modern massively parallel supercomputers typically separate computations from other services by using multiple types of nodes, they usually run different operating systems on different nodes, e.g. using a small and efficient lightweight kernel such as CNK or CNL on compute nodes, but a larger system such as a Linux-derivative on server and I/O nodes.
While in a traditional multi-user computer system job scheduling is, in effect, a tasking problem for processing and peripheral resources, in a massively parallel system, the job management system needs to manage the allocation of both computational and communication resources, as well as gracefully deal with inevitable hardware failures when tens of thousands of processors are present.
Although most modern supercomputers use the Linux operating system, each manufacturer has its own specific Linux-derivative, and no industry standard exists, partly due to the fact that the differences in hardware architectures require changes to optimize the operating system to each hardware design.
Software tools and message passing.
The parallel architectures of supercomputers often dictate the use of special programming techniques to exploit their speed. Software tools for distributed processing include standard APIs such as MPI and PVM, VTL, and open source-based software solutions such as Beowulf.
In the most common scenario, environments such as PVM and MPI for loosely connected clusters and OpenMP for tightly coordinated shared memory machines are used. Significant effort is required to optimize an algorithm for the interconnect characteristics of the machine it will be run on; the aim is to prevent any of the CPUs from wasting time waiting on data from other nodes. GPGPUs have hundreds of processor cores and are programmed using programming models such as CUDA.
Moreover, it is quite difficult to debug and test parallel programs. Special techniques need to be used for testing and debugging such applications.
Distributed supercomputing.
Opportunistic approaches.
Opportunistic Supercomputing is a form of networked grid computing whereby a "super virtual computer" of many loosely coupled volunteer computing machines performs very large computing tasks. Grid computing has been applied to a number of large-scale embarrassingly parallel problems that require supercomputing performance scales. However, basic grid and cloud computing approaches that rely on volunteer computing can not handle traditional supercomputing tasks such as fluid dynamic simulations.
The fastest grid computing system is the distributed computing project Folding@home. F@h reported 43.1 PFLOPS of x86 processing power as of 2014[ [update]]. Of this, 42.5 PFLOPS are contributed by clients running on various GPUs, and the rest from various CPU systems.
The BOINC platform hosts a number of distributed computing projects. s of 2011[ [update]], BOINC recorded a processing power of over 5.5 PFLOPS through over 480,000 active computers on the network The most active project (measured by computational power), MilkyWay@home, reports processing power of over 700 teraFLOPS (TFLOPS) through over 33,000 active computers.
s of 2011[ [update]], GIMPS's distributed Mersenne Prime search currently achieves about 60 TFLOPS through over 25,000 registered computers. The supports GIMPS's grid computing approach, one of the earliest and most successful grid computing projects, since 1997.
Quasi-opportunistic approaches.
Quasi-opportunistic supercomputing is a form of distributed computing whereby the “super virtual computer” of a large number of networked geographically disperse computers performs computing tasks that demand huge processing power. Quasi-opportunistic supercomputing aims to provide a higher quality of service than opportunistic grid computing by achieving more control over the assignment of tasks to distributed resources and the use of intelligence about the availability and reliability of individual systems within the supercomputing network. However, quasi-opportunistic distributed execution of demanding parallel computing software in grids should be achieved through implementation of grid-wise allocation agreements, co-allocation subsystems, communication topology-aware allocation mechanisms, fault tolerant message passing libraries and data pre-conditioning.
Performance measurement.
Capability vs capacity.
Supercomputers generally aim for the maximum in "capability computing" rather than "capacity computing". Capability computing is typically thought of as using the maximum computing power to solve a single large problem in the shortest amount of time. Often a capability system is able to solve a problem of a size or complexity that no other computer can, e.g. a very complex weather simulation application.
Capacity computing, in contrast, is typically thought of as using efficient cost-effective computing power to solve a small number of somewhat large problems or a large number of small problems. Architectures that lend themselves to supporting many users for routine everyday tasks may have a lot of capacity, but are not typically considered supercomputers, given that they do not solve a single very complex problem.
Performance metrics.
In general, the speed of supercomputers is measured and benchmarked in "FLOPS" ("FLoating point Operations Per Second"), and not in terms of "MIPS" (Million Instructions Per Second), as is the case with general-purpose computers. These measurements are commonly used with an SI prefix such as tera-, combined into the shorthand "TFLOPS" (1012 FLOPS, pronounced "teraflops"), or peta-, combined into the shorthand "PFLOPS" (1015 FLOPS, pronounced "petaflops".) "Petascale" supercomputers can process one quadrillion (1015) (1000 trillion) FLOPS. Exascale is computing performance in the exaFLOPS (EFLOPS) range. An EFLOPS is one quintillion (1018) FLOPS (one million TFLOPS).
No single number can reflect the overall performance of a computer system, yet the goal of the Linpack benchmark is to approximate how fast the computer solves numerical problems and it is widely used in the industry. The FLOPS measurement is either quoted based on the theoretical floating point performance of a processor (derived from manufacturer's processor specifications and shown as "Rpeak" in the TOP500 lists) which is generally unachievable when running real workloads, or the achievable throughput, derived from the LINPACK benchmarks and shown as "Rmax" in the TOP500 list. The LINPACK benchmark typically performs LU decomposition of a large matrix. The LINPACK performance gives some indication of performance for some real-world problems, but does not necessarily match the processing requirements of many other supercomputer workloads, which for example may require more memory bandwidth, or may require better integer computing performance, or may need a high performance I/O system to achieve high levels of performance.
The TOP500 list.
Since 1993, the fastest supercomputers have been ranked on the TOP500 list according to their LINPACK benchmark results. The list does not claim to be unbiased or definitive, but it is a widely cited current definition of the "fastest" supercomputer available at any given time.
This is a recent list of the computers which appeared at the top of the TOP500 list, and the "Peak speed" is given as the "Rmax" rating. For more historical data see History of supercomputing.
Largest Supercomputer Vendors according to the total (GFLOPS) operated.
Source : 
Applications of supercomputers.
The stages of supercomputer application may be summarized in the following table:
The IBM Blue Gene/P computer has been used to simulate a number of artificial neurons equivalent to approximately one percent of a human cerebral cortex, containing 1.6 billion neurons with approximately 9 trillion connections. The same research group also succeeded in using a supercomputer to simulate a number of artificial neurons equivalent to the entirety of a rat's brain.
Modern-day weather forecasting also relies on supercomputers. The National Oceanic and Atmospheric Administration uses supercomputers to crunch hundreds of millions of observations to help make weather forecasts more accurate.
In 2011, the challenges and difficulties in pushing the envelope in supercomputing were underscored by IBM's abandonment of the Blue Waters petascale project.
Research and development trends.
Given the current speed of progress, industry experts estimate that supercomputers will reach 1 EFLOPS (1018, one quintillion FLOPS) by 2018. In China industry experts estimate machines will start reaching 1,000-petaflop performance by 2018. Using the Intel MIC multi-core processor architecture, which is Intel's response to GPU systems, SGI plans to achieve a 500-fold increase in performance by 2018, in order to achieve one exaFLOPS. Samples of MIC chips with 32 cores, which combine vector processing units with standard CPU, have become available. The Indian government has also stated ambitions for an exaFLOPS-range supercomputer, which they hope to complete by 2017. In November 2014, it was reported that India is working on the Fastest supercomputer ever which is set to work at 132 EFLOPS.
Erik P. DeBenedictis of Sandia National Laboratories theorizes that a zettaFLOPS (1021, one sextillion FLOPS) computer is required to accomplish full weather modeling, which could cover a two-week time span accurately. Such systems might be built around 2030.

</doc>
<doc id="37154" url="http://en.wikipedia.org/wiki?curid=37154" title="Coxsackie A virus">
Coxsackie A virus

Coxsackie A virus (CAV) is a cytolytic coxsackie virus of the "Picornaviridae" family, an enterovirus (a group containing the polioviruses, coxsackieviruses, and echoviruses).
Diseases.
The most well known Coxsackie A disease is Hand, foot and mouth disease (unrelated to hoof and mouth disease), a common childhood illness which affects mostly children aged 5 or under, often produced by Coxsackie A16. In most cases infection is asymptomatic or causes only mild symptoms. In others, infection produces short-lived (7–10 days) fever and painful blisters in the mouth (a condition known as "herpangina"), on the palms and fingers of the hand, or on the soles of the feet. There can also be blisters in the throat, or on or above the tonsils. Adults can also be affected. The rash, which can appear several days after high temperature and painful sore throat, can be itchy and painful, especially on the hands/fingers and bottom of feet.
Other diseases include acute haemorrhagic conjunctivitis (A24 specifically), herpangina, and aseptic meningitis (both Coxsackie A and B viruses). Coxsackievirus A7 infrequently causes polio-like permanent paralysis.
Treatment.
Treatment is dependent on the disease process initiated by the virus.
There is no known cure or vaccine against the coxsackie.

</doc>
<doc id="37160" url="http://en.wikipedia.org/wiki?curid=37160" title="Reinhard">
Reinhard

Reinhard is a surname or given name, and may refer to:
A surname:
A given name:
History:

</doc>
<doc id="37161" url="http://en.wikipedia.org/wiki?curid=37161" title="Fuel injection">
Fuel injection

Fuel injection is a system for admitting fuel into an internal combustion engine. It has become the primary fuel delivery system used in automotive engines, having replaced carburetors during the 1980s and 1990s. A variety of injection systems have existed since the earliest usage of the internal combustion engine.
The primary difference between carburetors and fuel injection is that fuel injection atomizes the fuel through a small nozzle under high pressure, while a carburetor relies on suction created by intake air accelerated through a Venturi tube to draw the fuel into the airstream.
Modern fuel injection systems are designed specifically for the type of fuel being used. Some systems are designed for multiple grades of fuel (using sensors to adapt the tuning for the fuel currently used). Most fuel injection systems are for gasoline or diesel applications. 
Objectives.
The functional objectives for fuel injection systems can vary. All share the central task of supplying fuel to the combustion process, but it is a design decision how a particular system is optimized. There are several competing objectives such as:
The modern digital electronic fuel injection system is more capable at optimizing these competing objectives consistently than earlier fuel delivery systems (such as carburetors). Carburetors have the potential to atomize fuel better (see Pogue and Allen Caggiano patents).
Benefits.
Benefits of fuel injection include smoother and more consistent transient throttle response, such as during quick throttle transitions, easier cold starting, more accurate adjustment to account for extremes of ambient temperatures and changes in air pressure, more stable idling, decreased maintenance needs, and better fuel efficiency.
Fuel injection also dispenses with the need for a separate mechanical choke, which on carburetor-equipped vehicles must be adjusted as the engine warms up to normal temperature. Furthermore on spark ignition engines (direct) fuel injection has the advantage of being able to facilitate stratified combustion which have not been possible with carburetors.
It is only with the advent of multi-point fuel injection certain engine configurations such as inline five cylinder gasoline engines have become more feasible for mass production, as traditional carburetor arrangement with single or twin carburetors could not provide even fuel distribution between cylinders, unless a more complicated individual carburetor per cylinder is used.
Fuel injection systems are also able to operate normally regardless of orientation, whereas carburetors with floats are not able to operate upside down or in zero gravity, such as encountered on airplanes.
Environmental benefits.
Fuel injection generally increases engine fuel efficiency. With the improved cylinder-to-cylinder fuel distribution of multi-point fuel injection, less fuel is needed for the same power output (when cylinder-to-cylinder distribution varies significantly, some cylinders receive excess fuel as a side effect of ensuring that all cylinders receive "sufficient" fuel).
Exhaust emissions are cleaner because the more precise and accurate fuel metering reduces the concentration of toxic combustion byproducts leaving the engine, and because exhaust cleanup devices such as the catalytic converter can be optimized to operate more efficiently since the exhaust is of consistent and predictable composition.
History and development.
Herbert Akroyd Stuart developed the first device with a design similar to modern fuel injection, using a 'jerk pump' to meter out fuel oil at high pressure to an injector. This system was used on the hot bulb engine and was adapted and improved by Bosch and Clessie Cummins for use on diesel engines (Rudolf Diesel's original system employed a cumbersome 'air-blast' system using highly compressed air). Fuel injection was in widespread commercial use in diesel engines by the mid-1920s.
An early use of indirect gasoline injection dates back to 1902, when French aviation engineer Leon Levavasseur installed it on his pioneering Antoinette 8V aircraft powerplant, the first V8 engine of any type ever produced in any quantity.
Another early use of gasoline direct injection was on the Hesselman engine invented by Swedish engineer Jonas Hesselman in 1925. Hesselman engines use the ultra lean burn principle; fuel is injected toward the end of the compression stroke, then ignited with a spark plug. They are often started on gasoline and then switched to diesel or kerosene.
Direct fuel injection was used in notable World War II aero-engines such as the Junkers Jumo 210, the Daimler-Benz DB 601, the BMW 801, the Shvetsov ASh-82FN (M-82FN). German direct injection petrol engines used injection systems developed by Bosch from their diesel injection systems. Later versions of the Rolls-Royce Merlin and Wright R-3350 used single point fuel injection, at the time called "Pressure Carburettor". Due to the wartime relationship between Germany and Japan, Mitsubishi also had two radial aircraft engines utilizing fuel injection, the Mitsubishi Kinsei ("kinsei" means "venus") and the Mitsubishi Kasei ("kasei" means "mars").
Alfa Romeo tested one of the first electronic injection systems (Caproni-Fuscaldo) in Alfa Romeo 6C 2500 with "Ala spessa" body in 1940 Mille Miglia. The engine had six electrically operated injectors and were fed by a semi-high-pressure circulating fuel pump system.
Development in diesel engines.
All diesel engines (with the exception of some tractors and scale model engines) have fuel injected into the combustion chamber. See diesel engines.
Development in gasoline/petrol engines.
Mechanical injection.
The invention of mechanical injection for gasoline-fueled aviation engines was by the French inventor of the V8 engine configuration, Leon Levavasseur in 1902. Levavasseur designed the original Antoinette firm's series of V-form aero engines, starting with the Antoinette 8V to be used by the aircraft the Antoinette firm built that Levavasseur also designed, flown from 1906 to the firm's demise in 1910, with the world's first V16 engine, using Levavasseur's direct injection and producing around 100 hp flying an Antoinette VII monoplane in 1907.
The first post-World War I example of direct gasoline injection was on the Hesselman engine invented by Swedish engineer Jonas Hesselman in 1925. Hesselman engines used the ultra lean burn principle and injected the fuel in the end of the compression stroke and then ignited it with a spark plug, it was often started on gasoline and then switched over to run on diesel or kerosene. The Hesselman engine was a low compression design constructed to run on heavy fuel oils.
Direct gasoline injection was applied during the Second World War to almost all higher-output production aircraft powerplants made in Germany (the widely used BMW 801 radial, and the popular inverted inline V12 Daimler-Benz DB 601, DB 603 and DB 605, along with the similar Junkers Jumo 210G, Jumo 211 and Jumo 213, starting as early as 1937 for both the Jumo 210G and DB 601), the Soviet Union (Shvetsov ASh-82FN radial, 1943, Chemical Automatics Design Bureau - KB Khimavtomatika) and the USA (Wright R-3350 "Duplex Cyclone" radial, 1944).
Immediately following the war, hot rodder Stuart Hilborn started to offer mechanical injection for race cars, salt cars, and midgets, well-known and easily distinguishable because of their prominent velocity stacks projecting upwards from the engines on which they were used.
The first automotive direct injection system used to run on gasoline was developed by Bosch, and was introduced by Goliath for their Goliath GP700 automobile, and Gutbrod in 1952. This was basically a high-pressure diesel direct-injection pump with an intake throttle valve. (Diesels only change the amount of fuel injected to vary output; there is no throttle.) This system used a normal gasoline fuel pump, to provide fuel to a mechanically driven injection pump, which had separate plungers per injector to deliver a very high injection pressure directly into the combustion chamber. The 1954 Mercedes-Benz W196 Formula 1 racing car engine used Bosch direct injection derived from wartime aero engines. Following this racetrack success, the 1955 Mercedes-Benz 300SL, the first production sports car to use fuel injection, used direct injection. The 1955 Mercedes-Benz 300SLR, in which Stirling Moss drove to victory in the 1955 Mille Miglia and Pierre Levegh crashed and died in the 1955 Le Mans disaster, had an engine developed from the W196 engine. The Bosch fuel injectors were placed into the bores on the cylinder wall used by the spark plugs in other Mercedes-Benz six-cylinder engines (the spark plugs were relocated to the cylinder head). Later, more mainstream applications of fuel injection favored the less-expensive indirect injection methods.
Chevrolet introduced a mechanical fuel injection option, made by General Motors' Rochester Products division, for its 283 V8 engine in 1956 (1957 U.S. model year). This system directed the inducted engine air across a "spoon shaped" plunger that moved in proportion to the air volume. The plunger connected to the fuel metering system that mechanically dispensed fuel to the cylinders via distribution tubes. This system was not a "pulse" or intermittent injection, but rather a constant flow system, metering fuel to all cylinders simultaneously from a central "spider" of injection lines. The fuel meter adjusted the amount of flow according to engine speed and load, and included a fuel reservoir, which was similar to a carburetor's float chamber. With its own high-pressure fuel pump driven by a cable from the distributor to the fuel meter, the system supplied the necessary pressure for injection. This was a "port" injection where the injectors are located in the intake manifold, very near the intake valve.
In 1956, Lucas developed its injection system, which was first used for Jaguar racing cars at Le Mans. The system was subsequently adopted very successfully in Formula One racing, securing championships by Cooper, BRM, Lotus, Brabham, Matra and Tyrrell in the years 1959 through 1973. While the racing systems used a simple "fuel cam" for metering, a more sophisticated "Mk 2" vacuum based "shuttle metering" was developed for production cars. This mechanical system was used by some Maserati, Aston Martin, and Triumph models between 1963 and 1975.
During the 1960s, other mechanical injection systems such as Hilborn were occasionally used on modified American V8 engines in various racing applications such as drag racing, oval racing, and road racing. These racing-derived systems were not suitable for everyday street use, having no provisions for low speed metering, or often none even for starting (starting required that fuel be squirted into the injector tubes while cranking the engine). However, they were a favorite in the aforementioned competition trials in which essentially wide-open throttle operation was prevalent. Constant-flow injection systems continue to be used at the highest levels of drag racing, where full-throttle, high-RPM performance is key.
In 1967, one of the first Japanese designed cars to use mechanical fuel injection was the Daihatsu Compagno.
Another mechanical system, made by Bosch called Jetronic, but injecting the fuel into the port above the intake valve, was used by several European car makers, particularly Porsche from 1969 until 1973 in the 911 production range and until 1975 on the Carrera 3.0 in Europe. Porsche continued using this system on its racing cars into the late seventies and early eighties. Porsche racing variants such as the 911 RSR 2.7 & 3.0, 904/6, 906, 907, 908, 910, 917 (in its regular normally aspirated or 5.5 Liter/1500 HP turbocharged form), and 935 all used Bosch or Kugelfischer built variants of injection. The early Bosch Jetronic systems were also used by Audi, Volvo, BMW, Volkswagen, and many others. The Kugelfischer system was also used by the BMW 2000/2002 Tii and some versions of the Peugeot 404/504 and Lancia Flavia. 
A system similar to the Bosch inline mechanical pump was built by SPICA for Alfa Romeo, used on the Alfa Romeo Montreal and on U.S. market 1750 and 2000 models from 1969 to 1981. This was designed to meet the U.S. emission requirements with no loss in performance and it also reduced fuel consumption.
Electronic injection.
The first commercial electronic fuel injection (EFI) system was Electrojector, developed by the Bendix Corporation and was offered by American Motors Corporation (AMC) in 1957. The Rambler Rebel, showcased AMC's new 327 CID engine. The Electrojector was an option and rated at 288 bhp. The EFI produced peak torque 500 rpm lower than the equivalent carburetored engine The Rebel Owners Manual described the design and operation of the new system. (due to cooler, therefore denser, intake air). The cost of the EFI option was US$395 and it was available on 15 June 1957. Electrojector's teething problems meant only pre-production cars were so equipped: thus, very few cars so equipped were ever sold and none were made available to the public. The EFI system in the Rambler ran fine in warm weather, but suffered hard starting in cooler temperatures.
Chrysler offered Electrojector on the 1958 Chrysler 300D, DeSoto Adventurer, Dodge D-500, and Plymouth Fury, arguably the first series-production cars equipped with an EFI system. It was jointly engineered by Chrysler and Bendix. The early electronic components were not equal to the rigors of underhood service, however, and were too slow to keep up with the demands of "on the fly" engine control. Most of the 35 vehicles originally so equipped were field-retrofitted with 4-barrel carburetors. The Electrojector patents were subsequently sold to Bosch.
Bosch developed an electronic fuel injection system, called "D-Jetronic" ("D" for "Druck", German for "pressure"), which was first used on the VW 1600TL/E in 1967. This was a speed/density system, using engine speed and intake manifold air density to calculate "air mass" flow rate and thus fuel requirements. This system was adopted by VW, Mercedes-Benz, Porsche, Citroën, Saab, and Volvo. Lucas licensed the system for production with Jaguar.
Bosch superseded the D-Jetronic system with the "K-Jetronic" and "L-Jetronic" systems for 1974, though some cars (such as the Volvo 164) continued using D-Jetronic for the following several years. In 1970, the Isuzu 117 Coupé was introduced with a Bosch-supplied D-Jetronic fuel injected engine sold only in Japan.
In Japan, the Toyota Celica used electronic, multi-port fuel injection in the optional 18R-E engine in January 1974. Nissan offered electronic, multi-port fuel injection in 1975 with the Bosch L-Jetronic system used in the Nissan L28E engine and installed in the Nissan Fairlady Z, Nissan Cedric, and the Nissan Gloria. Nissan also installed multi-point fuel injection in the Nissan Y44 V8 engine in the Nissan President. Toyota soon followed with the same technology in 1978 on the 4M-E engine installed in the Toyota Crown, the Toyota Supra, and the Toyota Mark II. In the 1980s, the Isuzu Piazza, and the Mitsubishi Starion added fuel injection as standard equipment, developed separately with both companies history of diesel powered engines. 1981 saw Mazda offer fuel injection in the Mazda Luce with the Mazda FE engine, and in 1983, Subaru offered fuel injection in the Subaru EA81 engine installed in the Subaru Leone. Honda followed in 1984 with their own system, called PGM-FI in the Honda Accord, and the Honda Vigor using the Honda ES3 engine.
The limited production Chevrolet Cosworth Vega was introduced in March 1975 using a Bendix EFI system with pulse-time manifold injection, four injector valves, an electronic control unit (ECU), five independent sensors and two fuel pumps. The EFI system was developed to satisfy stringent emission control requirements and market demands for a technologically advanced responsive vehicle. 5000 hand-built Cosworth Vega engines were produced but only 3,508 cars were sold through 1976.
The Cadillac Seville was introduced in 1975 with an EFI system made by Bendix and modelled very closely on Bosch's D-Jetronic. L-Jetronic first appeared on the 1974 Porsche 914, and uses a mechanical airflow meter (L for Luft, German for "air") that produces a signal that is proportional to "air volume". This approach required additional sensors to measure the atmospheric pressure and temperature, to ultimately calculate "air mass". L-Jetronic was widely adopted on European cars of that period, and a few Japanese models a short time later.
In 1980, Motorola (now Freescale) introduced the first electronic engine control unit, the EEC-III. Its integrated control of engine functions (such as fuel injection and spark timing) is now the standard approach for fuel injection systems. The Motorola technology was installed in Ford North American products.
Ellimination of carburetors.
In the 1970s and 1980s in the U.S. and Japan, the respective federal governments imposed increasingly strict exhaust emission regulations. During that time period, the vast majority of gasoline-fueled automobile and light truck engines did not use fuel injection. To comply with the new regulations, automobile manufacturers often made extensive and complex modifications to the engine carburetor(s). While a simple carburetor system is cheaper to manufacture than a fuel injection system, the more complex carburetor systems installed on many engines in the 1970s were much more costly than the earlier simple carburetors. To more easily comply with emissions regulations, automobile manufacturers began installing fuel injection systems in more gasoline engines during the late 1970s.
The open loop fuel injection systems had already improved cylinder-to-cylinder fuel distribution and engine operation over a wide temperature range, but did not offer further scope to sufficient control fuel/air mixtures, in order to further reduce exhaust emissions. Later Closed loop fuel injection systems improved the air/fuel mixture control with an exhaust gas oxygen sensor. Although not part of the injection control, a catalytic converter further reduces exhaust emissions.
Fuel injection was phased in through the latter 1970s and 80s at an accelerating rate, with the German, French, and U.S. markets leading and the UK and Commonwealth markets lagging somewhat. Since the early 1990s, almost all gasoline passenger cars sold in first world markets are equipped with electronic fuel injection (EFI). The carburetor remains in use in developing countries where vehicle emissions are unregulated and diagnostic and repair infrastructure is sparse. Fuel injection is gradually replacing carburetors in these nations too as they adopt emission regulations conceptually similar to those in force in Europe, Japan, Australia, and North America.
Many motorcycles still utilize carburetored engines, though all current high-performance designs have switched to EFI.
NASCAR finally replaced carburetors with fuel-injection, starting at the beginning of the 2012 NASCAR Sprint Cup Series season.
System components.
System overview.
The process of determining the necessary amount of fuel, and its delivery into the engine, are known as fuel metering. Early injection systems used mechanical methods to meter fuel, while nearly all modern systems use electronic metering.
Determining how much fuel to supply.
The primary factor used in determining the amount of fuel required by the engine is the amount (by weight) of air that is being taken in by the engine for use in combustion. Modern systems use a mass airflow sensor to send this information to the engine control unit.
Data representing the amount of power output desired by the driver (sometimes known as "engine load") is also used by the engine control unit in calculating the amount of fuel required. A throttle position sensor (TPS) provides this information. Other engine sensors used in EFI systems include a coolant temperature sensor, a camshaft or crankshaft position sensor (some systems get the position information from the distributor), and an oxygen sensor which is installed in the exhaust system so that it can be used to determine how well the fuel has been combusted, therefore allowing closed loop operation.
Supplying the fuel to the engine.
Fuel is transported from the fuel tank (via fuel lines) and pressurised using fuel pump(s). Maintaining the correct fuel pressure is done by a fuel pressure regulator. Often a fuel rail is used to divide the fuel supply into the required number of cylinders. The fuel injector injects liquid fuel into the intake air (the location of the fuel injector varies between systems). 
Unlike carburettor-based systems, where the float chamber provides a reservoir, fuel injected systems depend on an uninterrupted flow of fuel. To avoid fuel starvation when subject to lateral G-forces, vehicles are often provided by an anti-surge vessel, usually integrated in the fuel tank, but sometimes as a separate, small anti-surge tank.
EFI gasoline engine components.
"Note: These examples specifically apply to a modern EFI gasoline engine. Parallels to fuels other than gasoline can be made, but only conceptually."
Engine control unit.
The engine control unit is central to an EFI system. The ECU interprets data from input sensors to, among other tasks, calculate the appropriate amount of fuel to inject.
Fuel injector.
When signalled by the engine control unit the fuel injector opens and sprays the pressurised fuel into the engine. The duration that the injector is open (called the pulse width) is proportional to the amount of fuel delivered. Depending on the system design, the timing of when injector opens is either relative each individual cylinder (for a sequential fuel injection system), or injectors for multiple cylinders may be signalled to open at the same time (in a batch fire system).
Target air/fuel ratios.
The relative proportions of air and fuel vary according to the type of fuel used and the performance requirements (i.e. power, fuel economy, or exhaust emissions).
See air-fuel ratio, stoichiometry, and combustion.
Various injection schemes.
Single-point injection.
Single-point injection uses a single injector at the throttle body (the same location as was used by carburetors).
It was introduced in the 1940s in large aircraft engines (then called the pressure carburetor) and in the 1980s in the automotive world (called Throttle-body Injection by General Motors, Central Fuel Injection by Ford, PGM-CARB by Honda, and EGI by Mazda). Since the fuel passes through the intake runners (like a carburetor system), it is called a "wet manifold system".
The justification for single-point injection was low cost. Many of the carburetor's supporting components- such as the air cleaner, intake manifold, and fuel line routing- could be reused. This postponed the redesign and tooling costs of these components. Single-point injection was used extensively on American-made passenger cars and light trucks during 1980-1995, and in some European cars in the early and mid-1990s.
Continuous injection.
In a continuous injection system, fuel flows at all times from the fuel injectors, but at a variable flow rate. This is in contrast to most fuel injection systems, which provide fuel during short pulses of varying duration, with a constant rate of flow during each pulse. Continuous injection systems can be multi-point or single-point, but not direct.
The most common automotive continuous injection system is Bosch's K-Jetronic, introduced in 1974. K-Jetronic was used for many years between 1974 and the mid-1990s by BMW, Lamborghini, Ferrari, Mercedes-Benz, Volkswagen, Ford, Porsche, Audi, Saab, DeLorean, and Volvo. Chrysler used a continuous fuel injection system on the 1981-1983 Imperial.
In piston aircraft engines, continuous-flow fuel injection is the most common type. In contrast to automotive fuel injection systems, aircraft continuous flow fuel injection is all mechanical, requiring no electricity to operate. Two common types exist: the Bendix RSA system, and the TCM system. The Bendix system is a direct descendant of the pressure carburetor. However, instead of having a discharge valve in the barrel, it uses a "flow divider" mounted on top of the engine, which controls the discharge rate and evenly distributes the fuel to stainless steel injection lines to the intake ports of each cylinder. The TCM system is even more simple. It has no venturi, no pressure chambers, no diaphragms, and no discharge valve. The control unit is fed by a constant-pressure fuel pump. The control unit simply uses a butterfly valve for the air, which is linked by a mechanical linkage to a rotary valve for the fuel. Inside the control unit is another restriction, which controls the fuel mixture. The pressure drop across the restrictions in the control unit controls the amount of fuel flow, so that fuel flow is directly proportional to the pressure at the flow divider. In fact, most aircraft that use the TCM fuel injection system feature a fuel flow gauge that is actually a pressure gauge calibrated in "gallons per hour" or "pounds per hour" of fuel.
Central port injection.
From 1992 to 1996 General Motors implemented a system called Central Port Injection or Central Port Fuel Injection. The system uses tubes with poppet valves from a central injector to spray fuel at each intake port rather than the central throttle-body. Fuel pressure is similar to a single-point injection system. CPFI (used from 1992 to 1995) is a batch-fire system, while CSFI (from 1996) is a sequential system.
Multiport fuel injection.
Multiport fuel injection injects fuel into the intake ports just upstream of each cylinder's intake valve, rather than at a central point within an intake manifold. MPFI (or just MPI) systems can be sequential, in which injection is timed to coincide with each cylinder's intake stroke; batched, in which fuel is injected to the cylinders in groups, without precise synchronization to any particular cylinder's intake stroke; or simultaneous, in which fuel is injected at the same time to all the cylinders. The intake is only slightly wet, and typical fuel pressure runs between 40-60 psi.
Many modern EFI systems utilize sequential MPFI; however, in newer gasoline engines, direct injection systems are beginning to replace sequential ones.
Direct injection.
In a direct injection engine, fuel is injected into the combustion chamber as opposed to injection before the intake valve (petrol engine) or a separate pre-combustion chamber (diesel engine).
In a common rail system, the fuel from the fuel tank is supplied to the common header (called the accumulator). This fuel is then sent through tubing to the injectors, which inject it into the combustion chamber. The header has a high pressure relief valve to maintain the pressure in the header and return the excess fuel to the fuel tank. The fuel is sprayed with the help of a nozzle that is opened and closed with a needle valve, operated with a solenoid. When the solenoid is not activated, the spring forces the needle valve into the nozzle passage and prevents the injection of fuel into the cylinder. The solenoid lifts the needle valve from the valve seat, and fuel under pressure is sent in the engine cylinder. Third-generation common rail diesels use piezoelectric injectors for increased precision, with fuel pressures up to 1800 bar.
Direct fuel injection costs more than indirect injection systems: the injectors are exposed to more heat and pressure, so more costly materials and higher-precision electronic management systems are required.
Diesel engines.
Most diesel engines (with the exception of some tractors and scale model engines) have fuel injected into the combustion chamber.
Earlier systems, relying on simpler injectors, often injected into a sub-chamber shaped to swirl the compressed air and improve combustion; this was known as indirect injection. However, this was less efficient than the now common direct injection in which initiation of combustion takes place in a depression (often toroidal) in the crown of the piston.
Throughout the early history of diesels, they were always fed by a mechanical pump with a small separate chamber for each cylinder, feeding separate fuel lines and individual injectors. Most such pumps were in-line, though some were rotary.
Most modern diesel engines use common rail or unit injector direct injection systems.
Gasoline engines.
Modern gasoline engines also utilise direct injection, which is referred to as gasoline direct injection. This is the next step in evolution from multi-point fuel injection, and offers another magnitude of emission control by eliminating the "wet" portion of the induction system along the inlet tract.
By virtue of better dispersion and homogeneity of the directly injected fuel, the cylinder and piston are cooled, thereby permitting higher compression ratios and earlier ignition timing, with resultant enhanced power output. More precise management of the fuel injection event also enables better control of emissions. Finally, the homogeneity of the fuel mixture allows for leaner air/fuel ratios, which together with more precise ignition timing can improve fuel efficiency. Along with this, the engine can operate with stratified (lean burn) mixtures, and hence avoid throttling losses at low and part engine load. Some direct-injection systems incorporate piezoelectronic fuel injectors. With their extremely fast response time, multiple injection events can occur during each cycle of each cylinder of the engine.
Swirl injection.
Swirl injectors are used in liquid rocket, gas turbine, and diesel engines to improve atomization and mixing efﬁciency.
The circumferential velocity component is ﬁrst generated as the propellant enters through helical or tangential inlets producing a thin, swirling liquid sheet. A gas-ﬁlled hollow core is then formed along the centerline inside the injector due to centrifugal force of the liquid sheet. Because of the presence of the gas core, the discharge coefﬁcient is generally low. In swirl injector, the spray cone angle is controlled by the ratio of the circumferential velocity to the axial velocity and is generally wide compared with nonswirl injectors.
Maintenance hazards.
Fuel injection introduces potential hazards in engine maintenance due to the high fuel pressures used. Residual pressure can remain in the fuel lines long after an injection-equipped engine has been shut down. This residual pressure must be relieved, and if it is done so by external bleed-off, the fuel must be safely contained. If a high-pressure diesel fuel injector is removed from its seat and operated in open air, there is a risk to the operator of injury by hypodermic jet-injection, even with only 100 psi pressure. The first known such injury occurred in 1937 during a diesel engine maintenance operation.

</doc>
<doc id="37162" url="http://en.wikipedia.org/wiki?curid=37162" title="Roland Freisler">
Roland Freisler

Roland Freisler (30 October 1893 – 3 February 1945) was a prominent and notorious Nazi lawyer and judge. He was State Secretary of the Reich Ministry of Justice and President of the People's Court ("Volksgerichtshof"), which was set up outside constitutional authority. This court handled cases of political actions against Adolf Hitler's National-Socialist regime by conducting a series of infamous trials.
Early life.
In contrast to most of the Nazi leadership, not much beyond basic detail is known about Freisler. He was born on 30 October 1893 in Celle, the son of Julius Freisler (20 August 1862 in Klantendorf, Moravia – ??), an engineer and teacher, and Charlotte Auguste Florentine Schwerdtfeger (30 April 1863 in Celle – 20 March 1932 in Kassel). He was baptised as a Protestant on 13 December. Roland had a younger brother, Oswald (29 December 1895 in Hamelin – 4 March 1939 in Berlin). Roland saw active service during World War I. He was an officer cadet in 1914, and by 1915 he was a Lieutenant. He won the Iron Cross of both classes. In October 1915, after fighting and being wounded at the Eastern Front, he was captured by Russian troops.
While a prisoner of war in Russia, Freisler learned Russian. He is said to have developed an interest in Marxism after the Russian Revolution; the Bolsheviks made use of him as a commissar for the camp's food supplies. It is also said that after the prisoner camps were dissolved in 1918, Freisler became a convinced Communist, though this is not supported by any contemporaneous documents. However, historian H. W. Koch states that after the Bolshevik Revolution, the POW camps in Russia were handed over to German administration, and the title of commissar was merely functional, not political, and that "Freisler was never a Communist, though in the early days of his NS career [...] he belonged to the NSDAP's left wing."
Freisler himself rejected all accusations that he had even tentatively approached the hated enemy, but he could never fully escape the stigma of being a "bolshie".
He returned to Germany in 1920 to study law at the University of Jena, becoming a Doctor of Law in 1922. From 1924, he worked as a lawyer in Kassel. He was also elected a city councillor, as a member of the "Völkisch-Sozialer Block" (German, roughly "People's Social Block"), an extreme nationalist splinter party.
On 24 March 1928, he married Marion Russegger. Together, they had two sons, Harald and Roland.
Involvement with the Nazi Party.
Freisler joined the Nazi Party in July 1925. He registered with the NSDAP as member number 9679. During this period, he served as defence counsel for members of the nascent Party who got into trouble with the law. He was also a delegate to the Prussian Landtag, or state legislature, and later he became a member of the Reichstag.
In 1927, the "Gauleiter" of Kurhessen, Karl Weinrich, characterised Freisler in the following manner: 
Rhetorically Freisler is equal to our best speakers, if not superior. Particularly on the broad masses, he has influence, but thinking people mostly reject him. Party Comrade Freisler is only usable as a speaker. He is unsuitable for any leadership post, since he is an unreliable and moody person.
Career under Hitler.
In February 1933, Freisler was appointed department head in the Prussian Ministry of Justice. He was Secretary of State in the Prussian Ministry of Justice in 1933–1934, and in the Reich Ministry of Justice from 1934–1942. He represented the latter at the Wannsee Conference (20 January 1942), where he stood in for Minister Franz Schlegelberger, as regarding the detailed plans of the Final Solution, the murder of all European Jews.
Freisler's mastery of legal texts, mental agility and overwhelming verbal force combined well with strict adherence to the party line and the corresponding ideology, so that he became the most feared judge and the personification of the Nazis' "blood justice". Despite his undisputed legal competence, he was never appointed to cabinet. According to Uwe Wesel, this can be attributed to two factors. 
Firstly, Roland Freisler was regarded as a lone fighter and had no influential patron. Secondly, he was compromised by his brother Oswald Freisler's actions. Oswald, though also a Nazi, appeared as the defence counsel in politically significant trials which the Nazis sought to use for propaganda purposes. Oswald even wore his Nazi Party badge in court, which confused the Party's role in these trials. Propaganda minister Joseph Goebbels accordingly reproved Roland Freisler and reported the incident to Hitler, who, for his part, decreed the immediate exclusion of Oswald Freisler from the party.
According to Guido Knopp, however, Goebbels was the only Nazi leader well disposed towards Freisler. In 1941, at a round-table discussion in the "Führer"‍ '​s headquarters, Goebbels proposed Freisler to replace Reich Justice Minister Franz Gürtner, who had died. Allegedly, Hitler's dismissive retort was: "That old Bolshevik? No!" Uwe Wesel reports a similar remark by Hitler.
Contribution to the Nazification of the law.
Freisler published an article on ""Die rassebiologische Aufgabe bei der Neugestaltung des Jugendstrafrechts" ("The racial-biological task involved in the reform of Juvenile Criminal Law"). Freisler argued that "racially foreign, racially degenerate, racially incurable or seriously defective juveniles" should be sent to juvenile centres or correctional education centres and be segregated from those who are "German and racially valuable."
He strongly supported rigid laws against "Rassenschande" ("race defilement", the Nazi term for sexual relations between "Aryans" and "inferior races") as racial treason. In 1933, Freisler published a pamphlet that called for banning "mixed-blood" intercourse, regardless of the kind or proportion of "foreign blood" involved, which faced strong public criticism and, at the time, no support from Hitler. This led to conflict with his superior, Franz Gürtner.
In October 1939, Freisler introduced the concept of 'precocious juvenile criminal' in the "Juvenile Felons Decree". This decree "provided the legal basis for imposing the death penalty and penitentiary terms on juveniles for the first time in German legal history". In the period 1933 to 1945, the courts sentenced at least 72 German juveniles to death, among them 17-year-old Helmuth Hübener, found guilty of high treason for distributing anti-war leaflets in 1942.
The "Decree against National Parasites" (September 1939) introduced the term "perpetrator type", which was used in combination with another Nazi term, "parasite," The adoption of racial biological terminology portrayed juvenile criminality as parasitic, implying the need for harsher sentences. Freisler justified the new measures in the following manner: "In times of war, breach of loyalty and baseness cannot find any leniency and must be met with the full force of the law."
Presidency of the People's Court.
On 20 August 1942, Hitler promoted Otto Georg Thierack to Reich Justice Minister, replacing the retiring Schlegelberger, and named Freisler to succeed Thierack as president of the People's Court ("Volksgerichtshof"). This court, set up outside the frame of law, had jurisdiction over a rather broad array of "political offences", including black marketeering, work slowdowns and defeatism. These actions were viewed by Freisler's court as "Wehrkraftzersetzung" (undermining defensive capability) and were accordingly punished severely, the death penalty being meted out in numerous cases. The People's Court almost always sided with the prosecution, to the point that being brought before it was tantamount to a death sentence. Not surprisingly, it was viewed as a kangaroo court.
Freisler chaired the First Senate of the People's Court, and acted as judge, jury and prosecution embodied into one man. He also acted as court recorder; that way, he was responsible for the composition of the written grounds for the sentences that he wrote up in his own unique fashion, namely in accordance with his own notions of a "National Socialist criminal court".
The number of death sentences rose sharply under Freisler's stewardship. Approximately 90% of all proceedings ended with sentences of death or life imprisonment, the sentences frequently having been determined before the trial. Between 1942 and 1945, more than 5,000 death sentences were handed out, and of these, 2,600 through the court's First Senate, which Freisler headed. Thus, Freisler alone was responsible, in his three years on the court, for as many death sentences as all other senate sessions of the court put together in the entire time the court existed, between 1934 and 1945.
Freisler was known for humiliating defendants and shouting at them. He was known to be an admirer of Andrei Vyshinsky, the chief prosecutor of the Soviet purge trials, and reportedly copied his demeanor. A number of the trials for defendants in the 20 July Plot before the People's Court were filmed and recorded. In the 1944 trial against Ulrich Wilhelm Graf Schwerin von Schwanenfeld, for example, Freisler shouted so loudly that the technicians who were filming the proceeding had major problems making the defendant's words audible. Schwerin von Schwanenfeld, like many other defendants in the plot, was sentenced to death by hanging. Among this and other show trials, Freisler headed the 1943 proceedings against the members of the White Rose resistance group, and ordered many of its members to be executed by Fallbeil, a shorter German version of the French guillotine.
Death.
On 3 February 1945, Freisler was conducting a Saturday session of the People's Court, when American bombers attacked Berlin. Government and Nazi Party buildings were hit, including the Reich Chancellery, the Gestapo headquarters, the Party Chancellery and the People's Court.
According to one report, Freisler hastily adjourned court and had ordered that day's prisoners to be taken to a shelter, but paused to gather that day's files. Freisler was killed when an almost direct hit on the building caused him to be struck down by a beam in his own courtroom. His body was reportedly found crushed beneath a fallen masonry column, clutching the files that he had tried to retrieve. Among those files was that of Fabian von Schlabrendorff, a 20 July Plot member who was on trial that day and was facing execution.
According to a different report, Freisler "was killed by a bomb fragment while trying to escape from his law court to the air-raid shelter", and he "bled to death on the pavement outside the People's Court at Bellevuestrasse 15 in Berlin." Fabian von Schlabrendorff was "standing near his judge when the latter met his end."
Freisler's death saved Schlabrendorff, who after the war became a judge of the Constitutional Court of the Federal Republic of Germany ("Bundesverfassungsgericht").
Yet another version of Freisler's death states that he was killed by a British bomb that came through the ceiling of his courtroom as he was trying two women, who survived the explosion.
A foreign correspondent reported, "Apparently nobody regretted his death." Luise Jodl, then the wife of General Alfred Jodl, recounted more than 25 years later that she had been working at the Lützow Hospital when Freisler's body was brought in, and that a worker commented, "It is God's verdict." According to Mrs Jodl, "Not one person said a word in reply."
Freisler is interred in the plot of his wife's family at the Waldfriedhof Dahlem cemetery in Berlin. His name is not shown on the gravestone.
Fictional portrayals.
Freisler appears in fictionalised form in the 1947 Hans Fallada novel Alone in Berlin. In 1943 he tried and handed down death penalties to Otto and Elise Hampel, whose true story inspired Fallada's novel.
Freisler has been portrayed by screen actors at least five times: by Rainer Steffen in the 1984 German television film "Wannseekonferenz", by Roland Schäfer in the 1989 Anglo-French-German film "Reunion", by Brian Cox in the British 1996 television film "Witness Against Hitler", by Owen Teale in the 2001 BBC/HBO film "Conspiracy", by André Hennicke in the 2005 film "Sophie Scholl – The Final Days", and by Helmut Stauss in the 2008 film "Valkyrie".

</doc>
<doc id="37165" url="http://en.wikipedia.org/wiki?curid=37165" title="Grand Slam">
Grand Slam

Grand Slam, coming from the contract bridge term, may refer to:

</doc>
<doc id="37166" url="http://en.wikipedia.org/wiki?curid=37166" title="Openlaw">
Openlaw

Openlaw is a project at the Berkman Center for Internet and Society at Harvard Law School aimed at releasing case arguments under a copyleft license, in order to encourage public suggestions for improvement.
Berkman lawyers specialise in cyberlaw—hacking, copyright, encryption and so on—and the centre has strong ties with the EFF and the open source software community.
In 1998 faculty member Lawrence Lessig, now at Stanford Law School, was asked by online publisher Eldritch Press to mount a legal challenge to US copyright law. Eldritch takes books whose copyright has expired and publishes them on the Web,
but legislation called the Sonny Bono Copyright Term Extension Act extended copyright from 50 to 70 years after the author's death, cutting off its supply of new material.
Lessig invited law students at Harvard and elsewhere to help craft legal arguments challenging the new law on an online forum, which evolved into Open Law.
Normal law firms write arguments the way commercial software companies write code. Lawyers discuss a case behind closed doors, and although their final product is released in court, the discussions or "source code" that produced it remain secret. In contrast, Open Law crafts its arguments in public and releases them under a copyleft. "We deliberately used free software as a model," said Wendy Seltzer, who took over Open Law when Lessig moved to Stanford. Around 50 legal scholars worked on Eldritch's case, and Open Law has taken other cases, too.
"The gains are much the same as for software," Seltzer says. "Hundreds of people scrutinise the 'code' for bugs, and make suggestions how to fix it. And people will take underdeveloped parts of the argument, work on them, then patch them in." Armed with arguments crafted in this way, OpenLaw took Eldritch's case—deemed unwinnable at the outset—right through the system to the Supreme Court. The case, Eldred v. Ashcroft, lost in 2003.
Among the drawbacks to this approach: the arguments are made in public from the start, so OpenLaw can't spring a surprise in court. Nor can it take on cases where confidentiality is important. But where there's a strong public interest element, open sourcing has big advantages. Citizens' rights groups, for example, have taken parts of Open Law's legal arguments and used them elsewhere. "People use them on letters to Congress, or put them on flyers," Seltzer says.
Read further.
This modified article was originally written by New Scientist magazine (see http://www.newscientist.com/hottopics/copyleft/) and released under the copyleft license.

</doc>
<doc id="37167" url="http://en.wikipedia.org/wiki?curid=37167" title="Loris">
Loris

Loris is the common name for the strepsirrhine primates of the subfamily Lorinae (sometimes spelled Lorisinae) in the family Lorisidae. "Loris" is one genus in this subfamily and includes the slender lorises, while "Nycticebus" is the genus containing the slow lorises.
Lorises are nocturnal. They are found in tropical and woodland forests of India, Sri Lanka, and parts of southeast Asia. Loris locomotion is a slow and cautious climbing form of quadrupedalism. Some lorises are almost entirely insectivorous, while others also include fruits, gums, leaves, and slugs in their diet.
Female lorises practice infant parking, leaving their young infants behind in nests. Before they do this, they bathe their young with allergenic saliva that is acquired by licking patches on the insides of their elbows, which produce a mild toxin that discourages most predators, though orangutans occasionally eat lorises.
Taxonomic classification.
The family Lorisidae is found within the infraorder Lemuriformes and superfamily Lorisoidea, along with the family Galagidae, the galagos. This infraorder is a sister taxon of Lemuriformes, the lemurs. Within Lorinae, there are ten species (and several more subspecies) of lorises across two genera:

</doc>
<doc id="37168" url="http://en.wikipedia.org/wiki?curid=37168" title="Friday the 13th">
Friday the 13th

Friday the 13th, also known as Black Friday, is considered an unlucky day in Western superstition. It occurs when the 13th day of the month in the Gregorian calendar falls on a Friday. 
History.
King Philip IV of France, in collusion with then Pope Clement V, had Grand Master Jacques de Molay and sixty of his Templar brothers arrested as apostates on Friday the thirteenth of October 1307. This was done in order to claim the famed properties and wealth of the Knights Templars as their own. 
The fear of the number 13 has been given a scientific name: "triskaidekaphobia"; and on analogy to this the fear of Friday the 13th is called "paraskevidekatriaphobia", from the Greek words "Paraskeví" (Παρασκευή, meaning "Friday"), and "dekatreís" (δεκατρείς, meaning "thirteen").
The superstition surrounding this day may have arisen in the Middle Ages, "originating from the story of Jesus' last supper and crucifixion" in which there were 13 individuals present in the Upper Room on the 13th of Nisan Maundy Thursday, the night before his death on Good Friday. While there is evidence of both Friday and the number 13 being considered unlucky, there is no record of the two items being referred to as especially unlucky in conjunction before the 19th century.
An early documented reference in English occurs in Henry Sutherland Edwards' 1869 biography of Gioachino Rossini, who died on a Friday 13th:
He [Rossini] was surrounded to the last by admiring friends; and if it be true that, like so many Italians, he regarded Fridays as an unlucky day and thirteen as an unlucky number, it is remarkable that on Friday 13th of November he passed away.
It is possible that the publication in 1907 of Thomas W. Lawson's popular novel "Friday, the Thirteenth", contributed to disseminating the superstition. In the novel, an unscrupulous broker takes advantage of the superstition to create a Wall Street panic on a Friday the 13th.
The legend connecting the superstition with the date of Friday, 13 October 1307, when hundreds of the Knights Templar were arrested by King Philip IV, may date to the 20th century. It was referred to in Dan Brown's 2003 novel "The Da Vinci Code,' in Steve Berry's "The Templar Legacy' and in John J. Robinson's 1989 work "Born in Blood: The Lost Secrets of Freemasonry", and also in the Maurice Druon historical novel series: "The Accursed Kings" ("Les Rois Maudits").
Tuesday the 13th in Hispanic and Greek culture.
In Spanish-speaking countries, instead of Friday, Tuesday the 13th ("martes trece") is considered a day of bad luck. The Greeks also consider Tuesday (and especially the 13th) an unlucky day. Tuesday is considered dominated by the influence of Ares, the god of war. A connection can be seen in the etymology of the name in some European languages (Mardi in French or martes in Spanish). The fall of Constantinople to the Fourth Crusade occurred on Tuesday, April 13, 1204, and the Fall of Constantinople to the Ottomans happened on Tuesday, 29 May 1453, events that strengthen the superstition about Tuesday. In addition, in Greek the name of the day is Triti ("Τρίτη") meaning literally the third (day of the week), adding weight to the superstition, since bad luck is said to "come in threes".
Friday the 17th in Italy.
In Italian popular culture, Friday the 17th (and not the 13th) is considered a day of bad luck. The origin of this belief could be traced in the writing of number 17, in ancient Latin: XVII. By shuffling the digits of the number one can easily get the word VIXI ("I have lived", implying death in the present), an omen of bad luck. In fact, in Italy, 13 is generally considered a lucky number. However, due to Americanization, young people consider Friday the 13th unlucky as well.
The 2000 parody film "Shriek If You Know What I Did Last Friday the Thirteenth" was released in Italy with the title "Shriek – Hai impegni per venerdì 17?" ("Shriek – Do You Have Something to Do on Friday the 17th?").
Social impact.
According to the Stress Management Center and Phobia Institute in Asheville, North Carolina, an estimated 17 to 21 million people in the United States are affected by a fear of this day, making it the most feared day and date in history. Some people are so paralyzed by fear that they avoid their normal routines in doing business, taking flights or even getting out of bed. "It's been estimated that [US]$800 or $900 million is lost in business on this day". Despite this, representatives for both Delta Air Lines and Continental Airlines have stated that their airlines do not suffer from any noticeable drop in travel on those Fridays.
In Finland, a consortium of governmental and nongovernmental organizations led by the Ministry of Social Affairs and Health promotes the National Accident Day, which always falls on a Friday 13th.
Rate of accidents.
A study in the "British Medical Journal", published in 1993, concluded that there "is a significant level of traffic-related incidences on Friday the 13th as opposed to a random day, such as Friday the 6th, in the UK." However, the Dutch Centre for Insurance Statistics (CVS) on 12 June 2008 stated that "fewer accidents and reports of fire and theft occur when the 13th of the month falls on a Friday than on other Fridays, because people are preventatively more careful or just stay home. Statistically speaking, driving is slightly safer on Friday the 13th, at least in the Netherlands; in the last two years, Dutch insurers received reports of an average 7,800 traffic accidents each Friday; but the average figure when the 13th fell on a Friday was just 7,500."
Occurrence.
The following months have a Friday the 13th:
This sequence, given here for 1900–2099, follows a 28-year cycle from 1 March 1900 to 28 February 2100. The months with a Friday the 13th are determined by the Dominical letter (G, F, GF, etc.) of the year. Any month that starts on a Sunday contains a Friday the 13th, and there is at least one Friday the 13th in every calendar year. There can be as many as three Friday the 13ths in a single calendar year; either in February, March and November in a common year starting on Thursday (such as 2009 or 2015) (D), or January, April and July in a leap year starting on Sunday (such as 2012) (AG).
The longest period that can occur without a Friday the 13th is fourteen months, either from July to September the following year being a common year starting on Tuesday (e.g., between 2001–02, 2012–13, and 2018–19), or from August to October the following year being a leap year starting on Saturday (e.g., between 1999–2000 or 2027–28).
Each Gregorian 400-year cycle contains 146,097 days (365 × 400 = 146,000 normal days, plus 97 leap days). 146,097 days ÷ 7 days per week = 20,871 weeks. Thus, each cycle contains the same pattern of days of the week (and thus the same pattern of Fridays that are on the 13th). The 13th day of the month is slightly more likely to be a Friday than any other day of the week. On average, there is a Friday the 13th once every 212.35 days (compared to Thursday the 13th, which occurs only once every 213.59 days).
The distribution of the 13th day over the 4,800 months is as follows:

</doc>
<doc id="37170" url="http://en.wikipedia.org/wiki?curid=37170" title="Red slender loris">
Red slender loris

The red slender loris ("Loris tardigradus") is a small, nocturnal strepsirrhine primate native to the rainforests of Sri Lanka. This is #6 of the 10 focal species and #22 of the 100 EDGE mammal species worldwide considered the most evolutionarily distinct and globally endangered. Two subspecies have been identified, "L. t. tardigradus" and "L. t. nycticeboides".
Taxonomy.
The ears are less prominent in "L. tardigradus tardigradus" compared to "Loris lydekkerianus". The ears of "L. tardigradus nycticeboides" are almost invisible.
Description.
This small, slender primate is distinguished by large forward-facing eyes used for precise depth perception, long slender limbs, a well-developed index finger, the absence of tail, and large prominent ears, which are thin, rounded and hairless at the edges. The soft dense fur is reddish-brown color on the back, and the underside is whitish-grey with a sprinkling of silver hair. Its body length on average is 7 –, with an average weight of a mere 3 –. This loris has a four-way grip on each foot. The big toe opposes the other 4 toes for a pincer-like grip on branches and food. It has a dark face mask with central pale stripe, much like the slow lorises.
"L. tardigradus tardigradus" is reddish brown in the back and creamy yellow below, while "L. tardigradus nycticeboides" is dark brown dorsally and very light brown in upperparts.
Behavior.
The red slender loris favors lowland rainforests (up to 700 m in altitude), tropical rainforests and inter-monsoon forests of the south western wet-zone of Sri Lanka. Masmullah Proposed Forest Reserve harbors one of few remaining red slender loris populations, and is considered a biodiversity hotspot. The most common plant species eaten was "Humboldtia laurifolia", occurring at 676 trees/ha, with overall density at 1077 trees/ha. "Humboldtia laurifolia" is vulnerable and has a mutualistic relationship with ants, providing abundant food for lorises. Reports from the 1960s suggest that it once also occurred in the coastal zone, however it is now thought to be extinct there.
The red slender loris differ from its close relative the gray slender loris in its frequent use of rapid arboreal locomotion. It forms small social groups, containing adults of both sexes as well as young animals. This species is among the most social of the nocturnal primates. During daylight hours the animals sleep in groups in branch tangles, or curled up on a branch with their heads between their legs. The groups also undertake mutual grooming and play at wrestling. The adults typically hunt separately during the night. They are primarily insectivorous but also eat bird eggs, berries, leaves, buds and occasionally invertebrates as well as geckos and lizards. To maximize protein and nutrient uptake they consume every part of their prey, including the scales and bones. They make nests out of leaves or find hollows of trees or a similar secure place to live in.
Reproduction.
Females are dominant. The female reaches her sexual maturity at 10 months and is receptive to the male twice a year. This species mates while hanging upside down from branches; individuals in captivity will not breed if no suitable branch is available. The gestation period is 166–169 days after which the female will bear 1–2 young which feed from her for 6–7 months. The lifespan of this species is believed to be around 15–18 years in the wild.
Threats.
This slender loris is an endangered species. Habitat destruction is a major threat. It is widely trapped and killed for use in supposed remedies for eye diseases and get killed by snakes, dogs, and some fish. Other threats include: electrocution on live wires, road accidents and the pet trade.
Conservation.
The red slender loris was identified as one of the top-10 "focal species" in 2007 by the Evolutionarily Distinct and Globally Endangered (EDGE) project.
One early success has been the rediscovery of the virtually unknown Horton Plains slender loris ("Loris tardigradus nycticeboides"). Originally documented in 1937, there have only been four known encounters in the past 72 years, and for more than 60 years until 2002 the sub-species had been believed to be extinct. The sub-species was rediscovered in 2002 by a team led by Anna Nekaris in Horton Plains National Park. The late 2009 capture by a team working under the Zoological Society of London's EDGE programme has resulted in the first detailed physical examination of the Horton Plains sub-species and the first-ever photographs of it. The limited available evidence suggests there may be only about 100 animals still existing, which would make it among the top five most-threatened primates worldwide.

</doc>
<doc id="37171" url="http://en.wikipedia.org/wiki?curid=37171" title="Cray-1">
Cray-1

The Cray-1 was a supercomputer designed, manufactured and marketed by Cray Research. The first Cray-1 system was installed at Los Alamos National Laboratory in 1976 and it went on to become one of the best known and most successful supercomputers in history. The Cray-1's architect was Seymour Cray, the chief engineer was Cray Research co-founder Lester Davis.
History.
In the years 1968 to 1972 Cray was working at Control Data Corporation (CDC) on a new machine known as the CDC 8600, the logical successor to his earlier CDC 6600 and CDC 7600 designs. The 8600 was essentially made up of four 7600s in a box with an additional special mode that allowed them to operate lock-step in a SIMD fashion.
Jim Thornton, formerly Cray's engineering partner on earlier designs, had started a more radical project known as the CDC STAR-100. Unlike the 8600's brute-force approach to performance, the STAR took an entirely different route. In fact the main processor of the STAR had less performance than the 7600, but added additional hardware and instructions to speed up particularly common supercomputer tasks.
By 1972, the 8600 had reached a dead end — the machine was so incredibly complex that it was impossible to get one working properly. Even a single faulty component would render the machine non-operational. Cray went to William Norris, Control Data's CEO, saying that a redesign from scratch was needed. At the time the company was in serious financial trouble, and with the STAR in the pipeline as well, Norris simply could not invest the money.
As a result, Cray left CDC and started a new company HQ only yards from the CDC lab. In the back yard of the land he purchased in Chippewa Falls he and a group of former CDC employees started looking for ideas. At first the concept of building another supercomputer seemed impossible, but after Cray's Chief Technology Officer traveled to Wall Street and found a lineup of investors more than willing to back Cray, all that was needed was a design.
For four years Cray designed its first computer. In 1975 the 80 MHz Cray-1 was announced. Excitement was so high that a bidding war for the first machine broke out between Lawrence Livermore National Laboratory and Los Alamos National Laboratory, the latter eventually winning and receiving serial number 001 in 1976 for a six-month trial. The National Center for Atmospheric Research (NCAR) was first official customer of Cray Research in 1977, paying US$8.86 million ($7.9 million plus $1 million for the disks) for serial number 3. The NCAR machine was decommissioned in 1989. The company expected to sell perhaps a dozen of the machines, and set the selling price accordingly, but ultimately over eighty Cray-1s of all types were sold, priced from $5M to $8M. The machine made Cray a celebrity and the company a success, lasting until the supercomputer crash in the early 1990s.
The 80 MFLOPS Cray-1 was succeeded in 1982 by the 800 MFLOPS Cray X-MP, the first Cray multi-processing computer. In 1985 the very advanced Cray-2, capable of 1.9 GFLOPS peak performance, succeeded the first two models but met a somewhat limited commercial success because of certain problems at producing sustained performance in real-world applications. A more conservatively designed evolutionary successor of the Cray-1 and X-MP models was therefore made by the name Cray Y-MP and launched in 1988.
As a comparison standpoint, the processor in a typical modern smartphone performs at roughly 1 GFLOPS.
Background.
Typical scientific workloads consist of reading in large data sets, transforming them in some way and then writing them back out again. Normally the transformations being applied are identical across all of the data points in the set. For instance, the program might add 5 to every number in a set of a million numbers. In traditional computers the program would loop over all million numbers, adding five, thereby executing a million instructions saying codice_1. Internally the computer solves this instruction in several steps. First it reads the instruction from memory and decodes it, then it collects any additional information it needs, in this case the numbers b and c, and then finally runs the operation and stores the results.
Vector machines.
In the STAR, new instructions essentially wrote the loops for the user. The user told the machine where in memory the "big list of numbers" was stored, then fed in a single instruction codice_2. At first glance it appears the savings are limited; in this case the machine fetches and decodes only a single instruction instead of 1,000,000, thereby saving 1,000,000 fetches and decodes, perhaps one-fourth of the overall time.
The real savings are not so obvious. Internally, the CPU of the computer is built up from a number of separate parts dedicated to a single task, for instance, adding a number, or fetching from memory. Normally, as the instruction flows through the machine, only one part is active at any given time. This means that each sequential step of the entire process must complete before a result can be saved. The addition of an instruction pipeline changes this. In such machines the CPU will "look ahead" and begin fetching succeeding instructions while the current instruction is still being processed. In this assembly line fashion any one instruction still requires as long to complete, but as soon as it finishes executing, the next instruction is right behind it, with most of the steps required for its execution already completed.
Vector processors use this technique with one additional "trick". Because the data layout is "known" — a set of numbers arranged sequentially in memory — the pipelines can be tuned to improve the performance of fetches. On the receipt of a vector instruction, special hardware sets up the memory access for the arrays and stuffs the data into the processor as fast as possible.
CDC's approach in the STAR used what is today known as a "memory-memory architecture". This referred to the way the machine gathered data. It set up its pipeline to read from and write to memory directly. This allowed the STAR to use vectors of any length, making it highly flexible. Unfortunately, the pipeline had to be very long in order to allow it to have enough instructions in flight to make up for the slow memory. That meant the machine incurred a high cost when switching from processing vectors to performing operations on individual randomly located operands. Additionally, the low scalar performance of the machine meant that after the switch had taken place and the machine was running scalar instructions, the performance was quite poor. The result was rather disappointing real-world performance, something that could, perhaps, have been forecast by Amdahl's law.
Cray's approach.
Cray was able to look at the failure of the STAR and learn from it. He decided that in addition to fast vector processing, his design would also require excellent all-around scalar performance as well. That way when the machine switched modes, it would still provide superior performance. Additionally they noticed that the workloads could be dramatically improved in most cases through the use of registers.
Just as earlier machines had ignored the fact that most operations were being applied to many data points, the STAR ignored the fact that those same data points would be repeatedly operated on. Whereas the STAR would read and process the same memory five times to apply five vector operations on a set of data, it would be much faster to read the data into the CPU's registers once, and then apply the five operations. However, there were limitations with this approach. Registers were significantly more expensive in terms of circuitry, so only a limited number could be provided. This implied that Cray's design would have less flexibility in terms of vector sizes. Instead of reading any sized vector several times as in the STAR, the Cray-1 would have to read only a portion of the vector at a time, but it could then run several operations on that data prior to writing the results back to memory. Given typical workloads, Cray felt that the small cost incurred by being required to break large sequential memory accesses into segments was a cost well worth paying.
Since the typical vector operation would involve loading a small set of data into the vector registers and then running several operations on it, the vector system of the new design had its own separate pipeline. For instance, the multiplication and addition units were implemented as separate hardware, so the results of one could be internally pipelined into the next, the instruction decode having already been handled in the machine's main pipeline. Cray referred to this concept as "chaining", as it allowed programmers to "chain together" several instructions and extract higher performance.
Description.
The new machine was the first Cray design to use integrated circuits (ICs). Although ICs had been available since the 1960s, it was only in the early 1970s that they reached the performance necessary for high-speed applications. The Cray-1 used only four different IC types, an ECL dual 5-4 NOR gate (one 5-input, and one 4-input, each with differential output), another slower MECL 10K 5-4 NOR gate used for address fanout, a 16×4-bit high speed (6 ns) static RAM (SRAM) used for registers and a 1,024×1-bit 48 ns SRAM used for the main memory. These integrated circuits were supplied by Fairchild Semiconductor and Motorola. In all, the Cray-1 contained about 200,000 gates.
ICs were mounted on large five-layer printed circuit boards, with up to 144 ICs per board. Boards were then mounted back to back for cooling (see below) and placed in twenty-four 28 in racks containing 72 double-boards. The typical module (distinct processing unit) required one or two boards. In all the machine contained 1,662 modules in 113 varieties.
Each cable between the modules was a twisted pair, cut to a specific length in order to guarantee the signals arrived at precisely the right time and minimize electrical reflection. Each signal produced by the ECL circuitry was a differential pair, so the signals were balanced. This tended to make the demand on the power supply more constant and reduce switching noise. The load on the power supply was so evenly balanced that Cray boasted that the power supply was unregulated. To the power supply, the entire computer system looked like a simple resistor.
The high-performance ECL circuitry generated considerable heat, and Cray's designers spent as much effort on the design of the refrigeration system as they did on the rest of the mechanical design. In this case, each circuit board was paired with a second, placed back to back with a sheet of copper between them. The copper sheet conducted heat to the edges of the cage, where liquid Freon running in stainless steel pipes drew it away to the cooling unit below the machine. The first Cray-1 was delayed six months due to problems in the cooling system; lubricant that is normally mixed with the Freon to keep the compressor running would leak through the seals and eventually coat the boards with oil until they shorted out. New welding techniques had to be used to properly seal the tubing. The only patents issued for the Cray-1 computer concerned the cooling system design.
In order to bring maximum speed out of the machine, the entire chassis was bent into a large C-shape. Speed-dependent portions of the system were placed on the "inside edge" of the chassis, where the wire-lengths were shorter. This allowed the cycle time to be decreased to 12.5 ns (80 MHz), not as fast as the 8 ns 8600 he had given up on, but fast enough to beat CDC 7600 and the STAR. NCAR estimated that the overall throughput on the system was 4.5 times the CDC 7600.
The Cray-1 was built as a 64-bit system, a departure from the 7600/6600, which were 60-bit machines (a change was also planned for the 8600). Addressing was 24-bit, with a maximum of 1,048,571 64-bit words (1 megaword) of main memory, where each word also had 8 parity bits for a total of 72 bits per word. There were 64 data bits and 8 check bits. Memory was spread across 16 interleaved memory banks, each with a 50 ns cycle time, allowing up to four words to be read per cycle. Smaller configurations could have 0.25 or 0.5 megawords of main memory.
The main register set consisted of eight 64-bit scalar (S) registers and eight 24-bit address (A) registers. These were backed by a set of sixty-four registers each for S and A temporary storage known as T and B respectively, which could not be seen by the functional units. The vector system added another eight 64-element by 64-bit vector (V) registers, as well as a vector length (VL) and vector mask (VM). Finally, the system also included a 64-bit real-time clock register and four 64-bit instruction buffers that held sixty-four 16-bit instructions each. The hardware was set up to allow the vector registers to be fed at one word per cycle, while the address and scalar registers required two. In contrast, the entire 16-word instruction buffer could be filled in four cycles.
The Cray-1 had twelve pipelined functional units. The 24-bit address arithmetic was performed in an add unit and a multiply unit. The scalar portion of the system consisted of an add unit, a logical unit, a population count, a leading zero count unit and a shift unit. The vector portion consisted of add, logical and shift units. The floating point functional units were shared between the scalar and vector portions, and these consisted of add, multiply and reciprocal approximation units.
The system had limited parallelism. It could fetch one instruction per clock cycle, operate on multiple instructions in parallel and retire up to two every cycle. Its theoretical performance was thus 160 MIPS (80 MHz x 2 instructions), although there were a few limitations that made floating point performance generally about 160 MFLOPS. However, by using vector instructions carefully and building useful chains, the system could peak at 250 MFLOPS.
Since the machine was designed to operate on large data sets, the design also dedicated considerable circuitry to I/O. Earlier Cray designs at CDC had included separate computers dedicated to this task, but this was no longer needed. Instead the Cray-1 included four 6-channel controllers, each of which was given access to main memory once every four cycles. The channels were 16 bits wide and included 3 control bits and 4 for error correction, so the maximum transfer speed was 1 word per 100 ns, or 500 thousand words per second for the entire machine.
The initial model, the Cray-1A, weighed 5.5 tons including the Freon refrigeration system. Configured with 1 million words of main memory, the machine and its power supplies consumed about 115 kW of power; cooling and storage likely more than doubled this figure. A Data General SuperNova S/200 minicomputer served as the maintenance control unit (MCU), which was used to feed the Cray Operating System into the system at boot time, to monitor the CPU during use, and optionally as a front-end computer. Most, if not all Cray-1As were delivered using the follow-on Data General Eclipse as the MCU.
Cray-1S.
The Cray-1S, announced in 1979, was an improved Cray-1 that supported a larger main memory of 1, 2 or 4 million words. The larger main memory was made possible through the use of 4,096 x 1-bit bipolar RAM ICs with a 25 ns access time. The Data General minicomputers were optionally replaced with an in-house 16-bit design running at 80 MIPS. The I/O subsystem was separated from the main machine, connected to the main system via a 6 MB/s control channel and a 100 MB/s High Speed Data Channel. This separation made the 1S look like two "half Crays" separated by a few feet, which allowed the I/O system to be expanded as needed. Systems could be bought in a variety of configurations from the S/500 with no I/O and 0.5 million words of memory to the S/4400 with four I/O processors and 4 million words of memory.
Cray-1M.
The Cray-1M, announced in 1982, replaced the Cray-1S. It had a faster 12 ns cycle time and used less expensive MOS RAM in the main memory. The 1M was supplied in only three versions, the M/1200 with 1 million words in 8 banks, or the M/2200 and M/4200 with 2 or 4 million words in 16 banks. All of these machines included two, three or four I/O processors, and the system added an optional second High Speed Data Channel. Users could add a Solid-state Storage Device with 8 to 32 million words of MOS RAM.
Software.
In 1978 the first standard software package for the Cray-1 was released, consisting of three main products:
The United States Department of Energy funded sites from Lawrence Livermore Laboratory, Los Alamos Scientific Laboratory, Sandia National Laboratory and the National Science Foundation supercomputer centers (for high-energy physics) represented the second largest block with LLL's Cray Time Sharing System (CTSS). CTSS was written in a dynamic memory Fortran, first named LRLTRAN, which ran on CDC 7600s, renamed CVC (pronounced "Civic") when vectorization for the Cray-1 was added. Cray Research attempted to support these sites accordingly. These software choices had influences on later minisupercomputers, also known as "crayettes".
NCAR has its own operating system (NCAROS).
The National Security Agency developed its own operating system (Folklore) and language (IMP with ports of Cray Pascal and C and Fortran 90 later)
Libraries started with Cray Research's own offerings and Netlib.
Other operating systems existed, but most languages tended to be Fortran or Fortran-based. Bell Laboratories, as proof of both portability concept and circuit design, moved the first C compiler to their Cray-1 (non-vectorizing). This act would later give CRI a six-month head start on the Cray-2 Unix port to ETA Systems' detriment, and Lucasfilm's first computer generated test film, "The Adventures of André and Wally B.".
Application software generally tends to be either classified ("e.g." nuclear code, cryptanalytic code) or proprietary ("e.g." petroleum reservoir modeling). This was because little software was shared between customers and university customers. The few exceptions were climatological and meteorological programs until the NSF responded to the Japanese Fifth Generation Computer Systems project and created its supercomputer centers. Even then, little code was shared.
Museums.
Cray-1s are on display at the following locations:

</doc>
<doc id="37175" url="http://en.wikipedia.org/wiki?curid=37175" title="Lars Onsager">
Lars Onsager

Lars Onsager (November 27, 1903 – October 5, 1976) was a Norwegian-born American physical chemist and theoretical physicist, winner of the 1968 Nobel Prize in Chemistry.
He held the Gibbs Professorship of Theoretical Chemistry at Yale University.
Biography.
Lars Onsager was born in Kristiania (today's Oslo), Norway. His father was a lawyer. After completing secondary school in Oslo, he attended the Norwegian Institute of Technology (NTH) in Trondheim, graduating as a chemical engineer in 1925. 
In 1925 he arrived at a correction to the Debye-Hückel theory of electrolytic solutions, to specify Brownian movement of ions in solution, and during 1926 published it. He traveled to Zürich, where Peter Debye was teaching, and confronted Debye, telling him his theory was wrong. He impressed Debye so much that he was invited to become Debye's assistant at the Eidgenössische Technische Hochschule (ETH), where he remained until 1928.
Johns Hopkins University.
Eventually in 1928 he went to the United States of America to take a faculty position at the Johns Hopkins University in Baltimore, Maryland. At JHU he had to teach freshman classes in chemistry, and it quickly became apparent that, while he was a genius at developing theories in physical chemistry, he had little talent for teaching. He was dismissed by JHU after one semester.
Brown University.
On leaving JHU, he accepted a position (involving the teaching of statistical mechanics to graduate students in chemistry) at Brown University in Providence, Rhode Island, where it became clear that he was no better at teaching advanced students than freshmen, but he made significant contributions to statistical mechanics and thermodynamics. The only graduate student who could really understand his lectures on electrolyte systems, Raymond Fuoss, worked under him and eventually joined him on the Yale chemistry faculty. In 1933, when the Great Depression limited Brown's ability to support a faculty member who was only useful as a researcher and not a teacher, he was let go by Brown, being hired after a trip to Europe by Yale University, where he remained for most of the rest of his life, retiring in 1972.
His research at Brown was concerned mainly with the effects on diffusion of temperature gradients, and produced the Onsager reciprocal relations, a set of equations published in 1929 and, in an expanded form, in 1931, in statistical mechanics whose importance went unrecognized for many years. However, their value became apparent during the decades following World War II, and by 1968 they were considered important enough to gain Onsager that year's Nobel Prize in Chemistry. 
In 1933, just before taking up the position at Yale, Onsager traveled to Austria to visit electrochemist Hans Falkenhagen. He met Falkenhagen's sister-in-law, Margrethe Arledter. They were married on September 7, 1933, and had three sons and a daughter.
Yale University.
At Yale, an embarrassing situation occurred: he had been hired as a postdoctoral fellow, but it was discovered that he had never received a Ph.D. While he had submitted an outline of his work in reciprocal relations to the Norwegian Institute of Technology, they had decided it was too incomplete to qualify as a doctoral dissertation. He was told that he could submit one of his published papers to the Yale faculty as a dissertation, but insisted on doing a new research project instead. His dissertation, entitled, "Solutions of the Mathieu equation of period 4 pi and certain related functions", was beyond the comprehension of the chemistry and physics faculty, and only when some members of the mathematics department, including the chairman, insisted that the work was good enough that "they" would grant the doctorate if the chemistry department would not, was he granted a Ph.D. in chemistry in 1935. Even before the dissertation was finished, he was appointed assistant professor in 1934, and promoted to associate professor in 1940. He quickly showed at Yale the same traits he had at JHU and Brown: he produced brilliant theoretical research, but was incapable of giving a lecture at a level that a student (even a graduate student) could comprehend. He was also unable to direct the research of graduate students, except for the occasional outstanding one.
During the late 1930s, Onsager researched the dipole theory of dielectrics, making improvements for another topic that had been studied by Peter Debye. However, when he submitted his paper to a journal that Debye edited in 1936, it was rejected. Debye would not accept Onsager's ideas until after World War II. During the 1940s, Onsager studied the statistical-mechanical theory of phase transitions in solids, deriving a mathematically elegant theory which was enthusiastically received. He obtained the exact solution for the two dimensional Ising model in zero field in 1944. 
In 1945, Onsager was naturalized as an American citizen, and the same year he was awarded the title of J. Willard Gibbs Professor of Theoretical Chemistry. This was particularly appropriate because Onsager, like Willard Gibbs, had been involved primarily in the application of mathematics to problems in physics and chemistry and, in a sense, could be considered to be continuing in the same areas Gibbs had pioneered.
In 1947, he was elected to the National Academy of Sciences, and in 1950 he joined the ranks of Alpha Chi Sigma. 
After World War II, Onsager researched new topics of interest. 
He proposed a theoretical explanation of the superfluid properties of liquid helium in 1949; two years later the physicist Richard Feynman independently proposed the same theory. He also worked on the theories of liquid crystals and the electrical properties of ice. While on a Fulbright scholarship to Cambridge University, he worked on the magnetic properties of metals. He developed important ideas on the quantization of magnetic flux in metals. He was awarded the Lorentz Medal in 1958 and the Nobel Prize in Chemistry in 1968.
After Yale.
In 1972 Onsager retired from Yale and became emeritus. He then became a member of the Center for Theoretical Studies, University of Miami, and was appointed Distinguished University Professor of Physics. At the University of Miami he remained active in guiding and inspiring postdoctoral students as his teaching skills, although not his lecturing skills, had improved during the course of his career. He developed interests in semiconductor physics, biophysics and radiation chemistry. However, his death came before he could produce any breakthroughs comparable to those of his earlier years. 
He remained in Florida until his death from an aneurysm in Coral Gables, Florida in 1976. Onsager was buried next to John Gamble Kirkwood at New Haven's Grove Street Cemetery. While Kirkwood's tombstone has a long list of awards and positions, including the American Chemical Society Award in Pure Chemistry, the Richards Medal, and the Lewis Award, Onsager's tombstone, in its original form, simply said "Nobel Laureate." When Onsager's wife Gretel died in 1991 and was buried there, his children added an asterisk after "Nobel Laureate," and "*etc." in the lower right corner of the stone.
Legacy.
The Norwegian Institute of Technology established the Lars Onsager Lecture and The Lars Onsager Professorship in 1993 to award outstanding scientists in the scientific fields of Lars Onsager; Chemistry, Physics and Mathematics. In 1997 his sons and daughter donated his scientific works and professional belongings to NTNU (before 1996 NTH) in Trondheim, Norway as his Alma Mater. These are now organized as "The Lars Onsager Archive" at the Gunnerus Library in Trondheim.

</doc>
<doc id="37183" url="http://en.wikipedia.org/wiki?curid=37183" title="Novikov self-consistency principle">
Novikov self-consistency principle

The Novikov self-consistency principle, also known as the Novikov self-consistency conjecture, is a principle developed by Russian physicist Igor Dmitriyevich Novikov in the mid-1980s to solve the problem of paradoxes in time travel, which is theoretically permitted in certain solutions of general relativity (solutions containing what are known as closed timelike curves). The principle asserts that if an event exists that would give rise to a paradox, or to any "change" to the past whatsoever, then the probability of that event is zero. It would thus be impossible to create time paradoxes.
History of the principle.
Physicists have long been aware that there are solutions to the theory of general relativity which contain closed timelike curves, or CTCs—see for example the Gödel metric. Novikov discussed the possibility of CTCs in books written in 1975 and 1983, offering the opinion that only self-consistent trips back in time would be permitted. In a 1990 paper by Novikov and several others, "Cauchy problem in spacetimes with closed timelike curves", the authors state:
Among the coauthors of this 1990 paper were Kip Thorne, Mike Morris, and Ulvi Yurtsever, who in 1988 had stirred up renewed interest in the subject of time travel in general relativity with their paper "Wormholes, Time Machines, and the Weak Energy Condition", which showed that a new general relativity solution known as a traversable wormhole could lead to closed timelike curves, and unlike previous CTC-containing solutions it did not require unrealistic conditions for the universe as a whole. After discussions with another coauthor of the 1990 paper, John Friedman, they convinced themselves that time travel need not lead to unresolvable paradoxes, regardless of what type of object was sent through the wormhole.:509
In response, another physicist named Joseph Polchinski sent them a letter in which he argued that one could avoid questions of free will by considering a potentially paradoxical situation involving a billiard ball sent through a wormhole which sends it back in time. In this scenario, the ball is fired into a wormhole at an angle such that, if it continues along that path, it will exit the wormhole in the past at just the right angle to collide with its earlier self, thereby knocking it off course and preventing it from entering the wormhole in the first place. Thorne deemed this problem "Polchinski's paradox".:510–511
After considering the problem, two students at Caltech (where Thorne taught), Fernando Echeverria and Gunnar Klinkhammer, were able to find a solution beginning with the original billiard ball trajectory proposed by Polchinski which managed to avoid any inconsistencies. In this situation, the billiard ball emerges from the future at a different angle than the one used to generate the paradox, and delivers its younger self a glancing blow instead of knocking it completely away from the wormhole, a blow which changes its trajectory in just the right way so that it will travel back in time with the angle required to deliver its younger self this glancing blow. Echeverria and Klinkhammer actually found that there was more than one self-consistent solution, with slightly different angles for the glancing blow in each case. Later analysis by Thorne and Robert Forward showed that for certain initial trajectories of the billiard ball, there could actually be an infinite number of self-consistent solutions.:511–513
Echeverria, Klinkhammer and Thorne published a paper discussing these results in 1991; in addition, they reported that they had tried to see if they could find "any" initial conditions for the billiard ball for which there were no self-consistent extensions, but were unable to do so. Thus it is plausible that there exist self-consistent extensions for every possible initial trajectory, although this has not been proven.:184 This only applies to initial conditions which are outside of the chronology-violating region of spacetime,:187 which is bounded by a Cauchy horizon. This could mean that the Novikov self-consistency principle does not actually place any constraints on systems outside of the region of spacetime where time travel is possible, only inside it.
Even if self-consistent extensions can be found for arbitrary initial conditions outside the Cauchy Horizon, the finding that there can be multiple distinct self-consistent extensions for the same initial condition—indeed, Echeverria et al. found an infinite number of consistent extensions for every initial trajectory they analyzed:184—can be seen as problematic, since classically there seems to be no way to decide which extension the laws of physics will choose. To get around this difficulty, Thorne and Klinkhammer analyzed the billiard ball scenario using quantum mechanics,:514–515 performing a quantum-mechanical sum over histories (path integral) using only the consistent extensions, and found that this resulted in a well-defined probability for each consistent extension. The authors of "Cauchy problem in spacetimes with closed timelike curves" write:
Assumptions of the Novikov self-consistency principle.
The Novikov consistency principle assumes certain conditions about what sort of time travel is possible. Specifically, it assumes either that there is only one timeline, or that any alternative timelines (such as those postulated by the many-worlds interpretation of quantum mechanics) are not accessible.
Given these assumptions, the constraint that time travel must not lead to inconsistent outcomes could be seen merely as a tautology, a self-evident truth that cannot possibly be false, because if you make the assumption that it is false this would lead to a logical paradox. However, the Novikov self-consistency principle is intended to go beyond just the statement that history must be consistent, making the additional nontrivial assumption that the universe obeys the same local laws of physics in situations involving time travel that it does in regions of spacetime that lack closed timelike curves. This is made clear in the above-mentioned "Cauchy problem in spacetimes with closed timelike curves", where the authors write:
Time loop logic.
"Time loop logic", coined by the roboticist and futurist Hans Moravec, is the name of a hypothetical system of computation that exploits the Novikov self-consistency principle to compute answers much faster than possible with the standard model of computational complexity using Turing machines. In this system, a computer sends a result of a computation backwards through time and relies upon the self-consistency principle to force the sent result to be correct, providing the machine can reliably receive information from the future and providing the algorithm and the underlying mechanism are formally correct. An incorrect result or no result can still be produced if the time travel mechanism or algorithm are not guaranteed to be accurate.
A simple example is an iterative method algorithm. Moravec states:
Make a computing box that accepts an input, which represents an approximate solution to some problem, and produces an output that is an improved approximation. Conventionally you would apply such a computation repeatedly a finite number of times, and then settle for the better, but still approximate, result. Given an appropriate negative delay something else is possible: [...] the result of each iteration of the function is brought back in time to serve as the "first" approximation. As soon as the machine is activated, a so-called "fixed-point" of F, an input which produces an identical output, usually signaling a perfect answer, appears (by an extraordinary coincidence!) immediately and steadily. [...] If the iteration does not converge, that is, if F has no fixed point, the computer outputs and inputs will shut down or hover in an unlikely intermediate state.
Physicist David Deutsch showed in 1991 that this model of computation could solve NP problems in polynomial time, and Scott Aaronson later extended this result to show that the model could also be used to solve PSPACE problems in polynomial time.

</doc>
<doc id="37184" url="http://en.wikipedia.org/wiki?curid=37184" title="Colossus">
Colossus

Colossus or Colossos may refer to:

</doc>
<doc id="37185" url="http://en.wikipedia.org/wiki?curid=37185" title="Pope Leo VIII">
Pope Leo VIII

Pope Leo VIII (died 1 March 965) was Pope from 23 June 964 to his death in 965; before that, he was an antipope from 963 to 964, in opposition to Pope John XII and Pope Benedict V. An appointee of the Holy Roman Emperor, Otto I, his pontificate occurred during the period known as the Saeculum obscurum.
Biography.
Born in Rome in the region around the "Clivus Argentarius", Leo was the son of John who held the office of Protonotary, and a member of an illustrious noble family. Although a layperson, he was the "protoscriniarius" (or superintendent of the Roman public schools for scribes) in the papal court during the pontificate of John XII. In 963 he was included in a party that was sent by John to the Holy Roman Emperor, Otto I, who was besieging the King of Italy, Berengar II at the castle of St. Leo in Umbria. His instructions were to reassure the emperor that the pope was determined to correct the abuses of the papal court, as well as protesting about Otto’s actions in demanding that cities in the Papal States take an oath of fidelity to the emperor instead of the pope.
By the time Otto entered Rome to depose John, Leo had been appointed Protonotary to the Apostolic See. A synod convened by the emperor uncanonically deposed John (who had fled to Tibur) and proceeded to elect Leo, who was the emperor’s nominee, as pope on 4 December 963, although as he was still a layman such an election was also invalid. In the space of a day Leo was ordained Ostiarius, Lector, Acolyte, Subdeacon, Deacon and Priest by Sico, the cardinal-bishop of Ostia, who then proceeded to consecrate him as Bishop of Rome on 6 December 963. The deposed John however still had a large body of sympathisers within Rome; he offered large bribes to the Roman nobility if they would rise up and overthrow Otto and kill Leo, and so in early January 964, the Roman people staged an uprising that was quickly put down by Otto’s troops. Leo, hoping to reach out to the Roman nobility, persuaded Otto to release the hostages he had taken from the leading Roman families in exchange for their continued good behaviour. However, once Otto left Rome in around 12 January 964, the Romans again rebelled, and caused Leo to flee Rome and take refuge with Otto sometime in February 964.
John XII returned and in February convened a synod which in turn deposed Leo on 26 February 964, with John excommunicating Leo in the process. Leo remained with Otto, and with the death of John XII in May 964, the Romans elected Pope Benedict V. Otto proceeded to besiege Rome, taking Leo with him, and when the Romans eventually surrendered to Otto, Leo was reinstalled in the Lateran Palace as pope.
Together with Benedict’s clerical and lay supporters, and clad in his pontifical robes, the former Pope was then brought before Leo, who asked him how Benedict dared to assume the chair of Saint Peter while he was still alive. Benedict responded “If I have sinned, have mercy on me.” Having received a promise from the emperor that his life would be spared if he submitted, Benedict threw himself at Leo’s feet and acknowledged his guilt. Brought before a synod convened by Leo, Benedict’s ordination as Bishop was revoked, his pallium was torn from him, and his pastoral staff was broken over him by Leo. However, through the intercession of Otto, Benedict was allowed to retain the rank of deacon. Then, after having the Roman nobility swear an oath over the Tomb of Saint Peter to obey and be faithful to Leo, Otto departed Rome in late June 964.
Having been cowed by Otto, the remainder of Leo’s pontificate was reasonably trouble free. He issued numerous bulls, many of which detailed the granting of privileges to Otto and his successors. Some of the bulls were alleged to grant the German emperors the right of choosing their successors in the Kingdom of Italy, the right to nominate the Pope, and all popes, archbishops and bishops were to receive investiture from the emperor. In addition, Leo is also claimed to have relinquished to Otto all the territory of the Papal States that had been granted to the Apostolic See by Pepin the Short and Charlemagne. Although it is certain that Leo granted various concessions to his imperial patron, it is now believed that the “investiture” bulls associated with Leo were, if not completely fabricated during the Investiture Controversy, were at the very least so tampered with that it is now largely impossible to reconstruct them in their original form.
Leo VIII died on 1 March 965, and was succeeded by Pope John XIII. According to the Liber Pontificalis he was described as venerable, energetic and honourable. He had a number of streets dedicated to him in and around the "Clivus Argentarius", including the "descensus Leonis Prothi".
Status as pope.
Although Leo was for many years considered an antipope, his current status is still a source of confusion. The Annuario Pontificio makes the following point about the pontificate of Leo VIII:
”At this point, as again in the mid-eleventh century, we come across elections in which problems of harmonizing historical criteria and those of theology and canon law make it impossible to decide clearly which side possessed the legitimacy whose factual existence guarantees the unbroken lawful succession of the Successors of Saint Peter. The uncertainty that in some cases results has made it advisable to abandon the assignation of successive numbers in the list of the Popes.”
Due to Leo’s uncanonical election, it is now accepted that until the deposition of Benedict V, he was almost certainly an antipope. Further, although the deposition of John XII was invalid, the election of Benedict V certainly was canonical. However, if Liutprand of Cremona (who chronicled the events of this period) can be relied upon, if, as he wrote, Benedict did acquiesce to his deposition, and if as seems certain, no further protest was made against Leo's position, it has been the consensus of historians that he may be regarded as a true pope from July 964, to his death in 965. The fact that the next pope to assume the name Leo was consecrated Leo IX also seems to indicate that he is a true pope.

</doc>
<doc id="37186" url="http://en.wikipedia.org/wiki?curid=37186" title="Vagus nerve">
Vagus nerve

The vagus nerve ( ), historically cited as the pneumogastric nerve, is the tenth cranial nerve or CN X, and interfaces with parasympathetic control of the heart and digestive tract. The vagus nerves are paired; however, they are normally referred to in the singular.
Structure.
Upon leaving the medulla oblongata between the pyramid and the inferior cerebellar peduncle, it extends through the jugular foramen, then passes into the carotid sheath between the internal carotid artery and the internal jugular vein down to the neck, chest and abdomen, where it contributes to the innervation of the viscera. Besides giving some output to various organs, the vagus nerve comprises between 80% and 90% of afferent nerves mostly conveying sensory information about the state of the body's organs to the central nervous system.
Right and left vagus nerves descend from the cranial vault through the jugular foramina, penetrating the carotid sheath between the internal and external carotid arteries, then passing posterolateral to the common carotid artery. The cell bodies of visceral afferent fibers of the vagus nerve are located bilaterally in the inferior ganglion of the vagus nerve (nodose ganglia).
The right vagus nerve gives rise to the right recurrent laryngeal nerve, which hooks around the right subclavian artery and ascends into the neck between the trachea and esophagus. The right vagus then crosses anterior to the right subclavian artery, runs posterior to the superior vena cava, descends posterior to the right main bronchus, and contributes to cardiac, pulmonary, and esophageal plexuses. It forms the posterior vagal trunk at the lower part of the esophagus and enters the diaphragm through the esophageal hiatus.
The left vagus nerve enters the thorax between left common carotid artery and left subclavian artery and descends on the aortic arch. It gives rise to the left recurrent laryngeal nerve, which hooks around the aortic arch to the left of the ligamentum arteriosum and ascends between the trachea and esophagus. The left vagus further gives off thoracic cardiac branches, breaks up into pulmonary plexus, continues into the esophageal plexus, and enters the abdomen as the anterior vagal trunk in the esophageal hiatus of the diaphragm.
Branches.
The vagus runs parallel to the common carotid artery and internal jugular vein inside the carotid sheath.
Nuclei.
The vagus nerve includes axons which emerge from or converge onto four nuclei of the medulla:
Development.
The motor division of the vagus nerve is derived from the basal plate of the embryonic medulla oblongata, while the sensory division originates from the cranial neural crest.
Function.
The vagus nerve supplies motor parasympathetic fibers to all the organs except the suprarenal (adrenal) glands, from the neck down to the second segment of the transverse colon. The vagus also controls a few skeletal muscles, notable ones being:
This means that the vagus nerve is responsible for such varied tasks as heart rate, gastrointestinal peristalsis, sweating, and quite a few muscle movements in the mouth, including speech (via the recurrent laryngeal nerve). It also has some afferent fibers that innervate the inner (canal) portion of the outer ear (via the auricular branch, also known as Alderman's nerve) and part of the meninges. This explains why a person may cough when tickled on the ear (such as when trying to remove ear wax with a cotton swab).
The vagus nerve carries various types of axons. These include:
The vagus nerve and the heart.
Parasympathetic innervation of the heart is partially controlled by the vagus nerve and is shared by the thoracic ganglia. To be specific, vagal and spinal ganglionic nerves mediate the lowering of the heart rate. The right vagus branch innervates the sinoatrial node. Parasympathetic tone from these sources are obviously well matched to sympathetic tone in healthy people. Hyperstimulation of parasympathetic influence promotes bradyarrhythmias. When hyperstimulated, the left vagal branch predisposes the heart to conduction block at the atrioventricular node.
At this location, neuroscientist Otto Loewi first demonstrated that nerves secrete substances called neurotransmitters, which have effects on receptors in target tissues. In his experiment, Loewi electrically stimulated the vagus nerve of a frog heart, which slowed the heart. Then he took the fluid from the heart and transferred it to a second frog heart without a vagus nerve. The second heart slowed down without an electrical stimulation. Loewi described the substance released by the vagus nerve as vagusstoff, which was later found to be acetylcholine.
Drugs that inhibit the muscarinic receptors (anticholinergics) such as atropine and scopolamine, are called vagolytic because they inhibit the action of the vagus nerve on the heart, gastrointestinal tract, and other organs. Anticholinergic drugs increase heart rate and are used to treat bradycardia.
Physical and emotional effects.
Activation of the vagus nerve typically leads to a reduction in heart rate, blood pressure, or both. This occurs commonly in the setting of gastrointestinal illness such as viral gastroenteritis or acute cholecystitis, or in response to other stimuli, including carotid sinus massage, Valsalva maneuver or pain from any cause, in particular, having blood drawn. When the circulatory changes are great enough, vasovagal syncope results. Relative dehydration tends to amplify these responses. Symptoms of irritable Bowel Syndrome are thought to cause activation of the vagus nerve with many people reporting fainting, vision disturbances and dizziness, but there has been little research into this area as it is not deemed necessary and/or life-threatening.
Excessive activation of the vagal nerve during emotional stress, which is a parasympathetic overcompensation of a strong sympathetic nervous system response associated with stress, can also cause vasovagal syncope due to a sudden drop in cardiac output, causing cerebral hypoperfusion. Vasovagal syncope affects young children and women more than other groups. It can also lead to temporary loss of bladder control under moments of extreme fear.
Research has shown that women having had complete spinal cord injury can experience orgasms through the vagus nerve, which can go from the uterus, cervix, and, it is presumed, the vagina to the brain.
Insulin signaling activates the adenosine triphosphate (ATP)-sensitive potassium (KATP) channels in the arcuate nucleus, decreases AgRP release, and through the vagus nerve, leads to decreased glucose production by the liver by decreasing gluconeogenic enzymes: Phosphoenolpyruvate carboxykinase, Glucose 6-phosphatase.
Clinical significance.
Vagus nerve stimulation.
Vagus nerve stimulation (VNS) therapy using a pacemaker-like device implanted in the chest is a treatment used since 1997 to control seizures in epilepsy patients and has recently been approved for treating drug-resistant cases of clinical depression. A non-invasive VNS device that stimulates an afferent branch of the vagus nerve is also being developed and will soon undergo trials.
Clinical trials are currently underway in Antwerp, Belgium using VNS for the treatment of tonal tinnitus after a breakthrough study published in early 2011 by researchers at the University of Texas - Dallas showed successful tinnitus suppression in rats when tones were paired with brief pulses of stimulation of the vagus nerve.
VNS may also be achieved by one of the "vagal maneuvers": holding the breath for a few seconds, dipping the face in cold water, coughing, or tensing the stomach muscles as if to bear down to have a bowel movement. Patients with supraventricular tachycardia, atrial fibrillation, and other illnesses may be trained to perform vagal maneuvers (or find one or more on their own).
Vagus nerve blocking (VBLOC) therapy is similar to VNS but used only during the day. In a six-month open-label trial involving three medical centers in Australia, Mexico, and Norway, vagus nerve blocking has helped 31 obese participants lose an average of nearly 15 percent of their excess weight. A year-long 300-participant double-blind, phase II trial has begun.
Vagotomy.
Vagotomy (cutting of the vagus nerve) is a now-obsolete therapy that was performed for peptic ulcer disease. Vagotomy is currently being researched as a less invasive alternative weight-loss procedure to gastric bypass surgery. The procedure curbs the feeling of hunger and is sometimes performed in conjunction with putting bands on patients' stomachs, resulting in average weight loss of 43% at six months with diet and exercise.
One serious side-effect of a vagotomy is a vitamin B12 deficiency later in life - i.e., 10 years - that is similar to pernicious anemia. The vagus normally stimulates the stomach's parietal cells to secrete acid and intrinsic factor. Intrinsic factor is needed to absorb vitamin B12 from food. The vagotomy reduces this secretion and ultimately leads to the deficiency, which, if left untreated, causes nerve damage, tiredness, dementia, paranoia, and ultimately death.
Chagasic Disease.
The majority of gradual devastation by Chagasic neuropathy is channeled to the major parasympathetic branches of the Vagus Nerve. Depending upon load, Chagasic vagal disease can cause megaesophagus, megacolon and cardiomyopathy.
History.
Etymology.
The medieval Latin word "vagus" means literally "wandering" (the words "vagrant", "vagabond", and "vague" come from the same root). Sometimes the branches are spoken of in the plural and are thus called vagi (, ). The vagus is also called the "pneumogastric" nerve since it innervates both the lungs and the stomach.

</doc>
<doc id="37190" url="http://en.wikipedia.org/wiki?curid=37190" title="Thomas Becket">
Thomas Becket

Thomas Becket (; also known as Saint Thomas of Canterbury, Thomas of London, and later Thomas à Becket; 21 December c. 1118 (or 1120) – 29 December 1170) was Archbishop of Canterbury from 1162 until his murder in 1170. He is venerated as a saint and martyr by both the Catholic Church and the Anglican Communion. He engaged in conflict with Henry II of England over the rights and privileges of the Church and was murdered by followers of the king in Canterbury Cathedral. Soon after his death, he was canonised by Pope Alexander III.
Sources.
The main sources for the life of Becket are a number of biographies that were written by contemporaries. A few of these documents are by unknown writers, although traditional historiography has given them names. The known biographers are John of Salisbury, Edward Grim, Benedict of Peterborough, William of Canterbury, William fitz Stephen, Guernes of Pont-Sainte-Maxence, Robert of Cricklade, Alan of Tewkesbury, Benet of St Albans, and Herbert of Bosham. The other biographers, who remain anonymous, are generally given the pseudonyms of Anonymous I, Anonymous II (or Anonymous of Lambeth), and Anonymous III (or Lansdowne Anonymous). Besides these accounts, there are also two other accounts that are likely contemporary that appear in the "Quadrilogus II" and the "Thómas saga Erkibyskups". Besides these biographies, there is also the mention of the events of Becket's life in the chroniclers of the time. These include Robert of Torigni's work, Roger of Howden's "Gesta Regis Henrici Secundi" and "Chronica", Ralph Diceto's works, William of Newburgh's "Historia Rerum", and Gervase of Canterbury's works.
Early life.
Becket was born about 1118, or in 1120 according to later tradition. He was born in Cheapside, London, on 21 December, which was the feast day of St Thomas the Apostle. He was the son of Gilbert Beket and Gilbert's wife Matilda. Gilbert's father was from Thierville in the lordship of Brionne in Normandy, and was either a small landowner or a petty knight. Matilda was also of Norman ancestry, and her family may have originated near Caen. Gilbert was perhaps related to Theobald of Bec, whose family also was from Thierville. Gilbert began his life as a merchant, perhaps as a textile merchant, but by the 1120s he was living in London and was a property owner, living on the rental income from his properties. He also served as the sheriff of the city at some point. They were buried in Old St Paul's Cathedral.
One of Becket's father's wealthy friends, Richer de L'Aigle, often invited Thomas to his estates in Sussex where Becket was exposed to hunting and hawking. According to Grim, Becket learned much from Richer, who was later a signatory of the Constitutions of Clarendon against Thomas.
Beginning when he was 10, Becket was sent as a student to Merton Priory in England and later attended a grammar school in London, perhaps the one at St Paul's Cathedral. He did not study any subjects beyond the trivium and quadrivium at these schools. Later, he spent about a year in Paris around age 20. He did not, however, study canon or civil law at this time and his Latin skill always remained somewhat rudimentary. Sometime after Becket began his schooling, Gilbert Beket suffered financial reverses, and the younger Becket was forced to earn a living as a clerk. Gilbert first secured a place for his son in the business of a relative – Osbert Huitdeniers – and then later Becket acquired a position in the household of Theobald of Bec, by now the Archbishop of Canterbury.
Theobald entrusted him with several important missions to Rome and also sent him to Bologna and Auxerre to study canon law. Theobald in 1154 named Becket Archdeacon of Canterbury, and other ecclesiastical offices included a number of benefices, prebends at Lincoln Cathedral and St Paul's Cathedral, and the office of Provost of Beverley. His efficiency in those posts led to Theobald recommending him to King Henry II for the vacant post of Lord Chancellor, to which Becket was appointed in January 1155.
As Chancellor, Becket enforced the king's traditional sources of revenue that were exacted from all landowners, including churches and bishoprics. King Henry even sent his son Henry to live in Becket's household, it being the custom then for noble children to be fostered out to other noble houses. The younger Henry was reported#redirect to have said Becket showed him more fatherly love in a day than his father did for his entire life.
Primacy.
Becket was nominated as Archbishop of Canterbury in 1162, several months after the death of Theobald. His election was confirmed on 23 May 1162 by a royal council of bishops and noblemen. Henry may have hoped that Becket would continue to put the royal government first, rather than the church. The famous transformation of Becket into an ascetic occurred at this time.
Becket was ordained a priest on 2 June 1162 at Canterbury, and on 3 June 1162 was consecrated as archbishop by Henry of Blois, the Bishop of Winchester and the other suffragan bishops of Canterbury.
A rift grew between Henry and Becket as the new archbishop resigned his chancellorship and sought to recover and extend the rights of the archbishopric. This led to a series of conflicts with the king, including that over the jurisdiction of secular courts over English clergymen, which accelerated antipathy between Becket and the king. Attempts by King Henry to influence the other bishops against Becket began in Westminster in October 1163, where the King sought approval of the traditional rights of the royal government in regard to the church. This led to Clarendon, where Becket was officially asked to agree to the King's rights or face political repercussions.
The Constitutions of Clarendon.
King Henry II presided over the assemblies of most of the higher English clergy at Clarendon Palace on 30 January 1164. In sixteen constitutions, he sought less clerical independence and a weaker connection with Rome. He employed all his skills to induce their consent and was apparently successful with all but Becket. Finally, even Becket expressed his willingness to agree to the substance of the Constitutions of Clarendon, but he still refused to formally sign the documents. Henry summoned Becket to appear before a great council at Northampton Castle on 8 October 1164, to answer allegations of contempt of royal authority and malfeasance in the Chancellor's office. Convicted on the charges, Becket stormed out of the trial and fled to the Continent.
Henry pursued the fugitive archbishop with a series of edicts, aimed at all his friends and supporters as well as Becket himself; but King Louis VII of France offered Becket protection. He spent nearly two years in the Cistercian abbey of Pontigny, until Henry's threats against the order obliged him to return to Sens. Becket fought back by threatening excommunication and interdict against the king and bishops and the kingdom, but Pope Alexander III, though sympathising with him in theory, favoured a more diplomatic approach. Papal legates were sent in 1167 with authority to act as arbitrators.
In 1170, Alexander sent delegates to impose a solution to the dispute. At that point, Henry offered a compromise that would allow Thomas to return to England from exile.
Assassination.
In June 1170, Roger de Pont L'Évêque, the archbishop of York, along with Gilbert Foliot, the bishop of London, and Josceline de Bohon, the bishop of Salisbury, crowned the heir apparent, Henry the Young King, at York. This was a breach of Canterbury's privilege of coronation, and in November 1170 Becket excommunicated all three. While the three clergymen fled to the king in Normandy, Becket continued to excommunicate his opponents in the church, the news of which also reached Henry.
Upon hearing reports of Becket's actions, Henry is said to have uttered words that were interpreted by his men as wishing Becket killed. The king's exact words are in doubt and several versions have been reported. The most commonly quoted, as handed down by oral tradition, is "Will no one rid me of this turbulent priest?", but according to historian Simon Schama this is incorrect: he accepts the account of the contemporary biographer Edward Grim, writing in Latin, who gives us "What miserable drones and traitors have I nourished and brought up in my household, who let their lord be treated with such shameful contempt by a low-born cleric?" Many variations have found their way into popular culture.
Whatever Henry said, it was interpreted as a royal command, and four knights, Reginald fitzUrse, Hugh de Morville, William de Tracy, and Richard le Breton, set out to confront the Archbishop of Canterbury.
On 29 December 1170 they arrived at Canterbury. According to accounts left by the monk Gervase of Canterbury and eyewitness Edward Grim, they placed their weapons under a tree outside the cathedral and hid their mail armour under cloaks before entering to challenge Becket. The knights informed Becket he was to go to Winchester to give an account of his actions, but Becket refused. It was not until Becket refused their demands to submit to the king's will that they retrieved their weapons and rushed back inside for the killing. Becket, meanwhile, proceeded to the main hall for vespers. The four knights, wielding drawn swords, caught up with him in a spot near a door to the monastic cloister, the stairs into the crypt, and the stairs leading up into the quire of the cathedral, where the monks were chanting vespers.
Several contemporary accounts of what happened next exist; of particular note is that of Edward Grim, who was himself wounded in the attack. This is part of the account from Edward Grim:
...The wicked knight leapt suddenly upon him, cutting off the top of the crown which the unction of sacred chrism had dedicated to God. Next he received a second blow on the head, but still he stood firm and immovable. At the third blow he fell on his knees and elbows, offering himself a living sacrifice, and saying in a low voice, 'For the name of Jesus and the protection of the Church, I am ready to embrace death.' But the third knight inflicted a terrible wound as he lay prostrate. By this stroke, the crown of his head was separated from the head in such a way that the blood white with the brain, and the brain no less red from the blood, dyed the floor of the cathedral. The same clerk who had entered with the knights placed his foot on the neck of the holy priest and precious martyr, and, horrible to relate, scattered the brains and blood about the pavements, crying to the others, 'Let us away, knights; this fellow will arise no more.
Another account can be found in Expugnatio Hibernica ("Conquest of Ireland", 1189) written by Gerald of Wales.
Aftermath.
Following Becket's death, the monks prepared his body for burial. According to some accounts, it was discovered that Becket had worn a hairshirt under his archbishop's garments—a sign of penance. Soon after, the faithful throughout Europe began venerating Becket as a martyr, and on 21 February 1173—little more than two years after his death—he was canonised by Pope Alexander III in St Peter's Church in Segni. In 1173, Becket's sister Mary was appointed as abbess of Barking Abbey as reparation for the murder of her brother. On 12 July 1174, in the midst of the Revolt of 1173–1174, Henry humbled himself with public penance at Becket's tomb as well as at the church of St. Dunstan's, which became one of the most popular pilgrimage sites in England.
Becket's assassins fled north to Knaresborough Castle, which was held by Hugh de Morville, where they remained for about a year. De Morville held property in Cumbria and this may also have provided a convenient bolt-hole, as the men prepared for a longer stay in the separate kingdom of Scotland. They were not arrested and neither did Henry confiscate their lands, but he failed to help them when they sought his advice in August 1171. Pope Alexander excommunicated all four. Seeking forgiveness, the assassins travelled to Rome and were ordered by the Pope to serve as knights in the Holy Lands for a period of fourteen years.
This last also inspired Knights of Saint Thomas, incorporated in 1191 at Acre, and which was to be modelled on the Teutonic Knights. It is the only military order native to England (with chapters in not only Acre, but London, Kilkenny, and Nicosia), like the Gilbertine Order being the only monastic order native to England as well. Nevertheless, Henry VIII dissolved both of these English institutions upon passing the Reformation, rather than merging foreign orders with them and nationalising them as elements of the Protestant Church of England.
The monks were afraid that Becket's body might be stolen. To prevent this Becket's remains were placed beneath the floor of the eastern crypt of the cathedral. A stone cover was placed over the burial place with two holes where pilgrims could insert their heads and kiss the tomb; this arrangement is illustrated in the 'Miracle Windows' of the Trinity Chapel. A guard chamber (now called the Wax Chamber) had a clear view of the grave. In 1220, Becket's bones were moved to a new gold-plated and bejewelled shrine behind the high altar in the Trinity Chapel. The shrine was supported by three pairs of pillars, placed on a raised platform with three steps. This is also illustrated in one of the miracle windows. Canterbury, because of its religious history, had always seen a large number of pilgrims. However, after the death of Thomas Becket, the number of pilgrims visiting the city rose rapidly.
Cult in the Middle Ages.
On 7 July 1220, in the 50th jubilee year of his death, Becket's remains were relocated from this first tomb to a shrine, in the recently completed Trinity Chapel. This act of translation was 'one of the great symbolic events in the life of the medieval English Church' and was attended by King Henry III of England, the papal legate, the Archbishop of Canterbury Stephen Langton and large numbers of dignitaries and magnates secular and ecclesiastical. Thus a 'major new feast day was instituted, commemorating the translation, that was celebrated each July almost everywhere in England and also in many French churches.' This feast was suppressed in 1536 at the Reformation.
The shrine stood until it was destroyed in 1538, during the Dissolution of the Monasteries, on orders from King Henry VIII. The king also destroyed Becket's bones and ordered that all mention of his name be obliterated. The pavement where the shrine stood is today marked by a lit candle.
As the scion of the leading mercantile dynasty of later centuries, Mercers, Becket was very much regarded as a Londoner by the citizens and was adopted as London's co-patron saint with St Paul: both their images appeared on the seals of the city and of the Lord Mayor. The Bridge House Estates seal used only the image of Becket, while the reverse featured a depiction of his martyrdom.
Local legends regarding Becket arose after his canonisation. Though they are typical hagiographical stories, they also display Becket's particular gruffness. "Becket's Well", in Otford, Kent, is said to have been created after Becket had become displeased with the taste of the local water. Two springs of clear water are said to have bubbled up after he struck the ground with his crozier. The absence of nightingales in Otford is also ascribed to Becket, who is said to have been so disturbed in his devotions by the song of a nightingale that he commanded that none should sing in the town ever again. In the town of Strood, also in Kent, Becket is said to have caused the inhabitants of the town and their descendants to be born with tails. The men of Strood had sided with the king in his struggles against the archbishop, and to demonstrate their support, had cut off the tail of Becket's horse as he passed through the town.
The saint's fame quickly spread throughout the Norman world. The first holy image of Becket is thought to be a mosaic icon still visible in Monreale Cathedral, in Sicily, created shortly after his death. Becket's cousins obtained refuge at the Sicilian court during his exile, and King William II of Sicily wed a daughter of Henry II. The principal church of the Sicilian city of Marsala is dedicated to St Thomas Becket. Over forty-five medieval chasse reliquaries decorated in champlevé enamel showing similar scenes from Becket's life survive, including the Becket Casket in the Victoria and Albert Museum in London.
In Scotland, King William the Lion ordered the building of Arbroath Abbey in 1178. On completion in 1197 the new foundation was dedicated to Becket, whom the king had known personally while at the English court as a young man.
References.
</dl>

</doc>
<doc id="37194" url="http://en.wikipedia.org/wiki?curid=37194" title="Jonathan Edwards">
Jonathan Edwards

Jonathan Edwards may refer to:

</doc>
<doc id="37196" url="http://en.wikipedia.org/wiki?curid=37196" title="Causality">
Causality

Casualty
Causality (also referred to as causation) is the relation between an event (the "cause") and a second event (the "effect"), where the first event is understood to be responsible for the second.
In common usage, causality is also the relation between a set of factors (causes) and a phenomenon (the "effect"). Anything that affects an effect is a factor of that effect. A direct factor is a factor that affects an effect directly, that is, without any intervening factors. (Intervening factors are sometimes called "intermediate factors".) The connection between a cause(s) and an effect in this way can also be referred to as a "causal nexus".
Causes and effects are typically related to changes, events, or processes; such causes are Aristotle's moving causes. The word 'cause' is also used to mean 'explanation' or 'answer to a why question', including Aristotle's material, final, and formal causes; then the 'cause' is the explanans while the 'effect' is the explanandum. In this case, there are various recognizable kinds of 'cause'; candidates include objects, processes, properties, variables, facts, and states of affairs; failure to recognize that different kinds of 'cause' are being considered can lead to debate.
The philosophical treatment on the subject of causality extends over millennia. In the Western philosophical tradition, discussion stretches back at least to Aristotle, and the topic remains a staple in contemporary philosophy.
History.
Western philosophy.
Aristotelian.
Aristotle identified four kinds of answer or explanatory mode to various "Why?" questions. He thought that, for any given topic, all four kinds of explanatory mode were important, each in its own right. As a result of traditional specialized philosophical peculiarities of language, with translations between ancient Greek, Latin, and English, the word 'cause' is nowadays in specialized philosophical writings used to label Aristotle's four kinds.
Of Aristotle's four kinds or explanatory modes, only one, the 'efficient cause' is a cause as defined in the leading paragraph of this present article. The other three explanatory modes might be rendered material composition, structure and dynamics, and, again, criterion of completion. The word that Aristotle used was αἰτία. For the present purpose, that Greek word would be better translated as "explanation" than as "cause" as those words are most often used in current English. Another translation of Aristotle is that he meant "the four Becauses" as four kinds of answer to "why" questions.
In some works of Aristotle, the four causes are listed as (1) the essential cause, (2) the logical ground, (3) the moving cause, and (4) the final cause. In this listing, a statement of essential cause is a demonstration that an indicated object conforms to a definition of the word that refers to it. A statement of logical ground is an argument as to why an object statement is true. These are further examples of the idea that a "cause" in general in the context of Aristotle's usage is an "explanation".
The word "efficient" used here can also be translated from Aristotle as "moving" or "initiating".
Efficient causation was connected with Aristotelian physics, which recognized the four elements (earth, air, fire, water), and added the fifth element (aether). Water and earth by their intrinsic property "gravitas" or heaviness intrinsically fall toward, whereas air and fire by their intrinsic property "levitas" or lightness intrinsically rise away from, Earth's center—the motionless center of the universe—in a straight line while accelerating during the substance's approach to its natural place.
As air remained on Earth, however, and did not escape Earth while eventually achieving infinite speed—an absurdity—Aristotle inferred that the universe is finite in size and contains an invisible substance that held planet Earth and its atmosphere, the sublunary sphere, centered in the universe. And since celestial bodies exhibit perpetual, unaccelerated motion orbiting planet Earth in unchanging relations, Aristotle inferred that the fifth element, "aither", that fills space and composes celestial bodies intrinsically moves in perpetual circles, the only constant motion between two points. (An object traveling a straight line from point "A" to "B" and back must stop at either point before returning to the other.)
Left to itself, a thing exhibits "natural motion", but can—according to Aristotelian metaphysics—exhibit "enforced motion" imparted by an efficient cause. The form of plants endows plants with the processes nutrition and reproduction, the form of animals adds locomotion, and the form of humankind adds reason atop these. A rock normally exhibits "natural motion"—explained by the rock's material cause of being composed of the element earth—but a living thing can lift the rock, an "enforced motion" diverting the rock from its natural place and natural motion. As a further kind of explanation, Aristotle identified the final cause, specifying a purpose or criterion of completion in light of which something should be understood.
Aristotle himself explained,
 "Cause" means
(a) in one sense, that as the result of whose presence something comes into being—e.g., the bronze of a statue and the silver of a cup, and the classes which contain these [i.e., the material cause];
(b) in another sense, the form or pattern; that is, the essential formula and the classes which contain it—e.g. the ratio 2:1 and number in general is the cause of the octave—and the parts of the formula [i.e., the formal cause].
(c) The source of the first beginning of change or rest; e.g. the man who plans is a cause, and the father is the cause of the child, and in general that which produces is the cause of that which is produced, and that which changes of that which is changed [i.e., the efficient cause].
(d) The same as "end"; i.e. the final cause; e.g., as the "end" of walking is health. For why does a man walk? "To be healthy", we say, and by saying this we consider that we have supplied the cause [the final cause].
(e) All those means towards the end which arise at the instigation of something else, as, e.g., fat-reducing, purging, drugs and instruments are causes of health; for they all have the end as their object, although they differ from each other as being some instruments, others actions [i.e., necessary conditions].
 — Metaphysics, Book 5, section 1013a, translated by Hugh Tredennick
Aristotle further discerned two modes of causation: proper (prior) causation and accidental (chance) causation. All causes, proper and accidental, can be spoken as potential or as actual, particular or generic. The same language refers to the effects of causes, so that generic effects are assigned to generic causes, particular effects to particular causes, and actual effects to operating causes.
Averting infinite regress, Aristotle inferred the first mover—an unmoved mover. The first mover's motion, too, must have been caused, but, being an unmoved mover, must have moved only toward a particular goal or desire. So the universe of material causes, formal causes, and efficient causes reflected the universe's final cause.
Middle Ages.
In line with Aristotelian cosmology, Thomas Aquinas posed a hierarchy prioritizing Aristotle's four causes: "final > efficient > material > formal". Aquinas sought to identify the first efficient cause—now simply "first cause"—as everyone would agree, said Aquinas, to call it "God". Later in the Middle Ages, many scholars conceded that the first cause was God, but explained that many earthly events occur within God's design or plan, and thereby scholars sought freedom to investigate the numerous "secondary causes".
After the Middle Ages.
For Aristotelian philosophy before Aquinas, the word cause had a broad meaning. It meant 'answer to a why question' or 'explanation', and Aristotelian scholars recognized four kinds of such answers. With the end of the Middle Ages, in many philosophical usages, the meaning of the word 'cause' narrowed. It often lost that broad meaning, and was restricted to just one of the four kinds. For authors such as Niccolò Machiavelli, in the field of political thinking, and Francis Bacon, concerning science more generally, Aristotle's moving cause was the focus of their interest. A widely used modern definition of causality in this newly narrowed sense was assumed by David Hume. He undertook an epistemological and metaphysical investigation of the notion of moving cause. He denied that we can ever perceive cause and effect, except by developing a habit or custom of mind where we come to associate two types of object or event, always contiguous and occurring one after the other. In Part III, section XV of his book A Treatise of Human Nature, Hume expanded this to a list of eight ways of judging whether two things might be cause and effect. The first three:
And then additionally there are three connected criteria which come from our experience and which are "the source of most of our philosophical reasonings":
And then two more:
In 1949, physicist Max Born distinguished determination from causality. For him, determination meant that actual events are so linked by laws of nature that certainly reliable predictions and retrodictions can be made from sufficient present data about them. For him, there are two kinds of causation, which we may here call nomic or generic causation, and singular causation. Nomic causality means that cause and effect are linked by more or less certain or probabilistic general laws covering many possible or potential instances; we may recognize this as a probabilized version of criterion 3. of Hume mentioned just above. Singular causation means that unique particular chains of actual events are essentially and physically linked by antecedence and contiguity, which we may here recognize as criteria 1. and 2. of Hume mentioned just above.
19th century: The Second Law of Thermodynamics.
In thermodynamics, a branch of physics, the Second Law of Thermodynamics, discovered in the 19th century, helps define an arrow of time. This provides an opportunity to physically describe how causes differ from effects: The sum of effects can never have lower entropy than the sum of causes - provided equilibrium conditions.
This is more thoroughly described below.
Causality, determinism, and existentialism.
The deterministic world-view is one in which the universe is no more than a chain of events following one after another according to the law of cause and effect. To hold this worldview, as an incompatibilist, there is no such thing as "free will". However, compatibilists argue that determinism is compatible with, or even necessary for, free will. Existentialists argue that while no intrinsic meaning has been designed in a deterministic universe, we each can provide a meaning for ourselves.
Hindu philosophy.
Vedic period (ca.1750–500 BCE) literature has karma's Eastern origins. Karma is the belief held by Sanathana Dharma and major religions that a person's actions cause certain effects in the current life and/or in future life, positively or negatively. The various philosophical schools (darsanas) provide different accounts of the subject. The doctrine of satkaryavada affirms that the effect inheres in the cause in some way. The effect is thus either a real or apparent modification of the cause. The doctrine of asatkaryavada affirms that the effect does not inhere in the cause, but is a new arising. See Nyaya for some details of the theory of causation in the Nyaya school. In Brahma Samhita, Brahma describes Krishna as the prime cause of all causes.
 identifies five causes for any action (knowing which it can be perfected): the body, the individual soul, the senses, the efforts and the supersoul.
Buddhist philosophy.
According to the theory of action and result (karmaphala), our karmic actions are the principal cause of our happiness or suffering. From the Buddhist point of view, a positive or wholesome action is one that will lead to greater happiness for ourselves and others, and a negative or unwholesome action is one that will lead to greater suffering for ourselves or others.
The general or universal definition of pratityasamutpada (or "dependent origination" or "dependent arising" or "interdependent co-arising") is that everything arises in dependence upon multiple causes and conditions; nothing exists as a singular, independent entity. A traditional example in Buddhist texts is of three sticks standing upright and leaning against each other and supporting each other. If one stick is taken away, the other two will fall to the ground.
Logic.
Necessary and sufficient causes.
Causes are often distinguished into two types: Necessary and sufficient. A third type of causation, which requires neither necessity nor sufficiency in and of itself, but which contributes to the effect, is called a "contributory cause."
Necessary causes:
If "x" is a necessary cause of "y", then the presence of "y" necessarily implies the presence of "x". The presence of "x", however, does not imply that "y" will occur.
Sufficient causes:
If "x" is a sufficient cause of "y", then the presence of "x" necessarily implies the presence of "y". However, another cause "z" may alternatively cause "y". Thus the presence of "y" does not imply the presence of "x".
Contributory causes:
A cause may be classified as a "contributory cause", if the presumed cause precedes the effect, and altering the cause alters the effect, regardless of whether either the cause or the effect appears only in the presence of the other.
J. L. Mackie argues that usual talk of "cause", in fact refers to INUS conditions (insufficient but non-redundant parts of a condition which is itself unnecessary but sufficient for the occurrence of the effect). An example is a short circuit as a cause for a house burning down. Consider the collection of events: the short circuit, the proximity of flammable material, and the absence of firefighters. Together these are unnecessary but sufficient to the house's burning down (since many other collections of events certainly could have led to the house burning down, for example shooting the house with a flamethrower in the presence of oxygen and so forth). Within this collection, the short circuit is an insufficient (since the short circuit by itself would not have caused the fire) but non-redundant (because the fire would not have happened without it, everything else being equal) part of a condition which is itself unnecessary but sufficient for the occurrence of the effect. So, the short circuit is an INUS condition for the occurrence of the house burning down.
Causality contrasted with conditionals.
Conditional statements are "not" statements of causality. An important distinction is that statements of causality require the antecedent to precede or coincide with the consequent in time, whereas conditional statements do not require this temporal order. Confusion commonly arises since many different statements in English may be presented using "If ..., then ..." form (and, arguably, because this form is far more commonly used to make a statement of causality). The two types of statements are distinct, however.
For example, all of the following statements are true when interpreting "If ..., then ..." as the material conditional:
The first is true since both the antecedent and the consequent are true. The second is true in sentential logic and indeterminate in natural language, regardless of the consequent statement that follows, because the antecedent is false.
The ordinary indicative conditional has somewhat more structure than the material conditional. For instance, although the first is the closest, neither of the preceding two statements seems true as an ordinary indicative reading. But the sentence
intuitively seems to be true, even though there is no straightforward causal relation in this hypothetical situation between Shakespeare's not writing Macbeth and someone else's actually writing it.
Another sort of conditional, the counterfactual conditional, has a stronger connection with causality, yet even counterfactual statements are not all examples of causality. Consider the following two statements:
In the first case, it would not be correct to say that A's being a triangle "caused" it to have three sides, since the relationship between triangularity and three-sidedness is that of definition. The property of having three sides actually determines A's state as a triangle. Nonetheless, even when interpreted counterfactually, the first statement is true.
A full grasp of the concept of conditionals is important to understanding the literature on causality. A crucial stumbling block is that conditionals in everyday English are usually loosely used to describe a general situation. For example, "If I drop my coffee, then my shoe gets wet" relates an infinite number of possible events. It is shorthand for "For any fact that would count as 'dropping my coffee', some fact that counts as 'my shoe gets wet' will be true". This general statement will be strictly false if there is any circumstance where I drop my coffee and my shoe doesn't get wet. However, an "If..., then..." statement in logic typically relates two "specific" events or facts—a specific coffee-dropping did or did not occur, and a specific shoe-wetting did or did not follow. Thus, with explicit events in mind, if I drop my coffee and wet my shoe, then it is true that "If I dropped my coffee, then I wet my shoe", regardless of the fact that yesterday I dropped a coffee in the trash for the opposite effect—the conditional relates to "specific facts". More counterintuitively, if I didn't drop my coffee at all, then it is also true that "If I drop my coffee then I wet my shoe", or "Dropping my coffee implies I wet my shoe", regardless of whether I wet my shoe or not by any means. This usage would not be counterintuitive if it were not for the everyday usage. Briefly, "If X then Y" is equivalent to the first-order logic statement "A implies B" or "not A-and-not-B", where A and B are predicates, but the more familiar usage of an "if A then B" statement would need to be written symbolically using a higher order logic using quantifiers ("for all" and "there exists").
Questionable cause.
Fallacies of questionable cause, also known as causal fallacies, "non-causa pro causa" (Latin for "non-cause for cause"), or false cause, are informal fallacies where a cause is incorrectly identified.
Theories.
Counterfactual theories.
Subjunctive conditionals are familiar from ordinary language. They are of the form, if A "were" the case, then B "would" be the case, or if A "had been" the case, then B "would have been" the case. Counterfactual conditionals are specifically subjunctive conditionals whose antecedents are in fact false, hence the name. However the term used technically may apply to conditionals with true antecedents as well.
Psychological research shows that people's thoughts about the causal relationships between events influences their judgments of the plausibility of counterfactual alternatives, and conversely, their counterfactual thinking about how a situation could have turned out differently changes their judgements of the causal role of events and agents. Nonetheless, their identification of the cause of an event, and their counterfactual thought about how the event could have turned out differently do not always coincide. People distinguish between various sorts of causes, e.g., strong and weak causes. Research in the psychology of reasoning shows that people make different sorts of inferences from different sorts of causes.
In the philosophical literature, the suggestion that causation is to be defined in terms of a counterfactual relation is made by the 18th Century Scottish philosopher David Hume. Hume remarks that we may define the relation of cause and effect such that ``where, if the first object had not been, the second never had existed." 
More full-fledged analysis of causation in terms of counterfactual conditionals only came in the 20th Century after development of the possible world semantics for the evaluation of counterfactual conditionals. In his 1973 paper "Causation," David Lewis proposed that the following definition of the notion of "causal dependence":
Causation is then defined as a chain of causal dependence. That is, C causes E if and only if there exists a sequence of events C, D1, D2, ... Dk, E such that each event in the sequence depends on the previous.
Note that the analysis does not purport to explain how we make causal judgements or how we reason about causation, but rather to give a metaphysical account of what it is for there to be a causal relation between some pair of events. If correct, the analysis has the power to explain certain features of causation. Knowing that causation is a matter of counterfactual dependence, we may reflect on the nature of counterfactual dependence to account for the nature of causation. For example, in his paper "Counterfactual Dependence and Time's Arrow," Lewis sought to account for the time-directedness of counterfactual dependence in terms of the semantics of the counterfactual conditional. If correct, this theory can serve to explain a fundamental part of our experience, which is that we can only causally affect the future but not the past.
Probabilistic causation.
Interpreting causation as a deterministic relation means that if "A" causes "B", then "A" must "always" be followed by "B". In this sense, war does not cause deaths, nor does smoking cause cancer. As a result, many turn to a notion of probabilistic causation. Informally, "A" probabilistically causes "B" if "A"'s occurrence increases the probability of "B". This is sometimes interpreted to reflect imperfect knowledge of a deterministic system but other times interpreted to mean that the causal system under study is inherently probabilistic, such as quantum mechanics.
Causal calculus.
When experiments are infeasible or illegal, the derivation of cause effect relationship from observational studies must rest on some qualitative theoretical assumptions, for
example, that symptoms do not cause diseases, usually
expressed in the form of missing arrows in causal graphs such as Bayesian networks or path diagrams. The mathematical theory underlying these derivations relies on the distinction between "conditional probabilities", as in formula_1, and "interventional probabilities", as in formula_2. The former reads:
"the probability of finding cancer in a person known to smoke"
while the latter reads: "the probability of finding cancer in
a person "forced" to smoke". The former is a statistical
notion that can be estimated directly in observational studies, while the latter is a causal notion (also called "causal effect") which is what we estimate in a controlled randomized experiment.
The theory of "causal calculus" permits one to infer interventional probabilities from conditional probabilities in causal Bayesian networks with unmeasured variables. One very practical result of this theory is the characterization of confounding variables, namely, a sufficient set of variables that, if adjusted for, would yield the correct causal effect between variables of interest. It can be shown that a sufficient set for estimating the causal effect of formula_3 on formula_4 is any set of non-descendants of formula_3 that formula_6-separate formula_3 from formula_4 after removing all arrows emanating from formula_3. This criterion, called "backdoor", provides a mathematical definition of "confounding" and helps researchers identify accessible sets of variables worthy of measurement.
Structure learning.
While derivations in causal calculus rely on the structure of the causal graph, parts of the causal structure can, under certain assumptions, be learned from statistical data. The basic idea goes back to Sewall Wright's 1921 work on path analysis. A "recovery" algorithm was developed by Rebane and Pearl (1987) which rests on Wright's distinction between the three possible types of causal substructures allowed in a directed acyclic graph (DAG):
Type 1 and type 2 represent the same statistical dependencies (i.e., formula_3 and formula_14 are independent given formula_4) and are, therefore, indistinguishable within purely cross-sectional data. Type 3, however, can be uniquely identified, since formula_3 and formula_14 are marginally independent and all other pairs are dependent. Thus, while the "skeletons" (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when formula_3 and formula_14 have common ancestors, except that one must first condition on those ancestors. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independencies observed.
Alternative methods of structure learning search through the "many" possible causal structures among the variables, and remove ones which are strongly incompatible with the observed correlations. In general this leaves a set of possible causal relations, which should then be tested by analyzing time series data or, preferably, designing appropriately controlled experiments. In contrast with Bayesian Networks, path analysis (and its generalization, structural equation modeling), serve better to estimate a known causal effect or to test a causal model than to generate causal hypotheses.
For nonexperimental data, causal direction can often be inferred if information about time is available. This is because (according to many, though not all, theories) causes must precede their effects temporally. This can be determined by statistical time series models, for instance, or with a statistical test based on the idea of Granger causality, or by direct experimental manipulation. The use of temporal data can permit statistical tests of a pre-existing theory of causal direction. For instance, our degree of confidence in the direction and nature of causality is much greater when supported by cross-correlations, ARIMA models, or cross-spectral analysis using vector time series data than by cross-sectional data.
Derivation theories.
The Nobel Prize holder Herbert A. Simon and Philosopher Nicholas Rescher claim that the asymmetry of the causal relation is unrelated to the asymmetry of any mode of implication that contraposes. Rather, a causal relation is not a relation between values of variables, but a function of one variable (the cause) on to another (the effect). So, given a system of equations, and a set of variables appearing in these equations, we can introduce an asymmetric relation among individual equations and variables that corresponds perfectly to our commonsense notion of a causal ordering. The system of equations must have certain properties, most importantly, if some values are chosen arbitrarily, the remaining values will be determined uniquely through a path of serial discovery that is perfectly causal. They postulate the inherent serialization of such a system of equations may correctly capture causation in all empirical fields, including physics and economics.
Manipulation theories.
Some theorists have equated causality with manipulability. Under these theories, "x" causes "y" only in the case that one can change "x" in order to change "y". This coincides with commonsense notions of causations, since often we ask causal questions in order to change some feature of the world. For instance, we are interested in knowing the causes of crime so that we might find ways of reducing it.
These theories have been criticized on two primary grounds. First, theorists complain that these accounts are circular. Attempting to reduce causal claims to manipulation requires that manipulation is more basic than causal interaction. But describing manipulations in non-causal terms has provided a substantial difficulty.
The second criticism centers around concerns of anthropocentrism. It seems to many people that causality is some existing relationship in the world that we can harness for our desires. If causality is identified with our manipulation, then this intuition is lost. In this sense, it makes humans overly central to interactions in the world.
Some attempts to defend manipulability theories are recent accounts that don't claim to reduce causality to manipulation. These accounts use manipulation as a sign or feature in causation without claiming that manipulation is more fundamental than causation.
Process theories.
Some theorists are interested in distinguishing between causal processes and non-causal processes (Russell 1948; Salmon 1984). These theorists often want to distinguish between a process and a pseudo-process. As an example, a ball moving through the air (a process) is contrasted with the motion of a shadow (a pseudo-process). The former is causal in nature while the latter is not.
Salmon (1984) claims that causal processes can be identified by their ability to transmit an alteration over space and time. An alteration of the ball (a mark by a pen, perhaps) is carried with it as the ball goes through the air. On the other hand an alteration of the shadow (insofar as it is possible) will not be transmitted by the shadow as it moves along.
These theorists claim that the important concept for understanding causality is not causal relationships or causal interactions, but rather identifying causal processes. The former notions can then be defined in terms of causal processes.
Systemic causality.
George Lakoff writes, in relation to the cause of Hurricane Sandy,"Systemic causation, because it is less obvious, is more important to understand. A systemic cause may be one of a number of multiple causes. It may require some special conditions. It may be indirect, working through a network of more direct causes. It may be probabilistic, occurring with a significantly high probability. It may require a feedback mechanism. In general, causation in ecosystems, biological systems, economic systems, and social systems tends not to be direct, but is no less causal. And because it is not direct causation, it requires all the greater attention if it is to be understood and its negative effects controlled. Above all, it requires a name: systemic causation."
Fields.
Science.
Within the frames of a dynamic method called the scientific method, scientists set up experiments, normally with the end of determining causality in the physical world. For instance, one may want to know whether a high intake of carrots causes humans to develop the bubonic plague. As an observation of a correlation does not imply causation, it is necessary to use inductive reasoning from particular observations in order to strengthen (through observed reproducibility) or disprove hypotheses about causal relationships. The fundamentally uncertain nature of inductive reasoning has been claimed to give rise to scientific paradigm shifts, as described by Thomas Kuhn.
This framework is sometimes called the scientific method, and forms part of the Philosophy of science. The dichotomy between hard and soft science can be regarded as stemming from the increased uncertainty and vagueness connected to the inductive proofs of causal links in "softer" sciences.
Physics.
Informally, physicists use the terminology of cause and effect in the same everyday fashion as most other people do. In the context of physical theory itself for example, some physicists will say that forces cause motions (or accelerations). However, strictly speaking, this is not the same as a formal theory of causality. Causality is not inherently implied in equations of motion, but postulated as an additional constraint that needs to be satisfied (i.e. a cause always precedes its effect). This constraint has mathematical implications such as the Kramers-Kronig relations.
Causal notions appear in physics in the context of information, where "information" is what links a cause to its effect. Formally, it is expected that information can not travel faster than the speed of light since otherwise, reference coordinate systems could be constructed (using the Lorentz transform of special relativity) in which an observer would see an effect precede its cause (i.e. the postulate of causality would be violated).
Causal notions also appear in the related context of the flow of mass-energy (since mass-energy flow is generally considered to be linked to information flow). For example, it is commonplace to make use of the causality argument to argue that the group velocity of waves (such as electromagnetic waves) can not exceed the speed of light.
Causal notions are important in general relativity to the extent that to have an arrow of time demands that the universe's semi-Riemannian manifold be orientable, so that "future" and "past" are globally definable quantities.
Arguably the most prominent role of causal notions in physics, however, is statistical mechanics. The Second Law of Thermodynamics states that entropy - which can be thought of as a measure of disorder - will always increase in any closed system. (See also the fluctuation theorem). The irreversible increase of entropy therefore provides another "arrow of time" by which past and future can be distinguished. (As an analogy, if there is a stacked cube of 64 dice in a box and someone shakes the box, then the dice will no longer be stacked in a cube. The process is not reversible; shaking the box again will not cause the dice to be reassembled into a neat cube.) A formal physical definition of cause-and-effect, if such a thing is possible, may be related to the second law. However, while much has been written about this topic, there is not yet any generally accepted formal theory of causation tied to the second law.
Engineering.
A causal system is a system with output and internal states that depends only on the current and previous input values. A system that has "some" dependence on input values from the future (in addition to possible past or current input values) is termed an acausal system, and a system that depends "solely" on future input values is an anticausal system. Acausal filters, for example, can only exist as postprocessing filters, because these filters can extract future values from a memory buffer or a file.
Biology, medicine and epidemiology.
Austin Bradford Hill built upon the work of Hume and Popper and suggested in his paper "The Environment and Disease: Association or Causation?" that aspects of an association such as strength, consistency, specificity and temporality be considered in attempting to distinguish causal from noncausal associations in the epidemiological situation. (See Bradford-Hill criteria.) He did not note however, that temporality is the only necessary criterion among those aspects. Directed acyclic graphs (DAGs) are increasingly used in epidemiology to help enlighten causal thinking.
Psychology.
Psychologists take an empirical approach to causality, investigating how people and non-human animals detect or infer causation from sensory information, prior experience and innate knowledge.
Attribution theory is the theory concerning how people explain individual occurrences of causation. Attribution can be external (assigning causality to an outside agent or force - claiming that some outside thing motivated the event) or internal (assigning causality to factors within the person - taking personal responsibility or accountability for one's actions and claiming that the person was directly responsible for the event). Taking causation one step further, the type of attribution a person provides influences their future behavior.
The intention behind the cause or the effect can be covered by the subject of action. See also accident; blame; intent; and responsibility.
Whereas David Hume argued that causes are inferred from non-causal observations, Immanuel Kant claimed that people have innate assumptions about causes. Within psychology, Patricia Cheng (1997) attempted to reconcile the Humean and Kantian views. According to her power PC theory, people filter observations of events through a basic belief that causes have the power to generate (or prevent) their effects, thereby inferring specific cause-effect relations.
Our view of causation depends on what we consider to be the relevant events. Another way to view the statement, "Lightning causes thunder" is to see both lightning and thunder as two perceptions of the same event, viz., an electric discharge that we perceive first visually and then aurally.
David Sobel and Alison Gopnik from the Psychology Department of UC Berkeley designed a device known as "the blicket detector" which would turn on when an object was placed on it. Their research suggests that "even young children will easily and swiftly learn about a new causal power of an object and spontaneously use that information in classifying and naming the object."
Some researchers such as Anjan Chatterjee at the University of Pennsylvania and Jonathan Fugelsang at the University of Waterloo are using neuroscience techniques to investigate the neural and psychological underpinnings of causal launching events in which one object causes another object to move. Both temporal and spatial factors can be manipulated.
See Causal Reasoning (Psychology) for more information.
Statistics and economics.
Statistics and economics usually employ pre-existing data or experimental data to infer causality by regression methods. The body of statistical techniques involves substantial use of regression analysis. Typically a linear relationship such as
is postulated, in which formula_21 is the "i"th observation of the dependent variable (hypothesized to be the caused variable), formula_22 for "j"=1...,"k" is the "i"th observation on the "j"th independent variable (hypothesized to be a causative variable), and formula_23 is the error term for the "i"th observation (containing the combined effects of all other causative variables, which must be uncorrelated with the included independent variables). If there is reason to believe that none of the formula_24s is caused by "y", then estimates of the coefficients formula_25 are obtained. If the null hypothesis that formula_26 is rejected, then the alternative hypothesis that formula_27 and equivalently that formula_24 causes "y" cannot be rejected. On the other hand, if the null hypothesis that formula_26 cannot be rejected, then equivalently the hypothesis of no causal effect of formula_24 on "y" cannot be rejected. Here the notion of causality is one of contributory causality as discussed above: If the true value formula_31, then a change in formula_24 will result in a change in "y" "unless" some other causative variable(s), either included in the regression or implicit in the error term, change in such a way as to exactly offset its effect; thus a change in formula_24 is "not sufficient" to change "y". Likewise, a change in formula_24 is "not necessary" to change "y", because a change in "y" could be caused by something implicit in the error term (or by some other causative explanatory variable included in the model).
The above way of testing for causality requires belief that there is no reverse causation, in which "y" would cause formula_24. This belief can be established in one of several ways. First, the variable formula_24 may be a non-economic variable: for example, if rainfall amount formula_24 is hypothesized to affect the futures price "y" of some agricultural commodity, it is impossible that in fact the futures price affects rainfall amount (provided that cloud seeding is never attempted). Second, the instrumental variables technique may be employed to remove any reverse causation by introducing a role for other variables (instruments) that are known to be unaffected by the dependent variable. Third, the principle that effects cannot precede causes can be invoked, by including on the right side of the regression only variables that precede in time the dependent variable; this principle is invoked, for example, in testing for Granger causality and in its multivariate analog, vector autoregression, both of which control for lagged values of the dependent variable while testing for causal effects of lagged independent variables.
Regression analysis controls for other relevant variables by including them as regressors (explanatory variables). This helps to avoid false inferences of causality due to the presence of a third, underlying, variable that influences both the potentially causative variable and the potentially caused variable: its effect on the potentially caused variable is captured by directly including it in the regression, so that effect will not be picked up as an indirect effect through the potentially causative variable of interest.
Management.
For quality control in manufacturing in the 1960s, Kaoru Ishikawa developed a cause and effect diagram, known as an Ishikawa diagram or fishbone diagram. The diagram categorizes causes, such as into the six main categories shown here. These categories are then sub-divided. Ishikawa's method identifies "causes" in brainstorming sessions conducted among various groups involved in the manufacturing process. These groups can then be labeled as categories in the diagrams. The use of these diagrams has now spread beyond quality control, and they are used in other areas of management and in design and engineering. Ishikawa diagrams have been criticized for failing to make the distinction between necessary conditions and sufficient conditions. It seems that Ishikawa was not even aware of this distinction.
Humanities.
History.
In the discussion of history, events are sometimes considered as if in some way being agents that can then bring about other historical events. Thus, the combination of poor harvests, the hardships of the peasants, high taxes, lack of representation of the people, and kingly ineptitude are among the "causes" of the French Revolution. This is a somewhat Platonic and Hegelian view that reifies causes as ontological entities. In Aristotelian terminology, this use approximates to the case of the "efficient" cause.
Some philosophers of history such as Arthur Danto have claimed that "explanations in history and elsewhere" describe "not simply an event – something that happens – but a change". Like many practicing historians, they treat causes as intersecting actions and sets of actions which bring about "larger changes", in Danto’s words: to decide "what are the elements which persist through a change" is "rather simple" when treating an individual’s "shift in attitude", but "it is considerably more complex and metaphysically challenging when we are interested in such a change as, say, the break-up of feudalism or the emergence of nationalism".
Much of the historical debate about causes has focused on the relationship between communicative and other actions, between singular and repeated ones, and between actions, structures of action or group and institutional contexts and wider sets of conditions. John Gaddis has distinguished between exceptional and general causes (following Marc Bloch) and between "routine" and "distinctive links" in causal relationships: "in accounting for what happened at Hiroshima on August 6, 1945, we attach greater importance to the fact that President Truman ordered the dropping of an atomic bomb than to the decision of the Army Air Force to carry out his orders." He has also pointed to the difference between immediate, intermediate and distant causes. For his part, Christopher Lloyd puts forward four "general concepts of causation" used in history: the "metaphysical idealist concept, which asserts that the phenomena of the universe are products of or emanations from an omnipotent being or such final cause"; "the empiricist (or Humean) regularity concept, which is based on the idea of causation being a matter of constant conjunctions of events"; "the functional/teleological/consequential concept", which is "goal-directed, so that goals are causes"; and the "realist, structurist and dispositional approach, which sees relational structures and internal dispositions as the causes of phenomena".
Law.
According to law and jurisprudence, legal cause must be demonstrated to hold a defendant liable for a crime or a tort (i.e. a civil wrong such as negligence or trespass). It must be proven that causality, or a "sufficient causal link" relates the defendant's actions to the criminal event or damage in question. Causation is also an essential legal element that must be proven to qualify for remedy measures under international trade law.
Theology.
Note the concept of omnicausality in Abrahamic theology, which is the belief that God has set in motion all events at the dawn of time; he is the determiner and the cause of all things. It is therefore an attempt to rectify the apparent incompatibility between determinism and the existence of an omnipotent god.
Further reading.
</dl>
 <span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=Determination%20des%20Indeterminierten.%20Kritische%20Anmerkungen%20zur%20Determinismus-%20und%20Freiheitskontroverse&rft.aulast=Schimbera%2C%20J%C3%BCrgen%20/%20Schimbera%2C%20Peter&rft.au=Schimbera%2C%20J%C3%BCrgen%20/%20Schimbera%2C%20Peter&rft.date=2010&rft.place=Hamburg&rft.pub=Verlag%20Dr.%20Kovac&rft.isbn=978-3-8300-5099-5&rfr_id=info:sid/en.wikipedia.org:"> 

</doc>
<doc id="37197" url="http://en.wikipedia.org/wiki?curid=37197" title="Isotope separation">
Isotope separation

Isotope separation is the process of concentrating specific isotopes of a chemical element by removing other isotopes. The use of the nuclides produced is various. The largest variety is used in research (e.g. in chemistry where atoms of "marker" nuclide are used to figure out reaction mechanisms). By tonnage, separating natural uranium into enriched uranium and depleted uranium is the largest application. In the following text, mainly the uranium enrichment is considered. This process is a crucial one in the manufacture of uranium fuel for nuclear power stations, and is also required for the creation of uranium based nuclear weapons. Plutonium-based weapons use plutonium produced in a nuclear reactor, which must be operated in such a way as to produce plutonium already of suitable isotopic mix or "grade". 
While different chemical elements can be purified through chemical processes, isotopes of the same element have nearly identical chemical properties, which makes this type of separation impractical, except for separation of deuterium.
Separation techniques.
There are three types of isotope separation techniques:
The third type of separation is still experimental; practical separation techniques all depend in some way on the atomic mass. It is therefore generally easier to separate isotopes with a larger relative mass difference. For example deuterium has twice the mass of ordinary (light) hydrogen and it is generally easier to purify it than to separate uranium-235 from the more common uranium-238. On the other extreme, separation of fissile plutonium-239 from the common impurity plutonium-240, while desirable in that it would allow the creation of gun-type nuclear weapons from plutonium, is generally agreed to be impractical.
Enrichment cascades.
All large-scale isotope separation schemes employ a number of similar stages which produce successively higher concentrations of the desired isotope. Each stage enriches the product of the previous step further before being sent to the next stage. Similarly, the tailings from each stage are returned to the previous stage for further processing. This creates a sequential enriching system called a cascade.
There are two important factors that affect the performance of a cascade. The first is the separation factor, which is a number greater than 1. The second is the number of required stages to get the desired purity.
Commercial materials.
To date, large-scale commercial isotope separation of only three elements has occurred. In each case, the rarer of the two most common isotopes of an element has been concentrated for use in nuclear technology:
Some isotopically purified elements are used in smaller quantities for specialist applications, especially in the semiconductor industry, where purified silicon is used to improve crystal structure and thermal conductivity, and carbon with greater isotopic purity to make diamonds with greater thermal conductivity.
Isotope separation is an important process for both peaceful and military nuclear technology, and therefore the capability that a nation has for isotope separation is of extreme interest to the intelligence community.
Alternatives.
The only alternative to isotope separation is to manufacture the required isotope in its pure form. This may be done by irradiation of a suitable target, but care is needed in target selection and other factors to ensure that only the required isotope of the element of interest is produced. Isotopes of other elements are not so great a problem as they can be removed by chemical means.
This is particularly relevant in the preparation of high-grade plutonium-239 for use in weapons. It is not practical to separate Pu-239 from Pu-240 or Pu-241. Fissile Pu-239 is produced following neutron capture by uranium-238, but further neutron capture will produce Pu-240 which is less fissile and worse, is a fairly strong neutron emitter, and Pu-241 which decays to Am-241, a strong alpha emitter that poses self-heating and radiotoxicity problems. Therefore, the uranium targets used to produce military plutonium must be irradiated for only a short time, to minimise the production of these unwanted isotopes. Conversely, blending plutonium with Pu-240 renders it less suitable for nuclear weapons.
Practical methods of separation.
Diffusion.
Often done with gases, but also with liquids, the diffusion method relies on the fact that in thermal equilibrium, two isotopes with the same energy will have different average velocities. The lighter atoms (or the molecules containing them) will travel more quickly and be more likely to diffuse through a membrane. The difference in speeds is proportional to the square root of the mass ratio, so the amount of separation is small and many cascaded stages are needed to obtain high purity. This method is expensive due to the work needed to push gas through a membrane and the many stages necessary.
The first large-scale separation of uranium isotopes was achieved by the United States in large gaseous diffusion separation plants at Oak Ridge Laboratories, which were established as part of the Manhattan Project. These used uranium hexafluoride gas as the process fluid. Nickel powder and electro-deposited nickel mesh diffusion barriers were pioneered by Edward Adler and Edward Norris. See gaseous diffusion.
Centrifugal effect.
Centrifugal effect schemes rapidly rotate the material allowing the heavier isotopes to go closer to an outer radial wall. This too is often done in gaseous form using a Zippe-type centrifuge.
The centrifugal separation of isotopes was first suggested by Aston and Lindemann in 1919 and the first successful experiments were reported by Beams and Haynes on isotopes of chlorine in 1936. However attempts to use the technology during the Manhattan project were unproductive. In modern times it is the main method used throughout the world to enrich uranium and as a result remains a fairly secretive process, hindering a more widespread uptake of the technology. In general a feed of UF6 gas is connected to a cylinder that is rotated at high speed. Near the outer edge of the cylinder heavier gas molecules containing U-238 collect, while molecules containing U-235 concentrate at the center and are then fed to another cascade stage. Use of gaseous centrifugal technology to enrich isotopes is desirable as power consumption is greatly reduced when compared to more conventional techniques such as diffusion plants since fewer cascade steps are required to reach similar degrees of separation. In fact, gas centrifuges using uranium hexafluoride have largely replaced gaseous diffusion technology for uranium enrichment. As well as requiring less energy to achieve the same separation, far smaller scale plants are possible, making them an economic possibility for a small nation attempting to produce a nuclear weapon. Pakistan is believed to have used this method in developing its nuclear weapons.
Vortex tubes were used by South Africa in their Helikon vortex separation process. The gas is injected tangentially into a chamber with special geometry that further increases its rotation to a very high rate, causing the isotopes to separate. The method is simple because vortex tubes have no moving parts, but energy intensive, about 50 times greater than gas centrifuges. A similar process, known as "jet nozzle", was created in Germany, with a demonstration plant built in Brazil, and they went as far as developing a site to fuel the country's nuclear plants.
Electromagnetic.
This method is a form of mass spectrometry, and is sometimes referred to by that name. It uses the fact that charged particles are deflected in a magnetic field and the amount of deflection depends upon the particle's mass. It is very expensive for the quantity produced, as it has an extremely low throughput, but it can allow very high purities to be achieved. This method is often used for processing small amounts of pure isotopes for research or specific use (such as isotopic tracers), but is impractical for industrial use.
At Oak Ridge and at the University of California, Berkeley, Ernest O. Lawrence developed electromagnetic separation for much of the uranium used in the first United States atomic bomb (see Manhattan Project). Devices using his principle are named calutrons. After the war the method was largely abandoned as impractical. It had only been undertaken (along with diffusion and other technologies) to guarantee there would be enough material for use, whatever the cost. Its main eventual contribution to the war effort was to further concentrate material from the gaseous diffusion plants to even higher levels of purity.
Laser.
In this method a laser is tuned to a wavelength which excites only one isotope of the material and ionizes those atoms preferentially. The resonant absorption of light for an isotope is dependent upon its mass and certain hyperfine interactions between electrons and the nucleus, allowing finely tuned lasers to interact with only one isotope. After the atom is ionized it can be removed from the sample by applying an electric field. This method is often abbreviated as AVLIS (atomic vapor laser isotope separation). This method has only recently been developed as laser technology has improved, and is currently not used extensively. However, it is a major concern to those in the field of nuclear proliferation because it may be cheaper and more easily hidden than other methods of isotope separation. Tunable lasers used in AVLIS include the dye laser and more recently diode lasers.
A second method of laser separation is known as molecular laser isotope separation (MLIS). In this method, an infrared laser is directed at uranium hexafluoride gas, exciting molecules that contain a U-235 atom. A second laser frees a fluorine atom, leaving uranium pentafluoride which then precipitates out of the gas. Cascading the MLIS stages is more difficult than with other methods because the UF5 must be refluorinated (back to UF6) before being introduced into the next MLIS stage. Alternative MLIS schemes are currently being developed (using a first laser in the near-infrared or visible region) where an enrichment of over 95% can be obtained in a single stage, but the methods have not (yet) reached industrial feasibility. This method is called OP-IRMPD (Overtone Pre-excitation—IR Multiple Photon Dissociation).
Finally, the SILEX process, developed by Silex Systems in Australia, has recently been licensed to General Electric for the development of a pilot enrichment plant. The method uses uranium hexafluoride as a feedstock, and uses magnets to separate the isotopes after one isotope is preferentially ionized. Further details of the process are not disclosed.
Quite recently yet another scheme has been proposed for the deuterium separation using Trojan wavepackets in circularly polarized electromagnetic field. The process of Trojan wave packet formation by the adiabatic-rapid passage depends in ultra-sensitive way on the reduced electron and nucleus mass which with the same field frequency further leads to excitation of Trojan or anti-Trojan wavepacket depending on the kind of the isotope. Those and their giant, rotating electric dipole moments are then formula_1-shifted in phase and the beam of such atoms splits in the gradient of the electric field in the analogy to Stern–Gerlach experiment.
Chemical methods.
Although isotopes of a single element are normally described as having the same chemical properties, this is not strictly true. In particular, reaction rates are very slightly affected by atomic mass.
Techniques using this are most effective for light atoms such as hydrogen. Lighter isotopes tend to react or evaporate more quickly than heavy isotopes, allowing them to be separated. This is how heavy water is produced commercially, see Girdler sulfide process for details. Lighter isotopes also disassociate more rapidly under an electric field. This process in a large cascade was used at the heavy water production plant at Rjukan.
One candidate for the largest kinetic isotopic effect ever measured at room temperature, 305, may eventually be used for the separation of tritium (T). The effects for the oxidation of triated formate anions to HTO were measured as:
Gravity.
Isotopes of carbon, oxygen, and nitrogen can be purified by chilling these gases or compounds nearly to their liquification temperature in very tall (200 to) columns. The heavier isotopes sink and the lighter isotopes rise, where they are easily collected. The process was developed in the late 1960s by scientists at Los Alamos National Laboratory. This process is also called "cryogenic distillation".
The SWU (separative work unit).
Separative Work Unit (SWU) is a complex unit which is a function of the amount of uranium processed and the degree to which it is enriched, "i.e." the extent of increase in the concentration of the U-235 isotope relative to the remainder.
The unit is strictly: Kilogram Separative Work Unit, and it measures the quantity of separative work (indicative of energy used in enrichment) when feed and product quantities are expressed in kilograms. The effort expended in separating a mass "F" of feed of assay "xf" into a mass "P" of product assay xp and waste of mass "W" and assay "xw" is expressed in terms of the number of separative work units needed, given by the expression SWU = "WV"("xw") + "PV"("xp") - "FV"("xf"), where "V"("x") is the "value function," defined as "V"("x") = (1 - 2"x") ln ((1 - "x") /"x").
Separative work is expressed in SWUs, kg SW, or kg UTA (from the German "Urantrennarbeit" )
If, for example, you begin with 100 kilograms (220 pounds) of natural uranium, it takes about 60 SWU to produce 10 kilograms (22 pounds) of uranium enriched in U-235 content to 4.5%
Isotope separators for research.
Radioactive beams of specific isotopes are widely used in the fields of experimental physics, biology and materials science. The production and formation of these radioactive atoms into an ionic beam for study is an entire field of research carried out at many laboratories throughout the world. The first isotope separator was developed at the Copenhagen Cyclotron by Bohr and co-workers using the principle of electromagnetic separation. Today, there are many laboratories around the world which supply beams of radioactive ions for use. Arguably the principal Isotope Separator On-Line (ISOL) is ISOLDE at CERN, which is a joint European facility spread across the Franco-Swiss border near the city of Geneva. This laboratory uses mainly proton spallation of uranium carbide targets to produce a wide range of radioactive fission fragments that are not found naturally on earth. During spallation (bombardment with high energy protons), a uranium carbide target is heated to several thousand degrees so that radioactive atoms produced in the nuclear reaction are released. Once out of the target, the vapour of radioactive atoms travels to an ionizer cavity. This ionizer cavity is a thin tube made of a refractory metal with a high work function allowing for collisions with the walls to liberate a single electron from a free atom (surface ionization effect). Once ionized, the radioactive species are accelerated by an electrostatic field and injected into an electromagnetic separator. As ions entering the separator are of approximately equal energy, those ions with a smaller mass will be deflected by the magnetic field by a greater amount than those with a heavier mass. This differing radius of curvature allows for isobaric purification to take place. Once purified isobarically, the ion beam is then sent to the individual experiments. In order to increase the purity of the isobaric beam, laser ionization can take place inside the ionizer cavity to selectively ionize a single element chain of interest. At CERN, this device is called the Resonance Ionization Laser Ion Source (RILIS). Currently over 60% of all experiments opt to use the RILIS to increase the purity of radioactive beams.
Beam production capability of ISOL facilities.
As the production of radioactive atoms by the ISOL technique depends on the free atom chemistry of the element to be studied, there are certain beams which cannot be produced by simple proton bombardment of thick actinide targets. Refractory metals such as tungsten and rhenium do not emerge from the target even at high temperatures due to their low vapour pressure. In order to produce these types of beams, a thin target is required. The Ion Guide Isotope Separator On Line (IGISOL) technique was developed in 1981 at the University of Jyvaskyla cyclotron laboratory in Finland. In this technique, a thin uranium target is bombarded with protons and nuclear reaction products recoil out of the target in a charged state. The recoils are stopped in a gas cell and then exit through a small hole in the side of the cell where they are accelerated electrostatically and injected into a mass separator. This method of production and extraction takes place on a shorter timescale compared to the standard ISOL technique and isotopes with short half-lives (sub millisecond) can be studied using an IGISOL. An IGISOL has also been combined with a laser ion source at the Leuven Isotope Separator On Line (LISOL) in Belgium. Thin target sources generally provide significantly lower quantities of radioactive ions than thick target sources and this is their main drawback.
As experimental nuclear physics progresses, it is becoming more and more important to study the most exotic of radioactive nuclei. In order to do so, more inventive techniques are required to create nuclei with extreme proton/neutron ratios. An alternative to the ISOL techniques described here is that of fragmentation beams, where the radioactive ions are produced by fragmentation reactions on a fast beam of stable ions impinging on a thin target (usually of beryllium atoms). This technique is used, for example, at the National Superconducting Cyclotron Laboratory (NSCL) at Michigan State University and at the Radioactive Isotope Beam Factory (RIBF) at RIKEN, in Japan.

</doc>
<doc id="37201" url="http://en.wikipedia.org/wiki?curid=37201" title="Larry Gelbart">
Larry Gelbart

Lawrence Simon "Larry" Gelbart (February 25, 1928 – September 11, 2009) was an American television writer, playwright, screenwriter and author, most famous as a creator and producer of the record-breaking hit TV show "M*A*S*H".
Biography.
Early life.
Gelbart was born in to Jewish immigrants Harry Gelbart, "a barber since his half of a childhood in Latvia," and Frieda Sturner, who migrated to America from Dąbrowa Górnicza, Poland. Marcia Gelbart Walkenstein was his sister.
His family later moved to Los Angeles and he attended Fairfax High School. Drafted shortly after World War Two, Gelbart worked for the Armed Forces Radio Service in Los Angeles.
Television.
Gelbart began as a writer at the age of sixteen for Danny Thomas's radio show after his father, who was Thomas's barber, showed Thomas some jokes Gelbart had written. During the 1940s Gelbart also wrote for Jack Paar and Bob Hope. In the 1950s, his most important work in television involved writing for Red Buttons, for Sid Caesar on "Caesar's Hour", and in Celeste Holm's "Honestly, Celeste!", as well as with writers Mel Tolkin, Michael Stewart, Selma Diamond, Neil Simon, Mel Brooks, Carl Reiner and Woody Allen on two Caesar specials.
In 1972, Gelbart was one of the main forces behind the creation of the television series "M*A*S*H", writing the pilot (for which he received a "Developed for Television by __" credit); then producing, often writing and occasionally directing the series for its first four seasons, from 1972 to 1976. "M*A*S*H" earned Gelbart a Peabody Award and an Emmy for Outstanding Comedy Series and went on to considerable commercial and critical success.
Films.
Gelbart's best known screen work is perhaps the screenplay for 1982's "Tootsie", which he co-wrote with Murray Schisgal. He was nominated for an Academy Award for that script, and also was Oscar-nominated for his original screenplay for 1977's "Oh, God!" starring John Denver and George Burns.
He collaborated with Burt Shevelove on the screenplay for the 1966 British film "The Wrong Box". Gelbart also co-wrote the golden-era film spoof "Movie Movie" (1978) starring George C. Scott in dual roles, the racy comedy "Blame It on Rio" (1984) starring Michael Caine and the 2000 remake of "Bedazzled" with Elizabeth Hurley and Brendan Fraser.
His script for "Rough Cut" (1980), a caper film starring Burt Reynolds, Lesley-Anne Down and David Niven, was credited under the pseudonym Francis Burns.
Gelbart-scripted films for television included "Barbarians at the Gate" (1993), a true story about the battle for control of the RJR Nabisco corporation starring James Garner that was based on the best-selling book of that name; the original comedy "Weapons of Mass Distraction" (1997) starring Ben Kingsley and Gabriel Byrne as rival media moguls; and "And Starring Pancho Villa as Himself" (2003) starring Antonio Banderas as the Mexican revolutionary leader.
Broadway.
Gelbart co-wrote the long-running Broadway musical farce "A Funny Thing Happened on the Way to the Forum" with Burt Shevelove and Stephen Sondheim in 1962. After the show received poor reviews and box-office returns during its previews in Washington, D.C., rewrites and restaging helped; it was a smash Broadway hit and ran for 964 performances. Its book won a Tony Award. A film version starring Zero Mostel, directed by Richard Lester, was released in 1966 with Gelbart and Shevelove's libretto largely rewritten. Gelbart was extremely critical of the movie.
Gelbart's other Broadway credits include the musical "City of Angels", which won him the Drama Desk Award for Outstanding Book of a Musical and an Edgar Award. He also wrote the Iran-contra satire "Mastergate", as well as "Sly Fox" and a musical adaptation of the Preston Sturges movie "Hail the Conquering Hero"; during that show's troubled development Gelbart uttered the now-classic line, "If Hitler is alive, I hope he's out of town with a musical."
Memoirs.
In 1997, Gelbart published his memoir, "Laughing Matters: On Writing M*A*S*H, Tootsie, Oh, God! and a Few Other Funny Things".
Blogger.
Gelbart was a contributing blogger at The Huffington Post, and also was a regular participant on the alt.tv.mash Usenet newsgroup as "Elsig".
Honors.
In 1995, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.
He won an Emmy Award for Outstanding Comedy Series in 1974 for M*A*S*H
In 2002, Gelbart was inducted into the American Theatre Hall of Fame.
In 2008, he was inducted into the Television Hall of Fame.
Death.
Gelbart was diagnosed with cancer in June and died at his Beverly Hills home on September 11, 2009. His wife of 53 years, Pat Gelbart, said that after being married for so long, "we finished each other's sentences." She declined to specify the type of cancer he had. He was buried at the Hillside Memorial Park Cemetery in Culver City, California.
"M*A*S*H" episodes.
The following is a list of "M*A*S*H" episodes (42 Total) written and/or directed by Gelbart.

</doc>
<doc id="37206" url="http://en.wikipedia.org/wiki?curid=37206" title="Robert Watson-Watt">
Robert Watson-Watt

Sir Robert Alexander Watson-Watt, KCB, FRS, FRAeS (13 April 1892 – 5 December 1973) was a pioneer and significant contributor to the development of radar. Radar was initially nameless and researched elsewhere but it was greatly expanded on 1 September 1936 when Watson-Watt became Superintendent of a new establishment under the Air Ministry, Bawdsey Research Station located in Bawdsey Manor, near Felixstowe, Suffolk. Work there resulted in the design and installation of aircraft detection and tracking stations called Chain Home along the east and south coasts of England in time for the outbreak of the Second World War in 1939. This system provided the vital advance information that helped the Royal Air Force win the Battle of Britain.
Early years.
Born in Brechin, Angus, Scotland, on 13 April 1892 Watson-Watt (the hyphenated name is used herein for consistency, although this was not adopted until 1942) was a descendant of James Watt, the famous engineer and inventor of the practical steam engine. After attending Damacre Primary School and Brechin High School, he was accepted to University College, Dundee (then part of the University of St Andrews but became the University of Dundee in 1967). Watt had a successful time as a student, winning the Carnelley Prize for Chemistry and a class medal for Ordinary Natural Philosophy in 1910.
He graduated with a BSc in engineering in 1912, and was offered an assistantship by Professor William Peddie, the holder of the Chair of Physics at University College, Dundee from 1907 to 1942. It was Peddie who encouraged Watson-Watt to study radio, or "wireless telegraphy" as it was then known and who took him through what was effectively a postgraduate class of one on the physics of radio frequency oscillators and wave propagation. At the start of the Great War Watson-Watt was working as an assistant in the College's Engineering Department.
Early experiments.
In 1916 Watson-Watt wanted a job with the War Office, but nothing obvious was available in communications. Instead he joined the Meteorological Office, which was interested in his ideas on the use of radio for the detection of thunderstorms. Lightning gives off a radio signal as it ionizes the air, and his goal was to detect this signal to warn pilots of approaching thunderstorms. The signal occurs across a wide range of frequencies, and could be easily detected and amplified by naval longwave sets, in fact, lightning was a major problem for communications at these common wavelengths.
His early experiments were successful in detecting the signal and he quickly proved to be able to do so at ranges up to 2,500 km. However, there was some difficulty in determining location. This was accomplished by rotating a loop antenna to maximise (or minimise) the signal, thus "pointing" to the storm. However, the strikes were so fleeting that it was very difficult to turn the antenna in time to positively locate one. Instead, the operator would listen to many strikes and develop a rough average location.
At first, he worked at the Wireless Station of Air Ministry Meteorological Office in Aldershot, Hampshire. In 1924 when the War Department gave notice that they wished to re-occupy their Aldershot site, he moved to Ditton Park near Slough, Berkshire. The National Physical Laboratory (NPL) was already using this site and had two main devices that would prove pivotal to his work.
The first was an Adcock antenna, an arrangement of four masts that allowed the signal to be directed through phase differences. Using these as two separate loop antennas at right angles, one could make a simultaneous measurement of the lightning's direction in two axes. However, displaying the fleeting signals was a problem. This was solved by the second device, the WE-224 oscilloscope, recently acquired from Bell Labs. By feeding the signals from the two antennas into the X and Y channels of the oscilloscope, a single strike caused the appearance of a line on the display, indicating the direction of the strike. The scope's relatively "slow" phosphor allowed the signal to be read long after the strike had occurred. Watt's new system was being used in 1926 and was the topic of an extensive paper by Watt and Herd.
The Met and NPL radio teams were amalgamated in 1927 to form the Radio Research Station with Watt as director. Continuing research throughout, the teams had become interested in the causes of "static" radio signals, and found that much could be explained by distant signals located over the horizon being reflected off the upper atmosphere. This was the first direct indication of the reality of the Heaviside layer, proposed earlier but at this time largely dismissed by engineers. To determine the altitude of the layer, Watt, Appleton and others developed the 'squegger' to develop a 'time base' display, which would cause the oscilloscope's dot to move smoothly across the display at very high speed. By timing the squegger so that the dot arrived at the far end of the display at the same time as expected signals reflected off the Heaviside layer, the altitude of the layer could be determined. This time base circuit was key to the development of radar.
After a further reorganization in 1933, Watt became Superintendent of the Radio Department of NPL in Teddington.
RADAR.
The air defence problem.
During the First World War, the Germans had used Zeppelins as long-range bombers over London and other cities and defences had struggled to counter the threat. Since that time aircraft capabilities had improved considerably and the prospect of widespread aerial bombardment of civilian areas was causing the government anxiety. Heavy bombers were now able to approach at altitudes that anti-aircraft guns of the day were unable to reach. With enemy airfields across the English Channel potentially only 20 minutes’ flying-time away, bombers would have dropped their bombs and be returning to base before any intercepting fighters could get to altitude. The only answer seemed to be to have standing patrols of fighters in the air at all times but, with the limited cruising time of a fighter, this would require a huge air force. An alternative solution was urgently needed and in 1934, the Air Ministry set up a committee, the CSSAD (Committee for the Scientific Survey of Air Defence), chaired by Sir Henry Tizard to find ways to improve air defence in the UK.
Nazi Germany was rumoured to have a "death ray" using radio waves that was capable of destroying towns, cities and people. In January 1935, H.E. Wimperis, Director of Scientific Research at the Air Ministry, asked Watson-Watt about the possibility of building their version of a death-ray, specifically to be used against aircraft. Watson-Watt quickly returned a calculation carried out by his colleague, Arnold Wilkins, showing that the device was impossible to construct, and fears of a Nazi version soon vanished. However, he also mentioned in the same report a suggestion that was originally made to him by Wilkins, who had recently heard of aircraft disturbing shortwave communications, that radio waves may be capable of detecting aircraft: "Meanwhile attention is being turned to the still difficult, but less unpromising, problem of radio detection and numerical considerations on the method of detection by reflected radio waves will be submitted when required." Wilson's idea, checked by Watt, was promptly presented by Tizard to the CSSAD on January 28.
Aircraft detection and location.
On 12 February 1935, Watson-Watt sent the secret memo of the proposed system to the Air Ministry, "Detection and location of aircraft by radio methods". Although not as exciting as a death-ray, the concept clearly had potential but the Air Ministry, before giving funding, asked for a demonstration proving that radio waves could be reflected by an aircraft. This was ready by 26 February and consisted of two receiving antennas located about 6 mile away from one of the BBC's shortwave broadcast stations at Daventry. The two antennas were phased such that signals travelling directly from the station cancelled themselves out, but signals arriving from other angles were admitted, thereby deflecting the trace on a CRT indicator (passive radar). Such was the secrecy of this test that only three people witnessed it: Watson-Watt, his colleague Arnold Wilkins, and a single member of the committee, A. P. Rowe. The demonstration was a success; on several occasions a clear signal was seen from a Handley Page Heyford bomber being flown around the site. Most importantly, the Prime Minister, Stanley Baldwin, was kept quietly informed of radar's progress. On 2 April 1935, Watson-Watt received a patent on a radio device for detecting and locating an aircraft.
In mid-May 1935, Wilkins left the Radio Research Station with a small party, including Edward George Bowen, to start further research at Orford Ness, an isolated peninsula on the Suffolk coast of the North Sea. By June they were detecting aircraft at a distance of 16 mile, which was enough for scientists and engineers to stop all work on competing sound-based detection systems. By the end of the year the range was up to 60 mile, at which point plans were made in December to set up five stations covering the approaches to London.
One of these stations was to be located on the coast near Orford Ness, and Bawdsey Manor was selected to become the main centre for all radar research. In an effort to put a radar defence in place as quickly as possible, Watson-Watt and his team created devices using existing available components, rather than creating new components for the project, and the team did not take additional time to refine and improve the devices. So long as the prototype radars were in workable condition they were put into production. They soon conducted "full scale" tests of a fixed radar radio tower system that would soon be known as Chain Home, an early detection system that attempted to detect an incoming bomber by radio signals. The tests were a complete failure, with the fighter only seeing the bomber after it had passed its target. The problem was not the radar, but the flow of information from trackers from the Observer Corps to the fighters, which took many steps and was very slow. Henry Tizard with Patrick Blackett and Hugh Dowding immediately set to work on this problem, designing a 'command and control air defence reporting system' with several layers of reporting that were eventually sent to a single large room for mapping. Observers watching the maps would then tell the fighter groups what to do via direct communications.
By 1937 the first three stations were ready, and the associated system was put to the test. The results were encouraging, and an immediate order by the government to commission an additional 17 stations was given, resulting in a chain of fixed radar towers along the east and south coast of England. By the start of the Second World War, 19 were ready to play a key part in the Battle of Britain, and by the end of the war over 50 had been built. The Germans were aware of the construction of Chain Home but were not sure of its purpose. They tested their theories with a flight of the Zeppelin LZ 130, but concluded the stations were a new long-range naval communications system.
As early as 1936, it was realized that the Luftwaffe would turn to night bombing if the day campaign did not go well, and Watson-Watt had put another of the staff from the Radio Research Station, Edward Bowen, in charge of developing a radar that could be carried by a fighter. Night time visual detection of a bomber was good to about 300 m, and the existing Chain Home systems simply did not have the accuracy needed to get the fighters that close. Bowen decided that an airborne radar should not exceed 90 kg (200 lb) in weight, 8 ft³ (230 L) in volume, and require no more than 500 watts of power. To reduce the drag of the antennas the operating wavelength could not be much greater than one m, difficult for the day's electronics. "AI" - Airborne Interception, was perfected by 1940, and was instrumental in eventually ending the Blitz of 1941. Bowen also fitted airborne radar to maritime patrol aircraft (known in this application as "ASV" - Air to Surface Vessel) and this eventually reduced the threat from submarines.
Watson-Watt justified his choice of a non-optimal frequency for his radar with his often-quoted “cult of the imperfect,” which he stated as “Give them the third-best to go on with; the second-best comes too late, [and] the best never comes.”
Contribution to Second World War.
In his "English History 1914–1945", historian A. J. P. Taylor paid the highest of praise to Watson-Watt, Sir Henry Tizard and their associates who developed and put in place radar, crediting them with being fundamental to victory in the Second World War.
In July 1938 Watson-Watt left Bawdsey Manor and took up the post of Director of Communications Development (DCD-RAE). In 1939 Sir George Lee took over the job of DCD, and Watson-Watt became Scientific Advisor on Telecommunications (SAT) to the Ministry of Aircraft Production, travelling to the USA in 1941 to advise them on the severe inadequacies of their air defence efforts illustrated by the Pearl Harbor attack. He was knighted in 1942.
Ten years after his knighthood, Watson-Watt was awarded £50,000 by the UK government for his contributions in the development of radar. He established a practice as a consulting engineer. In the 1950s he moved to Canada and later he lived in the USA, where he published "Three Steps to Victory" in 1958. Around 1958 he appeared as a mystery challenger on the American television programme "To Tell The Truth".
Watson-Watt reportedly was pulled over for speeding in Canada by a radar gun-toting policeman. His remark was, "Had I known what you were going to do with it I would never have invented it!" He wrote an ironic poem ("Rough Justice") afterwards:
Pity Sir Robert Watson-Watt,
And thus, with others I can mention,
His magical all-seeing eye
but now by some ironic twist
and bites, no doubt with legal wit,
Legacy.
On 3 September 2014 a statue of Sir Robert was unveiled in Brechin by HRH the Princess Royal.
On 4 September Watson-Watt featured in the BBC Two drama "Castles in the Sky", with Eddie Izzard in the role. Reviewing the film "The Daily Telegraph" concluded: "Overall, it all felt a bit worthy. This was history that everybody should know, but the erection of a statue might have done the job just as well."
Family life.
Watson-Watt was married on 20 July 1916 in Hammersmith, London to Margaret Robertson, the daughter of a draughtsman; they later divorced and he remarried in 1952 in Canada. His second wife was Jean Wilkinson, who died in 1964. He returned to Scotland in the 1960s. In 1966, at the age of 72, he proposed to Dame Katherine Trefusis Forbes, who was 67 years old at the time and had also played a significant role in the Battle of Britain as the founding Air Commander of the Women's Auxiliary Air Force, which supplied the radar-room operatives. They lived together in London in the winter, and at "The Observatory" – Trefusis Forbes' summer home in Pitlochry, Perthshire, during the warmer months. They remained together until her death in 1971. Watson-Watt died in 1973, aged 81, in Inverness. Both are buried in the churchyard of the Episcopal Church of the Holy Trinity at Pitlochry.

</doc>
<doc id="37207" url="http://en.wikipedia.org/wiki?curid=37207" title="Nuclear engineering">
Nuclear engineering

Nuclear engineering is the branch of engineering concerned with the application of the breakdown (fission) as well as the fusion of atomic nuclei and/or the application of other sub-atomic physics, based on the principles of nuclear physics. In the sub-field of nuclear fission, it particularly includes the interaction and maintenance of systems and components like nuclear reactors, nuclear power plants, and/or nuclear weapons. The field also includes the study of medical and other applications of (generally ionizing) radiation, nuclear safety, heat/thermodynamics transport, nuclear fuel and/or other related technology (e.g., radioactive waste disposal), and the problems of nuclear proliferation.
Professional areas.
The United States generates about 18% of its electricity from nuclear power plants. Nuclear engineers in this field generally work, directly or indirectly, in the nuclear power industry or for national laboratories. Current research in the industry is directed at producing economical, proliferation-resistant reactor designs with passive safety features. Although government labs research the same areas as industry, they also study a myriad of other issues such as nuclear fuels and nuclear fuel cycles, advanced reactor designs, and nuclear weapon design and maintenance. A principal pipeline for trained personnel for US reactor facilities is the Navy Nuclear Power Program. The job outlook for nuclear engineering from the year 2012 to the year 2022 is predicted to grow 9% due to many elder nuclear engineers retiring, safety systems needing to be updated in power plants, and the advancements made in nuclear medicine.
Nuclear medicine and medical physics.
An important field is medical physics, and its subfields nuclear medicine, radiation therapy, health physics, and diagnostic imaging. From x-ray machines to MRI to PET, among many others, medical physics provides most of modern medicine's diagnostic capability along with providing many treatment options. 
Nuclear materials and nuclear fuels.
Nuclear materials research focuses on two main subject areas, nuclear fuels and irradiation-induced modification of materials. Improvement of three nuclear fuels is crucial for obtaining increased efficiency from nuclear reactors. Irradiation effects studies have many purposes, from studying structural changes to reactor components to studying nano-modification of metals using ion-beams or particle accelerators.
Radiation protection and measurement.
Radiation measurement is fundamental to the science and practice of radiation protection, sometimes known as radiological protection, which is the protection of people and the environment from the harmful effects of ionizing radiation
Nuclear engineers and radiological scientists are interested in the development of more advanced systems, and using these to improve imaging technologies. This includes detector design, fabrication and analysis, measurements of fundamental atomic and nuclear parameters, and radiation imaging systems, among other things.

</doc>
<doc id="37208" url="http://en.wikipedia.org/wiki?curid=37208" title="Landslide">
Landslide

A landslide, also known as a landslip, is a geological phenomenon that includes a wide range of ground movements, such as rockfalls, deep failure of slopes and shallow debris flows. Landslides can occur in offshore, coastal and onshore environments. Although the action of gravity is the primary driving force for a landslide to occur, there are other contributing factors affecting the original slope stability. Typically, pre-conditional factors build up specific sub-surface conditions that make the area/slope prone to failure, whereas the actual landslide often requires a trigger before being released.
Causes.
Landslides occur when the stability of the slope changes from a stable to an unstable condition. A change in the stability of a slope can be caused by a number of factors, acting together or alone. Natural causes of landslides include:
Landslides are aggravated by human activities, such as
Types.
Debris flow.
Slope material that becomes saturated with water may develop into a debris flow or mud flow. The resulting slurry of rock and mud may pick up trees, houses and cars, thus blocking bridges and tributaries causing flooding along its path.
Debris flow is often mistaken for flash flood, but they are entirely different processes.
Muddy-debris flows in alpine areas cause severe damage to structures and infrastructure and often claim human lives.
Muddy-debris flows can start as a result of slope-related factors and shallow landslides can dam stream beds, resulting in temporary water blockage. As the impoundments fail, a "domino effect" may be created, with a remarkable growth in the volume of the flowing mass, which takes up the debris in the stream channel. The solid-liquid mixture can reach densities of up to 2 tons/m³ and velocities of up to 14 m/s (Chiarle and Luino, 1998; Arattano, 2003). These processes normally cause the first severe road interruptions, due not only to deposits accumulated on the road (from several cubic metres to hundreds of cubic metres), but in some cases to the complete removal of bridges or roadways or railways crossing the stream channel. Damage usually derives from a common underestimation of mud-debris flows: in the alpine valleys, for example, bridges are frequently destroyed by the impact force of the flow because their span is usually calculated only for a water discharge. For a small basin in the Italian Alps (area = 1.76 km²) affected by a debris flow, Chiarle and Luino (1998) estimated a peak discharge of 750 m3/s for a section located in the middle stretch of the main channel. At the same cross section, the maximum foreseeable water discharge (by HEC-1), was 19 m³/s, a value about 40 times lower than that calculated for the debris flow that occurred.
Earthflows.
Earthflows are downslope, viscous flows of saturated, fine-grained materials, which move at any speed from slow to fast. Typically, they can move at speeds from 0.17 to 20 km/h. Though these are a lot like mudflows, overall they are more slow moving and are covered with solid material carried along by flow from within. They are different from fluid flows because they are more rapid. Clay, fine sand and silt, and fine-grained, pyroclastic material are all susceptible to earthflows. The velocity of the earthflow is all dependent on how much water content is in the flow itself: if there is more water content in the flow, the higher the velocity will be.
These flows usually begin when the pore pressures in a fine-grained mass increase until enough of the weight of the material is supported by pore water to significantly decrease the internal shearing strength of the material. This thereby creates a bulging lobe which advances with a slow, rolling motion. As these lobes spread out, drainage of the mass increases and the margins dry out, thereby lowering the overall velocity of the flow. This process causes the flow to thicken. The bulbous variety of earthflows are not that spectacular, but they are much more common than their rapid counterparts. They develop a sag at their heads and are usually derived from the slumping at the source.
Earthflows occur much more during periods of high precipitation, which saturates the ground and adds water to the slope content. Fissures develop during the movement of clay-like material which creates the intrusion of water into the earthflows. Water then increases the pore-water pressure and reduces the shearing strength of the material.
Debris landslide.
A debris slide is a type of slide characterized by the chaotic movement of rocks soil and debris mixed with water or ice (or both). They are usually triggered by the saturation of thickly vegetated slopes which results in an incoherent mixture of broken timber, smaller vegetation and other debris. Debris avalanches differ from debris slides because their movement is much more rapid. This is usually a result of lower cohesion or higher water content and commonly steeper slopes.
Steep coastal cliffs can be caused by catastrophic debris avalanches. These have been common on the submerged flanks of ocean island volcanos such as the Hawaiian Islands and the Cape Verde Islands.
Another slip of this type was Storegga landslide.
Movement: Debris slides generally start with big rocks that start at the top of the slide and begin to break apart as they slide towards the bottom. This is much slower than a debris avalanche. Debris avalanches are very fast and the entire mass seems to liquefy as it slides down the slope. This is caused by a combination of saturated material, and steep slopes. As the debris moves down the slope it generally follows stream channels leaving a v-shaped scar as it moves down the hill. This differs from the more U-shaped scar of a slump. Debris avalanches can also travel well past the foot of the slope due to their tremendous speed.
Sturzstrom.
A sturzstrom is a rare, poorly understood type of landslide, typically with a long run-out. Often very large, these slides are unusually mobile, flowing very far over a low angle, flat, or even slightly uphill terrain. 
Shallow landslide.
Landslide in which the sliding surface is located within the soil mantle or weathered bedrock (typically to a depth from few decimetres to some metres)is called a shallow landslide. They usually include debris slides, debris flow, and failures of road cut-slopes. Landslides occurring as single large blocks of rock moving slowly down slope are sometimes called block glides.
Shallow landslides can often happen in areas that have slopes with high permeable soils on top of low permeable bottom soils. The low permeable, bottom soils trap the water in the shallower, high permeable soils creating high water pressure in the top soils. As the top soils are filled with water and become heavy, slopes can become very unstable and slide over the low permeable bottom soils. Say there is a slope with silt and sand as its top soil and bedrock as its bottom soil. During an intense rainstorm, the bedrock will keep the rain trapped in the top soils of silt and sand. As the topsoil becomes saturated and heavy, it can start to slide over the bedrock and become a shallow landslide. 
R. H. Campbell did a study on shallow landslides on Santa Cruz Island California. He notes that if permeability decreases with depth, a perched water table may develop in soils at intense precipitation. When pore water pressures are sufficient to reduce effective normal stress to a critical level, failure occurs.
Deep-seated landslide.
Landslides in which the sliding surface is mostly deeply located below the maximum rooting depth of trees (typically to depths greater than ten meters). Deep-seated landslides usually involve deep regolith, weathered rock, and/or bedrock and include large slope failure associated with translational, rotational, or complex movement. This type of landslides are potentially occur in an tectonic active region like Zagros Mountain in Iran. These typically move slowly, only several meters per year, but occasionally move faster. They tend to be larger than shallow landslides and form along a plane of weakness such as a fault or bedding plane. They can be visually identified by concave scarps at the top and steep areas at the toe.
Causing tsunamis.
Landslides that occur undersea, or have impact into water, can generate tsunamis. Massive landslides can also generate megatsunamis, which are usually hundreds of meters high. In 1958, one such tsunami occurred in Lituya Bay in Alaska.
Landslide prediction mapping.
Landslide hazard analysis and mapping can provide useful information for catastrophic loss reduction, and assist in the development of guidelines for sustainable land use planning. The analysis is used to identify the factors that are related to landslides, estimate the relative contribution of factors causing slope failures, establish a relation between the factors and landslides, and to predict the landslide hazard in the future based on such a relationship. The factors that have been used for landslide hazard analysis can usually be grouped into geomorphology, geology, land use/land cover, and hydrogeology. Since many factors are considered for landslide hazard mapping, GIS is an appropriate tool because it has functions of collection, storage, manipulation, display, and analysis of large amounts of spatially referenced data which can be handled fast and effectively. Remote sensing techniques are also highly employed for landslide hazard assessment and analysis. Before and after aerial photographs and satellite imagery are used to gather landslide characteristics, like distribution and classification, and factors like slope, lithology, and land use/land cover to be used to help predict future events. Before and after imagery also helps to reveal how the landscape changed after an event, what may have triggered the landslide, and shows the process of regeneration and recovery.
Using satellite imagery in combination with GIS and on-the-ground studies, it is possible to generate maps of likely occurrences of future landslides. Such maps should show the locations of previous events as well as clearly indicate the probable locations of future events. In general, to predict landslides, one must assume that their occurrence is determined by certain geologic factors, and that future landslides will occur under the same conditions as past events. Therefore, it is necessary to establish a relationship between the geomorphologic conditions in which the past events took place and the expected future conditions.
Natural disasters are a dramatic example of people living in conflict with the environment. Early predictions and warnings are essential for the reduction of property damage and loss of life. Because landslides occur frequently and can represent some of the most destructive forces on earth, it is imperative to have a good understanding as to what causes them and how people can either help prevent them from occurring or simply avoid them when they do occur. Sustainable land management and development is an essential key to reducing the negative impacts felt by landslides.
GIS offers a superior method for landslide analysis because it allows one to capture, store, manipulate, analyze, and display large amounts of data quickly and effectively. Because so many variables are involved, it is important to be able to overlay the many layers of data to develop a full and accurate portrayal of what is taking place on the Earth's surface. Researchers need to know which variables are the most important factors that trigger landslides in any given location. Using GIS, extremely detailed maps can be generated to show past events and likely future events which have the potential to save lives, property, and money.
Extraterrestrial landslides.
Evidence of past landslides has been detected on many bodies in the solar system, but since most observations are made by probes that only observe for a limited time and most bodies in the solar system appear to be geologically inactive not many landslides are known to have happened in recent times. Both Venus and Mars have been subject to long-term mapping by orbiting satellites, and examples of landslides have been observed on both.

</doc>
<doc id="37211" url="http://en.wikipedia.org/wiki?curid=37211" title="William I of the Netherlands">
William I of the Netherlands

William I (Willem Frederik, Prince of Orange-Nassau; 24 August 1772 – 12 December 1843) was a Prince of Orange and the first King of the Netherlands and Grand Duke of Luxembourg.
In Germany, he was ruler (as Fürst) of the Principality of Nassau-Orange-Fulda from 1803 until 1806 and of the Principality of Orange-Nassau in the year 1806 and from 1813 until 1815. In 1813 he proclaimed himself 'Sovereign Prince' of the "United Netherlands." He proclaimed himself King of the Netherlands and Duke of Luxembourg on 16 March 1815. In the same year on 9 June William I became also the Grand Duke of Luxembourg and after 1839 he was furthermore the Duke of Limburg. After his abdication in 1840 he styled himself "King William Frederick, Count of Nassau".
Prince of Orange.
King William I's parents were the last stadtholder William V, Prince of Orange of the Dutch Republic, and his wife Wilhelmina of Prussia. Until 1806, William was formally known as William VI, Prince of Orange-Nassau, and between 1806 and 1813 also as Prince of Orange. In Berlin on 1 October 1791, William married his first cousin (Frederica Louisa) Wilhelmina, born in Potsdam. She was the daughter of King Frederick William II of Prussia. After Wilhelmina died in 1837, William married Countess Henriette d'Oultremont de Wégimont (Maastricht, 28 February 1792 – Schloss Rahe, 26 October 1864), created Countess of Nassau, on 17 February 1841, also in Berlin.
Youth and early military career.
As eldest son of the Prince of Orange (of whom there could only be one at a time) William was informally referred to as "Erfprins" (Hereditary Prince) by contemporaries (and later historians) in the period between his majority in 1790 and the death of his father in 1806 to distinguish him from William V.
Like his younger brother Prince Frederick of Orange-Nassau he was tutored by the Swiss mathematician Leonhard Euler and the Dutch historian Herman Tollius. They were both tutored in the military arts by general Prince Frederick Stamford. After the Patriot revolt had been suppressed in 1787, he in 1788-89 attended the military academy in Brunswick which was considered an excellent military school, together with his brother. In 1790 he visited a number of foreign courts like the one in Nassau and the Prussian capital Berlin, where he first met his future wife.:100
William subsequently studied briefly at the University of Leiden. In 1790 he was appointed a general of infantry in the States Army of which his father was Captain general, and he was made a member of the Council of State of the Netherlands.In November 1791 he took his new bride to The Hague.:101
After the National Convention of the First French Republic had declared war on the stadtholder of the Dutch Republic in February 1793, William was appointed commander-in-chief of the "veldleger" (mobile army) of the States Army (his father remained the nominal head of the armed forces).:157 As such he commanded the troops that took part in the Flanders Campaign of 1793-1795. He took part in the battles of Veurne, Menin, and Wervik (where his brother was wounded) in 1793, the siege of Landrecies (1794), which fortress surrendered to him, and the Battle of Fleurus (1794), to name the most important. In May 1794 he had replaced general Kaunitz as commander of the combined Austro-Dutch forces on the instigation of Emperor Francis II who apparently had a high opinion of him.:270 But the French armies proved too strong, and the allied leadership too inept, and the allies were defeated. The French first entered Dutch Brabant which they dominated after the Battle of Boxtel. When in the winter of 1794-95 the rivers in the Rhine delta froze over, the French breached the southern Dutch Water Line and the situation became militarily untenable. In many places Dutch revolutionaries took over the local government. After the Batavian Revolution in Amsterdam on 18 January 1795 the stadtholder decided to flee to Britain, and his sons accompanied him. (On this last day in Holland his father relieved William honorably of his commands). The next day the Batavian Republic was proclaimed.:341–365, 374–404, 412
Exile.
Soon after his departure to Britain the Hereditary Prince went back to the Continent, where his brother was assembling former members of the States Army in Osnabrück for a planned foray into the Batavian Republic in the Summer of 1795. However, the neutral Prussian government forbade this.:231–235
In 1799, William landed in the current North Holland as part of an Anglo-Russian invasion of Holland. The Hereditary Prince was instrumental in fomenting a mutiny on the Batavian naval squadron in the Vlieter, resulting in the surrender of the ships without a fight to the Royal Navy, which accepted the surrender in the name of the stadtholder. The local Dutch population, however, was not pleased with the arrival of the prince. One local Orangist was even executed. The hoped-for popular uprising failed to materialise. After several minor battles the Hereditary Prince was forced to leave the country again after the Convention of Alkmaar. The mutineers of the Batavian fleet and a number of deserters from the Batavian army accompanied the retreating British troops to Britain. There William formed the King's Dutch Brigade with these troops, a military unit in British service, that swore oaths of allegiance to the British King, but also to the States General, defunct since 1795, "whenever those would be reconstituted." This brigade trained on the Isle of Wight in 1800 and was eventually used by the British in Ireland.:241–265
When peace was concluded between Great Britain and the French Republic under First Consul Napoleon Bonaparte the Orange exiles were at their nadir. The Dutch Brigade was dissolved on 12 July 1802. Many members of the brigade went home to the Batavian Republic, thanks to an amnesty. The surrendered ships of the Batavian navy were not returned, due to an agreement between the stadtholder and the British government of 11 March 1800.:329–330 Instead the stadtholder was allowed to sell them to the Royal Navy for an appreciable sum.
The stadtholder, feeling betrayed by the British, left for Germany. The Hereditary Prince, having a more flexible mind, went to visit Napoleon at St. Cloud in 1802. He apparently charmed the First Consul, and was charmed by him. Napoleon raised hopes for William that he might have an important role in a reformed Batavian Republic. Meanwhile William's brother-in-law Frederick William III of Prussia, neutral at the time, promoted a Franco-Prussian convention of 23 May 1802, in addition to the Treaty of Amiens, that gave the House of Orange a few abbatial domains in Germany, that were combined to the Principality of Nassau-Orange-Fulda by way of indemnification for its losses in the Batavian Republic. The stadtholder gave this principality immediately to his son.:452
When Napoleon invaded Germany in 1806 and war broke out between the French Empire and Prussia, William supported his Prussian relatives, though he was nominally a French vassal. He received command of a Prussian division which took part in the Battle of Jena–Auerstedt. The Prussians lost that battle and William was forced to surrender his troops rather ignominiously at Erfurt the day after the battle. He was made a prisoner of war, but was paroled soon. Napoleon punished him for his betrayal, however, by taking away his principality. As a parolee, William was not allowed to take part in the hostilities anymore. After the Peace of Tilsit William received a pension from France in compensation.:454–469, 471, 501
In the same year, 1806, his father, the Prince of Orange died, and William not only inherited the title, but also his father's claims on the inheritance embodied in the Nassau lands. This would become important a few years later, when developments in Germany coincided to make William the Fürst (Prince) of a divers assembly of Nassau lands that had belonged to other branches of the House of Nassau.
But before this came about, in 1809 tensions between Austria and France became intense. William did not hesitate to join the Austrian army as a "Feldmarschalleutnant" (major-general) in May 1809:516 As a member of the staff of the Austrian supreme commander, Archduke Charles he took part in the Battle of Wagram, where he was wounded in the leg.:520–523
Czar Alexander I of Russia played a central role in the restoration of the Netherlands. Prince William VI (as he was now known), who had been living in exile in Prussia, met with Alexander I in March 1813. Alexander promised to support William and help restore an independent Netherlands with William as king. Russian troops in the Netherlands participated with their Prussian allies in liberating the country; dynastic considerations of marriage between the royal houses of Great Britain and the Netherlands, assured British approval.
Return.
After Napoleon's defeat at Leipzig (October 1813), the French troops retreated to France from all over Europe. The Netherlands had been annexed to the French Empire by Napoleon in 1810. But now city after city was evacuated by the French occupation troops. In the power vacuum that this created a number of former Orangist politicians and former Patriots formed a provisional government in November 1813. Although a large number of the members of the provisional government had helped drive out William V 18 years earlier, it was taken for granted that his son would have to head any new regime. They also agreed it would be better in the long term for the Dutch to restore him themselves, rather than have the Great Powers impose him on the country. The Dutch population was pleased with the departure of the French, who had ruined the Dutch economy, and this time welcomed the prince.:634–642
After having been invited by the Driemanschap (Triumvirate) of 1813, on 30 November 1813 William disembarked from HMS "Warrior" and landed at Scheveningen beach, only a few yards from the place where he had left the country with his father 18 years previously, and on 6 December the provisional government offered him the title of King. William refused, instead proclaiming himself "sovereign prince". He also wanted the rights of the people to be guaranteed by "a wise constitution".:643
The constitution offered William extensive (almost absolute) powers. Ministers were only responsible to him, while a unicameral parliament (the States General) exercised only limited power. He was inaugurated as sovereign prince in the New Church in Amsterdam. In August 1814, he was appointed Governor-General of the former Austrian Netherlands (modern-day Belgium) by the Allied Powers who occupied that country. He was also made Grand Duke of Luxembourg, having received that territory in return for trading his hereditary German lands to Prussia and the Duke of Nassau. The Great Powers had already agreed via the secret Eight Articles of London to unite the Low Countries into a single kingdom. It was believed that a united country on the North Sea would help keep France in check. With the de facto addition of the Austrian Netherlands and Luxembourg to his realm, William had fulfilled his family's three-century quest of uniting the Low Countries.
King of the Netherlands.
Feeling threatened by Napoleon, who had escaped from Elba, William proclaimed the Netherlands a kingdom on 16 March 1815 at the urging of the powers gathered at the Congress of Vienna. His son, the future king William II, fought as a commander at the Battle of Waterloo. After Napoleon had been sent into exile, William adopted a new constitution which included many features of the old constitution, such as extensive royal powers. He was formally confirmed as hereditary ruler of what was known as the United Kingdom of the Netherlands at the Congress of Vienna.
He was the 876th Knight of the Order of the Golden Fleece in Spain and the 648th Knight of the Order of the Garter in 1814.
Principal changes.
The States General was divided into two chambers. The "Eerste Kamer" (First Chamber or Senate or House of Lords) was appointed by the King. The "Tweede Kamer" (Second Chamber or House of Representatives or House of Commons) was elected by the Provincial States, which were in turn chosen by census suffrage. The 110 seats were divided equally between the North and the South, although the population of the North (2 million) was significantly less than that of the South (3.5 million). The States General's primary function was to approve the King's laws and decrees. The constitution contained many present-day Dutch political institutions; however, their functions and composition have changed greatly over the years.
The constitution was accepted in the North, but not in the South. The under-representation of the South was one of the causes of the Belgian Revolution. Referendum turnout was low, in the Southern provinces, but William interpreted all abstentions to be "yes" votes. He prepared a lavish inauguration for himself in Brussels, where he gave the people copper coins (leading to his first nickname, "the Copper King").
The spearhead of King William's policies was economic progress. As he founded many trade institutions, his second nickname was "the King-Merchant". In 1822, he founded the Algemeene Nederlandsche Maatschappij ter Begunstiging van de Volksvlijt, which would become one of the most important institutions of Belgium after its independence. Industry flourished, especially in the South. In 1817, he also founded three universities in the Southern provinces, such as a new University of Leuven, the University of Ghent and the University of Liège. The Northern provinces, meanwhile, were the centre of trade. This, in combination with the colonies (Dutch East Indies, Surinam, Curaçao and Dependencies, and the Dutch Gold Coast) created great wealth for the Kingdom. However, the money flowed into the hands of Dutch directors. Only a few Belgians managed to profit from the economic growth. Feelings of economic inequity were another cause of the Belgian uprising.
William was also determined to create a unified people, even though the north and the south had drifted far apart culturally and economically since the south was reconquered by Spain after the Act of Abjuration of 1581. The North was commercial, Protestant and entirely Dutch-speaking; the south was industrial, Roman Catholic and divided between Dutch and French-speakers.
Officially, a separation of church and state existed in the kingdom. However, William himself was a strong supporter of the Reformed Church. This led to resentment among the people in the mostly Catholic south. William had also devised controversial language and school policies. Dutch was imposed as the official language in (the Dutch-speaking region of) Flanders; this angered French-speaking aristocrats and industrial workers. Schools throughout the Kingdom were required to instruct students in the Reformed faith and the Dutch language. Many in the South feared that the King sought to extinguish Catholicism and the French language.
Belgian uprising.
In August 1830 Daniel Auber's opera "La Muette de Portici", about the repression of Neapolitans, was staged in Brussels. Performances of this show seemed to crystallize a sense of nationalism and "Hollandophobia" in Brussels, and spread to the rest of the South. Rioting ensued, chiefly aimed at the kingdom's unpopular justice minister, Cornelis Felix van Maanen, who lived in Brussels. An infuriated William responded by sending troops to repress the riots. However, the riots had spread to other Southern cities. The riots quickly became popular uprisings. Soon an independent state of Belgium was proclaimed.
The next year, William sent his sons William, the Prince of Orange, and Prince Frederick to invade the new state. Although initially victorious in this Ten Days' Campaign, the Dutch army was forced to retreat after the threat of French intervention. Some support for the Orange dynasty (chiefly among Flemings) persisted for years but the Dutch never regained control over Belgium. William nevertheless continued the war for eight years. His economic successes became overshadowed by a perceived mismanagement of the war effort. High costs of the war came to burden the Dutch economy, fueling public resentment. In 1839, William was forced to end the war. The United Kingdom of the Netherlands was dissolved by the Treaty of London (1839) and the northern part continued as the Kingdom of the Netherlands. It was not renamed, however, as the "United"-prefix had never been part of its official name, but rather was retrospectively added by historians for descriptive purposes (cf. Weimar Republic).
Constitutional changes and abdication in later life.
Constitutional changes were initiated in 1840 because the terms which involved the United Kingdom of the Netherlands had to be removed. These constitutional changes also included the introduction of judicial ministerial responsibility. Although the policies remained uncontrolled by parliament, the prerogative was controllable now. The very conservative William could not live with these constitutional changes. This, the disappointment about the loss of Belgium, and William's intention to marry Henrietta d'Oultremont (paradoxically both "Belgian" and Roman Catholic) made him wish to abdicate. He fulfilled this intent on 7 October 1840 and his eldest son acceded to the throne as king William II. William I died in 1843 in Berlin at the age of 71.
Children.
With his wife Wilhelmina, King William I had six children:

</doc>
<doc id="37219" url="http://en.wikipedia.org/wiki?curid=37219" title="Billung">
Billung

The House of Billung was a dynasty of Saxon noblemen in the 9th through 12th centuries. 
The first known member of the house was Count Wichmann, mentioned as a Billung in 811. Oda, the wife of Count Liudolf, oldest known member of the Liudolfing House, was also a Billung. 
In the 10th century, the property of the family was centered in the Bardengau around Lüneburg and they controlled the march named after them. In the middle of the 10th century, when the Saxon dukes of the House of Liudolfing had also become German kings, King Otto the Great entrusted more and more of his ducal authority to Hermann Billung. For five generations, the House of Billung ruled the Duchy of Saxony. 
The house became extinct when Duke Magnus died in 1106 without sons; the family's property was divided between his two daughters. His daughter Wulfhilde married Henry IX, Duke of Bavaria, a member of the House of Welf; his daughter Eilika married Otto, Count of Ballenstedt, a member of the House of Ascania. As a consequence, for the following decades control of Saxony was contested between the Welfs and Ascanians.
The Billung dukes of Saxony were:

</doc>
<doc id="37220" url="http://en.wikipedia.org/wiki?curid=37220" title="Infection">
Infection

Infection is the invasion of an organism's body tissues by disease-causing agents, their multiplication, and the reaction of host tissues to these organisms and the toxins they produce. Infectious diseases, also known as transmissible diseases or communicable diseases, comprise clinically evident illness (i.e., characteristic medical signs or symptoms of disease) resulting from the infection, presence and growth of pathogens in an individual host organism.
Infections are caused by infectious agents such as viruses, viroids, and prions, microorganisms such as bacteria, nematodes such as roundworms and pinworms, arthropods such as ticks, mites, fleas, and lice, fungi such as ringworm, and other macroparasites such as tapeworms.
Hosts can fight infections using their immune system. Mammalian hosts react to infections with an innate response, often involving inflammation, followed by an adaptive response.
The branch of medicine that focuses on infections and pathogens is infectious disease medicine. Physicians and veterinarians may use specific pharmaceutical drugs to treat infections (antibiotics, antivirals, antifungals, antiprotozoals, antihelminthics).
Classification.
Bacterial infections are classified by the causative agent, as well as the symptoms and medical signs produced.
Symptomatic infections are "apparent," whereas an infection that is active but does not produce noticeable symptoms may be called "inapparent," "silent," or "subclinical." An infection that is inactive or dormant is called a "latent infection".
A short-term infection is an "acute" infection. A long-term infection is a chronic infection.
Primary versus opportunistic.
Among the vast varieties of microorganisms, relatively few cause disease in otherwise healthy individuals. Infectious disease results from the interplay between those few pathogens and the defenses of the hosts they infect. The appearance and severity of disease resulting from any pathogen, depends upon the ability of that pathogen to damage the host as well as the ability of the host to resist the pathogen. Clinicians therefore classify infectious microorganisms or microbes according to the status of host defenses - either as "primary pathogens" or as "opportunistic pathogens":
Occult infection.
Occult infection is a hidden infection first recognized by secondary manifestations. Dr. Fran Giampietro discovered this type, and coined the term "occult infection" in the late 1930s.
Infectious or not.
One way of proving that a given disease is "infectious", is to satisfy Koch's postulates (first proposed by Robert Koch), which demands that the infectious agent be identified only in patients and not in healthy controls, and that patients who contract the agent also develop the disease. These postulates were first used in the discovery that Mycobacteria species cause tuberculosis. Koch's postulates can not be applied ethically for many human diseases because they require experimental infection of a healthy individual with a pathogen produced as a pure culture. Often, even clearly infectious diseases do not meet the infectious criteria. For example, "Treponema pallidum", the causative spirochete of syphilis, cannot be cultured "in vitro" - however the organism can be cultured in rabbit testes. It is less clear that a pure culture comes from an animal source serving as host than it is when derived from microbes derived from plate culture.
Epidemiology is another important tool used to study disease in a population. For infectious diseases it helps to determine if a disease outbreak is sporadic (occasional occurrence), endemic (regular cases often occurring in a region), epidemic (an unusually high number of cases in a region), or pandemic (a global epidemic).
Contagiousness.
Infectious diseases are sometimes called contagious disease when they are easily transmitted by contact with an ill person or their secretions (e.g., influenza). Thus, a contagious disease is a subset of infectious disease that is especially infective or easily transmitted. Other types of infectious/transmissible/communicable diseases with more specialized routes of infection, such as vector transmission or sexual transmission, are usually not regarded as "contagious," and often do not require medical isolation (sometimes loosely called quarantine) of victims. However, this specialized connotation of the word "contagious" and "contagious disease" (easy transmissibility) is not always respected in popular use.
By anatomic location.
Infections can be classified by the anatomic location or organ system infected, including:
In addition, locations of inflammation where infection is the most common cause include pneumonia, meningitis and salpingitis.
Signs and symptoms.
The symptoms of an infection depend on the type of disease. Some signs of infection affect the whole body generally, such as fatigue, loss of appetite, weight loss, fevers, night sweats, chills, aches and pains. Others are specific to individual body parts, such as skin rashes, coughing, or a runny nose.
In certain cases, infectious diseases may be asymptomatic for much or even all of their course in a given host. In the latter case, the disease may only be defined as a "disease" (which by definition means an illness) in hosts who secondarily become ill after contact with an asymptomatic carrier. An infection is not synonymous with an infectious disease, as some infections do not cause illness in a host.
Bacterial or viral.
Bacterial and viral infections can both cause the same kinds of symptoms. It can be difficult to distinguish which is the cause of a specific infection. It's important to distinguish, because viral infections cannot be cured by antibiotics.
Pathophysiology.
There is a general chain of events that applies to infections. For infections to occur, a given chain of events must occur. The chain of events involves several steps—which include the infectious agent, reservoir, entering a susceptible host, exit and transmission to new hosts. Each of the links must be present in a chronological order for an infection to develop. Understanding these steps helps health care workers target the infection and prevent it from occurring in the first place.
Colonization.
Infection begins when an organism successfully colonizes by entering the body, growing and multiplying. Most humans are not easily infected. Those who are weak, sick, malnourished, have cancer or are diabetic have increased susceptibility to chronic or persistent infections. Individuals who have a suppressed immune system are particularly susceptible to opportunistic infections. Entrance to the host at host-pathogen interface, generally occurs through the mucosa in orifices like the oral cavity, nose, eyes, genitalia, anus, or the microbe can enter through open wounds. While a few organisms can grow at the initial site of entry, many migrate and cause systemic infection in different organs. Some pathogens grow within the host cells (intracellular) whereas others grow freely in bodily fluids.
Wound colonization refers to nonreplicating microorganisms within the wound, while in infected wounds, replicating organisms exist and tissue is injured. All multicellular organisms are colonized to some degree by extrinsic organisms, and the vast majority of these exist in either a mutualistic or commensal relationship with the host. An example of the former is the anaerobic bacteria species, which colonizes the mammalian colon, and an example of the latter is various species of staphylococcus that exist on human skin. Neither of these colonizations are considered infections. The difference between an infection and a colonization is often only a matter of circumstance. Non-pathogenic organisms can become pathogenic given specific conditions, and even the most virulent organism requires certain circumstances to cause a compromising infection. Some colonizing bacteria, such as "Corynebacteria sp." and "viridans streptococci", prevent the adhesion and colonization of pathogenic bacteria and thus have a symbiotic relationship with the host, preventing infection and speeding wound healing.
The variables involved in the outcome of a host becoming inoculated by a pathogen and the ultimate outcome include:
As an example, the staphylococcus species remains harmless on the skin, but, when present in a normally sterile space, such as in the capsule of a joint or the peritoneum, multiplies without resistance and creates a burden on the host.
It can be difficult to know which chronic wounds are infected. Despite the huge number of wounds seen in clinical practice, there are limited quality data for evaluated symptoms and signs. A review of chronic wounds in the Journal of the American Medical Association's "Rational Clinical Examination Series" quantified the importance of increased pain as an indicator of infection. The review showed that the most useful finding is an increase in the level of pain [likelihood ratio (LR) range, 11-20] makes infection much more likely, but the absence of pain (negative likelihood ratio range, 0.64-0.88) does not rule out infection (summary LR 0.64-0.88).
Disease.
Disease can arise if the host's protective immune mechanisms are compromised and the organism inflicts damage on the host. Microorganisms can cause tissue damage by releasing a variety of toxins or destructive enzymes. For example, Clostridium tetani releases a toxin that paralyzes muscles, and staphylococcus releases toxins that produce shock and sepsis. Not all infectious agents cause disease in all hosts. For example less than 5% of individuals infected with polio develop disease. On the other hand, some infectious agents are highly virulent. The prion causing mad cow disease and Creutzfeldt–Jakob disease kills almost all animals and people that are infected.
Persistent infections occur because the body is unable to clear the organism after the initial infection. Persistent infections are characterized by the continual presence of the infectious organism, often as latent infection with occasional recurrent relapses of active infection. There are some viruses that can maintain a persistent infection by infecting different cells of the body. Some viruses once acquired never leave the body. A typical example is the herpes virus, which tends to hide in nerves and become reactivated when specific circumstances arise.
Persistent infections cause millions of deaths globally each year. Chronic infections by parasites account for a high morbidity and mortality in many underdeveloped countries.
Transmission.
For infecting organisms to survive and repeat the infection cycle in other hosts, they (or their progeny) must leave an existing reservoir and cause infection elsewhere. Infection transmission can take place via many potential routes:
The relationship between "virulence versus transmissibility" is complex; if a disease is rapidly fatal, the host may die before the microbe can be passed along to another host.
Prevention.
Techniques like hand washing, wearing gowns, and wearing face masks can help prevent infections from being passed from the surgeon to the patient or vice versa. Frequent hand washing remains the most important defense against the spread of unwanted organisms. Nutrition must be improved and one has to make changes in life style- such as avoiding the use of illicit drugs, using a condom, and entering an exercise program. Cooking foods well and avoiding foods that have been left outside for a long time is also important.
Antimicrobial substances used to prevent transmission of infections include:
One of the ways to prevent or slow down the transmission of infectious diseases is to recognize the different characteristics of various diseases. Some critical disease characteristics that should be evaluated include virulence, distance traveled by victims, and level of contagiousness. The human strains of Ebola virus, for example, incapacitate their victims extremely quickly and kill them soon after. As a result, the victims of this disease do not have the opportunity to travel very far from the initial infection zone. Also, this virus must spread through skin lesions or permeable membranes such as the eye. Thus, the initial stage of Ebola is not very contagious since its victims experience only internal hemorrhaging. As a result of the above features, the spread of Ebola is very rapid and usually stays within a relatively confined geographical area. In contrast, the Human Immunodeficiency Virus (HIV) kills its victims very slowly by attacking their immune system. As a result, many of its victims transmit the virus to other individuals before even realizing that they are carrying the disease. Also, the relatively low virulence allows its victims to travel long distances, increasing the likelihood of an epidemic.
Another effective way to decrease the transmission rate of infectious diseases is to recognize the effects of small-world networks. In epidemics, there are often extensive interactions within hubs or groups of infected individuals and other interactions within discrete hubs of susceptible individuals. Despite the low interaction between discrete hubs, the disease can jump to and spread in a susceptible hub via a single or few interactions with an infected hub. Thus, infection rates in small-world networks can be reduced somewhat if interactions between individuals within infected hubs are eliminated (Figure 1). However, infection rates can be drastically reduced if the main focus is on the prevention of transmission jumps between hubs. The use of needle exchange programs in areas with a high density of drug users with HIV is an example of the successful implementation of this treatment method. [6] Another example is the use of ring culling or vaccination of potentially susceptible livestock in adjacent farms to prevent the spread of the foot-and-mouth virus in 2001.
A general method to prevent transmission of vector-borne pathogens is pest control.
Immunity.
Infection with most pathogens does not result in death of the host and the offending organism is ultimately cleared after the symptoms of the disease have waned. This process requires immune mechanisms to kill or inactivate the inoculum of the pathogen. Specific acquired immunity against infectious diseases may be mediated by antibodies and/or T lymphocytes. Immunity mediated by these two factors may be manifested by:
The immune system response to a microorganism often causes symptoms such as a high fever and inflammation, and has the potential to be more devastating than direct damage caused by a microbe.
Resistance to infection (immunity) may be acquired following a disease, by asymptomatic carriage of the pathogen, by harboring an organism with a similar structure (crossreacting), or by vaccination. Knowledge of the protective antigens and specific acquired host immune factors is more complete for primary pathogens than for opportunistic pathogens.
Immune resistance to an infectious disease requires a critical level of either antigen-specific antibodies and/or T cells when the host encounters the pathogen. Some individuals develop natural serum antibodies to the surface polysaccharides of some agents although they have had little or no contact with the agent, these natural antibodies confer specific protection to adults and are passively transmitted to newborns.
Host genetic factors.
The clearance of the pathogens, either treatment-induced or spontaneous, it can be influenced by the genetic variants carried by the individual patients. For instance, for genotype 1 hepatitis C treated with Pegylated interferon-alpha-2a or Pegylated interferon-alpha-2b (brand names Pegasys or PEG-Intron) combined with ribavirin, it has been shown that genetic polymorphisms near the human IL28B gene, encoding interferon lambda 3, are associated with significant differences in the treatment-induced clearance of the virus. This finding, originally reported in Nature, showed that genotype 1 hepatitis C patients carrying certain genetic variant alleles near the IL28B gene are more possibly to achieve sustained virological response after the treatment than others. Later report from Nature demonstrated that the same genetic variants are also associated with the natural clearance of the genotype 1 hepatitis C virus.
Diagnosis.
Diagnosis of infectious disease sometimes involves identifying an infectious agent either directly or indirectly. In practice most minor infectious diseases such as warts, cutaneous abscesses, respiratory system infections and diarrheal diseases are diagnosed by their clinical presentation. Conclusions about the cause of the disease are based upon the likelihood that a patient came in contact with a particular agent, the presence of a microbe in a community, and other epidemiological considerations. Given sufficient effort, all known infectious agents can be specifically identified. The benefits of identification, however, are often greatly outweighed by the cost, as often there is no specific treatment, the cause is obvious, or the outcome of an infection is benign.
Diagnosis of infectious disease is nearly always initiated by medical history and physical examination. More detailed identification techniques involve the culture of infectious agents isolated from a patient. Culture allows identification of infectious organisms by examining their microscopic features, by detecting the presence of substances produced by pathogens, and by directly identifying an organism by its genotype. Other techniques (such as X-rays, CAT scans, PET scans or NMR) are used to produce images of internal abnormalities resulting from the growth of an infectious agent. The images are useful in detection of, for example, a bone abscess or a spongiform encephalopathy produced by a prion.
Symptomatic diagnostics.
The diagnosis is aided by the presenting symptoms in any individual with an infectious disease, yet it usually needs additional diagnostic techniques to confirm the suspicion. Some signs are specifically characteristic and indicative of a disease and are called pathognomonic signs; but these are rare. Not all infections are symptomatic.
In children the presence of cyanosis, rapid breathing, poor peripheral perfusion, or a petechial rash increases the risk of a serious infection by greater than 5 fold. Other important indicators include parental concern, clinical instinct, and temperature greater than 40 °C.
Microbial culture.
Microbiological culture is a principal tool used to diagnose infectious disease. In a microbial culture, a growth medium is provided for a specific agent. A sample taken from potentially diseased tissue or fluid is then tested for the presence of an infectious agent able to grow within that medium. Most pathogenic bacteria are easily grown on nutrient agar, a form of solid medium that supplies carbohydrates and proteins necessary for growth of a bacterium, along with copious amounts of water. A single bacterium will grow into a visible mound on the surface of the plate called a colony, which may be separated from other colonies or melded together into a "lawn". The size, color, shape and form of a colony is characteristic of the bacterial species, its specific genetic makeup (its strain), and the environment which supports its growth. Other ingredients are often added to the plate to aid in identification. Plates may contain substances that permit the growth of some bacteria and not others, or that change color in response to certain bacteria and not others. Bacteriological plates such as these are commonly used in the clinical identification of infectious bacterium. Microbial culture may also be used in the identification of viruses: the medium in this case being cells grown in culture that the virus can infect, and then alter or kill. In the case of viral identification, a region of dead cells results from viral growth, and is called a "plaque". Eukaryotic parasites may also be grown in culture as a means of identifying a particular agent.
In the absence of suitable plate culture techniques, some microbes require culture within live animals. Bacteria such as "Mycobacterium leprae" and "Treponema pallidum" can be grown in animals, although serological and microscopic techniques make the use of live animals unnecessary. Viruses are also usually identified using alternatives to growth in culture or animals. Some viruses may be grown in embryonated eggs. Another useful identification method is Xenodiagnosis, or the use of a vector to support the growth of an infectious agent. Chagas disease is the most significant example, because it is difficult to directly demonstrate the presence of the causative agent, "Trypanosoma cruzi" in a patient, which therefore makes it difficult to definitively make a diagnosis. In this case, xenodiagnosis involves the use of the vector of the Chagas agent "T. cruzi", an uninfected triatomine bug, which takes a blood meal from a person suspected of having been infected. The bug is later inspected for growth of "T. cruzi" within its gut.
Microscopy.
Another principal tool in the diagnosis of infectious disease is microscopy. Virtually all of the culture techniques discussed above rely, at some point, on microscopic examination for definitive identification of the infectious agent. Microscopy may be carried out with simple instruments, such as the compound light microscope, or with instruments as complex as an electron microscope. Samples obtained from patients may be viewed directly under the light microscope, and can often rapidly lead to identification. Microscopy is often also used in conjunction with biochemical staining techniques, and can be made exquisitely specific when used in combination with antibody based techniques. For example, the use of antibodies made artificially fluorescent (fluorescently labeled antibodies) can be directed to bind to and identify a specific antigens present on a pathogen. A fluorescence microscope is then used to detect fluorescently labeled antibodies bound to internalized antigens within clinical samples or cultured cells. This technique is especially useful in the diagnosis of viral diseases, where the light microscope is incapable of identifying a virus directly.
Other microscopic procedures may also aid in identifying infectious agents. Almost all cells readily stain with a number of basic dyes due to the electrostatic attraction between negatively charged cellular molecules and the positive charge on the dye. A cell is normally transparent under a microscope, and using a stain increases the contrast of a cell with its background. Staining a cell with a dye such as Giemsa stain or crystal violet allows a microscopist to describe its size, shape, internal and external components and its associations with other cells. The response of bacteria to different staining procedures is used in the taxonomic classification of microbes as well. Two methods, the Gram stain and the acid-fast stain, are the standard approaches used to classify bacteria and to diagnosis of disease. The Gram stain identifies the bacterial groups Firmicutes and Actinobacteria, both of which contain many significant human pathogens. The acid-fast staining procedure identifies the Actinobacterial genera "Mycobacterium" and "Nocardia".
Biochemical tests.
Biochemical tests used in the identification of infectious agents include the detection of metabolic or enzymatic products characteristic of a particular infectious agent. Since bacteria ferment carbohydrates in patterns characteristic of their genus and species, the detection of fermentation products is commonly used in bacterial identification. Acids, alcohols and gases are usually detected in these tests when bacteria are grown in selective liquid or solid media.
The isolation of enzymes from infected tissue can also provide the basis of a biochemical diagnosis of an infectious disease. For example, humans can make neither RNA replicases nor reverse transcriptase, and the presence of these enzymes are characteristic of specific types of viral infections. The ability of the viral protein hemagglutinin to bind red blood cells together into a detectable matrix may also be characterized as a biochemical test for viral infection, although strictly speaking hemagglutinin is not an "enzyme" and has no metabolic function.
Serological methods are highly sensitive, specific and often extremely rapid tests used to identify microorganisms. These tests are based upon the ability of an antibody to bind specifically to an antigen. The antigen, usually a protein or carbohydrate made by an infectious agent, is bound by the antibody. This binding then sets off a chain of events that can be visibly obvious in various ways, dependent upon the test. For example, "Strep throat" is often diagnosed within minutes, and is based on the appearance of antigens made by the causative agent, "S. pyogenes", that is retrieved from a patients throat with a cotton swab. Serological tests, if available, are usually the preferred route of identification, however the tests are costly to develop and the reagents used in the test often require refrigeration. Some serological methods are extremely costly, although when commonly used, such as with the "strep test", they can be inexpensive.
Complex serological techniques have been developed into what are known as Immunoassays. Immunoassays can use the basic antibody – antigen binding as the basis to produce an electro - magnetic or particle radiation signal, which can be detected by some form of instrumentation. Signal of unknowns can be compared to that of standards allowing quantitation of the target antigen. To aid in the diagnosis of infectious diseases, immunoassays can detect or measure antigens from either infectious agents or proteins generated by an infected organism in response to a foreign agent. For example, immunoassay A may detect the presence of a surface protein from a virus particle. Immunoassay B on the other hand may detect or measure antibodies produced by an organism’s immune system which are made to neutralize and allow the destruction of the virus.
Instrumentation can be used to read extremely small signals created by secondary reactions linked to the antibody – antigen binding. Instrumentation can control sampling, reagent use, reaction times, signal detection, calculation of results, and data management to yield a cost effective automated process for diagnosis of infectious disease.
Molecular diagnostics.
Technologies based upon the polymerase chain reaction (PCR) method will become nearly ubiquitous gold standards of diagnostics of the near future, for several reasons. First, the catalog of infectious agents has grown to the point that virtually all of the significant infectious agents of the human population have been identified. Second, an infectious agent must grow within the human body to cause disease; essentially it must amplify its own nucleic acids in order to cause a disease. This amplification of nucleic acid in infected tissue offers an opportunity to detect the infectious agent by using PCR. Third, the essential tools for directing PCR, primers, are derived from the genomes of infectious agents, and with time those genomes will be known, if they are not already.
Thus, the technological ability to detect any infectious agent rapidly and specifically are currently available. The only remaining blockades to the use of PCR as a standard tool of diagnosis are in its cost and application, neither of which is insurmountable. The diagnosis of a few diseases will not benefit from the development of PCR methods, such as some of the clostridial diseases (tetanus and botulism). These diseases are fundamentally biological poisonings by relatively small numbers of infectious bacteria that produce extremely potent neurotoxins. A significant proliferation of the infectious agent does not occur, this limits the ability of PCR to detect the presence of any bacteria.
Indication of tests.
There is usually an indication for a specific identification of an infectious agent only when such identification can aid in the treatment or prevention of the disease, or to advance knowledge of the course of an illness prior to the development of effective therapeutic or preventative measures. For example, in the early 1980s, prior to the appearance of AZT for the treatment of AIDS, the course of the disease was closely followed by monitoring the composition of patient blood samples, even though the outcome would not offer the patient any further treatment options. In part, these studies on the appearance of HIV in specific communities permitted the advancement of hypotheses as to the route of transmission of the virus. By understanding how the disease was transmitted, resources could be targeted to the communities at greatest risk in campaigns aimed at reducing the number of new infections. The specific serological diagnostic identification, and later genotypic or molecular identification, of HIV also enabled the development of hypotheses as to the temporal and geographical origins of the virus, as well as a myriad of other hypothesis. The development of molecular diagnostic tools have enabled physicians and researchers to monitor the efficacy of treatment with anti-retroviral drugs. Molecular diagnostics are now commonly used to identify HIV in healthy people long before the onset of illness and have been used to demonstrate the existence of people who are genetically resistant to HIV infection. Thus, while there still is no cure for AIDS, there is great therapeutic and predictive benefit to identifying the virus and monitoring the virus levels within the blood of infected individuals, both for the patient and for the community at large.
Anti-infective treatments.
When infection attacks the body, "anti-infective" drugs can suppress the infection. Four types of "anti-infective" or drugs exist: antibacterial (antibiotic), antiviral, antitubercular, and antifungal. Depending on the severity and the type of infection, the antibiotic may be given by mouth, injection or may be applied topically. Severe infections of the brain are usually treated with intravenous antibiotics. Sometimes, multiple antibiotics are used to decrease the risk of resistance and increase efficacy. Antibiotics only work for bacteria and do not affect viruses. Antibiotics work by slowing down the multiplication of bacteria or killing the bacteria. The most common classes of antibiotics used in medicine include penicillin, cephalosporins, aminoglycosides, macrolides, quinolones and tetracyclines.
Epidemiology.
"About 10 million people around the world died of communicable diseases in 2010," CBC News reports, citing data from the Institute for Health Metrics and Evaluation.
The World Health Organization collects information on global deaths by International Classification of Disease (ICD) code categories. The following table lists the top infectious disease killers which caused more than 100,000 deaths in 2002 (estimated). 1993 data is included for comparison.
The top three single agent/disease killers are HIV/AIDS, TB and malaria. While the number of deaths due to nearly every disease have decreased, deaths due to HIV/AIDS have increased fourfold. Childhood diseases include pertussis, poliomyelitis, diphtheria, measles and tetanus. Children also make up a large percentage of lower respiratory and diarrheal deaths.
Historic pandemics.
A pandemic (or global epidemic) is a disease that affects people over an extensive geographical area.
Emerging diseases.
In most cases, microorganisms live in harmony with their hosts via mutual or commensal interactions. Diseases can emerge when existing parasites become pathogenic or when new pathogenic parasites enter a new host.
Several human activities have led to the emergence of zoonotic human pathogens, including viruses, bacteria, protozoa, and rickettsia, and spread of vector-borne diseases, see also Globalization and Disease and Wildlife disease:
History.
Ideas of contagion became more popular in Europe during the Renaissance, particularly through the writing of the Italian physician Girolamo Fracastoro.
Anton van Leeuwenhoek (1632–1723) advanced the science of microscopy by being the first to observe microorganisms, allowing for easy visualization of bacteria.
In the mid-19th century John Snow and William Budd did important work demonstrating the contagiousness of typhoid and cholera through contaminated water. Both are credited with decreasing epidemics of cholera in their towns by implementing measures to prevent contamination of water.
Louis Pasteur proved beyond doubt that certain diseases are caused by infectious agents, and developed a vaccine for rabies.
Robert Koch, provided the study of infectious diseases with a scientific basis known as Koch's postulates.
Edward Jenner, Jonas Salk and Albert Sabin developed effective vaccines for smallpox and polio, which would later result in the eradication and near-eradication of these diseases, respectively.
Alexander Fleming discovered the world's first antibiotic Penicillin which Florey and Chain then developed.
Gerhard Domagk developed sulphonamides, the first broad spectrum synthetic antibacterial drugs.
Medical specialists.
The medical treatment of infectious diseases falls into the medical field of Infectiology and in some cases the study of propagation pertains to the field of Epidemiology. Generally, infections are initially diagnosed by primary care physicians or internal medicine specialists. For example, an "uncomplicated" pneumonia will generally be treated by the internist or the pulmonologist (lung physician).The work of the infectiologist therefore entails working with both patients and general practitioners, as well as laboratory scientists, immunologists, bacteriologists and other specialists.
An infectious disease team may be alerted when:
Society and culture.
A number of studies have reported associations between pathogen load in an area and human behavior. Higher pathogen load is associated with decreased size of ethnic and religious groups in an area. This may be due high pathogen load favoring avoidance other groups which may reduce pathogen transmission or a high pathogen load preventing the creation of large settlements and armies which enforce a common culture. Higher pathogen load is also associated with more restricted sexual behavior which may reduce pathogen transmission. It also associated with higher preferences for health and attractiveness in mates. Higher fertility rates and shorter or less parental care per child is another association which may be a compensation for the higher mortality rate. There is also an association with polygyny which may be due to higher pathogen load making selecting males with a high genetic resistance increasingly important. Higher pathogen load is also associated with more collectivism and less individualism which may limit contacts with outside groups and infections. There are alternative explanations for at least some of the associations although some of these explanations may in turn ultimately be due to pathogen load. Thus, polygny may also be due to a lower male:female ratio in these areas but this may ultimately be due to male infants having increased mortality from infectious diseases. Another example is that poor socioeconomic factors may ultimately in part be due to high pathogen load preventing economic development.
Fossil record.
Evidence of infection in fossil remains is a subject of interest for paleopathologists, scientists who study occurrences of injuries and illness in extinct life forms. Signs of infection have been discovered in the bones of carnivorous dinosaurs. When present, however, these infections seem to tend to be confined to only small regions of the body. A skull attributed to the early carnivorous dinosaur "Herrerasaurus ischigualastensis" exhibits pit-like wounds surrounded by swollen and porous bone. The unusual texture of the bone around the wounds suggests they were afflicted by a short-lived, non-lethal infection. Scientists who studied the skull speculated that the bite marks were received in a fight with another "Herrerasaurus". Other carnivorous dinosaurs with documented evidence of infection include "Acrocanthosaurus", "Allosaurus", "Tyrannosaurus" and a tyrannosaur from the Kirtland Formation. The infections from both tyrannosaurs were received by being bitten during a fight, like the "Herrerasaurus" specimen.

</doc>
<doc id="37221" url="http://en.wikipedia.org/wiki?curid=37221" title="North Coast Athletic Conference">
North Coast Athletic Conference

The North Coast Athletic Conference (NCAC) is an NCAA Division III athletic conference composed of colleges located in the Midwestern United States. When founded in 1984, the league was a pioneer in gender equality, offering competition in a then-unprecedented 10 women's sports. Today it remains true to that legacy, sponsoring 23 sports, 11 for men and 12 for women.
The NCAC is respected for the academic strength of its member institutions — all of which have Phi Beta Kappa chapters. In its most recent college rankings, "U.S. News & World Report" recognized all 10 members as top-tier liberal arts colleges, and ranked five NCAC institutions among the nation's top 70 such colleges.
History.
The formation of the NCAC is announced at joint news conferences in Cleveland, Columbus and Pittsburgh on February 1983. Allegheny College, Case Western Reserve University (CWRU), Denison University, Kenyon College, Oberlin College, Ohio Wesleyan University, and The College of Wooster were charter members in 1984, the same year that NCAC athletic conference play began.
In 1988, Earlham College and Wittenberg College accepted invitations to join the NCAC, pushing conference membership to nine schools in three states. The two schools would begin play in the fall of 1989. In 1998, Hiram College, and Wabash College accepted invitations to join the NCAC, pushing conference membership to 10 schools in three states, which both schools began play in the fall of 1999. Case Western Reserve, a charter member of the NCAC, announced that it would leave the NCAC following the 1998-99 academic year. The Spartans would compete on a full-time basis in the University Athletic Association (UAA) after more than a decade of joint conference membership affiliation.
Most recently, Earlham announced that it would depart the NCAC for the Heartland Collegiate Athletic Conference (HCAC), beginning with the 2010-11 season. DePauw University would become the 10th member of the NCAC, beginning in the 2011-12 season.
Member schools.
Current members.
The league currently has 10 full members:

</doc>
<doc id="37222" url="http://en.wikipedia.org/wiki?curid=37222" title="Philip Pullman">
Philip Pullman

Philip Pullman CBE, FRSL (born 19 October 1946) is a British writer. He is the author of several best-selling books, most notably the fantasy trilogy "His Dark Materials" and the fictionalised biography of Jesus, "The Good Man Jesus and the Scoundrel Christ". In 2008, "The Times" named Pullman one of the "50 greatest British writers since 1945".
"Northern Lights", the first book of the "His Dark Materials" trilogy, won the 1995 Carnegie Medal from the Library Association, recognising the year's outstanding English-language children's book.
For the 70th anniversary of the Medal it was named one of the top ten winning works by a panel, composing the ballot for a public election of the all-time favourite. 
"Northern Lights" won the public vote from that shortlist and was thus named the all-time "Carnegie of Carnegies" on 21 June 2007. It has been adapted as a film under its U.S. title, "The Golden Compass".
Life and career.
Philip Pullman was born in Norwich, England, the son of Audrey Evelyn Pullman (née Merrifield) and Royal Air Force pilot Alfred Outram Pullman. The family travelled with his father's job, including to Southern Rhodesia, though the majority of his formative years was spent in Llanbedr in Ardudwy, north Wales.
His father, an RAF pilot, was killed in a plane crash in 1954 in Kenya when Pullman was seven, being posthumously awarded the Distinguished Flying Cross (DFC). Pullman said at the beginning of a 2008 exchange that to him as a boy, his father "was a hero, steeped in glamour, killed in action defending his country" and had been "training pilots, I think." Pullman was then presented with a report from The London Gazette of 1954 "which carried the official RAF news of the day [and] said that the medal was given for 'gallant and distinguished service' during the Mau Mau uprising. 'The main task of the Harvards [the squadron of planes led by his father] has been bombing and machine-gunning Mau Mau and their hideouts in densely wooded and difficult country.' This included 'diving steeply into the gorges of [various] rivers, often in conditions of low cloud and driving rain.' Testing conditions, yes, but not much opposition from the enemy, the journalist in the exchange continued. Very few of the Mau Mau had guns that could land a blow on an aircraft." Pullman responded to this new information, writing "my father probably doesn't come out of this with very much credit, judged by the standards of modern liberal progressive thought" and accepted the new information as "a serious challenge to his childhood memory."
His mother remarried and, with a move to Australia, came Pullman's discovery of comic books including "Superman" and "Batman", a medium which he continues to espouse. From 1957 he was educated at Ysgol Ardudwy in Harlech, Gwynedd, and spent time in Norfolk with his grandfather, a clergyman. Around this time Pullman discovered John Milton's "Paradise Lost", which would become a major influence for "His Dark Materials".
From 1963, Pullman attended Exeter College, Oxford, receiving a Third class BA in 1968. In an interview with the "Oxford Student" he stated that he "did not really enjoy the English course" and that "I thought I was doing quite well until I came out with my third class degree and then I realised that I wasn’t — it was the year they stopped giving fourth class degrees otherwise I’d have got one of those". He discovered William Blake's illustrations around 1970, which would also later influence him greatly.
Pullman married Judith Speller in 1970 and began teaching children aged 9 to 13 at Bishop Kirk Middle School in Summertown, North Oxford and writing school plays. His first published work was "The Haunted Storm", which was joint-winner of the New English Library's Young Writer's Award in 1972. He nevertheless refuses to discuss it. "Galatea", an adult fantasy-fiction novel, followed in 1978, but it was his school plays which inspired his first children's book, "Count Karlstein", in 1982. He stopped teaching shortly after the publication of "The Ruby in the Smoke" (1986), his second children's book, which has a Victorian setting.
Pullman taught part-time at Westminster College, Oxford, between 1988 and 1996, continuing to write children's stories. He began "His Dark Materials" in about 1993. The first book, "Northern Lights" was published in 1995 (entitled "The Golden Compass" in the U.S., 1996). Pullman won both the annual Carnegie Medal and the Guardian Children's Fiction Prize, a similar award that authors may not win twice.
Pullman has been writing full-time since 1996, he continues to deliver talks and writes occasionally for "The Guardian", including writing and lecturing about education, where he is often critical of unimaginative education policies. He was awarded a CBE in the New Year's Honours list in 2004. He also co-judged the Christopher Tower Poetry Prize (awarded by Oxford University) in 2005 with Gillian Clarke. In 2004, he was elected President of the Blake Society. In 2004 Pullman also guest-edited "The Mays Anthology", a collection of new writing from students at the Universities of Oxford and Cambridge.
In 2005 Pullman won the annual Astrid Lindgren Memorial Award from the Swedish Arts Council, recognising his career contribution to "children's and young adult literature in the broadest sense". According to the presentation, "Pullman radically injects new life into fantasy by introducing a variety of alternative worlds and by allowing good and evil to become ambiguous." In every genre, "he combines storytelling and psychological insight of the highest order."
He was one of five finalists for the biennial, international Hans Christian Andersen Medal in 2006 and he was the British nominee again in 2012.
In 2008, he started working on "The Book of Dust", a companion book to his "His Dark Materials" trilogy, and "The Adventures of John Blake", a story for the British children's comic "The DFC", with artist John Aggs.
On 23 November 2007, Pullman was made an honorary professor at Bangor University. In June 2008, he became a Fellow supporting the MA in Creative Writing at Oxford Brookes University. In September 2008, he hosted "The Writer's Table" for Waterstone's bookshop chain, highlighting 40 books which have influenced his career. In October 2009, he became a patron of the Palestine Festival of Literature. He is also a patron of the Shakespeare Schools Festival, a charity that enables school children across the UK to perform Shakespeare in professional theatres
On 24 June 2009, Pullman was awarded the degree of D. Litt. (Doctor of Letters), "honoris causa", by the University of Oxford at the Encænia ceremony in the Sheldonian Theatre.
In 2012, during a break from writing "The Book Of Dust", a companion book to the Dark Material Trilogy, Pullman was asked by Penguin Classics to curate 50 of Grimms' classic fairytales, from their compendium of over 200 stories. "They are not all of the same quality," said Pullman, "Some are easily much better than others. And some are obvious classics. You can't do a selected Grimms' without Rumpelstiltskin, Cinderella and so on."
Beginning in August 2013, Pullman was elected President of the Society of Authors - the "ultimate honour" awarded by the British writers body, and a position first held by Alfred, Lord Tennyson.
"His Dark Materials".
"His Dark Materials" is a trilogy consisting of "Northern Lights" (titled "The Golden Compass" in North America), "The Subtle Knife" and "The Amber Spyglass". "Northern Lights" won the Carnegie Medal for children's fiction in the UK in 1995. "The Amber Spyglass" was awarded both 2001 Whitbread Prize for best children's book and the Whitbread Book of the Year prize in January 2002, the first children's book to receive that award. The series won popular acclaim in late 2003, taking third place in the BBC's Big Read poll. Pullman later wrote two companion pieces to the trilogy, entitled "Lyra's Oxford", and "Once Upon a Time in the North". A third companion piece Pullman refers to as the "green book" will expand upon his character Will. He has plans for one more, the as-yet-unpublished "The Book of Dust". This book is not a continuation of the trilogy but will include characters and events from "His Dark Materials".
Pullman has narrated unabridged audiobooks of the three main novels in His Dark Materials, the other parts are read by actors, including Jo Wyatt, Steven Webb, Peter England, Stephen Thorne and Douglas Blackwell.
In a discussion on fantasy as escapism, Pullman admitted he never reads fantasy as "it's not satisfying." He then went on to argue that he sees "His Dark Materials" as "stark realism", not fantasy.
Public campaigns.
Pullman has been a vocal campaigner on a number of book-related and political issues.
Age and gender labelling of books.
In 2008, Pullman led a campaign against the introduction of age bands on the covers of children’s books, saying: "It's based on a one-dimensional view of growth, which regards growing older as moving along a line like a monkey climbing a stick: now you're seven, so you read these books; and now you're nine so you read these". More than 1,200 authors, booksellers, illustrators, librarians and teachers joined the campaign; Pullman’s own publisher, Scholastic agreed to his request not to put the age bands on his book covers. Joel Rickett, deputy editor of The Bookseller, said: "The steps taken by Mr Pullman and other authors have taken the industry by surprise and I think these proposals are now in the balance".
In 2014, Pullman supported the Let Books Be Books campaign to stop children’s books being labelled as ‘for girls’ or ‘for boys’, saying: "I'm against anything, from age-ranging to pinking and blueing, whose effect is to shut the door in the face of children who might enjoy coming in. No publisher should announce on the cover of any book the sort of readers the book would prefer. Let the readers decide for themselves".
Civil liberties.
Pullman has a strong commitment to traditional British civil liberties and is noted for his criticism of growing state authority and government encroachment into everyday life. In February 2009, he was the keynote speaker at the Convention on Modern Liberty in London and wrote an extended piece in "The Times" condemning the Labour government for its attacks on basic civil rights. Later, he and other authors threatened to stop visiting schools in protest at new laws requiring them to be vetted to work with youngsters—though officials claimed that the laws had been misinterpreted.
Public jury.
In July 2011, Pullman was one of the lead campaigners signing a declaration which called for a 1,000-strong "public jury", selected at random, to draw up a "public interest first" test to ensure that power is taken away from "remote interest groups". The declaration was also signed by 56 academics, writers, trade unionists and politicians from the Labour party, the Liberal Democrats and the Green party.
Library closures.
In October 2011, Pullman backed a campaign to stop 600 library closures in England calling it a "war against stupidity". Of the London Borough of Brent’s claims that it was closing half of its libraries to fulfil its 'exciting plans’ to improve its library service Pullman said: "All the time, you see, the council had been longing to improve the library service, and the only thing standing in the way was – the libraries”.
Speaking at a conference organised by The Library Campaign and Voices for the Library he added: ”The book is second only to the wheel as the best piece of technology human beings have ever invented. A book symbolises the whole intellectual history of mankind; it's the greatest weapon ever devised in the war against stupidity. Beware of anyone who tries to make books harder to get at. And that is exactly what these closures are going to do – oh, not intentionally, except in a few cases; very few people are stupid intentionally; but that will be the effect. Books will be harder to get at. Stupidity will gain a little ground."
ebook library loans.
In advance of becoming president of the Society of Authors in August 2013, Pullman led a call for authors to be fairly paid for ebook library loans. Under arrangements in force at the time, authors were paid 6p per library loan by the government for physical books, but nothing for ebook loans. In addition, the Society found that publishers had possibly been inadvertently underpaying authors for ebook loans. Altogether, this may have resulted in authors losing up to two-thirds of the income they would have received on the sale and loan of a physical book. Addressing this issue, Pullman said: "New media and new forms of buying and lending are all very interesting, for all kinds of reasons, but one principle remains unchanged: authors must be paid fairly for their work. Any arrangement that doesn't acknowledge that principle is a bad one, and needs to be changed. That is our whole argument".
William Blake’s cottage.
As a long-time enthusiast of William Blake, and president of the Blake Society, Pullman led a campaign in winter 2014 to buy the Sussex cottage where the poet lived between 1800 and 1803, saying: “Surely it isn’t beyond the resources of a nation that can spend enormous amounts of money on acts of folly and unnecessary warfare, a nation that likes to boast about its literary heritage, to find the money to pay for a proper memorial and a centre for the study of this great poet and artist. Not least because this is the place where he wrote the words now often sung as an alternative (and better) national anthem, the poem known as Jerusalem: ‘And did those feet in ancient time’. Blake’s feet walked in Felpham. Let’s not let this opportunity pass by”.
Perspective on religion.
Pullman is a supporter of the British Humanist Association and an Honorary Associate of the National Secular Society. "New Yorker" journalist Laura Miller has described Pullman as one of England's most outspoken atheists. Pullman has also referred to himself as knowingly "of the Devil's party", a reference to William Blake's revisionist take on Milton in "The Marriage of Heaven and Hell". On 15 September 2010, Pullman along with 54 other public figures (including Stephen Fry, Professor Richard Dawkins, Terry Pratchett, Jonathan Miller and Ken Follett) signed an open letter, published in "The Guardian" newspaper, stating their opposition to Pope Benedict XVI being given "the honour of a state visit" to the UK; the letter argued that the Pope has led and condoned global abuses of human rights, leading a state which has "resisted signing many major human rights treaties and has formed its own treaties ("concordats") with many states which negatively affect the human rights of citizens of those states".
Literary critic Alan Jacobs (of Wheaton College) said that in "His Dark Materials" Pullman replaced the theist world-view of John Milton's "Paradise Lost" with a Rousseauist one. The books in the series have been criticised for their attitude to religion, especially Catholicism, by the Catholic League for Religious and Civil Rights and Focus on the Family. However, Donna Freitas, professor of religion at Boston University, argued on BeliefNet.com that challenges to traditional images of God should be welcomed as part of a "lively dialogue about faith", and Rowan Williams, the former Archbishop of Canterbury, proposed that "His Dark Materials" be taught as part of religious education in schools. The Christian writers Kurt Bruner and Jim Ware "also uncover spiritual themes within the books". Pullman's contribution to the Canongate Myth Series, "The Good Man Jesus and the Scoundrel Christ", was described as "a far more direct exploration of the foundations of Christianity and the church as well as an examination of the fascination and power of storytelling" by Mike Collett-White.
Writing in the "Catholic Herald" in 1999, Leonie Caldecott cited Pullman's work as an example of fiction "far more worthy of the bonfire than Harry [Potter]" on the grounds that "[by] co-opting Catholic terminology and playing with Judaeo-Christian theological concepts, Pullman is effectively removing, among a mass audience of a highly impressionable age, some of the building blocks for future evangelisation". Pullman was "flattered" and asked his publisher to include quotes from Caldecott's article in his next book. In 2002, the "Catholic Herald" published an article by Sarah Johnson that compared Pullman to a "playground bully" whose work "attacks a religious minority". The following year, after Benedict Allen's reference to the criticism during the BBC TV series The Big Read, the "Catholic Herald" republished both articles and Caldecott claimed her "bonfire" comment was a joke and accused Pullman and his supporters of quoting her out of context. In a longer article for Touchstone Magazine earlier in 2003, Caldecott had also described Pullman's work as "axe-grinding" and "a kind of Luciferian enterprise".
Peter Hitchens has also argued that Pullman actively pursues an anti-Christian agenda, citing an interview in which Pullman is quoted as saying: "I'm trying to undermine the basis of Christian belief." In the same interview, Pullman acknowledges that a controversy would be likely to boost sales, but continues "I'm not in the business of offending people. I find the books upholding certain values that I think are important, such as life is immensely valuable and this world is an extraordinarily beautiful place. We should do what we can to increase the amount of wisdom in the world." Peter Hitchens also views the "His Dark Materials" series as a direct rebuttal of C. S. Lewis's "The Chronicles of Narnia"; Pullman has criticized the Narnia books as religious propaganda. Peter Hitchens' brother Christopher Hitchens, author of "God Is Not Great", praised "His Dark Materials" as a fresh alternative to Lewis, J. R. R. Tolkien and J. K. Rowling, describing the author as one "whose books have begun to dissolve the frontier between adult and juvenile fiction". However, he was more critical of "The Good Man Jesus and the Scoundrel Christ", accusing Pullman of being a "Protestant atheist" for supporting the teachings of Christ but being critical of organised religion.
Pullman has, however, found support from some Christians, most notably from Rowan Williams, the former Archbishop of Canterbury (spiritual head of the Anglican church), who argues that Pullman's attacks focus on the constraints and dangers of dogmatism and the use of religion to oppress, not on Christianity itself.
Williams has also recommended the "His Dark Materials" series of books for inclusion and discussion in Religious Education classes, and stated that "To see large school-parties in the audience of the Pullman plays at the National Theatre is vastly encouraging". Pullman and Williams took part in a National Theatre platform debate a few days later to discuss myth, religious experience and its representation in the arts.
Pullman has singled out certain elements of Christianity for criticism, as in the following: "I suppose technically, you'd have to put me down as an agnostic. But if there is a God, and he is as the Christians describe him, then he deserves to be put down and rebelled against". However, Pullman has also said in interviews and appearances that his argument can extend to all religions.

</doc>
<doc id="37223" url="http://en.wikipedia.org/wiki?curid=37223" title="His Dark Materials">
His Dark Materials

His Dark Materials is an epic trilogy of fantasy novels by Philip Pullman consisting of "Northern Lights" (1995, published as "The Golden Compass" in North America), "The Subtle Knife" (1997), and "The Amber Spyglass" (2000). It follows the coming of age of two children, Lyra Belacqua and Will Parry, as they wander through a series of parallel universes. The three novels have won a number of awards, most notably the 2001 Whitbread Book of the Year prize, won by "The Amber Spyglass". "Northern Lights" won the Carnegie Medal for children's fiction in the UK in 1995. The trilogy took third place in the BBC's Big Read poll in 2003.
The fantasy elements include witches and armoured polar bears, but the trilogy also alludes to ideas from physics, philosophy and theology. The trilogy functions in part as a retelling and inversion of John Milton's epic "Paradise Lost", with Pullman commending humanity for what Milton saw as its most tragic failing, original sin. The series has drawn criticism for its negative portrayal of Christianity and religion in general.
Pullman's publishers have primarily marketed the series to young adults, but Pullman also intended to speak to both older children and adults. North American printings of "The Amber Spyglass" have censored passages describing Lyra's incipient sexuality.
Pullman has published two short stories related to "His Dark Materials": "Lyra and the Birds", which appears with accompanying illustrations in the small hardcover book "Lyra's Oxford" (2003), and "Once Upon a Time in the North" (2008). He has been working[ [update]] on another, larger companion book to the series, "The Book of Dust", for several years.
The National Theatre in London staged a major, two-part adaptation of the series in 2003–2004, and New Line Cinema released a film based on "Northern Lights", titled "The Golden Compass", in 2007.
Settings.
The trilogy takes place across a multiverse, moving between many parallel worlds. In "Northern Lights", the story takes place in a world with some similarities to our own; dress-style resembles that of the UK's Victorian era, and technology has not evolved to include automobiles or fixed-wing aircraft, while zeppelins feature as a notable mode of transport.
The dominant religion has parallels with Christianity, and is at certain points in the series "(especially in the later books)" explicitly named so; while Adam and Eve are referenced in the text (particularly in "The Subtle Knife", in which Dust tells Mary Malone that Lyra Belacqua is a new Eve to whom she is to be the serpent), Jesus Christ is not. The Church "(called the "Magisterium", the same name as the Catholic body)" exerts a strong control over society and has some of the appearance and organisation of the Catholic Church, but one in which the centre of power had moved from Rome to Geneva, moved there by Pullman's fictional "Pope John Calvin" (Geneva was the home of the real, historical John Calvin).
In "The Subtle Knife", the story moves between the world of the first novel, our own world, and in another world, a city called Cittàgazze. In "The Amber Spyglass" the story crosses through an array of diverse worlds.
At first glance, the universe of "Northern Lights" appears considerably behind that of our own world (resembling an industrial society between the late 19th century and the outbreak of the First World War), but in many fields it equals or surpasses ours. For instance, it emerges that Lyra's world has the same knowledge of particle physics, referred to as "experimental theology", that we do. In "The Amber Spyglass", discussion takes place about an advanced inter-dimensional weapon which, when aimed using a sample of the target's DNA, can track the target to any universe and disrupt the very fabric of space-time to form a bottomless abyss into nothing, forcing the target to suffer a fate far worse than normal death. Other advanced devices include the Intention Craft, which carries (amongst other things) an extremely potent energy-weapon, though this craft, first seen and used outside Lyra's universe, may originate in the work of engineers from other universes.
Series.
Titles.
The title of the series, "His Dark Materials", comes from 17th century poet John Milton's "Paradise Lost", Book 2:
<poem>
Into this wilde Abyss,
The Womb of nature and perhaps her Grave,
Of neither Sea, nor Shore, nor Air, nor Fire,
But all these in their pregnant causes mixt 
Confus'dly, and which thus must ever fight,
Unless th' Almighty Maker them ordain
His dark materials to create more Worlds,
Into this wilde Abyss the warie fiend
Stood on the brink of Hell and look'd a while,
Pondering his Voyage; for no narrow frith
He had to cross.
</poem>
— Paradise Lost, Book 2, lines 910–920
Pullman earlier proposed to name the series "The Golden Compasses", also a reference to "Paradise Lost", where they denote God's circle-drawing instrument used to establish and set the bounds of all creation:
<poem>
Then staid the fervid wheels, and in his hand
He took the golden compasses, prepared
In God's eternal store, to circumscribe
This universe, and all created things:
One foot he centered, and the other turned
Round through the vast profundity obscure
</poem>
— Paradise Lost, Book 7, lines 224–229
Despite the confusion with the other common meaning of "compass" (the navigational instrument) "The Golden Compass" became the title of the American edition of "Northern Lights" (the book features an 'alethiometer', a device that one might label a "golden compass"). In "The Subtle Knife" Pullman rationalizes the first book's American title, by having Mary twice refer to Lyra's alethiometer as a "compass" or "compass thing."
"Northern Lights" (or "The Golden Compass)".
"Northern Lights" (published in some countries as "The Golden Compass" revolves around Lyra Belacqua, a young girl who lives in a world in which humans are constantly accompanied by dæmons: the animal embodiments of their inner-selves. Dæmons alter their forms frequently when people are young but begin to settle into a fixed, animal form when children reach puberty. Lyra, whose dæmon is named Pantalaimon, is brought up in the cloistered world of Jordan College, Oxford, where she accidentally learns of the existence of Dust — a strange elementary particle being researched by Lord Asriel, whom Lyra has been told is her uncle. The Magisterium is a powerful Church body that represses heresy, and believes Dust to be related to Original Sin. Dust is less attracted to children than to adults, and a desire to learn why and to prevent children from acquiring Dust when they become adults leads to grisly experiments, carried out to separate kidnapped children from their dæmons. The experiments are directed by Mrs. Coulter and conducted in the distant North by experimental "theologists" (scientists) of the Magisterium. The Master of Jordan College, who has been raising Lyra, turns her over to Mrs. Coulter under pressure from the Church. But first he gives Lyra the alethiometer, an instrument that uses Dust to reveal any truth and can answer any question when properly manipulated. Lyra, initially excited at being placed in the care of the elegant and mysterious Mrs. Coulter, discovers to her horror that Coulter heads the secretive General Oblation Board, who are rumoured to be the ones kidnapping children throughout England for experimentation; they are known among children as the "Gobblers" (from the initials of General Oblation Board). Learning of Mrs. Coulter's Gobbler activity, Lyra runs away and the Gyptians, who live on riverboats, rescue her from pursuers. From them she learns that Mrs. Coulter is her mother and Lord Asriel is her father, not her uncle. Taking Lyra along, the Gyptians mount an expedition to rescue the missing children, many of whom are Gyptian children. Lyra hopes to find and save her best friend, Roger Parslow, who she suspects has been taken by the Gobblers. Aided by the exiled armoured bear Iorek Byrnison and a clan of witches, the Gyptians save the kidnapped children, including Roger. Lyra and Iorek, along with the balloonist Lee Scoresby, next continue on to Svalbard, home of the armoured bears. There Lyra helps Iorek regain his kingdom by killing his rival, King Iofur Raknison. Lyra then carries on to find Lord Asriel, exiled to Svalbard at Mrs. Coulter's request. She mistakenly thinks her mission all along has been to bring Asriel her alethiometer, when in fact she was destined to bring him a child, Roger. Lord Asriel has been developing a means of building a bridge to another world that can be seen in the sky through the northern lights. The bridge requires a vast amount of energy to split open the boundary between the two worlds. Asriel acquires the energy by severing Roger from his dæmon, killing Roger in the process. Lyra arrives too late to save Roger. Asriel then travels across the bridge to the new world in order to find the source of Dust. Lyra and Pantalaimon follow Asriel into the new world.
"The Subtle Knife".
In "The Subtle Knife", Lyra journeys through the Aurora to Cittàgazze, an otherworldly city whose denizens have discovered a clean path between worlds at a far earlier point in time than others in the storyline. Cittàgazze's reckless use of the technology has released soul-eating Spectres, to which children are immune, rendering much of the world incapable of transit by adults. Here Lyra meets Will Parry, a twelve-year-old boy from our world. Will, who recently killed a man to protect his ailing mother, has stumbled into Cittàgazze in an effort to locate his long-lost father. Will becomes the bearer of the eponymous Subtle Knife, a tool forged 300 years ago by Cittàgazze's scientists from the same materials used to make Bolvangar's silver guillotine. One edge of the knife can divide even subatomic particles and form subtle divisions in space, creating portals between worlds; the other edge easily cuts through any form of matter. After meeting with witches from Lyra's world, they journey on. Will finds his father, who had gone missing in Lyra's world under the assumed name of Stanislaus Grumman, only to watch him murdered almost immediately by a witch who loved him but was turned down, and Lyra is kidnapped.
"The Amber Spyglass".
"The Amber Spyglass" tells of Lyra's kidnapping by her mother, Mrs. Coulter, an agent of the Magisterium who has learned of the prophecy identifying Lyra as the next Eve. A pair of angels, Balthamos and Baruch, inform Will that he must travel with them to give the Subtle Knife to Lyra's father, Lord Asriel, as a weapon against The Authority. Will ignores the angels; with the help of a local girl named Ama, the Bear King Iorek Byrnison, and Lord Asriel's Gallivespian spies, the Chevalier Tialys and the Lady Salmakia, he rescues Lyra from the cave where her mother has hidden her from the Magisterium, which has become determined to kill her before she yields to temptation and sin like the original Eve.
Will, Lyra, Tialys and Salmakia journey to the Land of the Dead, temporarily parting with their dæmons to release the ghosts from their captivity. Mary Malone, a scientist from Will's world interested in "shadows" (or Dust in Lyra's world), travels to a land populated by strange sentient creatures called Mulefa. There she comes to understand the true nature of Dust, which is both created by and nourishes life which has become self-aware. Lord Asriel and the reformed Mrs. Coulter work to destroy the Authority's Regent Metatron. They succeed, but themselves suffer annihilation in the process by pulling Metatron into the abyss. The Authority himself dies of his own frailty when Will and Lyra free him from the crystal prison wherein Metatron had trapped him, able to do so because an attack by cliff-ghasts kills or drives away the prison's protectors. When Will and Lyra emerge from the land of the dead, they find their dæmons. The book ends with Will and Lyra falling in love but realising they cannot live together in the same world, because all windows — except one from the underworld to the world of the Mulefa — must be closed to prevent the loss of Dust, and because each of them can only live full lives in their native worlds. This is the temptation that Mary was meant to give them; to help them fall in love and then choose whether they should stay together or not. During the return, Mary learns how to see her own dæmon, who takes the form of a black Alpine Chough. Lyra loses her ability to intuitively read the alethiometer and determines to learn how to use her conscious mind to achieve the same effect.
Related works by Philip Pullman.
"Lyra's Oxford".
The first of two short novels, "Lyra's Oxford" takes place two years after the timeline of "The Amber Spyglass". A witch who seeks revenge for her son's death in the war against the Authority draws Lyra, now 15, into a trap. Birds mysteriously rescue her and Pan, and she makes the acquaintance of an alchemist, formerly the witch's lover.
"Once Upon a Time in the North".
This short novel serves as a prequel to "His Dark Materials" and focuses on the 24-year-old Texan aeronaut Lee Scoresby. After winning his hot-air balloon, Scoresby heads to the North, landing on the Arctic island Novy Odense, where he finds himself pulled into a dangerous conflict between the oil-tycoon Larsen Manganese, the corrupt mayoral candidate Ivan Poliakov, and his longtime enemy from the Dakota Country, Pierre McConville. The story tells of Lee and Iorek's first meeting and of how they overcame these enemies.
"The Book of Dust".
The forthcoming companion to the trilogy, "The Book of Dust" will not continue the story, but was originally said to offer several short stories with the same characters, world, etc. Later, however, it was said it would be about Lyra when she is older, about 2 years after Lyra's Oxford, when she will go on a new adventure and learn to read the alethiometer again. The book will touch on research into Dust as well as on the portrayal of religion in "His Dark Materials". Pullman has not yet[ [update]] finished writing this work.
Future books.
Pullman has also told of his hope to publish a small green book about Will:
"Lyra's Oxford" was a dark red book. "Once Upon a Time in the North" will be a dark blue book. There still remains a green book. And that will be Will's book. Eventually...—Philip Pullman
Pullman confirmed this in an interview with two fans in August 2007.
Characters.
All humans in Lyra's world, including witches, have a Dæmon. It is a the physical manifestation of a person's 'inner being', soul or spirit. It takes the form of a creature (moth, bird, dog, monkey, snake, etc.) and is usually the opposite sex to its human counterpart. The dæmons of children have the ability to change form - from one creature to another - but towards the end of a child's puberty, their dæmon "settles" into a permanent form, which reflects the person's personality. When a person dies, the dæmon dies too. Armoured bears, cliff ghasts and other creatures do not have dæmons. An armoured bear's armour is his soul.
Dæmons.
One distinctive aspect of Pullman's story is the presence of "dæmons" (pronounced "demon"). In the birth-universe of the story's protagonist Lyra Belacqua, a human individual's inner-self manifests itself throughout life as an animal-shaped "dæmon", that almost always stays near its human counterpart.
Dæmons usually only talk to their own associated humans, but they can communicate with other humans and with other dæmons autonomously. During the childhood of its associated human, a dæmon can change its shape at will, but with the onset of adolescence it settles into a fixed, final form that reveals the person's true nature and personality. In Lyra's world, it is considered to be "the grossest breach of etiquette imaginable" for one person to touch another's dæmon — this violates the strictest of taboos. "A human being with no dæmon is like someone without a face, or with their ribs laid open and their heart torn out: something unnatural and uncanny that belonged to the world of night-ghasts, not the waking world of sense."
Dæmons and their humans can become separated through intercision, a process involving cutting the link between the dæmon and the human. This process can take place in a medical setting, as with the guillotine used at Bolvangar, or as a form of torture used by the Skraelings. This separation entails a high mortality rate and changes both human and dæmon into a zombie-like state. Severing the link using the silver guillotine method releases tremendous amounts of unnamed energy, convertible to anbaric (electric) power.
Influences.
Pullman has identified three major literary influences on "His Dark Materials": the essay "On the Marionette Theatre" by Heinrich von Kleist, the works of William Blake, and, most important, John Milton's "Paradise Lost", from which the trilogy derives its title. In his introduction, he adapts a famous description of Milton by Blake to quip that he (Pullman) "is of the Devil's party and "does" know it."
Critics have compared the trilogy with "The Chronicles of Narnia", by C. S. Lewis, Pullman however has characterised the "Narnia" series as "blatantly racist", "monumentally disparaging of women", "immoral", and "evil". The trilogy has also been compared with such fantasy books as "Bridge to Terabithia" by Katherine Paterson and "A Wrinkle in Time" by Madeleine L'Engle
Awards and recognition.
"The Amber Spyglass" won the 2001 Whitbread Book of the Year award, a prestigious British literary award. This is the first time that such an award has been bestowed on a book from their "children's literature" category.
The first volume, "Northern Lights", won the Carnegie Medal for children's fiction in the UK in 1995. In 2007, the judges of the CILIP Carnegie Medal for children's literature selected it as one of the ten most important children's novels of the previous 70 years. In June 2007 it was voted, in an online poll, as the best Carnegie Medal winner in the seventy-year history of the award, the Carnegie of Carnegies.
"The Observer" cites "Northern Lights" as one of the 100 best novels.
On 19 May 2005, Pullman attended the British Library in London to receive formal congratulations for his work from culture secretary Tessa Jowell "on behalf of the government".
On 25 May 2005, Pullman received the Swedish government's Astrid Lindgren Memorial Award for children's and youth literature (sharing it with Japanese illustrator Ryōji Arai). Swedes regard this prize as second only to the Nobel Prize in Literature; it has a value of 5 million Swedish Kronor or approximately £385,000.
The trilogy came third in the 2003 BBC's "Big Read", a national poll of viewers' favourite books, after "The Lord of the Rings" and "Pride and Prejudice". At the time, only "His Dark Materials" and "Harry Potter and the Goblet of Fire" amongst the top five works lacked a screen-adaptation (the film version of "Harry Potter and the Goblet of Fire", which came fifth, was released in 2005).
Controversies.
"His Dark Materials" has occasioned controversy, primarily amongst some Christian groups.
Pullman has expressed surprise over what he perceives as a low level of criticism for "His Dark Materials" on religious grounds, saying "I've been surprised by how little criticism I've got. Harry Potter's been taking all the flak... Meanwhile, I've been flying under the radar, saying things that are far more subversive than anything poor old Harry has said. My books are about killing God".
Some of the characters criticise institutional religion. Ruta Skadi, a witch and friend of Lyra's calling for war against the Magisterium in Lyra's world, says that ""For all of [the Church's] history... it's tried to suppress and control every natural impulse. And when it can't control them, it cuts them out". Skadi later extends her criticism to all organised religion: "That's what the Church does, and every church is the same: control, destroy, obliterate every good feeling". By this part of the book, the witches have made reference to how they are treated criminally by the church in their worlds. Mary Malone, one of Pullman's main characters, states that "the Christian religion... is a very powerful and convincing mistake, that's all"". Formerly a Catholic nun, she gave up her vows when the experience of falling in love caused her to doubt her faith. Pullman has warned, however, against equating these views with his own, saying of Malone: "Mary is a character in a book. Mary's not me. It's a story, not a treatise, not a sermon or a work of philosophy". In another inversion, the tenet that the Church can absolve a penitent of sin is subverted when the priest selected to assassinate Lyra has built up sufficient penitential credit "before" attempting to carry out this sin for the Church.
Pullman portrays life after death very differently from the Christian concept of heaven: In the third book, the afterlife plays out in a bleak underworld, similar to the Greek vision of the afterlife, wherein harpies torment people until Lyra and Will descend into the land of the dead. At their intercession, the harpies agree to stop tormenting the dead souls, and instead receive the true stories of the dead in exchange for leading them again to the upper world. When the dead souls emerge, they dissolve into atoms and merge with the environment.
Pullman's "Authority", though worshipped on Lyra's earth as God, emerges as the first conscious creature to evolve. Pullman makes it explicit that the Authority did not create worlds, and his trilogy does not speculate on who or what (if anything) might have done so. Members of the Church are typically displayed as zealots.
Cynthia Grenier, in the "Catholic Culture", said: "In the world of Pullman, God Himself (the Authority) is a merciless tyrant". His Church is an instrument of oppression, and true heroism consists of overthrowing both."
William A. Donohue of the Catholic League has described Pullman's trilogy as "atheism for kids". Pullman has said of Donohue's call for a boycott, "Why don't we trust readers? [...] Oh, it causes me to shake my head with sorrow that such nitwits could be loose in the world".
Pullman has, however, found support from some other Christians, most notably from Rowan Williams, the former Archbishop of Canterbury (spiritual head of the Anglican church), who argues that Pullman's attacks focus on the constraints and dangers of dogmatism and the use of religion to oppress, not on Christianity itself.
Williams has also recommended the "His Dark Materials" series of books for inclusion and discussion in Religious Education classes, and stated that "To see large school-parties in the audience of the Pullman plays at the National Theatre is vastly encouraging". Pullman and Williams took part in a National Theatre platform debate a few days later to discuss myth, religious experience and its representation in the arts.
Pullman has singled out certain elements of Christianity for criticism, as in the following: "I suppose technically, you'd have to put me down as an agnostic. But if there is a God, and he is as the Christians describe him, then he deserves to be put down and rebelled against". However, Pullman has also said in interviews and appearances that his argument can extend to all religions.
In a November 2002 interview Pullman was asked to respond to the fact that the Catholic Herald had called his books "the stuff of nightmares" and "worthy of the bonfire". He replied, "My response to that was to ask the publishers to print it in the next book, which they did! I think it's comical, it's just laughable". The original remark in Catholic Herald "(which was "there are numerous candidates that seem to me to be far more worthy of the bonfire than Harry Potter")" was written in the context of parents in South Carolina pressing their Board of Education to ban the Harry Potter books.
Terminology used in the books.
To enhance the feeling of being in parallel universes, Pullman renames various common objects or ideas of our world with archaic terms or new words of his own. The names he chooses often follow plausible alternative etymologies to those which have prevailed in modern English, thus making it possible to guess what everyday object or person he is referring to. Below are some of the significant renamings as well as new words the author has developed.
  List of terms
Pullman underlines the differences between the history of Lyra's world and ours by using archaic or adapted names for otherwise familiar peoples, regions and places.
  List of renamings of peoples and places
Unless stated otherwise, these words are all capitalised.
Pronunciation.
The pronunciations given here are those used in the radio plays and the audio book readings of the trilogy "(narrated by Pullman himself)".
  List of pronunciations
Adaptations.
"His Dark Materials" has been adapted for radio, theatre and film, In addition there have been unabridged audio books of the three main novels in "His Dark Materials" on which Philip Pullman himself is the narrator, the other parts are read by various actors, including Jo Wyatt, Steven Webb, Peter England, Stephen Thorne and Douglas Blackwell.
Radio.
The BBC made "His Dark Materials" into a radio drama on BBC Radio 4 starring Terence Stamp as Lord Asriel and Lulu Popplewell as Lyra. The play was broadcast in 2003 and is now published by the BBC on CD and cassette. In the same year, a radio drama of "Northern Lights" was made by RTÉ (Irish public radio).
The BBC Radio 4 version of "His Dark Materials" was repeated on BBC Radio 7 between 7 December 2008 to 11 January 2009. With 3 episodes in total, each episode was 2.5 hours long.
Theatre.
Nicholas Hytner directed a theatrical version of the books as a two-part, six-hour performance for London's Royal National Theatre in December 2003, running until March 2004. It starred Anna Maxwell-Martin as Lyra, Dominic Cooper as Will, Timothy Dalton as Lord Asriel and Patricia Hodge as Mrs Coulter with dæmon puppets designed by Michael Curry. The play was enormously successful and was revived (with a different cast and a revised script) for a second run between November 2004 and April 2005. It has since been staged by several other theatres in the UK and elsewhere.
A new production was staged at in March and April 2009, directed by Rachel Kavanaugh and Sarah Esdaile and starring Amy McAllister as Lyra. This version toured the UK and included a performance in Philip Pullman's hometown of Oxford. Philip Pullman made a cameo appearance much to the delight of the audience and Oxford media. The production finished up at West Yorkshire Playhouse in June 2009.
Film.
New Line Cinema released a film adaptation, titled "The Golden Compass", on 7 December 2007. Directed by Chris Weitz, the production had a mixed reception, and though worldwide sales were strong, its U.S. earnings were not as high as the studio had hoped.
The filmmakers obscured the explicitly Biblical character of the Authority to avoid offending viewers. Weitz declared that he would not do the same for the planned sequels. "Whereas "The Golden Compass" had to be introduced to the public carefully", he said, "the religious themes in the second and third books can't be minimised without destroying the spirit of these books. ...I will not be involved with any 'watering down' of books two and three, since what I have been working towards the whole time in the first film is to be able to deliver on the second and third". In May 2006, Pullman said of a version of the script that "all the important scenes are there and will have their full value"; in March 2008, he said of the finished film that "a lot of things about it were good... Nothing can bring out all that's in the book. There are always compromises".
"The Golden Compass" film stars Dakota Blue Richards as Lyra, Nicole Kidman as Mrs. Coulter, and Daniel Craig as Lord Asriel. Eva Green plays Serafina Pekkala, Ian McKellen voices Iorek Byrnison, and Freddie Highmore voices Pantalaimon.
No sequels are planned yet. Much publicity was given to "Compass" actor's Sam Elliott blaming Catholic Church opposition for forcing their cancellation, but UK Guardian film critic Stuart Heritage thinks critical "disappointment" with the first film may have been the real reason.

</doc>
<doc id="37225" url="http://en.wikipedia.org/wiki?curid=37225" title="Brabançonne">
Brabançonne

The Brabançonne is the national anthem of Belgium. In the originally French language, the term normally refers to Brabant, literally "Brabantian" in English. The untranslated initial name is maintained for the French, Dutch and the German lyrics, that at a later stage ensured reflecting all three official languages of the country.
History.
According to legend, the Belgian national anthem was written in September 1830, during the Belgian Revolution, by a young revolutionary called "Jenneval", who read the lyrics during a meeting at the "Aigle d'Or" café.
Jenneval, a Frenchman whose real name was Alexandre Dechet (sometimes known as Louis-Alexandre Dechet), did in fact write the Brabançonne. At the time, he was an actor at the theatre where, in August 1830, the revolution started which led to independence from the Netherlands. Jenneval died in the war of independence. François Van Campenhout composed the accompanying score, based on the tune of a French song called "L'Air des lanciers polonais" ("the tune of the Polish Lancers"), written by the French poet Eugène de Pradel, whose tune was itself an adaptation of the tune of a song, "L'Air du magistrat irréprochable", found in a popular collection of drinking songs called "La Clé du caveau" ("The Key to the cellar") and it was first performed in September 1830.
In 1860, Belgium formally adopted the song and music as its national anthem, although the then prime minister, Charles Rogier edited out lyrics attacking the Dutch Prince of Orange.
The ending, pledging loyalty to "Le Roi, la Loi, la Liberté!" ("The King, and Law, and Liberty!") is an obvious parallel to the French "Liberté, Égalité, Fraternité" - with the republican sentiment of the original replaced in the Belgian version by the promotion of constitutional monarchy (the combination of "The King" and "(the) Law" is what produces "Liberty"). Actually, a slogan similar to the Belgian one - "la Nation, la Loi, le Roi" ("The Nation, The Law, The King") - had been used in the early days of the French Revolution, when that revolution was still considered to be aimed toward constitutional monarchy rather than a republic.
The Brabançonne is also a monument (1930) by the sculptor Charles Samuel on the Surlet de Chokier square in Brussels. The monument contains partial lyrics of both the French and Dutch versions of the anthem. Like many elements in Belgian folklore, this is mainly based on the French "La Marseillaise" which is also both an anthem and the name of a monument - the sculptural group "Departure of the Volunteers of 1792", commonly called "La Marseillaise", at the base of the Arc de Triomphe in Paris.
Lyrics.
Current version.
Various committees were charged with reviewing the text and tune of the Brabançonne and establishing an official version. A ministerial circular of the Ministry of the Interior on August 8, 1921, decreed that only the fourth verse of the text by Charles Rogier should be considered official for all three, French, German and in Dutch. Here below:
Modern short trilingual version.
In recent years, an unofficial short version of the anthem is sung during Belgian National Day on July 21 yearly, combining the words of the anthem in all three of Belgium's official languages, similar to the bilingual version of O Canada.

</doc>
<doc id="37229" url="http://en.wikipedia.org/wiki?curid=37229" title="Jan Kjærstad">
Jan Kjærstad

Jan Kjærstad (born 6 March 1953 in Oslo) is a Norwegian author. Kjærstad is a theology graduate from MF Norwegian School of Theology and the University of Oslo (cand. theol.). He has written a string of novels, short stories and essays and was editor of the literary magazine "Vinduet" ("The Window"). He has received a number of prizes, the most important being the Nordic Council Literature Prize, which he received for the perspectivist trilogy about the TV personality Jonas Wergeland ("The Seducer", "The Conqueror" and "The Discoverer").
His books have been translated to English, French, German, Danish, Swedish, and Hungarian, among others.

</doc>
<doc id="37231" url="http://en.wikipedia.org/wiki?curid=37231" title="13 (number)">
13 (number)

13 (thirteen ) is the natural number following 12 and preceding 14.
In spoken English, the numbers 13 and 30 are often confused. When carefully enunciated, they differ in which syllable is stressed: 13 vs. 30 . However, in dates such as 1300 ("thirteen hundred") or when contrasting numbers in the teens, such as "13, 14, 15," the stress shifts to the first syllable: 13 .
Strikingly similar folkloric aspects of the number 13 have been noted in various cultures around the world: one theory is that this is due to the cultures employing lunar-solar calendars (there are approximately 12.41 lunations per solar year, and hence 12 "true months" plus a smaller, and often portentous, thirteenth month). This can be witnessed, for example, in the "Twelve Days of Christmas" of Western European tradition.
In mathematics.
The number 13 is the sixth prime number, and the smallest emirp (a prime that is a different prime when reversed). It is also a Fibonacci number, a happy number, the third centered square number, and one of only 3 known Wilson primes.
Since 52 + 122 = 132, (5, 12, 13) forms a Pythagorean triple.
There are 13 Archimedean solids, and a standard torus can be sliced into 13 pieces with just 3 plane cuts. There are also 13 different ways for the three fastest horses in a horse race to finish, allowing for ties, a fact that can be expressed mathematically by 13 being the third ordered Bell number.
Spelling.
In Germany, according to an old tradition, 13 ("dreizehn") as the first compound number was the first number written in digits; the numbers 0 ("null") through 12 ("zwölf") were spelled out. The "Duden" (the German standard dictionary) now calls this tradition ("which was actually never written down as an official rule") outdated and no longer valid, but many writers still follow it.
For the English language, different systems are used: Sometimes it is recommended to spell out numbers up to and including "nine" or "ten" or "twelve", like formerly in German, or even "ninety-nine" or "one hundred". Another system spells out all numbers written in one or two words ("sixteen", "twenty-seven", "fifteen thousand", but "372" or "15,001" ).
In religion.
Roman Catholicism.
The apparitions of the Virgin of Fátima in 1917 were claimed to occur on the 13th day of six consecutive months.
In Catholic devotional practice, the number thirteen is also associated with Saint Anthony of Padua, since his feast day falls on June 13. A traditional devotion called the Thirteen Tuesdays of St. Anthony involves praying for the saint every Tuesday over a period of thirteen weeks. Another devotion, St. Anthony's Chaplet, consists of thirteen decades of three beads each.
Sikhism.
According to famous Sakhi (Evidence) or story of Guru Nanak Dev Ji, when he was an accountant at a town of Sultanpur Lodhi, he was distributing groceries to people. When he gave groceries to the 13th person, he stopped because in Gurmukhi and Hindi the word 13 is called Terah, which means yours. And Guru Nanak Dev Ji kept saying, "Yours, yours, yours..." remembering God. People reported to the emperor that Guru Nanak Dev Ji was giving out free food to the people. When treasures were checked, there was more money than before.
The Vaisakhi, which commemorates the creation of "Khalsa" or pure Sikh was celebrated on April 13 for many years.
Zoroastrianism.
The number 13 had been considered sinister and wicked in ancient Iranian civilization and Zoroastrianism. Since beginning of the Nourooz tradition, the 13th day of each new Iranian year is called Sizdah Be-dar, and this tradition is still alive among Iranian people both within Iran and abroad. Since Sizdah Be-dar is the 13th day of the year, it is considered a day when evil's power might cause difficulties for people. Therefore people leave urban areas for one day and camp in the countryside. Even in the current post-1979 Revolution era, and despite the wishes of Islamic government, this tradition continues to be practiced by the majority of the population throughout Iran.
Islam.
In Shia Islam 13 signifies the 13th day of the month of Rajab (Lunar calendar), which is the birth of Imam Ali. 13 also is a total of 1 Prophet and 12 Imams in the Shia school of thought.
Wicca.
In Wicca, most covens have 13 members, although sometimes there can be fewer.
Lucky and unlucky.
Unlucky 13.
The number 13 is considered an unlucky number in some countries. The end of the Mayan calendar's 13th Baktun was superstitiously feared as a harbinger of the apocalyptic 2012 phenomenon. Fear of the number 13 has a specifically recognized phobia, Triskaidekaphobia, a word coined in 1911. The superstitious sufferers of triskaidekaphobia try to avoid bad luck by keeping away from anything numbered or labelled thirteen. As a result, companies and manufacturers use another way of numbering or labeling to avoid the number, with hotels and tall buildings being conspicuous examples (thirteenth floor). It's also considered unlucky to have thirteen guests at a table. Friday the 13th has been considered the unluckiest day of the month.
There is a number of theories behind the cause of the association between thirteen and bad luck, but none of them have been accepted as likely.
Lucky 13.
In Italy, 13 is also considered a lucky number. The expression 'fare tredici' (to do 13) means hit the jackpot. 17 is considered a unlucky number instead. 
Music.
American born Horror-Punk singer and musician Joseph Poole (Murderdolls) uses the name Wednesday 13 as his stage name, taking "Wednesday" from the girl Wednesday from the Addams Family and 13 from Friday the 13th.
American country-pop singer-songwriter Taylor Swift was born on December 13. She considers 13 her lucky number due to lucky events happening to her when the number appears (her first album going gold in 13 weeks, being seated at awards shows in the 13th seat, row or section). She also wears the number written on her hand at her concerts so she has it with her everywhere she goes.
The heavy metal band, Megadeth, released their 13th studio album entitled TH1RT3EN on November 1, 2011. It consists of 13 tracks including the final song "13".
Famous American country singer and songwriter Johnny Cash first released his song called "".
There are 13 notes, by inclusive counting, in a full chromatic musical octave.
Track No. 12 on American heavy metal band Danzig's album is called "Thirteen".
The band Big Star wrote a song called 13.
The band Teenage Fanclub named their album Thirteen after Big Star's song. The band were heavily influenced by Big Star.
American alternative rock band, Pixies recorded "Number 13 Baby" for their Doolittle LP. The lyrics to the song include the line: "Standing in her chinos shirt pulled off clean, gotta tattooed tit say number 13"
English alternative rock band Blur's sixth studio album, entitled "13" was released in 1999.
The British heavy metal pioneers Black Sabbath's latest studio album is entitled "13". It was released on 11 June 2013.
Other.
Colgate University also considers 13 a lucky number. They were founded in 1819 by 13 men with 13 dollars, 13 prayers and 13 articles. (To this day, members of the Colgate community consider the number 13 a good omen.) In fact, the campus address is 13 Oak Drive in Hamilton, New York, and the male "a cappella" group is called the Colgate 13.
In the Mayan Tzolk'in calendar, trecenas mark cycles of 13 day periods. The pyramids are also set up in 9 steps divided into 7 days and 6 nights, 13 days total.
In the standard 52-card deck of playing cards there are four suits, each of 13 ranks.
In a tarot card deck, XIII is the card of Death, usually picturing the Pale horse with its rider.
A 24-hour clock uses the number 13 in the form of 13:00-13:59, to represent the fourteenth hour of its 24 hour cycle.
A baker's dozen, devil's dozen, long dozen, or long measure is 13, one more than a standard dozen.
In sports.
In rugby league:
The jersey number 13 has been retired by several North American sports teams, usually in honor of past playing greats:
In triathlon, the number 13 is not used. As such, the numbering goes 11, 12, "14", 15 under the current numbering system. The number was not used in Formula One from 1977 to 2013.
In U.S. college athletics, schools that are members of NCAA Division I are allowed to provide athletic scholarships to a maximum of 13 men's basketball players in a given season.
In rugby union, the jersey number 13 is worn by one of the two starting centres, usually the outside centre but sometimes the inside centre.
The number 13 is the most-commonly registered jersey number in modern roller derby.

</doc>
<doc id="37232" url="http://en.wikipedia.org/wiki?curid=37232" title="Fermat's principle">
Fermat's principle

In optics, Fermat's principle or the principle of least time is the principle that the path taken between two points by a ray of light is the path that can be traversed in the least time. This principle is sometimes taken as the definition of a ray of light. However, this version of the principle is not general; a more modern statement of the principle is that rays of light traverse the path of stationary optical length with respect to variations of the path. In other words, a ray of light prefers the path such that there are other paths, arbitrarily nearby on either side, along which the ray would take almost exactly the same time to traverse.
Fermat's principle can be used to describe the properties of light rays reflected off mirrors, refracted through different media, or undergoing total internal reflection. It follows mathematically from Huygens' principle (at the limit of small wavelength). Fermat's text "Analyse des réfractions" exploits the technique of adequality to derive Snell's law of refraction and the law of reflection.
Fermat's principle has the same form as Hamilton's principle and it is the basis of Hamiltonian optics.
Modern version.
The time T a point of the electromagnetic wave needs to cover a path between the points A and B is given by:
"c" is the speed of light in vacuum, "ds" an infinitesimal displacement along the ray, "v" = "ds"/"dt" the speed of light in a medium and "n" = "c"/"v" the refractive index of that medium, formula_2 is the starting time (the wave front is in A), formula_3 is the arrival time at B. The optical path length of a ray from a point A to a point B is defined by:
and it is related to the travel time by "S" = "cT". The optical path length is a purely geometrical quantity since time is not considered in its calculation. An extremum in the light travel time between two points A and B is equivalent to an extremum of the optical path length between those two points. The historical form proposed by French mathematician Pierre de Fermat is incomplete. A complete modern statement of the variational Fermat principle is that the optical length of the path followed by light between two fixed points, A and B, is an extremum. The optical length is defined as the physical length multiplied by the refractive index of the material."
 In the context of calculus of variations this can be written as
In general, the refractive index is a scalar field of position in space, that is, formula_6 in 3D euclidean space. Assuming now that light has a component that travels along the "x"3 axis, the path of a light ray may be parametrized as formula_7 and
where formula_9. The principle of Fermat can now be written as
which has the same form as Hamilton's principle but in which "x"3 takes the role of time in classical mechanics. Function formula_12 is the optical Lagrangian from which the Lagrangian and Hamiltonian (as in Hamiltonian mechanics) formulations of geometrical optics may be derived.
Derivation.
Classically, Fermat's principle can be considered as a mathematical consequence of Huygens' principle. Indeed, of all secondary waves (along all possible paths) the waves with the extrema (stationary) paths contribute most due to constructive interference. Suppose that light waves propagate from A to B by all possible routes ABj, unrestricted initially by rules of geometrical or physical optics. The various optical paths ABj will vary by amounts greatly in excess of one wavelength, and so the waves arriving at B will have a large range of phases and will tend to interfere destructively. But if there is a shortest route AB0, and the optical path varies smoothly through it, then a considerable number of neighboring routes close to AB0 will have optical paths differing from AB0 by second-order amounts only and will therefore interfere constructively. Waves along and close to this shortest route will thus dominate and AB0 will be the route along which the light is seen to travel.
Fermat's principle is the main principle of quantum electrodynamics which states that any particle (e.g. a photon or an electron) propagates over all available, unobstructed paths and that the interference, or superposition, of its wavefunction over all those paths at the point of observer gives the probability of detecting the particle at this point. Thus, because the extremal paths (shortest, longest, or stationary) cannot be completely canceled out, they contribute most to this interference.
In the classic mechanics of waves, Fermat's principle follows from the extremum principle of mechanics (see variational principle).
History.
Hero of Alexandria (Heron) (c. 60) described a principle of reflection, which stated that a ray of light that goes from point A to point B, suffering any number of reflections on flat mirrors, in the same medium, has a smaller path length than any nearby path.
Ibn al-Haytham (Alhacen), in his "Book of Optics" (1021), expanded the principle to both reflection and refraction, and expressed an early version of the principle of least time. His experiments were based on earlier works on refraction carried out by the Greek scientist Ptolemy
The generalized principle of least time in its modern form was stated by Fermat in a letter dated January 1, 1662, to Cureau de la Chambre. It was met with objections made in May 1662 by Claude Clerselier, an expert in optics and leading spokesman for the Cartesians at that time. Amongst his objections, Clerselier states:
"... Fermat's principle can not be the cause, for otherwise we would be attributing knowledge to nature: and here, by nature, we understand only that order and lawfulness in the world, such as it is, which acts without foreknowledge, without choice, but by a necessary determination."
The original French, from Mahoney, is as follows:
"Le principe que vous prenez pour fondement de votre démonstration, à savoir que la nature agit toujours par les voies les plus courtes et les plus simples, n’est qu’un principe moral et non point physique, qui n’est point et qui ne peut être la cause d’aucun effet de la nature."
Indeed Fermat's principle does not hold standing alone, we now know it can be derived from earlier principles such as Huygens' principle. 
Historically, Fermat's principle has served as a guiding principle in the formulation of physical laws with the use of variational calculus (see Principle of least action).

</doc>
<doc id="37235" url="http://en.wikipedia.org/wiki?curid=37235" title="Society">
Society

A human society is a group of people involved in persistent interpersonal relationships, or a large social grouping sharing the same geographical or social territory, typically subject to the same political authority and dominant cultural expectations. Human societies are characterized by patterns of relationships (social relations) between individuals who share a distinctive culture and institutions; a given society may be described as the sum total of such relationships among its constituent members. In the social sciences, a larger society often evinces stratification or dominance patterns in subgroups.
Insofar as it is collaborative, a society can enable its members to benefit in ways that would not otherwise be possible on an individual basis; both individual and social (common) benefits can thus be distinguished, or in many cases found to overlap.
A society can also consist of like-minded people governed by their own norms and values within a dominant, larger society. This is sometimes referred to as a subculture, a term used extensively within criminology.
More broadly, and especially within structuralist thought, a society may be illustrated as an economic, social, industrial or cultural infrastructure, made up of, yet distinct from, a varied collection of individuals. In this regard society can mean the objective relationships people have with the material world and with other people, rather than "other people" beyond the individual and their familiar social environment.
Etymology and usage.
A half-section of the 12th-century South Tang Dynasty version of "Night Revels of Han Xizai", original by Gu Hongzhong. The painting portrays servants, musicians, monks, children, guests, and hosts all in a single social environment. It serves as an in-depth look into the Chinese social structure of the time.
The term "society" came from the Latin word "", which in turn was derived from the noun "socius" ("comrade, friend, ally"; adjectival form "socialis") used to describe a bond or interaction between parties that are friendly, or at least civil. Without an article, the term can refer to the entirety of humanity (also: "society in general", "society at large", etc.), although those who are unfriendly or uncivil to the remainder of society in this sense may be deemed to be "antisocial". Adam Smith wrote that a society "may subsist among different men, as among different merchants, from a sense of its utility without any mutual love or affection, if only they refrain from doing injury to each other." 
Used in the sense of an association, a society is a body of individuals outlined by the bounds of functional interdependence, possibly comprising characteristics such as national or cultural identity, social solidarity, language, or hierarchical structure.
Conceptions.
Society, in general, addresses the fact that an individual has rather limited means as an autonomous unit. The great apes have always been more ("Bonobo", "Homo", "Pan") or less ("Gorilla", "Pongo") social animals, so Robinson Crusoe-like situations are either fictions or unusual corner cases to the ubiquity of social context for humans, who fall between presocial and eusocial in the spectrum of animal ethology.
Human societies are most often organized according to their primary means of subsistence. Social scientists have identified hunter-gatherer societies, nomadic pastoral societies, horticulturalist or simple farming societies, and intensive agricultural societies, also called civilizations. Some consider industrial and post-industrial societies to be qualitatively different from traditional agricultural societies.
Today, anthropologists and many social scientists vigorously oppose the notion of cultural evolution and rigid "stages" such as these. In fact, much anthropological data has suggested that complexity (civilization, population growth and density, specialization, etc.) does not always take the form of hierarchical social organization or stratification.
Cultural relativism as a widespread approach or ethic has largely replaced notions of "primitive", better/worse, or "progress" in relation to cultures (including their material culture/technology and social organization).
According to anthropologist Maurice Godelier, one critical novelty in human society, in contrast to humanity's closest biological relatives (chimpanzees and bonobos), is the parental role assumed by the males, which supposedly would be absent in our nearest relatives for whom paternity is not generally determinable.
In political science.
Societies may also be structured politically. In order of increasing size and complexity, there are bands, tribes, chiefdoms, and state societies. These structures may have varying degrees of political power, depending on the cultural, geographical, and historical environments that these societies must contend with. Thus, a more isolated society with the same level of technology and culture as other societies is more likely to survive than one in closer proximity to others that may encroach on their resources. A society that is unable to offer an effective response to other societies it competes with will usually be subsumed into the culture of the competing society.
In sociology.
Sociologist Gerhard Lenski differentiates societies based on their level of technology, communication, and economy: (1) hunters and gatherers, (2) simple agricultural, (3) advanced agricultural, (4) industrial, and (5) special (e.g. fishing societies or maritime societies). This is similar to the system earlier developed by anthropologists Morton H. Fried, a conflict theorist, and Elman Service, an integration theorist, who have produced a system of classification for societies in all human cultures based on the evolution of social inequality and the role of the state. This system of classification contains four categories:
In addition to this there are:
Over time, some cultures have progressed toward more complex forms of organization and control. This cultural evolution has a profound effect on patterns of community. Hunter-gatherer tribes settled around seasonal food stocks to become agrarian villages. Villages grew to become towns and cities. Cities turned into city-states and nation-states.
Many societies distribute largess at the behest of some individual or some larger group of people. This type of generosity can be seen in all known cultures; typically, prestige accrues to the generous individual or group. Conversely, members of a society may also shun or scapegoat members of the society who violate its norms. Mechanisms such as gift-giving, joking relationships and scapegoating, which may be seen in various types of human groupings, tend to be institutionalized within a society. Social evolution as a phenomenon carries with it certain elements that could be detrimental to the population it serves.
Some societies bestow status on an individual or group of people when that individual or group performs an admired or desired action. This type of recognition is bestowed in the form of a name, title, manner of dress, or monetary reward. In many societies, adult male or female status is subject to a ritual or process of this type. Altruistic action in the interests of the larger group is seen in virtually all societies. The phenomena of community action, shunning, scapegoating, generosity, shared risk, and reward are common to many forms of society.
Types.
Societies are social groups that differ according to subsistence strategies, the ways that humans use technology to provide needs for themselves. Although humans have established many types of societies throughout history, anthropologists tend to classify different societies according to the degree to which different groups within a society have unequal access to advantages such as resources, prestige, or power. Virtually all societies have developed some degree of inequality among their people through the process of social stratification, the division of members of a society into levels with unequal wealth, prestige, or power. Sociologists place societies in three broad categories: pre-industrial, industrial, and postindustrial.
Pre-industrial.
In a pre-industrial society, food production, which is carried out through the use of human and animal labor, is the main economic activity. These societies can be subdivided according to their level of technology and their method of producing food. These subdivisions are hunting and gathering, pastoral, horticultural, agricultural, and feudal.
Hunting and gathering.
The main form of food production in such societies is the daily collection of wild plants and the hunting of wild animals. Hunter-gatherers move around constantly in search of food. As a result, they do not build permanent villages or create a wide variety of artifacts, and usually only form small groups such as bands and tribes. However, some hunting and gathering societies in areas with abundant resources (such as the Tlingit) lived in larger groups and formed complex hierarchical social structures such as chiefdoms. The need for mobility also limits the size of these societies. They generally consist of fewer than 60 people and rarely exceed 100. Statuses within the tribe are relatively equal, and decisions are reached through general agreement. The ties that bind the tribe are more complex than those of the bands. Leadership is personal—charismatic—and used for special purposes only in tribal society. There are no political offices containing real power, and a chief is merely a person of influence, a sort of adviser; therefore, tribal consolidations for collective action are not governmental. The family forms the main social unit, with most societal members being related by birth or marriage. This type of organization requires the family to carry out most social functions, including production and education.
Pastoral.
Pastoralism is a slightly more efficient form of subsistence. Rather than searching for food on a daily basis, members of a pastoral society rely on domesticated herd animals to meet their food needs. Pastoralists live a nomadic life, moving their herds from one pasture to another. Because their food supply is far more reliable, pastoral societies can support larger populations. Since there are food surpluses, fewer people are needed to produce food. As a result, the division of labor (the specialization by individuals or groups in the performance of specific economic activities) becomes more complex. For example, some people become craftworkers, producing tools, weapons, and jewelry. The production of goods encourages trade. This trade helps to create inequality, as some families acquire more goods than others do. These families often gain power through their increased wealth. The passing on of property from one generation to another helps to centralize wealth and power. Over time emerge hereditary chieftainships, the typical form of government in pastoral societies.
Horticultural.
Fruits and vegetables grown in garden plots that have been cleared from the jungle or forest provide the main source of food in a horticultural society. These societies have a level of technology and complexity similar to pastoral societies. Some horticultural groups use the slash-and-burn method to raise crops. The wild vegetation is cut and burned, and ashes are used as fertilizers. Horticulturists use human labor and simple tools to cultivate the land for one or more seasons. When the land becomes barren, horticulturists clear a new plot and leave the old plot to revert to its natural state. They may return to the original land several years later and begin the process again. By rotating their garden plots, horticulturists can stay in one area for a fairly long period of time. This allows them to build semipermanent or permanent villages. The size of a village's population depends on the amount of land available for farming; thus villages can range from as few as 30 people to as many as 2000.
As with pastoral societies, surplus food leads to a more complex division of labor. Specialized roles in horticultural societies include craftspeople, shamans (religious leaders), and traders. This role specialization allows people to create a wide variety of artifacts. As in pastoral societies, surplus food can lead to inequalities in wealth and power within horticultural political systems, developed because of the settled nature of horticultural life.
Agrarian.
Agrarian societies use agricultural technological advances to cultivate crops over a large area. Sociologists use the phrase Agricultural Revolution to refer to the technological changes that occurred as long as 8,500 years ago that led to cultivating crops and raising farm animals. Increases in food supplies then led to larger populations than in earlier communities. This meant a greater surplus, which resulted in towns that became centers of trade supporting various rulers, educators, craftspeople, merchants, and religious leaders who did not have to worry about locating nourishment.
Greater degrees of social stratification appeared in agrarian societies. For example, women previously had higher social status because they shared labor more equally with men. In hunting and gathering societies, women even gathered more food than men. However, as food stores improved and women took on lesser roles in providing food for the family, they increasingly became subordinate to men. As villages and towns expanded into neighboring areas, conflicts with other communities inevitably occurred. Farmers provided warriors with food in exchange for protection against invasion by enemies. A system of rulers with high social status also appeared. This nobility organized warriors to protect the society from invasion. In this way, the nobility managed to extract goods from “lesser” members of society.
Feudal.
Feudalism was a form of society based on ownership of land. Unlike today's farmers, vassals under feudalism were bound to cultivating their lord's land. In exchange for military protection, the lords exploited the peasants into providing food, crops, crafts, homage, and other services to the landowner. The estates of the realm system of feudalism was often multigenerational; the families of peasants may have cultivated their lord's land for generations.
Industrial.
Between the 15th and 16th centuries, a new economic system emerged that began to replace feudalism. Capitalism is marked by open competition in a free market, in which the means of production are privately owned. Europe's exploration of the Americas served as one impetus for the development of capitalism. The introduction of foreign metals, silks, and spices stimulated great commercial activity in European societies.
Industrial societies rely heavily on machines powered by fuels for the production of goods. This produced further dramatic increases in efficiency. The increased efficiency of production of the industrial revolution produced an even greater surplus than before. Now the surplus was not just agricultural goods, but also manufactured goods. This larger surplus caused all of the changes discussed earlier in the domestication revolution to become even more pronounced.
Once again, the population boomed. Increased productivity made more goods available to everyone. However, inequality became even greater than before. The breakup of agricultural-based feudal societies caused many people to leave the land and seek employment in cities. This created a great surplus of labor and gave capitalists plenty of laborers who could be hired for extremely low wages.
Post-industrial.
Post-industrial societies are societies dominated by information, services, and high technology more than the production of goods. Advanced industrial societies are now seeing a shift toward an increase in service sectors over manufacturing and production. The United States is the first country to have over half of its work force employed in service industries. Service industries include government, research, education, health, sales, law, and banking.
Contemporary usage.
The term "society" is currently used to cover both a number of political and scientific connotations as well as a variety of associations.
Western.
The development of the Western world has brought with it the emerging concepts of Western culture, politics, and ideas, often referred to simply as "Western society". Geographically, it covers at the very least the countries of Western Europe, North America, Australia, and New Zealand. It sometimes also includes Eastern Europe, South America, and Israel.
The cultures and lifestyles of all of these stem from Western Europe. They all enjoy relatively strong economies and stable governments, allow freedom of religion, have chosen democracy as a form of governance, favor capitalism and international trade, are heavily influenced by Judeo-Christian values, and have some form of political and military alliance or cooperation.
Information.
Although the concept of information society has been under discussion since the 1930s, in the modern world it is almost always applied to the manner in which information technologies have impacted society and culture. It therefore covers the effects of computers and telecommunications on the home, the workplace, schools, government, and various communities and organizations, as well as the emergence of new social forms in cyberspace.
One of the European Union's areas of interest is the information society. Here policies are directed towards promoting an open and competitive digital economy, research into information and communication technologies, as well as their application to improve social inclusion, public services, and quality of life.
The International Telecommunications Union's World Summit on the Information Society in Geneva and Tunis (2003 and 2005) has led to a number of policy and application areas where action is required. These include:
Knowledge.
As access to electronic information resources increased at the beginning of the 21st century, special attention was extended from the information society to the knowledge society. An analysis by the Irish government stated, "The capacity to manipulate, store and transmit large quantities of information cheaply has increased at a staggering rate over recent years. The digitisation of information and the associated pervasiveness of the Internet are facilitating a new intensity in the application of knowledge to economic activity, to the extent that it has become the predominant factor in the creation of wealth. As much as 70 to 80 percent of economic growth is now said to be due to new and better knowledge."
The Second World Summit on the Knowledge Society, held in Chania, Crete, in September 2009, gave special attention to the following topics:
Other uses.
People of many nations united by common political and cultural traditions, beliefs, or values are sometimes also said to form a society (such as Judeo-Christian, Eastern, and Western). When used in this context, the term is employed as a means of contrasting two or more "societies" whose members represent alternative conflicting and competing worldviews.
Some academic, professional, and scientific associations describe themselves as "societies" (for example, the American Mathematical Society, the American Society of Civil Engineers, or the Royal Society).
In some countries, e.g. the United States, France, and Latin America, the term "society' is used in commerce to denote a partnership between investors or the start of a business. In the United Kingdom, partnerships are not called societies, but co-operatives or mutuals are often known as societies (such as friendly societies and building societies).

</doc>
<doc id="37238" url="http://en.wikipedia.org/wiki?curid=37238" title="Kevin J. Anderson">
Kevin J. Anderson

Kevin J. Anderson (born March 27, 1962) is an American science fiction author with over 50 bestsellers. He has written spin-off novels for "Star Wars", "StarCraft", "Titan A.E." and "The X-Files", and with Brian Herbert is the co-author of the "Dune" prequel series. His original works include the "Saga of Seven Suns" series and the Nebula Award-nominated "Assemblers of Infinity". He has also written several comic books, including the Dark Horse "Star Wars" collection "Tales of the Jedi" written in collaboration with Tom Veitch, Dark Horse "Predator" titles, and "X-Files" titles for Topps. Some of Anderson's superhero novels include "Enemies & Allies", about the first meeting of Batman and Superman, and "The Last Days of Krypton", telling the story of how Superman's planet Krypton came to be destroyed.
Anderson has published over 120 books, over 50 of which have been on US and international bestseller lists, and he has more than 23 million books in print worldwide.
His wife is author Rebecca Moesta. They currently reside near Monument, Colorado.
Early life.
Kevin J. Anderson (aka Kevin James Anderson) was born March 27, 1962 in Racine, Wisconsin. According to Anderson, "The War of the Worlds" greatly influenced him. He wrote his first story at eight years old entitled "Injection." At ten, he bought a typewriter and has written ever since. In his freshman year in high school, he submitted his first short story to a magazine, but it took two more years before one of his manuscripts was accepted. When it was accepted, they paid him in copies of the magazine. In his senior year, he sold his first story for money for $12.50.
For 12 years Anderson worked at the Lawrence Livermore National Laboratory, where he met fellow writers Rebecca Moesta and Doug Beason. Anderson would later marry Moesta, and frequently coauthors novels with both her and Beason.
Writing.
Anderson's first novel, "Resurrection, Inc.", was published in 1988 and nominated for a Bram Stoker Award for Best First Novel. His 1993 collaboration with Beason, "Assemblers of Infinity", was nominated for both a Nebula and Locus Award. Anderson wrote the "X-Files" novels "Ground Zero" (1995), "Ruins" (1996) and "Antibodies" (1997). "Ground Zero" reached #1 on the "London Sunday Times" Best Seller List and "Ruins" made the "New York Times" Best Seller list. Contracted to write novels in the "Star Wars" expanded universe, Anderson published the "Jedi Academy" trilogy in 1994, followed by the 1996 novel "Darksaber". He and Moesta also wrote the 14-volume "Young Jedi Knights" series from 1995 to 1998. As a noted "Star Wars" novelist, Anderson was a participant in the FidoNet "Star Wars" Echo, a pre-internet 1990s bulletin board system forum cited as one of the earliest influential forms of "Star Wars" on-line fandom.
In 1997, Anderson and Brian Herbert signed a $3 million deal with Bantam Books to coauthor a prequel trilogy to the 1965 novel "Dune" and its five sequels (1969-1985) by Herbert's deceased father, Frank Herbert. Starting with 1999's ', the ongoing "Dune" prequel series has expanded to ten novels to date. In 2011 "Publishers Weekly" called the series "a sprawling edifice that Frank Herbert’s son and Anderson have built on the foundation of the original "Dune" novels." Anderson and Brian Herbert have also published "Hunters of Dune" (2006) and "Sandworms of Dune" (2007), sequels to Frank Herbert's final novel ' (1985) which complete the chronological progression of his original series and wrap up storylines that began with his "Heretics of Dune" (1984). Between 2011 and 2014, Anderson and Herbert also released their "Hellhole" trilogy of novels unrelated to "Dune".
In 2002, Anderson released the steampunk/adventure novel "", and was subsequently asked to write "The League of Extraordinary Gentlemen" (2003), a novelization of the film of the same name. The following year he also wrote the novelization for the 2004 film "Sky Captain and the World of Tomorrow".
Between 2002 and 2008, Anderson published a seven novel original space opera series called "The Saga of Seven Suns". In 2014 he began published a sequel trilogy called "The Saga of Shadows". Anderson published four novels and two short stories in his "Dan Shamble, Zombie P.I." series between 2012 and 2014.
WordFire Press.
In 2011, Anderson and Moesta founded their own publishing imprint, WordFire Press, to reissue some of their out-of-print books in paperback and/or e-book formats. They have subsequently published and reprinted works in various genres, including Allen Drury's 1959 Pulitzer Prize-winning political novel "Advise and Consent" and several out-of-print or previously unpublished novels by Frank Herbert.
In 2013, WordFire acquired the reprint rights to the works of Allen Drury, including his Pulitzer Prize-winning political novel "Advise and Consent". That novel, out of print for nearly 15 years, ranked #27 on the 2013 BookFinder.com list of the Top 100 Most Searched for Out of Print Books before WordFire reissued it in February 2014. The company also reprinted "Advise and Consent"‍‍ '​‍s five sequels — "A Shade of Difference" (1962), "Capable of Honor" (1966), "Preserve and Protect" (1968), "Come Nineveh, Come Tyre" (1973) and "The Promise of Joy" (1975) — as well as Drury's later novels "Mark Coffin, U.S.S." (1979) and "Decision" (1983).
WordFire released four previously unpublished novels by Frank Herbert, who died in 1986: "High-Opp" (2012), "Angels' Fall" (2013), "A Game of Authors" (2013) and "A Thorn in the Bush" (2014). Anderson announced these in his blog. WordFire also reissued several of Herbert's unavailable titles — "" (1966), "The Heaven Makers" (1968), "Soul Catcher" (1972), "The Godmakers" (1972) and "Direct Descent" (1980) — as well as "Man of Two Worlds" (1986), an out-of-print novel cowritten by Herbert and his son Brian.
Bibliography.
Anderson has published over 120 books, over 50 of which have been on US and international bestseller lists, and he has more than 23 million books in print worldwide.

</doc>
<doc id="37241" url="http://en.wikipedia.org/wiki?curid=37241" title="Eik, Rogaland">
Eik, Rogaland

Eik is a village in Lund, Norway. Since 1981 it has had a station for world-wide Inmarsat access. It is part of a communication system that is used by Denmark, and other countries.

</doc>
<doc id="37242" url="http://en.wikipedia.org/wiki?curid=37242" title="Booker T. Washington">
Booker T. Washington

Booker Taliaferro Washington (April 5, 1856 – November 14, 1915) was an African-American educator, author, orator, and advisor to presidents of the United States. Between 1890 and 1915, Washington was the dominant leader in the African-American community.
Washington was of the last generation of black American leaders born into slavery and became the leading voice of the former slaves and their descendants, who were newly oppressed by disfranchisement and the Jim Crow discriminatory laws enacted in the post-Reconstruction Southern states in the late 19th and early 20th centuries. In 1895 his Atlanta compromise called for avoiding confrontation over segregation and instead putting more reliance on long-term educational and economic advancement in the black community.
His base was the Tuskegee Institute, a historically black college in Alabama. As lynchings in the South reached a peak in 1895, Washington gave a speech in Atlanta that made him nationally famous. The speech called for black progress through education and entrepreneurship. His message was that it was not the time to challenge Jim Crow segregation and the disfranchisement of black voters in the South. Washington mobilized a nationwide coalition of middle-class blacks, church leaders, and white philanthropists and politicians, with a long-term goal of building the community's economic strength and pride by a focus on self-help and schooling. Secretly, he supported court challenges to segregation. Black militants in the North, led by W. E. B. Du Bois, at first supported the Atlanta compromise but after 1909 they set up the NAACP and tried with little success to challenge Washington's political machine for leadership in the black community. Decades after Washington's death in 1915, the Civil Rights movement generally moved away from his policies to take the more militant NAACP approach.
Booker T. Washington mastered the nuances of the political arena in the late 19th century which enabled him to manipulate the media, raise money, strategize, network, pressure, reward friends and distribute funds while punishing those who opposed his plans for uplifting blacks. His long-term goal was to end the disfranchisement of the vast majority of African Americans living in southern states, where most of the millions of black Americans still lived.
Overview.
In 1856, Washington was born a slave in Virginia to a woman named Jane. After emancipation, his family resettled in West Virginia. He worked his way through Hampton Normal and Agricultural Institute (now Hampton University) and attended college at Wayland Seminary (now Virginia Union University). In 1881, he was named as the first leader of the new Tuskegee Institute in Alabama.
Washington attained national prominence for his Atlanta Address of 1895, which attracted the attention of politicians and the public, making him a popular spokesperson for African-American citizens. He built a nationwide network of supporters in many black communities, with black ministers, educators and businessmen composing his core supporters. Washington played a dominant role in black politics, winning wide support in the black community of the South and among more liberal whites (especially rich Northern whites). He gained access to top national leaders in politics, philanthropy and education. Washington's efforts included cooperating with white people and enlisting the support of wealthy philanthropists, helping to raise funds to establish and operate thousands of small community schools and institutions of higher education for the betterment of blacks throughout the South. This work continued for many years after his death. Washington argued that the surest way for blacks to gain equal social rights was to demonstrate "industry, thrift, intelligence and property."
Northern critics called Washington's widespread organization the "Tuskegee Machine". After 1909, Washington was criticized by the leaders of the new NAACP, especially W. E. B. Du Bois, who demanded a stronger tone of protest for advancement of civil rights needs. Washington replied that confrontation would lead to disaster for the outnumbered blacks in society, and that cooperation with supportive whites was the only way to overcome pervasive racism in the long run. At the same time, he secretly funded litigation for civil rights cases, such as challenges to southern constitutions and laws that disfranchised blacks. Washington was on close terms with national Republican Party leaders, and often was asked for political advice by presidents Theodore Roosevelt and William Howard Taft.
In addition to his contributions in education, Washington wrote 14 books; his autobiography, "Up From Slavery", first published in 1901, is still widely read today. During a difficult period of transition, he did much to improve the working relationship between the races. His work greatly helped blacks to achieve higher education, financial power and understanding of the U.S. legal system. This contributed to blacks' attaining the skills to create and support the Civil Rights Movement of the 1960s, leading to the passage of important federal civil rights laws.
Career overview.
Washington was born into slavery to Jane, an enslaved African-American woman on the Burroughs Plantation in southwest Virginia. She never identified his white father, said to be a nearby planter, and the man played no significant role in Washington's life. She married another slave who escaped to West Virginia, which became a state in the Union during the war. Washington's family gained freedom in early 1865 under the Emancipation Proclamation as US troops occupied their region. Later his mother took all the children to West Virginia to join her husband. 
As a boy of 9 in Virginia, Booker was thrilled by the day of emancipation in early 1865:
As the great day drew nearer, there was more singing in the slave quarters than usual. It was bolder, had more ring, and lasted later into the night. Most of the verses of the plantation songs had some reference to freedom... Some man who seemed to be a stranger (a United States officer, I presume) made a little speech and then read a rather long paper—the Emancipation Proclamation, I think. After the reading we were told that we were all free, and could go when and where we pleased. My mother, who was standing by my side, leaned over and kissed her children, while tears of joy ran down her cheeks. She explained to us what it all meant, that this was the day for which she had been so long praying, but fearing that she would never live to see.
She and her husband, the freedman Washington Ferguson, were formally married in West Virginia. When he started school, Booker took the surname Washington after his stepfather.
The youth worked in salt furnaces and coal mines in West Virginia for several years to earn money. He made his way east to Hampton Institute, a school established to educate freedmen, where he worked to pay for his studies. He also attended Wayland Seminary in Washington, D.C. in 1878 and left after 6 months. In 1881, the Hampton Institute president Samuel C. Armstrong recommended Washington to become the first leader of Tuskegee Institute, the new normal school (teachers' college) in Alabama. He led the institution for the rest of his life.
Washington was instrumental in having West Virginia State University, founded in 1891, located in the Kanawha Valley of West Virginia. He visited the campus often and spoke at its first commencement exercise.
Washington was a dominant figure of the African-American community, then largely based in the South, from 1890 to his death in 1915, especially after his Atlanta Address of 1895. To many he was seen as a popular spokesman for African-American citizens. Representing the last generation of black leaders born into slavery, Washington was generally perceived as a supporter of education for freedmen and their descendants in the post-Reconstruction, Jim Crow-era South. Throughout the final twenty years of his life, he maintained his standing through a nationwide network of supporters including black educators, ministers, editors, and businessmen, especially those who supported his views on social and educational issues for blacks. He also gained access to top national white leaders in politics, philanthropy and education, raised large sums, was consulted on race issues, and was awarded honorary degrees from leading American universities.
Late in his career, Washington was criticized by leaders of the NAACP, a civil rights organization formed in 1909. W. E. B. Du Bois advocated activism to achieve civil rights. He labeled Washington "the Great Accommodator". Washington's response was that confrontation could lead to disaster for the outnumbered blacks. He believed that cooperation with supportive whites was the only way to overcome racism in the long run.
Washington contributed secretly and substantially to legal challenges against segregation and disfranchisement of blacks. In his public role, he believed he could achieve more by skillful accommodation to the social realities of the age of segregation.
Washington's work on education problems helped him enlist both the moral and substantial financial support of many major white philanthropists. He became a friend of such self-made men as Standard Oil magnate Henry Huttleston Rogers; Sears, Roebuck and Company President Julius Rosenwald; and George Eastman, inventor and founder of Kodak. These individuals and many other wealthy men and women funded his causes, including Hampton and Tuskegee institutes.
The schools which Washington supported were founded primarily to produce teachers, as blacks strongly supported literacy and education as the keys to their future. Graduates had often returned to their largely impoverished rural southern communities to find few schools and educational resources, as the white-dominated state legislatures consistently underfunded black schools in their segregated system. To address those needs, Washington enlisted his philanthropic network to create matching funds programs to stimulate construction of numerous rural public schools for black children in the South. Working especially with Julius Rosenwald from Chicago, Washington had Tuskegee architects develop model school designs. The Rosenwald Fund helped support the construction and operation of more than 5,000 schools and supporting resources for the betterment of blacks throughout the South in the late 19th and early 20th centuries. The local schools were a source of communal pride and were priceless to African-American families when poverty and segregation severely limited the life chances of the pupils. A major part of Washington's legacy, the model rural schools continued to be constructed into the 1930s, with matching funds from the Rosenwald Fund. Washington also helped with the Progressive Era by forming the National Negro Business League.
His autobiography, "Up From Slavery", first published in 1901, is still widely read today.
Tuskegee Normal and Industrial Institute.
The organizers of the new, all-black state school in Alabama called the Tuskegee Normal and Industrial Institute — the forerunner of Tuskegee University — found the energetic leader they sought in 25-year-old Washington. He believed that with self-help, people could go from poverty to success. The new school opened on July 4, 1881, initially using space in a local church. The next year, Washington purchased a former plantation, which became the permanent site of the campus. Under his direction, his students literally built their own school: making bricks, constructing classrooms, barns and outbuildings; and growing their own crops and raising livestock; both for learning and to provide for most of the basic necessities. Both men and women had to learn trades as well as academics. Washington helped raise funds to establish and operate hundreds of small community schools and institutions of higher educations for blacks. The Tuskegee faculty used all the activities to teach the students basic skills to take back to their mostly rural black communities throughout the South. The main goal was not to produce farmers and tradesmen, but teachers of farming and trades who taught in the new schools and colleges for blacks across the South. The school expanded over the decades, adding programs and departments, to become the present-day Tuskegee University.
Washington expressed his vision for his race in his direction of the school. He believed that by providing needed skills to society, African Americans would play their part, leading to acceptance by white Americans. He believed that blacks would eventually gain full participation in society by acting as responsible, reliable American citizens. Shortly after the Spanish–American War, President William McKinley and most of his cabinet visited Booker Washington. He led the school until his death in 1915. By then Tuskegee's endowment had grown to over $1.5 million, compared to its initial $2,000 annual appropriation.
Marriages and children.
Washington was married three times. In his autobiography "Up From Slavery", he gave all three of his wives credit for their contributions at Tuskegee. His first wife Fannie N. Smith was from Malden, West Virginia, the same Kanawha River Valley town where Washington had lived from age nine to sixteen. He maintained ties there all his life. Washington and Smith were married in the summer of 1882. They had one child, Portia M. Washington. Fannie died in May 1884.
Washington next wed Olivia A. Davidson in 1885. Born in Virginia, she had studied at Hampton Institute and the Massachusetts State Normal School at Framingham. She taught in Mississippi and Tennessee before going to Tuskegee to work as a teacher. Washington met Davidson at Tuskegee, where she was promoted to assistant principal. They had two sons, Booker T. Washington Jr. and Ernest Davidson Washington, before she died in 1889.
In 1893 Washington married Margaret James Murray. She was from Mississippi and had graduated from Fisk University, a historically black college. They had no children together, but she helped rear Washington's three children. Murray outlived Washington and died in 1925.
Politics and the Atlanta compromise.
Washington's 1895 Atlanta Exposition address was viewed as a "revolutionary moment" by both African Americans and whites across the country. At the time W. E. B. Du Bois supported him, but they grew apart as Du Bois sought more action to remedy disfranchisement and improve educational opportunities for blacks. After their falling out, Du Bois and his supporters referred to Washington's speech as the "Atlanta Compromise" to express their criticism that Washington was too accommodating to white interests.
Washington advocated a "go slow" approach to avoid a harsh white backlash. The effect was that many youths in the South had to accept sacrifices of potential political power, civil rights and higher education. His belief was that African Americans should "concentrate all their energies on industrial education, and accumulation of wealth, and the conciliation of the South." Washington valued the "industrial" education, as it provided critical skills for the jobs then available to the majority of African Americans at the time, as most lived in the South, which was overwhelmingly rural and agricultural. He thought these skills would lay the foundation for the creation of stability that the African-American community required in order to move forward. He believed that in the long term, "blacks would eventually gain full participation in society by showing themselves to be responsible, reliable American citizens." His approach advocated for an initial step toward equal rights, rather than full equality under the law, gaining economic power to back up black demands for political equality in the future. he believed that such achievements would prove to the deeply-prejudiced white America that African Americans were not "'naturally' stupid and incompetent."
Well-educated blacks in the North advocated a different approach, in part due to the differences they perceived in opportunities. Du Bois wanted blacks to have the same "classical" liberal arts education as upscale whites did, along with voting rights and civic equality, the latter two elements granted since 1870 by constitutional amendments after the Civil War. He believed that an elite, which he called the Talented Tenth, would advance to lead the race to a wider variety of occupations. Du Bois and Washington were divided in part by differences in treatment of African Americans in the North versus the South; although both groups suffered discrimination, the mass of blacks in the South were far more constrained by legal segregation and exclusion from the political process. Many in the North objected to being 'led', and authoritatively spoken for, by a Southern accommodationist strategy which they considered to have been "imposed on them [Southern blacks] primarily by Southern whites." Historian Clarence E. Walker wrote that, for white Southerners, 
"Free black people were 'matter out of place'. Their emancipation was an affront to southern white freedom. Booker T. Washington did not understand that his program was perceived as subversive of a natural order in which black people were to remain forever subordinate or unfree."Both Washington and Du Bois sought to define the best means to improve the conditions of the post-Civil War African-American community through education.
Blacks were solidly Republican in this period, having gained emancipation and suffrage with the President Lincoln and his party. Southern states disfranchised most blacks and many poor whites from 1890–1908 through constitutional amendments and statutes that created barriers to voter registration and voting, such as poll taxes and literacy tests. By the late nineteenth century, Southern white Democrats defeated some biracial Populist-Republican coalitions and regained power in the state legislatures of the former Confederacy; they passed laws establishing racial segregation and Jim Crow. In the border states and North, blacks continued to exercise the vote; the well-established Maryland African-American community defeated attempts there to disfranchise them.
Washington worked and socialized with many national white politicians and industry leaders. He developed the ability to persuade wealthy whites, many of them self-made men, to donate money to black causes by appealing to values they had exercised in their rise to power. He argued that the surest way for blacks to gain equal social rights was to demonstrate "industry, thrift, intelligence and property." He believed these were key to improved conditions for African Americans in the United States. Because African Americans had only recently been emancipated and most lived in a hostile environment, Washington believed they could not expect too much at once. He said, "I have learned that success is to be measured not so much by the position that one has reached in life as by the obstacles which he has had to overcome while trying to succeed."
Along with Du Bois, Washington partly organized the "Negro exhibition" at the 1900 Exposition Universelle in Paris, where photos of Hampton Institute's black students were displayed. These were taken by his friend Frances Benjamin Johnston. The exhibition demonstrated African Americans' positive contributions to United States' society.
Washington privately contributed substantial funds for legal challenges to segregation and disfranchisement, such as the case of "Giles v. Harris", which was heard before the United States Supreme Court in 1903. Even when such challenges were won at the Supreme Court, southern states quickly responded with new laws to accomplish the same ends, for instance, adding "grandfather clauses" that covered whites and not blacks.
Wealthy friends and benefactors.
State and local governments gave little money to black schools, but white philanthropists proved willing to invest heavily. Washington encouraged them and directed millions of their money to projects all across the South that Washington thought best reflected his self-help philosophy. Washington associated with the richest and most powerful businessmen and politicians of the era. He was seen as a spokesperson for African Americans and became a conduit for funding educational programs.
His contacts included such diverse and well-known entrepreneurs and philanthropists as Andrew Carnegie, William Howard Taft, John D. Rockefeller, Henry Huttleston Rogers, George Eastman, Julius Rosenwald, Robert Ogden, Collis Potter Huntington, and William Henry Baldwin Jr.. The latter donated large sums of money to agencies such as the Jeanes and Slater Funds. As a result, countless small rural schools were established through his efforts, under programs that continued many years after his death. Along with rich white men, the black communities helped their communities directly by donating time, money, and labor to schools in a sort of matching fund.
Henry Huttleston Rogers.
A representative case of an exceptional relationship was Washington's friendship with millionaire industrialist and financier Henry H. Rogers (1840–1909). Henry Rogers was a self-made man, who had risen from a modest working-class family to become a principal officer of Standard Oil, and one of the richest men in the United States. Around 1894 Rogers heard Washington speak at Madison Square Garden. The next day he contacted Washington and requested a meeting, during which Washington later recounted that he was told that Rogers "was surprised that no one had 'passed the hat' after the speech." The meeting began a close relationship that was to extend over a period of 15 years. Although Washington and the very-private Rogers were seen by the public as friends, the true depth and scope of their relationship was not publicly revealed until after Rogers' sudden death of a stroke in May 1909. Washington was a frequent guest at Rogers' New York office, his Fairhaven, Massachusetts summer home, and aboard his steam yacht "Kanawha". 
A few weeks later Washington went on a previously planned speaking tour along the newly completed Virginian Railway, a $40-million enterprise which had been built almost entirely from Rogers' personal fortune. As Washington rode in the late financier's private railroad car, "Dixie", he stopped and made speeches at many locations, where his companions later recounted that he had been warmly welcomed by both black and white citizens at each stop.
Washington revealed that Rogers had been quietly funding operations of 65 small country schools for African Americans, and had given substantial sums of money to support Tuskegee and Hampton institutes. He also disclosed that Rogers had encouraged programs with matching funds requirements so the recipients had a stake in the outcome.
Anna T. Jeanes.
In 1907 Philadelphia Quaker Anna T. Jeanes (1822–1907) donated one million dollars to Washington for elementary schools for black children in the South. Her contributions and those of Henry Rogers and others funded schools in many poor communities.
Julius Rosenwald.
Julius Rosenwald (1862–1932) was another self-made wealthy man with whom Washington found common ground. By 1908 Rosenwald, son of an immigrant clothier, had become part-owner and president of Sears, Roebuck and Company in Chicago. Rosenwald was a philanthropist who was deeply concerned about the poor state of African-American education, especially in the Southern states, where their schools were underfunded.
In 1912 Rosenwald was asked to serve on the Board of Directors of Tuskegee Institute, a position he held for the remainder of his life. Rosenwald endowed Tuskegee so that Washington could spend less time fundraising and more managing the school. Later in 1912 Rosenwald provided funds for a pilot program to build six new small schools in rural Alabama. They were designed, constructed and opened in 1913 and 1914 and overseen by Tuskegee; the model proved successful. Rosenwald established the Rosenwald Fund. The school building program was one of its largest programs. Using architectural model plans developed by professors at Tuskegee Institute, the Rosenwald Fund spent over $4 million to help build 4,977 schools, 217 teachers' homes, and 163 shop buildings in 883 counties in 15 states, from Maryland to Texas. The Rosenwald Fund made matching grants, requiring community support, cooperation from the white school boards, and fundraising. Black communities raised more than $4.7 million to aid the construction; essentially they taxed themselves twice to do so. These schools became informally known as Rosenwald Schools. By 1932, the facilities could accommodate one third of all African-American children in Southern U.S. schools.
"Up from Slavery" to the White House.
Washington's long-term adviser, Timothy Thomas Fortune (1856–1928), was a respected African-American economist and editor of the "The New York Age," the most widely read newspaper in the black community within the United States. He was the ghost writer and editor of Washington's first autobiography, "The Story of My Life and Work." Washington published five books during his lifetime with the aid of ghost-writers Timothy Fortune, Max Bennett Thrasher and Robert E. Park.
They included compilations of speeches and essays:
In an effort to inspire the "commercial, agricultural, educational, and industrial advancement" of African Americans, Washington founded the National Negro Business League (NNBL) in 1900.
When Washington's second autobiography, "Up From Slavery", was published in 1901, it became a bestseller and had a major effect on the African-American community, its friends and allies. In October 1901 President Theodore Roosevelt invited Washington to dine with him and his family at the White House; he was the first African American to be invited there. Democratic Party politicians of the South, including future Governor of Mississippi James K. Vardaman and Senator Benjamin Tillman of South Carolina indulged in racist personal attacks when they learned of the invitation. Vardaman described the White House as 
"so saturated with the odor of the nigger that the rats have taken refuge in the stable", and declared "I am just as much opposed to Booker T. Washington as a voter as I am to the cocoanut-headed, chocolate-colored typical little coon who blacks my shoes every morning. Neither is fit to perform the supreme function of citizenship." Tillman said, "The action of President Roosevelt in entertaining that nigger will necessitate our killing a thousand niggers in the South before they will learn their place again."
Austro-Hungarian ambassador to the United States Ladislaus Hengelmüller von Hengervár, who was visiting the White House on the same day, claimed to have found a rabbit's foot in Washington's coat pocket when he mistakenly put on the coat. "The Washington Post" elaborately described it as "the left hind foot of a graveyard rabbit, killed in the dark of the moon". "The Detroit Journal" quipped the next day, "The Austrian ambassador may have made off with Booker T. Washington's coat at the White House, but he'd have a bad time trying to fill his shoes."
Death.
Despite his travels and widespread work, Washington remained as principal of Tuskegee. Washington's health was deteriorating rapidly in 1915; he collapsed in New York City and was brought home to Tuskegee, where he died on November 14, 1915, at the age of 59. He was buried on the campus of Tuskegee University near the University Chapel.
His death was believed at the time to have been a result of congestive heart failure, aggravated by overwork. In March 2006, with the permission of his descendants, examination of medical records indicated that he died of hypertension, with a blood pressure more than twice normal, confirming what had long been suspected.
At his death Tuskegee's endowment exceeded $1.5 million. Washington's greatest life's work, the education of blacks in the South, was well underway and expanding.
Honors and memorials.
For his contributions to American society, Washington was granted an honorary master's degree from Harvard University in 1896 and an honorary doctorate from Dartmouth College in 1901.
At the end of the 2008 presidential election, the defeated Republican candidate, Senator John McCain, referred to Booker Washington's visit to Theodore Roosevelt's White House, a century before, as the seed that blossomed into Barack Obama as the first African American to be elected President of the United States.
In 1934 Robert Russa Moton, Washington's successor as president of Tuskegee University, arranged an air tour for two African-American aviators. Afterward he had the plane named the "Booker T. Washington". 
On April 7, 1940, Washington became the first African American to be depicted on a United States postage stamp. Several years later, he was honored on the first coin to feature an African American, the Booker T. Washington Memorial Half Dollar, which was minted by the United States from 1946 to 1951. He was also depicted on a U.S. Half Dollar from 1951–1954.
In 1942, the liberty ship "Booker T. Washington" was named in his honor, the first major oceangoing vessel to be named after an African American. The ship was christened by Marian Anderson.
On April 5, 1956, the hundredth anniversary of Washington's birth, the house where he was born in Franklin County, Virginia, was designated as the Booker T. Washington National Monument. A state park in Chattanooga, Tennessee, was named in his honor, as was a bridge spanning the Hampton River adjacent to his "alma mater", Hampton University.
In 1984 Hampton University dedicated a Booker T. Washington Memorial on campus near the historic Emancipation Oak, establishing, in the words of the University, "a relationship between one of America's great educators and social activists, and the symbol of Black achievement in education."
Numerous high schools, middle schools and elementary schools across the United States have been named after Booker T. Washington.
At the center of the campus at Tuskegee University, the Booker T. Washington Monument, called "Lifting the Veil," was dedicated in 1922. The inscription at its base reads:
He lifted the veil of ignorance from his people and pointed the way to progress through education and industry.
On October 19, 2009, West Virginia State University dedicated a monument to the memory of noted African American educator and statesman Booker T. Washington. The event took place at West Virginia State University's Booker T. Washington Park in Malden, West Virginia. The monument also honors the families of African ancestry who lived in Old Malden in the early 20th century and who knew and encouraged Booker T. Washington. Special guest speakers at the event included West Virginia Governor Joe Manchin III, Malden attorney Larry L. Rowe, and the president of WVSU. Musical selections were provided by the WVSU "Marching Swarm."
Legacy.
Washington was held in high regard by business-oriented conservatives, both white and black. Historian Eric Foner argues that the freedom movement of the late nineteenth century changed directions so as to align with America's new economic and intellectual framework. Black leaders emphasized economic self-help and individual advancement into the middle class as a more fruitful strategy than political agitation. There was emphasis on education and literacy throughout the period after the Civil War. Washington's famous Atlanta speech of 1895 marked this transition, as it called on blacks to develop their farms, their industrial skills and their entrepreneurship as the next stage in emerging from slavery. By this time, Mississippi had passed a new constitution, and other southern states were following suit, or using electoral laws to complete disfranchisement of blacks and maintain white political supremacy. At the same time, Washington secretly arranged to fund numerous legal challenges to voting exclusions and segregation.
Washington repudiated the abolitionist emphasis on unceasing agitation for full equality, advising blacks that it was counterproductive to fight segregation at this point. Foner concludes that Washington's strong support in the black community was rooted in its widespread realization that frontal assaults on white supremacy were impossible, and the best way forward was to concentrate on building up the economic and social structures inside segregated communities. C. Vann Woodward concluded, "The businessman's gospel of free enterprise, competition, and "laissez faire" never had a more loyal exponent."
Historians since the late 20th century have been divided in their characterization of Washington: some describe him as a visionary capable of "read[ing] minds with the skill of a master psychologist," who expertly played the political game in 19th century Washington by its own rules. Others say he was a self-serving, crafty narcissist who threatened and punished those in the way of his personal interests, traveled with an entourage and spent much time fundraising, signing autographs, and giving flowery patriotic speeches with lots of flag waving - acts more indicative of an artful political boss than an altruistic civil rights leader.
People called Washington the "Wizard of Tuskegee" because of his highly developed political skills, and his creation of a nationwide political machine based on the black middle class, white philanthropy, and Republican Party support. Opponents called this network the "Tuskegee Machine." Washington maintained control because of his ability to gain support of numerous groups, including influential whites and the black business, educational and religious communities nationwide. He advised on the use of financial donations from philanthropists, and avoided antagonizing white Southerners with his accommodation to the political realities of the age of Jim Crow segregation.
References.
Secondary sources.
</dl>
Primary sources.
</dl>

</doc>
<doc id="37243" url="http://en.wikipedia.org/wiki?curid=37243" title="Harriet Tubman">
Harriet Tubman

Harriet Tubman (born Araminta Ross; 1822 – March 10, 1913) was an African-American abolitionist, humanitarian, and during the American Civil War, a Union spy. Born into slavery, Tubman escaped and subsequently made about thirteen missions to rescue approximately seventy enslaved family and friends, using the network of antislavery activists and safe houses known as the Underground Railroad. She later helped John Brown recruit men for his raid on Harpers Ferry, and in the post-war era struggled for women's suffrage.
Early life and education.
Tubman was born Araminta "Minty" Ross to slave parents, Harriet ("Rit") Green and Ben Ross. Rit was owned by Mary Pattison Brodess (and later her son Edward). Ben was held by Anthony Thompson, who became Mary's second husband, and who ran a large plantation near Blackwater River in Madison, Maryland. As with many slaves in the United States, neither the exact year nor place of Araminta's birth is known, and historians differ as to the best estimate. Kate Larson records the year as 1822, based on a midwife payment and several other historical documents, including her runaway advertisement, while Jean Humez says "the best current evidence suggests that Tubman was born in 1820, but it might have been a year or two later." Catherine Clinton notes that Tubman reported the year of her birth as 1825, while her death certificate lists 1815 and her gravestone lists 1820. In her Civil War widow's pension records, Tubman claimed she was born in 1820, 1822, and 1825, an indication, perhaps, that she had only a general idea of when she was born.
Modesty, Tubman's maternal grandmother, arrived in the United States on a slave ship from Africa; no information is available about her other ancestors. As a child, Tubman was told that she was of Ashanti lineage (from what is now Ghana), though no evidence exists to confirm or deny this assertion. Her mother Rit (who may have had a white father) was a cook for the Brodess family. Her father Ben was a skilled woodsman who managed the timber work on Thompson's plantation. They married around 1808 and, according to court records, they had nine children together: Linah, Mariah Ritty, Soph, Robert, Minty (Harriet), Ben, Rachel, Henry, and Moses.
Rit struggled to keep her family together as slavery threatened to tear it apart. Edward Brodess sold three of her daughters (Linah, Mariah Ritty, and Soph), separating them from the family forever. When a trader from Georgia approached Brodess about buying Rit's youngest son, Moses, she hid him for a month, aided by other slaves and free blacks in the community. At one point she confronted her owner about the sale. Finally, Brodess and "the Georgia man" came toward the slave quarters to seize the child, where Rit told them, "You are after my son; but the first man that comes into my house, I will split his head open." Brodess backed away and abandoned the sale. Tubman's biographers agree that stories told about this event within the family influenced her belief in the possibilities of resistance.
Childhood.
Tubman's mother was assigned to "the big house" and had scarce time for her family; consequently, as a child Tubman took care of a younger brother and baby, as was typical in large families. When she was five or six years old, Brodess hired her out as a nursemaid to a woman named "Miss Susan." She was ordered to keep watch on the baby as it slept; when it woke up and cried, she was whipped. She later recounted a particular day when she was lashed five times before breakfast. She carried the scars for the rest of her life. She found ways to resist, running away for five days, wearing layers of clothing as protection against beatings, and fighting back.
As a child, Tubman also worked at the home of a planter named James Cook. She had to check the muskrat traps in nearby marshes, even after contracting measles. She became so ill that Cook sent her back to Brodess, where her mother nursed her back to health. Brodess then hired her out again. She spoke later of her acute childhood homesickness, comparing herself to "the boy on the Swanee River," an allusion to Stephen Foster's song "Old Folks at Home." As she grew older and stronger, she was assigned to field and forest work, driving oxen, plowing, and hauling logs.
Head injury.
As a child in Dorchester County, Maryland, Tubman was beaten by masters to whom she was hired out. Early in her life, she suffered a severe head wound when hit by a heavy metal weight. The injury caused disabling epileptic seizures, headaches, powerful visions, and dream experiences, which occurred throughout her life. A devout Christian, Tubman ascribed the visions and vivid dreams to revelations from God.
One day, the adolescent Tubman was sent to a dry-goods store for supplies. There, she encountered a slave owned by another family, who had left the fields without permission. His overseer, furious, demanded that she help restrain him. She refused, and as he ran away, the overseer threw a two-pound weight at him. He struck her instead, which she said "broke my skull." She later explained her belief that her hair – which "had never been combed and ... stood out like a bushel basket" – might have saved her life. Bleeding and unconscious, she was returned to her owner's house and laid on the seat of a loom, where she remained without medical care for two days. She was sent back into the fields, "with blood and sweat rolling down my face until I couldn't see." Her boss said she was "not worth a sixpence" and returned her to Brodess, who tried unsuccessfully to sell her. She began having seizures and would seemingly fall unconscious, although she claimed to be aware of her surroundings while appearing to be asleep. These episodes were alarming to her family, who were unable to wake her when she fell asleep suddenly and without warning. This condition remained with her for the rest of her life; Larson suggests she may have suffered from temporal lobe epilepsy as a result of the injury.
The severe head wound occurred when Tubman was becoming deeply religious—although one must note this is not insinuating this as being the chief cause. As an illiterate child, she had been told Bible stories by her mother. The particular variety of her early Christian belief remains unclear, but she acquired a passionate faith in God. She rejected the teachings of the New Testament that urged slaves to be obedient and found guidance in the Old Testament tales of deliverance. Additionally, she began having visions and potent dreams, which she considered signs from the divine. This religious perspective instructed her throughout her life.
Family and marriage.
By 1840, Tubman's father, Ben, was manumitted from slavery at the age of 45, as stipulated in a former owner's will, though his actual age was closer to 55. He continued working as a timber estimator and foreman for the Thompson family, who had held him as a slave. Several years later, Tubman contacted a white attorney and paid him five dollars to investigate her mother's legal status. The lawyer discovered that a former owner had issued instructions that Rit, like her husband, would be manumitted at the age of 45. The record showed that a similar provision would apply to Rit's children, and that any children born after she reached 45 years of age were legally free, but the Pattison and Brodess families had ignored this stipulation when they inherited the slaves. Challenging it legally was an impossible task for Tubman.
Around 1844, she married a free black man named John Tubman. Although little is known about him or their time together, the union was complicated because of her slave status. Since the mother's status dictated that of children, any children born to Harriet and John would be enslaved. Such blended marriages – free people of color marrying enslaved people – were not uncommon on the Eastern Shore of Maryland, where by this time, half the black population was free. Most African-American families had both free and enslaved members. Larson suggests that they might have planned to buy Tubman's freedom.
Tubman changed her name from Araminta to Harriet soon after her marriage, though the exact timing is unclear. Larson suggests this happened right after the wedding, and Clinton suggests that it coincided with Tubman's plans to escape from slavery. She adopted her mother's name, possibly as part of a religious conversion, or to honor another relative.
Escape from slavery.
In 1849, Tubman became ill again, and her value as a slave was diminished as a result. Edward Brodess tried to sell her, but could not find a buyer. Angry at his action and the unjust hold he kept on her relatives, Tubman began to pray for her owner, asking God to make him change his ways.
"I prayed all night long for my master," she said later, "till the first of March; and all the time he was bringing people to look at me, and trying to sell me." When it appeared as though a sale was being concluded, she switched tactics. "I changed my prayer," she said. "First of March I began to pray, 'Oh Lord, if you ain't never going to change that man's heart, kill him, Lord, and take him out of the way."A week later, Brodess died, and Tubman expressed regret for her earlier sentiments.
As in many estate settlements, Brodess's death increased the likelihood of Tubman being sold and her family being broken apart; his widow, Eliza, began working to sell the family's slaves. Tubman refused to wait for the Brodess family to decide her fate, despite her husband's efforts to dissuade her. "[T]here was one of two things I had a right to," she explained later, "liberty or death; if I could not have one, I would have the other."
Tubman and her brothers, Ben and Henry, escaped from slavery on September 17, 1849. Tubman had been hired out to Dr. Anthony Thompson, who owned a large plantation in an area called Poplar Neck in neighboring Caroline County; it is likely her brothers labored for Thompson as well. Because the slaves were hired out to another household, Eliza Brodess probably did not recognize their absence as an escape attempt for some time. Two weeks later, she posted a runaway notice in the Cambridge "Democrat", offering a reward of up to 100 dollars for each slave returned. Once they had left, Tubman's brothers had second thoughts. Ben may have just become a father. The two men went back, forcing Tubman to return with them.
Soon afterward, Tubman escaped again, this time without her brothers. Beforehand, she tried to send word to her mother of her plans. She sang a coded song to Mary, a trusted fellow slave, that was a farewell. "I'll meet you in the morning," she intoned, "I'm bound for the promised land." While her exact route is unknown, Tubman made use of the network known as the Underground Railroad. This informal, but well-organized, system was composed of free and enslaved blacks, white abolitionists, and other activists. Most prominent among the latter in Maryland at the time were members of the Religious Society of Friends, often called Quakers. The Preston area near Poplar Neck in Caroline County contained a substantial Quaker community, and was probably an important first stop during Tubman's escape. From there, she probably took a common route for fleeing slaves - northeast along the Choptank River, through Delaware and then north into Pennsylvania. A journey of nearly 90 miles (145 kilometers), her traveling by foot would have taken between five days and three weeks.
Tubman had to travel by night, guided by the North Star, and trying to avoid slave catchers, eager to collect rewards for fugitive slaves. The "conductors" in the Underground Railroad used deceptions for protection. At an early stop, the lady of the house ordered Tubman to so sweep the yard as to seem to be working for the family. When night fell, the family hid her in a cart and took her to the next friendly house. Given her familiarity with the woods and marshes of the region, Tubman during the day likely hid in these locales. Tubman only later described her routes because other fugitive slaves used them.
Particulars of her first journey remain shrouded in secrecy. She crossed into Pennsylvania with a feeling of relief and awe, and recalled the experience years later:
When I found I had crossed that line, I looked at my hands to see if I was the same person. There was such a glory over everything; the sun came like gold through the trees, and over the fields, and I felt like I was in Heaven.
"Moses".
After reaching Philadelphia, Tubman thought of her family. "I was a stranger in a strange land," she said later. "[M]y father, my mother, my brothers, and sisters, and friends were [in Maryland]. But I was free, and "they" should be free." She worked odd jobs and saved money. The U.S. Congress meanwhile passed the Fugitive Slave Law of 1850, which heavily punished abetting escape and forced law enforcement officials—even in states that had outlawed slavery—to assist in their capture. The law increased risks for escaped slaves, more of whom therefore sought refuge in Southern Ontario (then called the United Province of Canada), which, as part of the British Empire, had abolished slavery. Racial tensions were also increasing in Philadelphia as waves of poor Irish immigrants competed with free blacks for work.
In December 1850 Tubman was warned that her niece Kessiah and her two children, six-year-old James Alfred, and baby Araminta, soon would be sold in Cambridge. Tubman went to Baltimore, where her brother-in-law Tom Tubman hid her until the sale. Kessiah's husband, a free black man named John Bowley, made the winning bid for his wife. Then, while he pretended to make arrangements to pay, Kessiah and their children escaped to a nearby safe house. When night fell, Bowley sailed the family on a log canoe 60 miles (100 kilometers) to Baltimore, where they met with Tubman, who brought the family to Philadelphia.
The next spring she returned to Maryland to help guide away other family members. During her second trip she recovered her brother Moses and two unidentified men. Tubman likely worked with abolitionist Thomas Garrett, a Quaker working in Wilmington, Delaware. Word of her exploits had encouraged her family, and biographers agree that with each trip to Maryland she became more confident. Her leading more individuals from slavery caused abolitionist William Lloyd Garrison to name her "Moses," alluding to the prophet in the Book of Exodus who led the Hebrews to freedom from Egypt.
While being interviewed by author Wilbur Siebert in 1897, Tubman named some of the people who helped her and places that she stayed along the Underground Railroad. She stayed with Sam Green, a free black minister living in East New Market, Maryland; she also hid near her parents' home at Poplar Neck in Caroline County, Maryland. She would travel from there northeast to Sandtown and Willow Grove, Delaware, and to the Camden area where free black agents, William and Nat Brinkley and Abraham Gibbs, guided her north past Dover, Smyrna, and Blackbird, where other agents would take her across the Chesapeake and Delaware Canal to New Castle and Wilmington. In Wilmington, Quaker Thomas Garrett would secure transportation to William Still's office or the homes of other Underground Railroad operators in the greater Philadelphia area. Still, a famous black agent, is credited with aiding hundreds of freedom seekers escape to safer places farther north in New York, New England, and present-day Southern Ontario.
In the fall of 1851, Tubman returned to Dorchester County for the first time since her escape, this time to find her husband, John. She once again saved money from various jobs, purchased a suit for him, and made her way south. John, meanwhile, had married another woman named Caroline. Tubman sent word that he should join her, but he insisted that he was happy where he was. Tubman at first prepared to storm their house and make a scene, but then decided he was not worth the trouble. Suppressing her anger, she found some slaves who wanted to escape and led them to Philadelphia. John and Caroline raised a family together, until he was killed 16 years later in a roadside argument with a white man named Robert Vincent.
Because the Fugitive Slave Law had made the northern United States a more dangerous place for escaped slaves to remain, many escaped slaves began migrating to Southern Ontario. In December 1851, Tubman guided an unidentified group of 11 fugitives, possibly including the Bowleys and several others she had helped rescue earlier, northward. There is evidence to suggest that Tubman and her group stopped at the home of abolitionist and former slave Frederick Douglass. In his third autobiography, Douglass wrote: "On one occasion I had eleven fugitives at the same time under my roof, and it was necessary for them to remain with me until I could collect sufficient money to get them on to Canada. It was the largest number I ever had at any one time, and I had some difficulty in providing so many with food and shelter..." The number of travelers and the time of the visit make it likely that this was Tubman's group.
Douglass and Tubman admired one another greatly as they both struggled against slavery. When an early biography of Tubman was being prepared in 1868, Douglass wrote a letter to honor her. It read in part: You ask for what you do not need when you call upon me for a word of commendation. I need such words from you far more than you can need them from me, especially where your superior labors and devotion to the cause of the lately enslaved of our land are known as I know them. The difference between us is very marked. Most that I have done and suffered in the service of our cause has been in public, and I have received much encouragement at every step of the way. You, on the other hand, have labored in a private way. I have wrought in the day—you in the night. ... The midnight sky and the silent stars have been the witnesses of your devotion to freedom and of your heroism. Excepting John Brown—of sacred memory—I know of no one who has willingly encountered more perils and hardships to serve our enslaved people than you have.
Journeys and methods.
Over eleven years Tubman returned repeatedly to the Eastern Shore of Maryland, rescuing some 70 slaves in about thirteen expeditions, including her three other brothers, Henry, Ben, and Robert, their wives and some of their children. She also provided specific instructions to 50 to 60 additional fugitives who escaped to the north. Tubman's dangerous work required tremendous ingenuity; she usually worked during winter months, to minimize the likelihood that the group would be seen. One admirer of Tubman said: "She always came in the winter, when the nights are long and dark, and people who have homes stay in them." Once she had made contact with escaping slaves, they left town on Saturday evenings, since newspapers would not print runaway notices until Monday morning.
Her journeys into the land of slavery put her at tremendous risk, and she used a variety of subterfuges to avoid detection. Tubman once disguised herself with a bonnet and carried two live chickens to give the appearance of running errands. Suddenly finding herself walking toward a former owner in Dorchester County, she yanked the strings holding the birds' legs, and their agitation allowed her to avoid eye contact. Later she recognized a fellow train passenger as another former master; she snatched a nearby newspaper and pretended to read. Since Tubman was known to be illiterate, the man ignored her.
Tubman's religious faith was another important resource as she ventured repeatedly into Maryland. The visions from her childhood head injury continued, and she saw them as divine premonitions. She spoke of "consulting with God," and trusted that He would keep her safe. Thomas Garrett once said of her, "I never met with any person of any color who had more confidence in the voice of God, as spoken direct to her soul." Her faith in the divine also provided immediate assistance. She used spirituals as coded messages, warning fellow travelers of danger or to signal a clear path.
Tubman also carried a revolver, and was not afraid to use it. The gun afforded some protection from the ever-present slave catchers and their dogs, however she also purportedly threatened to shoot any escaped slave who tried to turn back on the journey since that would threaten the safety of the remaining group. Tubman told the tale of one man who insisted he was going to go back to the plantation when morale got low among a group of fugitive slaves. She pointed the gun at his head and said, "You go on or die." Several days later, he was with the group as they entered the United Province of Canada. 
Slaveholders in the region, meanwhile, never knew that "Minty," the petite, five-foot-tall, disabled slave who had run away years before and never come back, was behind so many slave escapes in their community. By the late 1850s, they began to suspect a northern white abolitionist was secretly enticing their slaves away. They considered that John Brown himself had come to the Eastern Shore to lure slaves away before his ill-fated raid on Harper's Ferry in October 1859.
While a popular legend persists about a reward of US$40,000 for Tubman's capture, this is a manufactured figure. In 1868, in an effort to drum up support for Tubman's claim for a Civil War military pension, a former abolitionist named Salley Holley wrote an article claiming US$40,000 "was not too great a reward for Maryland slaveholders to offer for her." Such a high reward would have garnered national attention, especially at a time when a small farm could be purchased for a mere US$400. No such reward has been found in period newspapers. (The federal government offered $25,000 for the capture of each of John Wilkes Booth's co-conspirators in Lincoln's assassination.) A reward offering of US$12,000 has also been claimed, though no documentation exists for that figure either. Catherine Clinton suggests that the US$40,000 figure may have been a combined total of the various bounties offered around the region.
Despite the best efforts of the slaveholders, Tubman was never captured, and neither were the fugitives she guided. Years later, she told an audience: "I was conductor of the Underground Railroad for eight years, and I can say what most conductors can't say – I never ran my train off the track and I never lost a passenger." One of her last missions into Maryland was to retrieve her aging parents. Her father, Ben, had purchased Rit, her mother, in 1855 from Eliza Brodess for 20 dollars. But even when they were both free, the area became hostile to their presence. Two years later, Tubman received word that her father had harbored a group of eight escaped slaves, and was at risk of arrest. She traveled to the Eastern Shore and led them north to St. Catharines, Ontario, where a community of former slaves (including Tubman's brothers, other relatives, and many friends) had gathered.
John Brown and Harpers Ferry.
In April 1858, Tubman was introduced to the abolitionist John Brown, an insurgent who advocated the use of violence to destroy slavery in the United States. Although she never advocated violence against whites, she agreed with his course of direct action and supported his goals. Like Tubman, he spoke of being called by God, and trusted the divine to protect him from the wrath of slaveholders. She, meanwhile, claimed to have had a prophetic vision of meeting Brown before their encounter.
Thus, as he began recruiting supporters for an attack on slaveholders, Brown was joined by "General Tubman," as he called her. Her knowledge of support networks and resources in the border states of Pennsylvania, Maryland and Delaware was invaluable to Brown and his planners. Although other abolitionists like Frederick Douglass and William Lloyd Garrison did not endorse his tactics, Brown dreamed of fighting to create a new state for freed slaves, and made preparations for military action. After he began the first battle, he believed, slaves would rise up and carry out a rebellion across the south. He asked Tubman to gather former slaves then living in present-day Southern Ontario who might be willing to join his fighting force, which she did.
On May 8, 1858, Brown held a meeting in Chatham-Kent, Ontario, where he unveiled his plan for a raid on Harpers Ferry, Virginia. When word of the plan was leaked to the government, Brown put the scheme on hold and began raising funds for its eventual resumption. Tubman aided him in this effort, and with more detailed plans for the assault.
Tubman was busy during this time, giving talks to abolitionist audiences and tending to her relatives. In the autumn of 1859, as Brown and his men prepared to launch the attack, Tubman could not be contacted. When the raid on Harpers Ferry took place on October 16, Tubman was not present. Some historians believe she was in New York at the time, ill with fever related to her childhood head injury. Others propose she may have been recruiting more escaped slaves in Ontario, and Kate Clifford Larson suggests she may have been in Maryland, recruiting for Brown's raid or attempting to rescue more family members. Larson also notes that Tubman may have begun sharing Frederick Douglass's doubts about the viability of the plan.
The raid failed; Brown was convicted of treason and hanged in December. His actions were seen by abolitionists as a symbol of proud resistance, carried out by a noble martyr. Tubman herself was effusive with praise. She later told a friend: "[H]e done more in dying, than 100 men would in living."
Auburn and Margaret.
In early 1859, abolitionist Republican U.S. Senator William H. Seward sold Tubman a small piece of land on the outskirts of Auburn, New York for US$1,200. The city was a hotbed of antislavery activism, and Tubman seized the opportunity to deliver her parents from the harsh Canadian winters. Returning to the U.S. meant that escaped slaves were at risk of being returned to the south under the Fugitive Slave Law, and Tubman's siblings expressed reservations. Catherine Clinton suggests that anger over the 1857 Dred Scott decision may have prompted Tubman to return to the U.S. Her land in Auburn became a haven for Tubman's family and friends. For years, she took in relatives and boarders, offering a safe place for black Americans seeking a better life in the north.
Shortly after acquiring the Auburn property, Tubman went back to Maryland and returned with her "niece," an eight-year-old light-skinned black girl named Margaret. The circumstances of this expedition remain clouded in mystery. There is great confusion about the identity of Margaret's parents, although Tubman indicated they were free blacks. The girl had left behind a twin brother and a loving home in Maryland. Years later, Margaret's daughter Alice called Tubman's actions selfish, saying, "she had taken the child from a sheltered good home to a place where there was nobody to care for her." Indeed, Alice described it as a "kidnapping."
However, both Clinton and Larson present the possibility that Margaret was in fact Tubman's daughter. Larson points out that the two shared an unusually strong bond, and argues that Tubman – knowing the pain of a child separated from her mother – would never have intentionally caused a free family to be split apart. Clinton presents evidence of strong physical similarities, which Alice herself acknowledged. Both historians agree that no concrete evidence exists for such a possibility, and the mystery of Tubman's relationship with young Margaret remains to this day.
In November 1860, Tubman conducted her last rescue mission. Throughout the 1850s, Tubman had been unable to effect the escape of her sister, Rachel, and Rachel's two children, Ben and Angerine. Upon returning to Dorchester County, Tubman discovered that Rachel had died, and the children could only be rescued if she could pay a US$30 bribe. She had no money, so the children remained enslaved. Their fates remain unknown. Never one to waste a trip, Tubman gathered another group, including the Ennalls family, ready and willing to take the risks of the journey north. It took them weeks to safely get away because of slave catchers, forcing them to hide out longer than expected. The weather was unseasonably cold and they had little food. The children were drugged with paregoric to keep them quiet while slave patrols rode by. They safely reached the home of David and Martha Wright in Auburn, New York on December 28, 1860.
American Civil War.
When the American Civil War broke out in 1861, Tubman saw a Union victory as a key step toward the abolition of slavery. General Benjamin Butler, for instance, aided escaped slaves flooding into Fort Monroe. Butler had declared these fugitives to be "contraband" – property seized by northern forces – and put them to work without pay in the fort. Tubman hoped to offer her own expertise and skills to the Union cause, too, and soon she joined a group of Boston and Philadelphia abolitionists heading to the Hilton Head District in South Carolina. She became a fixture in the camps, particularly in Port Royal, South Carolina, assisting fugitives.
Tubman soon met with General David Hunter, a strong supporter of abolition. He declared all of the "contrabands" in the Port Royal district free, and began gathering former slaves for a regiment of black soldiers. U.S. President Abraham Lincoln, however, was not prepared to enforce emancipation on the southern states, and reprimanded Hunter for his actions. Tubman condemned Lincoln's response and his general unwillingness to consider ending slavery in the U.S., for both moral and practical reasons. "God won't let master Lincoln beat the South till he does "the right thing"," she said. Master Lincoln, he's a great man, and I am a poor negro; but the negro can tell master Lincoln how to save the money and the young men. He can do it by setting the negro free. Suppose that was an awful big snake down there, on the floor. He bite you. Folks all scared, because you die. You send for a doctor to cut the bite; but the snake, he rolled up there, and while the doctor doing it, he bite you "again". The doctor dug out "that" bite; but while the doctor doing it, the snake, he spring up and bite you again; so he "keep" doing it, till you kill "him". That's what master Lincoln ought to know.
Tubman served as a nurse in Port Royal, preparing remedies from local plants and aiding soldiers suffering from dysentery. She rendered assistance to men with smallpox; that she did not contract the disease herself started more rumors that she was blessed by God. At first, she received government rations for her work, but newly freed blacks thought she was getting special treatment. To ease the tension, she gave up her right to these supplies and made money selling pies and root beer, which she made in the evenings.
Scouting and the Combahee River Raid.
When President Lincoln finally issued the Emancipation Proclamation in January 1863, Tubman considered it an important step toward the goal of liberating all black men, women, and children from slavery. She renewed her support for a defeat of the Confederacy, and before long she was leading a band of scouts through the land around Port Royal. The marshes and rivers in South Carolina were similar to those of the Eastern Shore of Maryland; thus her knowledge of covert travel and subterfuge among potential enemies were put to good use. Her group, working under the orders of Secretary of War Edwin M. Stanton, mapped the unfamiliar terrain and reconnoitered its inhabitants. She later worked alongside Colonel James Montgomery, and provided him with key intelligence that aided the capture of Jacksonville, Florida.
Later that year, Tubman became the first woman to lead an armed assault during the Civil War. When Montgomery and his troops conducted an assault on a collection of plantations along the Combahee River, Tubman served as a key adviser and accompanied the raid. On the morning of June 2, 1863, Tubman guided three steamboats around Confederate mines in the waters leading to the shore. Once ashore, the Union troops set fire to the plantations, destroying infrastructure and seizing thousands of dollars worth of food and supplies. When the steamboats sounded their whistles, slaves throughout the area understood that it was being liberated. Tubman watched as slaves stampeded toward the boats. "I never saw such a sight," she said later, describing a scene of chaos with women carrying still-steaming pots of rice, pigs squealing in bags slung over shoulders, and babies hanging around their parents' necks. Although their owners, armed with handguns and whips, tried to stop the mass escape, their efforts were nearly useless in the tumult. As Confederate troops raced to the scene, steamboats packed full of slaves took off toward Beaufort.
More than 750 slaves were rescued in the Combahee River Raid. Newspapers heralded Tubman's "patriotism, sagacity, energy, [and] ability," and she was praised for her recruiting efforts - most of the newly liberated men went on to join the Union army. Tubman later worked with Colonel Robert Gould Shaw at the assault on Fort Wagner, reportedly serving him his last meal. She described the battle by saying: "And then we saw the lightning, and that was the guns; and then we heard the thunder, and that was the big guns; and then we heard the rain falling, and that was the drops of blood falling; and when we came to get the crops, it was dead men that we reaped."
For two more years, Tubman worked for the Union forces, tending to newly liberated slaves, scouting into Confederate territory, and eventually nursing wounded soldiers in Virginia. She also made periodic trips back to Auburn, to visit her family and care for her parents. The Confederacy surrendered in April 1865; after donating several more months of service, Tubman headed home.
Tubman returned to Auburn at the end of the war. During a train ride to New York, the conductor told her to move into the smoking car. She refused, explaining her government service. He cursed at her and grabbed her, but she resisted and he summoned two other passengers for help. While she clutched at the railing, they muscled her away, breaking her arm in the process. They threw her into the smoking car, causing more injuries. As these events transpired, other white passengers cursed Tubman and shouted for the conductor to kick her off the train.
Despite her years of service, she had never received a regular salary and was for years denied compensation. Her unofficial status and the unequal payments offered to black soldiers caused great difficulty in documenting her service, and the U.S. government was slow in recognizing its debt to her. Tubman did not receive a pension for her service in the Civil War until 1899. Her constant humanitarian work for her family and former slaves, meanwhile, kept her in a state of constant poverty, and her difficulties in obtaining a government pension were especially taxing for her.
Later life.
Tubman spent her remaining years in Auburn, tending to her family and other people in need. She worked various jobs to support her elderly parents, and took in boarders to help pay the bills. One of the people Tubman took in was a Civil War veteran named Nelson Davis. He began working in Auburn as a bricklayer, and they soon fell in love. Though he was 22 years younger than she was, on March 18, 1869, they were married at the Central Presbyterian Church. They spent the next 20 years together, and in 1874 they adopted a baby girl named Gertie.
Tubman's friends and supporters from the days of abolition, meanwhile, raised funds to support her. One admirer, Sarah Hopkins Bradford, wrote an authorized biography entitled "Scenes in the Life of Harriet Tubman". The 132-page volume was published in 1869, and brought Tubman some US$1,200 in revenue. Criticized by modern biographers for its artistic license and highly subjective point of view, the book nevertheless remains an important source of information and perspective on Tubman's life. Bradford released another volume in 1886 called "Harriet, the Moses of her People", which presented a less caustic view of slavery and the South. It, too, was published as a way to help alleviate Tubman's poverty.
Because of the debt she had accumulated (including delayed payment for her property in Auburn), Tubman fell prey in 1873 to a swindle involving gold transfer. Two men, one named Stevenson and the other John Thomas, claimed to have in their possession a cache of gold smuggled out of South Carolina. They offered this treasure – worth about US$5,000, they claimed – for US$2,000 in cash. They insisted that they knew a relative of Tubman's, and she took them into her home, where they stayed for several days. She knew that white people in the South had buried valuables when Union forces threatened the region, and also that black men were frequently assigned to digging duties. Thus the situation seemed plausible, and a combination of her financial woes and her good nature led her to go along with the plan. She borrowed the money from a wealthy friend named Anthony Shimer, and arranged to receive the gold late one night. Once the men had lured her into the woods, however, they attacked her and knocked her out with chloroform, then stole her purse and bound and gagged her. When she was found by her family, she was dazed and injured, and the money was gone.
New York responded with outrage to the incident, and while some criticized Tubman for her naïveté, most sympathized with her economic hardship and lambasted the con men. The incident refreshed the public's memory of her past service and her economic woes. Wisconsin Representative Gerry W. Hazelton introduced a bill (H.R. 3786) providing that Tubman be paid "the sum of $2,000 for services rendered by her to the Union Army as scout, nurse, and spy..." It was defeated.
Suffragist activism.
In her later years, Tubman worked to promote the cause of women's suffrage. A white woman once asked Tubman whether she believed women ought to have the vote, and received the reply: "I suffered enough to believe it." Tubman began attending meetings of suffragist organizations, and was soon working alongside women such as Susan B. Anthony and Emily Howland.
Tubman traveled to New York, Boston, and Washington, D.C. to speak out in favor of women's voting rights. She described her actions during and after the Civil War, and used the sacrifices of countless women throughout modern history as evidence of women's equality to men. When the National Federation of Afro-American Women was founded in 1896, Tubman was the keynote speaker at its first meeting.
This wave of activism kindled a new wave of admiration for Tubman among the press in the United States. A publication called "The Woman's Era" launched a series of articles on "Eminent Women" with a profile of Tubman. An 1897 suffragist newspaper reported a series of receptions in Boston honoring Tubman and her lifetime of service to the nation. However, her endless contributions to others had left her in poverty, and she had to sell a cow to buy a train ticket to these celebrations.
AME Zion Church, illness, and death.
At the turn of the 20th century, Tubman became heavily involved with the African Methodist Episcopal Zion Church in Auburn. In 1903, she donated a parcel of real estate she owned to the church, under the instruction that it be made into a home for "aged and indigent colored people." The home did not open for another five years, and Tubman was dismayed when the church ordered residents to pay a $100 entrance fee. She said: "[T]hey make a rule that nobody should come in without they have a hundred dollars. Now I wanted to make a rule that nobody should come in unless they didn't have no money at all." She was frustrated by the new rule but was the guest of honor nonetheless when the Harriet Tubman Home for the Aged celebrated its opening on June 23, 1908.
As Tubman aged, the seizures, headaches, and suffering from her childhood head trauma continued to plague her. At some point in the late 1890s, she underwent brain surgery at Boston's Massachusetts General Hospital. Unable to sleep because of pains and "buzzing" in her head, she asked a doctor if he could operate. He agreed and, in her words, "sawed open my skull, and raised it up, and now it feels more comfortable." She had received no anesthesia for the procedure and reportedly chose instead to bite down on a bullet, as she had seen Civil War soldiers do when their limbs were amputated.
By 1911, her body was so frail that she had to be admitted into the rest home named in her honor. A New York newspaper described her as "ill and penniless," prompting supporters to offer a new round of donations. Surrounded by friends and family members, Harriet Tubman died of pneumonia in 1913. Just before she died, she told those in the room: "I go to prepare a place for you."
Legacy.
Harriet Tubman, widely known and well-respected while she was alive, became an American icon in the years after she died. A survey at the end of the 20th century named her as one of the most famous civilians in American history before the Civil War, third only to Betsy Ross and Paul Revere. She inspired generations of African Americans struggling for equality and civil rights; she was praised by leaders across the political spectrum.
When she died, Tubman was buried with semi-military honors at Fort Hill Cemetery in Auburn. The city commemorated her life with a plaque on the courthouse. Although it showed pride for her many achievements, its use of dialect ("I nebber run my train off de track"), apparently chosen for its authenticity, has been criticized for undermining her stature as an American patriot and dedicated humanitarian. Still, the dedication ceremony was a powerful tribute to her memory, and Booker T. Washington delivered the keynote address. The Harriet Tubman home was abandoned after 1920, but was later renovated by the AME Zion Church. Today, it welcomes visitors as a museum and education center.
Tubman was celebrated in many other ways throughout the nation in the 20th century. Dozens of schools were named in her honor, and both the Harriet Tubman Home in Auburn and the Harriet Tubman Museum in Cambridge serve as monuments to her life. In 1937 the gravestone for Harriet Tubman Davis was erected by the Empire State Federation of Women's Clubs; it was listed on the National Register of Historic Places in 1999. In 1944, the United States Maritime Commission launched the "SS Harriet Tubman", its first Liberty ship ever named for a black woman. In 1978, the United States Postal Service issued a stamp in honor of Tubman as the first in a series honoring African Americans. In March 2013, President Barack Obama signed a proclamation creating Harriet Tubman Underground Railroad National Monument on the Eastern Shore.
Tubman is commemorated together with Elizabeth Cady Stanton, Amelia Bloomer, and Sojourner Truth in the calendar of saints of the Episcopal Church on July 20. The calendar of saints of the Evangelical Lutheran Church in America remembers Tubman and Sojurner Truth on March 10.
In 1999, the Canadian government designated the Salem Chapel, British Methodist Episcopal Church in St. Catharines a National Historic Site because of its association with Tubman.
In 2002, scholar Molefi Kete Asante included Harriet Tubman on his list of the 100 Greatest African Americans. In 2008, Towson University named a new residence hall after Tubman.
In 2014 the asteroid was named after Harriet Tubman.
On February 1, 2014, Tubman was honored by being featured in a Google Doodle celebrating the first day of Black History Month.
Historiography.
Bradford's biographies were followed by Earl Conrad's "Harriet Tubman: Negro Soldier and Abolitionist". Conrad had experienced great difficulty in finding a publisher – the search took four years – and endured disdain and contempt for his efforts to construct a more objective, detailed account of Tubman's life for adults. Several highly dramatized versions of Tubman's life had been written for children, and many more came later, but Conrad wrote in an academic style to document the historical importance of her work for scholars and the nation's memory. The book was finally published by Carter G. Woodson's Associated Publishers in 1942. Despite her popularity and significance, another Tubman biography for adults did not appear for 60 years, until Jean Humez published a close reading of Tubman's life stories in 2003, and Larson and Clinton both published their biographies in 2004.
National Historic Site and Person of Canada.
In southern Ontario, the Salem Chapel BME Church was designated a National Historic Site of Canada in 1999, on the recommendation of the Historic Sites and Monuments Board of Canada. The chapel, in St. Catharines, Ontario, was a focus of Harriet Tubman's years in the city, when she lived nearby, in what was a major terminus of the Underground Railroad and center of abolitionist work. In Tubman's time, the chapel was known as Bethel Chapel, and was initially part of the African Methodist Episcopal (AME) Church, prior to a change in 1856.
Tubman, herself, was designated a National Historic Person of Canada after the Historic Sites and Monuments Board recommended it, in 2005. Several historical markers—two federal, one provincial, and one civic—surround the active chapel, as well as a bust of Tubman.
National Park designations.
As early as 2008, stakeholders in Maryland and New York, as well as their federal representatives, pushed for legislation to establish two national historical parks honoring Harriet Tubman: one at her place of birth on Maryland's eastern shore, the route of the Underground Railroad in Caroline, Dorchester, and Talbot counties, Maryland; and a second to include her home in Auburn, NY. For the next six years, bills to do so were introduced but never advanced due to one reason or another. Meantime, President Barack Obama used executive authority to create Harriet Tubman Underground Railroad National Monument in 2013, carved from the federal lands on Maryland's Eastern Shore at Blackwater National Wildlife Refuge. Then, in December 2014, authorization for the national historical parks was written into 2015's National Defense Authorization Act. Despite some opposition to the insertion, the bill as a whole advanced, passed, and was signed into law by the president on December 19, 2014, authorizing Harriet Tubman National Historical Park in Cayuga County, New York, pending the acquisition of lands; and immediately creating Harriet Tubman Underground Railroad National Historical Park in Maryland. The latter was created from within the authorized boundary of the national monument, while permitting additional acquisitions.

</doc>
<doc id="37245" url="http://en.wikipedia.org/wiki?curid=37245" title="Radionuclide">
Radionuclide

A radionuclide or radioactive nuclide is a nuclide that is radioactive. Also referred to as a radioisotope or radioactive isotope, it is an isotope with an unstable nucleus, characterized by excess energy available to be imparted either to a newly created radiation particle within the nucleus or via internal conversion. During this process, the radionuclide is said to undergo radioactive decay, resulting in the emission of gamma ray(s) and/or subatomic particles such as alpha or beta particles. These emissions constitute ionizing radiation. Many radionuclides occur naturally, and others are produced artificially, for example in nuclear reactors and cyclotrons.
There are about 650 radionuclides with half-lives longer than 60 minutes (see list of nuclides). Of these, 34 are primordial radionuclides that existed before the creation of the solar system, and there are another 50 radionuclides detectable in nature as daughters of these, or produced naturally on Earth by cosmic radiation. There is a much larger number of radionuclides, more than 2400, with decay half-lives shorter than 60 minutes. Most of these are only produced artificially, and have very short half-lives. For comparison, there are about 254 stable nuclides.
All chemical elements have radionuclides. Even the lightest element, hydrogen, has a well-known radionuclide, tritium. Elements heavier than lead, and the elements technetium and promethium, exist only as radionuclides.
Radionuclides with suitable half-lives play an important part in a number of technologies, for example ionization smoke detectors and nuclear medicine. A pharmaceutical drug made with radionuclides is called a radiopharmaceutical, and an imaging tracer made with radionuclides is called a radioactive tracer. Nuclear medicine makes use of these drugs and tracers for radiation therapy such as brachytherapy and medical imaging.
Radionuclides can also present both real and perceived dangers to health.
Origin.
Naturally occurring radionuclides fall into three categories: primordial radionuclides, secondary radionuclides, and cosmogenic radionuclides. Primordial radionuclides, such as uranium and thorium, originate mainly from the interiors of stars and are still present as their half-lives are so long they have not yet completely decayed. Secondary radionuclides are radiogenic isotopes derived from the decay of primordial radionuclides. They have shorter half-lives than primordial radionuclides. Cosmogenic isotopes, such as carbon-14, are present because they are continually being formed in the atmosphere due to cosmic rays.
Artificially produced radionuclides can be produced by nuclear reactors, particle accelerators or by radionuclide generators:
Trace radionuclides are those that occur in tiny amounts in nature either due to inherent rarity or due to half-lives that are significantly shorter than the age of the Earth. Synthetic isotopes are inherently not naturally occurring on Earth, but can be created by nuclear reactions.
Uses.
Radionuclides are used in two major ways: for their chemical properties and as sources of radiation.
Radionuclides of familiar elements such as carbon can serve as radioactive tracers because they are chemically very similar to the nonradioactive nuclides, so most chemical, biological, and ecological processes treat them in a nearly identical way. One can then examine the result with a radiation detector, such as a Geiger counter, to determine where the provided atoms ended up. For example, one might culture plants in an environment in which the carbon dioxide contained radioactive carbon; then the parts of the plant that had laid down atmospheric carbon would be radioactive.
In nuclear medicine, radioisotopes are used for diagnosis, treatment, and research. Radioactive chemical tracers emitting gamma rays or positrons can provide diagnostic information about a person's internal anatomy and the functioning of specific organs. This is used in some forms of tomography: single-photon emission computed tomography and positron emission tomography scanning and Cherenkov luminescence imaging.
Radioisotopes are also a method of treatment in hemopoietic forms of tumors; the success for treatment of solid tumors has been limited. More powerful gamma sources sterilise syringes and other medical equipment.
In biochemistry and genetics, radionuclides label molecules and allow tracing chemical and physiological processes occurring in living organisms, such as DNA replication or amino acid transport.
In food preservation, radiation is used to stop the sprouting of root crops after harvesting, to kill parasites and pests, and to control the ripening of stored fruit and vegetables.
In industry, and in mining, radionuclides examine welds, to detect leaks, to study the rate of wear, erosion and corrosion of metals, and for on-stream analysis of a wide range of minerals and fuels.
In particle physics, radionuclides help discover new physics (physics beyond the Standard Model) by measuring the energy and momentum of their beta decay products.
Radionuclides are also used to trace and analyze pollutants, to study the movement of surface water, and to measure water runoffs from rain and snow, as well as the flow rates of streams and rivers. Natural radionuclides are used in geology, archaeology, and paleontology to measure ages of rocks, minerals, and fossil materials.
Common examples.
Americium-241.
Most household smoke detectors contain americium produced in nuclear reactors. The radioisotope used is americium-241.
The element americium is created by bombarding plutonium with neutrons in a nuclear reactor. Its isotope americium-241 decays by emitting alpha particles and gamma radiation to become neptunium-237.
Most common household smoke detectors use a very small quantity of 241Am (about 0.29 micrograms per smoke detector) in the form of americium dioxide. Smoke detectors use 241Am since the alpha particles it emits collide with oxygen and nitrogen particles in the air. This occurs in the detector's ionization chamber where it produces charged particles or ions. Then, these charged particles are collected by a small electric voltage that will create an electric current that will pass between two electrodes. Then, the ions that are flowing between the electrodes will be neutralized when coming in contact with smoke, thereby decreasing the electric current between the electrodes, which will activate the detector's alarm.
Steps for creating americium-241.
The plutonium-241 is formed in any nuclear reactor by neutron capture from uranium-238.
This will decay both in the reactor and subsequently to form 241Am, which has a half-life of 432.2 years.
Gadolinium-153.
The 153Gd isotope is used in X-ray fluorescence and osteoporosis screening. It is a gamma-emitter with an 8-month half-life, making it easier to use for medical purposes. In nuclear medicine, it serves to calibrate the equipment needed like single-photon emission computed tomography systems (SPECT) to make x-rays. It ensures that the machines work correctly to produce images of radioisotope distribution inside the patient. This isotope is produced in a nuclear reactor from europium or enriched gadolinium. It can also detect the loss of calcium in the hip and back bones, allowing the ability to diagnose osteoporosis.
Dangers.
Radionuclides that find their way into the environment may cause harmful effects as radioactive contamination. They can also cause damage if they are excessively used during treatment or in other ways exposed to living beings, by radiation poisoning. Potential health damage from exposure to radionuclides depends on a number of factors, and "can damage the functions of healthy tissue/organs. Radiation exposure can produce effects ranging from skin redness and hair loss, to radiation burns and acute radiation syndrome. Prolonged exposure can lead to cells being damaged and in turn lead to cancer. Signs of cancerous cells might not show up until years, or even decades, after exposure."
Summary table for classes of nuclides, "stable" and radioactive.
Following is a summary table for the total list of nuclides with half-lives greater than one hour. Ninety of these 905 nuclides are theoretically stable, except to proton-decay (which has never been observed). About 254 nuclides have never been observed to decay, and are classically considered stable.
The remaining 650 radionuclides have half-lives longer than 1 hour, and are well-characterized (see list of nuclides for a complete tabulation). They include 28 nuclides with measured half-lives longer than the estimated age of the universe (13.8 billion years), and another 6 nuclides with half-lives long enough (> 80 million years) that they are radioactive primordial nuclides, and may be detected on Earth, having survived from their presence in interstellar dust since before the formation of the solar system, about 4.6 billion years ago. Another ~51 short-lived nuclides can be detected naturally as daughters of longer-lived nuclides or cosmic-ray products. The remaining known nuclides are known solely from artificial nuclear transmutation.
Numbers are not exact, and may change slightly in the future, as "stable nuclides" are observed to be radioactive with very long half-lives.
This is a summary table for the 905 nuclides with half-lives longer than one hour (including those that are stable), given in list of nuclides.
List of commercially available radionuclides.
This list covers common isotopes, most of which are available in very small quantities to the general public in most countries. Others that are not publicly accessible are traded commercially in industrial, medical, and scientific fields and are subject to government regulation. For a complete list of all known isotopes for every element (minus activity data), see List of nuclides and Isotope lists. For a table, see Table of nuclides.

</doc>
<doc id="37250" url="http://en.wikipedia.org/wiki?curid=37250" title="Scooby Gang (Buffy the Vampire Slayer)">
Scooby Gang (Buffy the Vampire Slayer)

The Scooby Gang, or "Scoobies", are a group of characters in the cult television series and comic book "Buffy the Vampire Slayer" who battle the supernatural forces of evil. The team consists of Buffy Summers and her friends and colleagues who assist her in her duties as the Slayer. First forming in the Season One episode "The Harvest" to prevent The Master from opening a portal to hell, the line-up of the group varied from year to year, but the core that remained intact throughout the series' run was Buffy herself and her best friends, Xander Harris and Willow Rosenberg, as well as her Watcher, Rupert Giles. This group was also called the "slayerettes."
The group was first referred to as the Scooby Gang by Xander Harris in the Season Two episode "What's My Line, Part One" as a reference to the group of monster-hunting teenagers from the Hanna-Barbera animated series "Scooby-Doo" that is otherwise known as Mystery Inc., who investigate and solve supernatural/occult monsters mysteries much like The Scoobies in Buffy. The first visible sign was in Season 1, Episode 11, where Willow is seen wearing a Scooby Doo T-shirt, Buffy sports the Daphne trademark neck scarf, and later Xander responds, "Can you say, 'Gulp'?" as Shaggy often does. Sarah Michelle Gellar, the actress who plays Buffy, would later also appear as Daphne Blake in the films "Scooby-Doo" and "".
The Scooby Gang usually take an unimpressed, flippant attitude towards extraordinary events and supernatural occurrences. Originally a group of teenage friends attending Sunnydale High in a town built on top of a Hellmouth, the group gradually expanded, eventually merging with the Watchers' Council to become a global organization dealing with demonic threats worldwide and training thousands of Slayers. In the beginning, Buffy often had to protect her friends from monsters, but over the course of the series most of them gained superpowers and fighting skills of their own, with Xander the only notable member with no mystical abilities or connection to the supernatural, though he does acquire extensive military knowledge thanks to a spell. Due to the show's strong themes of female empowerment, the most powerful members of the team are often female (Buffy and Willow), while the men play passive roles such as that of father-figure (Giles) and supportive best friend (Xander). Also, in keeping with the show's themes of redemption and moral ambiguity, Willow, Angel, Spike, Faith, Giles, Anya, and Andrew have at some point in their lives descended into darkness. Each of these characters has murdered at least one person during their life, although for most it was substantially more.
At least initially, in "Season Eight" (a canonical continuation of the series in comic book format), the Scoobies are scattered across the world. In addition to the primary gang, there are short of 2000 Slayers, many of whom work with the expanded organization in addition to witches, psychics and other staff. The core Scoobies now each act as important figures in the new Watcher's Council and maintain a role of leadership and authority amongst the Slayers they employ. With the group scattered, communication is maintained primarily through phone calls, and the group leaders (in particular the core three) now assume more superheroic positions and parallels than in previous seasons. Former allies in Season 7 such as Andrew and Rona now lead squads of their own.
Alternative names.
The group is also sometimes referred to as the "Slayerettes," mostly in early episodes or later in the scene descriptions to refer to the Potentials, but mostly it didn't catch on with fans. Both Willow (in the season one episode "Witch") and Spike (in the season four episodes "The I in Team" and "The Yoko Factor") have used the term in the show on different occasions.
Headquarters.
Along with variations in the team roster, the place which serves as a sort of "headquarters" for the Scooby Gang has also changed a few times over the series. The most notable of these headquarters, before its explosion in the Season Three finale, is the Sunnydale High School library. Rupert Giles' apartment later becomes a temporary base for the gang in Season 4, until Giles becomes the new owner of the magic shop in Season 5, "The Magic Box," where it becomes replacement for the school library. However, after the events of the series finale, the Scoobies have, as stated above, been spread across the world, with Buffy and Xander maintaining a "command center" in Scotland.
To the crew of "Buffy", the headquarters was the set where the most exposition writing of the show took place, and thus was the most used and most hated of the "Buffy" sets.

</doc>
<doc id="37252" url="http://en.wikipedia.org/wiki?curid=37252" title="John Knox">
John Knox

John Knox (c. 1514 – 24 November 1572) was a Scottish clergyman and writer who was a leader of the Protestant Reformation and is considered the founder of the Presbyterian denomination in Scotland. He is believed to have been educated at the University of St Andrews and worked as a notary-priest. Influenced by early church reformers such as George Wishart, he joined the movement to reform the Scottish church. He was caught up in the ecclesiastical and political events that involved the murder of Cardinal Beaton in 1546 and the intervention of the regent of Scotland Mary of Guise. He was taken prisoner by French forces the following year and exiled to England on his release in 1549.
While in exile, Knox was licensed to work in the Church of England, where he rose in the ranks to serve King Edward VI of England as a royal chaplain. He exerted a reforming influence on the text of the "Book of Common Prayer". In England he met and married his first wife, Margery Bowes. When Mary Tudor ascended the throne and re-established Roman Catholicism, Knox was forced to resign his position and leave the country. Knox moved to Geneva and then to Frankfurt. In Geneva he met John Calvin, from whom he gained experience and knowledge of Reformed theology and Presbyterian polity. He created a new order of service, which was eventually adopted by the reformed church in Scotland. He left Geneva to head the English refugee church in Frankfurt but he was forced to leave over differences concerning the liturgy, thus ending his association with the Church of England.
On his return to Scotland he led the Protestant Reformation in Scotland, in partnership with the Scottish Protestant nobility. The movement may be seen as a revolution, since it led to the ousting of Mary of Guise, who governed the country in the name of her young daughter Mary, Queen of Scots. Knox helped write the new confession of faith and the ecclesiastical order for the newly created reformed church, the Kirk. He continued to serve as the religious leader of the Protestants throughout Mary's reign. In several interviews with the Queen, Knox admonished her for supporting Catholic practices. When she was imprisoned for her alleged role in the murder of her husband Lord Darnley, and King James VI enthroned in her stead, he openly called for her execution. He continued to preach until his final days.
Early life, 1505–1546.
John Knox was born sometime between 1505 and 1515 in or near Haddington, the county town of East Lothian. His father, William Knox, was a farmer. All that is known of his mother is that her maiden name was Sinclair and that she died when John Knox was a child.
Knox was probably educated at the grammar school in Haddington. In this time, the priesthood was the only path for those whose inclinations were academic rather than mercantile or agricultural. He proceeded to further studies at the University of St Andrews or possibly at the University of Glasgow. He studied under John Major, one of the greatest scholars of the time.
Knox first appears in public records as a priest and a notary in 1540. He was still serving in these capacities as late as 1543 when he described himself as a "minister of the sacred altar in the diocese of St. Andrews, notary by apostolic authority" in a notarial deed dated 27 March. Rather than taking up parochial duties in a parish, he became tutor to two sons of Hugh Douglas of Longniddry. He also taught the son of John Cockburn of Ormiston. Both of these lairds had embraced the new religious ideas of the Reformation, which were sweeping Europe.
Embracing the Protestant Reformation, 1546–1547.
Knox did not record when or how he was converted to the Protestant faith, but perhaps the key formative influences on Knox were Patrick Hamilton and George Wishart. Wishart was a reformer who had fled Scotland in 1538 to escape punishment for heresy. He first moved to England, where in Bristol he preached against the veneration of the Virgin Mary. He was forced to make a public recantation and was burned in effigy at the Church of St Nicholas as a sign of his abjuration. He then took refuge in Germany and Switzerland. While on the Continent, he translated the First Helvetic Confession into English. He returned to Scotland in 1544, but the timing of his return was unfortunate. In December 1543, James Hamilton, Duke of Châtellerault, the appointed regent for the infant Mary, Queen of Scots, had decided with the Queen Mother, Mary of Guise, and Cardinal David Beaton to persecute the Protestant sect that had taken root in Scotland. Wishart travelled throughout Scotland preaching in favour of the reformation and when he arrived in East Lothian, Knox became one of his closest associates. Knox acted as his bodyguard, bearing a two-handed sword in order to defend him. In December 1545, Wishart was seized on Cardinal Beaton's orders by the Earl of Bothwell and taken to the Castle of St Andrews. Knox was present on the night of Wishart's arrest and was prepared to follow him into captivity, but Wishart persuaded him against this course saying, "Nay, return to your bairns [children] and God bless you. One is sufficient for a sacrifice." Wishart was subsequently prosecuted by Beaton's Public Accuser of Heretics, Archdeacon John Lauder. On 1 March 1546, he was burnt at the stake in the presence of Cardinal Beaton.
Knox had avoided being arrested by Lord Bothwell through Wishart's advice to return to tutoring. He took shelter with Douglas in Longniddry. Several months later he was still in charge of the pupils, the sons of Douglas and Cockburn, who wearied of moving from place to place while being pursued. He toyed with the idea of fleeing to Germany and taking his pupils with him. While Knox remained a fugitive, Cardinal Beaton was murdered on 29 May 1546, within his residence, the Castle of St Andrews, by a gang of five persons in revenge for Wishart's execution. The assassins seized the castle and eventually their families and friends took refuge with them, about a hundred and fifty men in all. Among their friends was Henry Balnaves, a former secretary of state in the government, who negotiated with England for the financial support of the rebels. Douglas and Cockburn suggested to Knox to take their sons to the relative safety of the castle to continue their instruction in reformed doctrine. Knox arrived at the castle on 10 April 1547.
Knox's powers as a preacher came to the attention of the chaplain of the garrison, John Rough. While Rough was preaching in the parish church on the Protestant principle of the popular election of a pastor, he proposed Knox to the congregation for that office. Knox did not relish the idea. According to his own account, he burst into tears and fled to his room. Within a week, however, he was giving his first sermon to a congregation that included his old teacher, John Major. He expounded on the seventh chapter of the Book of Daniel, comparing the Pope with the Antichrist. His sermon was marked by his consideration of the Bible as his sole authority and the doctrine of justification by faith alone, two elements that would remain in his thoughts throughout the rest of his life. A few days later, a debate was staged that allowed him to lay down additional theses including the rejection of the Mass, Purgatory, and prayers for the dead.
Confinement in the French galleys, 1547–1549.
John Knox's chaplaincy of the castle garrison was not to last long. While Hamilton was willing to negotiate with England to stop their support of the rebels and bring the castle back under his control, Mary of Guise decided that it could only be taken by force and requested the king of France, Henry II to intervene. On 29 June 1547, 21 French galleys approached St Andrews under the command of Leone Strozzi, prior of Capua. The French besieged the castle and forced the surrender of the garrison on 31 July. The Protestant nobles and others, including Knox, were taken prisoner and forced to row in the French galleys. The galley slaves were chained to benches and rowed throughout the day without a change of posture while an officer watched over them with a whip in hand. They sailed to France and navigated up the Seine to Rouen. The nobles, some of whom would have an impact later in Knox's life such as William Kirkcaldy and Henry Balnaves, were sent to various castle-prisons in France. Knox and the other galley slaves continued to Nantes and stayed on the Loire throughout the winter. They were threatened with torture if they did not give proper signs of reverence when mass was performed on the ship. Knox recounted an incident in which one Scot—possibly himself, as he tended to narrate personal anecdotes in the third person—was required to show devotion to a picture of the Virgin Mary. The prisoner was told to give it a kiss of veneration. He refused and when the picture was pushed up to his face, the prisoner seized the picture and threw it into the sea, saying, "Let our Lady now save herself: she is light enough: let her learn to swim." After that, according to Knox, the Scottish prisoners were no longer forced to perform such devotions.
In summer 1548, the galleys returned to Scotland to scout for English ships. Knox's health was now at its lowest point due to the severity of his confinement. He was ill with a fever and others on the ship were afraid for his life. Even in this state, Knox recalled, his mind remained sharp and he comforted his fellow prisoners with hopes of release. While the ships were lying offshore between St Andrews and Dundee, the spires of the parish church where he preached appeared in view. James Balfour, a fellow prisoner, asked Knox whether he recognised the landmark. He replied that he knew it well, recognising the steeple of the place where he first preached and he declared that he would not die until he had preached there again.
In February 1549, after spending a total of 19 months in the galley-prison, Knox was released. It is uncertain how he obtained his liberty. Later in the year, Henry II arranged with Edward VI of England the release of all remaining Castilian prisoners.
Exile in England, 1549–1554.
On his release, Knox took refuge in England. The Reformation in England was a less radical movement than its Continental counterparts, but there was a definite breach with Rome. The Archbishop of Canterbury, Thomas Cranmer, and the regent of King Edward VI, the Duke of Somerset, were decidedly Protestant-minded. However, much work remained to bring reformed ideas to the clergy and to the people. On 7 April 1549, Knox was licensed to work in the Church of England. His first commission was in Berwick-upon-Tweed. He was obliged to use the recently released "Book of Common Prayer", which was mainly a translation of the Latin mass into English and was largely left intact and unreformed. He therefore modified its use along Protestant lines. In the pulpit he preached Protestant doctrines with great effect as his congregation grew.
In England, Knox met his wife, Margery Bowes (died c.1560). Her father, Richard Bowes (d.1558), was a descendant of an old Durham family and her mother, Elizabeth Aske, was an heiress of a Yorkshire family, the Askes of Richmondshire. Elizabeth Bowes presumably met Knox when he was employed in Berwick. Several letters reveal a close friendship between them. It is not recorded when Knox married Margery Bowes. Knox attempted to obtain the consent of the Bowes family, but her father and her brother Robert Bowes were opposed to the marriage.
Towards the end of 1550, Knox was appointed a preacher of St Nicholas' Church in Newcastle upon Tyne. The following year he was appointed one of the six royal chaplains serving the King. On 16 October 1551, John Dudley, 1st Duke of Northumberland, overthrew the Duke of Somerset to become the new regent of the young King. Knox condemned the "coup d'état" in a sermon on All Saints Day. When Dudley visited Newcastle and listened to his preaching in June 1552, he had mixed feelings about the fire-brand preacher, but he saw Knox as a potential asset. Knox was asked to come to London to preach before the Court. In his first sermon, he advocated a change for the second edition of the "Book of Common Prayer". The liturgy required worshippers to kneel during communion. Knox and the other chaplains considered this to be idolatry. It triggered a debate where Archbishop Cranmer was called upon to defend the practice. The end result was a compromise in which the famous Black Rubric, which declared that no adoration is intended while kneeling, was included in the second edition.
Soon afterwards, Dudley, who saw Knox as a useful political tool, offered him the bishopric of Rochester. Knox refused, and he returned to Newcastle. On 2 February 1553 Cranmer was ordered to appoint Knox as vicar of Allhallows Church in London placing him under the authority of the Bishop of London, Nicholas Ridley. Knox returned to London in order to deliver a sermon before the King and the Court during Lent and he again refused to take the assigned post. Knox was then told to preach in Buckinghamshire and he remained there until Edward's death on 6 July. Edward's successor, Mary Tudor, re-established Roman Catholicism in England and restored the Mass in all the churches. With the country no longer safe for Protestant preachers, Knox left for the Continent in January 1554 on the advice of friends. On the eve of his flight, he wrote:
Sometime I have thought that impossible it had been, so to have removed my affection from the realm of Scotland, that any realm or nation could have been equal dear to me. But God I take to record in my conscience, that the troubles present (and appearing to be) in the realm of England are double more dolorous unto my heart than ever were the troubles of Scotland.
From Geneva to Frankfurt and Scotland, 1554–1556.
Knox disembarked in Dieppe, France, and continued to Geneva, where John Calvin had established his authority. When Knox arrived Calvin was in a difficult position. He had recently prosecuted the execution of the scholar Michael Servetus for heresy. Knox asked Calvin four difficult political questions: whether a minor could rule by divine right, whether a female could rule and transfer sovereignty to her husband, whether people should obey ungodly or idolatrous rulers, and what party godly persons should follow if they resisted an idolatrous ruler. Calvin gave cautious replies and referred him to the Swiss reformer Heinrich Bullinger in Zürich. Bullinger's responses were equally cautious; but Knox had already made up his mind. On 20 July 1554, he published a pamphlet attacking Mary Tudor and the bishops who had brought her to the throne. He also attacked the Holy Roman Emperor, Charles V, calling him "no less enemy to Christ than was Nero".
In a letter dated 24 September 1554, Knox received an invitation from a congregation of English exiles in Frankfurt to become one of their ministers. He accepted the call with Calvin's blessing. But no sooner had he arrived than he found himself in a conflict. The first set of refugees to arrive in Frankfurt had subscribed to a reformed liturgy and used a modified version of the "Book of Common Prayer". More recently arrived refugees, however, including Edmund Grindal, the future Archbishop of Canterbury, favoured a stricter application of the book. When Knox and a supporting colleague, William Whittingham, wrote to Calvin for advice, they were told to avoid contention. Knox therefore agreed on a temporary order of service based on a compromise between the two sides. This delicate balance was disturbed when a new batch of refugees arrived that included Richard Cox, one of the principal authors of the "Book of Common Prayer". Cox brought Knox's pamphlet attacking the emperor to the attention of the Frankfurt authorities, who advised that Knox leave. His departure from Frankfurt on 26 March 1555 marked his final breach with the Church of England.
After his return to Geneva, Knox was chosen to be the minister at a new place of worship petitioned from Calvin. In the meantime, Elizabeth Bowes wrote to Knox, asking him to return to Margery in Scotland, which he did at the end of August. Despite initial doubts about the state of the Reformation in Scotland, Knox found the country significantly changed since he was carried off in the galley in 1547. When he toured various parts of Scotland preaching the reformed doctrines and liturgy, he was welcomed by many of the nobility including two future regents of Scotland, the Earl of Moray and the Earl of Mar.
Though the Queen Regent, Mary of Guise, made no move to act against Knox, his activities caused concern among the church authorities. The bishops of Scotland viewed him as a threat to their authority and summoned him to appear in Edinburgh on 15 May 1556. He was accompanied to the trial by so many influential persons that the bishops decided to call the hearing off. Knox was now free to preach openly in Edinburgh. William Keith, the Earl Marischal, was impressed and urged Knox to write to the Queen Regent. Knox's unusually respectful letter urged her to support the Reformation and overthrow the church hierarchy. Queen Mary took the letter as a joke and ignored it.
Return to Geneva, 1556–1559.
Shortly after Knox sent the letter to the Queen Regent, he suddenly announced that he felt his duty was to return to Geneva. In the previous year on 1 November 1555, the congregation in Geneva had elected Knox as their minister and he decided to take up the post. He wrote a final letter of advice to his supporters and left Scotland with his wife and mother-in-law. He arrived in Geneva on 13 September 1556.
For the next two years, he lived a happy life in Geneva. He recommended Geneva to his friends in England as the best place of asylum for Protestants. In one letter he wrote:
I neither fear nor eschame to say, is the most perfect school of Christ that ever was in the earth since the days of the apostles. In other places I confess Christ to be truly preached; but manners and religion so sincerely reformed, I have not yet seen in any other place...
Knox led a busy life in Geneva. He preached three sermons a week, each lasting well over two hours. The services used a liturgy that was derived by Knox and other ministers from Calvin's "Formes des Prières Ecclésiastiques". The church in which he preached, the "Église de Notre Dame la Neuve"—now known as the Auditoire de Calvin—had been granted by the municipal authorities, at Calvin's request, for the use of the English and Italian congregations. Knox's two sons, Nathaniel and Eleazar, were born in Geneva, with Whittingham and Myles Coverdale their respective godfathers.
In the summer of 1558, Knox published his best known pamphlet, "The first blast of the trumpet against the monstruous regiment of women". In calling the "regiment" or rule of women "monstruous", he meant that it was "unnatural". The pamphlet has been called a classic of misogyny. Knox states that his purpose was to demonstrate "how abominable before God is the Empire or Rule of a wicked woman, yea, of a traiteresse and bastard". The women rulers that Knox had in mind were Queen Mary I of England and Mary of Guise, the Dowager Queen of Scotland and regent on behalf of her daughter, Mary, Queen of Scots. Knox's prejudices against women were not unusual in his day; however, even he was aware that the pamphlet was dangerously seditious. He therefore published it anonymously and did not tell Calvin, who denied knowledge of it until a year after its publication, that he had written it. In England, the pamphlet was officially condemned by royal proclamation. The impact of the document was complicated later that year, when Elizabeth Tudor became Queen of England. Although Knox had not targeted Elizabeth, he had deeply offended her, and she never forgave him.
With a Protestant on the throne, the English refugees in Geneva prepared to return home. Knox himself decided to return to Scotland. Before his departure, various honours were conferred on him, including the freedom of the city of Geneva. Knox left in January 1559, but he did not arrive in Scotland until 2 May 1559, owing to Elizabeth's refusal to issue him a passport through England.
Revolution and end of the regency, 1559–1560.
Two days after Knox arrived in Edinburgh, he proceeded to Dundee where a large number of Protestant sympathisers had gathered. Knox was declared an outlaw, and the Queen Regent summoned the Protestants to Stirling. Fearing the possibility of a summary trial and execution, the Protestants proceeded instead to Perth, a walled town that could be defended in case of a siege. At the church of St John the Baptist, Knox preached a fiery sermon and a small incident precipitated into a riot. A mob poured into the church and it was soon gutted. The mob then attacked two friaries in the town, looting their gold and silver and smashing images. Mary of Guise gathered those nobles loyal to her and a small French army. She dispatched the Earl of Argyll and Lord Moray to offer terms and avert a war. She promised not to send any French troops into Perth if the Protestants evacuated the town. The Protestants agreed, but when the Queen Regent entered Perth, she garrisoned it with Scottish soldiers on the French pay roll. This was seen as treacherous by Lord Argyll and Lord Moray, who both switched sides and joined Knox, who now based himself in St Andrews. Knox's return to St Andrews fulfilled the prophecy he made in the galleys that he would one day preach again in its church. When he did give a sermon, the effect was the same as in Perth. The people engaged in vandalism and looting.
With Protestant reinforcements arriving from neighbouring counties, the Queen Regent retreated to Dunbar. By now, the mob fury had spilled over central Scotland. Her own troops were on the verge of mutiny. On 30 June, the Protestant Lords of the Congregation occupied Edinburgh, though they were only able to hold it for a month. But even before their arrival, the mob had already sacked the churches and the friaries. On 1 July, Knox preached from the pulpit of St Giles', the most influential in the capital. The Lords of the Congregation negotiated their withdrawal from Edinburgh by the Articles of Leith signed 25 July 1559, and Mary of Guise promised freedom of conscience.
Knox knew that the Queen Regent would ask for help from France. So he negotiated by letter under the assumed name John Sinclair with William Cecil, Elizabeth's chief adviser, for English support. Knox sailed secretly to Lindisfarne, off the northeast coast of England at the end of July, to meet James Croft and Sir Henry Percy at Berwick upon Tweed. Knox was indiscreet and news of his mission soon reached Mary of Guise. He returned to Edinburgh telling Croft he had to return to his flock, and suggested that Henry Balnaves should go to Cecil.
When additional French troops arrived in Leith, Edinburgh's seaport, the Protestants responded by retaking Edinburgh. This time, on 24 October 1559, the Scottish nobility formally deposed Mary of Guise from the regency. Her secretary, William Maitland of Lethington, defected to the Protestant side, bringing his administrative skills. From then on, Maitland took over the political tasks, freeing Knox for the role of religious leader. For the final stage of the revolution, Maitland appealed to Scottish patriotism to fight French domination. Following the Treaty of Berwick, support from England finally arrived and by the end of March, a significant English army joined the Scottish Protestant forces. The sudden death of Mary of Guise in Edinburgh Castle on 10 June 1560 paved the way for an end to hostilities, the signing of the Treaty of Edinburgh, and the withdrawal of French and English troops from Scotland. On 19 July, Knox held a National Thanksgiving Service at St Giles'.
Reformation in Scotland, 1560–1561.
On 1 August, the Scottish Parliament met to settle religious issues. Knox and five other ministers were called upon to draw up a new confession of faith. Within four days, the Scots Confession was presented to Parliament, voted upon, and approved. A week later, the Parliament passed three acts in one day: the first abolished the jurisdiction of the Pope in Scotland, the second condemned all doctrine and practice contrary to the reformed faith, and the third forbade the celebration of Mass in Scotland. Before the dissolution of Parliament, Knox and the other ministers were given the task of organising the newly reformed church or the Kirk. They would work for several months on the "Book of Discipline", the document describing the organisation of the new church. During this period, in December 1560, Knox's wife, Margery, died, leaving Knox to care for their two sons, aged three and a half and two years old. John Calvin, who had lost his own wife in 1549, wrote a letter of condolence.
Parliament reconvened on 15 January 1561 to consider the "Book of Discipline". The Kirk was to be run on democratic lines. Each congregation was free to choose or reject its own pastor, but once he was chosen he could not be fired. Each parish was to be self-supporting, as far as possible. The bishops were replaced by ten to twelve "superintendents". The plan included a system of national education based on universality as a fundamental principle. Certain areas of law were placed under ecclesiastical authority. The Parliament did not approve the plan, however, mainly for reasons of finance. The Kirk was to be financed out of the patrimony of the Roman Catholic Church in Scotland. Much of this was now in the hands of the nobles, who were reluctant to give up their possessions. A final decision on the plan was delayed because of the impending return of Mary, Queen of Scots.
Knox and Queen Mary, 1561–1564.
On 19 August 1561, cannons were fired in Leith to announce Queen Mary's arrival in Scotland. When she attended Mass being celebrated in the royal chapel at Holyrood Palace five days later, this prompted a protest in which one of her servants was jostled. The next day she issued a proclamation that there would be no alteration in the current state of religion and that her servants should not be molested or troubled. Many nobles accepted this, but not Knox. The following Sunday, he protested from the pulpit of St Giles'. As a result, just two weeks after her return, Mary summoned Knox. She accused him of inciting a rebellion against her mother and of writing a book against her own authority. Knox answered that as long as her subjects found her rule convenient, he was willing to accept her governance, noting that Paul the Apostle had been willing to live under Nero's rule. Mary noted, however, that he had written against the principle of female rule itself. He responded that she should not to be troubled by what had never harmed her. When Mary asked him whether subjects had a right to resist their ruler, he replied that if monarchs exceeded their lawful limits, they might be resisted, even by force.
On 13 December 1562, Mary sent for Knox again after he gave a sermon denouncing certain celebrations which Knox had interpreted as rejoicing at the expense of the Reformation. She charged that Knox spoke irreverently of the Queen in order to make her appear contemptible to her subjects. After Knox gave an explanation of the sermon, Mary stated that she did not blame Knox for the differences of opinion and asked that in the future he come to her directly if he heard anything about her that he disliked. Despite her friendly gesture, Knox replied that he would continue to voice his convictions in his sermons and would not wait upon her.
During Easter in 1563, some priests in Ayrshire celebrated Mass, thus defying the law. Some Protestants tried to enforce the law themselves by apprehending these priests. This prompted Mary to summon Knox for the third time. She asked Knox to use his influence to promote religious toleration. He defended their actions and noted she was bound to uphold the laws and if she did not, others would. Mary surprised Knox by agreeing that the priests would be brought to justice.
The most dramatic interview between Mary and Knox took place on 24 June 1563. Mary summoned Knox to Holyrood after hearing that he had been preaching against her proposed marriage to Don Carlos, the son of Philip II of Spain. Mary began by scolding Knox, then she burst into tears. "What have ye to do with my marriage?" she asked, and "What are ye within this commonwealth?" "A subject born within the same, Madam," Knox replied. He noted that though he was not of noble birth, he had the same duty as any subject to warn of dangers to the realm. When Mary started to cry again, he said, "Madam, in God's presence I speak: I never delighted in the weeping of any of God's creatures; yea I can scarcely well abide the tears of my own boys whom my own hand corrects, much less can I rejoice in your Majesty's weeping." He added that he would rather endure her tears, however, than remain silent and "betray my Commonwealth". At this, Mary ordered him out of the room.
Knox's final encounter with Mary was prompted by an incident at Holyrood. While Mary was absent from Edinburgh on her summer progress in 1563, a crowd forced its way into her private chapel as Mass was being celebrated. During the altercation, the priest's life was threatened. As a result, two of the ringleaders, burgesses of Edinburgh, were scheduled for trial on 24 October 1563. In order to defend these men, Knox sent out letters calling the nobles to convene. Mary obtained one of these letters and asked her advisors if this was not a treasonable act. Stewart and Maitland, wanting to keep good relations with both the Kirk and the Queen, asked Knox to admit he was wrong and to settle the matter quietly. Knox refused and he defended himself in front of Mary and the Privy Council. He argued that he had called a legal, not an illegal, assembly as part of his duties as a minister of the Kirk. After he left, the councillors voted not to charge him with treason.
Final years in Edinburgh, 1564–1572.
On 26 March 1564 Knox stirred controversy again, when he married Margaret Stewart, the daughter of an old friend, Andrew Stewart, 2nd Lord Ochiltree, a member of the Stuart family and a distant relative of the Queen, Mary Stuart. The marriage was unusual because he was a widower of fifty, while the bride was only seventeen. Very few details are known of their domestic life. They had three daughters, Martha, Margaret, and Elizabeth.
When the General Assembly convened in June 1564, an argument broke out between Knox and Maitland over the authority of the civil government. Maitland told Knox to refrain from stirring up emotions over Mary's insistence on having mass celebrated and he quoted from Martin Luther and John Calvin about obedience to earthly rulers. Knox retorted that the Bible notes that Israel was punished when it followed an unfaithful king and that the Continental reformers were refuting arguments made by the Anabaptists who rejected all forms of government. The debate revealed his waning influence on political events as the nobility continued to support Mary.
On 29 July 1565 when Mary married Henry Stuart, Lord Darnley, some of the Protestant nobles, including James Stewart, 1st Earl of Moray, rose up in rebellion. Knox revealed his own objection while preaching in the presence of the new King Consort on 19 August 1565. He made passing allusions on ungodly rulers which caused Darnley to walk out. Knox was summoned and prohibited from preaching while the court was in Edinburgh.
On 9 March 1566, Mary's secretary, David Rizzio, was murdered by conspirators loyal to Darnley. Mary escaped from Edinburgh to Dunbar and by 18 March returned with a formidable force. Knox fled to Kyle in Ayrshire, where he completed the major part of his "magnum opus", "History of the Reformation in Scotland". When he returned to Edinburgh, he found the Protestant nobles divided over what to do with Mary. Lord Darnley had been murdered and the Queen almost immediately married the chief suspect, the Earl of Bothwell. The indictment of murder thus upon her, she had been forced to abdicate and was imprisoned in Loch Leven Castle. Lord Moray had become the regent of King James VI. Other old friends of Knox's, Lord Argyll and William Kirkcaldy, stood by Mary. On 29 July 1567, Knox preached James VI's coronation sermon at the church in Stirling. During this period Knox thundered against her in his sermons, even to the point of calling for her death. However, Mary's life was spared, and she escaped on 2 May 1568.
The fighting in Scotland continued as a civil war. Lord Moray was assassinated on 23 January 1570. The regent who succeeded him, the Earl of Lennox, was also a victim of violence. On 30 April 1571, the controller of Edinburgh Castle, Kirkcaldy, ordered all enemies of the Queen to leave the city. But for Knox, his former friend and fellow galley-slave, he made an exception. If Knox did not leave, he could stay in Edinburgh, but only if he remained captive in the castle. Knox chose to leave, and on 5 May he left for St Andrews. He continued to preach, spoke to students, and worked on his "History". At the end of July 1572, after a truce was called, he returned to Edinburgh. Although by this time exceedingly feeble and his voice faint, he continued to preach at St Giles'.
After inducting his successor, Lawson of Aberdeen, as minister of St Giles' on 9 November, Knox returned to his home for the last time. With his friends and some of the greatest Scottish nobles around him, he asked for the Bible to be read aloud. On his last day, 24 November 1572, his young wife read from Paul's first letter to the Corinthians. A testimony to Knox was pronounced at his grave in the churchyard of St Giles' by James Douglas, 4th Earl of Morton and newly elected regent of Scotland: "Here lies one who never feared any flesh".
Legacy.
In his will, Knox claimed: "None have I corrupted, none have I defrauded; merchandise have I not made." The paltry sum of money Knox bequeathed to his family, which would have left them in dire poverty, showed that he had not profited from his work in the Kirk. The regent, Lord Morton, asked the General Assembly to continue paying his stipend to his widow for one year after his death, and the regent ensured that Knox's dependents were decently supported.
Knox was survived by his five children and his second wife. Nathaniel and Eleazar, his two sons by his first wife, attended Cambridge University, and died at a young age without issue. His second wife, Margaret Stewart, remarried to Andrew Ker, one of those involved in the murder of David Rizzio. Knox's three daughters also married: Martha to Alexander Fairlie; Margaret to Zachary Pont, son of Robert Pont and brother of Timothy Pont; and Elizabeth to John Welsh, a minister of the Kirk.
Knox's death was barely noticed at the time. Although his funeral was attended by the nobles of Scotland, no major politician or diplomat mentioned his death in their letters that survive. Mary, Queen of Scots made only two brief references to him in her letters. However, what the rulers feared were Knox's ideas more than Knox himself. He was a ruthless and successful revolutionary and it was this revolutionary philosophy that had a great impact on the English Puritans. Despite his strictness and dogmatism, he has also been described as contributing to the struggle for genuine human freedom, by teaching a duty to oppose unjust government in order to bring about moral and spiritual change.
Knox was notable not so much for the overthrow of Roman Catholicism in Scotland, but for assuring the replacement of the papal religion with Presbyterianism rather than Anglicanism. It was thanks to Knox that the Presbyterian polity was established. In that regard, Knox is considered the founder of the Presbyterian denomination, whose members number millions worldwide. 
Gallery.
<Gallery>
File:John Knox statue, St Giles Edinburgh.jpg|John Knox statue in the High Kirk of St. Giles, Edinburgh.
File:John Knox's House, High Street, Edinburgh.JPG|John Knox's House in Edinburgh traditionally associated with Knox in the last year of his life.
File:Grave of John Knox, Parliament Square, Edinburgh.JPG|A yellow marker indicates Knox's final resting place beneath parking space No.23 behind St. Giles' Cathedral, Edinburgh.
File:John Knox Church in Brighton.jpg|John Knox church (built 1876) in Brighton, Victoria
</Gallery>

</doc>
<doc id="37256" url="http://en.wikipedia.org/wiki?curid=37256" title="Michael I of Romania">
Michael I of Romania

Michael I (Romanian: "Mihai I" ]; born 25 October 1921) was King of Romania from 20 July 1927 to 8 June 1930 and again from 6 September 1940 to 30 December 1947. He was forced to abdicate in 1947 by the government controlled by the Communist Party of Romania. In addition to being the current claimant to the disestablished throne of Romania, he was also a Prince of Hohenzollern-Sigmaringen until 10 May 2011, when he renounced this title.
As a great-great-grandson of Queen Victoria of the United Kingdom through both of his parents he is a third cousin of: Queen Margrethe II of Denmark, King Harald V of Norway, King Juan Carlos I of Spain, King Carl XVI Gustav of Sweden and Queen Elizabeth II of the United Kingdom. He is the last surviving monarch from the Interbellum and the oldest of only three surviving heads of state from World War II, the others being the former King Simeon II of Bulgaria and Tenzin Gyatso, 14th Dalai Lama of Tibet.
Early life.
Michael was born at Foișor Castle, Sinaia, Romania, the son of Carol II of Romania (then Crown Prince of Romania) and Princess Elena of Greece. He was born as the grandson of the then-reigning King Ferdinand of Romania. When Carol eloped with his mistress Elena "Magda" Lupescu and renounced 'temporarily' his rights to the throne in December 1925, Michael was declared heir apparent. He succeeded to the throne upon Ferdinand's death in July 1927.
Rule.
1930s and the Antonescu era.
A regency, which included his uncle, Prince Nicolae, Patriarch Miron Cristea, and the country's Chief Justice (Gheorghe Buzdugan, from October 1929 Constantin Sărățeanu) functioned on behalf of the 6-year-old Michael, when he succeeded Ferdinand in 1927. In 1930, Carol II returned to the country at the invitation of politicians dissatisfied with the Regency, and was proclaimed king by the Parliament, designating Michael as Crown Prince with the title "Grand Voievod of Alba-Iulia". In November 1939, Michael joined the Romanian Senate, as the 1938 Constitution guaranteed him a seat there upon reaching the age of eighteen. In September 1940, the pro-German anti-Bolshevik régime of Prime Minister Marshal Ion Antonescu staged a "coup d'état" against Carol II, whom the Marshal claimed to be 'anti-German'. Antonescu suspended the Constitution, dissolved the Parliament, and re-installed the 18-year-old Michael as king, by popular acclaim. (Although the Constitution was restored in 1944, and the Romanian Parliament in 1946, Michael did not either subsequently take a formal oath or have his reign approved retroactively by Parliament.) Michael was crowned with the Steel Crown and anointed King of Romania by the Orthodox Patriarch of Romania, Nicodim Munteanu, in the Patriarchal Cathedral of Bucharest, on the day of his accession, 6 September 1940. Although King Michael was formally the Supreme Head of the Army, and entitled to appoint the Prime Minister with full powers named 'Leader of the people' ("Conducător"), in reality he was forced to remain only a figurehead until August 1944. Michael had lunch with Adolf Hitler twice, once with his father in Bavaria in 1937, and with his mother in Berlin in 1941. He also met Benito Mussolini in 1941, in Italy.
Turning against Nazi Germany.
In 1944, World War II was going badly for the Axis powers, but the military dictator Prime Minister Marshal Ion Antonescu was still in control of Romania. By August 1944, the Soviet conquest of Romania had become inevitable, being expected in a few months. On 23 August 1944, Michael joined the pro-Allied politicians, a number of army officers, and armed communist-led civilians in staging a "coup" against Antonescu, whereas it was recognized in the late 1970s that King Michael ordered his arrest by the Royal Palace Guard. On the same night, the new Prime Minister, Lt. General Constantin Sănătescu—who was appointed by King Michael—gave custody of Antonescu to the communists (in spite of alleged instructions to the contrary by the King), who delivered him to the Soviets on 1 September. In a radio broadcast to the Romanian nation and army, Michael issued a cease-fire just as the Red Army was penetrating the Moldavian front, proclaimed Romania's loyalty to the Allies, announced the acceptance of the armistice offered by the United Kingdom, the United States, and the USSR, and declared war on Germany. However, this did not avert a rapid Soviet occupation and capture of about 130,000 Romanian soldiers, who were transported to the Soviet Union where many perished in prison camps. Although the country's alliance with the Nazis was ended, the "coup" sped the Red Army's advance into Romania. The armistice was signed three weeks later on 12 September 1944, on terms the Soviets virtually dictated. Under the terms of the armistice, Romania recognized its defeat by the USSR and was placed under occupation of the Allied forces with the Soviets, as their representative, in control of media, communication, post, and civil administration behind the front. The coup effectively amounted to a "capitulation", an "unconditional" "surrender". It has been suggested that the "coup" may have shortened World War II by six months, thus saving hundreds of thousands of lives.
At the end of the war, King Michael was awarded the highest degree (Chief Commander) of the Legion of Merit by U.S. President Harry S. Truman. He was also decorated with the Soviet Order of Victory by Joseph Stalin "for the courageous act of the radical change in Romania's politics towards a break-up from Hitler's Germany and an alliance with the United Nations, at the moment when there was no clear sign yet of Germany's defeat," according to the official description of the decoration. With the death of Michał Rola-Żymierski in 1989, Michael became the sole surviving recipient of the Order of Victory.
Reign under communism.
In March 1945, political pressures forced King Michael to appoint a pro-Soviet government headed by Petru Groza. For the next two-plus years Michael functioned again as little more than a figurehead. Between August 1945 and January 1946, during what was later known as the "royal strike," King Michael tried unsuccessfully to oppose the Groza government by refusing to sign its decrees. In response to Soviet, British, and American pressures, King Michael eventually gave up his opposition to the communist government and stopped demanding its resignation.
He did not pardon former Marshal Antonescu, who was sentenced to death "for betrayal of the Romanian people for the benefit of Nazi Germany, for the economic and political subjugation of Romania to Germany, for cooperation with the Iron Guard, for murdering his political opponents, for the mass murder of civilians and crimes against peace". Nor did King Michael manage to save such leaders of the opposition as Iuliu Maniu and the Bratianus, victims of communist political trials, as the Constitution prevented him from doing so without the counter-signature of communist Justice Minister Lucrețiu Pătrășcanu (who himself was later eliminated by Gheorghiu-Dej's opposing communist faction). The memoirs of King Michael's aunt Princess Ileana quoted Emil Bodnăraș — her alleged lover, Romania's communist minister of defence, and a Soviet spy—as saying: "Well, if the King decides not to sign the death warrant, I promise that we will uphold his point of view." Princess Ileana was sceptical: "You know quite well (...) that the King will never of his free will sign such an unconstitutional document. If he does, it will be laid at your door, and before the whole nation your government will bear the blame. Surely you do not wish this additional handicap at this moment!"
Forced abdication.
In November 1947, King Michael travelled to London for the wedding of his cousins, Princess Elizabeth (later Queen Elizabeth II) and Prince Philip of Greece and Denmark, an occasion during which he met Princess Anne of Bourbon-Parma (his second cousin once removed), who was to become his wife. According to unconfirmed claims by so-called Romanian 'royalists', King Michael did not want to return home, but certain Americans and Britons present at the wedding encouraged him to do so. Winston Churchill is said to have counselled Michael to return because "above all things, a King must be courageous." According to his own account, King Michael rejected any offers of asylum and decided to return to Romania, contrary to the confidential, strong advice of the British Ambassador to Romania.
Early on the morning of 30 December 1947, Michael was preparing for a New Year's party at Peleș Castle in Sinaia, when Groza summoned him back to Bucharest. Michael returned to Elisabeta Palace in Bucharest, to find it surrounded by troops from the Tudor Vladimirescu Division, an army unit completely loyal to the Communists. Groza and Communist Party boss Gheorghe Gheorghiu-Dej were waiting for him, and demanded that he sign a pre-typed instrument of abdication. Unable to call in loyal troops, due to his telephone lines allegedly being cut, and with either Groza or Gheorghiu-Dej (depending on the source) holding a gun on him, Michael signed the document. Later the same day, the Communist-dominated government announced the 'permanent' abolition of the monarchy, and its replacement by a People's Republic, broadcasting the King's pre-recorded radio proclamation of his own abdication. On 3 January 1948, Michael was forced to leave the country, followed over a week later by Princesses Elisabeth and Ileana, who collaborated so closely with the Soviets that they became known as the King's "Red Aunts." He was the last monarch behind the Iron Curtain to lose his throne.
According to Michael's own account, Groza had threatened him at gun point and warned that the government would shoot 1,000 arrested students, if the king did not abdicate. In an interview with "The New York Times" from 2007, Michael recalls the events: "It was blackmail. They said, 'If you don't sign this immediately we are obliged' — why obliged I don't know — 'to kill more than 1,000 students' that they had in prison." According to "Time" Magazine, Groza threatened to arrest thousands of people and order a bloodbath unless Michael abdicated.
However, according to the autobiography of the former head of the Soviet intelligence agency NKVD, Major General Pavel Sudoplatov, the Deputy Soviet Foreign Commissar Andrey Vyshinsky personally conducted negotiations with King Michael for his abdication, guaranteeing part of a pension to be paid to Michael in Mexico. According to a few articles in "Jurnalul Naţional", Michael's abdication was negotiated with the Communist government, which allowed him to leave the country with the goods he requested, and by some of the royal retinue.
According to Albanian Communist leader Enver Hoxha's account of his conversations with the Romanian Communist leaders on the monarch's abdication, it was Gheorgiu-Dej, not Groza, who forced Michael's abdication at gunpoint. He was allowed to leave the country accompanied by some of his entourage and, as confirmed also by the Soviet leader Nikita Khrushchev recounting Gheorghiu-Dej's confessions, with whatever properties he desired, including gold and rubies. Hoxha also wrote that pro-Communist troops surrounded the palace, to counter army units who were still loyal to the king.
In March 1948, Michael denounced his abdication as illegal, and contended he was still the rightful king of Romania. According to "Time" magazine, he would have done so sooner, but for much of early 1948, he had been negotiating with the Communists over properties he had left in Romania.
There are reports that Romanian communist authorities, obedient to Stalin, allowed King Michael to depart with 42 valuable Crown-owned paintings in November 1947, so that he would leave Romania faster. Some of these paintings were reportedly sold through the famed art dealer Daniel Wildenstein. One of the paintings belonging to the Romanian Crown, which was supposedly taken out of the country by King Michael in November 1947, returned to Romania in 2004 as a donation made by John Kreuger, the former husband of King Michael's daughter Princess Irina.
In 2005, Romanian Prime Minister Călin Popescu-Tăriceanu denied these accusations about King Michael, stating that the Romanian government has no proof of any such action by King Michael and that, prior to 1949, the government had no official records of any artwork taken over from the former royal residences. However, according to some historians, such records existed as early as April 1948, having been, in fact, officially published in June 1948.
According to Ivor Porter's authorized biography, "Michael of Romania: The King and The Country" (2005), which quotes Queen-Mother Helen's daily diary, the Romanian royals took out paintings belonging to the Romanian Royal Crown, on their November 1947 trip to London to the wedding of the future Queen Elizabeth II; two of these paintings, signed by El Greco, were sold in 1976.
According to recently declassified Foreign Office documents, when he left Romania, the exiled King Michael's only assets amounted to 500,000 Swiss francs. Recently declassified Soviet transcripts of talks between Joseph Stalin and the Romanian Prime-Minister Petru Groza show that shortly before his abdication, King Michael received from the communist government assets amounting to 500,000 Swiss francs. King Michael, however, repeatedly denied that the communist government had allowed him to take into exile any financial assets or valuable goods besides four personal automobiles loaded on two train cars.
Life in exile.
In January 1948, Michael began using one of his family's ancestral titles, "Prince of Hohenzollern," instead of using the title of "King of Romania." After denouncing his abdication as forced and illegal in March 1948, Michael resumed use of the kingly title.
On 10 June 1948 in Athens, Greece, he married Princess Anne of Bourbon-Parma (born Paris, 18 September 1923), his second cousin once removed. They lived first in England and later settled in Switzerland. The Communist Romanian authorities stripped him of his Romanian citizenship in 1948. He became a test pilot and worked for an aircraft equipment company. He and his wife have five daughters.
Return and rehabilitation.
On 25 December 1990—a year after the revolution which overthrew the Communist dictatorship of Nicolae Ceaușescu—Michael, accompanied by several members of the royal family, landed at Otopeni airport and entered Romania for the first time in 43 years. Using a Danish diplomatic passport, Michael was able to obtain a 24-hour visa. He intended to reach Curtea de Argeș Cathedral, pray at the tombs of his royal ancestors and attend the Christmas religious service. However, on their way to Curtea de Argeș, the King and his companions were stopped by a police filter, taken to the airport and forced to leave the country.
In 1992, the Romanian government allowed Michael to return to Romania for Easter celebrations, where he drew large crowds. In Bucharest over a million people turned out to see him. Michael's popularity alarmed the government of President Ion Iliescu, so Michael was forbidden to visit Romania again for five years.
In 1997, after Iliescu's defeat by Emil Constantinescu, the Romanian government restored Michael's citizenship and again allowed him to visit the country. He now lives partly in Switzerland at Aubonne and partly in Romania, either at Săvârșin Castle in Arad County or in an official residence in Bucharest—the Elisabeta Palace—voted by the Romanian Parliament by a law concerning arrangements for former heads of state.
Later years.
Michael has neither encouraged nor opposed monarchist agitation in Romania and royalist parties have made little impact in post-communist Romanian politics. He takes the view that the restoration of the monarchy in Romania can only result from a decision by the Romanian people. "If the people want me to come back, of course, I will come back," he said in 1990. "Romanians have had enough suffering imposed on them to have the right to be consulted on their future." King Michael's belief is that there is still a role for, and value in, the monarchy today: "We are trying to make people understand what the Romanian monarchy was, and what it can still do" (for them). According to a 2007 opinion poll conducted at the request of the Romanian Royal House, only 14% of Romanians were in favour of the restoration of the monarchy. Another 2008 poll found that only 16% of Romanians are monarchists.
Michael has undertaken some quasi-diplomatic roles on behalf of post-communist Romania. In 1997 and 2002 he toured Western Europe, lobbying for Romania's admission into NATO and the European Union, and was received by heads of state and government officials.
In December 2003, allegedly to the "stupefaction of the public opinion in Romania", Michael awarded the "Man of The Year 2003" prize to Prime Minister Adrian Năstase, leader of the Social Democratic Party (PSD), on behalf of the tabloid "VIP". The daily "Evenimentul Zilei" subsequently complained that 'such an activity was unsuited to a king and that Michael was wasting away his prestige', with the majority of the political analysts 'considering his gesture as a fresh abdication'.
On 10 May 2007, King Michael received the Prague Society for International Cooperation and Global Panel Foundation's 6th Annual Hanno R. Ellenbogen Citizenship Award, previously awarded to Vladimir Ashkenazy, Madeleine Albright, Václav Havel, Lord Robertson, and Miloš Forman. On 8 April 2008, King Michael and Patriarch Daniel were elected as honorary members of the Romanian Academy.
Michael participated in the Victory parade in Moscow in 2010 as the only living Supreme Commander-in-Chief of a European State in the Second World War. The name of Michael I is listed on the memorial in the Grand Kremlin Palace as a recipient of the Order of Victory.
In old age, Michael has enjoyed a strong revival in popularity. On 25 October 2011, on the occasion of his 90th birthday, he delivered a speech before the assembled chambers of the Romanian Parliament. An opinion poll in January 2012 placed him as the most trusted public figure in Romania, far ahead of the political leaders. Later, in October 2012, celebrating Michael's 91st birthday, a square in Bucharest was renamed after him.
In a July 2013 survey, 45% of Romanians had a good or very good opinion of Michael, with 6.5% thinking the opposite. The Royal House also enjoyed similar numbers, with 41% having a good or very good opinion of it, and just 6.5% having a poor or very poor one.
Changes in House rules.
According to the succession provisions of the Romanian kingdom's last democratically approved monarchical constitution of 1923, upon the death of King Michael without sons, the claim to the Crown devolves once again upon the Hohenzollern-Sigmaringen family (see "Line of succession to the former Romanian throne").
However, on 30 December 2007, on the 60th anniversary of his abdication, King Michael signed the "Fundamental Rules of the Royal Family of Romania", by which he designated Princess Margarita as his heir to the throne with the titles of "Crown Princess of Romania" and "Custodian of the Romanian Crown."
This act is considered to be null and void according to some commentators, during the republican form of government and in the absence of approval by the Parliament. On the same occasion, Michael also asked the Romanian Parliament that, should it consider restoring the monarchy, it should also abolish the salic law of succession.
On 10 May 2011, on a background of lawsuits in Germany brought against his family by his German relatives regarding the former name Hohenzollern-Veringen of his son in law, Radu, and of fears expressed by some that the German Hohenzollerns may claim succession to the headship of the Romanian royal house, Michael severed all of the dynastic and historical ties with the princely house of Hohenzollern-Sigmaringen, changed the name of his family to "of Romania", and gave up all princely titles conferred to him and to his family by the German Hohenzollerns.
Personality and personal interests.
At age 16, Michael, at that time crown prince, hit a bicyclist while driving a car according to the official Censorship Records, confirmed by the memoirs of the former prime minister Constantin Argetoianu, an accident that apparently resulted in the death of the bicyclist, censored in the press.
Michael was head of the Romanian Boy Scouts in the 1930s. He is passionate about cars, especially military jeeps. He is also interested in aircraft, having worked as a test pilot during his exile.
Family.
Michael and Queen Anne have five daughters, three sons-in law, five grandchildren and three great-grandchildren:
Maternally Michael is the first cousin of; the late Alexandra, Queen consort of Yugoslavia, Queen Sofía of Spain, ex-King Constantine II of Greece and Prince Amadeo, The Duke of Aosta; Paternally Michael is the cousin of the late ex-King Peter II of Yugoslavia

</doc>
