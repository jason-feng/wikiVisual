<doc id="39039" url="http://en.wikipedia.org/wiki?curid=39039" title="Lawrence Livermore National Laboratory">
Lawrence Livermore National Laboratory

Lawrence Livermore National Laboratory (LLNL) is a federal research facility in Livermore, California, founded by the University of California in 1952. A Federally Funded Research and Development Center (FFRDC), it is primarily funded by the United States Department of Energy (DOE) and managed and operated by Lawrence Livermore National Security, LLC (LLNS), a partnership of the University of California, Bechtel, Babcock & Wilcox, URS, and Battelle Memorial Institute in affiliation with the Texas A&M University System. The laboratory was honored in 2012 by having the synthetic chemical element livermorium named after it.
Background.
LLNL is self-described as "a premier research and development institution for science and technology applied to national security." Its principal responsibility is ensuring the safety, security and reliability of the nation’s nuclear weapons through the application of advanced science, engineering and technology. The Laboratory also applies its special expertise and multidisciplinary capabilities to preventing the proliferation and use of weapons of mass destruction, bolstering homeland security and solving other nationally important problems, including energy and environmental security, basic science and economic competitiveness.
LLNL is home to many unique facilities and a number of the most powerful computer systems in the world, according to the TOP500 list, including Blue Gene/L, the world's fastest computer from 2004 until Los Alamos National Laboratory's IBM Roadrunner supercomputer surpassed it in 2008. On June 18, 2012, LLNL re-took the lead on the latest edition of the list of the world’s Top 500 supercomputers with IBM Sequoia, a 16.32 petaflops system packing more than 1.5 million custom Power cores. It is based on the same IBM BlueGene/Q architecture used in three other top ten systems which also were the most power efficient on the list. Since 1978, LLNL has received a total of 118 R&D 100 Awards, including five in 2007. The awards are given annually by the editors of "R&D Magazine" to the most innovative ideas of the year.
The Laboratory is located on a one-square-mile (2.6 km2) site at the eastern edge of Livermore. It also operates a 7000 acre remote experimental test site, called Site 300, situated about 15 mi southeast of the main lab site. LLNL has an annual budget of about $1.5 billion and a staff of roughly 5,800 employees.
Origins.
LLNL was established in 1952 as the University of California Radiation Laboratory at Livermore, an offshoot of the existing UC Radiation Laboratory at Berkeley. It was intended to spur innovation and provide competition to the nuclear weapon design laboratory at Los Alamos in New Mexico, home of the Manhattan Project that developed the first atomic weapons. Edward Teller and Ernest Lawrence, director of the Radiation Laboratory at Berkeley, are regarded as the co-founders of the Livermore facility.
The new laboratory was sited at a former naval air station of World War II. It was already home to several UC Radiation Laboratory projects that were too large for its location in the hills above the Berkeley campus, including one of the first experiments in the magnetic approach to confined thermonuclear reactions (i.e. fusion). About half an hour southeast of Berkeley, the Livermore site provided much greater security for classified projects than an urban university campus.
Lawrence tapped 32-year-old Herbert York, a former graduate student of his, to run Livermore. Under York, the Lab had four main programs: Project Sherwood (the Magnetic Fusion Program), Project Whitney (the weapons design program), diagnostic weapon experiments (both for the Los Alamos and Livermore laboratories), and a basic physics program. York and the new lab embraced the Lawrence "big science" approach, tackling challenging projects with physicists, chemists, engineers, and computational scientists working together in multidisciplinary teams.
Lawrence died in August 1958 and shortly after, the university's board of regents named both laboratories for him, as the Lawrence Radiation Laboratory.
Historically, the Berkeley and Livermore laboratories have had very close relationships on research projects, business operations and staff. The Livermore Lab was established initially as a branch of the Berkeley Laboratory. The Livermore Lab was not officially severed administratively from the Berkeley Lab until 1971. To this day, in official planning documents and records, Lawrence Berkeley National Laboratory is designated as Site 100, Lawrence Livermore National Lab as Site 200, and LLNL's remote test location as Site 300.
The laboratory was renamed Lawrence Livermore Laboratory (LLL) in 1971. On October 1, 2007 LLNS assumed management of LLNL from the University of California, which had exclusively managed and operated the Laboratory since its inception 55 years before. The laboratory was honored in 2012 by having the synthetic chemical element livermorium named after it. The LLNS takeover of the Laboratory has not been without controversy. In May 2013, an Alameda County jury awarded over $2.7 million to five former Laboratory employees who were among 430 employees LLNS laid off during 2008. The jury found that LLNS breached a contractual obligation to terminate the employees only for “reasonable cause.” The five plaintiffs also have pending age discrimination claims against LLNS, which will be heard by a different jury in a separate trial. Additionally, there are 125 co-plaintiffs awaiting trial on similar claims against LLNS. The May 2008 layoff was the first layoff at the Laboratory in nearly 40 years.
On March 14, 2011, the City of Livermore officially expanded the city's boundaries to annex LLNL and move it within the city limits. The unanimous vote by the Livermore City Council expanded Livermore’s southeastern boundaries to cover 15 land parcels covering 1,057 acre that comprise the LLNL site. Prior to this, the site was in an unincorporated area of Alameda County. The LLNL campus continues to be owned by the federal government.
Nuclear weapons projects.
From its inception, Livermore focused on innovative weapon design concepts; as a result, its first three nuclear tests were unsuccessful. However, the Lab persevered and its subsequent designs proved increasingly successful. In 1957, the Livermore Lab was selected to develop the warhead for the Navy's Polaris missile. This warhead required numerous innovations to fit a nuclear warhead into the relatively small confines of the missile nosecone.
During the decades of the Cold War, scores of Livermore-designed warheads entered the nation's nuclear stockpile. These were used in missiles ranging in size from the Lance surface-to-surface tactical missile to the megaton-class Spartan antiballistic missile. Over the years, LLNL designed the following warheads: W27 (Regulus cruise missile; 1955; joint with Los Alamos), W38 (Atlas/Titan ICBM; 1959), B41 (B52 bomb; 1957), W45 (Little John/Terrier missiles; 1956), W47 (Polaris SLBM; 1957), W48 (155-mm howitzer; 1957), W55 (submarine rocket; 1959), W56 (Minuteman ICBM; 1960), W58 (Polaris SLBM; 1960), W62 (Minuteman ICBM; 1964), W68 (Poseidon SLBM; 1966), W70 (Lance missile; 1969), W71 (Spartan missile; 1968), W79 (8-in. artillery gun; 1975), W82 (155-mm howitzer; 1978), B83 (modern strategic bomb; 1979), and W87 (Peacekeeper/MX ICBM; 1982). The W87, and the B83 are the only LLNL designs still in the U.S. nuclear stockpile.
With the collapse of the Soviet Union in 1991 and the end of the Cold War, the United States began a moratorium on nuclear testing and development of new nuclear weapon designs. To sustain existing warheads for the indefinite future, a science-based Stockpile Stewardship Program (SSP) was defined that emphasized the development and application of greatly improved technical capabilities to assess the safety, security, and reliability of existing nuclear warheads without the use of nuclear testing. Confidence in the performance of weapons, without nuclear testing, is maintained through an ongoing process of stockpile surveillance, assessment and certification, and refurbishment or weapon replacement.
With no new designs of nuclear weapons, the warheads in the U.S. stockpile must continue to function far past their original expected lifetimes. As components and materials age, problems can arise. Stockpile Life Extension Programs can extend system lifetimes, but they also can introduce performance uncertainties and require maintenance of outdated technologies and materials. Because there is concern that it will become increasingly difficult to maintain high confidence in the current warheads for the long term, the Department of Energy/National Nuclear Security Administration initiated the Reliable Replacement Warhead (RRW) Program. RRW designs could reduce uncertainties, ease maintenance demands, and enhance safety and security. In March 2007, the LLNL design was chosen for the Reliable Replacement Warhead. Since that time, however, Congress has not allocated funding for any further development of the RRW.
The Livermore Action Group organized many mass protests, from 1981 to 1984, against nuclear weapons which were being produced by the Lawrence Livermore National Laboratory. Peace activists Ken Nightingale and Eldred Schneider were involved. On June 22, 1982, more than 1,300 anti-nuclear protesters were arrested in a nonviolent demonstration. More recently, there has been an annual protest against nuclear weapons research at Lawrence Livermore. In August 2003, 1,000 people protested at Livermore Labs against "new-generation nuclear warheads". In the 2007 protest, 64 people were arrested. More than 80 people were arrested in March 2008 while protesting at the gates. 31 people were arrested in August 2013 during a protest marking the 68th anniversary of the atomic bombings of Hiroshima and Nagasaki, including famous whistle blower and author of the Pentagon Papers, Daniel Ellsberg.
In the 1980s, Lawrence's widow petitioned the Regents of the University of California on several occasions to remove her husband's name from the Livermore laboratory, due to its focus on nuclear weapons. She outlived her husband by more than 44 years and died in Walnut Creek at the age of 92 in January 2003.
Plutonium research.
LLNL conducts research into the properties and behavior of plutonium to learn how plutonium performs as it ages and how it behaves under high pressure (e.g., with the impact of high explosives). Plutonium has seven temperature-dependent solid allotropes. Each possesses a different density and crystal structure. Alloys of plutonium are even more complex; multiple phases can be present in a sample at any given time. Experiments are being conducted at LLNL and elsewhere to measure the structural, electrical and chemical properties of plutonium and its alloys and to determine how these materials change over time. Such measurements will enable scientists to better model and predict plutonium's long-term behavior in the aging stockpile.
The Lab’s plutonium research is conducted in a specially designed, ultra-safe, and highly secure facility called the SuperBlock. Work with highly enriched uranium is also conducted here. In March 2008, the National Nuclear Security Administration (NNSA) presented its preferred alternative for the transformation of the nation’s nuclear weapons complex. Under this plan, LLNL would be a center of excellence for nuclear design and engineering, a center of excellence for high explosive research and development, and a science magnet in high-energy-density (i.e., laser) physics. In addition, most of its special nuclear material would be removed and consolidated at a more central, yet-to-be-named site.
On September 30, 2009, the NNSA announced that about two thirds of the special nuclear material (e.g., plutonium) at LLNL requiring the highest level of security protection had been removed from LLNL. The move was part of NNSA's efforts initiated in October 2006 to consolidate special nuclear material at five sites by 2012, with significantly reduced square footage at those sites by 2017. The federally mandated project intends to improve security and reduce security costs, and is part of NNSA's overall effort to transform the Cold War era "nuclear weapons" enterprise into a 21st-century "nuclear security" enterprise. The original date to remove all high-security nuclear material from LLNL, based on equipment capability and capacity, was 2014. NNSA and LLNL developed a timeline to remove this material as early as possible, accelerating the target completion date to 2012.
Global security program.
The Lab’s work in global security aims to reduce and mitigate the dangers posed by the spread or use of weapons of mass destruction and by threats to energy and environmental security. Livermore has been working on global security and homeland security for decades, predating both the collapse of the Soviet Union in 1991 and the September 11, 2001, terrorist attacks. LLNL staff have been heavily involved in the cooperative nonproliferation programs with Russia to secure at-risk weapons materials and assist former weapons workers in developing peaceful applications and self-sustaining job opportunities for their expertise and technologies. In the mid-1990s, Lab scientists began efforts to devise improved biodetection capabilities, leading to miniaturized and autonomous instruments that can detect biothreat agents in a few minutes instead of the days to weeks previously required for DNA analysis.
Today, Livermore researchers address the full spectrum of threats – radiological/nuclear, chemical, biological, explosives, and cyber. They combine physical and life sciences, engineering, computations, and analysis to develop technologies that solve real-world problems. Activities are grouped into five programs:
Other programs.
LLNL supports capabilities in a broad range of scientific and technical disciplines, applying current capabilities to existing programs and developing new science and technologies to meet future national needs.
Lawrence Livermore National Laboratory has worked out several energy technologies in the field of coal gasification, shale oil extraction, geothermal energy, advanced battery research, solar energy, and fusion energy. Main oil shale processing technologies worked out by the Lawrence Livermore National Laboratory are LLNL HRS (hot-recycled-solid), LLNL RISE ("in situ" extraction technology) and LLNL radiofrequency technologies.
Key accomplishments.
Over its 60-year history, Lawrence Livermore has made many scientific and technological achievements, including:
On July 17, 2009 LLNL announced that the Laboratory had captured eight R&D 100 Awards – more than it had ever received in the annual competition. The previous LLNL record of seven awards was reached five times – in 1987, 1988, 1997, 1998 and 2006.
Also known as the “Oscars of invention”, the awards are given each year for the development of cutting-edge scientific and engineering technologies with commercial potential.
The awards raises LLNL’s total to 129 since 1978. The winning technologies were:
Largest computers.
Throughout its history, LLNL has been a leader in computers and scientific computing. Even before the Livermore Lab opened its doors, E.O. Lawrence and Edward Teller recognized the importance of computing and the potential of computational simulation. Their purchase of one of the first UNIVAC computers, set the precedent for LLNL’s history of acquiring and exploiting the fastest and most capable supercomputers in the world. A succession of increasingly powerful and fast computers have been used at the Lab over the years:
The November 2007 release of the 30th TOP500 list of the 500 most powerful computer systems in the world, has LLNL’s Blue Gene/L computer in first place for the seventh consecutive time. Five other LLNL computers are in the top 100. However, the November 2008 release of the TOP500 list places the Blue Gene/L supercomputer behind the Pleiades supercomputer in NASA/Ames Research Center, the Jaguar supercomputer in Oak Ridge National Laboratory, and the IBM Roadrunner supercomputer in Los Alamos National Laboratory. Currently, the Blue Gene/L computer can sustain 478.2 trillion operations per second, with a peak of 596.4 trillion operations per second.
On June 22, 2006, researchers at LLNL announced that they had devised a scientific software application that sustained 207.3 trillion operations per second. The record performance was made at LLNL on Blue Gene/L, the world's fastest supercomputer with 131,072 processors. The record was a milestone in the evolution of predictive science, a field in which researchers use supercomputers to answer questions about such subjects as: materials science simulations, global warming, and reactions to natural disasters.
LLNL has a long history of developing computing software and systems. Initially, there was no commercially available software, and computer manufacturers considered it the customer’s responsibility to develop their own. Users of the early computers had to write not only the codes to solve their technical problems, but also the routines to run the machines themselves. Today, LLNL computer scientists focus on creating the highly complex physics models, visualization codes, and other unique applications tailored to specific research requirements. A great deal of software also has been written by LLNL personnel to optimize the operation and management of the computer systems, including operating system extensions such as CHAOS (Linux Clustering) and resource management packages such as SLURM. The Peloton procurements in late 2006 (Atlas and other computers) were the first in which a commercial resource management package, Moab, was used to manage the clusters.
Livermore Valley Open Campus (LVOC).
In August 2009 a joint venture was announced between Sandia National Laboratories/California campus and LLNL to create an open, unclassified research and development space called the Livermore Valley Open Campus (LVOC). The motivation for the LVOC stems from current and future national security challenges that require increased coupling to the private sector to understand threats and deploy solutions in areas such as high performance computing, energy and environmental security, cyber security, economic security, and non-proliferation.
The LVOC is modeled after research and development campuses found at major industrial research parks and other U.S. Department of Energy laboratories with campus-like security, a set of business and operating rules devised to enhance and accelerate international scientific collaboration and partnerships with U.S. government agencies, industry and academia. Ultimately, the LVOC will consist of an approximately 110-acre parcel along the eastern edge of the Livermore Laboratory and Sandia sites, and will house additional conference space, collaboration facilities and a visitor's center to support educational and research activities.
Objectives of LVOC
Initial research areas for the LVOC:
The architecture of the LVOC is planned in stages; first steps including:
Sponsors.
LLNL's principal sponsor is the Department of Energy/National Nuclear Security Administration (DOE/NNSA) , which supports its stockpile stewardship and advanced scientific computing programs. Funding to support LLNL's global security and homeland security work comes from the DOE/NNSA as well as the Department of Homeland Security. LLNL also receives funding from DOE’s Office of Science, , and Office of Nuclear Energy. In addition, LLNL conducts work-for-others research and development for various Defense Department sponsors, other federal agencies, including NASA, Nuclear Regulatory Commission (NRC), National Institutes of Health, and Environmental Protection Agency, a number of California State agencies, and private industry.
Budget.
For Fiscal Year 2009 LLNL spent $1.497 billion on research and laboratory operations activities:
Research/Science Budget:
Site Management/Operations Budget:
Directors.
The LLNL Director is appointed by the Board of Governors of Lawrence Livermore National Security, LLC (LLNS) and reports to the board. The Laboratory Director also serves as the President of LLNS. Over the course of its history, the following eminent scientists have served as LLNL Director:
Organization.
The LLNL Director is supported by a senior executive team consisting of the Deputy Director, the Deputy Director for Science and Technology, Principal Associate Directors, and other senior executives who manage areas/functions directly reporting to the Laboratory Director.
The Directors Office is organized into these functional areas/offices:
The Laboratory is organized into four principal directorates, each headed by a Principal Associate Director:
Three other directorates are each headed by an Associate Director who reports to the LLNL Director:
Corporate management.
The LLNL Director reports to the Lawrence Livermore National Security, LLC (LLNS) Board of Governors, a group of key scientific, academic, national security and business leaders from the LLNS partner companies that jointly own and control LLNS. The LLNS Board of Governors has a total of 16 positions, with six of these Governors constituting an Executive Committee. All decisions of the Board are made by the Governors on the Executive Committee. The other Governors are advisory to the Executive Committee and do not have voting rights.
The University of California is entitled to appoint three Governors to the Executive Committee, including the Chair. Bechtel is also entitled to appoint three Governors to the Executive Committee, including the Vice Chair. However, one of the Bechtel Governors must be a representative of Babcock and Wilcox (B&W) or the Washington Division of URS Corporation (URS), who is nominated jointly by B&W and URS each year, and who must be approved and appointed by Bechtel. The Executive Committee actually has a seventh Governor who is appointed by Battelle, however they are non-voting and merely advisory to the Executive Committee. The remaining Board positions are known as Independent Governors (also referred to as Outside Governors), and are selected from among individuals, preferably of national stature, and can not be employees or officers of the partner companies.
The University of California-appointed Chair has tie-breaking authority over most decisions of the Executive Committee. The Board of Governors is the ultimate governing body of LLNS and is charged with overseeing the affairs of LLNS in its operations and management of LLNL.
LLNS managers and employees who work at LLNL, up to and including the President/Laboratory Director, are generally referred to as Laboratory Employees. All Laboratory Employees report directly or indirectly to the LLNS President. While most of the work performed by LLNL is funded by the federal government, Laboratory employees are paid by LLNS which is responsible for all aspects of their employment including providing health care benefits and retirement programs.
Within the Board of Governors, authority resides in the Executive Committee to exercise all rights, powers, and authorities of LLNS, excepting only certain decisions that are reserved to the parent companies. The LLNS Executive Committee is free to appoint officers or other managers of LLNS and LLNL, and may delegate its authorities as it deems appropriate to such officers, employees, or other representatives of LLNS/LLNL. The Executive Committee may also retain auditors, attorneys, or other professionals as necessary. For the most part the Executive Committee has appointed senior managers at LLNL as the primary officers of LLNS. As a practical matter most operational decisions are delegated to the President of LLNS, who is also the Laboratory Director. The positions of President/Laboratory Director and Deputy Laboratory Director are filled by joint action of the Chair and Vice Chair of the Executive Committee, with the University of California nominating the President/Laboratory Director and Bechtel nominating the Deputy Laboratory Director.
The current LLNS Chairman is Norman J. Pattiz - founder and chairman of Westwood One, America's largest radio network, and he also currently serves on the Board of Regent of the University of California. The Vice Chairman is J. Scott Ogilvie - president of Bechtel Systems & Infrastructure, Inc., he serves on the Board of Directors of Bechtel Group, Inc. (BGI) and on the BGI Audit Committee.
The Board of Governors uses the following committees to oversee the management and operations of LLNL by LLNS:

</doc>
<doc id="39040" url="http://en.wikipedia.org/wiki?curid=39040" title="Ernest Lawrence">
Ernest Lawrence

Ernest Orlando Lawrence (August 8, 1901 – August 27, 1958) was a pioneering American nuclear scientist, winner of the Nobel Prize for Physics in 1939 for his invention of the cyclotron. He is also known for his work on uranium-isotope separation for the Manhattan Project, and for founding the Lawrence Berkeley Laboratory and the Lawrence Livermore Laboratory.
A graduate of the University of South Dakota and University of Minnesota, Lawrence completed his Doctor of Philosophy degree in physics at Yale in 1925. In 1928, he was hired as an associate professor of physics at the University of California, becoming the youngest full professor there two years later. In its library one evening, Lawrence was intrigued by a diagram of an accelerator that produced high-energy particles. He contemplated how it could be made compact, and came up with an idea for a circular accelerating chamber between the poles of an electromagnet. The result was the first cyclotron. Lawrence went on to build a series of ever larger and more expensive cyclotrons. His Radiation Laboratory became an official department of the University of California in 1936, with Lawrence as its director.
During World War II, Lawrence developed electromagnetic isotope separation at the Radiation Laboratory. It used devices known as calutrons, a hybrid of the standard laboratory mass spectrometer and cyclotron. A huge electromagnetic separation plant was built at Oak Ridge, Tennessee, which came to be called Y-12. The process was inefficient, but it worked.
After the war, Lawrence campaigned extensively for government sponsorship of large scientific programs, and was a forceful advocate of "Big Science", with its requirements for big machines and big money. Lawrence strongly backed Edward Teller's campaign for a second nuclear weapons laboratory, which Lawrence located in Livermore, California. After his death, the Regents of the University of California renamed the Lawrence Livermore National Laboratory and Lawrence Berkeley National Laboratory after him. Chemical element number 103 was named lawrencium in his honor after its discovery at Berkeley in 1961.
Early life.
Ernest Orlando Lawrence was born in Canton, South Dakota on August 8, 1901. His parents, Carl Gustavus and Gunda (née Jacobson) Lawrence, were both the offspring of Norwegian immigrants who had met while teaching at the high school in Canton, where his father was also the superintendent of schools. He had a younger brother, John H. Lawrence. Growing up, his best friend was Merle Tuve, who would also go on to become a highly accomplished nuclear physicist.
Lawrence attended the public schools of Canton and Pierre, then enrolled at St. Olaf College in Northfield, Minnesota, but transferred after a year to the University of South Dakota in Vermillion. He completed his bachelor's degree in chemistry in 1922, and his Master of Arts (M.A.) degree in physics from the University of Minnesota in 1923 under the supervision of William Francis Gray Swann. For his master's thesis, Lawrence built an experimental apparatus that rotated an ellipsoid through a magnetic field.
Lawrence followed Swann to the University of Chicago, and then to Yale University in New Haven, Connecticut, where Lawrence completed his Doctor of Philosophy (Ph.D.) degree in physics in 1925 as a Sloane Fellow, writing his doctoral thesis on the photoelectric effect in potassium vapor. He was elected a member of Sigma Xi, and, on Swann's recommendation, received a National Research Council fellowship. Instead of using it to travel to Europe, he remained at Yale University with Swann as a researcher.
With Jesse Beams from the University of Virginia, Lawrence continued to research the photoelectric effect. They showed the photoelectrons appeared within 2 x 10−9 seconds of the photons striking the photoelectric surface—close to the limit of measurement at the time. By reducing the emission time by switch the light source on and off rapidly, the spectrum of energy emitted became broader, in conformance with Werner Heisenberg's uncertainty principle.
Lawrence received offers of assistant professorships from the University of Washington in Seattle and the University of California at a salary of $3,500 per annum. Yale promptly matched the offer of the assistant professorship, but at a salary of $3,000. Lawrence chose to stay at the more prestigious Yale, but found that the appointment without having first been an instructor was resented by some of his fellow faculty, and did not necessarily lift his social status among people unimpressed by his South Dakota immigrant background.
In 1928, Lawrence was hired as an associate professor of physics at the University of California, and two years later he became a full professor, becoming the university's youngest professor. Robert Gordon Sproul, who became university president the day after Lawrence became a professor, was a member of the Bohemian Club, and he sponsored Lawrence's membership in 1932. Through this club, Lawrence met William Henry Crocker, Edwin Pauley, and John Francis Neylan. They were influential men who helped him obtain money for his energetic nuclear particle investigations. There was great hope for medical uses to come from the development of particle physics, and this led to much of the early funding for advances Lawrence was able to obtain.
While at Yale, Lawrence met Mary Kimberly (Molly) Blumer (1910–2003), the eldest of four daughters of George Blumer, the dean of the Yale School of Medicine. They first met in 1926 and became engaged in 1931, and were married on May 14, 1932, at Trinity Church on the Green in New Haven, Connecticut. They had six children: Eric, Margaret, Mary, Robert, Barbara, and Susan. Molly's sister Elsie married Edwin McMillan in 1941. Lawrence named his son Robert after theoretical physicist Robert Oppenheimer, his closest friend in Berkeley.
The developments of the cyclotron.
The invention that brought Lawrence to international fame started out as a sketch on a scrap of a paper napkin. While sitting in the library one evening, Lawrence glanced over a journal article by Rolf Widerøe, and was intrigued by one of the diagrams. This depicted a device that produced high-energy particles required for atomic disintegration by means of a succession of small "pushes." The device depicted was laid out in a straight line using increasingly longer electrodes.
Lawrence saw that such a particle accelerator would soon become too long and unwieldy for his university laboratory. In pondering a way to make the accelerator more compact, Lawrence decided to set a circular accelerating chamber between the poles of an electromagnet. The magnetic field would hold the charged protons in a spiral path as they were accelerated between just two semicircular electrodes connected to an alternating potential. After a hundred turns or so, the protons would impact the target as a beam of high-energy particles. Lawrence excitedly told his colleagues that he had discovered a method for obtaining particles of very high energy without the use of any high voltage. He initially worked with Niels Edlefsen. Their first cyclotron was made out of brass, wire, and sealing wax and was only four inches (10 cm) in diameter—it could literally be held in one hand, and probably cost $25 in all.
What Lawrence needed to develop the idea was capable graduate students to do the work. Edlefsen left to take up an assistant professorship in September 1930, and Lawrence replaced him with M. Stanley Livingston and David H. Sloan, who he set to work on developing Widerøe's accelerator and Edlefsen's cyclotron respectively. Both had their own financial support. Both designs proved practical, and by May 1931, Sloan's linear accelerator was able to accelerate ions to 1 MeV. Livingston had a greater technical challenge, but when he applied 1,800 V to his 11-inch cyclotron on January 2, 1931, he got 80,000-electron volt protons spinning around. A week later, he had 1.22 MeV with 3,000 V, more than enough for his PhD thesis on its construction.
In what would become a recurring pattern, as soon as there was the first sign of success, Lawrence started planning a new, bigger machine. Lawrence and Livingston drew up a design for a 27 in cyclotron in early 1932. The magnet for the $800 11-inch cyclotron weighed 2 tons, but Lawrence found a massive 80-ton magnet for the 27-inch that had originally been built during World War I to power a transatlantic radio link, but was now rusting in a junkyard in Palo Alto. In the cyclotron, he had a powerful scientific instrument, but this did not translate into scientific discovery. In April 1932, John Cockcroft and Ernest Walton at the Cavendish laboratory in England announced that they had bombarded lithium with protons and succeeded in transmuting it into helium. The energy required turned out to be quite low—well within the capability of the 11-inch cyclotron. On learning about it, Lawrence sent a wire to Berkeley and asked for Cockcroft and Walton's results to be verified. It took the team until September to do so, mainly due to lack of adequate detection apparatus.
Although important discoveries continued to elude Lawrence's Radiation Laboratory, mainly due to its focus on the development of the cyclotron rather than its scientific use, through his increasingly larger machines, Lawrence was able to provide crucial equipment needed for experiments in high energy physics. Around this device, he built what became the world's foremost laboratory for the new field of nuclear physics research in the 1930s. He received a patent for the cyclotron in 1934, which he assigned to the Research Corporation. In February 1936, Harvard University's president, James B. Conant, made attractive offers to Lawrence and Oppenheimer. In response, the Radiation Laboratory became an official department of the University of California on July 1, 1936, with Lawrence formally appointed its director, and the University agreed to make $20,000 a year available for its activities.
Using the new 27-inch cyclotron, the team at Berkeley discovered that every element that they bombarded with recently discovered deuterium emitted energy, and at the same range. They therefore postulated the existence of a new and hitherto unknown particle, and a possible source of limitless energy. William Laurence of "The New York Times" described Lawrence as "a new miracle worker of science". At Cockroft's invitation, Lawrence was invited to the 1933 Solvay Conference, to give a presentation on the cyclotron. Lawrence ran into withering skepticism from James Chadwick, who suggested that what Lawrence's team was observing was contamination of their apparatus.
After he returned to Berkeley, Lawrence mobilized his team to go painstakingly over the results in order to gather enough evidence to convince Chadwick. Meanwhile, at the Cavendish laboratory, Ernest Rutherford and Mark Oliphant found that deuterium fuses to form helium-3, which causes the effect that the cyclontroneers had observed. Not only was Chadwick correct in that they had been observing contamination, but they had overlooked another important discovery, of nuclear fusion. Lawrence pressed on with the creation of larger cyclotrons. The 27-inch cyclotron was superseded by a 37-inch cyclotron in June 1937. In May 1939, the 60-inch cyclotron was started it. It was used to bombard iron and produced its first radioactive isotopes in June, and the first cancer patient received neutron therapy from it on November 20.
Lawrence was awarded the Nobel Prize in Physics in November 1939 "for the invention and development of the cyclotron and for results obtained with it, especially with regard to artificial radioactive elements". He was the first at Berkeley as well as the first South Dakotan to become a Nobel Laureate, and the first to be so honored while at a state-supported university. The award ceremony was held on February 29, 1940, in Berkeley, California due to World War II, in the auditorium of Wheeler Hall on the campus of the university. Lawrence received his medal from Carl E. Wallerstedt, Sweden's Consul General in San Francisco. Robert W. Wood wrote to Lawrence and presciently noted "As you are laying the foundations for the cataclysmic explosion of uranium... I'm sure old Nobel would approve."
In March 1940, Arthur Compton, Vannevar Bush, James B. Conant, Karl T. Compton, and Alfred Lee Loomis travelled to Berkeley to discuss Lawrence's proposal for a 184-inch cyclotron with a 4,500-ton magnet that was estimated to cost $2.65 million. The Rockefeller Foundation put up $1.15 million to get the project started.
World War II and the Manhattan Project.
After the outbreak of World War II in Europe, Lawrence became drawn into military projects. He helped recruit staff for the MIT Radiation Laboratory, where American physicists developed the cavity magnetron invented by Oliphant's team in Britain. The name of the new laboratory was deliberately copied from Lawrence's laboratory in Berkeley for security reasons. He also became involved in recruiting staff for underwater sound laboratories to develop techniques for detecting German submarines. Meanwhile, work continued at Berkeley with cyclotrons. In December 1940, Glenn T. Seaborg and Emilio Segré used the 60 in cyclotron to bombard with deuterons uranium-238 producing a new element, Neptunium-238, which decayed by beta emission to form plutonium-238. The discovery of plutonium was kept secret until a year after the end of World War II after the discovery that one of its isotopes, plutonium-239, could undergo nuclear fission in a way that might be useful in an atomic bomb.
In September 1941, Oliphant met with Lawrence and Oppenheimer at Berkeley, where they showed him the site for the new 184 in cyclotron. Oliphant in turn took the Americans to task for not following up the recommendations of the British MAUD Committee, which advocated a program to develop an atomic bomb. Lawrence had already thought about the problem of separating the fissile isotope uranium-235 from uranium-238, a process known today as uranium enrichment. Separating uranium isotopes was difficult because the two isotopes have very nearly identical chemical properties, and could only be separated gradually using small mass differences. Separating these isotopes with a mass spectrometer was one of the technologies developed to produce weapon-grade uranium-235, so Lawrence began converting his old 37-inch cyclotron into a giant mass spectrometer. It was on Lawrence's recommendation that the director of the Manhattan Project, Brigadier General Leslie R. Groves, Jr., appointed Oppenheimer as head of the Los Alamos Laboratory.
Electromagnetic isotope separation was developed by Lawrence at the Radiation Laboratory. It used devices known as calutrons, a hybrid of two laboratory instruments, the mass spectrometer and cyclotron. The name was derived from the words "California", "university" and "cyclotron". On November 1943, Lawrence's team at Berkeley was bolstered by 29 British scientists, including Oliphant. In the electromagnetic process, a magnetic field deflected charged particles according to mass. The process was neither scientifically elegant nor industrially efficient. Compared with a gaseous diffusion plant or a nuclear reactor, an electromagnetic separation plant would consume more scarce materials, require more manpower to operate, and cost more to build. Nonetheless, the process was approved because it was based on proven technology and therefore represented less risk. Moreover, it could be built in stages, and would rapidly reach industrial capacity.
Responsibility for the design and construction of the electromagnetic separation plant at Oak Ridge, Tennessee, which came to be called Y-12, was assigned to Stone & Webster. The design called for five first-stage processing units, known as Alpha racetracks, and two units for final processing, known as Beta racetracks. In September 1943 Groves authorized construction of four more racetracks, known as Alpha II.
When the plant was started up for testing on schedule in October 1943, the 14-ton vacuum tanks crept out of alignment because of the power of the magnets, and had to be fastened more securely. A more serious problem arose when the magnetic coils started shorting out. In December Groves ordered a magnet to be broken open, and handfuls of rust were found inside. Groves then ordered the racetracks to be torn down and the magnets sent back to the factory to be cleaned. A pickling plant was established on-site to clean the pipes and fittings.
Tennessee Eastman was hired to manage Y-12. Y-12 initially enriched the uranium-235 content to between 13% and 15%, and shipped the first few hundred grams of this to Los Alamos laboratory in March 1944. Only 1 part in 5,825 of the uranium feed emerged as final product. The rest was splattered over equipment in the process. Strenuous recovery efforts helped raise production to 10% of the uranium-235 feed by January 1945. In February the Alpha racetracks began receiving slightly enriched (1.4%) feed from the new S-50 thermal diffusion plant. The next month it received enhanced (5%) feed from the K-25 gaseous diffusion plant. By April 1945 K-25 was producing uranium sufficiently enriched to feed directly into the Beta tracks.
On July 16, 1945, Lawrence observed the Trinity nuclear test of the first atomic bomb with Chadwick and Charles A. Thomas. Few were more excited at its success than Lawrence. The question of how to use the now functional weapon on Japan became an issue for the scientists. While Oppenheimer favored no demonstration of the power of the new weapon to Japanese leaders, Lawrence felt strongly that a demonstration would be wise. Nonetheless, when a uranium bomb was used without warning in the atomic bombing of Hiroshima, Lawrence felt great pride in his accomplishment.
Lawrence hoped that the Manhattan Project would develop improved calutrons and construct Alpha III racetracks, but they were judged to be uneconomical. The Alpha tracks were closed down in September 1945. Although performing better than ever, they could not compete with K-25 and the new K-27, which commenced operation in January 1946. In December, the Y-12 plant was closed, thereby cutting the Tennessee Eastman payroll from 8,600 to 1,500 and saving $2 million a month. Staff numbers at the Radiation laboratory fell from 1,086 in May 1945 to 424 by the end of the year.
Post-war career and legacy.
After the war, Lawrence campaigned extensively for government sponsorship of large scientific programs. Lawrence was a forceful advocate of "Big Science" with its requirements for big machines and big money. In 1946, Lawrence asked the Manhattan Project for over $2 million for research at the Radiation Laboratory. Groves approved the money, but cut a number of programs, including Seaborg's proposal for a "hot" radiation laboratory in densely populated Berkeley, and John Lawrence's for production of medical isotopes, because this need could now be better met from nuclear reactors. One obstacle was the University of California, which was eager to divest its wartime military obligations. Lawrence and Groves managed to persuade Sproul to accept a contract extension.
Responsibility for the national laboratories passed to the newly created Atomic Energy Commission on 1 January 1947. In 1947, Lawrence asked for $15 million for his projects, which included a new linear accelerator and a new gigaelectronvolt synchrotron which became known as the bevatron. Unfortunately, University of California's contract to run the Los Alamos laboratory was due to expire on July 1, 1948, and some board members wished to divest the university of the responsibility for running a site outside California. After some negotiation, the university agreed to extend the contract for the Los Alamos National Laboratory for four more years, and to appoint Norris Bradbury, who had replaced Oppenheimer as its director on October 1945, as a professor.
To most of his colleagues, Lawrence appeared to have almost an aversion to mathematical thought. He had a most unusual intuitive approach to involved physical problems, and when explaining new ideas to him, one quickly learned not to befog the issue by writing down the differential equation that might appear to clarify the situation. Lawrence would say something to the effect that he didn't want to be bothered by the mathematical details, but "explain the physics of the problem to me." One could live close to him for years, and think of him as being almost mathematically illiterate, but then be brought up sharply to see how completely he retained his skill in the mathematics of classical electricity and magnetism.
Luis Alvarez
The 184-inch cyclotron was completed with wartime dollars from the Manhattan Project. It incorporated new ideas by Ed McMillan, and was completed as a synchrotron. It commenced operation on November 13, 1946. For the first time since 1935, Lawrence actively participated in the experiments, working unsuccessfully with Eugene Gardner in an attempt to create recently discovered pi mesons with the synchrotron. César Lattes then used the apparatus they had created to find negative pi mesons in 1948.
In the chilly Cold War climate at the University of California, Lawrence was forced to defend Radiation Laboratory staff members like Robert Serber who were investigated by the University's Personnel Security Board. Lawrence barred Robert Oppenheimer's brother Frank from the Radiation Laboratory, damaging his relationship with Robert. An acrimonious loyalty oath campaign drove away faculty members.
Lawrence was alarmed by the Soviet Union's first nuclear test in August 1949. The proper response, he concluded, was an all-out effort to build a bigger nuclear weapon: the hydrogen bomb. To create the tritium and the then-difficult to produce plutonium it required, Lawrence proposed to use accelerators to produce neutrons instead of nuclear reactors. He first proposed the construction of Mark-I, a prototype $7 million, 25 MeV linear accelerator, codenamed Materials Test Accelerator (MTA), mainly used to produce polonium for the nuclear weapon program. He was soon talking about a new, even larger MTA known as the Mark II which could produce tritium or plutonium from depleted uranium-238. Serber and Segré attempted in vain to explain the technical problems that made it impractical, but Lawrence felt that they were being unpatriotic.
Lawrence strongly backed Edward Teller's campaign for a second nuclear weapons laboratory, which Lawrence proposed to locate with the MTA Mark I at Livermore, California. Lawrence and Teller had to argue their case not only with the Atomic Energy Commission, which did not want it, and the Los Alamos National Laboratory, which was implacably opposed, but with proponents who felt that Chicago was the obvious site for the new laboratory. The new laboratory at Livermore was finally approved on July 17, 1952. The Mark II MTA was cancelled at the same time. By this time, the Atomic Energy Commission had spent $45 million on the Mark I, which had commenced operation. By this time the Brookhaven National Laboratory's Cosmotron was already in operation, and had generated a 1 GeV beam.
For his service to his country, Lawrence received the Enrico Fermi Award from the Atomic Energy Commission in 1957, and was the first recipient of the prestigious Sylvanus Thayer Award by the United States Military Academy in 1958. In July 1958, President Eisenhower requested that Lawrence travel to Geneva, Switzerland, to help negotiate a proposed treaty with the Soviet Union to ban nuclear weapons testing. Despite suffering from a serious flare-up of his chronic ulcerative colitis, Lawrence decided to go, but he became ill while in Geneva, and was rushed to the hospital at Stanford University. Surgeons removed much of his large intestine, but found other problems, including severe atherosclerosis of the superior mesenteric artery. Large quantities of blood were transfused during and after surgery. The doctors did not believe that he regained consciousness, but Molly claimed that he had spoken to her. He died in Palo Alto Hospital on August 27, 1958. Molly did not want a public funeral, but agreed to a memorial service at the First Congregationalist Church in Berkeley. President Clark Kerr delivered the eulogy.
Just 23 days after his death, the Regents of the University of California voted to rename two of the university's nuclear research sites after Lawrence: the Lawrence Livermore National Laboratory and Lawrence Berkeley National Laboratory. The Ernest Orlando Lawrence Award was established in his memory in 1959. Chemical element number 103, discovered at the Lawrence Berkeley National Laboratory in 1961, was named lawrencium in his honor. In 1968 the Lawrence Hall of Science public science education center was established in his honor.
In the 1980s, Lawrence's widow petitioned the University of California Board of Regents on several occasions to remove her husband's name from the Livermore laboratory, due to its focus on nuclear weapons. She outlived her husband by more than 44 years and died in Walnut Creek at the age of 92 on January 6, 2003.

</doc>
<doc id="39043" url="http://en.wikipedia.org/wiki?curid=39043" title="African clawed frog">
African clawed frog

The African clawed frog ("Xenopus laevis", also known as the xenopus, African clawed toad, African claw-toed frog or the platanna) is a species of African aquatic frog of the Pipidae family. Its name is derived from the three short claws on each hind foot, which it uses to tear apart its food. The word "Xenopus" means "strange foot" and "laevis" means "smooth".
African clawed frogs can grow up to a length of 5 in. They have a flattened head and body, but no tongue or external ears.
The species is found throughout much of Sub-Saharan Africa (Nigeria and Sudan to South Africa), and in isolated, introduced populations in North America, South America, and Europe. All species of the Pipidae family are tongueless, toothless and completely aquatic. They use their hands to shove food in their mouths and down their throats and a hyobranchial pump to draw or suck things in their mouth. Pipidae have powerful legs for swimming and lunging after food. They also use the claws on their feet to tear pieces of large food. They lack true ears but have lateral lines running down the length of the body and underside, which is how they can sense movements and vibrations in the water. They use their sensitive fingers, sense of smell, and lateral line system to find food. Pipidae are scavengers and will eat almost anything living, dying, or dead and any type of organic waste.
Description.
These frogs are plentiful in ponds and rivers within the south-eastern portion of Sub-Saharan Africa. They are aquatic and are often greenish-grey in color. Albino varieties are commonly sold as pets. “Wild type" African Clawed Frogs are also frequently sold as pets, and often incorrectly labeled as a Congo Frog or African Dwarf Frog because of similar colorings. They are easily distinguished from African Dwarf Frogs because African Clawed Frogs have webbing only on their hind feet while African Dwarf Frogs have webbing on all four feet. They reproduce by laying eggs (see frog reproduction). Also, the clawed frogs are the only amphibians to have actual claws used to climb and shred foods like fish or tadpoles. They lay their eggs from winter till spring during wet rainy seasons they will travel to other ponds or paddles of water to search for food.
The average life-span of these frogs ranges from 5–15 years with some individuals recorded to have lived for 20–25 years. They shed their skin every season, and eat their own shed skin.
Although lacking a vocal sac, the males make a mating call of alternating long and short trills, by contracting the intrinsic laryngeal muscles. Females also answer vocally, signaling either acceptance (a rapping sound) or rejection (slow ticking) of the male. This frog has smooth slippery skin which is multicolored on its back with blotches of olive gray or brown. The underside is creamy white with a yellow tinge.
Male and female frogs can be easily distinguished through the following differences. Male frogs are usually about 20% smaller than females, with slim bodies and legs. Males make mating calls to attract females, sounding very much like a cricket calling underwater. Females are larger than the males, appearing far more plump with hip-like bulges above their rear legs (where their eggs are internally located). While they do not sing or call out like males do, they do answer back (an extremely rare phenomenon in the animal world).
Both males and females have a cloaca, which is a chamber through which digestive and urinary wastes pass and through which the reproductive systems also empty. The cloaca empties by way of the vent which in reptiles and amphibians is a single opening for all three systems.
In the wild.
In the wild, "Xenopus laevis" are native to wetlands, ponds, and lakes across arid/semiarid regions of Sub-Saharan Africa. "Xenopus laevis" and "Xenopus muelleri" occur along the western boundary of the Great African Rift. The people of the sub-Saharan are generally very familiar with this frog, and some cultures use it as a source of protein, an aphrodisiac, or as fertility medicine. Two historic outbreaks of priapism have been linked to consumption of frog legs from frogs that ate insects containing cantharidin. Wild "Xenopus" are much larger than their captive bred counterparts.
"Xenopus laevis" in the wild are commonly infected by various parasites, including monogeneans in the urinary bladder.
Use in research.
"Xenopus" embryos and eggs are a popular model system for a wide variety of biological studies. This animal is widely used because of its powerful combination of experimental tractability and close evolutionary relationship with humans, at least compared to many model organisms. For a more comprehensive discussion of the use of these frogs in biomedical research, see the Wikipedia entry for "Xenopus".
"Xenopus" has long been an important tool for in vivo studies in molecular, cell, and developmental biology of vertebrate animals. However, the wide breadth of "Xenopus" research stems from the additional fact that cell-free extracts made from "Xenopus" are a premier in vitro system for studies of fundamental aspects of cell and molecular biology. Thus, "Xenopus" is the only vertebrate model system that allows for high-throughput in vivo analyses of gene function and high-throughput biochemistry. Finally, "Xenopus" oocytes are a leading system for studies of ion transport and channel physiology.
Although "X. laevis" does not have the short generation time and genetic simplicity generally desired in genetic model organisms, it is an important model organism in developmental biology, cell biology, toxicology and neurobiology. "X. laevis" takes 1 to 2 years to reach sexual maturity and, like most of its genus, it is tetraploid. It does have a large and easily manipulated embryo, however. The ease of manipulation in amphibian embryos has given them an important place in historical and modern developmental biology. A related species, "Xenopus tropicalis", is now being promoted as a more viable model for genetics.
Roger Wolcott Sperry used "X. laevis" for his famous experiments describing the development of the visual system. These experiments led to the formulation of the Chemoaffinity hypothesis.
"Xenopus" oocytes provide an important expression system for molecular biology. By injecting DNA or mRNA into the oocyte or developing embryo, scientists can study the protein products in a controlled system. This allows rapid functional expression of manipulated DNAs (or mRNA). This is particularly useful in electrophysiology, where the ease of recording from the oocyte makes expression of membrane channels attractive. One challenge of oocyte work is eliminating native proteins that might confound results, such as membrane channels native to the oocyte. Translation of proteins can be blocked or splicing of pre-mRNA can be modified by injection of Morpholino antisense oligos into the oocyte (for distribution throughout the embryo) or early embryo (for distribution only into daughter cells of the injected cell).
Extracts from the eggs of "X. laevis" frogs are also commonly used for biochemical studies of DNA replication and repair, as these extracts fully support DNA replication and other related processes in a cell-free environment which allows easier manipulation.
The first vertebrate ever to be cloned was an African clawed frog, an experiment for which Sir John Gurdon was awarded the Nobel Prize in Physiology or Medicine 2012 "for the discovery that mature cells can be reprogrammed to become pluripotent". 
Additionally, several African clawed frogs were present on the space shuttle Endeavour (which was launched into space on September 12, 1992) so that scientists could test whether reproduction and development could occur normally in zero gravity.
"X. laevis" is also notable for its use in the first well-documented method of pregnancy testing when it was discovered that the urine from pregnant women induced "X. laevis" oocyte production. Human chorionic gonadotropin (HCG) is a hormone found in substantial quantities in the urine of pregnant women. Today, commercially available HCG is injected into "Xenopus" males and females to induce mating behavior and to breed these frogs in captivity at any time of the year.
Model Organism Database for "Xenopus".
Xenbase is the Model Organism Database (MOD) for both "Xenopus laevis" and "Xenopus tropicalis".
As pets.
"Xenopus laevis" have been kept as pets and research subjects since as early as the 1950s. They are extremely hardy and long lived, having been known to live up to 20 or even 30 years in captivity.
African Clawed Frogs are frequently mislabeled as African Dwarf Frogs in pet stores. The astute pet owner will recognize the difference, however, because of the following characteristics:
They are as easy to take care of as fish.
As a pest.
African Clawed Frogs are voracious predators and easily adapt to many habitats. For this reason, they can easily become a harmful invasive species. They can travel short distances to other bodies of water, and some have even been documented to survive mild freezes. They have been shown to devastate native populations of frogs and other creatures by eating their young.
In 2003, "Xenopus laevis" frogs were discovered in a pond at San Francisco's Golden Gate Park. Much debate now exists in the area on how to exterminate these creatures and keep them from spreading. It is unknown if these frogs entered the San Francisco ecosystem through intentional release or escape into the wild. San Francisco officials recently drained Lily Pond and fenced off the area to prevent the frogs from escaping to other ponds in the hopes they starve to death.
Due to incidences in which these frogs were released and allowed to escape into the wild, African Clawed Frogs are illegal to own, transport or sell without a permit in the following US states: Arizona, California, Kentucky, Louisiana, New Jersey, North Carolina, Oregon, Virginia, Hawaii, Nevada, and Washington state. However, it is legal to own "Xenopus laevis" in New Brunswick and Ohio.
Known feral colonies of "Xenopus laevis" do also exist in South Wales, United Kingdom.
The African clawed frog may be an important vector and the initial source of "Batrachochytrium dendrobatidis", a chytrid fungus that has been implicated in the drastic decline in amphibian populations in many parts of the world. Unlike in many other amphibian species (including the closely related western clawed frog) where this chytrid fungus causes the disease Chytridiomycosis, it does not appear to affect the African clawed frog, making it an effective carrier.

</doc>
<doc id="39044" url="http://en.wikipedia.org/wiki?curid=39044" title="Mesoderm">
Mesoderm

In all bilaterian animals, the mesoderm is one of the three primary "germ layers" in the very early "embryo". The other two layers are the "ectoderm" (outside layer) and "endoderm" (inside layer), with the mesoderm as the "middle" layer between them.
The mesoderm forms mesenchyme, mesothelium, non-epithelial blood cells and coelomocytes. Mesothelium lines coeloms. Mesoderm forms the muscles in a process known as myogenesis, septa (cross-wise partitions) and mesenteries (length-wise partitions); and forms part of the gonads (the rest being the gametes).
Myogenesis is specifically a function of Mesenchyme. 
The mesoderm differentiates from the rest of the embryo through intercellular signaling, after which the mesoderm is polarized by an organizing center.
The position of the organizing center is in turn determined by the regions in which beta-catenin is protected from degradation by GSK-3. Beta-catenin acts as a co-factor that alters the activity of the transcription factor tcf-3 from repressing to activating, which initiates the synthesis of gene products critical for mesoderm differentiation and gastrulation. Furthermore, mesoderm has the capability to induce the growth of other structures, such as the neural plate, the precursor to the nervous system.
Definition.
The mesoderm is one of the three germinal layers that appears in the third week of embryonic development. It is formed through a process called gastrulation. There are three important components, the paraxial mesoderm, the intermediate mesoderm and the lateral plate mesoderm. The paraxial mesoderm forms the somitomeres, which give rise to mesenchyme of the head and organize into somites in occipital and caudal segments. Somites give rise to the myotome (muscle tissue), sclerotome (cartilage and bone), and dermatome (subcutaneous tissue of the skin). Signals for somite differentiation are derived from surroundings structures, including the notochord, neural tube and epidermis. The intermediate mesoderm connects the paraxial mesoderm with the lateral plate, eventually it differentiates into urogenital structures consisting of the kidneys, gonads, their associated ducts, and the adrenal glands. The lateral plate mesoderm give rise to the heart, blood vessels and blood cells of the circulatory system as well as to the mesodermal component of the limbs.
Some of the mesoderm derivatives include the muscle (smooth, cardiac and skeletal), the muscles of the tongue (occipital somites), the pharyngeal arches muscle (muscles of mastication, muscles of facial expressions), connective tissue, dermis and subcutaneous layer of the skin, bone and cartilage, dura mater, endothelium of blood vessels, red blood cells, white blood cells, microglia and Kupffer cells, the kidneys and the adrenal cortex.
Development of the mesodermal germ layer.
During the third week a process called gastrulation creates a mesodermal layer between the endoderm and the ectoderm. This process begins with formation of a primitive streak on the surface of the epiblast. The cells of the layers move between the epiblast and hypoblast and begin to spread laterally and cranially. The cells of the epiblast move toward the primitive streak and slip beneath it in a process called invagination. Some of the migrating cells displace the hypoblast and create the endoderm, and others migrate between the endoderm and the epiblast to create the mesoderm. The remaining cells form the ectoderm. After that, the epiblast and the hypoblast establish contact with the extraembryonic mesoderm until they cover the yolk sac and amnion. They move onto either side of the prechordal plate. The prechordal cells migrate to the midline to form the notochordal plate. The chordamesoderm is the central region of trunk mesoderm. This forms the notochord which induces the formation of the neural tube and establishes the anterior-posterior body axis. The notochord extends beneath the neural tube from the head to the tail. The mesoderm moves to the midline until it covers the notochord, when the mesoderm cells proliferate they form the paraxial mesoderm. In each side, the mesoderm remains thin and is known as the lateral plate. The intermediate mesoderm lies between the paraxial mesoderm and the lateral plate.
Between days 13 and 15, the proliferation of extraembryonic mesoderm, primitive streak and embryonic mesoderm take place. The notochord process occurs between days 15 and 17. Eventually, the development of the notochord canal and the axial canal takes place between days 17 and 19 when the first three somites are formed.
Paraxial mesoderm.
During the third week, the paraxial mesoderm is organized into segments. If they appear in the cephalic region and grow with cephalocaudal direction, they are called somitomeres. If they appear in the cephalic region but establish contact with the neural plate, they are known as neuromeres, which later will form the mesenchyme in the head. The somitomeres organize into somites which grow in pairs. In the fourth week the somites lose their organization and cover the notochord and spinal cord to form the backbone. In the fifth week, there are 4 occipital somites, 8 cervical, 12 thoracic, 5 lumbar, 5 sacral and 8 to 10 coccygeal that will form the axial skeleton. Somatic derivatives are determined by local signaling between adjacent embryonic tissues, in particular the neural tube, notochord, surface ectoderm and the somatic compartments themselves. The correct specification of the deriving tissues, skeletal, cartilage, endothelia and connective tissue is achieved by a sequence of morphogenic changes of the paraxial mesoderm, leading to the three transitory somatic compartments: dermomyotome, myotome and sclerotome. These structures are specified from dorsal to ventral and from medial to lateral. each somite will form its own sclerotome that will differentiate into the tendon cartilage and bone component. Its myotome will form the muscle component and the dermatome that will form the dermis of the back. The myotome and dermatome have a nerve component.
Molecular Regulation of Somite Differentiation.
Surrounding structures such as the notochord, neural tube, epidermis and lateral plate mesoderm send signals for somite differentiation Notochord protein accumulates in presomitic mesoderm destined to form the next somite and then decreases as that somite is established. The notochord and the neural tube activate the protein SHH which helps the somite to form its sclerotome. The cells of the sclerotome express the protein PAX1 that induces the cartilage and bone formation. The neural tube activates the protein WNT1 that expresses PAX 2 so the somite creates the myotome and dermatome. Finally, the neural tube also secretes neurotrophin 3 (NT-3), so that the somite creates the dermis. Boundaries for each somite are regulated by retinoic acid (RA) and a combination of FGF8and WNT3a. So the retinoic acid is and endogenous signal that maintains the bilateral synchrony of mesoderm segmentation and controls bilateral symmetry in vertebrates. The bilaterally symmetric body plan of vertebrate embryos is obvious in somites and their derivates such as the vertebral column. Therefore asymmetric somite formation correlates with a left-right desynchronization of the segmentation oscillations.
Many studies with Xenopus and zebrafish have analyzed the factors of this development and how they interact in signaling and transcription. However, there are still some doubts in how the prospective mesodermal cells integrate the various signals they receive and how they regulate their morphogenic behaviours and cell-fate decisions. Human embryonic stem cells for example have the potential to produce all of the cells in the body and they are able to self-renew indefinitely so they can be used for a large-scale production of therapeutic cell lines. They are also able to remodel and contract collagen and were induced to express muscle actin. This shows that these cells are multipotent cells.
Intermediate mesoderm.
The intermediate mesoderm connects the paraxial mesoderm with the lateral plate and differentiates into urogenital structures. In upper thoracic and cervical regions this forms the nephrotomes, and in caudally regions this forms the nephrogenic cord. It also helps to develop the excretory units of the urinary system and the gonads.
Lateral plate mesoderm.
The lateral plate mesoderm splits into parietal (somatic) and visceral (splanchnic) layers. The formation of these layers starts with the appearance of intercellular cavities. The somatic layer depends on a continuous layer with mesoderm that covers the amnion. The splanchnic depends on a continuous layer that covers the yolk sac. The two layers cover the intraembryonic cavity. The parietal layer together with overlying ectoderm forms the lateral body wall folds. The visceral layer forms the walls of the gut tube. Mesoderm cells of the parietal layer form the mesothelial membranes or serous membranes which line the peritoneal, pleural and pericardial cavities.

</doc>
<doc id="39047" url="http://en.wikipedia.org/wiki?curid=39047" title="Peter Tork">
Peter Tork

Peter Tork (born Peter Halsten Thorkelson, February 13, 1942) is an American musician and actor, best known as the keyboardist and bass guitarist of the Monkees.
Early life.
Tork was born at Doctor's Hospital, in Lanham, Maryland, a northeastern suburb of Washington, D.C. Although he was born in Maryland in 1942, many news articles incorrectly report him as born in 1944 in New York City, which was the date and place given on early Monkees press releases. He is the son of Virginia Hope (née Straus) and Halsten John Thorkelson, an economics professor at the University of Connecticut. His paternal grandfather was of Norwegian descent, while his mother was of half German Jewish and half British Isles ancestry. He began studying piano at the age of nine, showing an aptitude for music by learning to play several different instruments, including the banjo and both acoustic bass and guitars. Tork attended Windham High School in Willimantic, Connecticut, and was a member of the first graduating class at E.O. Smith High School in Storrs, Connecticut. He attended Carleton College before he moved to New York City, where he became part of the folk music scene in Greenwich Village during the first half of the 1960s. While there, he befriended other up-and-coming musicians such as Stephen Stills.
The Monkees.
Stephen Stills had auditioned for the new television series about four pop-rock musicians but was turned down because the show's producers felt his hair and teeth would not photograph well on camera. They asked Stills if he knew of someone with a similar "open, Nordic look," and Stills suggested Tork audition for the part. Tork got the job and became one of the four members of the Monkees, a fictitious pop band in the mid 1960s, created for a television comedy sitcom written about the fictitious band.
Tork was a proficient musician, and though the group was not allowed to play their own instruments on their first two albums, he was an exception, playing what he described as "third chair guitar" on Mike Nesmith's song, "Papa Gene's Blues," from their first album. He subsequently played keyboards, bass guitar, banjo, harpsichord, and other instruments on their recordings. He also co-wrote, along with Joey Richards, the closing theme song of the second season of "The Monkees", "For Pete's Sake". On the television show, he was relegated to playing the "lovable dummy," a persona Tork had developed as a folk singer in New York's Greenwich Village.
The DVD release of the first season of the show contained commentary from the various bandmates. In it, Nesmith stated that Tork was better at playing guitar than bass. In Tork's commentary, he stated that Jones was a good drummer and had the live performance lineups been based solely on playing ability, it should have been Tork on guitar, Nesmith on bass, and Jones on drums, with Dolenz taking the fronting role, rather than as it was done (with Nesmith on guitar, Tork on bass, and Dolenz on drums). Jones filled in briefly for Tork on bass when he played keyboards.
Recording and producing as a group was Tork's major interest, and he hoped that the four members would continue working together as a band on future recordings. However, the four did not have enough in common regarding their musical interests. In commentary for the DVD release of the second season of the show, Tork said that Dolenz was "incapable of repeating a triumph".
Tork, once free from Don Kirshner's restrictions, in 1967, contributed some of the most memorable and catchy instrumental flourishes, such as the piano introduction to "Daydream Believer" and the banjo part on "You Told Me", as well as exploring occasional songwriting with the likes of "For Pete's Sake" and "Lady's Baby".
Tork was close to his grandmother, staying with her sometimes in his Greenwich Village days, and after he became a Monkee. "Grams" was one of his most ardent supporters and managed his fan club, often writing personal letters to members, and visiting music stores to make sure they carried Monkees records.
Six albums were produced with the original Monkees lineup, four of which went to No 1 on the "Billboard" chart. This success was supplemented by two years of the TV show, a series of successful concert tours both across America and abroad, and a trippy-psychedelic movie, "Head", a bit ahead of its time. However, tensions, both musical and personal, were increasing within the group. The band finished a Far East tour in December 1968 (where his copy of "Naked Lunch" was confiscated by Australian Customs) and then filmed an NBC television special, "33⅓ Revolutions Per Monkee", which rehashed many of the ideas from "Head", only with the Monkees playing a strangely second-string role.
No longer getting the group dynamic he wanted, and pleading "exhaustion" from the grueling schedule, Tork bought out the remaining four years of his contract after filming was complete on December 20, 1968, at a default of $150,000/year. In the DVD commentary for the "33⅓ Revolutions Per Monkee" TV special—originally broadcast April 14, 1969—Dolenz noted that Nesmith gave Tork a gold watch as a going-away present, engraved "From the guys down at work". Tork kept the back, but replaced the watch several times in later years.
Post-Monkees.
During a trip to London in December 1967, Tork contributed banjo to George Harrison's soundtrack to the 1968 film "Wonderwall". His playing featured in the movie, but not on the official "Wonderwall Music" soundtrack album released in November 1968. Tork's brief five-string banjo piece can be heard 16 minutes into the film, as Professor Collins is caught by his mother while spying on his neighbour Penny Lane.
Striking out on his own, he formed a group called 'Peter Tork And/Or Release' with girlfriend Reine Stewart on drums (she had played drums on part of "33⅓ Revolutions Per Monkee"), Riley "Wyldflower" Cummings (ex The Gentle Soul) on bass and – sometimes – singer/keyboard player Judy Mayhan. Tork said in April 1969, "We sometimes have four. We're thinking of having a rotating fourth. Right now, the fourth is that girl I'm promoting named Judy Mayhan." "We're like Peter's back-up band", added Stewart, "except we happen to be a group instead of a back-up band." Release hoped to have a record out immediately, and Tork has said that they did record some demos, which he may still have stored away somewhere. According to Stewart the band were supposed to go to Muscle Shoals as the backing band for Mayhan's Atlantic Records solo album "Moments" (1970) but they were ultimately replaced. They mainly played parties for their "in" friends and one of their songs was considered for the soundtrack to "Easy Rider", but the producers – who had also produced "Head" – eventually decided not to include it. Release could not secure a record contract, and by 1970 Tork was once again a solo artist, as he later recalled, "I didn't know how to stick to it. I ran out of money and told the band members, 'I can't support us as a crew any more, you'll just have to find your own way'."
Tork's record and movie production entity, the Breakthrough Influence Company (BRINCO), also failed to launch, despite such talent as future Little Feat guitarist, Lowell George. He was forced to sell his house in 1970, and he and a pregnant Reine Stewart moved into the basement of David Crosby's home. Tork was credited with co-arranging a Micky Dolenz solo single on MGM Records in 1971 ("Easy on You", b/w "Oh Someone"). An arrest and conviction for possession of hashish resulted in three months in an Oklahoma penitentiary in 1972. He moved to Fairfax in Marin County, California, in the early 1970s, where he joined the 35-voice Fairfax Street Choir and played guitar for a shuffle blues band called Osceola. Tork returned to Southern California in the mid-1970s, where he married and had a son and took a job teaching at Pacific Hills School in Santa Monica for a year and a half. He spent a total of three years as a teacher of music, social studies, math, French and history and coaching baseball at a number of schools, but enjoyed some more than others.
Peter Tork joined 'Dolenz, Jones, Boyce & Hart' onstage for a guest appearance on their concert tour on July 4, 1976 in Disneyland. Later that year he reunited with Jones and Dolenz in the studio for the recording of the single "Christmas Is My Time of The Year" b/w "White Christmas", which saw a limited release for fan club members that holiday season.
Sire Records.
A chance meeting with Sire Records executive Pat Horgan at the Bottom Line in New York City led to Tork recording a six-song demo, his first recording in many years. Recorded in summer 1980, it featured Tork, who sang, played rhythm guitar, keyboards, and banjo; it was backed by Southern rock band Cottonmouth, led by guitarist/singer/songwriter Johnny Pontiff, featuring Gerard Trahan on guitar/keyboards/vocals, Gene Pyle on bass guitar/vocals and Gary Hille on drums/percussion.
Horgan produced the six tracks (which included two Monkees covers, "Shades of Gray" and "Pleasant Valley Sunday"), with George Dispigno as engineer. The four other tracks were "Good Looker," "Since You Went Away" (which appeared on the Monkees 1987 CD "Pool It"), "Higher & Higher" and "Hi Hi Babe." Also present at the sessions were Joan Jett, Chrissie Hynde of The Pretenders, and Tommy Ramone of the Ramones. The tracks were recorded at Blue Horizon House, 165 West 74th Street, home of Sire Records, but Seymour Stein, president of Sire, rejected the demo, stating "there's nothing there." Tork recorded a second set of demos in New York City, but little is known about these (other than the fact that one track was a yet another version of "Pleasant Valley Sunday" with an unknown rock band, and featured a violin solo).
During this time Tork appeared regularly on "The Uncle Floyd Show" broadcast on U-68 out of New Jersey. He performed comedy bits and lip-synced the Sire recordings. Floyd claimed Tork was the "first real star" to appear on the show. (Later, Davy Jones, the Ramones, Shrapnel, and others would follow in his footsteps.)
In 1981 he released the 45 rpm single "(I'm Not Your) Steppin' Stone" (b/w "Higher And Higher") with "The New Monks". This was Tork's third (counting "Easy On You" and "Christmas Is My Time Of Year") and final non-Monkees single. He also did some club performances and live television appearances, including taking part in a "Win A Date With Peter Tork" bit on "Late Night with David Letterman".
Monkees reunion.
In 1986, Tork rejoined fellow Monkees Davy Jones and Micky Dolenz for a highly successful 20th anniversary reunion tour. Three new songs were recorded by Tork and Dolenz for a greatest hits release. The three Monkees recorded "Pool It!". A decade later, all four group members recorded "Justus", the first recording with all four members since 1968. The quartet performed live in the United Kingdom in 1997, but for the next several years only the trio of Tork, Dolenz and Jones toured together. The trio of Monkees parted ways in 2001 with a public feud but reunited in 2011 for a series of 45th anniversary concerts in England and the United States.
Since 1986, Tork has intermittently toured with his former band mates and also played with his own bands The Peter Tork Project and Shoe Suede Blues. In 1991, Tork formed a band called the Dashboard Saints and played at a pizza restaurant in Guerneville, California. In 1994, he released his first album length solo project, "Stranger Things Have Happened", which featured brief appearances by Micky Dolenz and Michael Nesmith. In 1996, Tork collaborated on an album called "Two Man Band" with James Lee Stanley. The duo followed up in 2001 with a second release, "Once Again".
In 2001, Tork took time out from touring to appear in a leading role in the short film "Mixed Signals", written and directed by John Graziano.
In 2002, Tork resumed working with his band Shoe Suede Blues. The band performs original blues music, Monkees covers (blues versions of some), and covers of classic blues hits by greats such as Muddy Waters and has shared the stage with bands such as Captain Zig. The band toured extensively in 2006-7 following the release of the album "Cambria Hotel".
Tork also had an occasional roles as Topanga Lawrence's father on the sitcom "Boy Meets World", as well as a guest character on "7th Heaven". In 1995, Tork appeared as himself on the show "Wings", bidding against Crystal Bernard's character for the Monkeemobile. In 1999, he appeared as The Bandleader in season one episode 13, "Best Man", of "The King of Queens".
In early 2008, Tork added "advice columnist" to his extensive resume by authoring an online advice and info column called "Ask Peter Tork" at the webzine "The Daily Panic". 
In 2011, he joined Dolenz and Jones for the 2011 tour, .
In 2012, Peter joined Micky Dolenz and Michael Nesmith with a Monkees tour in honor of the album "Headquarters" 45th anniversary as well as in tribute to the late Davy Jones. The trio would tour again in 2013 and 2014. 
Cancer.
On March 3, 2009, Tork reported on his website that he had been diagnosed with adenoid cystic carcinoma, a rare, slow-growing form of head and neck cancer. A preliminary biopsy discovered that the cancer had not spread beyond the initial site. "It's a bad news, good news situation," explained Tork. "It's so rare a combination (on the tongue) that there isn't a lot of experience among the medical community about this particular combination. On the other hand, the type of cancer it is, never mind the location, is somewhat well known, and the prognosis, I'm told, is good." Tork underwent radiation treatment to prevent the cancer from returning.
On March 4, 2009, Tork underwent extensive surgery in New York City, which was successful.
On June 11, 2009, a spokesman for Tork reported that his cancer had returned. Tork was reportedly "shaken but not stirred" by the news, and said that the doctors had given him an 80% chance of containing and shrinking the new tumor.
In July 2009, while undergoing radiation therapy, he was interviewed by the "Washington Post": "I recovered very quickly after my surgery, and I've been hoping that my better-than-average constitution will keep the worst effects of radiation at bay. My voice and energy still seem to be in decent shape, so maybe I can pull these gigs off after all." He continued to tour and perform while receiving his treatments.
On September 15, 2009, Tork received an "all clear" from his doctor.
Tork documented his cancer experience on Facebook and encouraged his fans to support research efforts of the Adenoid Cystic Carcinoma Research Foundation.
Personal life.
Tork currently resides in Mansfield, Connecticut. He has been married three times, and has a child each from two of the marriages and one child from a relationship:
Song list.
All songs written by Peter Tork or co-written by Tork as indicated.
Discography.
Solo:
with James Lee Stanley:
with Shoe Suede Blues:

</doc>
<doc id="39048" url="http://en.wikipedia.org/wiki?curid=39048" title="Laboratory for Atmospheric and Space Physics">
Laboratory for Atmospheric and Space Physics

The Laboratory for Atmospheric and Space Physics (LASP) is a research organization at the University of Colorado Boulder. LASP is a research institute with over one hundred research scientists ranging in fields from solar influences, to Earth's and other planetary atmospherics processes, space weather, space plasma and dusty plasma physics. LASP has advanced technical capabilities specializing in designing, building, and operating spacecraft and spacecraft instruments.
History.
Founded after World War II, the first scientific instruments built at LASP were launched into space using captured German V-2 rockets. To this day LASP continues a suborbital rocket program through periodic calibration instrument flights from White Sands Missile Range. It was originally called the Upper Air Laboratory, but changed to its current name in 1965. LASP has historical ties to Ball Aerospace Corporation and the Center for Astrophysics and Space Astronomy (CASA).
Facilities.
LASP has two facilities: offices on the main CU-Boulder campus, and the “Space Technology Building” in the University’s research park.
LASP’s new facilities allow it to handle almost every aspect of space missions, itself. Hardware facilities allow for the construction of single instruments or entire spacecraft. A Mission Operations Center allows for the control of spacecraft data collection, and a large research staff analyzes the data.
Being part of the University, LASP has heavy student involvement in every aspect of its operations, including science, hardware design / construction and mission operations.
Satellites and Instruments.
LASP supports the following spacecraft and instruments:
Upcoming Missions.
LASP is involved in upcoming missions:

</doc>
<doc id="39049" url="http://en.wikipedia.org/wiki?curid=39049" title="Robert S. Mulliken">
Robert S. Mulliken

Robert Sanderson Mulliken ForMemRS (June 7, 1896 – October 31, 1986) was an American physicist and chemist, primarily responsible for the early development of molecular orbital theory, i.e. the elaboration of the molecular orbital method of computing the structure of molecules. Dr. Mulliken received the Nobel Prize for chemistry in 1966. He received the Priestley Medal in 1983.
Early years.
Mulliken was born in Newburyport, Massachusetts. His father, Samuel Parsons Mulliken, was a professor of organic chemistry at the Massachusetts Institute of Technology. As a child, Robert Mulliken learned the name and botanical classification of plants and, in general, had an excellent, but selective, memory. For example, he learned German well enough to skip the course in scientific German in college, but could not remember the name of his high school German teacher. He also made the acquaintance, while still a child, of the physical chemist Arthur Amos Noyes.
Mulliken helped with some of the editorial work when his father wrote his four-volume text on organic compound identification, and thus became an expert on organic chemical nomenclature.
Education.
In high school in Newburyport, Mulliken followed a scientific curriculum. He graduated in 1913 and succeeded in getting a scholarship to MIT which had earlier been won by his father. Like his father, he majored in chemistry. Already as an undergraduate, he conducted his first publishable research: on the synthesis of organic chlorides. Because he was unsure of his future direction, he included some chemical engineering courses in his curriculum and spent a summer touring chemical plants in Massachusetts and Maine. He received his B. S. degree in chemistry from MIT in 1917.
Early career.
At this time, the United States had just entered World War I, and Mulliken took a position at American University in Washington, D.C., making poison gas under James B. Conant. After nine months, he was drafted into the Army's Chemical Warfare Service, but continued on the same task. His laboratory techniques left much to be desired, and he was out of service for months with burns. Later he got a bad case of influenza, and was still in the hospital at war's end.
After the war, he took a job investigating the effects of zinc oxide and carbon black on rubber, but quickly decided that this was not the kind of chemistry he wanted to pursue. So in 1919 he entered the Ph.D. program at the University of Chicago.
Graduate and early postdoctoral education.
He got his doctorate in 1921 based on research into the separation of isotopes of mercury by evaporation, and continued in his isotope separation by this method. While at Chicago, he took a course under the Nobel Prize-winning physicist Robert A. Millikan, which exposed him to the old quantum theory. He also became interested in strange molecules after exposure to work by Hermann I. Schlesinger on diborane.
At Chicago, he had received a grant from the National Research Council (NRC) which had paid for much of his work on isotope separation. The NRC grant was extended in 1923 for two years so he could study isotope effects on band spectra of such diatomic molecules as boron nitride (BN) (comparing molecules with B10 and B11). He went to Harvard University to learn spectrographic technique from Frederick A. Saunders and quantum theory from E. C. Kemble. At the time, he was able to associate with many future luminaries, including J. Robert Oppenheimer, John H. Van Vleck, and Harold C. Urey. He also met John C. Slater, who had worked with Niels Bohr.
In 1925 and 1927, Mulliken traveled to Europe, working with outstanding spectroscopists and quantum theorists such as Erwin Schrödinger, Paul A. M. Dirac, Werner Heisenberg, Louis de Broglie, Max Born, and Walther Bothe (all of whom eventually received Nobel Prizes) and Friedrich Hund, who was at the time Born's assistant. They all, as well as Wolfgang Pauli, were developing the new quantum mechanics that would eventually supersede the old quantum theory. Mulliken was particularly influenced by Hund, who had been working on quantum interpretation of band spectra of diatomic molecules, the same spectra which Mulliken had investigated at Harvard. In 1927 Mulliken worked with Hund and as a result developed his molecular orbital theory, in which electrons are assigned to states that extend over an entire molecule. In consequence, molecular orbital theory was also referred to as the Hund-Mulliken theory.
Early scientific career.
From 1926 to 1928, he taught in the physics department at New York University (NYU). This was his first recognition as a physicist; though his work had been considered important by chemists, it clearly was on the borderline between the two sciences and both would claim him from this point on. Then he returned to the University of Chicago as an associate professor of physics, being promoted to full professor in 1931. He would ultimately hold a position jointly in both the physics and chemistry departments. At both NYU and Chicago, he continued to refine his molecular-orbital theory.
Up to this point, the primary way to calculate the electronic structure of molecules was based on a calculation by Walter Heitler and Fritz London on the hydrogen molecule (H2) in 1927. With the conception of hybridized atomic orbitals by John C. Slater and Linus Pauling, which rationalized observed molecular geometries, the method was based on the premise that the bonds in any molecule could be described in a manner similar to the bond in H2, namely, as overlapping atomic orbitals centered on the atoms involved. Since it corresponded to chemists' ideas of localized bonds between pairs of atoms, this method (called the Valence-Bond (VB) or Heitler-London-Slater-Pauling (HLSP) method), was very popular. However, particularly in attempting to calculate the properties of excited states (molecules that have been excited by some source of energy), the VB method does not always work well. With its description of the electron wave functions in molecules as delocalized molecular orbitals that possess the same symmetry as the molecule, Hund and Mulliken's molecular-orbital method, including contributions by John Lennard-Jones, proved to be more flexible and applicable to a vast variety of types of molecules and molecular fragments, and has eclipsed the valence-bond method. As a result of this development, he received the Nobel Prize in Chemistry in 1966.
Mulliken became a member of the National Academy of Sciences in 1936, the youngest member in the organization's history, at that time.
Mulliken population analysis is named after him, a method of assigning charges to atoms in a molecule.
Family.
On December 24, 1929, he married Mary Helen von Noé, daughter of Adolf Carl Noé, a geology professor at the University of Chicago. They had two daughters.
Later years.
In 1934, he derived a new scale for measuring the electronegativity of elements. This does not entirely correlate with the scale of Linus Pauling, but is generally in close correspondence.
In World War II, from 1942 to 1945, Mulliken directed the Information Office for the University of Chicago's Plutonium project. Afterward, he developed mathematical formulas to enable the progress of the molecular-orbital theory.
In 1952 he began to apply quantum mechanics to the analysis of the reaction between Lewis acid and base molecules. (See Acid-base reaction theories.) He became Distinguished Professor of Physics and Chemistry in 1961 and continued in his studies of molecular structure and spectra, ranging from diatomic molecules to large complex aggregates. He retired in 1985. His wife died in 1975.
At the age of 90, Mulliken died of congestive heart failure at his daughter's home in Arlington, Virginia on October 31, 1986. His body was returned to Chicago for burial.

</doc>
<doc id="39051" url="http://en.wikipedia.org/wiki?curid=39051" title="Cape Cod National Seashore">
Cape Cod National Seashore

The Cape Cod National Seashore (CCNS), created on August 7, 1961 by President John F. Kennedy, encompasses 43,607 acres on Cape Cod, in Massachusetts. It includes ponds, woods and beachfront of the Atlantic coastal pine barrens ecoregion. The CCNS includes nearly 40 mi of seashore along the Atlantic-facing eastern shore of Cape Cod, in the towns of Provincetown, Truro, Wellfleet, Eastham, Orleans and Chatham. It is administered by the National Park Service.
Places of interest.
Notable sites encompassed by the CCNS include Marconi Station, site of the first two-way transatlantic radio transmission, and the , formerly the North Truro Air Force Station. Dune Shacks of Peaked Hill Bars Historic District is a 1,950-acre historic district containing dune shacks and the dune environment. The glacial erratic known as Doane Rock is also located in the park.
A former United States Coast Guard station on the ocean in Truro is now operated as a 42-bed youth hostel by Hostelling International USA.
There are several paved bike trails:
Restoration efforts.
As part of the NPS Centennial Initiative, the Herring River estuary will be restored to its natural state through removal of dikes and drains that date back to 1909.

</doc>
<doc id="39052" url="http://en.wikipedia.org/wiki?curid=39052" title="Provincetown, Massachusetts">
Provincetown, Massachusetts

Provincetown is a New England town located at the extreme tip of Cape Cod in Barnstable County, Massachusetts, in the United States. A small coastal resort town with a year-round population of just under 3,000, Provincetown has a summer population of as high as 60,000. Often called "P-town", the town is known for its beaches, harbor, artists, tourist industry, and its status as a vacation destination for the LGBT community.
History.
At the time of European encounter, the area was long settled by the historic Nauset tribe, who had a settlement known as Meeshawn. They spoke an Algonquian language and were affiliated with other Algonquian tribes of the language families along the East Coast.
Bartholomew Gosnold named Cape Cod in Provincetown Harbor in 1602. In 1620, the Pilgrims signed the Mayflower Compact when they arrived at the harbor. They agreed to settle and build a self-governing community, and came ashore in the West End.
Though the Pilgrims chose to settle across the bay in Plymouth, the outermost portion of Cape Cod enjoyed an early reputation for its valuable fishing grounds. The harbor was considered the best along the coast. In 1654, the Governor of the Plymouth colony purchased this land from the Chief of the Nausets, for a selling price of two brass kettles, six coats, 12 hoes, 12 axes, 12 knives and a box.
The land, which spanned from East Harbor (Pilgrim Lake) – near the present-day border between Provincetown and Truro – to Long Point, was kept for the benefit of Plymouth colony, which began leasing fishing rights to roving fishermen. The collected fees were used to defray the costs of schools and other projects throughout the colony. In 1678, the fishing grounds were opened up to allow the inclusion of fishermen from the Massachusetts Bay colony.
In 1692, a new Royal Charter combined the Plymouth and Massachusetts Bay colonies into the Province of Massachusetts Bay. "Cape Cod" was thus officially renamed the "Province Lands".
The first record of a municipal government with jurisdiction over the Province Lands was in 1714, with an Act that declared it the "Precinct of Cape Cod", annexed under control of Truro.
On June 14, 1727, after harboring ships for more than a century, the Precinct of Cape Cod was incorporated as a township. The name chosen by its inhabitants was "Herrington", which was rejected by the Massachusetts General Court in favor of "Provincetown". The act of incorporation provided that inhabitants of Provincetown could be land holders, but not land owners. They received a quit claim to their property, but the Province retained the title. The land was to be used as it had been from the beginning of the colony — a place for the making of fish. All resources, including the trees, could be used for that purpose.
The population of Provincetown remained small through most of the 18th century.
The town was affected by the American Revolution the same way most of Cape Cod was: the effective British blockade shut down most fish production and shipping and the town dwindled. It was, by happenstance, the location of the wreck of a British warship, "HMS Somerset" at the Peaked Hill Bars off the Atlantic Coast of Provincetown in 1778.
Following the American Revolution, Provincetown grew rapidly as a fishing and whaling center. The population was bolstered by numerous Portuguese sailors, many of whom were from the Azores, and settled in Provincetown after being hired to work on US ships.
By the 1890s, Provincetown was booming, and began to develop a resident population of writers and artists, as well as a summer tourist industry. After the 1898 Portland Gale severely damaged the town's fishing industry, members of the town's art community took over many of the abandoned buildings. By the early decades of the 20th century, the town had acquired an international reputation for its artistic and literary productions. The Provincetown Players was an important experimental theatre company formed during this period. Many of its members lived during other parts of the year in Greenwich Village in New York, and intellectual and artistic connections were woven between the places. In 1898 Charles Webster Hawthorne opened the Cape Cod School of Art, said to be the first outdoor school for figure painting, in Provincetown. Film of his class from 1916 has been preserved.
The town includes eight buildings and two historic districts on the National Register of Historic Places: Provincetown Historic District and Dune Shacks of Peaked Hill Bars Historic District.
In the mid-1960s, Provincetown saw population growth. The town's rural character appealed to the hippies of the era; property was relatively cheap and rents were correspondingly low, especially during the winter. Many of those who came stayed and raised families. Commercial Street, the town's equivalent to "Main Street", gained numerous cafés, leather shops, head shops – various hip small businesses blossomed and many flourished.
By the 1970s Provincetown had a significant gay population, especially during the summer tourist season, when restaurants, bars and small shops serving the tourist trade were open. There had been a gay presence in Provincetown as early as the start of the 20th century as the artists' colony developed, along with experimental theatre. Drag queens could be seen in performance as early as the 1940s in Provincetown. In 1978 the Provincetown Business Guild (PBG) was formed to promote gay tourism. Today more than 200 businesses belong to the PBG, and Provincetown is perhaps the best-known gay summer resort on the East Coast. The 2010 US Census revealed Provincetown to have the highest rate of same-sex couples in the country, at 163.1 per 1000 couples.
Since the 1990s, property prices have risen significantly, causing some residents economic hardship. The housing bust of 2005 - 2012 caused property values in and around town to fall by 10 percent or more in less than a year. This did not slow down the town's economy, however. Provincetown's tourist season has expanded, and the town has scheduled created festivals and week-long events throughout the year. The most established are in the summer: the Portuguese Festival, Bear Week and PBG's Carnival Week.
Geography.
Provincetown is located at the very tip of Cape Cod, encompassing a total area of 17.5 sqmi − 55% of that, or 9.7 sqmi, is land area, and the remaining 7.8 sqmi water area. Surrounded by water in every direction except due east, the town has 21.3 mi of coastal shoreline. Provincetown is bordered to the east by its only neighbor, the town of Truro, and by Provincetown Harbor to the southeast, Cape Cod Bay to the south and west, Massachusetts Bay to the northwest and north, and the Atlantic Ocean to the northeast.
The town is 45 mi north (by road) from Barnstable, Hyannis, Massachusetts and 62 mi by road to the Sagamore Bridge, which spans the Cape Cod Canal and connects Cape Cod to the mainland. Provincetown is 45 mi east by southeast from Boston by air or sea, and 115 mi by road.
About 4,500 acres, or about 73% of the town's land area, is owned by the National Park Service, which operates the Cape Cod National Seashore, leaving about 2.7 sqmi of land under the town's jurisdiction. To the north lie the "Province Lands", the area of dunes and small ponds extending from Mount Ararat in the east to Race Point in the west, along the Massachusetts Bay shore. The Cape Cod Bay shoreline extends from Race Point to the far west, to Wood End in the south, eastward to Long Point, which in turn points inward towards the town, and provides a natural barrier for Provincetown Harbor. All three points are marked by lighthouses. The town's population center extends along the harbor, south of the Seashore's lands.
Climate.
The water surrounding Provincetown has the effect of moderating temperatures, such that the entire town is included in USDA plant hardiness zone 7a, which indicates an average annual extreme minimum temperature (1976–2005) of between 0 and. The water also has the effect of delaying the onset of the seasons, by keeping spring temperatures cooler and fall temperatures warmer than the rest of the state.
Transportation.
Historic transportation.
For nearly all of Provincetown's recorded history, life has revolved around the waterfront − especially the waterfront on its southern shore − which offers a naturally-deep harbor with easy and safe boat access, plus natural protection from the wind and waves. An additional element of Provincetown's geography tremendously influenced the manner in which the town evolved: the town was physically isolated, being at the hard-to-reach tip of a long, narrow peninsula.
The East Harbor, which provided the most protected mooring place in Provincetown, had a 1000 ft inlet from Provincetown Harbor, and effectively blocked off access to Provincetown by land. Until the late 19th century, no road led to Provincetown – the only land route connecting the village to points back toward the mainland was along a thin stretch of beach along the shore to the north (known locally as the "backshore"). A wooden bridge was erected over the East Harbor in 1854, only to be destroyed by a winter storm and ice two years later. Although the bridge was replaced the following year, any traveler who crossed it still needed to traverse several miles over sand routes, which, together with the backshore route, was occasionally washed out by storms. This made Provincetown very much like an island. Its residents relied almost entirely upon its harbor for its communication, travel, and commerce needs.
That changed in 1868, when the mouth of the East Harbor was diked to enable the laying of track for the arrival of the railroad. The railroad was completed, to great fanfare, in 1873; and the wooden bridge and sand road was finally replaced by a formal roadway in 1877. The railroad terminated at Railroad Wharf, known today as MacMillan Wharf. It provided an easy means for fishermen to offload their vessels and ship their catch to the cities by rail.
The railroad was not the only late arrival to Provincetown. Even roads "within" the town were slow to be constructed: 
"There was then no road through the town. With no carts, carriages, wagons, horses or oxen, why a road? ... Here every man had a path from his house to his boat or vessel, and once launched, he was on the broad highway of nations without tax or toll. There were paths to the neighbors, paths to school, paths to church; tortuous paths perhaps, but they were good pilots by night or day, on land or water. Besides, at low water there was a road such as none else could boast, washed completely twice a day from year to year, wide enough and free enough and long enough, if followed, for the armies of the Netherlands."—Shebnah Rich, "Truro—Cape Cod. Or, Land Marks and Sea Marks" (1883)
The town's internal road layout reflects the historic importance of the waterfront, the key to communication and commerce with the outside world. As the town grew, it organically expanded along the harborfront. The main "thoroughfare" was the hard-packed beach, where all commerce and socializing took place. Early deeds refer to a "Town Rode", which was little more than a footpath that ran behind the houses. In 1835, County Commissioners turned that into "Front Street", now known as Commercial Street. "Back Street" ran parallel to Front Street, but was set back from the harbor − today it is known as Bradford Street. 
"The houses faced the water then; since then some of the houses have been turned around; some of them still have the front door on the shore side. One man, a doctor, who had not lived long in town, proposed that the street be made sixty-four feet wide, but they soon voted down such foolishness as that from foreigners. He tried to compromise on thirty-two feet, but twenty-two feet seemed wide enough for all possible purposes, and twenty-two feet wide it is."—Nancy W. Paine Smith, "The Provincetown Book" (1922)
Modern-day transportation.
Provincetown is the eastern terminus of U.S. Route 6, both in the state and in the nation. Although the terminus is directed east officially, geographically speaking, the road, having curved around Cape Cod, is facing west-southwest at the point, and is marked only by its junction with Route 6A. The state-controlled portion ends with a "State Highway Ends" sign as the road enters the Cape Cod National Seashore, after which the road is under federal maintenance. Route 6A passes through the town as well, mostly following Bradford Street (whereas US 6 originally followed Commercial Street before the bypass was built and Commercial Street was switched to one-way westbound), and ending just south of the Herring Cove Beach.
Provincetown is served by two seasonal ferries to Boston and one to Plymouth. They all dock at MacMillan Pier, located just east of the Town Hall in the center of town. The town has no rail service; the town's only railway operated from 1873 until the early 1960s, when it was abandoned by the New York, New Haven and Hartford Railroad. A large portion of the "road" later converted into three roads (Harry Kemp Way, Railroad Avenue and Rear Howland) plus the "Old Colony Nature Pathway", a 1.3 mi pedestrian path and greenway.
The Cape Cod Regional Transit Authority offers flex route buses between MacMillan Pier and Harwich and a shuttle to Truro. Provincetown is at one end of the scenic "Bike Route 1" from Boston called the Claire Saltonstall Bikeway.
The Provincetown Municipal Airport is located just east of Race Point. This 378 acre airport is surrounded by the Cape Cod National Seashore, and is used mostly for General Aviation, but does receive regular scheduled service to Boston or White Plains, New York (with optional car service to Manhattan) via Cape Air, which also operates code-share flights for JetBlue. The airport is a well-equipped, if small, general-aviation airport with a single 3500 ft runway, an ILS approach, and full lighting. The nearest national and international service is from Logan International Airport in Boston.
Demographics.
United States census information.
According to the U.S. census of 2010, there were 2,942 people residing in the town (down 14.3% since 2000). The population density was 304.2 PD/sqmi. There were 4,494 housing units (up 15.5%) at an average density of 464.7 /sqmi. The racial makeup of the town was 91.5% White, 4.0% African American, 0.6% Native American, 0.6% Asian, 1.6% from other races, and 1.7% from two or more races. Hispanic or Latino of any race were 4.8% of the population.
The top reported ancestries were Irish (26.7%, up 9.3% from 2010), English (17.4%, up 2.6%), Portuguese (14.6%, down 8.2%), Italian (13.5%, up 3.4%), and German (12.5%, up 3.6%).
There were 1,765 households (down 3.9%), out of which 416 (23.6%) had families, 115 (6.5%) had children under the age of 18 living within them, and 76.4% were non-families. The average household size was 1.64 persons/household, and the average family size was 2.55.
The distribution of the population, broken down by age and gender, is shown in the population pyramid. In 2010, 6.8% of the population was under the age of 18, and the median age was 52.3. There were 1,602 males and 1,340 females.
For 2011, the estimated median income for a year-round household in the town was $46,547, with a mean household income of $74,840. For families, the median income was $87,228, and the mean is $84,050. For nonfamily households, the median income was $42,375, and the mean, $71,008. Median earnings for male full-time, year-round workers was $49,688, versus $36,471 for females. The per capita income for the town was $41,488. About 2.1% of families and 15.4% of the population were below the poverty line, including 26.0% of those under age 18 and 7.5% of those age 65 or over.
Provincetown's zip code has the highest concentration of same-sex couple households of any zip code in the United States.
Demographics in a resort town.
Data from traditional demographic sources like the U.S. Census, municipal voting rolls and property records may not accurately portray the demography of resort towns. They often reveal unusual results, as in this case, where the number of housing units far exceeds the Town's total population, where that number of housing units rose 15% while the population dropped 14%, and where nearly 61% of the housing stock is vacant, with 53% designated "for seasonal, recreational, or occasional use", according to the census.
In the decade spanning the years 2000 through 2010, Provincetown's small year-round population declined 14.3% from 3,431 to 2,942, yet during the summer months, population estimates vary wildly, ranging from 19,000 to 60,000. Census figures are unable to capture these dynamic population fluctuations that are associated with seasonal tourism. Part-time residents are not counted in the census. These people may own a second home in the town or pay rent for up to six months each year. Many of them pay property and other taxes, hold jobs in the community and even own businesses.
Government.
Provincetown is represented in the Massachusetts House of Representatives as a part of the Fourth Barnstable District, which includes (with the exception of Brewster) all the towns east and north of Harwich on the Cape. The seat is held by Democrat Sarah Peake, a former Provincetown selectman. The town is represented in the Massachusetts Senate as a part of the Cape and Islands District, which includes all of Cape Cod, Martha's Vineyard and Nantucket except the towns of Bourne, Falmouth, Sandwich and a portion of Barnstable. The Senate seat is held by Democrat Dan Wolf, President of Cape Air. Provincetown is patrolled by its own Police Department as well as the Second (Yarmouth) Barracks of Troop D of the Massachusetts State Police.
On the national level, Provincetown is a part of Massachusetts's 9th congressional district, and is currently represented by Bill Keating. Following the death of Ted Kennedy, the state's senior (Class I) member of the United States Senate was John Kerry (last re-elected in 2008) until he became Secretary of State; that seat has been occupied by Ed Markey since July 16, 2013. The other (Class II) senate seat is held by Elizabeth Warren, a Democrat, elected in the November 2012 elections and sworn in as senator in January 2013. 
Provincetown is governed by the open town meeting form of government, and is led by a town manager and a board of selectmen. The town has its own police and fire departments, both of which are stationed on Shankpainter Road. The town's post office is located along Commercial Street, near the town's Fourth Wharf. The Provincetown Public Library is a member of the Cape Libraries Automated Materials Sharing library network and is also located along Commercial Street, in the former Center Methodist Episcopal Church building since 2005.
Education.
Provincetown Schools is an International Baccalaureate World School. Verified in 2013 in the prestigious Primary Years Program and in 2014 in the Middle Years program. Provincetown Schools formally joins the IB community as an IB World School - PYP and MYP continuum program. This is a unique teaching and learning environment where students are encouraged to be global citizens, creative thinkers and open-minded learners. Provincetown Schools is now the only school in Massachusetts to offer the IB continuum, grades PK-8 - an incredible accomplishment.
Provincetown Schools educates approximately 120 children in Grades PreK - Grade 8. The Veterans Memorial Community Center houses Provincetown Schools Early Learning Center (Wee Care and Preschool ages 3–5).
In 2010, the Provincetown school board elected to phase out the high school program Provincetown High School at the end of the 2012−2013 school year, and send students to nearby Nauset Regional High School in North Eastham, beginning with the 2013−2014 academic year. Provincetown High School's last senior class graduated on June 7, 2013. The final Senior class, consisted of eight students. There are no private schools in Provincetown; high school students from the town will now attend Cape Cod Regional Technical High School in Harwich or Nauset Regional High School in North Eastham. Prior to its closing, Provincetown High School (PHS) served students from seventh through twelfth grades (and for a time also accepted students from Truro). In 2012, Provincetown High School was recognized as one of the smallest high schools in the country with a student population of 32 students in grades 10-12.
PHS's sports teams were known as the Fishermen, and the school colors were black and orange.
Culture.
The Fine Arts Work Center is a nonprofit educational enterprise, located in Provincetown since 1968. Its stated mission is to encourage the growth and development of emerging visual artists and writers through residency programs, to propagate aesthetic values and experience, and to restore the year-round vitality of the historic art colony of Provincetown.
In 2003, Provincetown received a $1.95 million low interest loan from the Rural Development program of the U.S. Department of Agriculture to help rebuild the town's MacMillan Pier. It primarily serves tourists and high-speed ferries that charge their passengers up to $45 per one-way trip. Between 2004 and 2007, the Provincetown Art Association and Museum (PAAM) received four Rural Development grants and loans totaling $3 million to increase the museum's space, add climate-controlled facilities, renovate a historic sea captain's house (the Hargood House) and cover cost overruns. As the mission of the Rural Development program is "To increase economic opportunity and improve the quality of life for all rural Americans", the USDA considered Provincetown's residents in the 2000s to still be rural and to still require such federal assistance.
The Atlantic House in Provincetown is considered the oldest gay bar in the US and Frommer's calls it "the nation's premier gay bar".
The Art House provides a venue for numerous entertainers and shows during the summer season, in particular Varla Jean Merman, Miss Richfield 1981, Ms. CoCo Peru, and other town favorites. In off season, the Art House remains open providing nightly entertainment that includes a Wii Bowling League, Trivia Night, and similar events.
The Provincetown International Film Festival, honors the best in independent and avante garde film. Among the honorees for 2014 were actress Patricia Clarkson and director David Cronenberg. Previous honorees include Matt Dillon, Harmony Korine, Parker Posey, Roger Corman, Vera Farmiga, Darren Aronofsky, Quentin Tarantino, Jane Lynch, Gael García Bernal, Tilda Swinton, Kathleen Turner, Jim Jarmusch, Todd Haynes, Gus Van Sant, and John Waters. Waters, a summer resident, is a major participant in the festival.
In November, 2011, the Provincetown Theater Company became the first theater company in New England to stage a live-action dramatic theatrical presentation of horror-fantasy author H.P. Lovecraft. The story was Lovecraft's 1919 classic, "The Picture in the House," and was described as "...the macabre come to life." The adaptation was produced for the 22nd Fall Playwright's Festival.

</doc>
<doc id="39056" url="http://en.wikipedia.org/wiki?curid=39056" title="Saint Anselm">
Saint Anselm

Saint Anselm may be

</doc>
<doc id="39057" url="http://en.wikipedia.org/wiki?curid=39057" title="Straw man">
Straw man

A straw man is a common form of argument and is an informal fallacy based on false representation of an opponent's argument. To be successful, a straw man argument requires that the audience be ignorant or uninformed of the original argument.
The so-called typical "attacking a straw man" argument creates the illusion of having completely refuted or defeated an opponent's proposition by covertly replacing it with a different proposition (i.e., "stand up a straw man") and then to refute or defeat that false argument ("knock down a straw man") instead of the original proposition.
This technique has been used throughout history in polemical debate, particularly in arguments about highly charged emotional issues where a fiery, entertaining "battle" and the defeat of an "enemy" may be more valued than critical thinking or understanding both sides of the issue.
In the United Kingdom the argument is also known as an "Aunt Sally", after the pub game of the same name where patrons throw sticks or battens at a model of an old woman's head.
Origin.
As a fallacy, the identification and name of straw man arguments are of relatively recent date, although Aristotle makes remarks that suggest a similar concern; Douglas Walton identified "the first inclusion of it we can find in a textbook as an informal fallacy" in Stuart Chase's "Guides to Straight Thinking" from 1956 (p. 40). However, Hamblin's classic text "Fallacies" (1970) neither mentions it as a distinct type, nor even as a historical term. The idea of "men of straw" who can be knocked down by "the lightest puff, the smallest breath of truth," erected by invaders upon a field to scare away others who might join the movement, can be found in Victoria C. Woodhull's "The Scare-Crows of Sexual Slavery," written in 1873.
The origins of the term are unclear. The usage of the term in rhetoric suggests a human figure made of straw which is easily knocked down or destroyed, such as a military training dummy, scarecrow, or effigy. The rhetorical technique is sometimes called an Aunt Sally in the UK, with reference to a traditional fairground game in which objects are thrown at a fixed target. One common folk etymology is that it refers to men who stood outside courthouses with a straw in their shoe in order to indicate their willingness to be a false witness.
Structure.
The straw man fallacy occurs in the following pattern of argument:
This reasoning is a fallacy of relevance: it fails to address the proposition in question by misrepresenting the opposing position.
For example:
Examples.
Straw man arguments often arise in public debates such as a (hypothetical) prohibition debate:
The proposal was to relax laws on beer. Person B has exaggerated this to a position that is harder to defend, i.e., "unrestricted access to intoxicants". It is a logical fallacy because Person A never made that claim.
In a 1977 appeal of a U.S. bank robbery conviction, a prosecuting attorney said in his closing argument
I submit to you that if you can't take this evidence and find these defendants guilty on this evidence then we might as well open all the banks and say, "Come on and get the money, boys", because we'll never be able to convict them.
This was a straw man designed to alarm the appeal judges; the idea that the precedent set by one case would literally make it impossible to convict "any" bank robbers is remote.
An example often given of a straw man is US President Richard Nixon's 1952 "Checkers speech". When campaigning for vice president in 1952, Nixon was accused of having illegally appropriated $18,000 in campaign funds for his personal use. In a televised response, instead of addressing the funds, he spoke about another gift, a dog he had been given by a supporter:
This was a straw man response; his critics had never criticized the dog as a gift or suggested he return it. This argument was successful at distracting people from the funds, and portraying his critics as nitpicking and heartless. Nixon received an outpouring of public support, remained on the ticket, and was elected by a landslide.
Christopher Tindale presents, as an example, the following passage from a draft of a bill (HCR 74) considered by the Louisiana State Legislature in 2001:
Whereas, the writings of Charles Darwin, the father of evolution, promoted the justification of racism, and his books "On the Origin of Species" and "The Descent of Man" postulate a hierarchy of superior and inferior races. . . .
Therefore, be it resolved that the legislature of Louisiana does hereby deplore all instances and all ideologies of racism, does hereby reject the core concepts of Darwinist ideology that certain races and classes of humans are inherently superior to others, and does hereby condemn the extent to which these philosophies have been used to justify and approve racist practices.
Tindale comments that "the portrait painted of Darwinian ideology is a caricature, one not borne out by any objective survey of the works cited. That similar misrepresentations of Darwinian thinking have been used to justify and approve racist practices is beside the point: the position that the legislation is attacking and dismissing is a Straw Man. In subsequent debate this error was recognized, and the eventual bill omitted all mention of Darwin and Darwinist ideology."
Contemporary work.
In 2006, Robert Talisse and Scott Aikin expanded the application and use of the straw man fallacy beyond that of previous rhetorical scholars, arguing that the straw man fallacy can take two forms, the original form in which the opponent's position is misrepresented, which they call the representative form and a new form which they call the selection form.
The selection form focuses on a partial and weaker (and easier to refute) representation of the opponent's position. Then the easier refutation of this weaker position is claimed to refute the opponent's complete position. They point out the similarity of the selection form to the fallacy of hasty generalization, in which the refutation of an opposing position that is weaker than the opponent's is claimed as a refutation of all opposing arguments. Because they have found significantly increased use of the selection form in modern political argumentation, they view its identification as an important new tool for the improvement of public discourse.
Aikin and Casey expanded on this model in 2010, introducing a third form. Referring to the "representative form" as the classic straw man, and the "selection form" as the weak man, a third form is called the hollow man. A hollow man argument is one that is a complete fabrication, where both the viewpoint and the opponent expressing it do not in fact exist, or at the very least the arguer has never encountered them. Such arguments frequently takes the form of vague phrasing such as "some say," "someone out there thinks" or similar weasel words, or it might attribute a non-existent argument to a broad movement in general, rather than an individual or organization.
A variation on the selection form, or "weak man" argument, that combines with an ad hominem is nut picking, a neologism coined by Kevin Drum. A portmanteau of "nut" (i.e. insane person) and cherry picking, nut picking refers to intentionally seeking out extremely fringe, non-representative statements and/or individuals from members of an opposing group and parading these around as evidence of that entire group's incompetence or irrationality.
In 2015, Ferrie and Combes coined the term "anomalous straw man" to refer to a failed attempt at a straw man argument. In a comment published in Physical Review Letters, they wrote:
The actual content of Brodutch’s Comment is a failed attempt at a straw-man argument. Recall: a straw man, when performed properly, is a valid logical argument against a "misrepresentation" of an opponent’s position, which is clearly an invalid argument against the actual position of the opponent. Although Brodutch does indeed attack a misrepresentation of the argument in our Letter, the argument against the misrepresentation itself is fallacious. Since this technique of compounding fallacies has no name, we coin the term "anomalous straw man" for it—anomalous since it goes outside the range of usual logical fallacies.
The term "anomalous" appears to be a tongue-in-cheek reference to anomalous weak values—the object of the discussion.
External links.
Today, your boobs are pink!
What the fuckin' bitch you are!

</doc>
<doc id="39058" url="http://en.wikipedia.org/wiki?curid=39058" title="Wellfleet, Massachusetts">
Wellfleet, Massachusetts

Wellfleet is a town in Barnstable County, Massachusetts, United States, and is located halfway between the "tip" and "elbow" of Cape Cod. The town had a population of 2,750 at the 2010 census, which swells nearly sixfold during the summer. A total of 70% of the town's land area is in protection, and nearly half of it is part of the Cape Cod National Seashore. Wellfleet is famous for its eponymous oysters, which are celebrated in the annual October Wellfleet OysterFest.
History.
Wellfleet was encountered by Europeans as early as 1606, when the French explorer Samuel de Champlain explored and named it "Port Aux Huitres" (Oyster Port) for the bountiful oyster population resident to the area. Originally settled in the 1650s by the Europeans as Billingsgate (after the famous fish market in East London), Wellfleet was part of neighboring Eastham until 1763, achieving town status after nearly 30 years of petitioning. The name "Wellfleet" is disputed; some argue that it comes from "Whale Fleet", after the burgeoning whaling industry in the town, while some say it comes from a brand of oyster popular in England at the time, in order to help sales.
Wellfleet's oyster beds drove the early economy, as did whaling and other fishing endeavors. The town was home to 30 whaling ships at the time of the American Revolution. However, because of the decline of whaling and the mackerel catch in the late 19th century, the fleet declined, being completely free of schooners by 1900. The oyster fleet continues to this day, however, harvesting many other types of shellfish as well.
Guglielmo Marconi built America's first transatlantic radio transmitter station on a coastal bluff in South Wellfleet in 1901–02. The first radio telegraph transmission from America to England was sent from this station on January 18, 1903, a ceremonial telegram from President Theodore Roosevelt to King Edward VII. Most of the transmitter site is gone, however, as three quarters of the land it originally encompassed has been eroded into the sea. The South Wellfleet station's first call sign was "CC", for Cape Cod.
In 1961, President John F. Kennedy created the Cape Cod National Seashore, which encompasses most of the Atlantic shoreline of Cape Cod. In Wellfleet the territory circles the town, from Jeremy Point through the marshes and "islands" along the Herring River, and extending the length of the Atlantic shore of the town.
Construction of the Chequesset Inn in the late 19th century contributed to the development of a tourist economy in Wellfleet. The town has the second greatest concentration of art galleries on Cape Cod, right after Provincetown. It is also a popular retirement spot.
In 1717, the pirate "Black Sam" Bellamy was sailing near what is now Wellfleet when his ship, the "Whydah", sank off shore, together with over 4.5 ST of gold and silver and all but two of its 145 men. The wreck was discovered in 1984, the first of only two confirmed pirate shipwrecks ever to have been discovered.
Geography.
According to the United States Census Bureau, the town has a total area of 91.7 km2, of which 51.3 km2 is land and 40.5 km2, or 44.11%, is water. Wellfleet is bordered by Truro to the north, the Atlantic Ocean to the east, Eastham to the south, and Cape Cod Bay to the west. Wellfleet is approximately 14 mi south of Provincetown, 33 mi (by road) northeast of Barnstable, 48 mi from the Sagamore Bridge, and 100 mi (by road) southeast of Boston.
The lands of Wellfleet wrap around Wellfleet Harbor, extending from the main portion of the Cape around the harbor to Jeremy Point. At one time, Wellfleet Harbor included an island known as Billingsgate Island, which sat at the harbor's mouth, to the south of the point. Once a flourishing small community with a lighthouse, the island was destroyed by coastal erosion and now exists as a shoal that is exposed at low tide. The Billingsgate shoals are split between Wellfleet and neighboring Eastham. Several other inlets extend inland from the harbor, at the mouth of the Herring River (also called "The Gut"), Duck Creek, Blackfish Creek and Fresh Brook (commonly known as "The Run") which leads to several brooks.
In addition to the Seashore, Wellfleet Bay Wildlife Sanctuary, run by Massachusetts Audubon, surrounds much of The Run, including part of Small Island (between The Run and Blackfish Creek). Between the sanctuary, seashore and other small parks and beaches, seventy percent of the town's area is protected.
A small whaling community was founded on the land that is now Wellfleet Bay Wildlife Sanctuary, and was originally known as Silver Spring, after Silver Spring Brook. What remains of it is a marsh that was once its harbor, known as the Silver Spring Brook Marshes. This land is now protected by the Massachusetts Audubon Society in its Wellfleet Bay Sanctuary.
Transportation.
U.S. Route 6 passes from north to south through the town. The town's commercial center lies west of the route, along the shores of the harbor. The route was straightened in the mid-20th century, and some maps still consider the "old" Route 6 to be a portion of Route 6A. The town has no rail or air service. The last train left the area in the 1930s, the train station was razed and the tracks were torn up through Provincetown. The nearest municipal airports are in Chatham and Provincetown, both about 18 mi from town; the nearest national and international service can be found at Logan International Airport in Boston.
There is currently limited bus service between Wellfleet and Hyannis, and from there on to Boston and Logan Airport, on the Plymouth & Brockton Street Railway Company, a Plymouth-based bus service. The CCRTA, which runs between Hyannis and Provincetown, also makes stops in Wellfleet.
Demographics.
As of the census of 2000, there were 2,749 people, 1,301 households, and 724 families residing in the town. The population density was 138.6 PD/sqmi. There were 3,998 housing units at an average density of 201.6 /sqmi. The racial makeup of the town was 96.58% White, 0.95% African American, 0.29% Native American, 0.36% Asian, 0.04% Pacific Islander, 0.58% from other races, and 1.20% from two or more races. Hispanic or Latino of any race were 0.69% of the population.
There were 1,301 households out of which 20.0% had children under the age of 18 living with them, 44.8% were married couples living together, 8.2% had a female householder with no husband present, and 44.3% were non-families. 34.8% of all households were made up of individuals and 13.2% had someone living alone who was 65 years of age or older. The average household size was 2.11 and the average family size was 2.75.
In the town the population was spread out with 17.8% under the age of 18, 4.9% from 18 to 24, 23.3% from 25 to 44, 32.2% from 45 to 64, and 21.7% who were 65 years of age or older. The median age was 47 years. For every 100 females there were 89.2 males. For every 100 females age 18 and over, there were 83.5 males.
The median income for a household in the town was $43,558, and the median income for a family was $50,990. Males had a median income of $38,100 versus $35,964 for females. The per capita income for the town was $25,712. About 5.7% of families and 7.5% of the population were below the poverty line, including 9.7% of those under age 18 and 6.0% of those age 65 or over.
Government.
Wellfleet is represented in the Massachusetts House of Representatives as a part of the Fourth Barnstable district, which includes (with the exception of Brewster) all the towns east and north of Harwich on the Cape. The town is represented in the Massachusetts Senate as a part of the Cape and Islands District, which includes all of Cape Cod, Martha's Vineyard and Nantucket except the towns of Bourne, Falmouth, Sandwich and a portion of Barnstable. Wellfleet is patrolled by the Second (Yarmouth) Barracks of Troop D of the Massachusetts State Police.
On the national level, Wellfleet is a part of the 9th congressional district, currently represented by Bill Keating. The state's senior (Class I) member of the United States Senate, elected in 2012, is Elizabeth Warren. The junior (Class II) senator, elected on April 30, 2013, is Ed Markey.
Wellfleet is governed by the open town meeting form of government and a board of selectmen, who employ a town administrator to oversee day-to-day business. The town has its own police and fire departments, headquartered on Route 6 near the town center. There are two post offices, and both are also located along Route 6. The Wellfleet Public Library is located in the town center, in a former curtain and candle factory converted in 1989.
Education.
Wellfleet, Brewster, Eastham and Orleans make up the Nauset Regional School District. Each town operates its own elementary schools, with a regional middle school and high school accepting the students of all four towns. Wellfleet Elementary School is located just off Route 6 near the town center, and serves students from kindergarten to fifth grade. The Nauset Regional Middle School is located in Orleans, and the Nauset Regional High School is located in neighboring Eastham. There are no private schools in Wellfleet; high school students may, however, choose to attend Cape Cod Regional Technical High School in Harwich free of charge.
Notable residents.
Notable current and former residents of Wellfleet include:

</doc>
<doc id="39059" url="http://en.wikipedia.org/wiki?curid=39059" title="European bison">
European bison

The European Bison ("Bison bonasus"), also known as Wisent ( or ) or the European Wood Bison, is a Eurasian species of bison. It is one of two extant species of bison, alongside the American bison.
European bison were hunted to extinction in the wild, with the last wild animals being shot in the Białowieża Forest (on the Poland-Belarus border) in 1919 and in the northwestern Caucasus in 1927. They have since been reintroduced from captivity into several countries in Europe, all descendants of the Białowieża or lowland European bison. They are now forest-dwelling. They have few predators (besides humans), with only scattered reports from the 19th century of wolf and bear predation. European bison were first scientifically described by Carolus Linnaeus in 1758. Some later descriptions treat the European bison as conspecific with the American bison. It is not to be confused with the aurochs, the extinct ancestor of domestic cattle.
In 1996, the International Union for Conservation of Nature (IUCN) classified the European bison as an endangered species. It has since been downgraded to a vulnerable species. In the past, especially during the Middle Ages, it was commonly killed for its hide, and to produce drinking horns.
Description.
The European bison is the heaviest surviving wild land animal in Europe; a typical European bison is about 2.1 to long, not counting a tail of 30 to long, and 1.6 to tall. At birth, calves are quite small, weighing between 15 and. In the free-ranging population of the Białowieża Forest of Belarus and Poland, body masses among adults (aged 6 and over) are 634 kg on average in the cases of males, with a range of 400 to, and of 424 kg among females, with a range of 300 to. An occasional big bull European bison can weigh up to 1000 kg or more.
On average, it is slightly lighter in body mass and yet taller at the shoulder than the American bison ("Bison bison"). Compared to the American species, the wisent has shorter hair on the neck, head and forequarters, but longer tail and horns.
Etymology.
The modern English word 'wisent' was borrowed in the 19th century from modern German "Wisent" [ˈviːzɛnt], itself from Old High German "wisunt", "wisant", related to Old English "wesend", "weosend" and Old Norse "vísundr". The Old English cognate disappeared as the bison's range shrank away from English-speaking areas by the Late Middle Ages.
The English word 'bison' was borrowed around 1611 from Latin "bisōn" (pl. "bisontes"), itself from Germanic. The root *"wis"-, also found in "weasel", originally referred to the animal's musk.
The word "bonasus" was first mentioned by Aristotle in the 4th century BC when he precisely described the animal calling it Βόνασος(Bonasus) in Greek. He also notes that the Paeonians call it Μόναπος (Monapos).
History.
Historically, the lowland European bison's range encompassed all lowlands of Europe, extending from the Massif Central to the Volga River and the Caucasus. It may have once lived in the Asiatic part of what is now the Russian Federation. Its range decreased as human populations expanded cutting down forests. The last references (Oppian, Claudius Aelianus) to the animal in the transitional Mediterranean/Continental biogeographical region in the Balkans in the area of modern borderline between Greece, Macedonia and Bulgaria date to 3rd century AD. The population of Gaul was extinct in the 8th century AD. The European bison became extinct in southern Sweden in the 11th century, and Southern England in the 12th. The species survived in the Ardennes and the Vosges Mountains until the 15th century. In the Early Middle Ages, the wisent apparently still occurred in the forest steppes east of the Urals, in the Altay Mountains, and seems to have reached Lake Baikal in the east. The northern boundary in the Holocene was probably around 60°N in Finland.
European bison survived in a few natural forests in Europe, but its numbers dwindled. The last European bison in Transylvania died in 1790. In Poland, European bison in the Białowieża Forest were legally the property of the Polish kings until the Third partition of Poland. Wild European bison herds also existed in the forest until the mid-17th century. Polish kings took measures to protect the bison. King Sigismund II Augustus instituted the death penalty for poaching a European bison in Białowieża in the mid-16th century. In the early 19th century, Russian czars retained old Polish laws protecting the European bison herd in Białowieża. Despite these measures and others, the European bison population continued to decline over the following century, with only Białowieża and Northern Caucasus populations surviving into the 20th century.
During World War I, occupying German troops killed 600 of the European bison in the Białowieża Forest for sport, meat, hides and horns. A German scientist informed army officers that the European bison were facing imminent extinction, but at the very end of the war, retreating German soldiers shot all but nine animals. The last wild European bison in Poland was killed in 1919. The last wild European bison in the world was killed by poachers in 1927 in the western Caucasus. By that year, fewer than 50 remained, all held by zoos.
To help manage this captive population, Dr. Heinz Heck began the first studbook for a nondomesticated species, initially as a card index in 1923, leading to a full publication in 1932.
Genetic history.
A 2003 study of mitochondrial DNA indicated four distinct maternal lineages in the species "Bovini":
Y chromosome analysis associated wisent and American bison. An earlier study, using amplified fragment-length polymorphism fingerprinting, showed a close association of wisent and American bison and probably with yak. It noted the interbreeding of "Bovini" species made determining relationships problematic. 
Wisent-American bison hybrids were briefly experimented with in Germany, and a herd of such animals is maintained in Russia. A herd of cattle-wisent crossbreeds (zubron) is maintained in Poland. First-generation crosses do not occur naturally, requiring Caesarean delivery. First-generation males are infertile.
Differences from American bison.
Although superficially similar, a number of physical and behavioural differences are seen between the European bison and the American bison. The European bison has 14 pairs of ribs, while the American bison has 15. Adult European bison are (on average) taller than American bison, and have longer legs. European bison tend to browse more, and graze less than their American relatives, due to their necks being set differently. Compared to the American bison, the nose of the European bison is set further forward than the forehead when the neck is in a neutral position. The body of the European bison is less hairy, though its tail is hairier than that of the American species. The horns of the European bison point forward through the plane of their faces, making them more adept at fighting through the interlocking of horns in the same manner as domestic cattle, unlike the American bison, which favours charging. European bison are less tameable than the American ones, and breed with domestic cattle less readily.
Behaviour and biology.
Social structure and territorial behaviours.
The European bison is a herd animal, which lives in both mixed and solely male groups. Mixed groups consist of adult females, calves, young aged 2–3 years and young adult bulls. The average herd size is dependent on environmental factors, though on average, they number eight to 13 animals per herd. Herds consisting solely of bulls are smaller than mixed ones, containing two individuals on average. European bison herds are not family units. Different herds frequently interact, combine and quickly split after exchanging individuals.
Territory held by bulls is correlated by age, with young bulls aged between five and six tending to form larger home ranges than older males. The European bison does not defend territory, and herd ranges tend to greatly overlap. Core areas of territory are usually sited near meadows and water sources.
Reproduction.
The rutting season occurs from August through to October. Bulls aged 4–6 years, though sexually mature, are prevented from mating by older bulls. Cows usually have a gestation period of 264 days, and typically give birth to one calf at a time.
On average, male calves weigh 27.6 kg at birth, and females 24.4 kg. Body size in males increases proportionately to the age of 6 years. While females have a higher increase in body mass in their first year, their growth rate is comparatively slower than that of males by the age of 3–5. Bulls reach sexual maturity at the age of two, while cows do so in their third year.
Diet.
European bison feed predominantly on grasses, although they will also browse on shoots and leaves; in summer months, an adult male can consume 32 kg of food in a day. European bison in the Białowieża Forest in Poland have traditionally been fed hay in the winter for centuries, and vast herds may gather around this diet supplement. European bison need to drink every day, and in winter can be seen breaking ice with their heavy hooves. Despite their usual slow movements, European bison are surprisingly agile and can clear 3-m-wide streams or 2-m-high fences from a standing start.
Conservation.
The protection of the European bison has a long history; between the 15th and 18th centuries, those in the Forest of Białowieża were protected and their diet supplemented. Efforts to restore this species to the wild began in 1929, with the establishment of the Bison Restitution Centre at Białowieża, Poland. Subsequently, in 1948, the Bison Breeding Centre was established within the Prioksko-Terrasny Biosphere Reserve. On 24 April 2011, five bison were introduced in Pleistocene Park, a project to recreate the steppe ecosystem which began to be altered 10,000 years ago.
Reintroduction.
Beginning in 1951, European bison have been reintroduced into the wild. Free-ranging herds are currently found in Poland, Lithuania, Belarus, Ukraine, Romania, Russia, Slovakia, Latvia, Kyrgyzstan, Germany and in forest preserves in the Western Caucasus. Białowieża Forest, an ancient woodland that straddles the border between Poland and Belarus, is now home to 800 wild bison. Herds have also been introduced in Moldova (2005), Spain (2010), Denmark (2012), and Bulgaria (2012).
Numbers and distribution.
The total worldwide population is around 4,663 (including 2,701 free-ranging) and has been increasing.
Some local populations are estimated as follows:
Plans are made to reintroduce two herds in Germany and in Oostvaardersplassen Nature Reserve in Flevoland (Netherlands). In 2007, a bison pilot project in a fenced area was begun in Zuid-Kennemerland National Park in the Netherlands. Zoos in 30 countries also have quite a few bison. Because of their limited genetic pool, they are considered highly vulnerable to illnesses such as foot-and-mouth disease.
Since 1983, a small reintroduced population lives in the Altai Mountains. This population suffers from inbreeding depression and needs the introduction of unrelated animals for "blood refreshment". In the long term, authorities hope to establish a population of about 1,000 animals in the area. One of the northernmost current populations of the European bison lives in the Vologodskaya Oblast in the Northern Dvina River valley at about 60°N. It survives without supplementary winter feeding. Another Russian population lives in the forests around the Desna River on the border between Russia and Ukraine. The north-easternmost population lives in Pleistocene Park south of Chersky in Siberia. They were introduced in April 2011. The wisents were brought to the park from the Prioksko-Terrasny Nature Reserve near Moscow. Winter tempuratures often drop below -50°C.
In June 2012, one male and six females were moved to the Danish island of Bornholm. The plan is to release these animals into the wild after five years of adjusting to the island's environment. The plan is that the bison will aid biodiversity by naturally maintaining open grassland.
In 2011, three bison were introduced into Alladale Wilderness Reserve in Scotland. Plans to move more into the reserve were made, but the project failed due to not being "well thought through". In April 2013, eight European bison (one male, five females, and two calves) were released into the wild in the Bad Berleburg region of Germany, after 850 years of absence since the species became extinct in that region.
European bison have lived as long as 30 years in captivity, although in the wild their lifespans are shorter. Productive breeding years are between four and 20 years of age in females, and only between six and 12 years of age in males. Wisent occupy home ranges of as much as 100 km2, and some herds are found to prefer meadows and open areas in forests.
European bison can cross-breed with American bison. The products of a German interbreeding programme were destroyed after the Second World War. This programme was related to the impulse which created the Heck cattle. The cross-bred individuals created at other zoos were eliminated from breed books by the 1950s. A Russian back-breeding programme resulted in a wild herd of hybrid animals, which presently lives in the Caucasian Biosphere Reserve (550 animals in 1999).
Also, wisent-cattle hybrids occur, similar to North America beefalo. Cattle and European bison can hybridise fairly readily, but the calves cannot be born naturally (birth is not triggered correctly by the first-cross hybrid calf, and they must therefore be delivered by Caesarian section). In 1847, a herd of wisent-cattle hybrids named "żubroń" was created by Leopold Walicki. The animals were intended to become durable and cheap alternatives to cattle. The experiment was continued by researchers from the Polish Academy of Sciences until the late 1980s. Although the program resulted in a quite successful animal that was both hardy and could be bred in marginal grazing lands, it was eventually discontinued. Currently' the only surviving żubroń herd consists of just a few animals in Białowieża Forest, Poland and Belarus.
The modern herds are managed as two separate lines – one consisting of only "Bison bonasus bonasus" (all descended from only seven animals) and one consisting of all 12 ancestors including the one "B. b. caucasicus" bull. Only a limited amount of inbreeding depression from the population bottleneck has been found, having a small effect on skeletal growth in cows and a small rise in calf mortality. Genetic variability continues to shrink. From five initial bulls, all current European bison bulls have one of only two remaining Y chromosomes.
References.
This article incorporates text from the ARKive fact-file "European bison" under the and the .

</doc>
<doc id="39061" url="http://en.wikipedia.org/wiki?curid=39061" title="Argumentum ad baculum">
Argumentum ad baculum

Argumentum ad baculum (Latin for "argument to the cudgel" or "appeal to the stick"), also known as appeal to force, is an argument where force, coercion, or the "threat of force", is given as a justification. It is a specific case of the negative form of an argument to the consequences. For this reason, it is sometimes referred to as the "Might Makes Right" fallacy.
As a logical argument.
A fallacious argument based on "argumentum ad baculum" generally proceeds as follows:
This form of argument is an informal fallacy, because the attack Q may not necessarily reveal anything about the truth value of the premise P. This fallacy has been identified since the Middle Ages by many philosophers. This is a special case of "argumentum ad consequentiam", or "appeal to consequences".
The non-fallacious "ad baculum".
An "ad baculum" argument is fallacious when the punishment is not meaningfully related to the conclusion being drawn. Many "ad baculum" arguments are not fallacies. For example:
This is called a non-fallacious "ad baculum". The inference is valid because the existence of the punishment is not being used to draw conclusions about the nature of drunk driving itself, but about people for whom the punishment applies. It would become a fallacy if one proceeded from the first premise to argue, for example, that drunk driving is immoral or bad for society. Specifically, the above argument would become a fallacious "ad baculum" if the conclusion stated:
if using the form as above:
External links.
Today, your boobs are pink!
What the fuckin' bitch you are!

</doc>
<doc id="39062" url="http://en.wikipedia.org/wiki?curid=39062" title="French and Indian War">
French and Indian War

The French and Indian War (1754–1763) was the North American theater of the worldwide Seven Years' War. The war was fought between the colonies of British America and New France, with both sides supported by military units from their parent countries of Great Britain and France, as well as Native American allies. At the start of the war, the French North American colonies had a population of roughly 60,000 European settlers, compared to 2 million in the British North American colonies. The outnumbered French particularly depended on the Indians. Long in conflict, the metropole nations declared war on each other in 1756, escalating the war from a regional affair into an international conflict.
The name "French and Indian War" is used mainly in the United States and refers to the two main enemies of the British colonists: the royal French forces and the various indigenous forces allied with them. British and European historians use the term the "Seven Years' War", as do English speaking Canadians. French Canadians call it La guerre de la Conquête (War of Conquest). or the Fourth Intercontinental War.
The war was fought primarily along the frontiers between New France and the British colonies, from Virginia in the South to Nova Scotia in the North. It began with a dispute over control of the confluence of the Allegheny and Monongahela rivers, called the Forks of the Ohio, and the site of the French Fort Duquesne and present-day Pittsburgh, Pennsylvania. The dispute erupted into violence in the Battle of Jumonville Glen in May 1754, during which Virginia militiamen under the command of 22-year-old George Washington ambushed a French patrol.
In 1755, six colonial governors in North America met with General Edward Braddock, the newly arrived British Army commander, and planned a four-way attack on the French. None succeeded and the main effort by Braddock was a disaster; he was defeated in the Battle of the Monongahela on July 9, 1755 and died a few days later. British operations in 1755, 1756 and 1757 in the frontier areas of Pennsylvania and New York all failed, due to a combination of poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian offense. In 1755, the British captured Fort Beauséjour on the border separating Nova Scotia from Acadia; soon afterward they ordered the expulsion of the Acadians. Orders for the deportation were given by William Shirley, Commander-in-Chief, North America, without direction from Great Britain. The Acadians, both those captured in arms and those who had sworn the loyalty oath to His Britannic Majesty, were expelled. Native Americans were likewise driven off their land to make way for settlers from New England.
After the disastrous 1757 British campaigns (resulting in a failed expedition against Louisbourg and the Siege of Fort William Henry, which was followed by Indian torture and massacres of British victims), the British government fell. William Pitt came to power and significantly increased British military resources in the colonies at a time when France was unwilling to risk large convoys to aid the limited forces it had in New France. France concentrated its forces against Prussia and its allies in the European theatre of the war. Between 1758 and 1760, the British military launched a campaign to capture the Colony of Canada. They succeeded in capturing territory in surrounding colonies and ultimately Quebec. Though the British were later defeated at Sainte Foy in Quebec, the French ceded Canada in accordance with the 1763 treaty.
The outcome was one of the most significant developments in a century of Anglo-French conflict. France ceded its territory east of the Mississippi to Great Britain. It ceded French Louisiana west of the Mississippi River (including New Orleans) to its ally Spain, in compensation for Spain's loss to Britain of Florida (Spain had ceded this to Britain in exchange for the return of Havana, Cuba). France's colonial presence north of the Caribbean was reduced to the islands of Saint Pierre and Miquelon, confirming Britain's position as the dominant colonial power in eastern North America.
Origin of the name.
The conflict is known by multiple names. In British America, wars were often named after the sitting British monarch, such as King William's War or Queen Anne's War. As there had already been a King George's War in the 1740s, British colonists named the second war in King George's reign after their opponents, and it became known as the "French and Indian War". This traditional name continues as the standard in the United States, but it obscures the fact that Indians fought on both sides of the conflict, and that this was part of the Seven Years' War, a much larger conflict between France and Great Britain. American historians generally use the traditional name or sometimes the Seven Years' War. Other, less frequently used names for the war include the "Fourth Intercolonial War" and the "Great War for the Empire".
In Europe, the North American theater of the Seven Years' War usually is not given a separate name. The entire international conflict is known as the "Seven Years' War". "Seven Years" refers to events in Europe, from the official declaration of war in 1756 to the signing of the peace treaty in 1763. These dates do not correspond with the fighting on mainland North America, where the fighting between the two colonial powers was largely concluded in six years, from the Battle of Jumonville Glen in 1754 to the capture of Montreal in 1760.
In Canada, both French-speaking and English-speaking Canadians refer to both the European and North American conflicts as the Seven Years' War ("Guerre de Sept Ans"). French Canadians also use the term "War of Conquest" ("Guerre de la Conquête"), since it is the war in which New France was conquered by the British and became part of the British Empire.
North America in the 1750s.
At this time, North America east of the Mississippi River was largely claimed by either Great Britain or France. Large areas had no settlements by Europeans.
The French population numbered about 75,000 and was heavily concentrated along the St. Lawrence River valley, with some also in Acadia (present-day New Brunswick and parts of Nova Scotia, including Île Royale (present-day Cape Breton Island)). Fewer lived in New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries. French fur traders and trappers traveled throughout the St. Lawrence and Mississippi watersheds, did business with local tribes, and often married Indian women. Traders married daughters of chiefs, creating high-ranking unions.
British settlers outnumbered the French 20 to 1 with a population of about 1.5 million ranged along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south. Many of the older colonies had land claims that extended arbitrarily far to the west, as the extent of the continent was unknown at the time their provincial charters were granted. While their population centers were along the coast, the settlements were growing into the interior. Nova Scotia, which had been captured from France in 1713, still had a significant French-speaking population. Britain also claimed Rupert's Land, where the Hudson's Bay Company traded for furs with local tribes.
In between the French and the British, large areas were dominated by native tribes. To the north, the Mi'kmaq and the Abenaki were engaged in Father Le Loutre's War and still held sway in parts of Nova Scotia, Acadia, and the eastern portions of the province of Canada, as well as much of present-day Maine. The Iroquois Confederation dominated much of present-day Upstate New York and the Ohio Country, although the latter also included Algonquian-speaking populations of Delaware and Shawnee, as well as Iroquoian-speaking Mingo. These tribes were formally under Iroquois rule, and were limited by them in authority to make agreements.
Further south the Southeast interior was dominated by Siouan-speaking Catawba, Muskogee-speaking Creek and Choctaw, and the Iroquoian-speaking Cherokee tribes. When war broke out, the French used their trading connections to recruit fighters from tribes in western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British), including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi. The British were supported in the war by the Iroquois Six Nations, and also by the Cherokee – until differences sparked the Anglo-Cherokee War in 1758. In 1758 the Pennsylvania government successfully negotiated the Treaty of Easton, in which a number of tribes in the Ohio Country promised neutrality in exchange for land concessions and other considerations. Most of the other northern tribes sided with the French, their primary trading partner and supplier of arms. The Creek and Cherokee were subject to diplomatic efforts by both the French and British to gain either their support or neutrality in the conflict. It was not uncommon for small bands to participate on the "other side" of the conflict from formally negotiated agreements, as most tribes were decentralized and bands made their own decisions about warfare.
By this time, in eastern North America Spain claimed only the province of Florida; it controlled Cuba and other territories in the West Indies that became military objectives in the Seven Years' War. Florida's European population was a few hundred, concentrated in St. Augustine and Pensacola.
At the start of the war, no French regular army troops were stationed in North America, and few British troops. New France was defended by about 3,000 troupes de la marine, companies of colonial regulars (some of whom had significant woodland combat experience). The colonial government recruited militia support when needed. Most British colonies mustered local militia companies, generally ill trained and available only for short periods, to deal with native threats, but did not have any standing forces.
Because of its large frontier, Virginia had several companies of British regulars. The colonial governments were used to operating independently of each other and of the government in London, a situation that complicated negotiations with Native tribes. Their territories often encompassed land claimed by multiple colonies. After the war began, the leaders of the British Army establishment tried to impose constraints and demands on the colonial administrations.
Events leading to war.
Céloron's expedition.
In June 1747, concerned about the incursion and expanding influence of British traders such as George Croghan in the Ohio Country, Roland-Michel Barrin de La Galissonière, the Governor-General of New France, ordered Pierre-Joseph Céloron to lead a military expedition through the area. Its objectives were to confirm the original French claim to the territory, determine the level of British influence, and impress the Indians with a French show of force.
Céloron's expedition force consisted of about 200 Troupes de la marine and 30 Indians. The expedition covered about 3000 mi between June and November 1749. It went up the St. Lawrence, continued along the northern shore of Lake Ontario, crossed the portage at Niagara, and followed the southern shore of Lake Erie. At the Chautauqua Portage (near present-day Barcelona, New York), the expedition moved inland to the Allegheny River, which it followed to the site of present-day Pittsburgh. There Céloron buried lead plates engraved with the French claim to the Ohio Country. Whenever he encountered British merchants or fur-traders, Céloron informed them of the French claims on the territory and told them to leave.
When Céloron's expedition arrived at Logstown, the Native Americans in the area informed Céloron that they owned the Ohio Country and that they would trade with the British regardless of the French. Céloron continued south until his expedition reached the confluence of the Ohio and the Miami rivers, which lay just south of the village of Pickawillany, the home of the Miami chief known as "Old Briton". Céloron threatened "Old Briton" with severe consequences if he continued to trade with the British. "Old Briton" ignored the warning. Disappointed, Céloron returned to Montreal in November 1749.
In his extensively detailed report, Céloron wrote, "All I can say is that the Natives of these localities are very badly disposed towards the French, and are entirely devoted to the English. I don't know in what way they could be brought back." Even before his return to Montreal, reports on the situation in the Ohio Country were making their way to London and Paris, each side proposing that action be taken. William Shirley, the expansionist governor of the Province of Massachusetts Bay, was particularly forceful, stating that British colonists would not be safe as long as the French were present. Conflicts between the colonies, accomplished through raiding parties that included Indian allies, had taken place for decades, leading to a brisk trade in European colonial captives from either side.
Negotiations.
In 1749 the British government gave land to the Ohio Company of Virginia for the purpose of developing trade and settlements in the Ohio Country. The grant required that it settle 100 families in the territory, and construct a fort for their protection. But, as the territory was also claimed by Pennsylvania, both colonies began pushing for action to improve their respective claims. In 1750 Christopher Gist, acting on behalf of both Virginia and the company, explored the Ohio territory and opened negotiations with the Indian tribes at Logstown. He completed the 1752 Treaty of Logstown in which the local Indians, through their "Half-King" Tanacharison and an Iroquois representative, agreed to terms that included permission to build a "strong house" at the mouth of the Monongahela River (the site of present-day Pittsburgh, Pennsylvania). By the late 17th century, the Iroquois had pushed many tribes out of the Ohio Valley, and kept it as hunting ground by right of conquest.
The War of the Austrian Succession (whose North American theater is known as King George's War) formally ended in 1748 with the signing of the Treaty of Aix-la-Chapelle. The treaty was primarily focused on resolving issues in Europe. The issues of conflicting territorial claims between British and French colonies in North America were turned over to a commission to resolve, but it reached no decision. Frontiers from between Nova Scotia and Acadia in the north, to the Ohio Country in the south, were claimed by both sides. The disputes also extended into the Atlantic Ocean, where both powers wanted access to the rich fisheries of the Grand Banks off Newfoundland.
Attack on Pickawillany.
On March 17, 1752, the Governor-General of New France, Marquis de la Jonquière, died and was temporarily replaced by Charles le Moyne de Longueuil. His permanent replacement, the Marquis Duquesne, did not arrive in New France until 1752 to take over the post. The continuing British activity in the Ohio territories prompted Longueuil to dispatch another expedition to the area under the command of Charles Michel de Langlade, an officer in the Troupes de la Marine. Langlade was given 300 men, including French-Canadians and warriors of the Ottawa. His objective was to punish the Miami people of Pickawillany for not following Céloron's orders to cease trading with the British. On June 21, the French war party attacked the trading centre at Pickawillany, capturing three traders and killing 14 people of the Miami nation, including Old Briton. He was reportedly ritually cannibalized by some aboriginal members of the expedition.
French fort construction.
In the spring of 1753, Paul Marin de la Malgue was given command of a 2,000-man force of Troupes de la Marine and Indians. His orders were to protect the King's land in the Ohio Valley from the British. Marin followed the route that Céloron had mapped out four years earlier, but where Céloron had limited the record of French claims to the burial of lead plates, Marin constructed and garrisoned forts. He first constructed Fort Presque Isle (near present-day Erie, Pennsylvania) on Lake Erie's south shore. He had a road built to the headwaters of LeBoeuf Creek. Marin constructed a second fort at Fort Le Boeuf (present-day Waterford, Pennsylvania), designed to guard the headwaters of LeBoeuf Creek. As he moved south, he drove off or captured British traders, alarming both the British and the Iroquois. Tanaghrisson, a chief of the Mingo, who were remnants of Iroquois and other tribes who had been driven west by colonial expansion. He intensely disliked the French (whom he accused of killing and eating his father). Traveling to Fort Le Boeuf, he threatened the French with military action, which Marin contemptuously dismissed.
The Iroquois sent runners to the manor of William Johnson in upstate New York. The British Superintendent for Indian Affairs in the New York region and beyond, Johnson was known to the Iroquois as "Warraghiggey", meaning "He who does great things." He spoke their languages and had become a respected honorary member of the Iroquois Confederacy in the area. In 1746, Johnson was made a colonel of the Iroquois. Later he was commissioned as a colonel of the Western New York Militia. They met at Albany, New York with Governor Clinton and officials from some of the other American colonies. Mohawk Chief Hendrick, Speaker of their tribal council, insisted that the British abide by their obligations and block French expansion. When Clinton did not respond to his satisfaction, Chief Hendrick said that the "Covenant Chain", a long-standing friendly relationship between the Iroquois Confederacy and the British Crown, was broken.
Virginia's response.
Governor Robert Dinwiddie of Virginia was an investor in the Ohio Company, which stood to lose money if the French held their claim. To counter the French military presence in Ohio, in October 1753 Dinwiddie ordered the 21-year-old Major George Washington (whose brother was another Ohio Company investor) of the Virginia Regiment to warn the French to leave Virginia territory. Washington left with a small party, picking up along the way Jacob Van Braam as an interpreter; Christopher Gist, a company surveyor working in the area; and a few Mingo led by Tanaghrisson. On December 12, Washington and his men reached Fort Le Boeuf.
Jacques Legardeur de Saint-Pierre, who succeeded Marin as commander of the French forces after the latter died on October 29, invited Washington to dine with him. Over dinner, Washington presented Saint-Pierre with the letter from Dinwiddie demanding an immediate French withdrawal from the Ohio Country. Saint-Pierre said, "As to the Summons you send me to retire, I do not think myself obliged to obey it." He told Washington that France's claim to the region was superior to that of the British, since René-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.
Leaving Fort Le Boeuf early on December 16, Washington and his party arrived in Williamsburg on January 16, 1754. In his report, Washington stated, "The French had swept south", detailing the steps they had taken to fortify the area, and their intention to fortify the confluence of the Allegheny and Monongahela rivers.
Course of the war.
Even before Washington returned, Dinwiddie had sent a company of 40 men under William Trent to that point, where in the early months of 1754 they began construction of a small stockaded fort. Governor Duquesne sent additional French forces under Claude-Pierre Pecaudy de Contrecœur to relieve Saint-Pierre during the same period, and Contrecœur led 500 men south from Fort Venango on April 5, 1754. When these forces arrived at the fort on April 16, Contrecœur generously allowed Trent's small company to withdraw. He purchased their construction tools to continue building what became Fort Duquesne.
After Washington had returned to Williamsburg, Dinwiddie ordered him to lead a larger force to assist Trent in his work. While en route, Washington learned of Trent's retreat. Since Tanaghrisson had promised support to the British, Washington continued toward Fort Duquesne and met with the Mingo leader. Learning of a French scouting party in the area, Washington, with Tanaghrisson and his party, surprised the Canadians on May 28 in what became known as the Battle of Jumonville Glen. They killed many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville, whose head was reportedly split open by Tanaghrisson with a tomahawk. The historian Fred Anderson suggests that Tanaghrisson was acting to gain the support of the British and regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships. One of Tanaghrisson's men told Contrecoeur that Jumonville had been killed by British musket fire.
Historians generally consider the Battle of Jumonville Glen as the opening battle of the French and Indian War in North America, and the start of hostilities in the Ohio valley.
Following the battle, Washington pulled back several miles and established Fort Necessity, which the French attacked on July 3. Washington surrendered; he negotiated a withdrawal under arms. One of Washington's men reported that the French force was accompanied by Shawnee, Delaware, and Mingo native warriors—just those whom Tanaghrisson was seeking to influence.
News of the two battles reached England in August. After several months of negotiations, the government of the Duke of Newcastle decided to send an army expedition the following year to dislodge the French. They chose Major General Edward Braddock to lead the expedition. Word of the British military plans leaked to France well before Braddock's departure for North America. In response, King Louis XV dispatched six regiments to New France under the command of Baron Dieskau in 1755. The British, intending to blockade French ports, sent out their fleet in February 1755, but the French fleet had already sailed. Admiral Edward Hawke detached a fast squadron to North America in an attempt to intercept the French.
In a second British action, Admiral Edward Boscawen fired on the French ship "Alcide" on June 8, 1755, capturing her and two troop ships. The British harassed French shipping throughout 1755, seizing ships and capturing seamen. These actions contributed to the eventual formal declarations of war in spring 1756.
Albany Congress.
An early important political response to the opening of hostilities was the convening of the Albany Congress in June and July, 1754. The goal of the congress was to formalize a unified front in trade and negotiations with various Indians, since allegiance of the various tribes and nations was seen to be pivotal in the success in the war that was unfolding. The plan that the delegates agreed to was never ratified by the colonial legislatures nor approved of by the crown. Nevertheless, the format of the congress and many specifics of the plan became the prototype for confederation during the War of Independence.
British campaigns, 1755.
The British formed an aggressive plan of operations for 1755. General Braddock was to lead the expedition to Fort Duquesne. While the Massachusetts provincial governor William Shirley was given the task of fortifying Fort Oswego and attacking Fort Niagara, Sir William Johnson was to capture Fort St. Frédéric (at present-day Crown Point, New York). Lieutenant Colonel Robert Monckton was to capture Fort Beauséjour to the east, on the frontier between Nova Scotia and Acadia.
Braddock (with George Washington as one of his aides) led about 1,500 army troops and provincial militia on an expedition in June 1755 to take Fort Duquesne. The expedition was a disaster. It was attacked by French and Indian soldiers ambushing them from up in trees and behind logs. Braddock called for a retreat. He was killed. Approximately 1,000 British soldiers were killed or injured. The remaining 500 British troops, led by George Washington, retreated to Virginia. Two future opponents in the American Revolutionary War, Washington and Thomas Gage, played key roles in organizing the retreat.
The French acquired a copy of the British war plans, including the activities of Shirley and Johnson. Shirley's efforts to fortify Oswego were bogged down in logistical difficulties, exacerbated by Shirley's inexperience in managing large expeditions. In conjunction, Shirley was made aware that the French were massing for an attack on Fort Oswego in his absence when he planned to attack Fort Niagara.As a response, Shirley left garrisons at Oswego, Fort Bull, and Fort Williams (the latter two located on the Oneida Carry between the Mohawk River and Wood Creek at present-day Rome, New York). Supplies for use in the projected attack on Niagara were cached at Fort Bull.
Johnson's expedition was better organized than Shirley's, which was noticed by New France's governor, the Marquis de Vaudreuil. He had primarily been concerned about the extended supply line to the forts on the Ohio, and had sent Baron Dieskau to lead the defenses at Frontenac against Shirley's expected attack. When Johnson was seen as the larger threat, Vaudreuil sent Dieskau to Fort St. Frédéric to meet that threat. Dieskau planned to attack the British encampment at Fort Edward at the upper end of navigation on the Hudson River, but Johnson had strongly fortified it, and Dieskau's Indian support was reluctant to attack. The two forces finally met in the bloody Battle of Lake George between Fort Edward and Fort William Henry. The battle ended inconclusively, with both sides withdrawing from the field. Johnson's advance stopped at Fort William Henry, and the French withdrew to Ticonderoga Point, where they began the construction of Fort Carillon (later renamed Fort Ticonderoga after British capture in 1759).
Colonel Monckton, in the sole British success that year, captured Fort Beauséjour in June 1755, cutting the French fortress at Louisbourg off from land-based reinforcements. To cut vital supplies to Louisbourg, Nova Scotia's Governor Charles Lawrence ordered the deportation of the French-speaking Acadian population from the area. Monckton's forces, including companies of Rogers' Rangers, forcibly removed thousands of Acadians, chasing down many who resisted, and sometimes committing atrocities. More than any other factor, the cutting off of supplies to Louisbourg led to its demise. The Acadian resistance, in concert with native allies, including the Mi'kmaq, was sometimes quite stiff, with ongoing frontier raids (against Dartmouth and Lunenburg among others). Other than the campaigns to expel the Acadians (ranging around the Bay of Fundy, on the Petitcodiac and St. John rivers, and Île Saint-Jean), the only clashes of any size were at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757.
French victories, 1756–1757.
Following the death of Braddock, William Shirley assumed command of British forces in North America. At a meeting in Albany in December 1755, he laid out his plans for 1756. In addition to renewing the efforts to capture Niagara, Crown Point and Duquesne, he proposed attacks on Fort Frontenac on the north shore of Lake Ontario and an expedition through the wilderness of the Maine district and down the Chaudière River to attack the city of Quebec. Bogged down by disagreements and disputes with others, including William Johnson and New York's Governor Sir Charles Hardy, Shirley's plan had little support.
Newcastle replaced him in January 1756 with Lord Loudoun, with Major General James Abercrombie as his second in command. Neither of these men had as much campaign experience as the trio of officers France sent to North America. French regular army reinforcements arrived in New France in May 1756, led by Major General Louis-Joseph de Montcalm and seconded by the Chevalier de Lévis and Colonel François-Charles de Bourlamaque, all experienced veterans from the War of the Austrian Succession. During that time in Europe, on May 18, 1756, England formally declared war on France, which expanded the war into Europe, which was later to be known as the Seven Years' War.
Governor Vaudreuil, who harboured ambitions to become the French commander in chief (in addition to his role as governor), acted during the winter of 1756 before those reinforcements arrived. Scouts had reported the weakness of the British supply chain, so he ordered an attack against the forts Shirley had erected at the Oneida Carry. In the March Battle of Fort Bull, French forces destroyed the fort and large quantities of supplies, including 45,000 pounds of gunpowder. They set back any British hopes for campaigns on Lake Ontario, and endangered the Oswego garrison, already short on supplies. French forces in the Ohio valley also continued to intrigue with Indians throughout the area, encouraging them to raid frontier settlements. This led to ongoing alarms along the western frontiers, with streams of refugees returning east to get away from the action.
The new British command was not in place until July. When he arrived in Albany, Abercrombie refused to take any significant actions until Loudoun approved them. Montcalm took bold action against his inertia. Building on Vaudreuil's work harassing the Oswego garrison, Montcalm executed a strategic feint by moving his headquarters to Ticonderoga, as if to presage another attack along Lake George. With Abercrombie pinned down at Albany, Montcalm slipped away and led the successful attack on Oswego in August. In the aftermath, Montcalm and the Indians under his command disagreed about the disposition of prisoners' personal effects. The Europeans did not consider them prizes and prevented the Indians from stripping the prisoners of their valuables, which angered the Indians.
Loudoun, a capable administrator but a cautious field commander, planned one major operation for 1757: an attack on New France's capital, Quebec. Leaving a sizable force at Fort William Henry to distract Montcalm, he began organizing for the expedition to Quebec. He was then ordered by William Pitt, the Secretary of State responsible for the colonies, to attack Louisbourg first. Beset by delays of all kinds, the expedition was finally ready to sail from Halifax, Nova Scotia in early August. In the meantime French ships had escaped the British blockade of the French coast, and a fleet outnumbering the British one awaited Loudoun at Louisbourg. Faced with this strength, Loudoun returned to New York amid news that a massacre had occurred at Fort William Henry.
French irregular forces (Canadian scouts and Indians) harassed Fort William Henry throughout the first half of 1757. In January they ambushed British rangers near Ticonderoga. In February they launched a daring raid against the position across the frozen Lake George, destroying storehouses and buildings outside the main fortification. In early August, Montcalm and 7,000 troops besieged the fort, which capitulated with an agreement to withdraw under parole. When the withdrawal began, some of Montcalm's Indian allies, angered at the lost opportunity for loot, attacked the British column, killing and capturing several hundred men, women, children, and slaves. The aftermath of the siege may have contributed to the transmission of smallpox into remote Indian populations; as some Indians were reported to have traveled from beyond the Mississippi to participate in the campaign and returned afterward having been exposed to European carriers.
British conquest, 1758–1760.
Vaudreuil and Montcalm were minimally resupplied in 1758, as the British blockade of the French coastline limited French shipping. The situation in New France was further exacerbated by a poor harvest in 1757, a difficult winter, and the allegedly corrupt machinations of François Bigot, the intendant of the territory. His schemes to supply the colony inflated prices and were believed by Montcalm to line his pockets and those of his associates. A massive outbreak of smallpox among western tribes led many of them to stay away from trading in 1758. While many parties to the conflict blamed others (the Indians blamed the French for bringing "bad medicine" as well as denying them prizes at Fort William Henry), the disease was probably spread through the crowded conditions at William Henry after the battle. Montcalm focused his meager resources on the defense of the St. Lawrence, with primary defenses at Carillon, Quebec, and Louisbourg, while Vaudreuil argued unsuccessfully for a continuation of the raiding tactics that had worked quite effectively in previous years.
The British failures in North America, combined with other failures in the European theater, led to the fall from power of Newcastle and his principal military advisor, the Duke of Cumberland. Newcastle and Pitt joined in an uneasy coalition in which Pitt dominated the military planning. He embarked on a plan for the 1758 campaign that was largely developed by Loudoun. He had been replaced by Abercrombie as commander in chief after the failures of 1757. Pitt's plan called for three major offensive actions involving large numbers of regular troops, supported by the provincial militias, aimed at capturing the heartlands of New France. Two of the expeditions were successful, with Fort Duquesne and Louisbourg falling to sizable British forces.
1758.
The Forbes Expedition was a British campaign in September–October 1758, with 6,000 troops led by General John Forbes to drive the French out of the contested Ohio Country. After a British advance party on Fort Duquesne was repulsed on September 14, the French withdrew from Fort Duquesne, leaving the British in control of the Ohio River Valley. The great French fortress at Louisbourg in Nova Scotia was captured after a siege.
The third invasion was stopped with the improbable French victory in the Battle of Carillon, in which 3,600 Frenchmen famously and decisively defeated Abercrombie's force of 18,000 regulars, militia and Native American allies outside the fort the French called Carillon and the British called Ticonderoga. Abercrombie saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac, including caches of supplies destined for New France's western forts and furs destined for Europe. Abercrombie was recalled and replaced by Jeffery Amherst, victor at Louisbourg.
In the aftermath of generally poor French results in most theaters of the Seven Years' War in 1758, France's new foreign minister, the duc de Choiseul, decided to focus on an invasion of Britain, to draw British resources away from North America and the European mainland. The invasion failed both militarily and politically, as Pitt again planned significant campaigns against New France, and sent funds to Britain's ally on the mainland, Prussia, and the French Navy failed in the 1759 naval battles at Lagos and Quiberon Bay. In one piece of good fortune, some French supply ships managed to depart France, eluding the British blockade of the French coast.
1759–1760.
British victories continued in all theaters in the Annus Mirabilis of 1759, when they finally captured Ticonderoga, James Wolfe defeated Montcalm at Quebec (in a battle that claimed the lives of both commanders), and victory at Fort Niagara successfully cut off the French frontier forts further to the west and south. The victory was made complete in 1760, when, despite losing outside Quebec City in the Battle of Sainte-Foy, the British were able to prevent the arrival of French relief ships in the naval Battle of the Restigouche while armies marched on Montreal from three sides.
In September 1760, Governor Vaudreuil negotiated a surrender with General Amherst. Amherst granted Vaudreuil's request that any French residents who chose to remain in the colony would be given freedom to continue worshiping in their Roman Catholic tradition, continued ownership of their property, and the right to remain undisturbed in their homes. The British provided medical treatment for the sick and wounded French soldiers and French regular troops were returned to France aboard British ships with an agreement that they were not to serve again in the present war.
End of the war.
Most of the fighting between France and Britain in continental North America ended in 1760, while the fighting in Europe continued. The notable exception was the French seizure of St. John's, Newfoundland. When General Amherst heard of this surprise action, he immediately dispatched troops under his nephew William Amherst, who regained control of Newfoundland after the Battle of Signal Hill in September 1762.
Many troops from North America were reassigned to participate in further British actions in the West Indies, including the capture of Spanish Havana when Spain belatedly entered the conflict on the side of France, and a British expedition against French Martinique in 1762, led by (the now) Major General Robert Monckton.
General Amherst also oversaw the transition of French forts in the western lands to British control. The policies he introduced in those lands disturbed large numbers of Indians, and contributed to the outbreak in 1763 of the conflict known as Pontiac's Rebellion. This series of attacks on frontier forts and settlements required the continued deployment of British troops, and was not resolved until 1766.
The war in North America officially ended with the signing of the Treaty of Paris on February 10, 1763, and war in the European theatre of the Seven Years' War was settled by the Treaty of Hubertusburg on February 15, 1763. The British offered France the choice of surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique, which had been occupied by the British. France chose to cede the former, but was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence, along with fishing rights in the area. They viewed the economic value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent. The contemporaneous French philosopher Voltaire referred to Canada disparagingly as nothing more than a few acres of snow. The British, for their part, were happy to take New France, as defence of their North American colonies would no longer be an issue and also because they already had ample places from which to obtain sugar. Spain, which traded Florida to Britain to regain Cuba, also gained Louisiana, including New Orleans, from France in compensation for its losses. Great Britain and Spain also agreed that navigation on the Mississippi River was to be open to vessels of all nations.
Consequences.
The war changed economic, political, governmental and social relations between three European powers (Britain, France, and Spain), their colonies and colonists, and the natives that inhabited the territories they claimed. France and Britain both suffered financially because of the war, with significant long-term consequences.
Britain gained control of French Canada and Acadia, colonies containing approximately 80,000 primarily French-speaking Roman Catholic residents. The deportation of Acadians beginning in 1755 resulted in land made available to migrants from Europe and the colonies further south. The British resettled many Acadians throughout its North American provinces, but many went to France, and some went to New Orleans, which they had expected to remain French. Some were sent to colonize places as diverse as French Guiana and the Falkland Islands; these latter efforts were unsuccessful. Others migrated to places like Saint-Domingue, and fled to New Orleans after the Haitian Revolution. The Louisiana population contributed to the founding of the modern Cajun population. (The French word "Acadien" evolved to "Cadien", then to "Cajun".)
Following the treaty, King George III issued the Royal Proclamation of 1763 on October 7, 1763, which outlined the division and administration of the newly conquered territory, and to some extent continues to govern relations between the government of modern Canada and the First Nations. Included in its provisions was the reservation of lands west of the Appalachian Mountains to its Indian population, a demarcation that was at best a temporary impediment to a rising tide of westward-bound settlers. The proclamation also contained provisions that prevented civic participation by the Roman Catholic Canadians. When accommodations were made in the Quebec Act in 1774 to address this and other issues, religious concerns were raised in the largely Protestant Thirteen Colonies over the advance of "popery"; the Act maintained French Civil law in the form of the seigneurial system, a medieval code soon to be removed from France within a generation by the French Revolution.
The Seven Years' War nearly doubled Britain's national debt. The Crown, seeking sources of revenue to pay off the debt, attempted to impose new taxes on its colonies. These attempts were met with increasingly stiff resistance, until troops were called in so that representatives of the Crown could safely perform their duties. These acts ultimately led to the start of the American Revolutionary War.
France attached comparatively little value to its North American possessions, especially in respect to the highly profitable sugar-producing Antilles islands, which it managed to retain. Minister Choiseul considered he had made a good deal at the Treaty of Paris, and philosopher Voltaire wrote that Louis XV had lost "a few acres of snow". For France however, the military defeat and the financial burden of the war weakened the monarchy and contributed to the advent of the French Revolution in 1789.
For many native populations, the elimination of French power in North America meant the disappearance of a strong ally and counterweight to British expansion, leading to their ultimate dispossession. The Ohio Country was particularly vulnerable to legal and illegal settlement due to the construction of military roads to the area by Braddock and Forbes. Although the Spanish takeover of the Louisiana territory (which was not completed until 1769) had modest repercussions, the British takeover of Spanish Florida resulted in the westward migration of tribes that did not want to do business with the British, and a rise in tensions between the Choctaw and the Creek, historic enemies whose divisions the British at times exploited. The change of control in Florida also prompted most of its Spanish Catholic population to leave. Most went to Cuba, including the entire governmental records from St. Augustine, although some Christianized Yamasee were resettled to the coast of Mexico.
France returned to North America in 1778 with the establishment of a Franco-American alliance against Great Britain in the American War of Independence. This time France succeeded in prevailing over Great Britain, in what historian Alfred Cave describes as "French [...] revenge for Montcalm's death".
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="39064" url="http://en.wikipedia.org/wiki?curid=39064" title="Casimir IV Jagiellon">
Casimir IV Jagiellon

Casimir IV KG (Polish: "Kazimierz IV Jagiellończyk" ]; Lithuanian: "Kazimieras Jogailaitis"; 30 November 1427 – 7 June 1492) of the House of Jagiellon was Grand Duke of Lithuania from 1440, and King of Poland from 1447, until his death. He was one of the most active Polish rulers, under whom Poland, by defeating the Teutonic Knights in the Thirteen Years' War recovered Pomerania, and the Jagiellonian dynasty became one of the leading royal houses in Europe. He was a strong opponent of aristocracy, and helped to strengthen the importance of Parliament and the Senate.
The great triumph of his reign was the effective and final destruction of the Teutonic Order, which brought Prussia under Polish rule. The long and brilliant rule of Casimir corresponded to the age of “new monarchies” in western Europe. By the 15th century Poland had narrowed the distance separating it from western Europe and become a significant factor in international relations. The demand for raw materials and semi-finished goods stimulated trade, producing a positive balance, and contributed to the growth of crafts and mining in the entire country.
He was a recipient of the English Order of the Garter (KG), the highest order of chivalry and the most prestigious honour in England and of the United Kingdom, awarded at the Sovereign's pleasure as his or her personal gift, on recipients from the United Kingdom and other Commonwealth realms.
Casimir was the second son of King Władysław II Jagiełło, and the younger brother of King Władysław III of Varna.
Youth.
Casimir was the second son of King Władysław II Jagiełło and his fourth wife, Sophia of Halshany. His father was already over 75 at Casimir’s birth, and his brother Władysław III, three years his senior, was expected to become king before his majority. Casimir was thus the second in succession to the throne, and, after Władysław had succeeded his father in 1434, he became the legal heir. Strangely, little was done for his education; he was never taught Latin, nor was he trained for the responsibilities of office, despite the fact he was the only brother of the rightful sovereign.
Grand Duke of Lithuania.
The sudden death of Sigismund Kęstutaitis left the office of the Grand Duchy of Lithuania empty. The Voivode of Trakai, Jonas Goštautas, and other magnates of Lithuania, supported Casimir as a candidate to the throne. However many Polish noblemen hoped that the thirteen-year-old boy would become a Vice-regent for the Polish King in Lithuania. Casimir was invited by the Lithuanian magnates to Lithuania, and when he arrived in Vilnius in 1440, he was proclaimed as the Grand Duke of Lithuania on 29 June 1440 by the Council of Lords, contrary to the wishes of the Polish noble lords—an act supported and coordinated by Jonas Goštautas. When the news arrived in the Kingdom of Poland concerning the proclamation of Casimir as the Grand Duke of Lithuania, it was met with hostility, even to the point of military threats against Lithuania. Since the young Grand Duke was underage, the supreme control over the Grand Duchy of Lithuania was in the hands of the Council of Lords, presided by Jonas Goštautas. Casimir had been taught Lithuanian language and the customs of Lithuania by appointed court officials.
During Casimir's rule the rights of the Lithuanian nobility—dukes, magnates and boyars (lesser nobles), irrespective of their religion and ethnicity—were put on an equal footing to those of the Polish szlachta. Additionally, Casimir promised to protect the Grand Duchy's borders and not to appoint persons from the Polish Kingdom to the offices of the Grand Duchy. He accepted that decisions on matters concerning the Grand Duchy would not be made without the Council of Lords' consent. He also granted the subject region of Samogitia the right to elect its own elder. Casimir was the first ruler of Lithuania baptised at birth, becoming the first native Roman Catholic Grand Duke.
King of Poland.
Casimir succeeded his brother Władysław III (killed at the Battle of Varna in 1444) as King of Poland after a three-year interregnum on 25 June 1447. In 1454, he married Elisabeth of Austria, daughter of the late King of the Romans Albert II of Habsburg by his late wife Elisabeth of Bohemia. Her distant relative Frederick of Habsburg became Holy Roman Emperor and reigned as Frederick III until after Casimir's own death. The marriage strengthened the ties between the house of Jagiellon and the sovereigns of Hungary-Bohemia and put Casimir at odds with the Holy Roman Emperor through internal Habsburg rivalry.
That same year, Casimir was approached by the Prussian Confederation for aid against the Teutonic Order, which he promised, by making the separatist Prussian regions a protectorate of the Polish Kingdom. However, when the insurgent cities rebelled against the Order, it resisted and the Thirteen Years' War (1454–1466) ensued. Casimir and the Prussian Confederation defeated the Teutonic Order, taking over its capital at Marienburg (Malbork Castle). In the Second Peace of Thorn (1466), the Order recognized Polish sovereignty over the seceded western Prussian regions, Royal Prussia, and the Polish crown's overlordship over the remaining Teutonic Monastic State, transformed in 1525 into a duchy, Ducal Prussia.
Elisabeth's only brother Ladislas, king of Bohemia and Hungary, died in 1457, and after that Casimir and Elisabeth's dynastic interests were directed also towards her brother's former kingdoms.
King Casimir IV died on 7 June 1492 in the Old Hrodna Castle in the Grand Duchy of Lithuania, which was in a personal union with Poland.
Foreign policies.
The intervention of the Curia, which hitherto had been hostile to Casimir because of his steady and patriotic resistance to papal aggression, was due to the permutations of European politics. The pope was anxious to get rid of the Hussite King of Bohemia, George Podebrad, as the first step towards the formation of a league against the Turk. Casimir was to be a leading factor in this combination, and he took advantage of it to procure the election of his son Vladislaus II as King of Bohemia. But he would not commit himself too far, and his ulterior plans were frustrated by the rivalry of Matthias Corvinus, King of Hungary, who even went so far as to stimulate the Teutonic Order to rise against Casimir. The death of Matthias in 1490 was a great relief to Poland, and Casimir employed the two remaining years of his reign in consolidating his position still further.
Legacy and opinion of reign.
In domestic affairs Casimir was relatively passive but anxious to preserve the prerogatives of the crown, notably his right to nominate bishops. In the question of territories in dispute between his two states (Volhynia and Podolia) he favoured Lithuania. During the war against the Teutonic Order he was forced to grant the Polish nobility substantial concessions by the Privilege (statute) of Nieszawa (November 1454); these, however, became important only after his death, and royal power was not greatly diminished during his lifetime. The feature of Casimir's character which most impressed his contemporaries was his extraordinary simplicity and sobriety. He, one of the greatest monarchs in Europe, habitually wore plain cloth from Kraków, drank nothing but water, and kept the most austere of tables. His one passion was the chase. Yet his liberality to his ministers and servants was proverbial, and his vanquished enemies he always treated with magnificent generosity.
Casimir was neither a splendid ruler nor a good and wise administrator, but a mistrusting, cautious, and sober head of a large family who regarded Lithuania as his personal estate, however his reign was remembered as being both successful and the most peaceful in the history of Poland.
Culture.
During Casimir's rule the cultural progress was striking, with the reconstituted and enlarged University of Kraków playing a major role. Humanist trends found a promoter at Kraków in the Italian scholar Filippo de Buonacorsi, known as Callimachus. From the pen of Jan Długosz came the first major, royal history of Poland.
Curse of the Royal Tomb.
The remains of King Casimir IV and his wife Elisabeth were interred in a tomb situated in the chapel of the Wawel Castle in Kraków, Poland. With the consent of then Cardinal Karol Wojtyła (Archbishop of Kraków, who became Pope John Paul II), a team of scientists was given permission to open the tomb and examine the remains, with restoration as the ultimate objective. Casimir's tomb was opened on Friday, April 13, 1973. Twelve researchers were present. Inside the tomb they found a wooden coffin that was heavily rotted. It contained what was left of the king's decayed corpse.
Within a few days, four of the twelve scientists and researchers had died. Not long after, there were only two survivors: Dr. Bolesław Smyk, a microbiologist, and Dr. Edward Roszycki. Smyk was to suffer problems with his equilibrium for the next five years. In the course of his microbiological examinations, Dr. Smyk found traces of fungi on the royal insignia taken from the tomb. He identified three species - Aspergillus flavus, Penicillim rubrum, and Penicillim rugulosum. These fungi are known to produce aflatoxins that can be deadly when in contact with skin and inhaled into the lungs.

</doc>
<doc id="39066" url="http://en.wikipedia.org/wiki?curid=39066" title="List of religions and spiritual traditions">
List of religions and spiritual traditions

Religion is a collection of cultural systems, beliefs, and world views that establishes symbols that relate humanity to spirituality and, sometimes, to moral values. While religion is hard to define, one standard model of religion, used in religious studies courses, was proposed by Clifford Geertz, who simply called it a "cultural system". A critique of Geertz's model by Talal Asad categorized religion as "an anthropological category". Many religions have narratives, symbols, traditions and sacred histories that are intended to give meaning to life or to explain the origin of life or the universe. They tend to derive morality, ethics, religious laws or a preferred lifestyle from their ideas about the cosmos and human nature. According to some estimates, there are roughly 4,200 religions in the world.
The word "religion" is sometimes used interchangeably with "faith" or "belief system", but religion differs from private belief in that it has a public aspect. Most religions have organized behaviors, including clerical hierarchies, a definition of what constitutes adherence or membership, congregations of laity, regular meetings or services for the purposes of veneration of a deity or for prayer, holy places (either natural or architectural), and/or religious texts. Certain religions also have a sacred language often used in liturgical services. The practice of a religion may also include sermons, commemoration of the activities of a god or gods, sacrifices, festivals, feasts, trance, initiations, funerals, marriages, meditation, music, art, dance, public service, or other aspects of human culture. Religious beliefs have also been used to explain parapsychological phenomena such as out-of-body experiences, near-death experiences and reincarnation, along with many other paranormal experiences.
Some academics studying the subject have divided religions into three broad categories: world religions, a term which refers to transcultural, international faiths; indigenous religions, which refers to smaller, culture-specific or nation-specific religious groups; and new religious movements, which refers to recently developed faiths. One modern academic theory of religion, social constructionism, says that religion is a modern concept that suggests all spiritual practice and worship follows a model similar to the Abrahamic religions as an orientation system that helps to interpret reality and define human beings, and thus religion, as a concept, has been applied inappropriately to non-Western cultures that are not based upon such systems, or in which these systems are a substantially simpler construct.
Abrahamic religions.
A group of monotheistic traditions sometimes grouped with one another for comparative purposes, because all refer to a patriarch named Abraham.
Christianity.
Other Christian.
Certain Christian groups are difficult to classify as "Eastern" or "Western"
Gnosticism.
Many Gnostic groups were closely related to early Christianity, for example, Valentinism. Irenaeus wrote polemics against them from the standpoint of the then-unified Catholic Church.
The Yazidis are a syncretic Kurdish religion with a Gnostic influence:
None of these religions are still extant.
Judaism and related religions.
Samaritans use a slightly different version of the Pentateuch as their Torah, worshiping at Mount Gerizim instead of Jerusalem, and are possibly the descendants of the lost Northern Kingdom. They are definitely of ancient Israelite origin, but their status as Jews is disputed.
Second Temple Judaism
Indian religions.
Indian religions are the religions that originated in the Indian subcontinent; namely Hinduism, Jainism, Buddhism and Sikhism and religions and traditions related to, and descended from, them.
African diasporic religions.
African diasporic religions are a number of related religions that developed in the Americas among African slaves and their descendants in various countries of the Caribbean Islands and Latin America, as well as parts of the southern United States. They derive from African traditional religions, especially of West and Central Africa, showing similarities to the Yoruba religion in particular.
Indigenous traditional religions.
Traditionally, these faiths have all been classified "Pagan", but scholars prefer the terms "indigenous/primal/folk/ethnic religions".

</doc>
<doc id="39068" url="http://en.wikipedia.org/wiki?curid=39068" title="Digital electronics">
Digital electronics

Digital electronics, or digital (electronic) circuits, are electronics that represent signals by discrete bands of analog levels, rather than by continuous ranges (as used in analogue electronics). All levels within a band represent the same signal state. Because of this discretization, relatively small changes to the analog signal levels due to manufacturing tolerance, signal attenuation or parasitic noise do not leave the discrete envelope, and as a result are ignored by signal state sensing circuitry.
In most cases the number of these states is two, and they are represented by two voltage bands: one near a reference value (typically termed as "ground" or zero volts), and the other a value near the supply voltage. These correspond to the "false" ("0") and "true" ("1") values of the Boolean domain, respectively, yielding binary code.
Digital techniques are useful because it is easier to get an electronic device to switch into one of a number of known states than to accurately reproduce a continuous range of values.
Digital electronic circuits are usually made from large assemblies of logic gates, simple electronic representations of Boolean logic functions.
Advantages.
An advantage of digital circuits when compared to analog circuits is that signals represented digitally can be transmitted without degradation due to noise. For example, a continuous audio signal transmitted as a sequence of 1s and 0s, can be reconstructed without error, provided the noise picked up in transmission is not enough to prevent identification of the 1s and 0s. An hour of music can be stored on a compact disc using about 6 billion binary digits.
In a digital system, a more precise representation of a signal can be obtained by using more binary digits to represent it. While this requires more digital circuits to process the signals, each digit is handled by the same kind of hardware, resulting in an easily scalable system. In an analog system, additional resolution requires fundamental improvements in the linearity and noise characteristics of each step of the signal chain.
Computer-controlled digital systems can be controlled by software, allowing new functions to be added without changing hardware. Often this can be done outside of the factory by updating the product's software. So, the product's design errors can be corrected after the product is in a customer's hands.
Information storage can be easier in digital systems than in analog ones. The noise-immunity of digital systems permits data to be stored and retrieved without degradation. In an analog system, noise from aging and wear degrade the information stored. In a digital system, as long as the total noise is below a certain level, the information can be recovered perfectly.
Disadvantages.
In some cases, digital circuits use more energy than analog circuits to accomplish the same tasks, thus producing more heat which increases the complexity of the circuits such as the inclusion of heat sinks. In portable or battery-powered systems this can limit use of digital systems.
For example, battery-powered cellular telephones often use a low-power analog front-end to amplify and tune in the radio signals from the base station. However, a base station has grid power and can use power-hungry, but very flexible software radios. Such base stations can be easily reprogrammed to process the signals used in new cellular standards.
Digital circuits are sometimes more expensive, especially in small quantities.
Most useful digital systems must translate from continuous analog signals to discrete digital signals. This causes quantization errors. Quantization error can be reduced if the system stores enough digital data to represent the signal to the desired degree of fidelity. The Nyquist-Shannon sampling theorem provides an important guideline as to how much digital data is needed to accurately portray a given analog signal.
In some systems, if a single piece of digital data is lost or misinterpreted, the meaning of large blocks of related data can completely change. Because of the cliff effect, it can be difficult for users to tell if a particular system is right on the edge of failure, or if it can tolerate much more noise before failing.
Digital fragility can be reduced by designing a digital system for robustness. For example, a parity bit or other error management method can be inserted into the signal path. These schemes help the system detect errors, and then either correct the errors, or at least ask for a new copy of the data. In a state-machine, the state transition logic can be designed to catch unused states and trigger a reset sequence or other error recovery routine.
Digital memory and transmission systems can use techniques such as error detection and correction to use additional data to correct any errors in transmission and storage.
On the other hand, some techniques used in digital systems make those systems more vulnerable to single-bit errors. These techniques are acceptable when the underlying bits are reliable enough that such errors are highly unlikely.
A single-bit error in audio data stored directly as linear pulse code modulation (such as on a CD-ROM) causes, at worst, a single click. Instead, many people use audio compression to save storage space and download time, even though a single-bit error may corrupt the entire song.
Design issues in digital circuits.
Digital circuits are made from analog components. The design must assure that the analog nature of the components doesn't dominate the desired digital behavior. Digital systems must manage noise and timing margins, parasitic inductances and capacitances, and filter power connections.
Bad designs have intermittent problems such as "glitches", vanishingly fast pulses that may trigger some logic but not others, "runt pulses" that do not reach valid "threshold" voltages, or unexpected ("undecoded") combinations of logic states.
Additionally, where clocked digital systems interface to analog systems or systems that are driven from a different clock, the digital system can be subject to metastability where a change to the input violates the set-up time for a digital input latch. This situation will self-resolve, but will take a random time, and while it persists can result in invalid signals being propagated within the digital system for a short time.
Since digital circuits are made from analog components, digital circuits calculate more slowly than low-precision analog circuits that use a similar amount of space and power. However, the digital circuit will calculate more repeatably, because of its high noise immunity. On the other hand, in the high-precision domain (for example, where 14 or more bits of precision are needed), analog circuits require much more power and area than digital equivalents.
Construction.
A digital circuit is often constructed from small electronic circuits called logic gates that can be used to create combinational logic. Each logic gate represents a function of boolean logic. A logic gate is an arrangement of electrically controlled switches, better known as transistors.
Each logic symbol is represented by a different shape. The actual set of shapes was introduced in 1984 under IEEE/ANSI standard 91-1984. "The logic symbol given under this standard are being increasingly used now and have even started appearing in the literature published by manufacturers of digital integrated circuits."
The output of a logic gate is an electrical flow or voltage, that can, in turn, control more logic gates.
Logic gates often use the fewest number of transistors in order to reduce their size, power consumption and cost, and increase their reliability.
Integrated circuits are the least expensive way to make logic gates in large volumes. Integrated circuits are usually designed by engineers using electronic design automation software (see below for more information).
Another form of digital circuit is constructed from lookup tables, (many sold as "programmable logic devices", though other kinds of PLDs exist). Lookup tables can perform the same functions as machines based on logic gates, but can be easily reprogrammed without changing the wiring. This means that a designer can often repair design errors without changing the arrangement of wires. Therefore, in small volume products, programmable logic devices are often the preferred solution. They are usually designed by engineers using electronic design automation software.
When the volumes are medium to large, and the logic can be slow, or involves complex algorithms or sequences, often a small microcontroller is programmed to make an embedded system. These are usually programmed by software engineers.
When only one digital circuit is needed, and its design is totally customized, as for a factory production line controller, the conventional solution is a programmable logic controller, or PLC. These are usually programmed by electricians, using ladder logic.
Structure of digital systems.
Engineers use many methods to minimize logic functions, in order to reduce the circuit's complexity. When the complexity is less, the circuit also has fewer errors and less electronics, and is therefore less expensive.
The most widely used simplification is a minimization algorithm like the Espresso heuristic logic minimizer within a CAD system, although historically, binary decision diagrams, an automated Quine–McCluskey algorithm, truth tables, Karnaugh maps, and Boolean algebra have been used.
Representation.
Representations are crucial to an engineer's design of digital circuits. Some analysis methods only work with particular representations.
The classical way to represent a digital circuit is with an equivalent set of logic gates. Another way, often with the least electronics, is to construct an equivalent system of electronic switches (usually transistors). One of the easiest ways is to simply have a memory containing a truth table. The inputs are fed into the address of the memory, and the data outputs of the memory become the outputs.
For automated analysis, these representations have digital file formats that can be processed by computer programs. Most digital engineers are very careful to select computer programs ("tools") with compatible file formats.
Combinational vs. Sequential.
To choose representations, engineers consider types of digital systems. Most digital systems divide into "combinational systems" and "sequential systems." A combinational system always presents the same output when given the same inputs. It is basically a representation of a set of logic functions, as already discussed.
A sequential system is a combinational system with some of the outputs fed back as inputs. This makes the digital machine perform a "sequence" of operations. The simplest sequential system is probably a flip flop, a mechanism that represents a binary digit or "bit".
Sequential systems are often designed as state machines. In this way, engineers can design a system's gross behavior, and even test it in a simulation, without considering all the details of the logic functions.
Sequential systems divide into two further subcategories. "Synchronous" sequential systems change state all at once, when a "clock" signal changes state. "Asynchronous" sequential systems propagate changes whenever inputs change. Synchronous sequential systems are made of well-characterized asynchronous circuits such as flip-flops, that change only when the clock changes, and which have carefully designed timing margins.
Synchronous Systems.
The usual way to implement a synchronous sequential state machine is to divide it into a piece of combinational logic and a set of flip flops called a "state register." Each time a clock signal ticks, the state register captures the feedback generated from the previous state of the combinational logic, and feeds it back as an unchanging input to the combinational part of the state machine. The fastest rate of the clock is set by the most time-consuming logic calculation in the combinational logic.
The state register is just a representation of a binary number. If the states in the state machine are numbered (easy to arrange), the logic function is some combinational logic that produces the number of the next state.
Asynchronous Systems.
As of 2014, almost all digital machines are synchronous designs because it is easier to create and verify a synchronous design. However, asynchronous logic is thought can be superior because its speed is not constrained by an arbitrary clock; instead, it runs at the maximum speed of its logic gates. Building an asynchronous system using faster parts makes the circuit faster.
Many systems need circuits that allow external unsynchronized signals to enter synchronous logic circuits. These are inherently asynchronous in their design and must be analyzed as such. Examples of widely used asynchronous circuits include synchronizer flip-flops, switch debouncers and arbiters.
Asynchronous logic components can be hard to design because all possible states, in all possible timings must be considered. The usual method is to construct a table of the minimum and maximum time that each such state can exist, and then adjust the circuit to minimize the number of such states. Then the designer must force the circuit to periodically wait for all of its parts to enter a compatible state (this is called "self-resynchronization"). Without such careful design, it is easy to accidentally produce asynchronous logic that is "unstable," that is, real electronics will have unpredictable results because of the cumulative delays caused by small variations in the values of the electronic components. 
Register Transfer Systems.
Many digital systems are data flow machines. These are usually designed using synchronous register transfer logic, using hardware description languages such as VHDL or Verilog.
In register transfer logic, binary numbers are stored in groups of flip flops called registers. The outputs of each register are a bundle of wires called a "bus" that carries that number to other calculations. A calculation is simply a piece of combinational logic. Each calculation also has an output bus, and these may be connected to the inputs of several registers. Sometimes a register will have a multiplexer on its input, so that it can store a number from any one of several buses. Alternatively, the outputs of several items may be connected to a bus through buffers that can turn off the output of all of the devices except one. A sequential state machine controls when each register accepts new data from its input.
Asynchronous register-transfer systems (such as computers) have a general solution. In the 1980s, some researchers discovered that almost all synchronous register-transfer machines could be converted to asynchronous designs by using first-in-first-out synchronization logic. In this scheme, the digital machine is characterized as a set of data flows. In each step of the flow, an asynchronous "synchronization circuit" determines when the outputs of that step are valid, and presents a signal that says, "grab the data" to the stages that use that stage's inputs. It turns out that just a few relatively simple synchronization circuits are needed.
Computer Design.
The most general-purpose register-transfer logic machine is a computer. This is basically an automatic binary abacus. The control unit of a computer is usually designed as a microprogram run by a microsequencer. A microprogram is much like a player-piano roll. Each table entry or "word" of the microprogram commands the state of every bit that controls the computer. The sequencer then counts, and the count addresses the memory or combinational logic machine that contains the microprogram. The bits from the microprogram control the arithmetic logic unit, memory and other parts of the computer, including the microsequencer itself.A "specialized computer" is usually a conventional computer with special-purpose control logic or microprogram.
In this way, the complex task of designing the controls of a computer is reduced to a simpler task of programming a collection of much simpler logic machines.
Almost all computers are synchronous. However, true asynchronous computers have also been designed. One example is the Aspida DLX core. Another was offered by ARM Holdings. Speed advantages have not materialized, because modern computer designs already run at the speed of their slowest componment, usually memory. These do use somewhat less power because a clock distribution network is not needed. An unexpected advantage is that asynchronous computers do not produce spectrally-pure radio noise, so they are used in some mobile-phone base-station controllers. They may be more secure in cryptographic applications because their electrical and radio emissions can be more difficult to decode.
Computer Architecture.
Computer architecture is a specialized engineering activity that tries to arrange the registers, calculation logic, buses and other parts of the computer in the best way for some purpose. Computer architects have applied large amounts of ingenuity to computer design to reduce the cost and increase the speed and immunity to programming errors of computers. An increasingly common goal is to reduce the power used in a battery-powered computer system, such as a cell-phone. Many computer architects serve an extended apprenticeship as microprogrammers.
Automated design tools.
To save costly engineering effort, much of the effort of designing large logic machines has been automated. The computer programs are called "electronic design automation tools" or just "EDA."
Simple truth table-style descriptions of logic are often optimized with EDA that automatically produces reduced systems of logic gates or smaller lookup tables that still produce the desired outputs. The most common example of this kind of software is the Espresso heuristic logic minimizer.
Most practical algorithms for optimizing large logic systems use algebraic manipulations or binary decision diagrams, and there are promising experiments with genetic algorithms and annealing optimizations.
To automate costly engineering processes, some EDA can take state tables that describe state machines and automatically produce a truth table or a function table for the combinational logic of a state machine. The state table is a piece of text that lists each state, together with the conditions controlling the transitions between them and the belonging output signals.
It is common for the function tables of such computer-generated state-machines to be optimized with logic-minimization software such as Minilog.
Often, real logic systems are designed as a series of sub-projects, which are combined using a "tool flow." The tool flow is usually a "script," a simplified computer language that can invoke the software design tools in the right order.
Tool flows for large logic systems such as microprocessors can be thousands of commands long, and combine the work of hundreds of engineers.
Writing and debugging tool flows is an established engineering specialty in companies that produce digital designs. The tool flow usually terminates in a detailed computer file or set of files that describe how to physically construct the logic. Often it consists of instructions to draw the transistors and wires on an integrated circuit or a printed circuit board.
Parts of tool flows are "debugged" by verifying the outputs of simulated logic against expected inputs. The test tools take computer files with sets of inputs and outputs, and highlight discrepancies between the simulated behavior and the expected behavior.
Once the input data is believed correct, the design itself must still be verified for correctness. Some tool flows verify designs by first producing a design, and then scanning the design to produce compatible input data for the tool flow. If the scanned data matches the input data, then the tool flow has probably not introduced errors.
The functional verification data are usually called "test vectors." The functional test vectors may be preserved and used in the factory to test that newly constructed logic works correctly. However, functional test patterns don't discover common fabrication faults. Production tests are often designed by software tools called "test pattern generators". These generate test vectors by examining the structure of the logic and systematically generating tests for particular faults. This way the fault coverage can closely approach 100%, provided the design is properly made testable (see next section).
Once a design exists, and is verified and testable, it often needs to be processed to be manufacturable as well. Modern integrated circuits have features smaller than the wavelength of the light used to expose the photoresist. Manufacturability software adds interference patterns to the exposure masks to eliminate open-circuits, and enhance the masks' contrast.
Design for testability.
There are several reasons for testing a logic circuit. When the circuit is first developed, it is necessary to verify that the design circuit meets the required functional and timing specifications. When multiple copies of a correctly designed circuit are being manufactured, it is essential to test each copy to ensure that the manufacturing process has not introduced any flaws.
A large logic machine (say, with more than a hundred logical variables) can have an astronomical number of possible states. Obviously, in the factory, testing every state is impractical if testing each state takes a microsecond, and there are more states than the number of microseconds since the universe began. Unfortunately, this ridiculous-sounding case is typical.
Fortunately, large logic machines are almost always designed as assemblies of smaller logic machines. To save time, the smaller sub-machines are isolated by permanently installed "design for test" circuitry, and are tested independently.
One common test scheme known as "scan design" moves test bits serially (one after another) from external test equipment through one or more serial shift registers known as "scan chains". Serial scans have only one or two wires to carry the data, and minimize the physical size and expense of the infrequently used test logic.
After all the test data bits are in place, the design is reconfigured to be in "normal mode" and one or more clock pulses are applied, to test for faults (e.g. stuck-at low or stuck-at high) and capture the test result into flip-flops and/or latches in the scan shift register(s). Finally, the result of the test is shifted out to the block boundary and compared against the predicted "good machine" result.
In a board-test environment, serial to parallel testing has been formalized with a standard called "JTAG" (named after the "Joint Test Action Group" that proposed it).
Another common testing scheme provides a test mode that forces some part of the logic machine to enter a "test cycle." The test cycle usually exercises large independent parts of the machine.
Trade-offs.
Several numbers determine the practicality of a system of digital logic: cost, reliability, fanout and speed. Engineers explored numerous electronic devices to get an ideal combination of these traits.
Cost.
The cost of a logic gate is crucial. In the 1930s, the earliest digital logic systems were constructed from telephone relays because these were inexpensive and relatively reliable. After that, engineers always used the cheapest available electronic switches that could still fulfill the requirements.
The earliest integrated circuits were a happy accident. They were constructed not to save money, but to save weight, and permit the Apollo Guidance Computer to control an inertial guidance system for a spacecraft. The first integrated circuit logic gates cost nearly $50 (in 1960 dollars, when an engineer earned $10,000/year). To everyone's surprise, by the time the circuits were mass-produced, they had become the least-expensive method of constructing digital logic. Improvements in this technology have driven all subsequent improvements in cost.
With the rise of integrated circuits, reducing the absolute number of chips used represented another way to save costs. The goal of a designer is not just to make the simplest circuit, but to keep the component count down. Sometimes this results in slightly more complicated designs with respect to the underlying digital logic but nevertheless reduces the number of components, board size, and even power consumption.
For example, in some logic families, NAND gates are the simplest digital gate to build. All other logical operations can be implemented by NAND gates. If a circuit already required a single NAND gate, and a single chip normally carried four NAND gates, then the remaining gates could be used to implement other logical operations like logical and. This could eliminate the need for a separate chip containing those different types of gates.
Reliability.
The "reliability" of a logic gate describes its mean time between failure (MTBF). Digital machines often have millions of logic gates. Also, most digital machines are "optimized" to reduce their cost. The result is that often, the failure of a single logic gate will cause a digital machine to stop working.
Digital machines first became useful when the MTBF for a switch got above a few hundred hours. Even so, many of these machines had complex, well-rehearsed repair procedures, and would be nonfunctional for hours because a tube burned-out, or a moth got stuck in a relay. Modern transistorized integrated circuit logic gates have MTBFs greater than 82 billion hours (8.2×1010) hours, and need them because they have so many logic gates.
Fanout.
Fanout describes how many logic inputs can be controlled by a single logic output without exceeding the current ratings of the gate. The minimum practical fanout is about five. Modern electronic logic using CMOS transistors for switches have fanouts near fifty, and can sometimes go much higher.
Speed.
The "switching speed" describes how many times per second an inverter (an electronic representation of a "logical not" function) can change from true to false and back. Faster logic can accomplish more operations in less time. Digital logic first became useful when switching speeds got above fifty hertz, because that was faster than a team of humans operating mechanical calculators. Modern electronic digital logic routinely switches at five gigahertz (5×109 hertz), and some laboratory systems switch at more than a terahertz (1×1012 hertz).
Logic families.
Design started with relays. Relay logic was relatively inexpensive and reliable, but slow. Occasionally a mechanical failure would occur. Fanouts were typically about ten, limited by the resistance of the coils and arcing on the contacts from high voltages.
Later, vacuum tubes were used. These were very fast, but generated heat, and were unreliable because the filaments would burn out. Fanouts were typically five to seven, limited by the heating from the tubes' current. In the 1950s, special "computer tubes" were developed with filaments that omitted volatile elements like silicon. These ran for hundreds of thousands of hours.
The first semiconductor logic family was resistor–transistor logic. This was a thousand times more reliable than tubes, ran cooler, and used less power, but had a very low fan-in of three. Diode–transistor logic improved the fanout up to about seven, and reduced the power. Some DTL designs used two power-supplies with alternating layers of NPN and PNP transistors to increase the fanout.
Transistor–transistor logic (TTL) was a great improvement over these. In early devices, fanout improved to ten, and later variations reliably achieved twenty. TTL was also fast, with some variations achieving switching times as low as twenty nanoseconds. TTL is still used in some designs.
Emitter coupled logic is very fast but uses a lot of power. It was extensively used for high-performance computers made up of many medium-scale components (such as the Illiac IV).
By far, the most common digital integrated circuits built today use CMOS logic, which is fast, offers high circuit density and low-power per gate. This is used even in large, fast computers, such as the IBM System z.
Recent developments.
In 2009, researchers discovered that memristors can implement a boolean state storage (similar to a flip flop, implication and logical inversion), providing a complete logic family with very small amounts of space and power, using familiar CMOS semiconductor processes.
The discovery of superconductivity has enabled the development of rapid single flux quantum (RSFQ) circuit technology, which uses Josephson junctions instead of transistors. Most recently, attempts are being made to construct purely optical computing systems capable of processing digital information using nonlinear optical elements.

</doc>
<doc id="39070" url="http://en.wikipedia.org/wiki?curid=39070" title="Maximilian I, Holy Roman Emperor">
Maximilian I, Holy Roman Emperor

Maximilian I (22 March 1459 – 12 January 1519), the son of Frederick III, Holy Roman Emperor, and Eleanor of Portugal, was King of the Romans (also known as King of the Germans) from 1486 and Holy Roman Emperor from 1508 until his death, though he was never in fact crowned by the Pope, the journey to Rome always being too risky. He had ruled jointly with his father for the last ten years of his father's reign, from c. 1483. He expanded the influence of the House of Habsburg through war and his marriage in 1477 to Mary of Burgundy, the heiress to the Duchy of Burgundy, but he also lost the Austrian territories in today's Switzerland to the Swiss Confederacy.
Through marriage of his son Philip the Handsome to eventual queen Joanna of Castile in 1498, Maximilian helped to establish the Habsburg dynasty in Spain which allowed his grandson Charles to hold the throne of both León-Castile and Aragon, thus making Charles V the first "de jure" King of Spain. Since his father Philip died in 1506, Charles succeeded Maximilian as Holy Roman Emperor in 1519, and thus ruled both the Holy Roman Empire and the Spanish Empire simultaneously.
Background and childhood.
Maximilian was born at Wiener Neustadt on 22 March 1459. His father, Frederick III, named him for an obscure saint whom Frederick believed had once warned him of imminent peril in a dream. In his infancy, he and his parents were besieged in Vienna by Albert of Austria. One source relates that, during the siege's bleakest days, the young prince would wander about the castle garrison, begging the servants and men-at-arms for bits of bread .
At the time, the Dukes of Burgundy, a cadet branch of the French royal family, with their sophisticated nobility and court culture, were the rulers of substantial territories on the eastern and northern boundaries of modern-day France. The reigning duke of Burgundy, Charles the Bold, was the chief political opponent of Maximilian's father Frederick III. Frederick was concerned about Burgundy's expansive tendencies on the western border of his Holy Roman Empire and, to forestall military conflict, he attempted to secure the marriage of Charles's only daughter, Mary of Burgundy, to his son Maximilian. After the Siege of Neuss (1474–75), he was successful. The wedding between Maximilian and Mary took place on the evening of 16 August 1477.
Reign in Burgundy and The Netherlands.
Maximilian's wife had inherited the large Burgundian domains in France and the Low Countries upon her father's death in the Battle of Nancy on 5 January 1477. Already before his coronation as the King of the Romans in 1486, Maximilian decided to secure this distant and extensive Burgundian inheritance to his family, the House of Habsburg, at all costs.
The Duchy of Burgundy was also claimed by the French crown under Salic Law, with Louis XI, King of France vigorously contesting the Habsburg claim to the Burgundian inheritance by means of military force. Maximilian undertook the defence of his wife's dominions from an attack by Louis XI and defeated the French forces at Guinegate, the modern Enguinegatte, on 7 August 1479.
The wedding contract between Maximilian and Mary stipulated that only the children of bride and groom had a right to inherit from each, not the surviving parent. Mary tried to bypass this rule with a promise to transfer territories as a gift in case of her death, but her plans were confounded. After Mary's death in a riding accident on 27 March 1482 near the Wijnendale Castle, Maximilian's aim was now to secure the inheritance to one of his and Mary's children, Philip the Handsome.
Some of the Netherlander provinces were hostile to Maximilian, and they signed a treaty with Louis XI in 1482 that forced Maximilian to give up Franche-Comté and Artois to the French crown and openly rebelled twice in the period 1482–1492, in an attempt to regain the autonomy they had enjoined under Mary. Flemish rebels managed to capture Philip and even Maximilian himself, but were defeated when Frederick III intervened. Maximilian continued to govern Mary's remaining inheritance in the name of Philip the Handsome. After the regency ended, Maximilian and Charles VIII of France exchanged these two territories for Burgundy and Picardy in the Treaty of Senlis (1493). Thus a large part of the Netherlands (known as the Seventeen Provinces) stayed in the Habsburg patrimony.
Reign in the Holy Roman Empire.
Elected King of the Romans 16 February 1486 in Frankfurt-am-Main at his father's initiative and crowned on 9 April 1486 in Aachen, Maximilian also stood at the head of the Holy Roman Empire upon his father's death in 1493. During his first year as an Emperor, much of Austria was under Hungarian rule as they had occupied the territory under the reign of Frederick. In 1490, Maximilian finally reconquered it and entered Vienna.
Italian and Swiss wars.
As the Treaty of Senlis had resolved French differences with the Holy Roman Empire, King Louis XII of France had his borders secured in the north and turned his attention to Italy, where he made claims for the Duchy of Milan. In 1499/1500 he conquered it and drove the Sforza regent Lodovico il Moro into exile. This brought him into a potential conflict with Maximilian, who on 16 March 1494 had married Bianca Maria Sforza, a daughter of Galeazzo Maria Sforza, duke of Milan. However, Maximilian was unable to hinder the French from taking over Milan. The prolonged Italian Wars resulted in Maximilian joining the Holy League to counter the French. In 1513, with Henry VIII of England, Maximilian won an important victory at the battle of the Spurs against the French, stopping their advance in northern France. His campaigns in Italy were not as successful and there, his progress was quickly checked.
In the late 15th century the two kingdoms of Tyrol and Bavaria went to war. Bavaria demanded money back from Tyrol that had been loaned on the collateral of Tyrolean lands. In 1490, the two nations demanded that Maximilian I step in to mediate the dispute. In response, he assumed the control of Tyrol and its debt. Because Tyrol had no law code at this time, the nobility freely expropriated money from the populace, which caused the royal palace in Innsbruck to fester with corruption. After taking control, Maximilian instituted immediate financial reform. In order to symbolize his new wealth and power, he built the Golden Roof, a canopy overlooking the town center of Innsbruck, from which to watch the festivities celebrating his assumption of rule over Tyrol. It is made entirely from golden shingles. Gaining theoretical control of Tyrol for the Habsburgs was of strategic importance because it linked the Swiss Confederacy to the Habsburg-controlled Austrian lands, which facilitated some imperial geographic continuity.
The situation in Italy was not the only problem Maximilian had at the time. The Swiss won a decisive victory against the Empire in the Battle of Dornach on 22 July 1499. Maximilian had no choice but to agree to a peace treaty signed on 22 September 1499 in Basel that granted the Swiss Confederacy independence from the Holy Roman Empire.
Reforms.
Within the Holy Roman Empire, Maximilian faced pressure from local rulers who believed that the King's continued wars with the French to increase the power of his own house were not in their best interests. There was also a consensus that in order to preserve the unity of the Empire, deep reforms were needed. The reforms, which had been delayed for a long time, were launched in the 1495 Reichstag at Worms. A new organ, the "Reichskammergericht" was introduced, and it was to be largely independent from the Emperor. To finance it, a new tax, the "Gemeine Pfennig" was launched. However, its collection was never fully successful. The local rulers wanted more independence from the Emperor and a strengthening of their own territorial rule. This led to Maximilian agreeing to establish an organ called the "Reichsregiment", which would meet in Nuremberg and consist of the deputies of the Emperor, local rulers, commoners, and the prince-electors of the Holy Roman Empire. The new organ proved itself politically weak and its power returned to Maximilian again in 1502.
Due to the difficult external and internal situation he faced, Maximilian also felt it necessary to introduce reforms in the historic territories of the House of Habsburg in order to finance his army. Using Burgundian institutions as a model, he attempted to create a unified state. This was not very successful, but one of the lasting results was the creation of three different subdivisions of the Austrian lands: Lower Austria, Upper Austria, and Vorderösterreich.
Maximilian was always troubled by financial shortcomings; his income never seemed to be enough to sustain his large-scale goals and policies. For this reason he was forced to take substantial credits from Upper German banker families, especially from the families of Baumgarten, Fugger and Welser. Jörg Baumgarten even served as Maximilian's financial advisor. The Fuggers, who dominated the copper and silver mining business in Tyrol, provided a credit of almost 1 million gulden for the purpose of bribing the prince-electors to choose Maximilian's grandson Charles V as the new Emperor. At the end of Maximilian's rule, the Habsburgs' mountain of debt totalled 6 million gulden; this corresponded to a decade's worth of tax revenues from their inherited lands. It took until the end of the 16th century for this debt to be repaid.
In 1508, Maximilian, with the assent of Pope Julius II, took the title "Erwählter Römischer Kaiser" ("Elected Roman Emperor"), thus ending the centuries-old custom that the Holy Roman Emperor had to be crowned by the pope.
"Tu felix Austria nube".
As part of the Treaty of Arras, Maximilian betrothed his three-year-old daughter Margaret to the Dauphin of France (later Charles VIII), son of his adversary Louis XI. Under the terms of Margaret's betrothal, she was sent to Louis to be brought up under his guardianship. Despite Louis's death in 1483, shortly after Margaret arrived in France, she remained at the French court. The Dauphin, now Charles VIII, was still a minor, and his regent until 1491 was his sister Anne.
Dying shortly after signing the Treaty of Le Verger, Francis II, Duke of Brittany, left his realm to his daughter Anne. In her search of alliances to protect her domain from neighboring interests, she betrothed Maximilian I in 1490. About a year later, they married by proxy.
However, Charles and his sister wanted her inheritance for France. So, when the former came of age in 1491, and taking advantage of Maximilian and his father's interest in the succession of their adversary Mathias Corvinus, King of Hungary, Charles repudiated his betrothal to Margaret, invaded Brittany, forced Anne of Brittany to repudiate her unconsummated marriage to Maximilian, and married Anne of Brittany himself.
Margaret then remained in France as a hostage of sorts until 1493, when she was finally returned to her father with the signing of the Treaty of Senlis.
In the same year, as the hostilities of the lengthy Italian Wars with France were in preparation, Maximilian contracted another marriage for himself, this time to Bianca Maria Sforza, daughter of Galeazzo Maria Sforza, Duke of Milan, with the intercession of his brother, Ludovico Sforza, then regent of the duchy after the former's death.
Years later, in order to reduce the growing pressures on the Empire brought about by treaties between the rulers of France, Poland, Hungary, Bohemia, and Russia, as well as to secure Bohemia and Hungary for the Habsburgs, Maximilian met with the Jagiellonian kings Ladislaus II of Hungary and Bohemia and Sigismund I of Poland at the First Congress of Vienna in 1515. There they arranged for Maximilian's granddaughter Mary to marry Louis, the son of Ladislaus, and for Anne (the sister of Louis) to marry Maximilian's grandson Ferdinand (both grandchildren being the children of Philip the Handsome, Maximilian's son, and Joanna of Castile). The marriages arranged there brought Habsburg kingship over Hungary and Bohemia in 1526. Both Anne and Louis were adopted by Maximilian following the death of Ladislaus.
Thus Maximilian through his own marriages and those of his descendants (attempted unsuccessfully and successfully alike) sought, as was current practice for dynastic states at the time, to extend his sphere of influence. The marriages he arranged for both of his children more successfully fulfilled the specific goal of thwarting French interests, and after the turn of the sixteenth century, his matchmaking focused on his grandchildren, for whom he looked away from France towards the east.
These political marriages were summed up in the following Latin elegiac couplet: "Bella gerant aliī, tū fēlix Austria nūbe/ Nam quae Mars aliīs, dat tibi regna Venus", "Let others wage war, but thou, O happy Austria, marry; for those kingdoms which Mars gives to others, Venus gives to thee."
Succession.
After it became clear that Maximilian's policies in Italy had been unsuccessful, and after 1517 Venice reconquered the last pieces of their territory from Maximilian, the emperor now started to focus entirely on the question of his succession. His goal was to secure the throne for a member of his house and prevent Francis I of France from gaining the throne; the resulting "election campaign" was unprecedented due to the massive use of bribery. The Fugger family provided Maximilian a credit of 1 million gulden, which was used to bribe the prince-electors. At first, this policy seemed successful, and Maximilian managed to secure the votes from Mainz, Cologne, Brandenburg and Bohemia for his grandson Charles V. The death of Maximilian in 1519 seemed to put the succession at risk, but in a few months the election of Charles V was secured.
Death and legacy.
In 1501, Maximilian fell from his horse, an accident that badly injured his leg and caused him pain for the rest of his life. Some historians have suggested that Maximilian was "morbidly" depressed: From 1514, he travelled everywhere with his coffin.
Maximilian died in Wels, Upper Austria, and was succeeded as Emperor by his grandson Charles V, his son Philip the Handsome having died in 1506. For penitential reasons, he gave very specific instructions for the treatment of his body after death. After death he wanted his hair to be cut off and his teeth knocked out. The body should be whipped and covered with lime and ash, wrapped in linen and "publicly displayed to show the perishableness of all earthly glory". Although he is buried in the Castle Chapel at Wiener Neustadt, a cenotaph tomb for Maximilian is located in the Hofkirche, Innsbruck.
Maximilian was a keen supporter of the arts and sciences, and he surrounded himself with scholars such as Joachim Vadian and Andreas Stoberl (Stiborius), promoting them to important court posts. His reign saw the first flourishing of the Renaissance in Germany. He commissioned a series of three monumental woodblock prints – "The Triumphal Arch" (1512–18, 192 woodcut panels, 295 cm wide and 357 cm high – approximately 9'8" by 11'8½"), and a "Triumphal Procession" (1516–18, 137 woodcut panels, 54 m long) which is led by a "Large Triumphal Carriage" (1522, 8 woodcut panels, 1½' high and 8' long), created by artists including Albrecht Dürer, Albrecht Altdorfer and Hans Burgkmair.
Maximilian had a great passion for armour, not only as equipment for battle or tournaments, but as an art form. The style of armour that became popular during the second half of his reign featured elaborate fluting and metalworking, and became known as Maximilian armour. It emphasized the details in the shaping of the metal itself, rather than the etched or gilded designs popular in the Milanese style. Maximilian also gave a bizarre jousting helmet as a gift to King Henry VIII – the helmet's visor featured a human face, with eyes, nose and a grinning mouth, and was modeled after the appearance of Maximilian himself. It also sported a pair of curled ram's horns, brass spectacles, and even etched beard stubble.
Maximilian had appointed his daughter Margaret as both Regent of the Netherlands and the guardian and educator of his grandsons Charles and Ferdinand (their father, Philip, having predeceased Maximilian), and she fulfilled this task well. Through wars and marriages he extended the Habsburg influence in every direction: to the Netherlands, Spain, Bohemia, Hungary, Poland, and Italy. This influence would last for centuries and shape much of European history.
Official style.
"Maximilian I, by the grace of God elected Holy Roman Emperor, forever August, King of Germany, of Hungary, Dalmatia, Croatia, etc. Archduke of Austria, Duke of Burgundy, Brabant, Lorraine, Styria, Carinthia, Carniola, Limburg, Luxembourg, Gelderland, Landgrave of Alsace, Prince of Swabia, Count Palatine of Burgundy, Princely Count of Habsburg, Hainaut, Flanders, Tyrol, Gorizia, Artois, Holland, Seeland, Ferrette, Kyburg, Namur, Zutphen, Margrave of the Holy Roman Empire, the Enns, Burgau, Lord of Frisia, the Wendish March, Pordenone, Salins, Mechelen, etc. etc."
Chivalric order.
Maximilian I was a member of the Order of the Garter, nominated by King Henry VII of England in 1489. His Garter stall plate survives in St George's Chapel, Windsor Castle.
Marriages and offspring.
Maximilian was married three times, of which only the first marriage produced offspring:

</doc>
<doc id="39071" url="http://en.wikipedia.org/wiki?curid=39071" title="Samuel de Champlain">
Samuel de Champlain

Samuel de Champlain (] born Samuel Champlain; on or before August 13, 1574 – December 25, 1635), "The Father of New France", was a French navigator, cartographer, draughtsman, soldier, explorer, geographer, ethnologist, diplomat, and chronicler. He founded New France and Quebec City on July 3, 1608. He is important to Canadian history because he made the first accurate map of the coast and he helped establish the settlements.
Born into a family of mariners, Champlain, while still a young man, began exploring North America in 1603 under the guidance of François Gravé Du Pont, From 1604 to 1607 Champlain participated in the exploration and settlement of the first permanent European settlement north of Florida, Port Royal, Acadia (1605). Then, in 1608, he established the French settlement that is now Quebec City. Champlain was the first European to explore and describe the Great Lakes, and published maps of his journeys and accounts of what he learned from the natives and the French living among the Natives. He formed relationships with local Montagnais and Innu and later with others farther west (Ottawa River, Lake Nipissing, or Georgian Bay), with Algonquin and with Huron Wendat, and agreed to provide assistance in their wars against the Iroquois.
In 1620, Louis XIII ordered Champlain to cease exploration, return to Quebec, and devote himself to the administration of the country. In every way but formal title, Samuel "de" Champlain served as Governor of New France, a title that may have been formally unavailable to him owing to his non-noble status. He established trading companies that sent goods, primarily fur, to France, and oversaw the growth of New France in the St. Lawrence River valley until his death in 1635.
Champlain is memorialized as the "Father of New France" and "Father of Acadia", and many places, streets, and structures in northeastern North America bear his name, or have monuments established in his memory. The most notable of these is Lake Champlain, which straddles the border between northern New York and Vermont, extending slightly across the border into Canada. In 1609 he led an expedition up the Richelieu River and explored a long, narrow lake situated between the Green Mountains of present-day Vermont and the Adirondack Mountains of present-day New York; he named the lake after himself as the first European to map and describe it.
Birth year, location, and family.
Champlain was born to Antoine Champlain (also written "Anthoine Chappelain" in some records) and Marguerite Le Roy, in either Hiers-Brouage, or the port city of La Rochelle, in the French province of Aunis. He was born on or before August 13, 1574 according to a recent baptism record found by Jean-Marie Germe, French genealogist. Although in 1870, the Canadian Catholic priest Laverdière, in the first chapter of his "Œuvres de Champlain", accepted Pierre-Damien Rainguet's estimate and tried to justify it, his calculations were based on assumptions now believed, or proven, to be incorrect. Although Léopold Delayant (member, secretary, then president of "l'Académie des belles-lettres, sciences et arts de La Rochelle") wrote as early as 1867 that Rainguet's estimate was wrong, the books of Rainguet and Laverdière have had a significant influence. The 1567 date was carved on numerous monuments dedicated to Champlain and is widely regarded as accurate. In the first half of the 20th century, some authors disagreed, choosing 1570 or 1575 instead of 1567. In 1978 Jean Liebel published groundbreaking research about these estimates of Champlain's birth year and concluded, "Samuel Champlain was born about 1580 in Brouage." Liebel asserts that some authors, including the Catholic priests Rainguet and Laverdière, preferred years when Brouage was under Catholic control (which include 1567, 1570, and 1575).
Champlain claimed to be from Brouage in the title of his 1603 book, and to be "Saintongeois" in the title of his second book (1613). He belonged to either a Protestant family, or a tolerant Roman Catholic one, since Brouage was most of the time a Catholic city in a Protestant region, and his Old Testament first name (Samuel) was not usually given to Catholic children. The exact location of his birth is thus also not known with certainty, but at the time of his birth his parents were living in Brouage.
Born into a family of mariners (both his father and uncle-in-law were sailors, or navigators), Samuel Champlain learned to navigate, draw, make nautical charts, and write practical reports. His education did not include Ancient Greek or Latin, so he did not read or learn from any ancient literature. As each French fleet had to assure its own defense at sea, Champlain sought to learn fighting with the firearms of his time: he acquired this practical knowledge when serving with the army of King Henry IV during the later stages of France's religious wars in Brittany from 1594 or 1595 to 1598, beginning as a quartermaster responsible for the feeding and care of horses. During this time he claimed to go on a "certain secret voyage" for the king, and saw combat (including maybe the Siege of Fort Crozon, at the end of 1594). By 1597 he was a "capitaine d'une compagnie" serving in a garrison near Quimper.
Early travels.
In 1598, his uncle-in-law, a navigator whose ship "Saint-Julien" was chartered to transport Spanish troops to Cadiz pursuant to the Treaty of Vervins, gave Champlain the opportunity to accompany him. After a difficult passage, he spent some time in Cadiz before his uncle, whose ship was then chartered to accompany a large Spanish fleet to the West Indies, again offered him a place on the ship. His uncle, who gave command of the ship to Jeronimo de Vallebrera, instructed the young Champlain to watch over the ship. This journey lasted two years, and gave Champlain the opportunity to see or hear about Spanish holdings from the Caribbean to Mexico City. Along the way he took detailed notes, and wrote an illustrated report on what he learned on this trip, and gave this secret report to King Henry, who rewarded Champlain with an annual pension. This report was published for the first time in 1870, by Laverdière, as "Brief Discours des Choses plus remarquables que Sammuel Champlain de Brouage a reconneues aux Indes Occidentalles au voiage qu'il en a faict en icettes en l'année 1599 et en l'année 1601, comme ensuite" (and in English as "Narrative of a Voyage to the West Indies and Mexico 1599–1602"). The authenticity of this account as a work written by Champlain has frequently been questioned, due to inaccuracies and discrepancies with other sources on a number of points; however, recent scholarship indicates that the work probably was authored by Champlain.
On Champlain's return to Cadiz in August 1600, his uncle, who had fallen ill, asked him to look after his business affairs. This Champlain did, and when his uncle died in June 1601, Champlain inherited his substantial estate. It included an estate near La Rochelle, commercial properties in Spain, and a 150-ton merchant ship. This inheritance, combined with the king's annual pension, gave the young explorer a great deal of independence, as he was not dependent on the financial backing of merchants and other investors. From 1601 to 1603 Champlain served as a geographer in the court of King Henry. As part of his duties he traveled to French ports and learned much about North America from the fishermen that seasonally traveled to coastal areas from Nantucket to Newfoundland to capitalize on the rich fishing grounds there. He also made a study of previous French failures at colonization in the area, including that of Pierre de Chauvin at Tadoussac. When Chauvin forfeited his monopoly on fur trade in North America in 1602, responsibility for renewing the trade was given to Aymar de Chaste. Champlain approached de Chaste about a position on the first voyage, which he received with the king's assent.
Champlain's first trip to North America was as an observer on a fur-trading expedition led by François Gravé Du Pont. Du Pont was a navigator and merchant who had been a ship's captain on Chauvin's expedition, and with whom Champlain established a firm lifelong friendship. He educated Champlain about navigation in North America, including the Saint Lawrence River, and in dealing with the natives there (and in Acadia after). The "Bonne-Renommée" (the "Good Fame") arrived at Tadoussac on March 15, 1603. Champlain was anxious to see for himself all of the places that Jacques Cartier had seen and described about sixty years earlier, and wanted to go even further than Cartier, if possible. Champlain created a map of the Saint Lawrence on this trip and, after his return to France on September 20, published an account as "Des Sauvages: ou voyage de Samuel Champlain, de Brouages, faite en la France nouvelle l'an 1603" ("Concerning the Savages: or travels of Samuel Champlain of Brouages, made in New France in the year 1603"). Included in his account were meetings with Begourat, a chief of the Montagnais at Tadoussac, in which positive relationships were established between the French and the many Montagnais gathered there, with some Algonquin friends.
Promising to King Henry to report on further discoveries, Champlain joined a second expedition to New France in the spring of 1604. This trip, once again an exploratory journey without women and children, lasted several years, and focused on areas south of the St. Lawrence River, in what later became known as Acadia. It was led by Pierre Dugua de Mons, a noble and Protestant merchant who had been given a fur trading monopoly in New France by the king. Dugua asked Champlain to find a site for winter settlement. After exploring possible sites in the Bay of Fundy, Champlain selected Saint Croix Island in the St. Croix River as the site of the expedition's first winter settlement. After enduring a harsh winter on the island the settlement was relocated across the bay where they established Port Royal. Until 1607, Champlain used that site as his base, while he explored the Atlantic coast. Dugua was forced to leave the settlement for France in September 1605, because he learned that his monopoly was at risk. His monopoly was rescinded by the king in July 1607 under pressure from other merchants and proponents of free trade, leading to the abandonment of the settlement.
In 1605 and 1606, Champlain explored the North American coast as far south as Cape Cod, searching for sites for a permanent settlement. Minor skirmishes with the resident Nausets dissuaded him from the idea of establishing one near present-day Chatham, Massachusetts. He named the area Mallebar ("bad bar").
Founding of Quebec City.
In the spring of 1608, Dugua wanted Champlain to start a new French colony on the shores of the St. Lawrence. Dugua equipped, at his own expense, a fleet of three ships with workers, that left the French port of Honfleur. The main ship, called the "Don-de-Dieu" (the "Gift of God"), was commanded by Champlain. Another ship, the "Lévrier" (the "Hunt Dog"), was commanded by his friend Du Pont. The small group of male settlers arrived at Tadoussac on the lower St. Lawrence in June. Because of the dangerous strength of the Saguenay River ending there, they left the ships and continued up the "Big River" in small boats bringing the men and the materials.
On July 3, 1608, Champlain landed at the "point of Quebec" and set about fortifying the area by the erection of three main wooden buildings, each two stories tall, that he collectively called the "Habitation", with a wooden stockade and a moat 12 ft wide surrounding them. This was the very beginning of Quebec City. Gardening, exploring, and fortifying this place became great passions of Champlain for the rest of his life.
In the 1620s, the "Habitation" at Quebec was mainly a store for the "Compagnie des Marchands" (Traders Company), and Champlain lived in the wooden "Fort Saint Louis" newly built up the hill (south from the present-day "Château Frontenac" Hotel), near the only two houses built by the two settler families (the ones of Louis Hébert and Guillaume Couillard, his son-in-law).
Murder of the King.
In May 1610, King Henry was assassinated by a Catholic fanatic, and rule fell to his wife, Marie de' Medici, as regent for the nine-year-old Louis XIII. Marie was a staunch Catholic with little interest in New France, and many of Champlain's Protestant financial supporters, including Pierre Dugua de Mons, were denied access to court. Champlain, on hearing the news, returned to France in September 1610 to establish new political connections in support of efforts at colonization.
Marriage.
One route Champlain may have chosen to improve his access to the court of the regent was his decision to enter into marriage with the twelve-year-old Hélène Boullé. She was the daughter of Nicolas Boullé, a man charged with carrying out royal decisions at court. The marriage contract was signed on December 27, 1610 in presence of Dugua, who had dealt with the father (a Protestant like him), and the couple was married three days later. The terms of the contract called for the marriage to be consummated two years later.
Champlain's marriage was initially quite troubled, as Hélène rebelled when she was told to join him in August 1613. Their relationship, while it apparently lacked any physical connection, recovered and was apparently good for many years. Hélène lived in Quebec for several years, but returned to Paris and eventually decided to enter a convent. The couple had no children, although Champlain did adopt three Montagnais girls named Faith, Hope, and Charity in the winter of 1627-28.
Relations and war with natives.
During the summer of 1609, Champlain attempted to form better relations with the local native tribes. He made alliances with the Wendat (called "Huron" by the French) and with the Algonquin, the Montagnais and the Etchemin, who lived in the area of the St. Lawrence River. These tribes demanded that Champlain help them in their war against the Iroquois, who lived farther south. Champlain set off with nine French soldiers and 300 natives to explore the "Rivière des Iroquois" (now known as the Richelieu River), and became the first European to map Lake Champlain. Having had no encounters with the Iroquois at this point many of the men headed back, leaving Champlain with only 2 Frenchmen and 60 natives.
On July 29, somewhere in the area near Ticonderoga and Crown Point, New York (historians are not sure which of these two places, but Fort Ticonderoga historians claim that it occurred near its site), Champlain and his party encountered a group of Iroquois. In a battle begun the next day, two hundred Iroquois advanced on Champlain's position, and one of his guides pointed out the three Iroquois chiefs. In his account of the battle, Champlain recounts firing his arquebus and killing two of them with a single shot, after which one of his men killed the third. The Iroquois turned and fled. This action set the tone for poor French-Iroquois relations for the rest of the century.
The Battle of Sorel occurred on June 19, 1610, with Samuel de Champlain supported by the Kingdom of France and his allies, the Wyandot people, Algonquin people and Innu people against the Mohawk people in New France at present day Sorel-Tracy, Quebec. The forces of Champlain armed with the arquebus engaged and killed or captured nearly all of the Mohawks. The battle ended major hostilities with the Mohawks for twenty-years.
Exploration of New France.
On March 29, 1613, arriving back in New France, he first ensured that his new royal commission be proclaimed. Champlain set out on May 27 to continue his exploration of the Huron country and in hopes of finding the "northern sea" he had heard about (probably Hudson Bay). He traveled the Ottawa River, later giving the first description of this area. It was in June that he met with Tessouat, the Algonquin chief of Allumettes Island, and offered to build the tribe a fort if they were to move from the area they occupied, with its poor soil, to the locality of the Lachine Rapids.
By August 26 Champlain was back in Saint-Malo. There he wrote an account of his life from 1604 to 1612 and his journey up the Ottawa river, his "Voyages" and published another map of New France. In 1614 he formed the "Compagnie des Marchands de Rouen et de Saint-Malo" and "Compagnie de Champlain", which bound the Rouen and Saint-Malo merchants for eleven years. He returned to New France in the spring of 1615 with four Recollects in order to further religious life in the new colony. The Roman Catholic Church was eventually given "en seigneurie" large and valuable tracts of land estimated at nearly 30% of all the lands granted by the French Crown in New France.
Champlain continued to work to improve relations with the natives promising to help them in their struggles against the Iroquois. With his native guides he explored further up the Ottawa River and reached Lake Nipissing. He then followed the French River until he reached the fresh-water sea he called Lac Attigouautau (now Lake Huron).
In 1615, Champlain was escorted through the area that is now Peterborough, Ontario by a group of Hurons. He used the ancient portage between Chemong Lake and Little Lake (now Chemong Road), and stayed for a short period of time near what is now Bridgenorth.
Military expedition.
On September 1, at Cahiagué (A Huron community on what is now called Lake Simcoe), he and the northern tribes started a military expedition against the Iroquois. The party passed Lake Ontario at its eastern tip where they hid their canoes and continued their journey by land. They followed the Oneida River until they arrived at the main Onondaga fort on October 10, 1615. The exact location of this place is still a matter of debate. Although the traditional location, Nichols Pond, is regularly disproved by professional and amateur archeologists many still claim that Nichols Pond is the location of the battle. 10 mi south of Canastota, New York. Champlain attacked the stockaded Oneida Indian village. He was accompanied by 10 Frenchmen and 300 Huron Indians. Pressured by the Hurons to attack prematurely, the assault failed. Champlain was wounded twice in the leg by arrows, one in his knee. The conflict ended on October 16 when the French and Huron were forced to flee.
Although he did not want to, the Hurons insisted that Champlain spend the winter with them. During his stay he set off with them in their great deer hunt, during which he became lost and was forced to wander for three days living off game and sleeping under trees until he met up with a band of aboriginals by chance. He spent the rest of the winter learning "their country, their manners, customs, modes of life". On May 22, 1616, he left the Huron country and returned to Quebec before heading back to France on July 2.
Improving administration in New France.
Champlain returned to New France in 1620 and was to spend the rest of his life focusing on administration of the territory rather than exploration. Champlain spent the winter building Fort Saint-Louis on top of Cape Diamond. By mid-May he learned that the fur trading monopoly had been handed over to another company led by the Caen brothers. After some tense negotiations, it was decided to merge the two companies under the direction of the Caens. Champlain continued to work on relations with the natives and managed to impose on them a chief of his choice. He also negotiated a peace treaty with the Iroquois.
Champlain continued to work on the fortifications of what became Quebec City, laying the first stone on May 6, 1624. On August 15 he once again returned to France where he was encouraged to continue his work as well as to continue looking for a passage to China, something widely believed to exist at the time. By July 5 he was back at Quebec and continued expanding the city.
In 1627 the Caen brothers' company lost its monopoly on the fur trade, and Cardinal Richelieu (who had joined the Royal Council in 1624 and rose rapidly to a position of dominance in French politics that he would hold until his death in 1642) formed the Compagnie des Cent-Associés (the Hundred Associates) to manage the fur trade. Champlain was one of the 100 investors, and its first fleet, loaded with colonists and supplies, set sail in April 1628.
Champlain had overwintered in Quebec. Supplies were low, and English merchants pillaged Cap Tourmente in early July 1628. A war had broken out between France and England, and Charles I of England had issued letters of marque that authorized the capture of French shipping and its colonies in North America. Champlain received a summons to surrender on July 10 from some heavily armed, English based Scottish merchants, the Kirke brothers. Champlain refused to deal with them, misleading them to believe that Quebec's defenses were better than they actually were (Champlain had only 50 pounds of gunpowder to defend the community). Successfully bluffed, they withdrew, but encountered and captured the French supply fleet, cutting off that year's supplies to the colony. By the spring of 1629 supplies were dangerously low and Champlain was forced to send people to Gaspé and into Indian communities to conserve rations. On July 19, the Kirke brothers arrived before Quebec after intercepting Champlain's plea for help, and Champlain was forced to surrender the colony. Many colonists were taken first to England and then France by the Kirkes, but Champlain remained in London to begin the process of regaining the colony. A peace treaty had been signed in April 1629, three months before the surrender, and, under the terms of that treaty, Quebec and other prizes taken by the Kirkes after the treaty were supposed to be returned. It was not until the 1632 Treaty of Saint-Germain-en-Laye that Quebec was formally given back to France. (David Kirke was rewarded when Charles I knighted him and gave him a charter for Newfoundland.) Champlain reclaimed his role as commander of New France on behalf of Richelieu on March 1, 1633, having served in the intervening years as commander in New France "in the absence of my Lord the Cardinal de Richelieu" from 1629 to 1635. In 1632 Champlain published "Voyages de la Nouvelle France", which was dedicated to Cardinal Richelieu, and "Traitté de la marine et du devoir d'un bon marinier", a treatise on leadership, seamanship, and navigation. (Champlain made more than twenty-five round-trip crossings of the Atlantic in his lifetime, without losing a single ship.)
Last return, and last years working in Quebec.
Champlain returned to Quebec on May 22, 1633, after an absence of four years. Richelieu gave him a commission as Lieutenant General of New France, along with other titles and responsibilities, but not that of Governor. Despite this lack of formal status, many colonists, French merchants, and Indians treated him as if he had the title; writings survive in which he is referred to as "our governor". On August 18, 1634, he sent a report to Richelieu stating that he had rebuilt on the ruins of Quebec, enlarged its fortifications, and established two more habitations. One was 15 leagues upstream, and the other was at Trois-Rivières. He also began an offensive against the Iroquois, reporting that he wanted them either wiped out or "brought to reason".
Illness, last wills, death, and burial.
Champlain suffered a severe stroke in October 1635, and died on 25 December 1635, leaving no immediate heirs. Jesuit records tell us he died in the care of his friend and confessor Charles Lallemant.
Although his will (drafted in November 17, 1635) gave much of his French property to his wife Hélène, he made significant bequests to the Catholic missions and to individuals in the colony of Quebec. However, Marie Camaret, a cousin on his mother's side, challenged the will in Paris and had it successfully overturned. It is unclear exactly what happened to his estate.
He was temporarily buried in the church while a standalone chapel was built to hold his remains in the upper part of the city. Unfortunately, this small building, along many others, was destroyed by a large fire in 1640. Though immediately rebuilt, no traces of it exist anymore: his exact burial site is still unknown, despite much research since about 1850, including several archaeological digs in the city. There is general agreement that the previous Champlain chapel site, and the remains of Champlain, should be somewhere near the Notre-Dame de Québec Cathedral.
The search for Champlain's remains supplies a key plot-line in the crime writer Louise Penny's 2010 novel, "Bury Your Dead".
Memorials.
Many sites and landmarks have been named to honour Champlain, who remains, to this day, a prominent historical figure in many parts of Acadia, Ontario, Quebec, New York, and Vermont. They include:
Bibliography.
These are works that are known to have been written by Champlain:
Further reading.
</dl>

</doc>
<doc id="39073" url="http://en.wikipedia.org/wiki?curid=39073" title="Sifaka">
Sifaka

Sifakas (singular "sifaka"; ; ]) are a genus (Propithecus) of lemur from the family Indriidae within the order Primates. Their name of the family is an onomatopoeia of their characteristic "shi-fak" alarm call. Like all lemurs, they are found only on the island of Madagascar. All species of sifakas are threatened, ranging from vulnerable to critically endangered.
Sifakas are medium-sized indrids with a head and body length of 40 to and a weight of 3 to. Their tail is just as long as their body, which differentiates them from the Indri. Their fur is long and silky, with coloration varying by species from yellowish-white to black brown. The round, hairless face is always black. As with all lemurs, the sifaka has special adaptations for grooming, including a toilet-claw on its second toe and a toothcomb.
Sifakas move by vertical clinging and leaping, meaning they maintain an upright position leaping from tree trunk to tree trunk and moving along branches. They are skillful climbers and powerful jumpers, able to make leaps of up to 10 m from one tree to the next. On the ground they move like all indrids with bipedal sideways hopping movements of the hind legs, holding their forelimbs up for balance. Sifakas are diurnal and arboreal.
Sifakas are herbivores, eating leaves, flowers and fruits. When not searching for food they spend a good part of the day sun bathing, stretched on the branches. Sifakas live in larger groups than the other indrids (up to 13 animals). They have a firm territory, which they mark with scent glands. Edges of different sifaka territories can overlap. Even though they defend their territory from invasion by others of their species, they may peacefully co-exist with other lemur species such as red-bellied lemur and the common brown lemur. Successful invasions are known to result in death of male members, group takeover and infanticide.
A four to five-month gestation period ends with the birth of a single offspring in July. The young holds fast to the mother's belly when small, but then later is carried on her back. Young are weaned after about six months and reach full maturity at the age of two to three years. The life expectancy of the sifakas is up to 18 years.

</doc>
<doc id="39075" url="http://en.wikipedia.org/wiki?curid=39075" title="Spiro Agnew">
Spiro Agnew

Spiro Theodore Agnew (; November 9, 1918 – September 17, 1996) was a Greek-American politician who served as the 39th Vice President of the United States from 1969 to 1973, under President Richard Nixon.
Agnew was born in Baltimore, Maryland, and was a graduate of Johns Hopkins University and University of Baltimore School of Law. He was drafted into the United States Army in 1941, serving as an officer during World War II, and was recalled for service during the Korean War in 1950. Agnew worked as an aide for U.S. Representative James Devereux before he was appointed to the Baltimore County Board of Zoning Appeals in 1957. In 1960, he lost an election for the Baltimore City Circuit Court, but in 1962 was elected Baltimore County Executive. In 1966, Agnew was elected the 55th Governor of Maryland, defeating his Democratic opponent George P. Mahoney. He was the first Greek American to hold the position, serving between 1967 and 1969.
At the 1968 Republican National Convention, Agnew, who had earlier been asked to place Richard Nixon's name in nomination, was selected in private by Nixon and his campaign staff. He was then presented to the convention delegates for nomination for Vice President and ran alongside Nixon in the Presidential Election of 1968. Nixon and Agnew defeated the incumbent Vice President, Hubert Humphrey (formerly long-time Senator from Minnesota) and Senator Edmund Muskie of Maine. In 1972, Nixon and Agnew were reelected for a second term, defeating Senator George McGovern and Ambassador Sargent Shriver.
In 1973, Agnew was investigated by the United States Attorney for the District of Maryland on charges of extortion, tax fraud, bribery, and conspiracy. He was charged with having accepted bribes totaling more than $100,000 while holding office as Baltimore County Executive, Governor of Maryland, and Vice President. On October 10 that same year, Agnew was allowed to plead no contest to a single charge that he had failed to report $29,500 of income received in 1967, with the condition that he resign the office of Vice President. Nixon later replaced Agnew by appointing House Minority Leader Gerald Ford as Vice President. When Nixon resigned from the White House due to the Watergate scandal, Ford assumed to the presidency the following year.
Agnew was the second Vice President in United States history to resign, the other being John C. Calhoun, and the only one to do so because of criminal charges. Nearly ten years after leaving office, Agnew paid the state of Maryland nearly $270,000 as a result of a civil suit that stemmed from the bribery allegations. Critics have cited Agnew as being one of the worst Vice Presidents in American history.
Early life.
Spiro Agnew was born in Baltimore, Maryland. His parents were Theodore Spiros Agnew, a Greek immigrant who shortened his name from Anagnostopoulos (Αναγνωστόπουλος) (originally from Gargalianoi, Messenia) when he moved to the United States, and Margaret Marian (Akers) Pollard Agnew, a native of Virginia. Spiro had a half brother, Roy Pollard, from his mother's first marriage (she was widowed at the time she met Spiro's father). Agnew was raised in his father's Greek Orthodox Church. His Greek family has direct lineage from the island of Chios.
Agnew attended Forest Park Senior High School in Baltimore, before enrolling at Johns Hopkins University in 1937. He studied chemistry at Hopkins for three years.
Agnew was drafted into the United States Army in 1941 and was commissioned on May 25, 1942, upon graduation from Army Officer Candidate School. He served with the 10th Armored Division in Europe during World War II. He was awarded the Bronze Star for his service in France and Germany.
Before leaving for Europe, Agnew worked at the Maryland Casualty Company where he met Elinor Judefind, known as Judy. Agnew married her on May 27, 1942. They had four children: Pamela, James Rand, Susan and Kimberly.
Upon his return from the war, Agnew transferred to the evening program at the University of Baltimore School of Law. He studied law at night while working as a grocer and as an insurance salesman during the day. In 1947, Agnew received his LL.B. (later amended to Juris Doctor) and moved to the suburbs to begin practicing law. He passed the Maryland bar exam in June 1949.
Agnew was recalled to service with the Army in 1950 during the Korean War.
Early political career.
Spiro Agnew began his political career as the first president of the Loch Raven Community Council and the President of the Dumbarton Junior High School PTA. A Democrat from early youth, he switched parties and became a Republican. During the 1950s, he aided U.S. Congressman James Devereux in winning four successive elections. In 1957, he was appointed to the Baltimore County Board of Zoning Appeals by Democratic Baltimore County Executive Michael J. Birmingham. In 1960, he made his first run for office as a candidate for Judge of the Circuit Court, finishing last in a five-person contest. The following year, the new Democratic Baltimore County Executive, Christian H. Kahl, dropped him from the Zoning Board.
Agnew ran for election as Baltimore County Executive in 1962, seeking office in a predominantly Democratic county that had seen no Republican elected to that position in the 20th century, with only one (Roger B. Hayden) earning victory after he left. Running as a reformer and Republican outsider, he took advantage of a bitter split in the Democratic Party and was elected. Agnew backed and signed an ordinance outlawing discrimination in some public accommodations, among the first laws of this kind in the United States.
Governor of Maryland.
Agnew ran for the position of Governor of Maryland in 1966. In this overwhelmingly Democratic state, he was elected after the Democratic nominee, George P. Mahoney, a Baltimore paving contractor and perennial candidate running on an anti-integration platform, narrowly won the Democratic gubernatorial primary out of a crowded slate of eight candidates, trumping early favorite Carlton R. Sickles. Coming on the heels of the recently passed federal Fair Housing Act of 1965, Mahoney's campaign embraced the slogan "your home is your castle, protect it." Many Democrats opposed to segregation then crossed party lines to give Agnew the governorship by 82,000 votes.
As governor, Agnew worked with the Democratic legislature to pass tax and judicial reforms, as well as tough anti-pollution laws. Projecting an image of racial moderation, Agnew signed the state's first open-housing laws and succeeded in effecting the repeal of an anti-miscegenation law. However, during the riots that followed the assassination of Martin Luther King Jr., in the spring of 1968, Agnew angered many African-American leaders when he stated in reference to their constituents, "I call on you to publicly repudiate all black racists. This, so far, you have been unwilling to do."
Vice Presidency (1969–1973).
Agnew's moderate image, immigrant background, and success in a traditionally Democratic state made him an attractive running mate for the 1968 Republican presidential nominee, former Vice President Richard Nixon. In line with what would later be called Nixon's "Southern Strategy," Agnew was selected as a candidate because he was sufficiently from the South to attract Southern moderate voters, yet was not identified with the Deep South, which might have alienated Northern centrists.
As late as early 1968, Agnew was a strong supporter of Nelson Rockefeller, one of Nixon's opponents, but by June had switched to supporting Nixon. At the 1968 Republican National Convention, Agnew's nomination was supported by many conservatives within the Republican Party, and by Nixon himself. However, a small band of delegates started shouting "Spiro Who?" and tried to nominate George W. Romney. In the end, Nixon's wishes prevailed, with Agnew receiving 1119 out of the 1317 votes cast.
During the ensuing general election campaign against Vice President Hubert Humphrey, which took place against a backdrop of urban riots and anti-Vietnam War demonstrations, culminating in the violent confrontations at the Democratic convention in Chicago, Agnew repeatedly hammered the Democrats on the issue of "law and order." Agnew was considered something of a political joke at first. One Democratic television commercial featured the sounds of a man's hearty laughter as the camera panned to a TV with the words "Agnew for Vice President?" on the screen.
Agnew went from his first election as County Executive to Vice President in six years—one of the fastest rises in US political history, comparable with that of Nixon, who became Vice President after four years in the House of Representatives and two years in the Senate. Agnew's vice presidency was also the highest-ranking US political office ever reached by either a Greek American or a Marylander.
Agnew soon found his role as the voice of the so-called "silent majority," and by late 1969 he was ranking high on national "Most Admired Men" polls. He also inspired a fashion craze when one entrepreneur introduced Spiro Agnew watches (a takeoff on the popular Mickey Mouse watch); conservatives wore them to show their support for Agnew, while many liberals wore them to signify their contempt.
Agnew was known for his scathing criticisms of political opponents, especially journalists and anti-war activists. Agnew would attack his adversaries with relish, hurling unusual, often alliterative epithets, some of which were coined by White House speechwriters William Safire and Pat Buchanan, including "pusillanimous pussyfooters," "nattering nabobs of negativism" (written by Safire) and "hopeless, hysterical hypochondriacs of history." In a speech denouncing the Moratorium to End the War in Vietnam, he characterized the war's opponents as "an effete corps of impudent snobs who characterize themselves as intellectuals."
Agnew was often characterized as Nixon's "hatchet man" when defending the administration on the Vietnam War. Agnew was chosen to make several speeches in which he spoke out against anti-war protesters and the media portrayal of the Vietnam War, labeling them "Un-American". However, he also spoke publicly against the actions of the Ohio Army National Guard that led to the Kent State shootings in 1970, even describing their action as "murder."
Despite Agnew's continued loyalty to the Nixon administration, his relationship with Nixon deteriorated, almost from the start of their political affiliation. Although Nixon initially liked and respected Agnew, as time progressed he felt his vice president lacked the intelligence and vision, particularly in foreign affairs, to sit in the Oval Office, and he began freezing Agnew out of the White House decision-making process. By some accounts, the President was also resentful that the self-confident Agnew was so popular with many Americans. By 1970, Agnew was limited to seeing the president only during cabinet meetings or in the occasional and brief one-on-one, with Agnew given no opportunity to discuss much of substance.
Oval Office tapes reveal that in 1971, Nixon and his chief of staff, H. R. Haldeman, discussed their desire to have Agnew resign from office before the following year's campaign season. One plan to achieve this was to try to persuade conservative investors to purchase one of the television networks, and then invite Agnew to run it. Another was to see if Bob Hope would be willing to take Agnew on as his partner in his cable television investments. These and other plans never went beyond the talking stages.
Nixon would have liked to replace Agnew on the Republican ticket in 1972 with John Connally, his chosen successor for 1976, but he realized that Agnew's large conservative base of supporters would be in an uproar, so he reluctantly kept him as his running mate. When John Ehrlichman, the President's counsel and assistant, asked Nixon why he kept Agnew on the ticket in the 1972 election, Nixon replied that "No assassin in his right mind would kill me" because they would get Agnew (as President).
Agnew came to enjoy the privileges that being vice president brought to him, particularly access to the rich and famous. He became close friends with Frank Sinatra, Billy Graham, and Bob Hope, and consorted with leaders around the globe. He took in stride his newfound fame, as his utterances often made newspaper front pages and were major stories on the evening network news broadcasts. Invitations for Agnew to give speeches across the country flooded into his office, and he became a top fundraiser for the Republican Party.
In April 1973, when revelations about Watergate began to surface, Agnew was the choice of 35 percent of Republican voters to be the next Republican nominee for President, while then-California Governor Ronald Reagan was second on the Gallup poll.
Resignation.
On October 10, 1973, Spiro Agnew became the second Vice President to resign the office. Unlike John C. Calhoun, who resigned to take a seat in the Senate, Agnew resigned and then pleaded no contest to criminal charges of tax evasion, part of a negotiated resolution to a scheme wherein he was accused of accepting more than $100,000 in bribes during his tenure as governor of Maryland. Agnew was fined $10,000 and received three years probation. The $10,000 fine covered only the taxes and interest due on what was "unreported income" from 1967. The plea bargain was later mocked by former Maryland Attorney General Stephen H. Sachs as "the greatest deal since the Lord spared Isaac on the mountaintop." Students of Professor John F. Banzhaf III from the George Washington University Law School, collectively known as Banzhaf's Bandits, found four residents of the state of Maryland willing to put their names on a case and sought to have Agnew repay the state $268,482, the amount it was said he had taken in bribes. After two appeals by Agnew, he finally wrote a check for $268,482 that was turned over to Maryland State Treasurer William S. James in early 1983.
As a result of his no contest plea, the state of Maryland later disbarred Agnew, calling him "morally obtuse". As in most jurisdictions, Maryland lawyers are automatically disbarred after being convicted of a felony, and a no contest plea exposes the defendant to the same penalties as one would face with a guilty plea.
Agnew's resignation triggered the first use of the 25th Amendment, specifically Section 2, as the vacancy prompted the appointment and confirmation of Gerald Ford, the House Minority Leader, as his successor. This remains one of only two instances in which the amendment has been employed to fill a vice-presidential vacancy. The second time was when Ford, after becoming President upon Nixon's resignation, chose Nelson Rockefeller (originally Agnew's mentor in the moderate wing of the Republican Party) to succeed him as Vice President. Had Agnew remained as Vice President when Nixon resigned just 10 months later, Agnew himself would have become the 38th President, instead of Ford.
Later life and death.
After leaving politics, Agnew became an international trade executive with homes in Rancho Mirage, California; Arnold, Maryland; Bowie, Maryland; and near Ocean City, Maryland. In 1976, he briefly reentered the public spotlight and engendered controversy with what Gerald Ford publicly criticized as "unsavory remarks about Jews" and anti-Zionist statements that called for the United States to withdraw its support for the state of Israel, citing Israel's allegedly bad treatment of Christians.
In 1980, Agnew published a memoir in which he implied that Nixon and his Chief of Staff, Alexander Haig, had planned to assassinate him if he refused to resign the Vice Presidency, and that Haig told him to "go quietly...or else", the memoir's title. Agnew also wrote a novel, "The Canfield Decision", about a Vice President who was "destroyed by his own ambition."
Agnew always maintained that the tax evasion and bribery charges were an attempt by Nixon to divert attention from the growing Watergate scandal. After their resignations, Agnew and Nixon never spoke to each other again. As a gesture of reconciliation, Nixon's daughters invited Agnew to attend Nixon's funeral in 1994, and Agnew accepted. In 1996, when Agnew died, Nixon's daughters returned the favor by attending Agnew's funeral.
Agnew died on September 17, 1996, at age 77 at Atlantic General Hospital, in Berlin, Maryland, in Worcester County (near his Ocean City home), only a few hours after being hospitalized and diagnosed with an advanced, yet to that point undetected, form of leukemia. Agnew is buried at Dulaney Valley Memorial Gardens, a cemetery in Timonium, Maryland, in Baltimore County in the Garden of the Last Supper section of the cemetery.
Tributes.
Agnew's official portrait was taken down from the Maryland State House Governor's Reception Room from 1979 until 1995. Governor Parris Glendening stated that in re-including Agnew's portrait, it was not up to anyone to alter history, whether for good or bad; he cited the 1949 novel "Nineteen Eighty-Four".
Under the provisions of an 1886 Senate resolution, all former vice presidents are entitled to a portrait bust in the United States Capitol. Plans were set in motion for a bust of Agnew while he was still in office, but were shelved following his resignation. The idea was revived by the Senate Rules Committee in 1992 and a bust was commissioned from North Carolina artist William Behrends, for whom Agnew sat for four sessions. The bust was unveiled May 24, 1995, in the presence of Agnew, his family, friends, and onetime political supporters. Agnew made a short speech and was visibly moved by the occasion.
Electoral history.
Baltimore County Executive, 1962
Governor of Maryland, 1966
1968 Republican National Convention (Vice Presidential tally)
United States presidential election, 1968
1972 Republican National Convention (Vice Presidential tally)
United States presidential election, 1972

</doc>
<doc id="39076" url="http://en.wikipedia.org/wiki?curid=39076" title="Battle of Mohács">
Battle of Mohács

The Battle of Mohács (]; Hungarian: "Mohácsi Csata "or" Mohácsi Vész"; Turkish: "Mohaç Savaşı "or" Mohaç Meydan Savaşı"; Polish: "Bitwa pod Mohaczem"; Croatian: "Bitka na Mohačkom polju"; Slovak: "Bitka pri Moháči"; German: "Schlacht bei Mohatsch") was fought on 29 August 1526 near Mohács, Hungary and was a decisive event for the history of Europe, in particular Central Europe, for the following centuries. In the battle, forces of the Kingdom of Hungary led by King Louis II of Hungary and Bohemia were defeated by forces of the Ottoman Empire led by Sultan Suleiman the Magnificent.
The Ottoman victory led to the partition of Hungary for several centuries between the Ottoman Empire, the Habsburg Monarchy, and the Principality of Transylvania. The death of Louis II as he fled the battle marked the end of the Jagiellon dynasty in Hungary and Bohemia, whose dynastic claims were absorbed by the Habsburgs via the marriage of Louis' sister.
Background.
Decline of the royal power (1490–1526).
After the death of the absolutist King Matthias Corvinus in 1490, the Hungarian magnates, who did not want another heavy-handed king, procured the accession of Vladislaus II (reigned 1490–1516), King of Bohemia, because of his notorious weakness: he was known as King Dobže, or "Dobzse" in Hungarian orthography (meaning "good" or, loosely, "OK") from his habit of accepting, without question, every petition and document laid before him. The freshly elected King Vladislaus II donated most of the royal estates, régales and royalties to the nobility. By this method, the king tried to stabilize his new reign and preserve his popularity amongst the magnates. After the naive fiscal and land policy of the royal court, the central power began to experience severe financial difficulties, largely due to the enlargement of feudal lands at his expense. The noble estate of the parliament succeeded in reducing the tax burden by 70-80 percent, at the expense of the country's ability to defend itself. Vladislaus became the magnates' helpless "prisoner"; he could make no decision without their consent. The standing mercenary army (the Black Army) of Matthias Corvinus was dissolved by the aristocracy. The magnates also dismantled the national administration systems and bureaucracy throughout the country. The country's defenses sagged as border guards and castle garrisons went unpaid, fortresses fell into disrepair, and initiatives to increase taxes to reinforce defenses were stifled. Hungary's international role declined, its political stability shaken, and social progress was deadlocked.
In 1514, the weakened and old King Vladislaus II faced a major peasant rebellion led by György Dózsa, which was ruthlessly crushed by the nobles, led by John Zápolya. After the Dózsa Rebellion, the brutal suppression of the peasants greatly aided the 1526 Turkish invasion as the Hungarians were no longer a politically united people. The resulting degradation of order paved the way for Ottoman pre-eminence. In 1521, the strongest Hungarian fortress in the South (Belgrade) fell to the Turks. The strongest nobles were so busy oppressing the peasants and quarrelling with the gentry class in the parliament that they failed to heed the agonized calls of King Louis II against the Turks. The early appearance of Protestantism further worsened internal relations in the country.
The Hungarians had long opposed Ottoman expansion in southeastern Europe, but the fall of Nándorfehérvár (present-day Belgrade, Serbia) and Szabács (now Šabac, Serbia) in 1521 meant that most of southern Hungary was left indefensible. King Louis II, King of Hungary and Bohemia, entered into marriage with Mary of Habsburg in 1522. The Ottomans saw that growing alliance as a threat to their power in the Balkans and worked to break this alliance. After Suleiman I came to power, the High Porte made the Hungarians at least one and possibly two offers of peace. It is unclear why Louis refused the offer. It is possible that King Louis was well aware of Hungary's situation (especially after the Battle of Chaldiran and Polish-Ottoman peace from 1525) and he believed that war was a better option than peace. Even in peacetime the Ottomans raided Hungarian lands and conquered small territories (with border castles), but a final battle still offered a glimmer of hope. To such ends, in June 1526, an Ottoman expedition advanced up the Danube River.
European events, and the Franco-Ottoman alliance.
King Francis I of France was defeated at the Battle of Pavia on 24 February 1525 by the troops of Habsburg H.R. Emperor Charles V. After several months in prison, Francis I was forced to sign the Treaty of Madrid.
In a watershed moment in European diplomacy, Francis came to an understanding with the Ottoman Empire, which then led to a formal Franco-Ottoman alliance.
The objective for Francis I was clearly to find an ally against the powerful Habsburg Emperor Charles V, in the person of Ottoman Sultan Suleiman the Magnificent. The Ottoman-French strategic, and sometimes tactical, alliance lasted for about three centuries. It did however cause quite a scandal in the Christian world.
To relieve the Habsburg pressure on France, Francis asked Suleiman to make war on the Holy Roman Empire, and the road from Turkey to the Holy Roman Empire led across Hungary. The request of the French king coincided nicely with the ambitions of Suleiman in Europe and gave him an incentive to attack Hungary in 1526, leading to the Battle of Mohács.
Preparations.
The loss of Belgrade (Nandorfehervar) in 1521 caused great alarm in Hungary, but the too-late and too-slowly-recruited 60,000 strong royal army – led by the king - forgot to take food along, so therefore the army disbanded spontaneously under the pressure of hunger and disease without even trying to recapture Belgrade, the southern key of Hungary, from the newly installed Turkish garrisons. In 1523, Archbishop Pál Tomori, a valiant priest-soldier, was made Captain of Southern Hungary. The general apathy that had characterized the country forced him to lean on his own bishopric revenues when he started to repair and reinforce the second line of Hungary's border defense system.
Three years later, an Ottoman army set out from Istanbul on 16 April 1526, led by Suleiman the Magnificent personally. The Hungarian nobles, who still did not realize the dimensions of the approaching danger, did not heed their King's call to the colours. Louis II ordered them to encamp on 2 July but no one reported on that day – not even the King. Only when Louis himself furnished an example with his appearance in the camp did things start to move. The Hungarian war council – without waiting for their reinforcements only a few days march away – made a serious tactical error by choosing the battlefield near Mohacs, an open but uneven plain with some swampy marshes.
The Hungarian army was divided into three main units: the Transylvanian army under John Zápolya, charged with guarding the passes in the Transylvanian Alps, with between 8,000 and 13,000 men; the main army, led by Louis himself (beside numerous Spanish, German, Czech and Serbian mercenaries); and another smaller force, commanded by the Croatian count Christoph Frankopan, numbering around 5,000 men. Due to geography, the Ottoman army's ultimate goal could not be determined until it was crossing the Balkan Mountains. Unfortunately for the Hungarians, by the time the Ottoman army had crossed, the Transylvanian and Croatian army was further from Buda than the Ottomans were. Contemporary historical records, though sparse, indicate that Louis preferred a plan of retreat, in effect ceding the country to Ottoman advances, rather than directly engaging the Ottoman army in open battle.
The Hungarian forces chose the battlefield, an open but uneven plain with some swampy marshes near Mohács leading down to the Danube. The Ottomans had been allowed to advance almost unopposed. While Louis waited in Buda, they had besieged several towns and crossed the Sava and Drava Rivers. Louis assembled around 25,000 to 30,000 soldiers (with Croatian and Polish contingents and about 800-1,000 soldiers of the Papal States) while the Ottoman army numbered around 50,000. However, military history books from the 21st century calculate the number of the Ottoman Army around 100,000 men. The Ottomans are said to have numbered over twice as many — though this figure is exaggerated — and had up to 160 cannon. " The Hungarian army was arrayed to take advantage of the terrain and hoped to engage the Ottoman army piecemeal. The only advantage the Magyars had that day was that their troops were well-rested, while the Turks had just completed a strenuous march in scorching summer heat. But rather than attacking their fatigued enemy then, the Hungarians just watched as they struggled through the marshy terrain. It would have been "unchivalrous" to attack the enemy when they were not yet ready for battle.
Battle.
Hungary built up an expensive but obsolete army, structured similarly to that of King Francis I at the Battle of Pavia mostly reliant on old fashioned heavily armoured knights on armoured horses (gendarme knights ). The Hungarian line consisted of two lines, the first with a center of mercenary infantry and artillery and the majority of the cavalry on either flank. The second line was a mix of levy infantry and cavalry. The Ottoman army was a more modern force built around the elite, musket-armed Janissaries, and artillery. The rest of the army consisted of feudal Timari cavalry and conscripted levies from Rumelia and the Balkans.
Like the uncertainty over the number of actual combatants, there is debate over the length of the battle. Its starting time is generally placed between 1:00 PM and 2:00 PM, but the endpoint is difficult to ascertain. While some historians have placed the length of the battle at two to three hours, this seems unlikely given several important factors. The Ottoman army did not retreat from the field and enter camp after the battle; instead, they remained on the field all night without food, water, or shelter. Given that the Ottoman historians all note that it was raining, it seems likely that had the battle been short and ended early in the afternoon, by 5:00 PM at the latest, the Sultan would have ordered his army to camp or at least to return to their baggage. The few reliable sources indicate that Louis left the field at twilight and made his escape under cover of darkness; since the sun would not have set until 6:27 PM on 29 August 1526, this would imply that the battle lasted significantly longer than two to three hours (perhaps as long as four or five).
As the first of Suleiman's troops, the Rumelian army, advanced onto the battlefield, they were attacked and routed by Hungarian troops led by Pál Tomori. This attack by the Hungarian right was successful in causing considerable chaos among the irregular Ottoman troops, but even as the Hungarian attack pressed forward, the Ottomans rallied with the arrival of Ottoman regulars deployed from the reserves. While the Hungarian right advanced far enough at one time to place Suleiman in danger from Hungarian arrows that struck his cuirass, the superiority of the Ottoman regulars and the timely charge of the Janissaries, the elite troops of the Ottomans, probably overwhelmed the attackers, particularly on the Hungarian left. The Hungarians took serious casualties from the skillfully handled Turkish artillery and musket volleys. The Hungarians could not hold their positions, and those who did not flee were surrounded and killed or captured. The result was a disaster, with the Hungarians advancing into withering fire and flank attacks. The king left the battlefield sometime around twilight but was thrown from his horse in a river at Csele and died, weighed down by his heavy armor. Some 1,000 other Hungarian nobles and leaders were also killed. It is generally accepted that more than 14,000 Hungarian soldiers were killed in the initial battle.
Suleiman could not believe that this small, "suicidal" army was all that once powerful country could muster against him, so he waited at Mohacs for a few days before moving cautiously against Buda.
Aftermath.
The victory did not give the Ottomans the security they wanted. Buda was left undefended, only the French and Venetian ambassadors waited for the Sultan to congratulate him on his great victory. Though they entered the unguarded evacuated Buda and pillaged the castle and surroundings, they retreated soon afterwards. It was not until 1541 that the Ottomans finally captured and occupied Buda (see main article) without a fight. However, for all intents and purposes, the Battle of Mohács meant the end of the independent Kingdom of Hungary as a unified entity. Amid political chaos, the divided Hungarian nobility elected two kings simultaneously, John Zápolya in 1526, and Ferdinand of Austria in 1527. The Ottoman occupation was contested by the Habsburg Archduke of Austria, Ferdinand I, Louis's brother-in-law and successor by treaty with King Vladislaus II.
Bohemia fell to the Habsburgs, who also dominated the Northern and western parts of Hungary and the remnants of the Kingdom of Croatia, while the Ottomans held central Hungary and suzerainty over semi-independent Transylvania. This provided the Hungarians with sufficient impetus to continue to resist the Ottoman occupation, which they did for another seventy years.
The subsequent near constant warfare required a sustained commitment of Ottoman forces, proving a drain on resources that the largely rural and war torn kingdom proved unable to repay. Christian armies besieged Buda several times during the 16th century, and Suleiman himself died of natural causes in Hungary during the Battle of Szigetvár in 1566; there were also two unsuccessful Ottoman sieges of Eger, which did not fall until 1596, seventy years after the Ottoman victory at Mohacs. The Turks proved unable to conquer the Northern and Western parts of Hungary which belonged to the Habsburg monarchs.
Legacy.
Mohács is seen by many Hungarians as the decisive downward turning point in the country's history, a national trauma that persists in the nation's folk memory. For moments of bad luck, Hungarians still say: "more was lost at Mohács" ("Több is veszett Mohácsnál"). Hungarians view Mohács as marking the end of an independent and powerful European nation.
Whilst Mohács was a decisive loss, it was the aftermath that truly put an end to independent Hungary. The ensuing two hundred years of near constant warfare between the two empires, Habsburg and Ottoman, turned Hungary into a perpetual battlefield. The countryside was regularly ravaged by armies moving back and forth, in turn devastating the population. Only in the 20th century would Hungary regain its political independence, but denuded of much of its land, and it has never regained its former political power.
The battlefield, beside the village of Sátorhely, became an official national historical memorial site in 1976 on the 450th anniversary of the battle. The memorial was designed by architect György Vadász. A new reception hall and exhibition building, also designed by Vadász and partially funded by the European Union, was completed in 2011.

</doc>
<doc id="39082" url="http://en.wikipedia.org/wiki?curid=39082" title="Peter Schickele">
Peter Schickele

Johann Peter Schickele (; born July 17, 1935) is an American composer, musical educator, and parodist, best known for comedy albums featuring music written by Schickele, but which he presents as being composed by the fictional P. D. Q. Bach.
Early life.
Schickele was born in Ames, Iowa, to Alsatian immigrant parents, and brought up in Washington, D.C., and Fargo, North Dakota, where he studied composition with Sigvald Thompson. He attended Fargo Central High School, graduating in 1952. He then attended Swarthmore College, graduating in 1957 with a degree in music; he was the first student at Swarthmore, and the only student in his class, with a music degree. He was a contemporary of Ted Nelson at Swarthmore, and he scored Nelson's experimental film, "The Epiphany of Slocum Furlow". It was his first film score. He graduated from the Juilliard School with an M.S. in musical composition; in the ensuing years he has frequently cited Roy Harris as the most influential of his teachers.
Early career.
Schickele wrote music for a number of folk musicians, most notably Joan Baez, for whom he also orchestrated and arranged three albums during the mid-1960s, "Noël" (1966), "Joan" (1967), and "" (1968).
Schickele, an accomplished bassoonist, was also a member of the chamber rock trio "Open Window", which wrote and performed music for the 1969 revue "Oh! Calcutta!".
The humorous aspect of Schickele's musical career came from his early interest in the music of Spike Jones, whose musical ensemble lampooned popular music in the 1940s and 1950s. While at Juilliard (1959) Schickele teamed with conductor Jorge Mester to present a humorous concert, which became an annual event at the college. In 1965, Schickele moved the concept to The Town Hall and invited the public to attend; by 1972, they had become so popular that they were moved to Avery Fisher Hall in Lincoln Center. Vanguard Records released an album of that concert, and P. D. Q. Bach's career was launched.
P. D. Q. Bach.
Besides composing music under his own name, Schickele has developed an elaborate parodic persona built around his studies of the fictional "youngest and the oddest of the twenty-odd children" of Johann Sebastian Bach, P.D.Q. Bach. Among the fictional composer's "forgotten" repertory supposedly "uncovered" by Schickele are such farcical works as "The Abduction of Figaro", "Canine Cantata: "Wachet Arf!"" (S. K9), "Good King Kong Looked Out", the "Trite Quintet" (S. 6 of 1), "O Little Town of Hackensack", "A Little Nightmare Music", the cantata "Iphigenia in Brooklyn", the "Concerto for Horn and Hardart", "The Art of The Ground Round (S. $1.19/lb.)," "Blaues Grasse (The Bluegrass Cantata)," and perhaps best known of all, the dramatic oratorio, "Oedipus Tex", featuring the "O.K. Chorale". Though P.D.Q. Bach is ostensibly a Baroque composer, Schickele extends his repertoire to parody much more modern works such as "Einstein on the Fritz", a parody of his Juilliard classmate Philip Glass.
His fictitious "home establishment," where he reports having tenure as "Very Full Professor Peter Schickele" of "musicolology" and "musical pathology", is the University of Southern North Dakota at Hoople, which is described as "a little-known institution which does not normally welcome out-of-state visitors". To illustrate the work of his uncovered composer, Schickele invented a range of rather unusual instruments. The most complicated of these is the Hardart, a variety of tone-generating devices mounted on the frame of an "automat", a coin-operated food dispenser. The modified automat is used in the "Concerto for Horn and Hardart", a play on the name of proprietors Horn & Hardart, who pioneered the North American use of the automat in their restaurants.
Schickele also invented the "dill piccolo" for playing sour notes, the "left-handed sewer flute", the "tromboon" ("a cross between a trombone and a bassoon, having all the disadvantages of both"), the "lasso d'amore", the double-reed slide music stand, which he described as having "a range of major third and even less expressiveness," the "tuba mirum", a flexible tube filled with wine, and the "pastaphone", an uncooked tube of manicotti pasta played as a horn. And then there was the extensively described but (so far as is known) never demonstrated "über klavier" or super piano, with a keyboard ranging from sounds which only dogs can hear down to sounds which only whales can make. Sample music was provided in P.D.Q.'s unauthorized autobiography, published in 1976. P.D.Q's "Pervertimento for Bagpipes, Bicycle and Balloons" (1965) demonstrated the inherent musical qualities of everyday objects in ways not equally agreeable to all who listen to them.
To some degree, Schickele's music written as P.D.Q. Bach has overshadowed Schickele's work as a "serious" composer.
For a period of time in the 1970s and early 1980s, performances by Schickele of the works of P.D.Q. Bach often involved guest appearances by the Swarthmore College Choir, often advertised as "fresh from their recent tour of Swarthmore, Pennsylvania."
Other musical career.
Schickele has composed more than 100 original works for symphony orchestra, choral groups, chamber ensemble, voice, film (e.g. "Silent Running" and an animated adaptation of "Where the Wild Things Are" (which he also narrated)), and television. He has also written music for school bands, as well as a number of musicals, and has organized numerous concert performances as both musical director and performer. Schickele was active on the international and North American concert circuit.
Schickele's musical creations have won him multiple awards. His extensive body of work is marked by a distinctive style which integrates the European classical tradition with an unmistakable American idiom.
In recent years, Schickele has created such not-quite-P.D.Q. Bach albums as "Hornsmoke", "Sneaky Pete and the Wolf", and "The Emperor's New Clothes".
Schickele's music is published by the Theodore Presser Company.
Radio.
As a musical educator he also hosted the classical music educational radio program "Schickele Mix" which was broadcast on many public radio stations in the United States. Lack of funding ended the production of new programs in the late 1990s, and rebroadcasts of the existing programs finally ceased in June 2007. Only 119 of the 169 programs were in the rebroadcast rotation, because earlier shows contained American Public Radio production IDs rather than ones crediting Public Radio International. In March 2006, some of the other "lost episodes" were added back to the rotation, with one notable program remnant of the "Periodic Table of Musics", listing the names of musicians and composers as mythical element names in a format reminiscent of the periodic table.
Awards.
Schickele's P. D. Q. Bach albums earned him four Grammy Awards for Best Comedy Recording/Album, in 1990, 1991, 1992, and 1993.
Personal life and family.
Schickele's two children, Matt and Karla, are both indie rock musicians. The two played together in the indie rock trio Beekeeper in the 1990s. Karla Schickele then joined the band Ida, and has recorded solo music under the name K. Matt Schickele is part of the M Shanghai String Band; he is also an orchestral music composer.
Peter Schickele's brother was the film director and musician David Schickele (d. 1999).

</doc>
<doc id="39083" url="http://en.wikipedia.org/wiki?curid=39083" title="The Stoned Guest">
The Stoned Guest

This musical work, while touted as "P. D. Q. Bach's Half-Act Opera: The Stoned Guest," is actually the work of Peter Schickele. The title is a play on the "stone guest" character in "Don Giovanni" by Mozart, as well as the opera "The Stone Guest" by Alexander Sergeyevich Dargomïzhsky. The work is a parody of classical opera, although some critics consider it to be the equal of many classical works in technical ability. The opera appears on the 1970 album of the same name.
The loose story combines elements of "Don Giovanni" with elements of "Carmen" by Georges Bizet. Some character names, such as "Don Octave" and "Donna Ribalda" play on the Mozart opera, referring to Don Ottavio and Donna Elvira respectively, while the castanet-clicking "Carmen Ghia" plays on the title character of Bizet's opera (and puns on the Volkswagen Karmann Ghia). The "Commendatoreador" plays on both operas at once, being a combination of "Il Commendatore" and the toreador Escamillo. The orchestral accompaniment for Donna Ribalda's opening aria, "Let's face it—I'm lost", bears more than a passing resemblance to the "Rex tremendae majestatis" from Mozart's "Requiem".
At one point in the opera, the rival divas Carmen Ghia and Donna Ribalda break character in the middle of a "recitative", so the singers can hold a conversation (still in recitative) about their singing careers. At a subsequent point, they have a contest to see who can hold a note the longest. The ending parodies many classical operas, "Don Giovanni" in particular, by apparently ending in tragedy, then having a completely unmotivated happy ending.

</doc>
<doc id="39085" url="http://en.wikipedia.org/wiki?curid=39085" title="Boris Godunov (opera)">
Boris Godunov (opera)

Boris Godunov (Russian: Борис Годунов, "Borís Godunóv") is an opera by Modest Mussorgsky (1839–1881). The work was composed between 1868 and 1873 in Saint Petersburg, Russia. It is Mussorgsky's only completed opera and is considered his masterpiece. Its subjects are the Russian ruler Boris Godunov, who reigned as Tsar (1598 to 1605) during the Time of Troubles, and his nemesis, the False Dmitriy (reigned 1605 to 1606). The Russian-language libretto was written by the composer, and is based on the drama "Boris Godunov" by Aleksandr Pushkin, and, in the Revised Version of 1872, on Nikolay Karamzin's "History of the Russian State".
"Boris Godunov", among major operas, shares with Giuseppe Verdi's "Don Carlos" (1867) the distinction of having the most complex creative history and the greatest wealth of alternative material. The composer created two versions—the Original Version of 1869, which was rejected for production by the Imperial Theatres, and the Revised Version of 1872, which received its first performance in 1874 in Saint Petersburg. These versions constitute two distinct ideological conceptions, not two variations of a single plan.
"Boris Godunov" has seldom been performed in either of the two forms left by the composer, frequently being subjected to cuts, recomposition, re-orchestration, transposition of scenes, or conflation of the original and revised versions.
Several composers, chief among them Nikolay Rimsky-Korsakov and Dmitriy Shostakovich, have created new editions of the opera to "correct" perceived technical weaknesses in the composer's original scores. Although these versions held the stage for decades, Mussorgsky's individual harmonic style and orchestration are now valued for their originality, and revisions by other hands have fallen out of fashion.
"Boris Godunov" comes closer to the status of a repertory piece than any other Russian opera, even Tchaikovsky's "Eugene Onegin", and is the most recorded Russian opera.
History.
Composition history.
"Note: Dates provided in this article for events taking place in Russia before 1918 are Old Style."
By the close of 1868, Mussorgsky had already started and abandoned two important opera projects—the antique, exotic, romantic tragedy "Salammbô", written under the influence of Aleksandr Serov's "Judith", and the contemporary, Russian, anti-romantic farce "Marriage", influenced by Aleksandr Dargomïzhsky's "The Stone Guest". Mussorgsky's next project would be a very original and successful synthesis of the opposing styles of these two experiments—the romantic-lyrical style of "Salammbô", and the realistic style of "Marriage" .
In the autumn of 1868, Vladimir Nikolsky, a professor of Russian history and language, and an authority on Pushkin, suggested to Mussorgsky the idea of composing an opera on the subject of Pushkin's "dramatic chronicle" "Boris Godunov". "Boris" the play, modelled on Shakespeare's histories, was written in 1825 and published in 1831, but was not approved for performance by the state censors until 1866, almost 30 years after the author's death. Production was permitted on condition that certain scenes were cut. Although enthusiasm for the work was high, Mussorgsky faced a seemingly insurmountable obstacle to his plans in that an Imperial ukaz of 1837 forbade the portrayal in opera of Russian Tsars (amended in 1872 to include only Romanov Tsars).
When Lyudmila Shestakova, the sister of Mikhail Glinka, learned of Mussorgsky's plans, she presented him with a volume of Pushkin's dramatic works, interleaved with blank pages and bound, and using this, Mussorgsky began work in October 1868 preparing his own libretto. consists of 25 scenes, written predominantly in blank verse. Mussorgsky adapted the most theatrically effective scenes, mainly those featuring the title character, along with a few other key scenes (Novodevichy, Cell, Inn), often preserving Pushkin's verses.
Mussorgsky worked rapidly, composing first the vocal score in about nine months (finished 18 July 1869), and completed the full score five months later (15 December 1869), at the same time working as a civil servant. In 1870, he submitted the libretto to the state censor for examination, and the score to the literary and music committees of the Imperial Theatres. However, the opera was rejected (10 February 1871) by a vote of 6 to 1, ostensibly for its lack of an important female role. Lyudmila Shestakova recalled the reply made by conductor Eduard Nápravník and stage manager Gennadiy Kondratyev of the Mariinsky Theatre in response to her question of whether "Boris" had been accepted for production:
"'No,' they answered me, 'it's impossible. How can there be an opera without the feminine element?! Mussorgsky has great talent beyond doubt. Let him add one more scene. Then "Boris" will be produced!'"—Lyudmila Shestakova, in "My Evenings", her recollections of Mussorgsky and The Mighty Handful, 1889
Other questionable accounts, such as Rimsky-Korsakov's, allege that there were additional reasons for rejection, such as the work's novelty:
"...Mussorgsky submitted his completed "Boris Godunov" to the Board of Directors of the Imperial Theatres ... The freshness and originality of the music nonplussed the honorable members of the committee, who reproved the composer, among other things, for the absence of a reasonably important female role."—Nikolay Rimsky-Korsakov, "Chronicle of My Musical Life", 1909
"All his closest friends, including myself, although moved to enthusiasm by the superb dramatic power and genuinely national character of the work, had constantly been pointing out to him that it lacked many essentials; and that despite the beauties with which it teemed, it might be found unsatisfactory in certain respects. For a long time he stood up (as every genuine artist is wont to do) for his creation, the fruit of his inspiration and meditations. He yielded only after "Boris" had been rejected, the management finding that it contained too many choruses and ensembles, whereas individual characters had too little to do. This rejection proved very beneficial to "Boris"."—Vladimir Stasov
Meanwhile, Pushkin's drama (18 of the published 24 scenes, condensed into 16) finally received its first performance in 1870 at the Mariinsky Theatre, three years in advance of the premiere of the opera in the same venue, using the same scene designs by Matvey Shishkov that would be recycled in the opera.
In 1871, Mussorgsky set about recasting and expanding the opera with enthusiasm, ultimately going beyond the requirements of the directorate of the Imperial Theatres, which called simply for the addition of a female role and a scene to contain it. Nevertheless, he added three scenes (the two Sandomierz scenes and the Kromï Scene), cut one (the scene 'At the Cathedral of Vasiliy the Blessed'), and recomposed another (the Terem Scene). The modifications resulted in the addition of an important prima donna role (Marina Mniszech), the expansion of existing female roles (additional songs for the Hostess, Fyodor, and the Nurse), and the expansion of the first tenor role (the Pretender). Mussorgsky augmented his adaptation of Pushkin's drama with his own lyrics, assisted by a study of the monumental "History of the Russian State" by Karamzin, to whom Pushkin's drama is dedicated.
The Revised Version was finished in 1872 (vocal score, 14 December 1871; full score 23 June 1872), and submitted to the Imperial Theatres in the autumn.
Most Mussorgsky biographers claim that the directorate of the Imperial Theatres also rejected the revised version of "Boris Godunov", even providing a date: 6 May 1872 (Calvocoressi), or 29 October 1872 (Lloyd-Jones). Recent researchers point out that there is insufficient evidence to support this claim, emphasizing that in his revision Mussorgsky had rectified the only objection the directorate is known to have made.
In any case, Mussorgsky's friends took matters into their own hands, arranging the performance of three scenes (the Inn and both Sandomierz scenes) at the Mariinsky Theatre on 5 February 1873, as a benefit for stage manager Gennadiy Kondratyev. César Cui's review noted the audience's enthusiasm:
"The success was enormous and complete; never, within my memory, had such ovations been given to a composer at the Mariinsky."—César Cui, "Sankt-Peterburgskie Vedomosti", 1873
The success of this performance led V. Bessel and Co. to announce the publication of the piano vocal score of Mussorgsky's opera, issued in January 1874.
The triumphant 1873 performance of three scenes paved the way for the first performance of the opera, which was accepted for production on 22 October 1873. The premiere took place on 27 January 1874, as a benefit for "prima donna" Yuliya Platonova. The Mariinsky Theatre was sold out, and the performance was a great success with the public. Mussorgsky had to take some 20 curtain calls; students sang choruses from the opera in the street. This time, however, the critical reaction was exceedingly hostile [see Critical Reception in this article for details].
Initial performances of "Boris Godunov" featured significant cuts. The entire Cell Scene was cut from the first performance, not, as is often supposed, due to censorship, but because Nápravník wished to avoid a lengthy performance, and frequently cut episodes he felt were ineffective. Later performances tended to be even more heavily cut, including the additional removal of the Kromï scene, likely for political reasons (starting 20 October 1876, the 13th performance). After protracted difficulties in obtaining the production of his opera, Mussorgsky was compliant with Nápravník's demands, and even defended these mutilations to his own supporters.
"Presently cuts were made in the opera, the splendid scene 'Near Kromï' was omitted. Some two years later, the Lord knows why, productions of the opera ceased altogether, although it had enjoyed uninterrupted success, and the performances under Petrov and, after his death, by F. I. Stravinsky, Platonova, and Komissarzhevsky had been excellent. There were rumors afloat that the opera had displeased the Imperial family; there was gossip that its subject was unpleasant to the censors; the result was that the opera was stricken from the repertory."—Nikolay Rimsky-Korsakov, "Chronicle of My Musical Life"
"Boris Godunov" was performed 21 times during the composer's lifetime, and 5 times after his death (in 1881) before being withdrawn from the repertory on 8 November 1882. When Mussorgsky's subsequent opera "Khovanshchina" was rejected for production in 1883, the Imperial Opera Committee reputedly said: "One radical opera by Mussorgsky is enough." "Boris Godunov" did not return to the stage of the Mariinsky Theatre until 9 November 1904, when the Rimsky-Korsakov edition was presented under conductor Feliks Blumenfeld with bass Fyodor Shalyapin in the title role.
"Boris Godunov" and the Imperial Family.
The reports of the antipathy of the Imperial family to Mussorgsky's opera are supported by the following accounts by Platonova and Stasov:
"During the [premiere], after the scene by the fountain, Grand Duke Konstantin Nikolayevich, a devoted friend of mine, but by the calumny of the Conservatory members, the sworn enemy of Musorgsky, approached me during the intermission with the following words: 'And you like this music so much that you chose this opera for a benefit performance?' 'I like it, Your Highness,' I answered. 'Then I am going to tell you that this is a shame to all Russia, and not an opera!' he screamed, almost foaming at the mouth, and then turning his back, he stomped away from me."—Yuliya Platonova, letter to Vladimir Stasov
"In the entire audience, I think only Konstantin Nikolayevich was unhappy (he does not like our school, in general) ... it was not so much the fault of the music as that of the libretto, where the 'folk scenes,' the riot, the scene where the police officer beats the people with his stick so that they cry out begging Boris to accept the throne, and so forth, were jarring to some people and infuriated them. There was no end to applause and curtain calls."—Vladimir Stasov, letter to his daughter, 1874
"When the list of operas for the winter was presented to His Majesty the Emperor, he, with his own hand, was pleased to strike out "Boris" with a wavy line in blue pencil."—Vladimir Stasov, letter to Nikolai Rimsky-Korsakov, 1888
Performance history.
"Note: This section lists performance data for the Saint Petersburg and Moscow premieres of each important version, the first performance of each version abroad, and premieres in English-speaking countries. Dates provided for events taking place in Russia before 1918 are Old Style."
The Coronation Scene was performed on 5 February 1872 by the Russian Music Society, conducted by Eduard Nápravník. The Polonaise from Act 3 was performed (without chorus) on 3 April 1872 by the Free School of Music, conducted by Miliy Balakirev.
Three scenes from the opera—the Inn Scene, the Scene in Marina's Boudoir, and the Fountain Scene—were performed on 5 February 1873 at the Mariinsky Theatre. Eduard Nápravník conducted. The cast included Darya Leonova (Innkeeper), Fyodor Komissarzhevsky (Pretender), Osip Petrov (Varlaam), Vasiliy Vasilyev (or 'Vasilyev II') (Misail), Mikhail Sariotti (Police Officer), Yuliya Platonova (Marina), Josef Paleček (Rangoni), and Feliks Krzesiński (Old Polish Noble).
The Revised Version of 1872 received its world premiere on 27 January 1874 at the Mariinsky Theatre. The Cell Scene was omitted. The Novodevichiy and Coronation scenes were combined into one continuous scene: 'The Call of Boris to the Tsardom'. Matvey Shishkov's design for the last scene of Pushkin's drama, 'The House of Boris' (see illustration, right), was substituted for this hybrid of the Novodevichiy and Coronation scenes. The scenes were grouped in five acts as follows:
Production personnel included Gennadiy Kondratyev (stage director), Ivan Pomazansky (chorus master), Matvey Shishkov, Mikhail Bocharov, and Ivan Andreyev (scene designers), and Vasiliy Prokhorov (costume designer). Eduard Nápravník conducted. The cast included Ivan Melnikov (Boris), Aleksandra Krutikova (Fyodor), Wilhelmina Raab (Kseniya), Olga Shryoder (nurse), Vasiliy Vasilyev, 'Vasilyev II' (Shuysky), Vladimir Sobolev (Shchelkalov), Vladimir Vasilyev, 'Vasilyev I' (Pimen, Lawicki), Fyodor Komissarzhevsky (Pretender), Yuliya Platonova (Marina), Josef Paleček (Rangoni), Osip Petrov (Varlaam), Pavel Dyuzhikov (Misail), Antonina Abarinova (Innkeeper), Pavel Bulakhov (Yuródivïy), Mikhail Sariotti (Nikitich), Lyadov (Mityukha), Sobolev (Boyar-in-Attendance), Matveyev (Khrushchov), and Sobolev (Czernikowski). The production ran for 26 performances over 9 years.
The premiere established traditions that have influenced subsequent Russian productions (and many abroad as well): 1) Cuts made to shorten what is perceived as an overlong work; 2) Declamatory and histrionic singing by the title character, often degenerating in climactic moments into shouting (initiated by Ivan Melnikov, and later reinforced by Fyodor Shalyapin); and 3) Realistic and historically accurate sets and costumes, employing very little stylization.
The Cell Scene (Revised Version) was first performed on 16 January 1879 in Kononov Hall, at a concert of the Free School of Music, in the presence of Mussorgsky. Nikolay Rimsky-Korsakov conducted. The cast included Vladimir Vasilyev, "Vasilyev I" (Pimen), and Vasiliy Vasilyev, 'Vasilyev II' (Pretender).
The Revised Version of 1872 received its Moscow premiere on 16 December 1888 at the Bolshoy Theatre. The Cell and Kromï scenes were omitted. Production personnel included Anton Bartsal (stage director), and Karl Valts (scene designer). Ippolit Altani conducted. The cast included Bogomir Korsov (Boris), Nadezhda Salina (Fyodor), Aleksandra Karatayeva (Kseniya), O. Pavlova (Nurse), Anton Bartsal (Shuysky), Pyotr Figurov (Shchelkalov), Ivan Butenko (Pimen), Lavrentiy Donskoy (Pretender), Mariya Klimentova (Marina), Pavel Borisov (Rangoni), Vladimir Streletsky (Varlaam), Mikhail Mikhaylov (Misail), Vera Gnucheva (Innkeeper), and Aleksandr Dodonov (Boyar-in-attendance). The production ran for 10 performances.
The Rimsky-Korsakov edition premiered on 28 November 1896 in the Great Hall of the Saint Petersburg Conservatory. Nikolay Rimsky-Korsakov conducted. The cast included Mikhail Lunacharsky (Boris), Gavriil Morskoy (Pretender), Nikolay Kedrov (Rangoni), and Fyodor Stravinsky (Varlaam). The production ran for 4 performances.
Bass Fyodor Shalyapin first appeared as Boris on 7 December 1898 at the Solodovnikov Theatre in a Private Russian Opera production. The Rimsky-Korsakov edition of 1896 was performed. Production personnel included Savva Mamontov (producer), and Mikhail Lentovsky (stage director). Giuseppe Truffi conducted. The cast also included Anton Sekar-Rozhansky (Pretender), Serafima Selyuk-Roznatovskaya (Marina), Varvara Strakhova (Fyodor), and Vasiliy Shkafer (Shuysky). The production ran for 14 performances.
The Rimsky-Korsakov edition of 1908 premiered on 19 May 1908 at the Paris Opéra. The Cell Scene preceded the Coronation Scene, the Inn Scene and the Scene in Marina's Boudoir were omitted, the Fountain Scene preceded the Terem Scene, and the Kromï Scene preceded the Death Scene. Production personnel included Sergey Dyagilev (producer), Aleksandr Sanin (stage director), Aleksandr Golovin, Konstantin Yuon, Aleksandr Benua, and Yevgeniy Lansere (scene designers), Ulrikh Avranek (chorusmaster), and Ivan Bilibin (costume designer). Feliks Blumenfeld conducted. The cast included Fyodor Shalyapin (Boris), Klavdiya Tugarinova (Fyodor), Dagmara Renina (Kseniya), Yelizaveta Petrenko (Nurse), Ivan Alchevsky (Shuysky), Nikolay Kedrov (Shchelkalov), Vladimir Kastorsky (Pimen), Dmitri Smirnov (Pretender), Nataliya Yermolenko-Yuzhina (Marina), Vasiliy Sharonov (Varlaam), Vasiliy Doverin-Kravchenko (Misail), Mitrofan Chuprïnnikov (Yuródivïy), and Khristofor Tolkachev (Nikitich). The production ran for 7 performances.
The United States premiere of the 1908 Rimsky-Korsakov edition took place on 19 March 1913 at the Metropolitan Opera, and was based on Sergey Dyagilev's Paris production. The opera was presented in three acts. The Cell Scene preceded the Coronation Scene, the scene in Marina's Boudoir was omitted, and the Kromï Scene preceded the Death Scene. However, the Inn Scene, which was omitted in Paris, was included. Scenery and costume designs were the same as used in Paris in 1908—made in Russia by Golovin, Benua, and Bilibin, and shipped from Paris. The opera was sung in Italian. Arturo Toscanini conducted. The cast included Adamo Didur (Boris), Anna Case (Fyodor), Leonora Sparkes (Kseniya), Maria Duchêne (Nurse), Angelo Badà (Shuysky), Vincenzo Reschiglian (Shchelkalov, Lawicki), Jeanne Maubourg (Innkeeper), Léon Rothier (Pimen), Paul Althouse (Pretender), Louise Homer (Marina), Andrés de Segurola (Varlaam), Pietro Audisio (Misail), Albert Reiss (Yuródivïy), Giulio Rossi (Nikitich), Leopoldo Mariani (Boyar-in-Attendance), and Louis Kreidler (Czernikowski).
The United Kingdom premiere of the 1908 Rimsky-Korsakov edition took place on 24 June 1913 at the Theatre Royal, Drury Lane in London. Production personnel included Sergey Dyagilev (producer) and Aleksandr Sanin (stage director). Emil Cooper conducted. The cast included Fyodor Shalyapin (Boris), Mariya Davïdova (Fyodor), Mariya Brian (Kseniya), Yelizaveta Petrenko (Nurse, Innkeeper), Nikolay Andreyev (Shuysky), A. Dogonadze (Shchelkalov), Pavel Andreyev (Pimen), Vasiliy Damayev (Pretender), Yelena Nikolayeva (Marina), Aleksandr Belyanin (Varlaam), Nikolay Bolshakov (Misail), Aleksandr Aleksandrovich (Yuródivïy), and Kapiton Zaporozhets (Nikitich).
The newly published St. Basil's Scene was performed on 18 January 1927 at the Bolshoy Theatre in the 1926 revision by Mikhail Ippolitov-Ivanov, commissioned in 1925 to accompany the Rimsky-Korsakov edition. Production personnel included Vladimir Lossky (stage director) and Fyodor Fedorovsky (scene designer). Ariy Pazovsky conducted. The cast included Ivan Kozlovsky (Yuródivïy), and Leonid Savransky (Boris). The production ran for 144 performances.
The Original Version of 1869 premiered on 16 February 1928 at the State Academic Theatre of Opera and Ballet. Production personnel included Sergey Radlov (stage director), and Vladimir Dmitriyev (scene designer). Vladimir Dranishnikov conducted. The cast included Mark Reyzen (Boris), Aleksandr Kabanov (Shuysky), Ivan Pleshakov (Pimen), Nikolay Pechkovsky (Pretender), Pavel Zhuravlenko (Varlaam), Yekaterina Sabinina (Innkeeper), and V. Tikhiy (Yuródivïy).
The first performance of the 1869 Original Version abroad took place on 30 September 1935 at the Sadler's Wells Theatre. The opera was sung in English. Lawrance Collingwood conducted. The cast included Ronald Stear (Boris).
The premiere of the Shostakovich orchestration of 1940 of Pavel Lamm's vocal score took place on 4 November 1959 at the Kirov Theatre. Sergey Yeltsin conducted. The cast included Boris Shtokolov (Boris).
Versions.
"Note: Musicologists do not agree on the terms used to refer to the two authorial versions of "Boris Godunov". Editors Pavel Lamm and Boris Asafyev used "preliminary redaction" and "principal redaction" for the 1st and 2nd versions, respectively, and David Lloyd-Jones designated them "initial" and "definitive." This article, aiming for utmost objectivity, uses "original" and "revised.""
The Original Version of 1869 is rarely heard. It is distinguished by its greater fidelity to Pushkin's drama and its almost entirely male cast of soloists. It also conforms to the recitative opera style (opéra dialogué) of "The Stone Guest" and "Marriage", and to the ideals of kuchkist realism, which include fidelity to text, formlessness, and emphasis on the values of spoken theatre, especially through naturalistic declamation. The unique features of this version include:
The terse Terem Scene of the 1869 version and the unrelieved tension of the two subsequent and final scenes make this version more dramatically effective to some critics (e.g., Boris Asafyev).
The Revised Version of 1872 represents a retreat from the ideals of Kuchkist realism, which had come to be associated with comedy, toward a more exalted, tragic tone, and a conventionally operatic style—a trend that would be continued in the composer's next opera, "Khovanshchina". This version is longer, is richer in musical and theatrical variety, and balances naturalistic declamation with more lyrical vocal lines. The unique features of this version include:
Mussorgsky rewrote the Terem Scene for the 1872 version, modifying the text, adding new songs and plot devices (the parrot and the clock), modifying the psychology of the title character, and virtually recomposing the music of the entire scene.
This version has made a strong comeback in recent years, and has become the dominant version.
The Piano Vocal Score of 1874 was the first published form of the opera, and is essentially the 1872 version with some minor musical variants and small cuts. The 1874 vocal score does not constitute a 'third version', but rather a refinement of the 1872 Revised Version.
The distribution of scenes in the authorial versions is as follows:
Upon revising the opera, Mussorgsky initially replaced the St. Basil's Scene with the Kromï Scene. However, on the suggestion of Vladimir Nikolsky, he transposed the order of the last two scenes, concluding the opera with the Kromï Scene rather than the Faceted Palace Scene. This gives the overall structure of the 1872 Revised Version the following symmetrical form:
The Rimsky-Korsakov Version of 1908 has been the most traditional version over the last century, but has recently been almost entirely eclipsed by Mussorgsky's Revised Version (1872). It resembles the Vocal Score of 1874, but the order of the last two scenes is reversed [see Versions by Other Hands in this article for more details].
Roles.
"Note:" Roles designated with an asterisk do not appear in the 1869 Original Version. 'Yuródivïy' is often translated as 'Simpleton' or 'Idiot'. However, 'Holy Fool' is a more accurate English equivalent. In other roles lists created by Mussorgsky, Pimen is designated a monk (инок), Grigoriy a novice (послушник), Rangoni a Cardinal (кардинал), Varlaam and Misail vagrant-monks (бродяги-чернецы), the Innkeeper a hostess (хозяйка), and Khrushchov a Voyevoda (воевода). Pimen, Grigoriy, Varlaam, and Misail were likely given non-clerical designations to satisfy the censor.
Historical basis of the plot.
An understanding of the drama of "Boris Godunov" may be facilitated by a basic knowledge of the historical events surrounding the Time of Troubles, the interregnum of relative anarchy following the end of the Ryurik Dynasty (1598) and preceding the Romanov Dynasty (1613). Key events are as follows:
Note: The culpability of Boris in the matter of Dmitriy's death can neither be proved nor disproved. Karamzin accepted his responsibility for the 'crime' as fact, and Pushkin and Mussorgsky after him assumed his guilt to be true, at least for the purpose of creating a tragedy in the mold of Shakespeare. Modern historians, however, tend to acquit Boris.
Synopsis.
Note: "Shishkov and Bocharev designed the sets (samples below), some of which were used in the first complete performance in 1874."
( ) = "Arias and numbers"
[ ] = "Passages cut from or added to the 1872 Revised Version [see Versions in this article for details]"
Part 1 / Prologue.
"Scene 1: The Courtyard of the Novodevichiy Monastery near Moscow (1598)"
There is a brief introduction foreshadowing the 'Dmitriy Motif'. The curtain opens on a crowd in the courtyard of the monastery, where the weary regent Boris Godunov has temporarily retired. Nikitich the police officer orders the assembled people to kneel. He goads them to clamor for Boris to accept the throne. They sing a chorus of supplication ("To whom dost thou abandon us, our father?"). The people are bewildered about their purpose and soon fall to bickering with each other, resuming their entreaties only when the policeman threatens them with his club. Their chorus reaches a feverish climax. Andrey Shchelkalov, the Secretary of the Duma, appears from inside the convent, informs the people that Boris still refuses the throne of Russia ("Orthodox folk! The boyar is implacable!"), and requests that they pray that he will relent. An approaching procession of pilgrims sings a hymn ("Glory to Thee, Creator on high"), exhorting the people to crush the spirit of anarchy in the land, take up holy icons, and go to meet the Tsar. They disappear into the monastery.
"Scene 2: [Cathedral] Square in the Moscow Kremlin (1598)."
The orchestral introduction is based on bell motifs. From the porch of the Cathedral of the Dormition, Prince Shuysky exhorts the people to glorify Tsar Boris. As the people sing a great chorus of praise ("Like the beautiful sun in the sky, glory"), a solemn procession of boyars exits the cathedral. The people kneel. Boris appears on the porch of the cathedral. The shouts of "Glory!" reach a climax and subside. Boris delivers a brief monologue ("My soul grieves") betraying a feeling of ominous foreboding. He prays for God's blessing, and hopes to be a good and just ruler. He invites the people to a great feast, and then proceeds to the Cathedral of the Archangel to kneel at the tombs of Russia's past rulers. The people wish Boris a long life ("Glory! Glory! Glory!"). A crowd breaks toward the cathedral. The police officers struggle to maintain order. The people resume their shouts of "Glory!"
Part 2 / Act 1.
"Scene 1: Night. A Cell in the Chudov Monastery [within the Moscow Kremlin] (1603)"
Pimen, a venerable monk, writes a chronicle ("Yet one last tale") of Russian history. The young novice Grigoriy awakes from a horrible (and prophetic) dream, which he relates to Pimen, in which he climbed a high tower, was mocked by the people of Moscow, and fell. Pimen advises him to fast and pray. Grigoriy regrets that he retired so soon from worldly affairs to become a monk. He envies Pimen's early life of adventure. Pimen speaks approvingly of Ivan the Terrible and his son Fyodor, who both exhibited great spiritual devotion, and draws a contrast with Boris, a regicide.
Upon discovering the similarity in age between himself and the murdered Tsarevich, Grigoriy conceives the idea of posing as the Pretender. As Pimen departs for Matins, Grigoriy declares that Boris shall escape neither the judgment of the people, nor that of God.
"Scene 2: An Inn on the Lithuanian Border (1603)"
There is a brief orchestral introduction based on three prominent themes from this scene.
The vagrants Varlaam and Misail, who are dressed as monks and are begging for alms, and their companion Grigoriy, who is in secular garb, arrive and enter. After exchanging greetings, Varlaam requests some wine. When the Hostess returns with a bottle, he drinks and launches into a ferocious song ("So it was in the city of Kazan") of Ivan the Terrible's siege of Kazan. The two monks quickly become tipsy, and soon begin to doze. Grigoriy quietly asks the Hostess for directions to the Lithuanian border. Policemen appear in search of a fugitive heretic monk (Grigoriy) who has run off from the Chudov Monastery declaring that he will become Tsar in Moscow. Noticing Varlaam's suspicious appearance and behavior, the lead policeman thinks he has found his man. He cannot read the ukaz (edict) he is carrying, however, so Grigoriy volunteers to read it. He does so, but, eyeing Varlaam carefully, he substitutes Varlaam's description for his own. The policemen quickly seize Varlaam, who protests his innocence and asks to read the ukaz. He haltingly reads the description of the suspect, which of course matches Grigoriy. Grigoriy brandishes a dagger, and leaps out of the window. The men set off in pursuit.
Part 3 / Act 2.
"The Interior of the Tsar's Terem in the Moscow Kremlin (1605)"
Kseniya (or Xenia), clutching a portrait of "Prince Ivan", her betrothed who has died, sings a brief mournful aria ("Where are you, my bridegroom?"). Fyodor studies a great map of the Tsardom of Russia.
Kseniya's nurse assures her that she will soon forget about "Prince Ivan".
Boris abruptly enters, briefly consoles Kseniya, and then sends her and her nurse to their own quarters. Fyodor shows Boris the map of Russia. After encouraging his son to resume his studies, Boris delivers a long and fine soliloquy ("I have attained supreme power").
The boyar-in-attendance brings word of the arrival of Prince Shuysky, and reports a denunciation against him for his intrigues.
Prince Shuysky now enters. Boris insults him, accusing him of conspiring with Pushkin, an ancestor of the poet. However, the prince brings grave tidings. A Pretender has appeared in Lithuania. Boris angrily demands to know his identity. Shuysky fears the Pretender might attract a following bearing the name of Tsarevich Dmitriy. Shaken by this revelation, Boris dismisses Fyodor. He orders Shuysky to seal the border with Lithuania, and, clearly on the edge of madness, asks Shuysky whether he has ever heard of dead children rising from their graves to question Tsars. Boris seeks assurance that the dead child the prince had seen in Uglich was really Dmitriy. He threatens Shuysky, if he dissembles, with a gruesome execution. The Prince describes the ghastly scene of Dmitriy's murder in a brief and beautiful aria ("In Uglich, in the cathedral"). But he gives hints that a miracle (incorruptibility) has occurred. Boris begins choking with guilt and remorse, and gives a sign for Shuysky to depart.
Boris hallucinates ("Hallucination or 'Clock' Scene"). The spectre of the dead Dmitriy reaches out to him. Addressing the apparition, he denies his responsibility for the crime: "Begone, begone child! I am not thy murderer... the will of the people!" He collapses, praying that God will have mercy on his guilty soul.
Act 3 (The 'Polish' Act).
"Scene 1: The Boudoir of Marina Mniszech in Sandomierz [Poland] (1604)"
Maidens sing a delicate, sentimental song ("On the blue Vistula") to entertain Marina as her chambermaid dresses her hair. Marina declares her preference for heroic songs of chivalry. She dismisses everyone. Alone, she sings of her boredom ("How tediously and sluggishly"), of Dmitriy, and of her thirst for adventure, power, and glory. The Jesuit Rangoni enters, bemoans the wretched state of the church, attempts to obtain Marina's promise that when she becomes Tsaritsa she will convert the heretics of Moscow (Russian Orthodox Church) to the true faith (Roman Catholicism), and encourages her to bewitch the Pretender. When Marina wonders why she should do this, Rangoni angrily insists that she stop short of nothing, including sacrificing her honor, to obey the dictates of the church. Marina expresses contempt of his hypocritical insinuations and demands he leave. As Rangoni ominously tells her she is in the thrall of infernal forces, Marina collapses in dread. Rangoni demands her obedience.
"Scene 2: Mniszech's Castle in Sandomierz. A Garden. A Fountain. A Moonlit Night (1604)"
Woodwind and harp accompany a pensive version of the 'Dmitriy Motif'. The Pretender dreams of an assignation with Marina in the garden of her father's castle. However, to his annoyance, Rangoni finds him. He brings news that Marina longs for him and wishes to speak with him. The Pretender resolves to throw himself at Marina's feet, begging her to be his wife and Tsaritsa. He entreats Rangoni to lead him to Marina. Rangoni, however, first begs the Pretender to consider him a father, allowing him to follow his every step and thought. Despite his mistrust of Rangoni, the Pretender agrees not to part from him if he will only let him see Marina. Rangoni convinces the Pretender to hide as the Polish nobles emerge from the castle dancing a polonaise ("Polonaise"). Marina flirts, dancing with an older man. The Poles sing of taking the Muscovite throne, defeating the army of Boris, and capturing him. They return to the castle. The Pretender comes out of hiding, cursing Rangoni. He resolves to abandon wooing Marina and begin his march on Moscow. But then Marina appears and calls to him. He is lovesick. She, however, only wants to know when he will be Tsar, and declares she can only be seduced by a throne and a crown. The Pretender kneels at her feet. She rejects his advances, and, attempting to spur him to action, dismisses him, calling him a lackey. Having reached his limit, he tells her he will depart the next day to lead his army to Moscow and to his father's throne. Furthermore, as Tsar he will take pleasure in watching her come crawling back looking for her own lost throne, and will command everyone to laugh at her. She quickly reverses course, tells him she adores him, and they sing a duet ("O Tsarevich, I implore you"). Rangoni observes the amorous couple from afar, and, joining them in a brief trio, cynically rejoices in his victory.
Part 4 / Act 4.
"Scene 1 [1869 Version only]: The Square before the Cathedral of Vasiliy the Blessed in Moscow (1605)"
A crowd mills about before the Cathedral of the Intercession (the Temple of Vasiliy the Blessed) in Red Square. Many are beggars, and policemen occasionally appear. A group of men enters, discussing the anathema the deacon had declared on Grishka (Grigoriy) Otrepyev in the mass. They identify Grishka as the Tsarevich. With growing excitement they sing of the advance of his forces to Kromï, of his intent to retake his father's throne, and of the defeat he will deal to the Godunovs. A yuródivïy enters, pursued by urchins. He sings a nonsensical song ("The moon is flying, the kitten is crying"). The urchins greet him and rap on his metal hat. The yuródivïy has a kopek, which the urchins promptly steal. He whines pathetically. Boris and his retinue exit the Cathedral. The boyars distribute alms. In a powerful chorus ("Benefactor father... Give us bread!"), the hungry people beg for bread. As the chorus subsides, the yuródivïy's cries are heard. Boris asks why he cries. The yuródivïy reports the theft of his kopek and asks Boris to order the boys' slaughter, just as he did in the case of the Tsarevich. Shuysky wants the yuródivïy seized, but Boris instead asks for the holy man's prayers. As Boris exits, the yuródivïy declares that the Mother of God will not allow him to pray for Tsar Herod (see Massacre of the Innocents). The yuródivïy then sings his lament ("Flow, flow, bitter tears!") about the fate of Russia.
"Scene 2 [1869 Version] / Scene 1 [1872 version]: The Faceted Palace in the Moscow Kremlin (1605)"
A session of the Duma is in progress.
After some arguments, the boyars agree ("Well, let's put it to a vote, boyars"), in a powerful chorus, that the Pretender and his sympathizers should be executed. Shuysky, whom they distrust, arrives with an interesting story. Upon leaving the Tsar's presence, he observed Boris attempting to drive away the ghost of the dead Tsarevich, exclaiming: "Begone, begone child!" The boyars accuse Shuysky of spreading lies. However, a dishevelled Boris now enters, echoing Shuysky: "Begone child!" The boyars are horrified. After Boris comes to his senses, Shuysky informs him that a humble old man craves an audience. Pimen enters and tells the story ("One day, at the evening hour") of a blind man who heard the voice of the Tsarevich in a dream. Dmitry instructed him to go to Uglich and pray at his grave, for he has become a miracle worker in heaven. The man did as instructed and regained his sight. This story is the final blow for Boris. He calls for his son, declares he is dying ("Farewell, my son, I am dying"), gives Fyodor final counsel, and prays for God's blessing on his children. In a very dramatic scene ("The bell! The funeral bell!"), he dies.
"Scene 2 [1872 Version only]: A Forest Glade near Kromï (1605)"
Tempestuous music accompanies the entry of a crowd of vagabonds who have captured the boyar Khrushchov. The crowd taunts him, then bows in mock homage ("Not a falcon flying in the heavens"). The yuródivïy enters, pursued by urchins. He sings a nonsensical song ("The moon is flying, the kitten is crying"). The urchins greet him and rap on his metal hat. The yuródivïy has a kopek, which the urchins promptly steal. He whines pathetically. Varlaam and Misail are heard in the distance singing of the crimes of Boris and his henchmen ("The sun and moon have gone dark"). They enter. The crowd gets worked up to a frenzy ("Our bold daring has broken free, gone on a rampage") denouncing Boris. Two Jesuits are heard in the distance chanting in Latin ("Domine, Domine, salvum fac"), praying that God will save Dmitriy. They enter. At the instigation of Varlaam and Misail, the vagabonds prepare to hang the Jesuits, who appeal to the Holy Virgin for aid. Processional music heralds the arrival of Dmitriy and his forces. Varlaam and Misail glorify him ("Glory to thee, Tsarevich!") along with the crowd. The Pretender calls those persecuted by Godunov to his side. He frees Khrushchov, and calls on all to march on Moscow. All exeunt except the yuródivïy, who sings a plaintive song ("Flow, flow, bitter tears!") of the arrival of the enemy and of woe to Russia.
Critical reception.
Russian opera of the early 1870s was dominated by Western European works—mainly Italian. The domestic product was regarded with skepticism and sometimes hostility. The playwright Tikhonov wrote in 1898:
"In those days, splendid performances of Russian operas were given, but the attendance was generally poor. ...to attend the Russian opera was not fashionable. At the first performance of "The Maid of Pskov" [in 1873] there was a good deal of protest. An energetic campaign was being waged against the 'music of the future'–that is, that of the 'mighty handful'."
As the most daring and innovative member of the Mighty Handful, Mussorgsky frequently became the target of conservative critics and rival composers, and was often derided for his supposedly clumsy and crude musical idiom. After the premiere of "Boris Godunov", influential critic Herman Laroche wrote:
"The general decorativeness and crudity of Mr. Mussorgsky's style, his passion for the brass and percussion instruments, may be considered to have been borrowed from Serov. But never did the crudest works of the model reach the naive coarseness we note in his imitator."
Reviews of the premiere performance of "Boris Godunov" were for the most part hostile. Some critics dismissed the work as "chaotic" and "a cacophony". Even his friends Mily Balakirev and César Cui, leading members of the Mighty Handful, minimized his accomplishment. Unable to overlook Mussorgsky's "trespasses against the conventional musical grammar of the time", they failed to recognize the giant step forward in musical and dramatic expression that "Boris Godunov" represented. Cui betrayed Mussorgsky in a notoriously scathing review of the premiere performance:
"Mr. Mussorgsky is endowed with great and original talent, but "Boris" is an immature work, superb in parts, feeble in others. Its main defects are in the disjointed recitatives and the disarray of the musical ideas... These defects are not due to a lack of creative power... The real trouble is his immaturity, his incapacity for severe self-criticism, his self-satisfaction, and his hasty methods of composition..."
Although he found much to admire, particularly the Inn Scene and the Song of the Parrot, he reproved the composer for a poorly constructed libretto, finding the opera to exhibit a lack of cohesion between scenes. He claimed that Mussorgsky was so deficient in the ability to write instrumental music that he dispensed with composing a prelude, and that he had "borrowed the cheap method of characterization by leitmotives from Wagner."
Other composers were even more censorious. Pyotr Tchaikovsky wrote in a letter to his brother Modest:
"I have studied "Boris Godunov" and "The Demon" thoroughly. Mussorgsky's music I send to the devil; it is the most vulgar and vile parody on music..."
Of the critics who evaluated the new opera, only the 19-year-old critic Vladimir Baskin defended Mussorgsky's skill as a composer. Writing under the pen name "Foma Pizzicato", Baskin wrote,
"Dramatization in vocal music could go no farther. Mussorgsky has proved himself to be a philosopher-musician, capable of expressing with rare truth the mind and soul of his characters. He also has a thorough understanding of musical resources. He is a master of the orchestra; his working-out is fluent, his vocal and chorus parts are beautifully written."
Although "Boris Godunov" is usually praised for its originality, dramatic choruses, sharply delineated characters, and for the powerful psychological portrayal of Tsar Boris, it has received an inordinate amount of criticism for technical shortcomings: weak or faulty harmony, part-writing, and orchestration. Nikolay Rimsky-Korsakov said:
"I both adore and abhor "Boris Godunov". I adore it for its originality, power, boldness, distinctiveness, and beauty; I abhor it for its lack of polish, the roughness of its harmonies, and, in some places, the sheer awkwardness of the music."
The perception that "Boris" needed correction due to Mussorgsky's poverty of technique prompted Rimsky-Korsakov to revise it after his death. His edition supplanted the composer's Revised Version of 1872 in Russia, and launched the work abroad, remaining the preferred edition for some 75 years (see Versions by Other Hands in this article for more details). For decades, critics and scholars pressing for the performance of Mussorgsky's own versions fought an often losing battle against the conservatism of conductors and singers, who, raised on the plush Rimsky-Korsakov edition, found it impossible to adapt to Mussorgsky's comparatively unrefined and bleak original scores.
Recently, however, a new appreciation for the rugged individuality of Mussorgsky's style has resulted in increasing performances and recording of his original versions. Musicologist Gerald Abraham wrote:
"...in the perspective of a hundred years we can see that Musorgsky's score did not really need 'correction' and reorchestration, that in fact the untouched "Boris" is finer than the revised "Boris"."
For many, "Boris Godunov" is the greatest of all Russian operas because of its originality, drama, and characterization, regardless of any cosmetic imperfections it may possess.
Performance practice.
A conflation of the 1869 and 1872 versions is often made when staging or recording "Boris Godunov". This typically involves choosing the 1872 version and augmenting it with the St. Basil's Scene from the 1869 version. This practice is popular because it conserves a maximum amount of music, it gives the title character another appearance on stage, and because in the St. Basil's Scene Boris is challenged by the Yurodivïy, the embodiment of his conscience.
However, because the composer transferred the episode of the Yurodivïy and the urchins from the St. Basil's Scene to the Kromï Scene when revising the opera, restoring the St. Basil's Scene to its former location creates a problem of duplicate episodes, which can be partially solved by cuts. Most performances that follow this practice cut the robbery of the Yurodivïy in the Kromï Scene, but duplicate his lament that ends each scene.
The Rimsky-Korsakov Version is often augmented with the Ippolitov-Ivanov reorchestration of the St. Basil's Scene (commissioned by the Bolshoy Theatre in 1925, composed in 1926, and first performed in 1927).
Conductors may elect to restore the cuts the composer himself made in writing the 1872 version [see Versions in this article for more details]. The 1997 Mariinsky Theatre recording under Valery Gergiev is the first and only to present the 1869 Original Version side by side with the 1872 Revised Version, and, it would seem, attempts to set a new standard for musicological authenticity. However, although it possesses many virtues, the production fails to scrupulously separate the two versions, admitting elements of the 1872 version into the 1869 recording, and failing to observe cuts the composer made in the 1872 version.
Critics argue that the practice of restoring the St. Basil's scene and all the cuts that Mussorgsky made when revising the opera—that is, creating a "supersaturated" version—can have negative consequences, believing that it destroys the symmetrical scene structure of the Revised Version, it undermines the composer's carefully devised and subtle system of leitmotiv deployment, and results in the overexposure of the Dmitriy motive.
Versions by other hands.
Nikolay Rimsky-Korsakov 1896.
After Mussorgsky's death in 1881, his friend Nikolay Rimsky-Korsakov undertook to put his scores in order, completing and editing "Khovanshchina" for publication (1883), reconstructing "Night on Bald Mountain" (1886), and "correcting" some songs. Next, he turned to "Boris".
"Although I know I shall be cursed for so doing, I will revise "Boris". There are countless absurdities in its harmonies, and at times in its melodies."—Nikolay Rimsky-Korsakov, 15 April 1893
He experimented first with the Polonaise, temporarily scoring it for a Wagner-sized orchestra in 1889. In 1892 he revised the Coronation Scene, and, working from the 1874 Vocal Score, completed the remainder of the opera, although with significant cuts, by 1896.
Nikolay Rimsky-Korsakov 1908.
In the spring of 1906, Rimsky-Korsakov revised and orchestrated several passages omitted in the 1896 revision:
Rimsky-Korsakov compiled a new edition in 1908, this time restoring the cuts, and making some significant changes:
These revisions clearly went beyond mere reorchestration. He made substantial modifications to harmony, melody, dynamics, etc., even changing the order of scenes.
"Maybe Rimsky-Korsakov's harmonies are softer and more natural, his part-writing better, his scoring more skillful; but the result is not Mussorgsky, nor what Mussorgsky aimed at. The genuine music, with all its shortcomings, was more appropriate. I regret the genuine "Boris", and feel that should it ever be revived on the stage of the Mariinskiy Theatre, it is desirable that it should be in the original."—César Cui, in an article in "", 1899
"Besides re-scoring "Boris" and correcting harmonies in it (which was quite justifiable), he introduced in it many arbitrary alterations, which disfigured the music. He also spoilt the opera by changing the order of scenes."—Miliy Balakirev, letter to Calvocoressi, 25 July 1906
Rimsky-Korsakov immediately came under fire from some critics for altering "Boris", particularly in France, where his revision was introduced. The defense usually made by his supporters was that without his ministrations, Mussorgsky's opera would have faded from the repertory due to difficulty in appreciating his raw and uncompromising idiom. Therefore, Rimsky-Korsakov was justified in making improvements to keep the work alive and increase the public's awareness of Mussorgsky's melodic and dramatic genius.
"I remained inexpressibly pleased with my revision and orchestration of "Boris Godunov", heard by me for the first time with a large orchestra. Moussorgsky's violent admirers frowned a bit, regretting something... But having arranged the new revision of "Boris Godunov", I had not destroyed its original form, had not painted out the old frescoes forever. If ever the conclusion is arrived at that the original is better, worthier than my revision, then mine will be discarded and "Boris Godunov" will be performed according to the original score."—Nikolay Rimsky-Korsakov, "Chronicle of My Musical Life", 1909
The Rimsky-Korsakov version remained the one usually performed in Russia, even after Mussorgsky's earthier original (1872) regained its place in Western opera houses. The Bolshoy Theatre has only recently embraced the composer's own version.
Dmitri Shostakovich 1940.
Dmitriy Shostakovich worked on "Boris Godunov" in 1939–1940 on a commission from the Bolshoy Theatre for a new production of the opera. A conflation of the 1869 and 1872 versions had been published by Pavel Lamm and aroused keen interest in the piece. However, it did not erase doubts as to whether Mussorgsky's own orchestration was playable. The invasion of Russia by Nazi Germany prevented this production from taking place, and it was not until 1959 that Shostakovich's version of the score was premiered.
To Shostakovich, Mussorgsky was successful with solo instrumental timbres in soft passages but did not fare as well with louder moments for the whole orchestra. Shostakovich explained:
"Mussorgsky has marvelously orchestrated moments, but I see no sin in my work. I didn't touch the successful parts, but there are many unsuccessful parts because he lacked mastery of the craft, which comes only through time spent on your backside, no other way."—Dmitry Shostakovich
Shostakovich confined himself largely to reorchestrating the opera, and was more respectful of the composer's unique melodic and harmonic style. However, Shostakovich greatly increased the contributions of the woodwind and especially brass instruments to the score, a significant departure from the practice of Mussorgsky, who exercised great restraint in his instrumentation, preferring to utilize the individual qualities of these instruments for specific purposes. Shostakovich also aimed for a greater symphonic development, wanting the orchestra to do more than simply accompany the singers.
"This is how I worked. I placed Mussorgsky's piano arrangement in front of me and then two scores—Mussorgsky's and Rimsky-Korsakov's. I didn't look at the scores, and I rarely looked at the piano arrangement either. I orchestrated by memory, act by act. Then I compared my orchestration with those of Mussorgsky and Rimsky-Korsakov. If I saw that either had done it better, then I stayed with that. I didn't reinvent bicycles. I worked honestly, with ferocity, I might say."—Dmitriy Shostakovich
Shostakovich remembered Alexander Glazunov telling him how Mussorgsky himself played scenes from "Boris" at the piano. Mussorgsky's renditions, according to Glazunov, were brilliant and powerful—qualities Shostakovich felt did not come through in the orchestration of much of "Boris". Shostakovich, who had known the opera since his student days at the Saint Petersburg Conservatory, assumed that Mussorgsky's orchestral intentions were correct but that Mussorgsky simply could not realize them:
"As far as I can tell, he imagined something like a singing line around the vocal parts, the way subvoices surround the main melodic line in Russian folk song. But Mussorgsky lacked the technique for that. What a shame! Obviously, he had a purely orchestral imagination, and purely orchestral imagery, as well. The music strives for "new shores," as they say—musical dramaturgy, musical dynamics, language, imagery. But his orchestral technique drags us back to the old shores."—Dmitriy Shostakovich
One of those "old shore" moments was the large monastery bell in the scene in the monk's cell. Mussorgsky and Rimsky-Korsakov both use the gong. To Shostakovich, this was too elemental and simplistic to be effective dramatically, since this bell showed the atmosphere of the monk's estragement. "When the bell tolls," Shostakovich told Solomon Volkov, "it's a reminder that there are powers mightier than man, that you can't escape the judgment of history." Therefore, Shostakovich reorchestrated the bell's tolling by the simultaneous playing of seven instruments—bass clarinet, double bassoon, French horns, gong, harps, piano, and double basses (at an octave). To Shostakovich, this combination of instruments sounded more like a real bell.
Shostakovich admitted Rimsky-Korsakov's orchestration was more colorful than his own and used brighter timbres. However, he also felt that Rimsky-Korsakov chopped up the melodic lines too much and, by blending melody and subvoices, may have subverted much of Mussorgsky's intent. Shostakovich also felt that Rimsky-Korsakov did not use the orchestra flexibly enough to follow the characters' mood changes, instead making the orchestra calmer, more balanced.
Igor Buketoff 1997.
The American conductor Igor Buketoff created a version in which he removed most of Rimsky-Korsakov's additions and reorchestrations, and fleshed out some other parts of Mussorgsky's original orchestration. This version had its first performance in 1997 at the Metropolitan Opera, New York, under Valery Gergiev.
References.
Notes
Sources
External links.
Libretto
Other

</doc>
<doc id="39086" url="http://en.wikipedia.org/wiki?curid=39086" title="Waylon Jennings">
Waylon Jennings

Waylon Arnold Jennings (pronounced ; June 15, 1937 – February 13, 2002) was an American singer, songwriter, musician, and actor. Jennings began playing guitar at 8 and began performing at 12 on KVOW radio. His first band was "The Texas Longhorns". Jennings worked as a D.J. on KVOW, KDAV, KYTI, and KLLL. In 1958, Buddy Holly arranged Jennings's first recording session, of "Jole Blon" and "When Sin Stops (Love Begins)". Holly hired him to play bass. In Clear Lake, Iowa, Jennings gave up his seat on the ill-fated flight that crashed and killed Holly, J. P. Richardson, and others. The day of the flight was later known as The Day the Music Died. Jennings then worked as a D.J. in Coolidge, Arizona, and Phoenix. He formed a rockabilly club band, The Waylors. He recorded for independent label Trend Records and A&M Records, before succeeding with RCA Victor after achieving creative control.
During the 1970s, Jennings joined the Outlaw movement. He released critically acclaimed albums "Lonesome, On'ry and Mean" and "Honky Tonk Heroes", followed by hit albums "Dreaming My Dreams" and "Are You Ready for the Country". In 1976 he released the album "Wanted! The Outlaws" with Willie Nelson, Tompall Glaser, and Jessi Colter, the first platinum country music album. That success was followed by "Ol' Waylon", and the hit song "Luckenbach, Texas". By the early 1980s, Jennings was struggling with a cocaine addiction, which he quit in 1984. Later he joined the country supergroup The Highwaymen with Nelson, Kris Kristofferson, and Johnny Cash. During that period, Jennings released the successful album "Will the Wolf Survive". He toured less after 1997, to spend more time with his family. Between 1999 and 2001, his appearances were limited by health problems. On February 13, 2002, Jennings died from complications of diabetes.
Jennings also appeared in movies and television series. He was the balladeer for "The Dukes of Hazzard"; composing and singing the show's theme song. In 2001 he was inducted into the Country Music Hall of Fame, which he chose not to attend. In 2007 he was posthumously awarded the Cliffie Stone Pioneer Award by the Academy of Country Music.
Early life.
Waylon Arnold Jennings was born on June 15, 1937 on the J.W Bittner farm, near Littlefield, Texas. He was the son of Lorene Beatrice (née Shipley) and William Albert Jennings. The Jennings family line descended from Irish and Black-Dutch. Meanwhile, the Shipley family moved from Tennessee and settled in Texas. The Shipley line descended from Cherokee and Comanche families.
The name on his birth certificate was Wayland, meaning land by the highway. His name was changed after a Baptist preacher visited Jennings's parents and congratulated his mother for naming him after the Wayland Baptist University in Plainview, Texas. Lorene Jennings, who had been unaware of the college, changed the spelling to Waylon. Jennings later expressed in his autobiography, "I didn't like Waylon. It sounded corny and hillbilly, but it's been good to me, and I'm pretty well at peace with it right now."
After working as a laborer on the Bittner farm, Jennings' father moved the family to Littlefield and established a retail creamery. When Jennings was eight, his mother taught him to play guitar with the tune "Thirty Pieces of Silver". Jennings used to practice with his relatives' guitars, until his mother bought him a used Stella, and later ordered a Harmony Patrician. Early influences were Bob Wills, Floyd Tillman, Ernest Tubb, Hank Williams, Carl Smith and Elvis Presley.
Beginning at family gatherings, Jennings advanced to perform at the Youth Center with Anthony Bonanno followed by appearances at the local Jaycees and Lions club. He won a talent show at Channel 13, in Lubbock, Texas, singing "Hey Joe". He later made frequent performances at the Palace Theater in Littlefield, during local talent night.
Music career.
Beginnings in music.
The 12-year-old Jennings auditioned for a spot on KVOW in Littlefield, Texas. Owner J.B. McShan, along with Emil Macha, recorded Jennings's performance. McShan liked his style and hired him for a weekly 30-minute program. Following this successful introduction, Jennings formed his own band. He asked Macha to play bass for him, and gathered other friends and acquaintances to form "The Texas Longhorns". The style of the band, a mixture of country and western and bluegrass, was often not well received.
At age sixteen, after several disciplinary infractions, tenth-grader Jennings was convinced to drop out of high school by the superintendent. Upon leaving school, he worked for his father in the produce store, also taking temporary jobs. Jennings felt that music, his favorite activity, would turn into his career. The next year, Jennings and The Texas Longhorns recorded a demo of the songs "Stranger in My Home" and "There'll Be a New Day" at KFYO radio in Lubbock, Texas. Meanwhile, he drove a truck for the Thomas Land Lumber Company, and a cement truck for the Roberts Lumber Company. Tired of the owner, and after a minor driving accident, Jennings quit. He and other local musicians often performed at country radio station KDAV; it was during this time that he met Buddy Holly at a Lubbock restaurant. He and Holly became friends, often meeting during local shows. Jennings also attended Holly's performances on KDAV's "Sunday Party".
In addition to performing on air for KVOW, Jennings started to work as a D.J. in 1956, and moved to Lubbock. His program ran for six hours, from four in the afternoon to ten in the evening. Jennings played two hours of country classics, two of current country and two of mixed recordings. During those final two hours, Jennings played artists such as Chuck Berry and Little Richard. The owner reprimanded him each time he aired the recordings, and when he then played two Richard records in a row, the owner fired him.
During his time at KVOW, Jennings was visited by D.J Sky Corbin, who worked at KLVT in Levelland, Texas. Corbin was impressed with his voice, and decided to visit Jennings at the station after hearing him sing a jingle to the tune of Hank Snow's "I'm Moving On". Jennings expressed his economic struggle to live on a US$50-a week salary. Corbin invited Jennings to visit KLVT, where he eventually took Corbin's then-vacated position. The Corbin family later purchased KLLL, in Lubbock. They changed the format of the station to country, becoming the main competition of KDAV. The Corbins hired Jennings as the station's first D.J.
Jennings produced commercials and created jingles with the rest of the D.J's. As their popularity increased, the D.J's made public appearances. Jennings' events included live performances. During one performance, Buddy's father, L.O Holley, approached them with his son's latest record, and requested them to play it at the station. L.O mentioned his son's intention to start producing artists himself, and Corbin recommended Jennings. After returning from his England tour, Buddy Holly visited KLLL.
Holly took Jennings as his first artist. He outfitted him with new clothes, and worked with him to improve his image. He arranged a session for Jennings at Norman Petty's recording studios in Clovis, New Mexico. On September 10 Jennings recorded the songs "Jole Blon" and "When Sin Stops (Love Begins)" with Holly and Tommy Allsup on guitars with saxophonist King Curtis. Holly then hired Jennings to play electric bass for him during his "Winter Dance Party Tour".
Winter Dance Party Tour.
Before the tour, Holly vacationed with his wife in Lubbock, and visited Jennings' radio station in December 1958. Jennings and Sky Corbin performed the hand claps to Holly's tune "You're the One". Jennings and Holly soon left for New York City, arriving on January 15, 1959. Jennings stayed at Holly's apartment by Washington Square Park, on the days prior to a meeting scheduled on the headquarters of the "General Artists Corporation", that organized the tour. They later took a train to Chicago to join the band.
The tour began in Milwaukee, Wisconsin, on January 23, 1959. The amount of travel created logistical problems. The distance between venues had not been considered when scheduling each performance. Adding to the disarray, the tour buses were not equipped for the weather and twice broke down. After a show in Clear Lake, Iowa, Buddy Holly chartered a plane for himself, Jennings and Allsup to avoid the long bus trip to Moorhead, Minnesota. Jennings gave up his seat to J. P. Richardson, who was suffering from the flu and complaining about how uncomfortable the bus was for a man of his size. Holly jokingly told Jennings, "I hope your ol' bus freezes up!" Jennings replied, "Well, I hope your ol' plane crashes!" During the early morning hours of February 3, 1959, later known as The Day the Music Died, the charter crashed outside Clear Lake, killing all on board.
Jennings's family heard on the radio that "Buddy Holly and his band had been killed". After calling his family, Jennings called Sky Corbin at KLLL from Fargo to say that he was alive. The General Artists Corporation promised to pay a first class ticket for Jennings and the band to assist Holly's funeral in Lubbock, in exchange for them playing that night in Moorhead, Minnesota. After the first show, they were initially denied their payment by the venue, but after Jennings' persistence, they were paid. The flights were never paid, and Jennings and Allsup continued the tour for two more weeks, featuring Jennings as the lead singer. They were paid less than half of the original agreed salary, and upon returning to New York, Jennings put Holly's guitar and amplifier in a locker in Grand Central Station and mailed the keys to Maria Elena Holly. Then, he returned to Lubbock. Jennings later admitted that he felt responsibility for the crash.
"Jole Blon" was released on Brunswick in March 1959 with limited success. Now unemployed, he returned to KLLL. Still affected by the death of Holly, his performance at the station worsened. He left the station after he was denied a raise, and later worked briefly for the competition, KDAV.
Phoenix and the Nashville Sound.
Due to Maxine's father's illness, Jennings had to shuttle between Arizona and Texas. While his family lived back in Littlefield, Jennings found a job briefly at KOYL in Odessa, Texas. He moved with his family to Coolidge, Arizona, where his wife's sister lived. He found a job performing at the "Galloping Goose" bar, where he was heard by Earl Perrin, who offered him a spot on KCKY. Jennings also played during the intermission on drive-in theaters and on bars. After a successful a performance at the Cross Keys Club in Phoenix, Arizona, he was approached by contractors who were building a club for Jimmy D. Musiel, called "JD's". Musiel employed Jennings as his main artist and designed the club around his act.
He formed his backing band, The Waylors with bassist Paul Foster, guitarist Jerry Gropp and drummer Richie Albright. Jennings and his band performed at the newly opened nightspot in Scottsdale, where they soon earned a strong local fanbase. At JD's, Jennings developed his "rock tempered" style of country music, that would define him on his later career.
In 1961, Jennings signed a recording contract with Trend Records, and experienced moderate success with his single, "Another Blue Day". His friend, Don Bowman took demos of Jennings to Jerry Moss, who at the time was starting A&M Records with associate Herb Alpert. On July 9, 1963, Jennings signed a contract with A&M that granted him five percent of record sales. At A&M he recorded "Love Denied" backed with "Rave On", and "Four Strong Winds" backed with "Just to Satisfy You". He followed up by recording demos of "The Twelfth of Never", "Kisses Sweeter than Wine" and "Don't Think Twice, It's All Right"; and also produced the single "Sing the Girls a Song, Bill", backed with "The Race Is On". The singles were released between April and October 1964.
His records had little success, because A&M's main releases were folk music rather than country. He had a few hits on local radio in Phoenix, with "Four Strong Winds" and "Just To Satisfy You" (co-written with Bowman). Meanwhile, he recorded an album on BAT records, called "JD's". After 500 copies were sold at the club, another 500 copies were pressed by the Sounds label. He also played lead guitar for Patsy Montana on a 1964 album.
Singer Bobby Bare heard Jennings' "Just to Satisfy You" on his car radio while passing through Phoenix, eventually recording it and "Four Strong Winds" After stopping in Phoenix to attend to a Jennings performance at JD's, while driving to Las Vegas, Bare stopped and called Chet Atkins in Nashville, suggesting that he needed to sign Jennings.
When he was made aware of the new deal, Waylon wasn't sure if he should quit his gig at JD's. He then went to get the advice of his friend, RCA artist Willie Nelson, who had gone to see one of Waylon's shows. When Willie and Waylon met, after talking about the possibilities and considering Waylon's profits at the club, Nelson suggested that Waylon should stay in Phoenix and not to move to Nashville.
Nonetheless, Jennings decided to accept the offer, and asked Herb Alpert to release him from his contract with A&M. Alpert agreed, though later A&M would compile all of Jennings' singles and unreleased material the label had and release it as "Don't Think Twice". Atkins formally signed Jennings to RCA Victor in 1965. On August 21, Jennings made his first appearance on the "Billboard's" Hot Country Songs chart with "That's the Chance I'll Have to Take".
In 1966, Jennings released his debut album for RCA "Folk-Country", followed by "Leavin' Town", and "Nashville Rebel". "Leavin' Town" resulted in significant chart success as the first two singles "Anita, You're Dreaming" and "Time to Burn Again" both peaking at No. 17 on the Billboard Hot Country Singles chart. The album's third single, a cover of Gordon Lightfoot's "(That's What You Get) For Lovin' Me", became Jennings' first top 10 single, peaking at No. 9. "Nashville Rebel" was the soundtrack to an independent film of the same name, starring Jennings. The single "Green River" charted on "Billboard" country singles at #11. In 1967 Jennings released a hit single, "Just to Satisfy You". During an interview, Jennings remarked that the song was a "pretty good example" of the influence of his work with Buddy Holly and rockabilly music. Jennings produced midchart albums that sold well, including "Just to Satisfy You", that included the same-named hit single of 1967. Jennings' singles enjoyed success. "The Chokin' Kind" peaked at number eight on "Billboard's" Hot Country Singles in 1967, while "Only Daddy That'll Walk the Line" hit number two the following year. In 1969, his collaboration with The Kimberlys on the single "MacArthur Park" earned a Grammy Award for Best Country Performance by a Duo or Group. His single "Brown Eyed Handsome Man" reached number three at the Hot Country Singles chart by the end of the year.
During this time, Jennings rented an apartment in Nashville with singer Johnny Cash. Jennings and Cash were both managed by "Lucky" Moeller's booking agency Moeller Talent, Inc. The tours organized by the agency were unproductive, with the artists being booked to venues located far from each other in close dates. After paying for the accommodation and travel expenditures, Jennings' profits were reduced, with him frequently requesting advances from the agency or RCA Records to play the next venue. While playing three-hundred days on the road, Jennings' debt increased along with his consumption of amphetamines, as he believed himself to be trapped on the circuit.
In 1972 Jennings released "Ladies Love Outlaws". The single that headlined the album became a hit for Jennings, and was his first approach to Outlaw Country. Jennings was accustomed to performing and recording with his own band, The Waylors, a practice that was not encouraged by powerful Nashville producers. Over time, however, Jennings felt limited by the Nashville sound's lack of artistic freedom. The music style publicized as "Countrypolitan" was characterized by orchestral arrangements, and the absence of traditional country music instruments. The producers did not let Jennings play his own guitar or select material to record.
Outlaw Country.
In an interview, Jennings recalled the restrictions of the Nashville establishment: "They wouldn't let you do anything. You had to dress a certain way: you had to do everything a certain way... They kept trying to destroy me... I just went about my business and did things my way... You start messing with my music, I get mean." By 1972, after the release of "Ladies Love Outlaws", his recording contract was nearing an end. Sick with hepatitis, Jennings was hospitalized. Afflicted by disease, debt and the music industry, he was considering retirement. Albright visited him and convinced him to continue. Albright talked to him about making Neil Reshen his new manager. Meanwhile, Jennings requested a US$25,000 royalty advance from RCA Records to cover his living expenses during his recovery. The same day he met Rashen, RCA sent Jerry Bradley to offer Jennings US$5,000 as a bonus for signing a new 5% royalty deal with RCA, the same terms he had accepted in 1965. After reviewing with Reshen, he rejected the offer and hired Reshen.
Reshen started to renegotiate Jennings' recording and touring contracts. At a meeting in a Nashville airport, Jennings introduced Reshen to Willie Nelson. By the end of the meeting, Reshen had become Nelson's manager as well. Jennings's new deal gained him a $75,000 advance and artistic control. Reshen advised Jennings to keep the beard that he had grown in the hospital, to match the image of outlaw country.
By 1973, Nelson had returned to music, finding success with Atlantic Records. Now based in Austin, Texas, he had made inroads into the rock and roll press by attracting rock audiences. Atlantic Records was now attempting to sign Jennings, but Nelson's rise to popularity persuaded RCA to renegotiate with Jennings before losing another potential star.
In 1973, Jennings released "Lonesome, On'ry and Mean" and "Honky Tonk Heroes", the first albums recorded and released under his creative control. The release of these albums heralded a major turning point for Jennings, kicking off his most critically and commercially successful years. More hit albums followed with "This Time" and "The Ramblin' Man", both released in 1974. The title tracks of both albums topped the "Billboard" country singles chart, with the self-penned "This Time" becoming Jennings' first No. 1 single. "Dreaming My Dreams", released in 1975, included the No. 1 single "Are You Sure Hank Done It This Way" and was his first album to be certified gold by the RIAA; it was also the first of his next six consecutive, solo studio albums to be certified gold or higher. In 1976, Jennings released "Are You Ready for the Country", Jennings wanted the record to be produced by Los Angeles producer Ken Mansfield. Although RCA denied the request, Jennings and The Waylors went to Los Angeles and recorded with Mansfield at his expense. After a month, Jennings presented the master tape to Chet Atkins, who decided to release it. The album hit number one on "Billboard"'s country albums three times the same year, topping the charts for 10 weeks. It was named country album of the year in 1976 by "Record World Magazine" and it was certified gold by the RIAA.
In 1976 Jennings released the album "Wanted! The Outlaws", recorded with Willie Nelson, Tompall Glaser, and Jessie Colter for RCA. The album was the first country music album certified platinum. The following year, RCA issued "Ol' Waylon", an album that produced a hit duet with Nelson, "Luckenbach, Texas". The album "Waylon and Willie" followed in 1978, producing the hit single "Mammas Don't Let Your Babies Grow Up to Be Cowboys". Jennings released "I've Always Been Crazy", also in 1978. The same year, at the peak of his success, Jennings began to feel limited by the outlaw movement. Jennings referred to the over-exploitation of the image in the song "Don't You Think This Outlaw Bit Has Done Got Out of Hand?", claiming that the movement had become a "self-fulfilling prophecy". In 1979 he released "Greatest Hits", which was certified gold the same year, and quintuple platinum in 2002.
Also in 1979, Jennings joined the cast of the CBS series "The Dukes of Hazzard" as The Balladeer, the narrator. The only episode to feature him in person was "Welcome, Waylon Jennings", during the seventh season. Jennings played himself, presented as an old friend of the Duke family. For the show, he also wrote and sang the theme song "Good Ol' Boys", which became the biggest hit of his career. Released as a single in promotion with the show, it became Jennings' twelfth single to reach number one on the Billboard Country Singles chart. It was also a crossover hit, peaking at twenty-one on the Billboard Hot 100.
Later years.
In the mid-1980s, Johnny Cash, Kris Kristofferson, Nelson, and Jennings formed a successful group called The Highwaymen. Aside from his work with The Highwaymen, Jennings' released a gold album "WWII" (1982) with Willie Nelson.
In 1985 Jennings joined with USA for Africa to record "We Are the World," but he left the studio because of a dispute over the song's lyrics that were to be sung in Swahili. Ironically, after Jennings left the session, the idea was dropped at the prompting of Stevie Wonder, who pointed out that Ethiopians did not speak Swahili. By this time, his sales had decreased. After the release of "Sweet Mother Texas", Jennings signed with Music Corporation of America. The debut release with the label "Will the Wolf Survive" (1985) peaked at number one in "Billboard's" Country albums in 1986. Jennings's initial success tailed off, and in 1990 he signed with Epic Records. His first release, "The Eagle", became his final top 10 album.
Also in 1985, he made a cameo appearance in the live-action children's film "". In the movie, he plays a turkey farm truck driver that gives Big Bird a lift. He also sings one of the film's songs, entitled "Ain't No Road Too Long".
In 1993, in collaboration with Rincom Children's Entertainment, Jennings recorded an album of children's songs, "Cowboys, Sisters, Rascals & Dirt", which included "Shooter's Theme," a tribute to his 14-year-old with the theme of "a friend of mine".
Despite low record sales, Jennings' live appearances attracted large audiences. In 1997, after the "Lollapalooza tour", he decreased his tour schedule and became centered on his family.
In 1998, Jennings teamed up with Bare, Jerry Reed, and Mel Tillis to form The Old Dogs. The group recorded a double album of songs by Shel Silverstein.
In mid-1999, Jennings assembled what he referred to as his "hand-picked dream team" and formed Waylon & The Waymore Blues Band. Consisting primarily of former Waylors, the 13-member group performed concerts from 1999 to 2001. In January 2000, Jennings recorded what would become his final album at Nashville's historic Ryman Auditorium, "".
Music style and image.
Jennings' music was characterized by his "powerful" singing voice, noted by his "rough-edged quality," as well as his phrasing and texture.
He was also recognized for his "spanky-twang" guitar style. To create his sound, he used a mixture of thumb and fingers during the rhythmic parts, while using picks for the lead runs. He combined hammer-on and pull-off riffs, with eventual upper-fret double stops and modulation effects. Jennings played a 1953 Fender Telecaster, a used guitar that was a gift from The Waylors. Jennings's bandmates adorned his guitar with a distinctive leather cover that featured a black background with a white floral work. Jennings further customized it by filing down the frets to lower the strings on the neck to obtain the slapping sound. Among his other guitars, Jennings used a 1950 Fender Broadcaster from the mid-1970s, until he gave it to guitarist Reggie Young in 1993. The leather covers of his guitars were carved by leather artist Terry Lankford.
His signature image was characterized by his long hair and beard, as well as his black hat and the black leather vest he wore during his appearances.
Personal life.
In 1969 Jennings met and married Jessi Colter.[5] Jennings and Colter then moved to Nashville, Tennessee. Colter and Jennings had one son, Waylon Albright "Shooter" Jennings (born May 19, 1979). In the early 1980s, Colter and Jennings nearly divorced due to his addiction to drugs and other forms of substance abuse.[6] However, they remained together until Jennings's death in 2002.
In 1997, he stopped touring to be close to his family. To set an example about the importance of education to his son Waylon Albright, Jennings earned a GED at age 52.
Addiction and recovery.
Jennings started to consume amphetamines while he lived with Johnny Cash during the mid-1960s. Jennings later stated, "Pills were the artificial energy on which Nashville ran around the clock." In 1977, Jennings was arrested by federal agents for conspiracy and possession of cocaine with intent to distribute. A private courier warned the Drug Enforcement Administration about the package sent to Jennings by a New York colleague that contained 27 grams of cocaine. The DEA and the police searched Jennings's recording studio. They found no evidence, because while they were waiting for a search warrant, Jennings disposed of the cocaine. The charges were later dropped and Jennings was released. The episode was recounted in Jennings's song "Don't You Think This Outlaw Bit's Done Got Outta Hand?"
During the early 1980s, his cocaine addiction intensified. Jennings claimed to have spent $1,500 daily to satisfy his addiction, draining his personal finances and leaving him bankrupt with debt of up to $2.5 million. Though he insisted on repaying the debt and did additional tours to earn the funds, his work became less focused and his tours deteriorated. Jennings decided to quit his addictions, leased a home in the Phoenix, Arizona, area and spent a month detoxing himself, intending to start using cocaine again in a more controlled fashion afterward. In 1984 he quit cocaine. Jennings claimed that his son Shooter Jennings was his main inspiration to quit permanently.
Illness and death.
Jennings's health had been deteriorating for years before his death. After quitting cocaine, he ended his habit of smoking six packs of cigarettes daily in 1988. The same year he underwent heart bypass surgery. By 2000 his diabetes worsened, and the pain reduced his mobility, forcing Jennings to end most touring. Later the same year he underwent surgery to improve his leg circulation. In December 2001 his left foot was amputated at a hospital in Phoenix, Arizona. On February 13, 2002, Jennings died in his sleep of diabetic complications in Chandler, Arizona. He was buried in the Mesa City Cemetery, in Mesa, Arizona. At the funeral ceremony, on February 15, Jessi Colter sang "Storms Never Last" for the attendees, who included Jennings's close friends and fellow musicians.
Recognition.
Between 1966 and 1995, 54 Jennings albums charted, with 11 reaching number one. Meanwhile between 1965 and 1991, 96 singles charted, with 16 number ones.
In October 2001, Jennings was inducted into the Country Music Hall of Fame. In one final act of defiance, he did not attend the ceremony and opted instead to send son Buddy Dean Jennings.
On July 6, 2006, Jennings was inducted to Hollywood's Rock Wall in Hollywood, California. On June 20, 2007, Jennings was posthumously awarded the Cliffie Stone Pioneer Award by the Academy of Country Music.
Legacy.
Jennings's music had a major influence on several neotraditionalist and alternative country artists, including Hank Williams Jr., The Marshall Tucker Band, Travis Tritt, Steve Earle, Jamey Johnson, John Anderson, his son, Shooter Jennings and Hank Williams III.
In 2008, his first posthumous album, "Waylon Forever", was released. The album consisted of songs recorded with his son Shooter when he was 16.
In 2012, ' a three-volume project, consisting in covers of Jennings's songs by different artists was released. The same year, it was announced for September the release of ', a set of 12 songs recorded by Jennings and bassist Robby Turner before his death in 2002. Jennings's family was reluctant to release any new material because they did not feel comfortable at the time. The songs only featured Jennings and Turner on the bass, while further accompaniment would be added later. Ten years after, Turner completed the recordings with the help of former Waylors. The Jennings family approved the release despite the launch of a new business focused on his estate. Shooter Jennings arranged deals for a clothing line, while also launching a renewed website, and started talks with different producers about the making of a biographical film.
References.
Bibliography.
</dl>

</doc>
<doc id="39087" url="http://en.wikipedia.org/wiki?curid=39087" title="Black tie">
Black tie

Black tie is a dress code of formal wear for evening events and social functions derived from British and American costume conventions of the 19th century. Worn only for events after 6 p.m., black tie is semi-formal, i.e. less formal than white tie but more formal than informal or business dress. It is also more formal than recent intermediate codes of “creative,” “alternate” or “optional” black tie.
Gentlemen's standard.
For men, the elements of black tie are: 
Ladies' standard.
Women's dress for black tie occasions has varied greatly through the years; traditionally it was:
Today women's dress for black tie occasions is:
History.
When the dinner jacket ("tuxedo" in American English) first came into fashion in the Victorian era, it was used as a less formal alternative for the tailcoat which men of the upper classes wore every evening. Thus it was worn with the standard accompaniments for the evening tailcoat at the time: matching trousers, white or black waistcoat, white bow tie, white detachable wing-collar formal shirt and black formal shoes. Lapels were often faced or edged in silk or satin in varying widths. Dinner jackets were considered from the first less formal than full dress (cutaway) and etiquette guides declared it inappropriate for wear in mixed company.
During the Edwardian era, the practice of wearing a black waistcoat and black bow tie with a dinner jacket became the convention, establishing the basis of the current black tie and white tie dress codes. The dinner jacket was also increasingly accepted at less formal evening occasions such as warm-weather gatherings or intimate dinners with friends.
After World War I, the dinner jacket became de facto evening wear, while the evening tailcoat was limited to extremely formal or ceremonial occasions. During this interwar period, double-breasted jackets, turndown-collar shirts and cummerbunds became popular for black-tie evenings as did white and colored jackets in warm weather.
In the decades following World War II, black tie became special occasion attire rather than standard evening wear. In the 1950s, colored and patterned jackets, cummerbunds and bow ties and narrow lapels became very popular; the 1960s and 1970s saw the color palette move from muted to bright day-glow and pastel, as well as ruffled-placket shirts as lapels got wider and piping was revived. The 1980s and 1990s saw a return to nostalgic styles, with black jackets and trousers again becoming nearly universal. In the 2000s (decade), midnight blue once again became popular, lapel facings were sometimes reduced to wide edging and long ties were often substituted for the iconic bow tie. Black or colored shirts were more frequently worn.
The elements of black tie.
Unlike white tie, which is very strictly regulated, black-tie ensembles can display more variation. In brief, the traditional components for men are:
Jacket.
The typical black-tie jacket is single-breasted, and black or midnight blue; usually of wool or a wool–mohair, or wool-polyester blend, although other materials, especially silk, are seen. Double breasted models are less common, but considered equally appropriate. Dinner jackets were commonly ventless before World War I, but today come ventless, with side vents, or with center vents. The ventless style is considered more formal. The lapels are usually faced with silk in either a grosgrain or a satin weave. The buttons should be covered in similarly coloured material to the main part of the jacket.
Emily Post, a resident of Tuxedo Park, New York, stated in 1909 that "[Tuxedos] can have lapels or be shawl-shaped, in either case they are to have facings of silk, satin or grosgrain." She later republished this statement in her 1922 book "Etiquette", adding that only single-breasted jackets are appropriately called "tuxedos". There is a fashion movement suggesting that a man's appearance when wearing the wider and higher peak lapel is superior to the narrower notch lapel.
White dinner jackets are often worn in warm climates. They are ivory in color rather than pure white, and have self-faced lapels (i.e., made of the same fabric as the jacket) rather than silk-faced lapels. They are generally worn with the same types of shirts and accessories as black dinner jackets, though the turndown collar and cummerbund preferred to the wing collar or waistcoat. Similarly, the shawl lapel is more common in white dinner jackets. In the United Kingdom, the 20th-century etiquette was that white dinner jackets are never worn, even on the hottest day of summer, but are reserved for wear abroad. Today, white dinner jackets are frequently seen at weddings, formal beach events, and high-school proms, in the United States and at some concerts (famously for instance the Last night of the proms) in the United Kingdom. In tropical climates, such as in Imperial Burma, desert fawn was historically used as the less formal color. At one time, the (civilian) mess jacket was also an option in warmer climates.
It is generally considered inappropriate for a man to remove his jacket during a formal social event, but when hot weather and humidity dictate, the ranking man (of the royal family, the guest of honor) may give men permission by noticeably taking off his jacket. In anticipated hot weather, Red Sea rig is specified in the invitation, although this dress is esoteric in civilian circles, and is particular to certain expatriate communities.
Trousers.
Black tie trousers traditionally have no cuffs (turn-ups in British English) or belt loops. The outer seams are usually decorated with a single braid of silk or a material that matches the lapel facing. Traditionally, braces (suspenders), hidden by the waistcoat, were used to support the pants but belts are often worn today in less formal settings. Evening trousers can be flat-fronted or pleated today; pleats first coming into fashion in the 1930s.
Waistcoat or cummerbund.
A waistcoat ("vest" in American English) or cummerbund should be worn when wearing a single-breasted coat. However, according to Debretts, "cummerbands or low cut black evening waistcoats are rarely worn nowadays". Waistcoasts come in the 'V' or rarer 'U' shape, in backless or fully backed versions, double or single breasted, with or without lapels. Single breasted styles typically have three buttons, and double breasted ones three or four rows. Before World War II, while black tie was still gaining acceptance, men would wear a white waistcoat, along with other details now associated primarily with white tie, such as stiff fronted shirts. However, this style, though increasingly viewed as an affectation, is still acceptable in the United States.
The cummerbund, derived from military dress uniform in British India, is worn with its pleats facing up, and is normally of the same cloth as the bow tie and lapels though strictly, the cummerbund, bow tie and lapels should be of different material. Maroon, a color commonly worn to accompany black tie, is often used for the cummerbund in less formal or summer situations. A cummerbund is never worn with a double breasted jacket, and a waistcoat now very rarely. Since this style of jacket is never unbuttoned, the waist of the trousers is never exposed, and therefore does not need to be covered, though before World War II an edge of waistcoat was often shown between the jacket and shirt.
Recently, and particularly in the United States, it has become more common for men to remove their jackets at less formal events such as weddings and proms. Because of this, full-back waistcoats have become more common; unlike the traditional waistcoat, these are often high, single breasted, and with the full five or six buttons of a daytime waistcoat.
Shirt.
Shirts designed to be worn with black tie are called "formal shirts," or "tuxedo shirts" in American English and "dress shirts" in British English. The shirt is conventionally white or off-white cotton or linen with a bibbed front that is either marcella or pleated. In the early 20th century, a stiff front such as is worn with white tie was sometimes used and in the 1960s and 1970s ruffled bibs were popular, but neither style is often seen today. Indeed, a trend in the 2000s (decade) has been to dispense with the bib front altogether. Shirts worn with black-tie have double (or "french") cuffs. In the late 19th century and through the 1920s, stiff shirts with winged detachable collars were common. Thereafter, collars for formal shirts were typically attached with either the standard fold-over collar, or the "winged" version of the standing collar.
The original and most formal version of the dress shirt fastens with matching shirt studs. Dress shirts with a fly-front placket are also traditional. However, lately it has become quite common to allow buttons to show, although this is less formal. Studs and links are most commonly in silver or gold settings, featuring onyx or mother-of-pearl; various geometrical shapes are worn, e.g., circles (most common for studs), octagons, or rectangles (most common for links). There has been no consistent fashion preference for gold or silver, but mother-of-pearl is more formal and therefore often associated with white tie.
Footwear.
The most formal and traditional shoes are patent leather opera pumps (court shoes) decorated with grosgrain bows. The more popular alternative currently is the black lace-up Oxford shoe, in patent leather or calfskin, with a rounded plain toe. Matte finish pumps are also seen. Shoes are almost invariably black and patent leather is considered more formal than matte finishes while pumps are considered more formal than lace-ups. Generally considered too informal for black tie are shoes with open lacing, such as the Derby shoe ("bluchers" in American English). Notable alternatives include the black button boot (primarily of historical interest only) and the monogrammed Albert slipper which was originally worn only at home. The black Gucci loafer in leather is also considered as an alternative, especially in urban British settings. Hosiery is black socks made from fine wool or silk.
Accessories.
Most etiquette and fashion guides of the current decade recommend keeping color touches and favoring a single color, usually dark; muted reds, such as maroon, are a traditional choice.
Handkerchief: A handkerchief in linen (traditional), silk, or cotton is usually worn in the breast pocket.
Boutonnière: A flower may be worn. Red and white carnation, blue cornflower, and rosebud have all been popular at times. In France, the boutonnière is usually a gardenia, and boutonnières and handkerchiefs are not worn simultaneously.
Outerwear: Black-tie events do not involve outerwear and coats and gloves are no longer considered part of the dress code. However, etiquette for what to wear in public in transit to and from black tie occasions was stiffer in earlier eras and remain an option: Matching overcoats are usually black, charcoal, or dark blue, and traditionally of the Chesterfield style. A guards coat was also once popular, and a lighter topcoat can be worn in summer. Historically, an Inverness coat was also worn. Until the mid-20th century, gloves and scarves were always worn, and are still occasionally seen in gray leather and white silk, respectively. White kid gloves have never been standard with black tie, remaining exclusive to white tie dress.
Hat: The 20th-century standard hat for black tie was a black (or midnight blue) Homburg in winter, or straw boater in spring and summer. Fedoras were originally regarded as too informal but have become more common recently. Top hats were originally worn with black-tie, but had been reserved to white tie and morning dress from World War I. Black-tie dress does not require a hat today.
Timepiece: Traditionally visible timepieces are not worn with formal evening dress, because timekeeping is not supposed to be considered a priority. Pocket watches are acceptable.
Decorations and orders: Military, civil, and organizational decorations are usually worn only to full dress events, generally of formal governmental or diplomatic significance. Miniature orders and awards are typically worn on the left lapel of the jacket, and neck badges, breast stars, and sashes are worn according to country-specific or organizational regulations. Unlike in white tie, where decorations are always permitted, the dress code will usually give some indication when decorations are to be worn with black tie.
Black-tie social occasions.
Black tie is worn to private and public dinners, balls, and parties. At the more formal end of the social spectrum, it has to a large extent replaced the more formal white tie. The black tie code is sometimes classified as "semi-formal" in contrast to the "formal" white tie, or as "formal" in contrast to the "most formal" of white tie. Once more common, white tie dress code is now fairly rare, being reserved for only extremely formal occasions. Black tie is traditionally worn only after six o'clock in the evening, or after sundown during winter months. Black tie's rough daytime equivalent is the stroller, which is less formal than morning dress because (as with black tie) it replaces the tailcoat with a lounge coat. Curiously, in opposition to the trend seen in evening dress, the less formal stroller is now extraordinarily rare, whereas morning dress is still relatively common.
Opera.
Traditionally black tie should be worn to the opera although a dark lounge suit is also now acceptable. In the 21st century, many opera houses in the English-speaking world do not stipulate black tie. For example neither the Royal Opera House nor the Sydney Opera House have a black tie dress code. English country house opera, such as at Glyndebourne, is more likely to require black tie.
Black tie should also be worn at a ballet or orchestra gala.
Cruise ships.
At formal dinners on cruise ships the dress code will typically be black tie although a dark lounge suite may be worn as a substitute. In 2013 Cunard, noted for its adherence to formal dress codes, relaxed its dress standards. As of 2015 Cunard requires one of a dinner jacket, a dark suit, formal national dress or military uniform for gentlemen diners on formal evenings.
Black tie at weddings.
In the last few decades, black tie has been increasingly seen in the United States at formal day wedding in place of the traditional Morning dress. However, etiquette and clothing experts continue to discourage or condemn the wearing of black tie before 6 pm. Prior to the late 1930s, black tie was even discouraged as too informal for evening weddings, with Amy Vanderbilt arguing that "no man should ever be caught in a church in a tuxedo." Indeed Emily Post would continue to argue in preference of white tie at evening weddings into the 1950s.
In England and Wales, black tie is seldom worn at Church weddings or civil ceremonies as morning dress or a lounge suit is normally favoured. However, in recent years black tie is sometimes worn at evening receptions. At Jewish weddings, however, black tie is often stipulated. In Scotland, a dinner jacket is also not common but highland dress is often chosen.
Corresponding forms of dress.
Mess dress.
For formal dining, uniformed services officers and non-commissioned officers often wear mess dress equivalents to the civilian black tie and evening dress. Mess uniforms may vary according to the wearers' respective branches of the armed services, regiments, or corps, but usually include a short Eton-style coat reaching to the waist. Some include white shirts, black bow ties, and low-cut waistcoats, while others feature high collars that fasten around the neck and corresponding high-gorge waistcoats.
Red Sea Rig.
In tropical areas, primarily in Western diplomatic and expatriate communities, Red Sea rig is sometimes worn, in which the jacket and waistcoat are omitted and a red cummerbund and trousers with red piping are worn instead.
Scottish Highland dress.
Scottish Highland dress is often worn to black- and white-tie occasions, especially at Scottish reels and ceilidhs; the black-tie version is more common, even at white-tie occasions. Traditionally, black-tie Scots Highland dress comprises:
Traditional black-tie Lowland dress is a variant of the normal black tie that includes tartan trews rather than the usual trousers and may include a suitable kilt jacket instead of the dinner jacket. Trews are often worn in summer and warm climes.

</doc>
<doc id="39088" url="http://en.wikipedia.org/wiki?curid=39088" title="Richard Somers">
Richard Somers

Richard Somers (1778 or 1779–4 September 1804) was an officer of the United States Navy, killed during a daring assault on Tripoli.
Life.
Born at Great Egg Harbor, New Jersey, he attended school in Philadelphia with future naval heroes Stephen Decatur and Charles Stewart. He was appointed midshipman on April 23 1797 and served in the West Indies during the Quasi-War with France on the frigate "United States" with Decatur and Stewart, a ship commanded by Captain John Barry. He was promoted to lieutenant on May 21 1799.
In 1800, Somers fought three duels on the same day with multiple opponents because they accused him of cowardice for failing to challenge Decatur over a joking insult they overheard. Somers was wounded in the first two duels and had to be supported during the third (by Decatur, who was acting as his second). 
Somers was detached from "United States" on June 13 1801 and ordered to "Boston" on 30 July 1801. He served in the latter frigate in the Mediterranean. After "Boston" returned to Washington, DC, Somers was furloughed on November 11 1802 to await orders.
On May 5 1803, Somers was ordered to Baltimore, Maryland, to man, fit out, and command USS "Nautilus", and when that schooner was ready for sea, to sail her to the Mediterranean. "Nautilus" got underway on 30 June, reached Gibraltar on July 27, and sailed four days later to Spain. He then returned to Gibraltar to meet Commodore Edward Preble, in "Constitution", who was bringing a new squadron for action against the Barbary pirates. "Nautilus" sailed with Preble on October 6 to Tangier where the display of American naval strength induced the Europeans of Morocco to renew the treaty of 1786. Thereafter, Tripoli became the focus of Preble's attention.
Somers' service as commanding officer of "Nautilus" during operations against Tripoli won him promotion to Master Commandant on May 18 1804. In the summer, he commanded a division of gunboats during five attacks on Tripoli, during the First Barbary War.
On September 4, 1804, Somers assumed command of fire ship "Intrepid" which had been fitted out as a "floating volcano" to be sailed into Tripoli harbor and blown up in the midst of the corsair fleet close under the walls of the city. That night, she got underway into the harbor, but she exploded prematurely, killing Somers and his entire crew of volunteers.
Somers is buried in Tripoli, Libya. In 2004, the New Jersey state assembly passed two resolutions calling for the return of his remains. It is hoped that with the fall of Muammar Gaddafi's regime in Libya in August 2011 that the effort to repatriate the remains will finally be successful. 
Since 1804, six ships of the US Navy have successively been named the USS "Somers" in his honor.
The town of Somers, New York, located in Westchester County is named in his honor. Somers Point, New Jersey, is named after Richard's great-grandfather. Every year there is a Richard Somers Day celebration in Somers Point.

</doc>
<doc id="39089" url="http://en.wikipedia.org/wiki?curid=39089" title="USS Somers">
USS Somers

USS "Somers" may refer to:

</doc>
<doc id="39093" url="http://en.wikipedia.org/wiki?curid=39093" title="OS-9">
OS-9

OS-9 is a family of real-time, process-based, multitasking, multi-user, Unix-like operating systems, developed in the 1980s, originally by Microware Systems Corporation for the Motorola 6809 microprocessor. It was purchased by Radisys Corp in 2001. It is currently owned by Microware LP.
The OS-9 family was popular for general-purpose computing and remains in use in commercial embedded systems and amongst hobbyists. Today, OS-9 is a product name used by both a Motorola 68000-series machine language OS and a portable (PowerPC, x86, ARM, MIPS, SH4, etc.) version written in C, originally known as OS-9000.
History.
The first version ("OS-9 Level One"), which dates back to 1979–80, was written in assembly language for the Motorola 6809 CPU, and provided a single 64 KB address space in which all processes ran. It was developed as a supporting operating system for the BASIC09 project, contracted for by Motorola as part of the 6809 development. A later 6809 version ("Level Two") takes advantage of memory mapping hardware, supported up to 2 MB of memory (ca 1980) in most implementations, and included a GUI on some platforms.
In 1983, OS-9/6809 was ported to Motorola 68000 assembly language and extended (called OS-9/68K); and a still later (1989) version was rewritten mostly in C for further portability. The portable version was initially called OS-9000 and was released for 80386 PC systems around 1989, then ported to PowerPC around 1995. These later versions lack the memory mapping facilities of OS-9/6809 Level Two simply because they do not need them. They used a single flat address space that all processes share; memory mapping hardware, if present, is mostly used to ensure that processes access only that memory they have the right to access. The 680x0 and 80386 (and later) MPUs all directly support far more than 1 MB of memory in any case.
As a consequence of early pervasive design decisions taking advantage of the easily used reentrant object code capabilities of the 6809 processor, programs intended for OS-9 are required to be reentrant; compilers produce reentrant code automatically and assemblers for OS-9 offer considerable support for it. OS-9 also uses position independent code and data because the 6809 also supported it directly; compilers and assemblers supported position independence. The OS-9 kernel loads programs (including shared code), and allocates data, wherever sufficient free space is available in the memory map. This allows the entire OS and all applications to be placed in ROM or Flash memory, and eases memory management requirements when programs are loaded into RAM and run. Programs, device drivers, and I/O managers under OS-9 are all 'modules' and can be dynamically loaded and unloaded (subject to link counts) as needed.
OS-9/6809 ran on Motorola EXORbus systems using the Motorola 6809, SS-50 Bus and SS-50C bus systems from companies such as SWTPC, Tano, Gimix, Midwest Scientific, and Smoke Signal Broadcasting, STD-bus 6809 systems from several suppliers, personal computers such as the Fujitsu FM-11, FM-8, FM-7 and FM-77, Hitachi MB-S1, and many others.
System Industries, a third-party provider of DEC compatible equipment, used a 68B09E processor running OS9 in its QIC (quarter inch cartridge) tape backup controllers in VAX installations.
The best known hardware (due to its low price and broad distribution) was the TRS-80 Color Computer (CoCo) and the similar Dragon series. Even on the CoCo, a quite minimalist hardware platform, it was possible under OS-9/6809 Level One to have more than one interactive user running concurrently (for example, one on the console keyboard, another in the background, and perhaps a third interactively via a serial connection) as well as several other non-interactive processes. A second processor implementation for the BBC Micro was produced by Cumana. It included on-board RAM, SASI hard disk interface and a MC68008 processor.
On a computer like an SS-50, machines which had more memory (for example, those from Gimix, Southwest Technical Products, etc.), and I/O controllers that did not load the CPU as did the CoCo, multiple users were common, even with only 64 KB of RAM (i.e., Level One). With hardware supporting memory management circuits (that is, address translation) and OS-9 Level 2, GUI use was successfully routine, even on the minimal resourced CoCo. This was several years prior to successful GUIs on the 16-bit IBM PC class machines, and many years prior to properly working multi-tasking, multi-user, access-controlled operating systems on IBM PC type machines or on any of Apple's machines.
OS-9's multi-user and multi-tasking capabilities make it usable as a general-purpose interactive computer system. Many third-party interactive applications have been written for it, such as the Dynacalc spreadsheet, the VED text formatter, and the Stylograph and Screditor-3 WYSIWYG word processors. TSC's nroff emulating formatter was ported to OS-9 by MicroWay, as well.
In mid 1980s, OS-9 was selected for the CD-i operating system. Around the same time, Microsoft approached Microware for acquisition of the company primarily because it was attracted by CD-RTOS, the CD-i operating system. The negotiation failed and no deal was made; Microware decided to remain independent.
In late 1980s, Microware released OS-9000, a more portable version of the operating system. The vast majority of the operating system kernel was rewritten in C leaving a handful of hardware-dependent parts in assembly language. A few "more advanced features" were added such as tree-like kernel module name space. OS-9000 was initially ported to the Motorola 680x0 family CPUs, Intel 80386, and PowerPC. The OS-9000/680x0 was a marketing failure and withdrawn very quickly, probably because few customers wanted to try the fatter and slower operating system over the existing OS-9/680x0 proven record of stability. That the Motorola 680x0 family and VME board computer system vendors were nearing their end of life might have affected the unpopularity of OS-9000/680x0. Microware later started calling all of its operating systems — including what had been originally called OS-9000 — simply OS-9, and started shifting its business interest towards portable consumer device markets such as cellphones, car navigation, and multimedia.
In late 1980s and early 1990s, the Character Generators computers used in Broadcast Systems used OS-9 and OS-9000 extensively. The now defunct Pesa Electronica used OS-9 on their CGs such as CG 4722 and CG4733.
Name conflicts and court decisions.
In 1999, nineteen years after the first release of OS-9, Apple Computer released Mac OS 9. Microware sued Apple that year for trademark infringement, although a judge ruled that there would be little chance for confusion between the two. Some Macintosh users who are unaware of Microware's relatively unknown OS-9 have posted to the news://comp.os.os9 newsgroup not realising what OS-9 is.
In 2001, RadiSys purchased Microware to acquire the Intel IXP-1200 network processor resources. This acquisition infused Microware with capital and allowed Microware to continue OS-9 development and support.
On February 21, 2013, Microware LP (a partnership formed by Freestation of Japan, Microsys Electronics of Germany and RTSI LLC of the USA) announced that they signed an Asset Purchase Agreement to buy the rights to the names Microware, OS-9 and all assets from RadiSys.
Technology.
Modern and archaic design.
OS-9 (especially the 68K version and thereafter) clearly distinguishes itself from the prior generation of embedded operating systems in many aspects.
When compared with more modern operating systems.
Task scheduling.
OS-9’s real-time kernel allows multiple independent applications to execute simultaneously through task switching and inter-process communication facilities. All OS-9 programs run as processes containing at least one lightweight process (thread) but may contain an effectively unlimited number of threads. Within a process, these lightweight processes share memory, I/O paths, and other resources in accordance with the POSIX threads specification and API. OS-9 schedules the threads using a fixed-priority preemptive scheduling algorithm with round-robin scheduling within each priority. Time slicing is supported. The priority levels can be divided into a range that supports aging and a higher-priority range that uses strict priority scheduling. Each process can access any system resource by issuing the appropriate OS-9 service request. At every scheduling point, OS-9 compares the priority of the thread at the head of the active queue to the priority of the current thread. It context switches to the thread on the active queue if its priority is higher than the current processes’ priority. Aging artificially increases the effective priority of threads in the active queue as time passes. At defined intervals, time slicing returns the current thread to the active queue behind other threads at the same priority.
Comparisons with Unix.
OS-9's notion of processes and I/O paths is quite similar to that of Unix in nearly all respects, but there are some significant differences. Firstly, the file system is not a single tree, but instead is a forest with each tree corresponding to a device. Second, OS-9 does not have a Unix-style fork() system call—instead it has a system call which creates a process running a specified program, performing much the same function as a fork-exec or a spawn. Additionally, OS-9 processes keep track of two "current directories" rather than just one; the "current execution directory" is where it will by default look first to load programs to run (which is of course similar to the use of PATH environment variable under UNIX). The other is the current data directory.
Another difference is that in OS-9, grandparent directories can be indicated by repeating periods three or more times, without any intervening slashes. For example, codice_1 in OS-9, is similar to codice_2 in Unix. But codice_3 and codice_4, with just one or two periods, each work the same in both OS-9 and Unix.
OS-9 has had a modular design from the beginning, influenced by notions of the designers of the 6809 and how they expected software would be distributed in the future (see the three-part series of articles in Jan-Mar 1979 "Byte" by Terry Ritter, et al. of Motorola who designed the CPU).
OS-9/non-68000 supports POSIX threads. A single process can start any number of threads.
Trivia and Easter Eggs.
The OS-9 version 2.4 manual had this entry describing UNIX in the Glossary of Appendix C of "Using Professional OS-9":
 UNIX:
 An operating system similar to OS-9, but with less functionality and
 special features designed to soak up excess memory, disk space and CPU
 time on large, expensive computers.
This entry was removed in the version 3.0 manual.
The OS-9 shell had an easter egg in its command history function, invoked by CTRL-A. Upon a fresh boot, the command history was supposedly empty, but if the user typed a single space followed by a backspace, then hit CTRL-A, the names of the authors would be displayed: 'by K. Kaplan, L. Crane, R. Doggett'.
Status.
OS-9 has faded from popular use, though Microware LP does still support it and it does run on modern architectures such as ARM and x86. The compiler provided, Ultra C/C++, supports C89, but supports neither C99 nor C++98. Ultra C++ does provide limited support for C++ templates.

</doc>
<doc id="39094" url="http://en.wikipedia.org/wiki?curid=39094" title="Extension (metaphysics)">
Extension (metaphysics)

In metaphysics, extension is, roughly speaking, the property of "taking up space". René Descartes defines extension as the property of existing in more than one dimension. For Descartes, the primary characteristic of matter is extension, just as the primary characteristic of mind is consciousness. This can be contrasted with , where the Planck length, an almost unimaginably tiny quantity, represents reaching that distance scale where, it has been theorized, all measurement seemingly breaks down to that which can be subsumed at this scale, as distance only, or extension.
John Locke, in An Essay Concerning Human Understanding, defined extension as "only the Space that lies between the Extremities of those solid coherent Parts" of a body. It is the space possessed by a body. Locke refers to the extension in conjunction with "solidity" and "impenetrability," the other primary characteristics of matter.
Extension also plays an important part in the philosophy of Baruch Spinoza, who says that substance (that which has extension) can be limited only by substance of the same sort, i.e. matter cannot be limited by ideas and vice versa. From this principle, he determines that substance is infinite. This infinite substance is what Spinoza calls God, or better yet nature, and it possesses both unlimited extension and unlimited consciousness.
The property of extension has not played a significant role in philosophy roughly since the time of Immanuel Kant. Kant maintained a distinction between the mind and the body, differentiating space as the realm of the body and time the realm of the mind. He makes only cursory mention of "extension," however, and no philosophers have dealt extensively with the topic since Kant's writing.
Infinite divisibility.
"Infinite divisibility" refers to the idea that extension, or quantity, when divided and further divided infinitely, cannot reach the point of zero quantity. It can be divided into very small or negligible quantity but not zero or no quantity at all. Using a mathematical approach, specifically geometric models, Gottfried Leibniz and Descartes discussed the infinite divisibility of extension. Actual divisibility may be limited due to unavailability of cutting instruments, but its possibility of breaking into smaller pieces is infinite.
Compenetration.
"Compenetration" refers to two or more extensions occupying the same space at the same time. This, according to scholastic philosophers, is impossible; according to this view, only spirits or spiritualized matter can occupy a place occupied already by an entity (matter or spirit).

</doc>
<doc id="39095" url="http://en.wikipedia.org/wiki?curid=39095" title="Extension (semantics)">
Extension (semantics)

In any of several studies that treat the use of signs — for example, in linguistics, logic, mathematics, semantics, and semiotics — the extension of a concept, idea, or sign consists of the things to which it applies, in contrast with its comprehension or intension, which consists very roughly of the ideas, properties, or corresponding signs that are implied or suggested by the concept in question.
In philosophical semantics or the philosophy of language, the 'extension' of a concept or expression is the set of things it extends to, or applies to, if it is the sort of concept or expression that a single object by itself can satisfy. Concepts and expressions of this sort are monadic or "one-place" concepts and expressions.
So the extension of the word "dog" is the set of all (past, present and future) dogs in the world: the set includes Fido, Rover, Lassie, Rex, and so on. The extension of the phrase "Wikipedia reader" includes each person who has ever read Wikipedia, including you.
The extension of a whole statement, as opposed to a word or phrase, is defined (since Frege 1892) as its truth value. So the extension of "Lassie is famous" is the logical value 'true', since Lassie is famous.
Some concepts and expressions are such that they don't apply to objects individually, but rather serve to relate objects to objects. For example, the words "before" and "after" do not apply to objects individually — it makes no sense to say "Jim is before" or "Jim is after" — but to one thing in relation to another, as in "The wedding is before the reception" and "The reception is after the wedding". Such "relational" or "polyadic" ("many-place") concepts and expressions have, for their extension, the set of all sequences of objects that satisfy the concept or expression in question. So the extension of "before" is the set of all (ordered) pairs of objects such that the first one is before the second one.
Mathematics.
In mathematics, the 'extension' of a mathematical concept is the set that is specified by that concept. 
For example, the extension of a function is a set of ordered pairs that pair up the arguments and values of the function; in other words, the function's graph. The extension of an object in abstract algebra, such as a group, is the underlying set of the object. The extension of a set is the set itself. That a set can capture the notion of the extension of anything is the idea behind the axiom of extensionality in axiomatic set theory.
This kind of extension is used so constantly in contemporary mathematics based on set theory that it can be called an implicit assumption.
Computer science.
In computer science, some database textbooks use the term 'intension' to refer to the schema of a database, and 'extension' to refer to particular instances of a database.
Metaphysical implications.
There is an ongoing controversy in metaphysics about whether or not there are, in addition to actual, existing things, non-actual or nonexistent things. If there are—if, for instance, there are possible but non-actual dogs (dogs of some non-actual but possible species, perhaps) or nonexistent beings (like Sherlock Holmes, perhaps), then these things might also figure in the extensions of various concepts and expressions. If not, only existing, actual things can be in the extension of a concept or expression. Note that "actual" may not mean the same as "existing". Perhaps there exist things that are merely possible, but not actual. (Maybe they exist in other universes, and these universes are other "possible worlds"--possible alternatives to the actual world.) Perhaps some actual things are nonexistent. (Sherlock Holmes seems to be an "actual" example of a fictional character; one might think there are many other characters Arthur Conan Doyle "might" have invented, though he actually invented Holmes.)
A similar problem arises for objects that no longer exist. The extension of the term "Socrates", for example, seems to be a (currently) non-existent object. Free logic is one attempt to avoid some of these problems.
General semantics.
Some fundamental formulations in the field of general semantics rely heavily on a valuation of extension over intension. See for example extension, and the 

</doc>
<doc id="39096" url="http://en.wikipedia.org/wiki?curid=39096" title="Mary Martin">
Mary Martin

Mary Virginia Martin (December 1, 1913 – November 3, 1990) was an American actress, singer and Broadway star. A muse of Rodgers and Hammerstein, she originated many leading roles over her career including Nellie Forbush in "South Pacific" and Maria von Trapp in "The Sound of Music". She was named a Kennedy Center Honoree in 1989. She was also the mother of actor Larry Hagman.
Early life.
Martin was born in Weatherford, Texas. Her life as a child, as she describes it in her autobiography "My Heart Belongs", was secure and happy. She had close relationships with both her mother and father, as well as her siblings. Her autobiography details how the young actress had an instinctive ear for recreating musical sounds.
Martin's father, Preston Martin, was a lawyer, and her mother, Juanita Presley, was a violin teacher. Although the doctors told Juanita that she would risk her life if she attempted to have another baby, she was determined to have a boy. Instead, she had Mary, who became quite a tomboy. Her birth was an event as all of the neighbors gathered around Juanita's bedroom window, waiting for the raising of a curtain to signal the baby’s arrival.
Her family had a barn and orchard that kept her entertained. She played with her older sister Geraldine (whom she called “Sister”), climbing trees and riding ponies. Martin adored her father. “He was tall, good-looking, silver-haired, with the kindest brown eyes. Mother was the disciplinarian, but it was Daddy who could turn me into an angel with just one look” (p. 19). Martin, who said “I’d never understand the law” (p. 19), began singing outside the courtroom where her father worked every Saturday night at a bandstand where the town band played. She sang in a trio of little girls dressed in bellhop uniforms. “Even in those days without microphones, my high piping voice carried all over the square. I have always thought that I inherited my carrying voice from my father” (p. 19).
She remembered having a photographic memory as a child, making it easy to memorize songs, as well as get her through school tests. She got her first taste of singing solo at a fire hall, where she soaked up the crowd’s appreciation. “Sometimes I think that I cheated my own family and my closest friends by giving to audiences so much of the love I might have kept for them. But that’s the way I was made; I truly don't think I could help it” (p. 20). Martin’s craft was developed by seeing movies and becoming a mimic. She’d win prizes for looking, acting and dancing like Ruby Keeler and singing exactly like Bing Crosby. “Never, never, never can I say I had a frustrating childhood. It was all joy. Mother used to say she never had seen such a happy child—that I awakened each morning with a smile. I don’t remember that, but I do remember that I never wanted to go to bed, to go to sleep, for fear I’d miss something” (p. 20).
Marriage.
During high school, Martin dated Benjamin Hagman, before she was packed off to finishing school at Ward-Belmont in Nashville, Tennessee. During that time, she enjoyed imitating Fanny Brice at singing gigs, but she found school dull and felt confined by its strict rules. She was homesick for Weatherford, her family, and Hagman. During a visit, Mary and Benjamin persuaded Mary's mother to allow them to marry. They did, and by the age of 17, Martin was legally married, pregnant with her first child (Larry Hagman) and forced to leave Ward-Belmont. She was, however, happy to begin her new life. But she soon learned that this life, as she would later say, was nothing but “role playing” (p. 39).
Their honeymoon was at her parents' house, and Martin's dream of life with a family and a white-picket fence faded. “I was 17, a married woman without real responsibilities, miserable about my mixed-up emotions, afraid there was something awfully wrong with me because I didn’t enjoy being a wife. Worst of all, I didn't have enough to do” (p. 39). It was “Sister” who came to her rescue, suggesting that she should teach dance. “Sister” taught Martin her first real dance—the waltz clog. Martin perfectly imitated her first dance move, and she opened a dance studio. Here, she created her own moves, imitated the famous dancers she watched in the movies, and taught “Sister’s” waltz clog. As she later recalled, “I was doing something I wanted to do—creating” (p. 44).
Apprenticeship.
Wanting to learn more moves, Martin went to California to attend the dance school at the Franchon and Marco School of the Theatre, and opened her own dance studio in Mineral Wells, Texas. She was given a ballroom studio with the premise that she would sing in the lobby every Saturday. There, she learned how to sing into a microphone and how to phrase blues songs. One day at work, she accidentally walked into the wrong room where auditions were being held. They asked her what key she’d like to sing “So Red Rose”. Having absolutely no idea what her key was, she sang regardless and got the job. She was hired to sing “So Red Rose” at the Fox Theater in San Francisco, followed by the Paramount Theater in Los Angeles. There would be one catch—she had to sing in the wings. She scored her first professional gig, unaware that she would soon be center stage.
Soon after, Martin learned that her studio had been burnt down by a man who thought dancing was a sin. She began to express her unhappiness. Her father gave her advice, saying that she was too young to be married. Martin left everything behind, including her young son, Larry, and went to Hollywood while her father handled the divorce for her. In Hollywood, Martin plunged herself into auditions—so many that she became known as “Audition Mary”. Her first professional audition and job was on a national radio network. Among Martin's first auditions in Hollywood, Martin sang, 'Indian Love Call'". After singing the song, “a tall, craggly man who looked like a mountain” told Martin that he thought she had something special. It was Oscar Hammerstein II (pp. 58–59). This marked the start of her career.
Radio.
Martin began her radio career in 1939 as the vocalist on a short-lived revival of "The Tuesday Night Party" on CBS. In 1940, she was a singer on NBC's "Good News of 1940", which was renamed "Maxwell House Coffee Time" during that year. In 1942, she joined the cast of Kraft Music Hall on NBC, replacing Connie Boswell. She was also one of the starts of "Stage Door Canteen" on CBS, 1942-1945.
Broadway.
Mary Martin struggled for nearly two years to break into show business. As a struggling young actress, Martin endured humorous and sometimes frightful luck trying to make it in the world, from car crashes leading to vocal instruction, unknowingly singing in front of Oscar Hammerstein II, to her final break on Broadway granted by the very prominent producer, Lawrence Schwab.
Using her maiden name, Mary Martin began pursuing a performing career singing on radio in Dallas and in nightclubs in Los Angeles. Her performance at one club impressed a theatrical producer, and he cast her in a play in New York, but that production did not open.
She was then cast in Cole Porter's "Leave It to Me!", making her Broadway debut in November 1938. In that production, she became popular on Broadway and received attention in the national media singing "My Heart Belongs to Daddy". With that one song in the second act, she became a star 'overnight'." Martin reprised the song in "Night and Day," a Hollywood film about Cole Porter, in which she played herself auditioning for Porter (Cary Grant). "My Heart Belongs to Daddy" catapulted her career and became very special to Martin—she even sang it to her ailing father in his hospital bed while he was in a coma. Martin did not learn immediately that her father had died. Headlines read "Daddy Girl Sings About Daddy as Daddy Dies." Because of the show’s demanding schedule, Martin was unable to attend her father’s funeral.
She appeared on Broadway in "South Pacific", opening on April 7, 1949 as nurse Nellie Forbush. Her performance was called "memorable...funny and poignant in turns", and she earned a Tony Award. Richard Watts, Jr. of the "New York Post" wrote: "nothing I have ever seen her do prepared me for the loveliness, humor, gift for joyous characterization, and sheer lovableness of her portrayal of Nellie Forbush... Hers is a completely irresistible performance." She opened in the West End production on November 1, 1951. Her next major success was in the role of Peter in the Broadway production of "Peter Pan" in October 1954, with Martin winning the Tony Award. Martin opened on Broadway in "The Sound of Music" as Maria on November 16, 1959, and stayed in the show until October 1961. She won the Tony Award for Best Actress in a Musical. The musical gave Martin "the chance to display her homespun charm." In 1966 she appeared on Broadway in the two-person musical "I Do! I Do!" with Robert Preston and was nominated for the Tony Award (Leading Actress in a Musical). A national tour with Preston began in March 1968 but was cancelled early due to Martin's illness.
Although she appeared in nine films in her career, all between 1938 and 1943, she was generally passed over for the filmed version of the musical plays in which she starred. She herself once explained that she did not enjoy making films, because she did not have the "connection" with an audience that she had in live performances. The closest she ever came to preserving her stage performances were her famous television appearances as "Peter Pan". The Broadway production from 1954 was subsequently performed on NBC television in RCA's compatible color in 1955, 1956 and 1960. Martin also preserved her 1957 stage performance as Annie Oakley in "Annie Get Your Gun" when NBC television broadcast the production live that year.
While Martin did not enjoy making theatrical films, she apparently did enjoy appearing on television, as she did frequently. Her last feature film appearance was a cameo as herself in MGM's "Main Street to Broadway" in 1953. Martin made an appearance in 1980 in a Royal Variety Performance in London, performing "Honeybun" from "South Pacific." Martin appeared in the play "Legends" with Carol Channing in a one-year US national tour, opening in Dallas on January 9, 1986.
Awards and honors.
Mary Martin was inducted into the American Theater Hall of Fame in 1973.
She received the Kennedy Center Honors, an annual honor for career achievements, in 1989.
She received the Donaldson Award and the New York Film Critics Circle Award in 1943 for "One Touch of Venus". A special Tony was presented to her in 1948 while she appeared in the national touring company of "Annie Get Your Gun" for "spreading theatre to the rest of the country while the originals perform in New York." In 1955 and 1956, she received, first, a Tony Award for "Peter Pan", and then an Emmy for appearing in the same role on television. She also received Tony Awards for "South Pacific", and, in 1959, for "The Sound of Music".
Personal.
Cultural scholar Lillian Faderman has written that Martin and actress Janet Gaynor often traveled together during the course of a long-term lesbian relationship.
While living in San Francisco in 1982 she was involved in a traffic accident that left her with two fractured ribs, a fractured pelvis, and a punctured lung. Also in the accident were Janet Gaynor, who died two years later from complications from her injuries, Gaynor's husband Paul Gregory, who survived, and Martin's press agent Ben Washer, who died in the accident.
Death.
Mary Martin died a month before her 77th birthday from colorectal cancer at her home in Rancho Mirage, California on November 3, 1990. She is buried in City Greenwood Cemetery in Weatherford, Texas.

</doc>
<doc id="39098" url="http://en.wikipedia.org/wiki?curid=39098" title="Physical law">
Physical law

A physical law or scientific law "is a theoretical principle deduced from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present." Physical laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. The production of a summary description of our environment in the form of such laws is a fundamental aim of science. These terms are not used the same way by all authors.
The distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from "physis", the Greek word (translated into Latin as "natura") for "nature".
Description.
Several general properties of physical laws have been identified (see Davies (1992) and Feynman (1965) as noted, although each of the characterizations are not necessarily original to them). Physical laws are:
Physical laws are distinguished from scientific theories by their simplicity. Scientific theories are generally more complex than laws; they have many component parts, and are more likely to be changed as the body of available experimental data and analysis develops. This is because a physical law is a summary observation of strictly empirical matters, whereas a theory is a model that accounts for the observation, explains it, relates it to other observations, and makes testable predictions based upon it. Simply stated, while a law notes "that" something happens, a theory explains "why" and "how" something happens.
Examples.
Some of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his "Philosophiae Naturalis Principia Mathematica", and in Albert Einstein's theory of relativity. Other examples of laws of nature include Boyle's law of gases, conservation laws, the four laws of thermodynamics, etc.
Laws as definitions.
Some "scientific laws" appear to be mathematical definitions (e.g., Newton's Second law "F" = dp⁄dt, or the uncertainty principle, or the principle of least action, or causality). While these "scientific laws" explain what our senses perceive, they are still empirical and, thus, they are not "mathematical" facts. (Reference to a "law" often suggests a "fact", although "facts" do not exist scientifically "a priori".)
Laws being consequences of mathematical symmetries.
Other laws reflect mathematical symmetries found in Nature (say, Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, Lorentz transformations reflect rotational symmetry of space–time). Laws are constantly being checked experimentally to higher and higher degrees of precision. This is one of the main goals of science. The fact that laws have never been seen to be violated does not preclude testing them at increased accuracy or new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence; should any be seen. However, fundamental changes to the laws are extremely unlikely, since this would imply a change to experimental facts they were derived from in the first place.
Well-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies can be said to generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations (see below), to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g., very large or very small scales of time or space, enormous speeds or masses, etc. Thus, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations.
Laws as approximations.
Some laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to Quantum Electrodynamics at large distances (compared to the range of weak interactions). In such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws.
Physical laws derived from symmetry principles.
Many fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other), while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different than any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. The rotational symmetry between time and space coordinate axes (when one is taken as imaginary, another as real) results in Lorentz transformations which in turn result in special relativity theory. Symmetry between inertial and gravitational mass results in general relativity.
The inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space.
One strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions.
History.
Compared to pre-modern accounts of causality, laws of nature fill the role played by divine causality on the one hand, and accounts such as Plato's theory of forms on the other.
The observation that there are underlying regularities in nature dates to prehistoric times, since the recognition of cause-and-effect relationships is an implicit recognition that there are laws of nature. The recognition of such regularities as independent scientific laws "per se", though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as meteorological, astronomical and biological phenomena—to the actions of various gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality.
In Europe, systematic theorizing about nature ("physis") began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount.The formula "law of nature" first appears as "a live metaphor" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative, the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture.
For the Romans . . . the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's "Natural Questions", and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof, we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself.
The precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and development of advanced form of mathematics.. The modern scientific method which took shape at this time (with Francis Bacon and Galileo) aimed at total separation of science from theology, with minimal speculation about metaphysics and ethics. Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period (by Grotius, Spinoza, and Hobbes, to name a few).
Other fields.
Some mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws.
Examples of other observed phenomena sometimes described as laws include the Titius-Bode law of planetary positions, Zipf's law of linguistics, Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as "laws". These include Occam's razor as a principle of philosophy and the Pareto principle of economics.

</doc>
<doc id="39102" url="http://en.wikipedia.org/wiki?curid=39102" title="Kidderminster">
Kidderminster

Kidderminster is a town in the Wyre Forest district of Worcestershire, England. It is located approximately 17 mi south-west of Birmingham city centre and approximately 15 mi north of Worcester city centre. The 2001 census recorded a population of 55,182 in the town. The town is twinned with Husum, Germany and it forms the majority of the Wyre Forest Conurbation, an urban area of 99,000.
History.
The land around Kidderminster may have been first populated by the Husmerae, an Anglo-Saxon tribe first mentioned in the Ismere Diploma, a document in which Ethelbald of Mercia granted a "parcel of land of ten hides" to Cyneberht. This became the settlement of Stour-in-Usmere, which was later the subject of a territorial dispute settled by Offa of Mercia in 781, where he restored certain rights to Bishop Heathored. This allowed for the creation of a monastery or "minstre" in the area, and the earliest written form of the name Kidderminster (Chedeminstre) was not seen until it appeared in the "Domesday Book" of 1086. It was a large manor held by William I with 16 outlying settlements (Bristitune, Fastochesfeld, Franche, Habberley, Hurcott, Mitton, Oldington, Ribbesford, Sudwale, Sutton, Teulesberge, Trimpley, Wannerton and Wribbenhall). Various spellings were in use – Kedeleministre or Kideministre (in the 12th and 13th centuries), Kidereministre (13th–15th centuries) – until the name of the town was settled as Kidderminster by the 16th century. Between 1156 and 1162 Henry II granted the manor to his steward, Manasser Biset, and as the settlement grew a fair (1228) and later a market (1240) were established there. In a visit to the town sometime around 1540, King's Antiquary John Leland noted that Kidderminster "standeth most by clothing". King Charles I granted the Borough of Kidderminster a Charter in 1636. the original charter can be viewed at Kidderminster Town Hall
A parliamentary report of 1777 listed Kidderminster Borough as having a parish workhouse accommodating up to 70 inmates. Under the so-called Gilbert's Act of 1782 Kidderminster Union was established for the purpose of relieving the indigent poor.
Kidderminster has two Commissioners' churches. The first was St. George's church, on Radford Avenue. This was designed by Francis Goodwin and built in 1821–1824, finally being consecrated in April 1824. It had the third largest grant by the Commission, of just over £17,000.00, of any church outside London. The second church was St. John's Church, on the Bewdley Road. This church was built in 1843 and the architect was Matthew Steele, although the grant in this case was just over £4,000. To the south by the river Stour, dating from the 15th century, is a single surviving tower of Caldwall (or Caldwell) Castle, a fortified manor house.
Geography.
The River Stour and the Staffordshire and Worcestershire Canal both flow through Kidderminster town centre.
Economy.
The modern carpet industry was founded in the area in 1785 by Brintons, and the carpet industry became extremely important to the local economy, so much so that the local newspaper is still named "The Shuttle" after the shuttles used on the carpet looms. By 1951 there were over thirty carpet manufacturers in the town, including, for example Quayle & Tranter (now defunct) who commissioned notable artists including George Bain for their traditional designs. 
Aided by a 2004 grant from the Heritage Lottery Fund, a museum dedicated to the Kidderminster carpet industry was officially opened by Lord Cobham in 2012.
The Wyre was the town's first local commercial radio station and began broadcasting on 12 September 2005 from studios in Kidderminster. Other radio stations providing local coverage are Free Radio, Sunshine Radio, and BBC Hereford & Worcester. The Wyre ceased broadcasting in 2012 and Signal 107 was launched on 26 March 2012.
Politics.
Kidderminster is an unparished area within Wyre Forest District, but Charter Trustees maintain the traditions of the town and elect a Mayor. As of the last election in 2014 Wyre Forest District Council currently has no party with a majority on the council. The area (initially as Kidderminster, then after 1983 as the Wyre Forest constituency) has been represented by Conservative MPs Gerald Nabarro 1950–63, Sir Tatton Brinton 1964–74, Esmond Bulmer 1974–87, Anthony Coombs 1987–97, and Labour MP David Lock 1997–2001. In the United Kingdom general election, 2001, the town returned Dr Richard Taylor as an independent MP for the Wyre Forest parliamentary constituency. Taylor had fought the election to protest against the proposed reduction in services at Kidderminster Hospital. He held his seat at the 2005 election, the first independent MP to do so since 1949.
In the May 2010 General Election Taylor lost to Conservative candidate Mark Garnier.
Architecture and landmarks.
In the 1968 Buildings of England volume on Worcestershire, Pevsner described the town as; "uncommonly devoid of visual pleasure and architectural interest." In the 2007 revision, Alan Brooks provides a reassessment, writing; "the 19th century mill buildings, together with the churches, provide most of the architectural interest in a town otherwise uncommonly lacking in visual pleasures."
Transport.
Two railway stations in the town share the same approach road and are located less than fifty metres apart. The main Network Rail station operated by London Midland is Kidderminster, from where trains run to Birmingham, Worcester and London. The other station, Kidderminster Town, is the terminus of the preserved Heritage Railway line, Severn Valley Railway from where trains run to Bridgnorth.
Several major routes run through the town including the A456 which runs from Birmingham to Woofferton, Shropshire, a few miles south of Woofferton, the A451 which runs from Stourbridge to Abberley, the A442 which runs from Droitwich to Hodnet, Shropshire, a few miles north of Telford, the A449 road which runs from Newport in south Wales to Stafford and crosses the A456 at the Land Oak, and the A448 road which starts in the town and goes to Bromsgrove. A major change in the town centre road infrastructure was the construction of the ring road in the 1970s and 1980s, which relieved the town's growing congestion problem. Unusually, the final phase of the ring road was never completed which results in it having a ring road that does not form a complete ring.
The Staffordshire and Worcestershire Canal passes through the town.
There are direct bus links with towns including Worcester, Halesowen, Bewdley, Stourport, Bridgnorth, Bromsgrove and Redditch. The majority of the services in Kidderminster are operated by Diamond Bus (previously First Midland Red), with the rest operated by Whittle's.
Education.
As part of educational restructuring in the Wyre Forest district, Kidderminster's schools were reorganised from a three-tier system of first, middle and high schools to the two-tier system more common in the UK as a whole with primary schools and secondary schools. Several first and middle schools were closed or merged into new primaries, with the three high schools of King Charles I School, Wolverley C E Secondary School, and Baxter College (formerly Harry Cheshire High School) becoming secondary schools with sixth forms. Independent schools include Heathfield School in Wolverley. Formerly independent, Holy Trinity School became a state-funded free school in 2014. Kidderminster College is located in Market Street in the town centre, having moved from older premises in Hoo Road in 2003. Other local secondary schools include The Stourport High School & VIth Form Centre, and The Bewdley School and Sixth Form Centre.
Sport.
Cricket.
Kidderminster Victoria CC is a local cricket club.
Football.
Formed in 1886, Kidderminster Harriers F.C. is the town's professional football club. Local rivals of the Harriers were traditionally Worcester City and Bromsgrove Rovers, and in recent years also Cheltenham Town and Hereford United, although as of 2013 Cheltenham are in a division above Kidderminster. In 2005 the Harriers were relegated to the Conference Premier after five years in the Football League Two division. They had reached the Football League as Conference champions in 2000, and remain as Worcestershire's only-ever representative in the league. They had won the title in 1994 but were denied promotion then as their stadium did not meet Football League capacity requirements – this came the same year that they eliminated Birmingham City from the FA Cup.
The Kidderminster & District League has operated since 1984 and draws teams from Worcestershire and South Staffordshire.
Rugby.
Kidderminster Carolians RFC is a local rugby union club, currently playing in Midlands Division 2 West Northern Section.
Field hockey.
Kidderminster Hockey club was founded in 1892 and in 2010 there are five men's hockey teams, a ladies team and a junior team.
External links.
Radio stations in Kidderminster:

</doc>
<doc id="39104" url="http://en.wikipedia.org/wiki?curid=39104" title="Chinese food therapy">
Chinese food therapy

Chinese food therapy (, also called nutrition therapy and dietary therapy) is a mode of dieting rooted in Chinese understandings of the effects of food on the human organism, and centred on concepts such as eating in moderation. Its basic precepts are a mix of folk views and concepts drawn from traditional Chinese medicine. It was the prescientific analog of modern medical nutrition therapy; that is, it was a state-of-the-art version of dietary therapy before the sciences of biology and chemistry allowed the discovery of present physiological knowledge. It now qualifies as alternative medicine.
Food therapy has long been a common approach to health among Chinese people both in China and overseas, and was popularized for western readers in the 1990s with the publication of books like "The Tao of Healthy Eating" (= ) and "The Wisdom of the Chinese Kitchen" (= )., which also cites , , and 
Origins.
A number of ancient Chinese cookbooks and treatises on food (now lost) display an early Chinese interest in food, but no known focus on its medical value. The literature on "nourishing life" ("yangsheng" 養生) integrated advice on food within broader advice on how to attain immortality. Such books, however, are only precursors of "dietary therapy", because they did not systematically describe the effect of individual food items.
The earliest extant Chinese dietary text is a chapter of Sun Simiao's "Prescriptions Worth a Thousand Gold" ("Qianjin Fang" 千金方), which was completed in the 650s during the Tang dynasty. Sun's work contains the earliest known use of the term "food (or dietary) therapy" ("shiliao"). Sun stated that he wanted to present current knowledge about food so that people would first turn to food rather than drugs when suffering from an ailment. His chapter contains 154 entries divided into four sections – on fruits, vegetables, cereals, and meat – in which Sun explains the properties of individual foodstuffs with concepts borrowed from the "Yellow Emperor's Inner Canon": "qi", the viscera, vital essence ("jing" 精), and correspondences between the Five Phases, the "five flavors" (sour, bitter, sweet, pungent, and salty), and the five grains. He also set a large number of "dietary interdictions" ("shijin" 食禁), some based on calendrical notions (no water chestnuts in the 7th month), others on purported interactions between foods (no clear wine with horse meat) or between different flavors.
Sun Simiao's disciple Meng Shen (孟詵; 621–713) compiled the first work entirely devoted to the therapeutic value of food: the "Materia Dietetica" ("Shiliao bencao" 食療本草; lit., "food therapy "materia medica""). This work has not survived, but it is quoted in later texts – like the 10th-century Japanese text Ishinpō – and a fragment of it has been found among the Dunhuang manuscripts. Surviving excerpts show that Meng gave less importance to dietary prohibitions than Sun, and that he provided information on how to prepare foodstuffs rather than just describe their properties. The works of Sun Simiao and Meng Shen established the genre of "materia dietetica" and shaped its development in the following centuries.
Precepts.
Although the precepts of Chinese food therapy are neither systematic nor identical in all times and places, some basic concepts can be isolated. Food items are classified as "heating" ("re" 熱; "hot") or "cooling" ("liang" 涼; "cool"). Heating food is typically "high-calorie, subjected to high heat in cooking, spicy or bitter, or 'hot' in color (red, orange)", and includes red meat, innards, baked and deep-fried goods, and alcohol. They are to be avoided in the summer and can be used to treat "cold" illnesses like excessive pallor, watery feces, fatigue, chills, and low body temperature caused by a number of possible causes, including anemia. Green vegetables are the most typical cooling food, which is "low-calorie, watery, soothing or sour in taste, or 'cool' in color (whitish, green)". They are recommended for "hot" conditions: rashes, dryness or redness of skin, heartburns, and other "symptoms similar to those of a burn", but also sore throat, swollen gums, and constipation.

</doc>
<doc id="39105" url="http://en.wikipedia.org/wiki?curid=39105" title="Boehm system">
Boehm system

The Boehm system is a system of keywork for the flute, created by inventor and flautist Theobald Boehm between 1831 and 1847. 
Prior to the development of the Boehm system, flutes were most commonly made of wood, with an inverse conical bore, eight keys, and tone holes (the openings where the fingers are placed to produce specific notes) that were small in size, and thus easily covered by the fingertips. Boehm's work was inspired by an 1831 concert in London given by soloist Charles Nicholson who, with his father in the 1820s, had introduced a flute constructed with larger tone holes than were used in previous designs. This large-holed instrument could produce greater volume of sound than other flutes, and Boehm set out to produce his own large-holed design.
In addition to large holes, Boehm provided his flute with "full venting", meaning that all keys were normally open (previously, several keys were normally closed, and opened only when the key was operated). Boehm also wanted to locate tone holes at acoustically optimum points on the body of the instrument, rather than locations conveniently covered by the player's fingers. To achieve these goals, Boehm adapted a system of axle-mounted keys with a series of "open rings" (called "brille", German for "eyeglasses", as they resembled the type of eyeglass frames common during the 19th century) that were fitted around other tone holes, such that the closure of one tone hole by a finger would also close a key placed over a second hole.
In 1832 Boehm introduced a new conical-bore flute, which achieved a fair degree of success. Boehm, however, continued to look for ways to improve the instrument. Finding that an increased volume of air produced a stronger and clearer tone, he replaced the conical bore with a cylindrical bore, finding that a parabolic contraction of the bore near the embouchure hole improved the instrument's low register. He also found that optimum tone was produced when the tone holes were too large to be covered by the fingertips, and he developed a system of finger plates to cover the holes. These new flutes were at first made of silver, although Boehm later produced wooden versions. 
The cylindrical Boehm flute was introduced in 1847, with the instrument gradually being adopted almost universally by professional and amateur players in Europe and around the world during the second half of the 19th century. The instrument was adopted for the performance of orchestral and chamber music, opera and theater, wind ensembles (e.g., military and civic bands), and most other music which might be loosely described as relating to "Western classical music" (including, for example, jazz). Many further refinements have been made, and countless design variations are common among flutes today (the "offset G" key, addition of the low B foot, etc.) The concepts of the Boehm system have been applied across the range of flutes available, including piccolos, alto flutes, bass flutes, and so on, as well as other wind instruments. The material of the instrument may vary (many piccolos are made of wood, some very large flutes are wooden or even made of PVC).
The flute is perhaps the oldest musical instrument, other than the human voice itself. There are very many flutes, both traversely blown and end-blown "fipple" flutes, currently produced which are not built on the Boehm model.
The fingering system for the saxophone closely resembles the Boehm system. A key system inspired by Boehm's for the clarinet family also is known as "Boehm system", although it was developed by Hyacinthe Klosé and not Boehm himself. The Boehm system was also adapted for a small number of flageolets. Boehm did work on a system for the bassoon, and Boehm-inspired oboes have been made, but non-Boehm systems remain predominant for these instruments. The Albert system is another key system for the clarinet.

</doc>
<doc id="39106" url="http://en.wikipedia.org/wiki?curid=39106" title="Tandy 1000">
Tandy 1000

The Tandy 1000 was the first in a line of more-or-less IBM PC compatible home computer systems produced by the Tandy Corporation for sale in its RadioShack chain of stores.
Overview.
In December 1983 an executive with Tandy Corporation, maker of TRS-80 computers, said about the new IBM PCjr home computer: "I'm sure a lot of people will be coming out with PCjr look-alikes. The market is big."
Released in November 1984, the $1,200 Tandy 1000 was designed as an inexpensive PC compatible with enhancements compatible with the PCjr, but with a better keyboard. "How could IBM have made that mistake with the PCjr?" an amazed Tandy executive said regarding its chiclet keyboard, and another claimed that the 1000 "is what the PCjr should have been".
Although the press saw the computer as the former personal-computer leader admitting that it could no longer focus on proprietary products in a market the IBM PC dominated, the 1000 sold more units in the first month than any other Tandy product and by early 1985 was Tandy's best-selling computer. Although it came in an IBM PC-like desktop case, the 1000 was followed by a series of models which used home computer-style cases with the keyboard, motherboard and disk drives in one enclosure. These models appended two or three letters to the name, after a space (e.g. Tandy 1000 EX, Tandy 1000 HX, Tandy 1000 SX, Tandy 1000 TX, Tandy 1000 RL, Tandy 1000 RLX). In a few instances, after these letters a slash was appended, followed by either a number or additional letters (e.g. Tandy 1000 TL/2, Tandy 1000 RL/HD).
The 1000 included joystick ports like the PCjr, and copied its 16-color graphics and 3-voice sound, but not the PCjr ROM cartridge ports. Since IBM discontinued the PCjr soon after the 1000's release, Tandy quickly removed mentions of the PCjr in its advertising while emphasizing the 1000's PC compatibility. The machine and its many successors were successful unlike the PCjr, partly because the Tandy 1000 was sold in ubiquitous Radio Shack stores and partly because it was less costly, easier to expand, and almost-entirely compatible with the IBM PC. The PCjr's enhanced graphics and sound standards became known as "Tandy-compatible". With its graphics, sound, and built-in joystick ports, the 1000 was the best computer for PC games until VGA graphics became popular in the 1990s. Software companies of the era advertised their support for the Tandy platform; 28 of 66 games that "Computer Gaming World" tested in 1989 supported Tandy graphics.
Design and architecture.
Tandy 1000 computers were some of the first IBM PC clones to incorporate a complete set of basic peripherals on the motherboard using proprietary ASICs, the forerunner of the chipset. All Tandy 1000 computers featured built-in Tandy video hardware with color graphics (CGA compatible with enhancements), enhanced sound (based on one of several variants of the Texas Instruments SN76496 sound generator), game ports compatible with those on the TRS-80 Color Computer, an IBM-standard floppy disk controller supporting two drives, and a parallel printer port, all integrated into the motherboard. This is in addition to the hardware standard on the IBM PC, PC/XT, and PC/AT motherboards: keyboard interface, expansion slots, memory subsystem, DMA, interrupt controller, and math coprocessor socket. (Hard disks were high end, not standard, equipment for home computers until the late years of the Tandy 1000 line, explaining the absence of an integrated hard disk controller from most Tandy 1000 motherboards.) An IBM PC, XT or AT would require at least 4 expansion cards for similar hardware: one video graphics adapter card, one floppy disk controller (FDC) card, one serial and parallel port card, and one sound card with a joystick port. (A third-party multi-IO card might merge the ports and FDC onto one card.) Therefore, the 5 XT slots of the original Tandy 1000, 1000 TX, 1000 SX, and similar models remained available for other hardware, making them equivalent or better than the 8 slots in IBM's XT and AT models (which had 8 slots because the original PC's 5 proved inadequate.)
The earlier models of the Tandy 1000 had a composite video output, and could be used with a color or monochrome composite monitor, or a TV with an RF modulator. The original 1000 and SX had a light-pen port. Unlike most PC clones, several Tandy 1000 computers had MS-DOS built into ROM and could boot in a few seconds. Tandy bundled DeskMate, a graphical suite of consumer-oriented applications, with various Tandy 1000 models. 
The original line was equipped with the Intel 8088 CPU, which was later extended to faster clock speeds and also the 8086, 80286 and toward the end of the line with the RSX, 80386SX processors. Tandy 1000s (at least all early models) used Phoenix BIOS. Common models of the machine included the Tandy 1000, 1000 EX, 1000 HX, 1000 SX, 1000 TX, 1000 SL, 1000 RL, and 1000 TL.
Hard disk drives.
Tandy 1000 computers did not feature integrated hard disk controllers until the release of the Tandy 1000 TL/2, which featured an on-board XT IDE controller. At this time hard disks were very expensive and needed only by high-performance users. However, it was possible to add a hard drive to most Tandy 1000 computers. Most of the desktop-type Tandy 1000 units could accept regular 8-bit ISA bus MFM, RLL and SCSI controllers like typical XT class machines; however, care had to be taken when configuring the cards so that they do not cause conflicts with the onboard Tandy-designed peripherals.
For most Tandy 1000 models other than the compact EX and HX that did not come already equipped with a hard drive, Tandy offered hard disk options in the form of hardcards that were installed in one of the computer's expansion slots and consisted of a controller and drive (typically a 3.5" MFM or RLL unit with a Western Digital controller) mounted together on a metal bracket. Although this arrangement provided a neat physical coupling between the controller and the disk, single-sector internal transfers and dependence on the speed of the host machine to transfer data to memory meant that a trial-and-error approach was still needed to set the disk interleave correctly to ensure optimum transfer rates. Even then, transfer rates could be as low as 40kB/s for 8088 and 8086 machines.
Starting with the Tandy 1000 TL/2, XT IDE controllers were integrated onto the motherboard. However, these are unable to support common AT IDE hard drives. The TL/2, TL/3, RL and RLX all used the XT IDE interface, where the later (and significantly upgraded) RSX was the first and only Tandy 1000 model computer to use a standard AT IDE interface.
Retirement.
By 1993, changes in the market made it increasingly difficult for Tandy Corporation to make a profit on its computer line. Tandy Corporation sold its computer manufacturing business to AST Computers, and all Tandy computer lines were terminated. RadioShack stores then began selling computers made by other manufacturers, such as Compaq.
Reception.
"Creative Computing" called the original Tandy 1000 "the machine IBM was too inept, incapable, or afraid to manufacture. It is sure to put a whopping dent not only into PCjr sales, but into sales of the PC 'senior' as well", favorably mentioning its low price, good PC-software compatibility, and bundled DeskMate ("you might never need another software package for your computer"). "InfoWorld" noted the 1000's low price ("fully one-third less than a comparably equipped IBM PC"), predicted that the computer was really intended for "the elusive home computer market", and speculated that "in retrospect it might have been the PCjr's final straw". The magazine called the 1000 "almost as fully IBM PC compatible as a computer can get", but gave DeskMate a mixed review and advised customers of the computer's inability to use full-length PC expansion cards. It concluded that "By making the 1000 inexpensive and adaptable" and including DeskMate, "Tandy produced a real home computer". "BYTE" called the 1000 "a good, reasonably priced IBM PC clone that has most of the best features of the IBM PC and PCjr ... at current prices it is a very good alternative". It noted the high level of software compatibility and the good keyboard, and stated that DeskMate was "fairly good ... but a little extra programming work could have turned [it] into a much better program", noting that—for example—the word processor did not have a Move command. The magazine also mentioned the computer's short slots. "PC Magazine" also noted the slots and criticized the Tandy 1000's fit and finish, but acknowledged the computer's low price and bundled hardware features.
Selected Tandy 1000 Models.
Tandy 1000.
The original Tandy 1000 was a large computer almost the size of the IBM PC, though with a plastic case over an aluminium lower chassis to reduce weight. The original Tandy 1000 featured a proprietary keyboard port (using an 8-pin DIN connector) along with 2 joystick ports (using 6-pin DIN connectors) on the front of the case. The rear featured an RGB monitor connector (a standard 9-pin female D-shell compatible with CGA/EGA monitors), an RCA-style composite video-out connector, a single RCA-style monophonic line-level audio connector, a light pen port, and an edge-card connector used to attach a parallel printer. The printer port followed the old Centronics standard and was not fully compatible with the parallel port found on PCs. The original Tandy 1000 came standard with one internal 5.25" double density floppy disk drive, with an additional exposed internal bay usable for the installation of a second 5.25" disk drive (available as a kit from RadioShack). The floppy drives used the old-fashioned method of selecting the drive number with jumpers instead of the IBM cable twist. 128 kB of memory was standard, with the computer accepting up to 640 kB of total memory with the addition of expansion cards.
MS-DOS 2.11, DeskMate 1.0, and a keyboard with the same layout as the Tandy 2000's were included with the computer. Like the PCjr. DMA was not supplied on the motherboard, but unlike the IBM system, DMA was added by a memory expansion board. While the Tandy 1000 had three XT-compatible expansion slots, early Tandy memory upgrade boards took up two of the slots to get to 640 kB. Because the slots were 11 1/2 inches in length instead of the PC's 13 inches, full-length cards did not fit, but reviewers noted that the many built-in hardware features reduced the need for cards.
A later revision of the original Tandy 1000 model was the Tandy 1000A. This revision fixed bugs, scanned expansion cards for bootable ROMs, and added a socket for a math coprocessor.
Tandy 1000 HD.
The original Tandy 1000 (and many other models), like most home computers sold at the time, did not have a hard disk drive. The Tandy 1000 HD was essentially an original Tandy 1000 with a hard disk option factory installed. The factory hard disk had a capacity on the order of 10 or 20 MB.
Tandy 1000 EX & SX.
The Tandy 1000 EX was designed as an entry-level IBM compatible personal computer. The EX was a compact computer that had the keyboard and 5.25" floppy drive built into the computer casing. The 5.25" drive was accessible on the right-hand side of the computer. The EX was marketed as a starter system for people new to computing, and sold for US $1000.00 from RadioShack in December 1986. The EX and later the HX would be among the most popular of the Tandy 1000 line because of their (relatively) low price. The EX doubled the on-board memory to 256kB.
The EX had a 7.16 MHz 8088 (capable of clocking down to 4.77 MHz) and one internal 5.25" floppy drive. An external drive could be connected to a port on the back. A useful feature for the EX and later systems is the ability to boot off either drive, as the drives can be logically swapped when the system boots, so that the drive that is normally drive B: becomes drive A:, and vice versa, and the drives remain swapped until the system is powered off or reset.
The EX was upgradable by Tandy PLUS cards, and system had bays for three. The PLUS cards' connector was electrically identical to the ISA slot connector, but used a BERG-style 62-pin connector instead of a 62-contact ISA card edge connector. The RAM could be upgraded in the EX and later the HX to 640kB, but required a PLUS memory expansion card. This card also provided DMA. Other PLUS cards could be installed to add serial ports, a 1200 baud modem, a clock/calendar and bus mouse board and a proprietary Tandy network interface.
The Tandy 1000 SX was essentially an upgraded reissue of the original Tandy 1000 with the additional features of the EX. It used a 7.16 MHz 8088-2 processor, had 384k of memory (upgradeable to 640 kB on the motherboard), came with either one or two 5.25" internal floppy disk drives, had the light pen port (not a serial port) like the original Tandy 1000. Unlike the EX, it did not have a volume dial or headphone jack, but did have an adjustable potentiometer inside the system to control the volume of the internal speaker. The Tandy AX was a Tandy SX rebadged for sale in Wal-Mart stores. The SX/AX are drop-in compatible with NEC's V20 processor for a noticeable improvement in performance.
The 1000 EX came with MS-DOS 2.11 and Personal Deskmate on 5.25" 360 kB diskettes. The 1000 SX came with MS-DOS 3.2 and Deskmate II on 5.25" 360 kB diskettes. While Deskmate II used a text-based interface, Personal Deskmate used a graphical interface and also supported a mouse-like cursor using a joystick-mouse driver or a Tandy bus mouse. The MS-DOS was a version specialized for and only bootable on the Tandy 1000, as it would announce on the screen of any other PC-compatible one tried to boot with it; it included a version of BASICA (Microsoft's Advanced GW-BASIC) with support for the enhanced CGA graphics modes (a.k.a. Tandy Graphics or TGA) and three-voice sound hardware of the Tandy 1000.
The Tandy 1000 SX and later the TX were the first models in the Tandy 1000 line to have a built-in DMA controller. Adding the DMA chip improved the speed of diskette operations and IBM PC-compatibility of these earlier Tandy 1000 models and ensured that input from a serial port or keyboard would not be ignored during floppy drive access.
Tandy 1000 HX & TX.
The Tandy 1000 HX was an updated version of the EX. It was mostly the same machine, but had two 3.5" floppy bays instead of a 5.25" one, and came with a 720kB floppy drive. It also had Tandy MS-DOS 2.11R in ROM, which could be accessed by starting the computer with no bootable disk present. By putting the basic elements of DOS and Deskmate in ROM and eliminating the memory test on startup, the HX will boot very quickly compared to other contemporary Tandy machines. System settings changed with a setup program and were stored via an EEPROM, and the system could be set to boot a program from the A: drive, boot to DOS or Personal Deskmate 2. The HX uses the same CPU and clock speeds as the EX, uses the same PLUS cards, keyboard and has the same ports as the EX. It also came with 256kB of RAM on the motherboard. The HX's sound output has a distinctive grounding loop noise, which was clearly audible when the volume knob was turned up above 50%.
The 1000 EX and HX did not come with a hard drive, nor was it available from Tandy as an option, although a number of third party vendors sold them. The design of the EX and HX did not make it easy to add a hard drive, however. RadioShack eventually sold an adapter card that allowed the installation of a "Plus Card" into a standard ISA slot, such as those in the larger Tandy 1000 models. On the back of the machine there was a port which allowed a user to connect an external 360 kB 5.25" or 720 kB 3.5" floppy disk drive unit, available from Tandy. It is possible to fit an NEC V20 to a Tandy 1000 EX or HX; however, this may require disassembly of the computer. 
The 1000 HX came with MS-DOS 2.11, modified to support 720kB drives and Personal Deskmate 2. Most versions of MS-DOS worked with the 1000 HX, including DOS 3.x, and some later versions. There was a quirk in the DOS 4.0 environment that prevented that version of DOS from working with Tandy 1000 HX computers.
The Tandy 1000 TX was very similar to the Tandy 1000 SX, having an external keyboard and similar casing. The major difference was the use of an 80286 CPU; otherwise, it was nearly identical to the Tandy 1000 SX, including the unique parallel port edge connector. Despite the 80286 processor, it was still an XT-class PC, not an AT-class PC, as it adapted the 80286 to operate over the same 8-bit data bus as previous Tandy 1000 models, and had 8-bit XT-style expansion slots. As such, it could not operate in 80286 protected mode or perform 16-bit memory or I/O transfers in one bus cycle, but it did benefit from the higher speed of the 80286 and its other added instructions in real mode. The TX had a 3.5" internal floppy disk drive, with an optional additional internal 5.25" floppy disk drive. It contained ports for two joysticks in the front along with the keyboard, and included a volume control with a 1/8" headphone jack on the front. The back had all of the same ports as the Tandy 1000, except that the light pen port was replaced with an RS-232 serial port. The memory size was 640k (upgradable to 768 kB, with the added 128 kB devoted to video) and the computer came bundled with Personal DeskMate 2.
Tandy 1000 SL, SL/2, TL, TL/2, TL/3.
The Tandy SL and TL series of computers were updates of the SX and TX respectively. In addition to having a redesigned case and a more integrated motherboard, the SL and TL each offered improved video hardware capable of 640 × 200 × 16 graphics, on-board Hercules Graphics Card compatible monochrome video offering an effective 720 × 350 resolution, and improved sound capabilities featuring an 8-bit mono DAC/ADC. The composite video output was also dropped. The ADC/DAC, which became known as the "Tandy DAC" in games that supported it, was broadly similar in function to sound devices which connected to the parallel port (such as the Disney Sound Source), but unlike those devices it was integrated onto the motherboard, supported DMA transfers and could sample at frequencies up to 48 kHz. While the Tandy DAC's features were comparable to those offered by Creative's 8-bit Sound Blaster audio cards, unlike the Sound Blaster or the Tandy's PCjr-compatible audio the DAC never saw widespread adoption by software developers despite BIOS support. The 640 × 200 × 16 graphics mode was even more rarely used, as it was not supported by the BIOS. The SL/TL lines also allowed the onboard floppy controller, parallel port and serial ports to be disabled, which the earlier models did not.
The Tandy 1000 SL and SL/2 feature an Intel 8086 processor running at 8 MHz. The 8086 processor's 16-bit bus and slightly higher clock speed gave the SL series a modest, yet appreciable increase in performance over the 8088-based Tandy 1000 models. The CPU can be replaced with NEC's V30 processor for a further increase in performance. The SL came with 384 kB of RAM preinstalled, whereas the SL/2 offered 512 kB. Both machines can be expanded to 640 kB, although only 608 kB could typically be used by the operating system. The SL line have the mic/earphone ports, volume knob and reset button on a small satellite board, which has an LM386 op-amplifier, a microphone input IC and a few small passives. There is also a jumper on the board to change the microphone input to a line-level output.
The Tandy 1000 TL and TL/2 use 8 MHz Intel 80286 processors, whereas the TL/3 uses a 10 MHz 80286. These computers had 640 kilobytes of memory preinstalled, with an option for an extra 128 kilobytes to be installed for use as video memory for the onboard video hardware. This extra 128 kilobytes could only be used for and by the on-board video controller, and it is impractical to expand the onboard memory beyond 640 kB if a VGA graphics card is installed. Notably, the TL/3 had a high-density floppy controller for the first time, although it only shipped with a double-density 3.5" drive. Also, the TL/2 and TL/3 feature an on-board 8-bit XT IDE interface, which was not compatible with common AT IDE hard drives. Compared to the TL, which has five expansion slots, the TL/2 and TL/3 lose an expansion slot to accommodate the XT IDE interface. Unlike the SL line, the TL comes equipped with an on-board Dallas DS1216E Smartwatch real-time clock chip. The Smartwatch chip is powered by a removable, 3-volt CR2032 lithium button cell on the motherboard. Later models have the Smartwatch logic integrated into the motherboard chipset.
Since the SL and TL series are XT-class machines, it is impossible to install or use extended memory(XMS), although expanded memory (EMS) can be used with an 8-bit LIM EMS memory card for software that supports expanded memory.
The SL and TL were also shipped with MS-DOS 3.3 and DeskMate 3 in ROM, and featured an EEPROM memory chip to store BIOS settings (which enabled similar functionality to today's CMOS NVRAMs, so that startup options could be saved). (Earlier Tandy 1000 models, with the exception of the HX, like IBM PC and PC/XT systems, used DIP switches and jumpers for startup configuration settings.) The machines could also run 'normal' MS-DOS 3.x, 5, and 6 and Windows 2 and 3.0 operating systems, although Windows was limited to real mode operations. In common with many PC clones of the era, MS-DOS 4 was problematic and generally avoided.
The SL is the only machine in this lineup that offers an upper 5.25" bay (and therefore the only machine in the line to offer two 5.25" bays), which can be fitted with an adapter to seat 3.5" drives; the SL/2 and TL series feature at least one upper 3.5" bay and one lower 5.25" bay. Fitting an internal hard drive to an SL that already has two floppy drives requires either the removal of one of the floppy drives, or finding a hard disk card-style bracket to mount the drive in one of the ISA slots. Another, often less expensive option would be to use an XT-CF card fitted with a CompactFlash adapter and memory card; however, as most modern CompactFlash devices offer much higher capacity than the 32MB that DOS 3.3 supports per-partition, an upgrade to the DOS operating system (or multiple 32MB partitions and possibly a disk-drive overlay program) may be required for optimal operation.
The TL could fit a non-Tandy MFM/RLL half-height hard drive in the 5.25" bay or a full-height drive with case removal (not supported) with an 8-bit ISA controller such as a Western Digital WD-XT GEN2.
Tandy 1000 RL, RL/HD, RLX, RSX.
The Tandy 1000 RL/RLX/RSX series were slim-line desktop home computers. The RL and RL/HD featured a 9.56 MHz 8086 processor, 512 kB of RAM (expandable to 768 kB to provide 128 kB for video and 640kB conventional memory), smaller keyboard and mouse ports (which were similar to the PS/2's ports but not 100% compatible), a DB-25 unidirectional parallel port instead of the edge-connector ports, and the SL's enhanced graphics and sound. Both the RL and RL/HD included a built-in XT IDE hard drive interface and the RL/HD came with a 20mB drive preinstalled. The RL/HD had a battery-backed real time clock chip to store Date & Time information with the RL lacked. These models also had MS-DOS and a portion of DeskMate in ROM, and could therefore boot much faster than many other computers on the market.
The RLX was the 'mid-range' offering of the RL line. It had a 10 MHz 286, and unlike other 286-based Tandy 1000s, it supported up to 384 kB of extended memory. However, it was not a full AT-class machine, as it still had an 8-bit ISA bus and only 8 IRQs and 4 DMA channels. While the 3-voice sound chip and DAC were still present, Tandy video was dropped. The RLX had VGA instead, offering 256 kB of video memory and a maximum 640x480x16 (or 320x480x256) graphics resolution. Also, the RLX featured a high-density, 1.44 MB 3.5" disk drive. The RLX offered 512 kB of memory preinstalled, which could be expanded to 1 MB. (The hard disk version came with 1 MB of RAM and a 20mB hard disk preinstalled.)
The RSX offered a 25 MHz 80386SX processor, two 16-bit ISA slots, AcuMos SVGA video, an AT compatible IDE interface and standard PS/2 keyboard and mouse ports. It was a full 386-class PC, and could use up to 9 MB of memory. The RSX still retained the 3-voice sound hardware and DAC, though the I/O address for the 3-voice sound chip was moved, rendering many games previously compatible with it unable to play music unless modified. The DAC could be used to emulate the Covox Speech Thing via MS-DOS device drivers for limited sound support in MS-DOS based software. This works with the game "Chuck Yeager's Air Combat".
Windows 3.xx sound device drivers were available that works in Windows 95 (with full 9MB RAM) on Tandy 1000 RSX.
The ACUMOS VGA graphics could be software updated with Cirrus Logic BIOS (via MS-DOS driver) to allow VESA/SVGA to function in Windows 95, as the Windows 3.xx Tandy VGA drivers were insufficient for Windows 95.

</doc>
<doc id="39108" url="http://en.wikipedia.org/wiki?curid=39108" title="McLean Hospital">
McLean Hospital

McLean Hospital (; also known as "Somerville Asylum" or "Charlestown Asylum") is a psychiatric hospital in Belmont, Massachusetts.
It is noted for its clinical staff expertise and ground-breaking neuroscience research. It is also known for the large number of famous people who have been treated there, including mathematician John Nash, singer-songwriters James Taylor and Ray Charles, poets Sylvia Plath, Robert Lowell, and Anne Sexton, and authors Susanna Kaysen and David Foster Wallace.
McLean maintains the world's largest neuroscientific and psychiatric research program in a private hospital. It is the largest psychiatric facility of Harvard Medical School, an affiliate of Massachusetts General Hospital and owned by Partners HealthCare, which also owns Brigham and Women's Hospital.
History.
McLean was founded in 1811 in a section of Charlestown, Massachusetts, that is now a part of neighboring Somerville, Massachusetts. Originally named Asylum for the Insane, it was the first institution organized by a cooperation of prominent Bostonians who were concerned about homeless mentally ill persons "abounding on the streets and by-ways in and about Boston." The effort was organized by Rev. John Bartlett, chaplain of the Boston Almshouse. The hospital was built around a Charles Bulfinch mansion, which became the hospital's administrative building; most of the other hospital buildings were completed by 1818. The institution was later given the name The McLean Asylum for the Insane in honor of one of its earliest benefactors, John McLean, who granted it enough money to build several such hospitals at the 1818 cost. A portrait of McLean now hangs in the present Administration Building, along with other paintings that were once displayed in the original hospital. In 1892, the facility was renamed McLean Hospital in recognition of broader views on the treatment of mental illness.
In 1895 the campus moved to Waverley Oaks Hill in Belmont, Massachusetts. The civil engineer Joseph Curtis and Frederick Law Olmsted, the renowned landscape architect who also conceptualized the Emerald Necklace public spaces of Boston, New York's Central Park, and Hartford's Institute of Living, were consulted on the selection of the hospital site. The move was necessitated by changes in Charlestown, including new rail lines and other distracting development. Olmsted was eventually treated at McLean, but there is no evidence that he was responsible for the design of the grounds. Once hospital construction began, Curtis was hired by the hospital and supervised the landscape work for many years.
In the 1990s, facing falling revenue in a changing health care industry, the hospital drafted a plan to sell a percentage of its grounds for development by the Town of Belmont. The sale of the land became the root of a divisive and somewhat baroque political debate in the town during the late 1990s. Ultimately a plan to preserve some of Olmsted's original open space and to allow the town to develop mixed residential and commercial real estate prevailed over a plan to create only high-end residential development. The deal was finalized in 2005 and land development was well underway at the end of the year. Most of its Belmont campus (more than 300 acre) were listed on the National Register of Historic Places in 2003.
McLean is known widely for its treatment of adolescents, most specifically its treatment of Borderline Personality Disorder using Dialectical Behavioral Therapy developed by Marsha M. Linehan.,
McLean is presently led by Scott L. Rauch, President and Psychiatrist in Chief, who is known for his innovative work using brain imaging methods to study psychiatric dysfunction.
McLean is differentiated from its New England peers (such as The Institute of Living and the Brattleboro Retreat) by its combination of teaching, treatment, and research. Most other facilities focus on one of these priorities. It is home to the Harvard Brain Tissue Resource Center, the largest "brain bank" in the world. The hospital developed and implemented national health screenings for alcohol, depression and memory disorders. The Cole Resource Center, a mental health consumer resource and advocacy center, is located at the hospital.
McLean Hospital is a teaching hospital for Harvard University Residents and the continued development of Harvard faculty.
Artistic works inspired by McLean.
One popular and anecdotal history of McLean is Alex Beam's "Gracefully Insane: Life and Death Inside America's Premier Mental Hospital" (ISBN 1-891620-75-4). More factual and scholarly accounts of the history are recorded in the Little and Sutton books listed below. Memoirs of time spent within McLean's walls include Sylvia Plath's novel "The Bell Jar" and Susanna Kaysen's "Girl, Interrupted" (ISBN 0-679-74604-8), which was made into a movie starring Winona Ryder and Angelina Jolie. Samuel Shem's roman à clef "Mount Misery" tells a story inspired at least in part by the author's experiences at McLean. The 1994 "Under Observation: Life Inside A Mental Hospital" (ISBN 0-14-025147-2, ISBN 0-395-63413-X) by Lisa Berger and Alexander Vuckovic uses some fictional techniques (composite characters, etc.) to describe some of the typical events at Mclean. James Taylor's "Knockin' 'Round the Zoo" recalls his stay at McLean as a teenager. Poems of Boston and Just Beyond: From the Back Bay to the Back Ward by Doug Holder (http://id.lib.harvard.edu/aleph/007991227/catalog )(Poems of McLean Hospital from poet Doug Holder, about his 30 plus years working there--archived at the poetry room at the Lamont Library at Harvard University)

</doc>
<doc id="39109" url="http://en.wikipedia.org/wiki?curid=39109" title="Ferdinand">
Ferdinand

Ferdinand is a Germanic name composed of the elements "frith" "protection" , frið "peace" (PIE "pri" to love, to make peace) or alternatively "farð" "journey, travel", Proto-Germanic *farthi, abstract noun from root *far- "to fare, travel" (PIE "par" "to lead, pass over"), and "nanth" "courage" or nand "ready, prepared" related to Old English neðan, Old High German nendan "to risk, venture."
The name was adopted in Romance languages from its use in the Visigothic Kingdom. It is reconstructed as either Gothic "Ferdinanths" or "Frithunanths".
It became popular in German-speaking Europe only from the 16th century, with Habsburg rule over Spain.
Variants of the name include "Fernán", "Fernando", "Hernando", and "Hernán" in Spanish, "Ferran" in Catalan, and "Fernando" and "Fernão" in Portuguese. 
The French forms are "Ferrand", "Fernand", and "Fernandel", and it is "Ferdinando" and "Fernando" in Italian. In Hungarian both "Ferdinánd" and "Nándor" are used equally.
There are numerous hypocorisms or short forms in many languages, such as Finnish version is Veeti.
There is a feminine Spanish, Portuguese and Italian form, "Fernanda".
Royalty and Nobility.
Italian States.
NAPLES, SICILY & TWO SICILIES
MANTUA & MONTFERRAT
PARMA
TUSCANY

</doc>
<doc id="39112" url="http://en.wikipedia.org/wiki?curid=39112" title="Road bicycle">
Road bicycle

The term road bicycle is used to describe bicycles built for traveling at speed on paved roads. Some sources use the term to mean racing bicycle. Other sources specifically exclude racing bicycles from the definition, using the term to mean a bicycle of a similar style but built more for endurance and less the fast bursts of speed desired in a racing bicycle; as such, they usually have more gear combinations and fewer hi-tech racing features. Certain of these bicycles have been referred to as 'sportive' bicycles to distinguish them from racing bicycles.
Compared to other styles of bicycle, road bicycles share common features:
The term "road bicycle" can also describe any type of bike used primarily on paved roads, in contrast to bikes primarily intended for off-road use, such as mountain bikes. Several variations of road bikes include:

</doc>
<doc id="39113" url="http://en.wikipedia.org/wiki?curid=39113" title="Tesla coil">
Tesla coil

A Tesla coil is an electrical resonant transformer circuit invented by Nikola Tesla around 1891. It is used to produce high-voltage, low-current, high frequency alternating-current electricity. Tesla experimented with a number of different configurations consisting of two, or sometimes three, coupled resonant electric circuits.
Tesla used these coils to conduct innovative experiments in electrical lighting, phosphorescence, X-ray generation, high frequency alternating current phenomena, electrotherapy, and the transmission of electrical energy without wires. Tesla coil circuits were used commercially in sparkgap radio transmitters for wireless telegraphy until the 1920s, and in medical equipment such as electrotherapy and violet ray devices. Today their main use is for entertainment and educational displays, although small coils are still used today as leak detectors for high vacuum systems.
Theory.
A Tesla coil is a radio frequency oscillator driving a double tuned resonant transformer. The Tesla coil operates in a significantly different fashion from a conventional iron-core transformer. In a conventional transformer, the windings are very tightly coupled (i.e., in close physical proximity to one another), and voltage gain is determined by the ratio of the numbers of turns in the windings. 
Unlike a conventional iron core transformer, which typically couple 97%+ of the magnetic fields between windings, the windings in a Tesla coil are more widely separated and the magnetic core is eliminated. The primary and secondary are "loosely coupled", typically sharing only 10–20% of their respective magnetic fields.
In addition, the inductance of each winding is part of a tuned LC circuit. The primary winding and primary ("tank") capacitor form the primary LC circuit. The secondary LC circuit consists of the secondary winding, and the combination of its stray capacitance and self-capacitance of the top terminal to ground. The primary and secondary LC circuits are tuned so that each circuit resonates at an identical high frequency. By loosely coupling the two LC circuits, energy in an oscillating primary LC circuit can be transferred to the secondary LC circuit (or vice versa) at a rate that depends on the coupling coefficient between windings.
Energy transfer between windings is significantly more complex than for a conventional iron core transformer. The primary circuit is typically driven into oscillation by discharging a fixed amount of energy from the charged primary capacitor into the primary winding through a spark gap). However, it takes a number of RF cycles for energy in the oscillating primary LC circuit to fully transfer into the secondary LC circuit. Energy in the primary circuit decreases it is transferred into the secondary circuit, until all of the system's initial energy, less losses, now resides in the oscillating secondary circuit. The lower the coupling coefficient, the greater the number of RF cycles required to complete an energy transfer from one LC circuit to the other. If the spark gap continues to conduct, energy in the oscillating secondary now begins to transfer back into the primary circuit until all the remaining system energy once again resides in the oscillating primary LC circuit many RF cycles later. This bidirectional energy transfer process, over multiple oscillations, is common behavior (called the Transient response) for a simple coupled resonant system.
As the primary's energy transfers to the secondary, the secondary's oscillating output voltage increases ("rings up") until all of the available primary energy (that was initially stored in the primary tank capacitor) has been transferred to the secondary (less losses, mainly from the spark gap switch). Even with significant spark gap losses, a well-designed Tesla coil can transfer over 85% of the energy initially stored in the primary capacitor to the secondary circuit. The voltage achievable from a Tesla coil can be significantly greater than a conventional low frequency transformer. Per Faraday's law of induction, the induced voltage per turn (or EMF) for the windings is proportional to the rate of change of magnetic flux. Operation at higher frequencies dramatically increases the volts-per-turn, and small-medium size Tesla coils typically develop EMF's of several hundreds of volts per turn.
The secondary of a Tesla coil is not simply a lumped element inductor. Its electrical behavior is considerably more complex due to the presence of distributed turn-to-turn capacitance and stray capacitance that is a function of the height of each turn above ground. It is more accurately described, using distributed modeling, as a helical resonator or a slow-wave helical resonator Unlike a simple LC circuit which resonates at a single frequency, a helical resonator resonates at a number of frequencies. Most Tesla coils operate the resonator at the lowest, or fundamental frequency, since this mode develops the highest voltage at the top of the resonator when the bottom of the resonator is grounded. This mode is sometimes is also called quarter-wave resonance, since the electrical length of the resonator is one-fourth of the slow-wave wavelength within the resonator when it is operating at its fundamental frequency. A resonator rings at many higher frequencies (or harmonics). However, operation at a harmonic is not usually done, since one or more high voltage nodes will be formed between the top and bottom of the resonator. When used in a spark gap Tesla coil, the resonator is operated at its fundamental mode, and its behavior can be approximated as a lumped inductor in parallel with the combination of its self-capacitance and topload capacitance. Similarly, when driven from a CW source at its fundamental frequency, the resonator can be approximated as a high-Q inductor in parallel with its self-capacitance and topload capacitance.
In a spark gap switched Tesla coil, voltage gain is proportional to the square root of the ratio of secondary and primary inductances. This is a direct consequence of Conservation of energy. Since the primary and secondary are tuned to the same frequency, LC (primary circuit) = LC (secondary circuit), so the voltage gain is also proportional to the square root of the ratio of the primary tank capacitance to the combination of secondary and topload stray capacitances to ground. In a Tesla Coil driven from a continuous RF source, such as some types of vacuum tube or solid state Tesla coils, voltage gain is proportional to the Q of the secondary coil.
History.
The original Tesla coil transformer employed a capacitor which, upon break-down of a short spark gap, became connected to a coil of a few turns (the primary winding set), forming a resonant circuit with the frequency of oscillation, usually 20–100 kHz, determined by the capacitance of the capacitor and the inductance of the coil. The capacitor was charged to the voltage necessary to rupture the air of the gap during the input line cycle, about 10 kV by a line-powered transformer connected across the gap. The line transformer was designed to have higher than normal leakage inductance to tolerate the short circuit occurring while the gap remained ionized, or for the few milliseconds until the high frequency current had died away.
The spark gap is set up so that its breakdown occurs at a voltage somewhat less than the peak output voltage of the transformer in order to maximize the voltage across the capacitor. The sudden current through the spark gap causes the primary resonant circuit to ring at its resonant frequency. The ringing primary winding magnetically couples energy into the secondary over several RF cycles, until all of the energy that was originally in the primary has been transferred to the secondary. Ideally, the gap would then stop conducting (quench), trapping all of the energy into the oscillating secondary circuit. Usually the gap reignites, and energy in the secondary transfers back to the primary circuit over several more RF cycles. Cycling of energy may repeat for several times until the spark gap finally quenches. Once the gap stops conducting, the transformer begins recharging the capacitor. Depending on the breakdown voltage of the spark gap, it may fire many times during a mains AC cycle.
A more prominent secondary winding, with vastly more turns of thinner wire than the primary, was positioned to intercept some of the magnetic field of the primary. The secondary was designed to have the same frequency of resonance as the primary using only the stray capacitance of the winding itself to ground and that of any "top hat" terminal placed at the top of the secondary. The lower end of the long secondary coil must be grounded to the surroundings.
The later and higher-power coil design has a single-layer primary and secondary. These Tesla coils are often used by hobbyists and at venues such as science museums to produce long sparks. The "American Electrician" gives a description of an early Tesla coil wherein a glass battery jar, 15 × 20 cm (6 × 8 in) is wound with 60 to 80 turns of AWG No. 18 B & S magnet wire (0.823 mm²). Into this is slipped a primary consisting of eight to ten turns of AWG No. 6 B & S wire (13.3 mm2) and the whole combination is immersed in a vessel containing linseed or mineral oil.
Magnifying transmitter.
Tesla built a laboratory in Colorado Springs and between 1899-1900 performed experiments on wireless power transmission there. The Colorado Springs laboratory possessed one of the largest Tesla coils ever built, which Tesla called a "magnifying transmitter" as it was intended to transmit power to a distant receiver. With an input power of 300 kilowatts it could produce potentials in the 12 to 20 megavolt range at a frequency of 150 kHz, creating huge 140 foot "lightning" bolts. The magnifying transmitter design is somewhat different from the classic two-coil Tesla coil circuit. In addition to the primary and secondary coils it had a third "resonator" coil, not magnetically coupled to the others, attached to the top terminal of the secondary. When driven by the secondary it produced additional high voltage by resonance, being adjusted to resonate with its own parasitic capacitance at the frequency of the other coils.
The Colorado Springs apparatus consisted of a 53-foot diameter Tesla coil around the periphery of the lab, with a single-turn primary buried in the ground and a secondary of 50 turns of heavy wire on a 9 foot high circular "fence". The primary was connected to a bank of oil capacitors to make a tuned circuit, excited by a rotary spark gap at 20 - 40 kilovolts from a powerful utility transformer. The top of the secondary was connected to a 20 ft diameter "resonator" coil in the center of the room, attached to a telescoping 143 foot "antenna" with a 30-inch metal ball on top which could project through the roof of the lab.
Wardenclyffe coil.
Tesla's 1902 design for his advanced magnifying transmitter used a top terminal consisting of a metal frame in the shape of a toroid, covered with hemispherical plates (constituting a very large conducting surface). The top terminal has relatively small capacitance, charged to as high a voltage as practicable. The outer surface of the elevated conductor is where the electrical voltage chiefly occurs. It had a large radius of curvature, or was composed of separate elements which, irrespective of their own radii of curvature, were arranged close to each other so that the outside ideal surface enveloping them has a large radius. This design allowed the terminal to support very high voltages without generating corona or sparks. Tesla, during his patent application process, described a variety of resonator terminals at the top of this later coil.
Modern-day Tesla coils.
Modern high-voltage enthusiasts usually build Tesla coils similar to some of Tesla's "later" 2-coil air-core designs. These typically consist of a primary tank circuit, a series LC (inductance-capacitance) circuit composed of a high-voltage capacitor, spark gap and primary coil, and the secondary LC circuit, a series-resonant circuit consisting of the secondary coil plus a terminal capacitance or "top load". In Tesla's more advanced (magnifier) design, a third coil is added. The secondary LC circuit is composed of a tightly coupled air-core transformer secondary coil driving the bottom of a separate third coil helical resonator. Modern 2-coil systems use a single secondary coil. The top of the secondary is then connected to a topload terminal, which forms one 'plate' of a capacitor, the other 'plate' being the earth (or "ground"). The primary LC circuit is tuned so that it resonates at the same frequency as the secondary LC circuit. The primary and secondary coils are magnetically coupled, creating a dual-tuned resonant air-core transformer. Earlier oil-insulated Tesla coils needed large and long insulators at their high-voltage terminals to prevent discharge in air. Later Tesla coils spread their electric fields over larger distances to prevent high electrical stresses in the first place, thereby allowing operation in free air. Most modern Tesla coils also use toroid-shaped output terminals. These are often fabricated from spun metal or flexible aluminum ducting. The toroidal shape helps to control the high electrical field near the top of the secondary by directing sparks outward and away from the primary and secondary windings.
A more complex version of a Tesla coil, termed a "magnifier" by Tesla, uses a more tightly coupled air-core resonance "driver" transformer (or "master oscillator") and a smaller, remotely located output coil (called the "extra coil" or simply the resonator) that has a large number of turns on a relatively small coil form. The bottom of the driver's secondary winding is connected to ground. The opposite end is connected to the bottom of the extra coil through an insulated conductor that is sometimes called the transmission line. Since the transmission line operates at relatively high RF voltages, it is typically made of 1" diameter metal tubing to reduce corona losses. Since the third coil is located some distance away from the driver, it is not magnetically coupled to it. RF energy is instead directly coupled from the output of the driver into the bottom of the third coil, causing it to "ring up" to very high voltages. The combination of the two-coil driver and third coil resonator adds another degree of freedom to the system, making tuning considerably more complex that for a 2-coil system. The transient response for multiple resonance networks (of which the Tesla magnifier is a sub-set) has only recently been solved. It is now known that a variety of useful tuning "modes" are available, and in most operating modes the extra coil will ring at a different frequency than the master oscillator.
Primary switching.
Modern transistor or vacuum tube Tesla coils do not use a primary spark gap. Instead, the transistor(s) or vacuum tube(s) provide the switching or amplifying function necessary to generate RF power for the primary circuit. Solid-state Tesla coils use the lowest primary operating voltage, typically between 155 to 800 volts, and drive the primary winding using either a single, half-bridge, or full-bridge arrangement of bipolar transistors, MOSFETs or IGBTs to switch the primary current. Vacuum tube coils typically operate with plate voltages between 1500 and 6000 volts, while most spark gap coils operate with primary voltages of 6,000 to 25,000 volts. The primary winding of a traditional transistor Tesla coil is wound around only the bottom portion of the secondary coil. This configuration illustrates operation of the secondary as a pumped resonator. The primary 'induces' alternating voltage into the bottom-most portion of the secondary, providing regular 'pushes' (similar to providing properly timed pushes to a playground swing). Additional energy is transferred from the primary to the secondary inductance and top-load capacitance during each "push", and secondary output voltage builds (called 'ring-up'). An electronic feedback circuit is usually used to adaptively synchronize the primary oscillator to the growing resonance in the secondary, and this is the only tuning consideration beyond the initial choice of a reasonable top-load.
In a dual resonant solid-state Tesla coil (DRSSTC), the electronic switching of the solid-state Tesla coil is combined with the resonant primary circuit of a spark-gap Tesla coil. The resonant primary circuit is formed by connecting a capacitor in series with the primary winding of the coil, so that the combination forms a series tank circuit with a resonant frequency near that of the secondary circuit. Because of the additional resonant circuit, one manual and one adaptive tuning adjustment are necessary. Also, an interrupter is usually used to reduce the duty cycle of the switching bridge, to improve peak power capabilities; similarly, IGBTs are more popular in this application than bipolar transistors or MOSFETs, due to their superior power handling characteristics. A current-limiting circuit is usually used to limit maximum primary tank current (which must be switched by the IGBT's) to a safe level. Performance of a DRSSTC can be comparable to a medium-power spark-gap Tesla coil, and efficiency (as measured by spark length versus input power) can be significantly greater than a spark-gap Tesla coil operating at the same input power.
Practical aspects of design.
High voltage production.
A large Tesla coil of more modern design often operates at very high peak power levels, up to many megawatts (millions of watts). It is therefore adjusted and operated carefully, not only for efficiency and economy, but also for safety. If, due to improper tuning, the maximum voltage point occurs below the terminal, along the secondary coil, a discharge (spark) may break out and damage or destroy the coil wire, supports, or nearby objects.
Tesla experimented with these, and many other, circuit configurations (see right). The Tesla coil primary winding, spark gap and tank capacitor are connected in series. In each circuit, the AC supply transformer charges the tank capacitor until its voltage is sufficient to break down the spark gap. The gap suddenly fires, allowing the charged tank capacitor to discharge into the primary winding. Once the gap fires, the electrical behavior of either circuit is identical. Experiments have shown that neither circuit offers any marked performance advantage over the other.
However, in the typical circuit, the spark gap's short circuiting action prevents high-frequency oscillations from 'backing up' into the supply transformer. In the alternate circuit, high amplitude high frequency oscillations that appear across the capacitor also are applied to the supply transformer's winding. This can induce corona discharges between turns that weaken and eventually destroy the transformer's insulation. Experienced Tesla coil builders almost exclusively use the top circuit, often augmenting it with low pass filters (resistor and capacitor (RC) networks) between the supply transformer and spark gap to help protect the supply transformer. This is especially important when using transformers with fragile high-voltage windings, such as neon sign transformers (NSTs). Regardless of which configuration is used, the HV transformer must be of a type that self-limits its secondary current by means of internal leakage inductance. A normal (low leakage inductance) high-voltage transformer must use an external limiter (sometimes called a ballast) to limit current. NSTs are designed to have high leakage inductance to limit their short circuit current to a safe level.
Tuning precautions.
The primary coil's resonant frequency is tuned to that of the secondary, by using low-power oscillations, then increasing the power (and retuning if necessary) until the system operates properly at maximum power. While tuning, a small projection (called a "breakout bump") is often added to the top terminal in order to stimulate corona and spark discharges (sometimes called streamers) into the surrounding air. Tuning can then be adjusted so as to achieve the longest streamers at a given power level, corresponding to a frequency match between the primary and secondary coil. Capacitive 'loading' by the streamers tends to lower the resonant frequency of a Tesla coil operating under full power. A toroidal topload is often preferred to other shapes, such as a sphere. A toroid with a major diameter that is much larger than the secondary diameter provides improved shaping of the electrical field at the topload. This provides better protection of the secondary winding (from damaging streamer strikes) than a sphere of similar diameter. And, a toroid permits fairly independent control of topload capacitance versus spark breakout voltage. A toroid's capacitance is mainly a function of its major diameter, while the spark breakout voltage is mainly a function of its minor diameter.
Air discharges.
While generating discharges, electrical energy from the secondary and toroid is transferred to the surrounding air as electrical charge, heat, light, and sound. The process is similar to charging or discharging a capacitor, except that a Tesla coil uses AC instead of DC. The current that arises from shifting charges within a capacitor is called a displacement current. Tesla coil discharges are formed as a result of displacement currents as pulses of electrical charge are rapidly transferred between the high-voltage toroid and nearby regions within the air (called space charge regions). Although the space charge regions around the toroid are invisible, they play a profound role in the appearance and location of Tesla coil discharges.
When the spark gap fires, the charged capacitor discharges into the primary winding, causing the primary circuit to oscillate. The oscillating primary current creates an oscillating magnetic field that couples to the secondary winding, transferring energy into the secondary side of the transformer and causing it to oscillate with the toroid capacitance to ground. Energy transfer occurs over a number of cycles, until most of the energy that was originally in the primary side is transferred to the secondary side. The greater the magnetic coupling between windings, the shorter the time required to complete the energy transfer. As energy builds within the oscillating secondary circuit, the amplitude of the toroid's RF voltage rapidly increases, and the air surrounding the toroid begins to undergo dielectric breakdown, forming a corona discharge.
As the secondary coil's energy (and output voltage) continue to increase, larger pulses of displacement current further ionize and heat the air at the point of initial breakdown. This forms a very electrically conductive "root" of hotter plasma, called a leader, that projects outward from the toroid. The plasma within the leader is considerably hotter than a corona discharge, and is considerably more conductive. In fact, its properties are similar to an electric arc. The leader tapers and branches into thousands of thinner, cooler, hair-like discharges (called streamers). The streamers look like a bluish 'haze' at the ends of the more luminous leaders. The streamers transfer charge between the leaders and toroid to nearby space charge regions. The displacement currents from countless streamers all feed into the leader, helping to keep it hot and electrically conductive.
The primary break rate of sparking Tesla coils is slow compared to the resonant frequency of the resonator-topload assembly. When the switch closes, energy is transferred from the primary LC circuit to the resonator where the voltage rings up over a short period of time up culminating in the electrical discharge. In a spark gap Tesla coil, the primary-to-secondary energy transfer process happens repetitively at typical pulsing rates of 50–500 times per second, depending on the frequency of the input line voltage. At these rates, previously-formed leader channels do not get a chance to fully cool down between pulses. So, on successive pulses, newer discharges can build upon the hot pathways left by their predecessors. This causes incremental growth of the leader from one pulse to the next, lengthening the entire discharge on each successive pulse. Repetitive pulsing causes the discharges to grow until the average energy available from the Tesla coil during each pulse balances the average energy being lost in the discharges (mostly as heat). At this point, dynamic equilibrium is reached, and the discharges have reached their maximum length for the Tesla coil's output power level. The unique combination of a rising high-voltage radio frequency envelope and repetitive pulsing seem to be ideally suited to creating long, branching discharges that are considerably longer than would be otherwise expected by output voltage considerations alone. High-voltage discharges create filamentary multibranched discharges which are purplish-blue in colour. High-energy discharges create thicker discharges with fewer branches, are pale and luminous, almost white, and are much longer than low-energy discharges, because of increased ionisation. A strong smell of ozone and nitrogen oxides will occur in the area. The important factors for maximum discharge length appear to be voltage, energy, and still air of low to moderate humidity. There are comparatively few scientific studies about the initiation and growth of pulsed lower-frequency RF discharges, so some aspects of Tesla coil air discharges are not as well understood when compared to DC, power-frequency AC, HV impulse, and lightning discharges.
Applications.
Tesla coil circuits were used commercially in sparkgap radio transmitters for wireless telegraphy until the 1920s, and in electrotherapy and pseudomedical devices such as violet ray. Today, although small Tesla coils are used as leak detectors in scientific high vacuum systems and igniters in arc welders, their main use is entertainment and educational displays, Tesla coils are built by many high-voltage enthusiasts, research institutions, science museums, and independent experimenters. Although electronic circuit controllers have been developed, Tesla's original spark gap design is less expensive and has proven extremely reliable.
Entertainment.
Tesla coils are very popular devices among certain electrical engineers and electronics enthusiasts. Builders of Tesla coils as a hobby are called "coilers". A very large Tesla coil, designed and built by Syd Klinge, is shown every year at the Coachella Valley Music and Arts Festival, in Coachella, Indio, California, USA. People attend "coiling" conventions where they display their home-made Tesla coils and other electrical devices of interest. Austin Richards, a physicist in California, created a metal Faraday Suit in 1997 that protects him from Tesla Coil discharges. In 1998, he named the character in the suit Doctor MegaVolt and has performed all over the world and at Burning Man 9 different years.
Low-power Tesla coils are also sometimes used as a high-voltage source for Kirlian photography.
Tesla coils can also be used to generate sounds, including music, by modulating the system's effective "break rate" (i.e., the rate and duration of high power RF bursts) via MIDI data and a control unit. The actual MIDI data is interpreted by a microcontroller which converts the MIDI data into a PWM output which can be sent to the Tesla coil via a fiber optic interface. The YouTube video shows a performance on matching solid state coils operating at 41 kHz. The coils were built and operated by designer hobbyists Jeff Larson and Steve Ward. The device has been named the Zeusaphone, after Zeus, Greek god of lightning, and as a play on words referencing the Sousaphone. The idea of playing music on the singing Tesla coils flies around the world and a few followers<ref name="http://www.youtube.com/user/teslamusicband"></ref> continue the work of initiators. An extensive outdoor musical concert has demonstrated using Tesla coils during the Engineering Open House (EOH) at the University of Illinois at Urbana-Champaign. The Icelandic artist Björk used a Tesla coil in her song "Thunderbolt" as the main instrument in the song. The musical group ArcAttack uses modulated Tesla coils and a man in a chain-link suit to play music.
The world's largest currently existing two-coil Tesla coil is a 130,000-watt unit, part of a 38 ft sculpture titled "Electrum" owned by Alan Gibbs and currently resides in a private sculpture park at Kakanui Point near Auckland, New Zealand. The most powerful conical Tesla coil (1.5 million volts) was installed in 2002 at the Mid-America Science Museum in Hot Springs, Arkansas. This is a replica of the Griffith Observatory conical coil installed in 1936.
Vacuum system leak detectors.
Scientists working with high vacuum systems test for the presence of tiny pin holes in the apparatus (especially a newly blown piece of glassware) using high-voltage discharges produced by a small handheld Tesla coil. When the system is evacuated the high voltage electrode of the coil is played over the outside of the apparatus. The discharge travels through any pin hole immediately below it, producing a corona discharge inside the evacuated space which illuminates the hole, indicating points that need to be annealed or reblown before they can be used in an experiment.
Wireless power transmission.
Tesla used his Tesla coil circuits to perform the first experiments in wireless power transmission at the turn of the 20th century, In the period 1891 to 1904 he experimented with transmitting RF power between elevated metal terminals by capacitive coupling and between coils of wire by inductive coupling. In demonstrations before the American Institute of Electrical Engineers and at the 1893 Columbian Exposition in Chicago he lit light bulbs from across a stage. He found he could increase the distance by using a receiving LC circuit tuned to resonance with the Tesla coil's LC circuit, transferring energy by resonant inductive coupling. At his Colorado Springs laboratory during 1899-1900, by using voltages of the order of 20 megavolts generated by his enormous magnifying transmitter coil, he was able to light three incandescent lamps at a distance of about 100 feet. The resonant inductive coupling technique pioneered by Tesla has recently become a central concept in modern wireless power development, and is being widely used in short range wireless transmission systems like cellphone charging pads.
The inductive and capacitive coupling used in Tesla's experiments are "near-field" effects, meaning that the energy transferred decreases with the sixth power of the distance between transmitter and receiver, so they cannot be used for long-distance transmission. However, Tesla was obsessed with developing a long range wireless power transmission system which could transmit power from power plants directly into homes and factories without wires, described in a visionary June, 1900 article in Century Magazine; "The Problem of Increasing Human Energy", and he believed resonance was the key. Tesla claimed to be able to transmit power on a "worldwide" scale, using a method that involved conduction through the Earth and atmosphere. Tesla was vague about his methods. One of his ideas was that transmitting and receiving terminals could be suspended in the air by balloons at 30,000 feet altitude, where the air pressure is lower. At this altitude, Tesla thought, an ionized layer would allow electricity to be sent at high voltages (millions of volts) over long distances.
In 1901, Tesla began construction of a high-voltage wireless power station, the Wardenclyffe Tower at Shoreham, New York. Essentially a large Tesla coil intended as a prototype transmitter for a "World Wireless System" that was to transmit both information and power worldwide, by 1904 he had lost funding and the facility was never completed. Although Tesla seems to have believed his ideas were proven, he had a history of making claims that he had not confirmed by experiment, and there seems to be no evidence that he ever transmitted significant power beyond the short-range demonstrations above. The only report of long-distance transmission by Tesla is a claim, not found in reliable sources, that in 1899 he wirelessly lit 200 light bulbs at a distance of 26 miles. There is no independent confirmation of this supposed demonstration; Tesla did not mention it, and it does not appear in his laboratory notes. It originated in 1944 from Tesla's first biographer, John J. O'Neill, who said he pieced it together from "fragmentary material... in a number of publications". In the 110 years since Tesla's experiments, efforts by others to achieve long distance power transmission using Tesla coils have failed, and the scientific consensus is his World Wireless system would not have worked. Contemporary scientists point out that while Tesla's coils function as radio transmitters, transmitting energy in the form of radio waves, the frequency he used, around 150 kHz, is far too low for practical long range power transmission. At these wavelengths the radio waves spread out in all directions and cannot be focused on a distant receiver. Long range wireless power transmission was only achieved in the 1960s with the development of microwave technology. Tesla's world power transmission scheme remains today what it was in Tesla's time: a bold, fascinating dream.
High-frequency electrical safety.
The 'skin effect'.
The dangers of contact with high-frequency electrical current are sometimes perceived as being less than at lower frequencies, because the subject usually does not feel pain or a 'shock'. This is often erroneously attributed to skin effect, a phenomenon that tends to inhibit alternating current from flowing inside conducting media. It was thought that in the body, Tesla currents travelled close to the skin surface, making them safer than lower-frequency electric currents.
Although skin effect limits Tesla currents to the outer fraction of an inch in metal conductors, the 'skin depth' of human flesh at typical Tesla coil frequencies is still of the order of 60 inches (150 cm) or more. This means high-frequency currents will still preferentially flow through deeper, better conducting, portions of an experimenter's body such as the circulatory and nervous systems. The reason for the lack of pain is that a human being's nervous system does not sense the flow of potentially dangerous electrical currents above 15–20 kHz; essentially, for nerves to be activated, a significant number of ions must cross their membranes before the current (and hence voltage) reverses. Since the body no longer provides a warning 'shock', novices may touch the output streamers of small Tesla coils without feeling painful shocks. However, anecdotal evidence among Tesla coil experimenters indicates temporary tissue damage may still occur and be observed as muscle pain, joint pain, or tingling for hours or even days afterwards. This is believed to be caused by the damaging effects of internal current flow, and is especially common with continuous wave, solid state or vacuum tube Tesla coils operating at relatively low frequencies (tens to hundreds of kHz). It is possible to generate very high frequency currents (tens to hundreds of MHz) that do have a smaller penetration depth in flesh. These are often used for medical and therapeutic purposes such as electrocauterization and diathermy. The designs of early diathermy machines were based on Tesla coils or Oudin coils.
Large Tesla coils and magnifiers can deliver dangerous levels of high-frequency current, and they can also develop significantly higher voltages (often 250,000–500,000 volts, or more). Because of the higher voltages, large systems can deliver higher energy, potentially lethal, repetitive high-voltage capacitor discharges from their top terminals. Doubling the output voltage quadruples the electrostatic energy stored in a given top terminal capacitance. If an unwary experimenter accidentally places himself in path of the high-voltage capacitor discharge to ground, the sudden pulse of current can cause involuntary spasms of major muscle groups electric shock and may induce life-threatening ventricular fibrillation and even cardiac arrest. Even lower power vacuum tube or solid state Tesla coils can deliver RF currents capable of causing temporary internal tissue, nerve, or joint damage through Joule heating. In addition, an RF arc can carbonize flesh, causing a painful and dangerous bone-deep RF burn that may take months to heal. Because of these risks, knowledgeable experimenters avoid contact with streamers from all but the smallest systems. Professionals usually use other means of protection such as a Faraday cage or a metallic mail suit to prevent dangerous currents from entering their bodies.
The most serious dangers associated with Tesla coil operation are associated with the primary circuit. It is capable of delivering a sufficient current at a significant voltage to stop the heart of a careless experimenter. Because these components are not the source of the trademark visual or auditory coil effects, they may easily be overlooked as the chief source of hazard. Should a high-frequency arc strike the exposed primary coil while, at the same time, another arc has also been allowed to strike to a person, the ionized gas of the two arcs forms a circuit that may conduct lethal, low-frequency current from the primary into the person.
Further, great care must be taken when working on the primary section of a coil even when it has been disconnected from its power source for some time. The tank capacitors can remain charged for days with enough energy to deliver a fatal shock. Proper designs always include 'bleeder resistors' to bleed off stored charge from the capacitors. In addition, a safety shorting operation is performed on each capacitor before any internal work is performed.
Further reading.
Reed, J. L., "Tesla transformer damping", Review of Scientific Instruments, 83, 076101-1 (2012).

</doc>
<doc id="39116" url="http://en.wikipedia.org/wiki?curid=39116" title="La Scala">
La Scala

La Scala (abbreviation in Italian language for the official name Teatro alla Scala) is a world-renowned opera house in Milan, Italy. The theatre was inaugurated on 3 August 1778 and was originally known as the New Royal-Ducal Theatre alla Scala (Nuovo Regio Ducale Teatro alla Scala). The premiere performance was Antonio Salieri's "Europa riconosciuta".
Most of Italy's greatest operatic artists, and many of the finest singers from around the world, have appeared at La Scala during the past 200 years. Today, the theatre is still recognised as one of the leading opera and ballet theatres in the world and is home to the La Scala Theatre Chorus, La Scala Theatre Ballet and La Scala Theatre Orchestra. The theatre also has an associate school, known as the La Scala Theatre Academy (Italian: "Accademia Teatro alla Scala"), which offers professional training in music, dance, stage craft and stage management.
Overview.
La Scala's season traditionally opens on 7 December, Saint Ambrose's Day, the feast day of Milan's patron saint. All performances must end before midnight, and long operas start earlier in the evening when necessary.
The Museo Teatrale alla Scala (La Scala Theatre Museum), accessible from the theatre's foyer and a part of the house, contains a collection of paintings, drafts, statues, costumes, and other documents regarding La Scala's and opera history in general. La Scala also hosts the Accademia d'Arti e Mestieri dello Spettacolo (Academy for the Performing Arts). Its goal is to train a new generation of young musicians, technical staff, and dancers (at the Scuola di Ballo del Teatro alla Scala, one of the Academy's divisions).
History.
A fire destroyed the previous theatre, the Teatro Regio Ducale, on 25 February 1776, after a carnival gala. A group of ninety wealthy Milanese, who owned "palchi" (private boxes) in the theatre, wrote to Archduke Ferdinand of Austria-Este asking for a new theatre and a provisional one to be used while completing the new one. The neoclassical architect Giuseppe Piermarini produced an initial design but it was rejected by Count Firmian (the governor of the then Austrian Lombardy).
A second plan was accepted in 1776 by Empress Maria Theresa. The new theatre was built on the former location of the church of Santa Maria alla Scala, from which the theatre gets its name. The church was deconsecrated and demolished, and over a period of two years the theatre was completed by Pietro Marliani, Pietro Nosetti and Antonio and Giuseppe Fe.
The theatre had a total of "3,000 or so" seats organized into 678 pit-stalls, arranged in six tiers of boxes above which is the 'loggione' or two galleries. Its stage is one of the largest in Italy (16.15m d x 20.4m w x 26m h).
Building expenses were covered by the sale of palchi, which were lavishly decorated by their owners, impressing observers such as Stendhal. La Scala (as it came to be known) soon became the preeminent meeting place for noble and wealthy Milanese people. In the tradition of the times, the "platea" (the main floor) had no chairs and spectators watched the shows standing up. The orchestra was in full sight, as the "golfo mistico" (orchestra pit) had not yet been built.
Above the boxes, La Scala has a gallery—called the "loggione"—where the less wealthy can watch the performances. The gallery is typically crowded with the most critical opera aficionados, known as the loggionisti, who can be ecstatic or merciless towards singers' perceived successes or failures. For their failures, artists receive a "baptism of fire" from these aficionados, and fiascos are long remembered an example being when, in 2006, tenor Roberto Alagna was booed off the stage during a performance of "Aïda"> this forced his understudy, Antonello Palombi, to quickly replace him mid-scene without time to change into a costume.
As with most of the theatres at that time, La Scala was also a casino, with gamblers sitting in the foyer. Conditions in the auditorium, too, could be frustrating for the opera lover, as Mary Shelley discovered in September 1840:
At the Opera they were giving Otto Nicolai's "Templario". Unfortunately, as is well known, the theatre of La Scala serves, not only as the universal drawing-room for all the society of Milan, but every sort of trading transaction, from horse-dealing to stock-jobbing, is carried on in the pit; so that brief and far between are the snatches of melody one can catch.
La Scala was originally illuminated with 84 oil lamps mounted on the "palcoscenico" and another thousand in the rest of theatre. To prevent the risks of fire, several rooms were filled with hundreds of water buckets. In time, oil lamps were replaced by gas lamps, these in turn were replaced by electric lights in 1883.
The original structure was renovated in 1907, when it was given its current layout with 1,987 seats. In 1943, during WWII, La Scala was severely damaged by bombing. It was rebuilt and reopened on 11 May 1946, with a memorable concert conducted by Arturo Toscanini—twice La Scala's principal conductor and an associate of the composers Giuseppe Verdi and Giacomo Puccini—with a soprano solo by Renata Tebaldi, which created a sensation.
La Scala hosted the "prima" (first production) of many famous operas, and had a special relationship with Verdi. For several years, however, Verdi did not allow his work to be played here, as some of his music had been modified (he said "corrupted") by the orchestra. This dispute originated in a disagreement over the production of his "Giovanna d'Arco" in 1845; however the composer later conducted his "Requiem" there on 25 May 1874 and he announced in 1886 that La Scala would host the premiere of what was to become his penultimate opera, "Otello". The premiere of his last opera, "Falstaff" was also given in the theatre.
In 1982, the Filarmonica della Scala was established, drawing its members from the larger pool of musicians that comprise the Orchestra della Scala.
Recent developments.
Major renovation, 2002 to 2004.
The theatre underwent a major renovation from early 2002 to late 2004. The theatre was closed following the traditional 7 December 2001 season opening performances of "Otello", which ran through December. From 19 January 2002 to November 2004, the opera company was transferred to the new Teatro degli Arcimboldi, built in the Pirelli-Bicocca industrial area 4.5 miles from the city centre.
The renovation by renowned architect Mario Botta proved controversial, as preservationists feared that historic details would be lost. However, the opera company was said to be impressed with improvements to the structure and the sound quality, which was enhanced when the heavy red carpets in the hall were removed. The stage was entirely re-constructed, and an enlarged backstage allows more sets to be stored, permitting more productions.
Seats now include monitors for the electronic libretto system provided by Radio Marconi, an Italian company, allowing audiences to follow opera libretti in English and Italian in addition to the original language.
The opera house re-opened on 7 December 2004 with a production, conducted by Riccardo Muti, of Salieri's "Europa riconosciuta", the opera that was performed at La Scala's inauguration in 1778. Tickets for the re-opening fetched up to €2,000.
The renovations cost a reported €61 million, and left a budget shortfall that the opera house overcame in 2006.
Management controversies and changes, 2005 onward.
Carlo Fontana, the general manager of La Scala since 1990, was dismissed in February 2005 by the board of governors over differences with the music director, Riccardo Muti. The resulting staff backlash caused serious disruptions and staff strikes. In a statement, the theatre's board said it was "urgent to unify the theatre's management." On 16 March 2005, the La Scala orchestra and other staff voted overwhelmingly in no confidence motion against Muti, and demanded the resignation of Fontana's replacement, Mauro Meli. Muti had already been forced to cancel a concert a few days earlier because of the disagreements. Italy's culture minister, Giuliano Urbani, supported the conductor but called for urgent action by management to safeguard the smooth operation and prestige of La Scala. On 2 April 2005, Muti resigned from La Scala, citing "hostility" from staff members.
In May 2005, Stéphane Lissner, formerly with the Aix-en-Provence Festival, was appointed as General Manager and Artistic Director of La Scala, becoming the first non-Italian in its history to hold the office. On 15 May 2006, Daniel Barenboim was named "Maestro Scaligero", or "de facto" principal guest conductor, of the company. In October 2011, Barenboim was appointed the next music director of La Scala, effective December 2011, with an initial contract of 5 years.
In December 2013, Riccardo Chailly was appointed the next music director of La Scala, effective 1 January 2015.
Stéphane Lissner left La Scala for the Paris Opera. His successor Alexander Pereira, formerly director of the Salzburg Festival, began his tenure on 1 October 2014.
References.
Notes
Sources

</doc>
<doc id="39120" url="http://en.wikipedia.org/wiki?curid=39120" title="Nvidia">
Nvidia

Nvidia Corporation ( ) is an American worldwide technology company based in Santa Clara, California. Nvidia manufactures graphics processing units (GPUs), as well as system on a chip units (SOCs) for the mobile computing market. Nvidia's primary GPU product line, labeled "GeForce", is in direct competition with AMD's "Radeon" products. Nvidia also joined the gaming industry with its handheld Shield Portable and Shield Tablet, as well as the tablet market with the Tegra Note 7.
In addition to GPU manufacturing, Nvidia provides parallel processing capabilities to researchers and scientists that allow them to efficiently run high-performance applications. They are deployed in supercomputing sites around the world. More recently, Nvidia has moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets, as well as vehicle navigation and entertainment systems. In addition to Advanced Micro Devices, its competitors include Intel and Qualcomm.
Company history.
Founders and initial investment.
Three people co-founded Nvidia in 1993:
The founders received venture capital funding from Sequoia Capital.
Major releases and acquisitions.
Autumn 1999 saw the release of the GeForce 256 (NV10), most notably introducing on-board transformation and lighting (T&L) to consumer-level 3D hardware. Running at 120 MHz and featuring four pixel pipelines, it implemented advanced video acceleration, motion compensation, and hardware sub-picture alpha blending. The GeForce outperformed existing products by a wide margin.
Due to the success of its products, Nvidia won the contract to develop the graphics hardware for Microsoft's Xbox game console, which earned Nvidia a $200 million advance. However, the project drew the time of many of Nvidia's best engineers away from other projects. In the short term this did not matter, and the GeForce2 GTS shipped in the summer of 2000.
In 2000, Nvidia acquired the intellectual assets of its one-time rival 3dfx, one of the biggest graphics companies of the mid-to-late 1990s.
In July 2002, Nvidia acquired Exluna for an undisclosed sum. Exluna made software rendering tools and the personnel were merged into the Cg project.
In August 2003, Nvidia acquired MediaQ for approximately US$70 million.
On April 22, 2004, Nvidia acquired iReady, also a provider of high performance TCP/IP and iSCSI offload solutions.
December 2004 saw the announcement that Nvidia would assist Sony with the design of the graphics processor (RSX) in the PlayStation 3 game console. In March 2006, it emerged that Nvidia would deliver RSX to Sony as an IP core, and that Sony alone would organize the manufacture of the RSX. Under the agreement, Nvidia would provide ongoing support to port the RSX to Sony's fabs of choice (Sony and Toshiba), as well as die shrinks to 65 nm. This practice contrasted with Nvidia's business arrangement with Microsoft, in which Nvidia managed production and delivery of the Xbox GPU through Nvidia's usual third-party foundry contracts. Meanwhile, in May 2005 Microsoft chose to license a design by ATI and to make its own manufacturing arrangements for the Xbox 360 graphics hardware, as had Nintendo for the Wii console (which succeeded the ATI-based Nintendo GameCube).
On December 14, 2005, Nvidia acquired ULI Electronics, which at the time supplied third-party southbridge parts for chipsets to ATI, Nvidia's competitor.
In March 2006, Nvidia acquired Hybrid Graphics.
In December 2006, Nvidia, along with its main rival in the graphics industry AMD (which had acquired ATI), received subpoenas from the U.S. Department of Justice regarding possible antitrust violations in the graphics card industry.
"Forbes" magazine named Nvidia its "Company of the Year" for 2007, citing the accomplishments it made during the said period as well as during the previous 5 years.
On January 5, 2007, Nvidia announced that it had completed the acquisition of PortalPlayer, Inc.
In February 2008, Nvidia acquired Ageia Technologies for an undisclosed sum. Ageia developed the PhysX physics engine hardware and SDK.
In April 2009, a court consolidated multiple class action suits into one case, titled The NVIDIA GPU Litigation. NVIDIA agreed to replace faulty chips in or reimburse purchasers who already spent to get their laptop repaired. Nvidia also gave replacement laptops to many users in lieu of making a repair. The replacements and payments were not made until the settlement was finalized in 2011. Users were required to show proof of purchase and mail in their original faulty laptop. The chips were present in a number of Dell and HP laptops, as well as two Apple MacBook Pro models. Although the settlement cost Nvidia millions of dollars, many of the individuals were unhappy with the settlement, and multiple websites and blogs reflected this. The website entitled Fair Nvidia Settlement was one such site.
On January 10, 2011, Nvidia signed a six-year cross-licensing agreement with Intel, marking the end of all outstanding legal disputes between these two companies. According to the agreement, Intel agreed to pay Nvidia $1.5 billion in licensing fees in five annual installments.
On February 15, 2011, Nvidia announced and demonstrated the first quad-core processor for mobile devices at the Mobile World Congress in Barcelona. It was announced that the chip was expected to ship with many tablets to be released in the second half of 2011, and the chip, dubbed the Tegra 3, was released on November 9, 2011.
In May 2011, it was announced that Nvidia had agreed to acquire Icera, a baseband chip making company in the UK, for $367 million in cash.
On July 29, 2013, NVIDIA Corporation announced that they acquired PGI from STMicroelectronics.
On January 6, 2013, Nvidia introduced at CES 2013, the Tegra 4 mobile processor (codename "Wayne"), containing 72 GPU cores, a Quad-core ARM Cortex-A15 CPU core, and LTE capability among its features.
On February 19, 2013, Nvidia announced the Tegra 4i (codename "Project Grey"), its first fully integrated 4G LTE mobile processor, featuring 5 times more GPU cores than Tegra 3, 1080p HD support, and Nvidia Chimera Computational Photography Architecture.
On January 6, 2014, Nvidia introduced at CES 2014, the Tegra K1 mobile processor (codename "Logan"), containing 192 GPU cores and a Quad ARM Cortex-A15 MPCore R3 + low power companion core (32-bit) or Dual-core Project Denver (64-bit). As of February 2014, Nvidia claims that the Tegra K1 outperforms both the Xbox 360 and the PS3 hardware.
Sales and market trends.
According to a survey conducted by market watch firm Jon Peddie Research, Nvidia shipped an estimated 33 million graphics chips in the first quarter of 2010, for a market share of 31.5%. AMD and Intel shipped an estimated 25.15 million units (24.0% market share) and an estimated 45.49 million units (43.5% market share) respectively. Nvidia's year-to-year growth was 41.9%.
In August 2011, Nvidia predicted the growth of its revenues would be 4% to 6%, instead of 4%, as analysts said.
In September 2011, Nvidia forecast strong sales for 2013 in the region of $4.75bn to $5bn, which surpasses analysts expectations of $4.45bn.
Product families.
Nvidia's product portfolio includes graphics processors, wireless communications processors, PC platform (motherboard core logic) chipsets, and digital media player software.
Some of Nvidia's product families are:
Free and open-source software support.
Until September 23, 2013, Nvidia had not published "any" documentation for its hardware, meaning that programmers could not write appropriate and effective free and open-source device driver for Nvidia's products without resorting to (clean room) reverse engineering.
Instead, Nvidia provides its own binary GeForce graphics drivers for X.Org and a thin open-source library that interfaces with the Linux, FreeBSD or Solaris kernels and the proprietary graphics software. Nvidia also provided but stopped supporting an obfuscated open-source driver that only supports two-dimensional hardware acceleration and ships with the X.Org distribution.
The proprietary nature of Nvidia's drivers has generated dissatisfaction within free-software communities. Some Linux and BSD users insist on using only open-source drivers, and regard Nvidia's insistence on providing nothing more than a binary-only driver as wholly inadequate, given that competing manufacturers (like Intel) offer support and documentation for open-source developers, and that others (like AMD) release partial documentation and provide some active development.
Because of the closed nature of the drivers, Nvidia video cards cannot deliver adequate features on some platforms and architectures given that Nvidia only provides x86/x64 driver builds. As a result, support for 3D graphics acceleration in Linux on PowerPC does not exist, nor does support for Linux on the hypervisor-restricted PlayStation 3 console.
Some users claim that Nvidia's Linux drivers impose artificial restrictions, like limiting the number of monitors that can be used at the same time, but the company has not commented on these accusations.
As of January 31, 2014, Nvidia's Alexandre Courbot committed an extensive patch set which add initial support for the GK20A (Tegra K1) to nouveau.

</doc>
<doc id="39122" url="http://en.wikipedia.org/wiki?curid=39122" title="Z/OS">
Z/OS

z/OS is a 64-bit operating system for mainframe computers, produced by IBM. It derives from and is the successor to OS/390, which in turn followed a string of MVS versions. Like OS/390, z/OS combines a number of formerly separate, related products, some of which are still optional. z/OS offers the attributes of modern operating systems but also retains much of the functionality originating in the 1960s and each subsequent decade that is still found in daily use (backward compatibility is one of z/OS's central design philosophies). z/OS was first introduced in October 2000. The newest version is z/OS Version 2 Release 1.
Major characteristics.
z/OS supports stable mainframe systems and standards such as CICS, IMS, DB2, RACF, SNA, WebSphere MQ, record-oriented data access methods, REXX, CLIST, SMP/E, JCL, TSO/E, and ISPF, among others. However, z/OS also supports 64-bit Java, C, C++, and UNIX (Single UNIX Specification) APIs and applications through UNIX System Services  — The Open Group certifies z/OS as a compliant UNIX operating system — with UNIX/Linux-style hierarchical HFS and zFS file systems. As a result, z/OS hosts a broad range of commercial and open source software of any vintage. z/OS can communicate directly via TCP/IP, including IPv6, and includes standard HTTP servers (one from Lotus, the other Apache-derived) along with other common services such as FTP, NFS, and CIFS/SMB. Another central design philosophy is support for extremely high quality of service (QoS), even within a single operating system instance, although z/OS has built-in support for Parallel Sysplex clustering.
z/OS has a Workload Manager (WLM) and dispatcher which automatically manages numerous concurrently hosted units of work running in separate key-protected address spaces according to dynamically adjustable goals. This capability inherently supports multi-tenancy within a single operating system image. However, modern IBM mainframes also offer two additional levels of virtualization: LPARs and (optionally) z/VM. These new functions within the hardware, z/OS, and z/VM — and Linux and OpenSolaris support — have encouraged development of new applications for mainframes. Many of them utilize the WebSphere Application Server for z/OS middleware.
From its inception z/OS has supported tri-modal addressing (24-bit, 31-bit, and 64-bit). Up through Version 1.5, z/OS itself could start in either 31-bit ESA/390 or 64-bit z/Architecture mode, so it could function on older hardware albeit without 64-bit application support on those machines. (Only the newer z/Architecture hardware manufactured starting in the year 2000 can run 64-bit code.) IBM support for z/OS 1.5 ended on March 31, 2007. Now z/OS is only supported on z/Architecture mainframes and only runs in 64-bit mode. z/Architecture hardware always starts running in 31-bit mode, but current z/OS releases quickly switch to 64-bit mode and will not run on hardware that does not support 64-bit mode. Application programmers can still use any addressing mode: all applications, regardless of their addressing mode(s), can coexist without modification, and IBM maintains commitment to tri-modal backward compatibility. However, increasing numbers of middleware products and applications, such as DB2 Version 8 and above, now require and exploit 64-bit addressing.
IBM markets z/OS as its flagship operating system, suited for continuous, high-volume operation with high security and stability.
z/OS is available under standard license pricing as well as via System z New Application License Charges (zNALC) and "System z Solution Edition," two lower priced offerings aimed at supporting newer applications ("new workloads"). U.S. standard commercial z/OS pricing starts at about $125 per month, including support, for the smallest zNALC installation running the base z/OS product plus a typical set of optional z/OS features.
z/OS introduced Variable Workload License Charges (VWLC) and Entry Workload License Charges (EWLC) which are sub-capacity billing options. VWLC and EWLC customers only pay for peak monthly z/OS usage, not for full machine capacity as with the previous OS/390 operating system. VWLC and EWLC are also available for most IBM software products running on z/OS, and their peaks are separately calculated but can never exceed the z/OS peak. To be eligible for sub-capacity licensing, a z/OS customer must be running in 64-bit mode (which requires z/Architecture hardware), must have completely eliminated OS/390 from the system, and must e-mail IBM monthly sub-capacity reports. Sub-capacity billing substantially reduces software charges for most IBM mainframe customers. Advanced Workload License Charges (AWLC) is the successor to VWLC on mainframe models starting with the zEnterprise 196, and EAWLC is an option on zEnterprise 114 models. AWLC and EAWLC offer further sub-capacity discounts.
Other features.
64-bit memory support.
Within each address space, z/OS typically only permits the placement of data above the 2 GB "bar", not code. z/OS enforces this distinction primarily for performance reasons. There are no architectural impediments to allowing more than 2 GB of application code per address space. IBM has started to allow Java code running on z/OS to execute above the 2 GB bar, again for performance reasons.
Memory is obtained as "Large Memory Objects" in multiples of 1 MB (with the expectation that applications and middleware will manage memory allocation within these large pieces). There are three types of large memory objects:
Release history.
IBMs release scheduling introduces new releases of z/OS each September.
IBM supports z/OS release coexistence and fallback on an "N+2" basis. For example, IBM customers running Release 9 can upgrade directly to Release 11 or Release 10, and both releases can operate concurrently within the same Sysplex (cluster) and without conflict using the same datasets, configurations, security profiles, etc. (coexistence), as long as so-called "toleration maintenance" is installed in the older release. If there is a problem with Release 11, the customer can return to Release 9 without experiencing dataset, configuration, or security profile compatibility problems (fallback) until ready to try moving forward again. z/OS customers using Parallel Sysplex (clustering) can operate N+2 releases (e.g. Release 9 and Release 11, or Release 9 and Release 10) in mixed release configurations, in production, as long as required to complete release upgrades. Supported release mixing within a cluster is one of the key strategies of z/OS for avoiding service outages.
IBM's standard support period for each z/OS release is three years. Most z/OS customers take advantage of the N+2 support model, and skip every other release. Thus most z/OS customers are either "odd" or "even."
IBM releases individual small enhancements and corrections (a.k.a. PTFs) for z/OS as needed, when needed. IBM labels critical PTFs as "HIPER" (High Impact PERvasive). IBM also "rolls up" multiple patches into a Recommended Service Update (RSU). RSUs are released periodically (in the range of every one to three months) and undergo additional testing. Although z/OS customers vary in their maintenance practices, IBM encourages every z/OS customer to adopt a reasonable preventive maintenance strategy, to avoid known problems before they might occur.

</doc>
<doc id="39124" url="http://en.wikipedia.org/wiki?curid=39124" title="Aloysius Lilius">
Aloysius Lilius

Aloysius Lilius (ca. 1510 – 1576), also variously referred to as Luigi Lilio, Luigi Giglio, was an Italian doctor, astronomer, philosopher and chronologist, and also the "primary author" who provided the proposal that (after modifications) became the basis of the Gregorian Calendar reform of 1582.
The crater Lilius on the Moon is named after him, as is the asteroid 2346 Lilio. In computer science, the Lilian date is the number of days since adoption of the Gregorian Calendar on 15 October 1582.
Life and work.
Not much is known about the early life of Lilius/Lilio/Giglio. It is known that he came from Calabria, Italy, from Cirò (or Zirò). He studied medicine and astronomy in Naples, after which he served Earl Carafa. He settled in Verona and died in 1576. Although he was still alive at the time when his proposal was presented at Rome, it does not seem that he made the presentation; it was handled by his brother Antonio, also a physician and astronomer.
He is primarily known as the "first author" of the Gregorian Calendar: he wrote the proposal on which (after modifications) the calendar reform was based. Lilio's brother Antonio presented the manuscript to Pope Gregory XIII; it was passed to the calendar reform commission in 1575. The commission issued a printed summary entitled "Compendium novae rationis restituendi kalendarium" (Compendium of a New Plan for the Restitution of the Calendar), printed in 1577 and circulated within the Roman Catholic world in early 1578 as a consultation document. Lilio's manuscript itself is not known to have survived; the printed 'Compendium' is the nearest known source for the details it contained.
The processes of consultation and deliberation meant that the reform to the calendar did not occur until 1582, six years after the death of Luigi Lilio in 1576. The reform had by then received some modifications in points of detail by the reform commission, in which one of the leading members was Clavius, who afterwards wrote defences and an explanation of the reformed calendar, including an emphatic acknowledgement of Lilio's work, especially for his provision of a useful reform for the lunar cycle: "We owe much gratitude and praise to Luigi Giglio who contrived such an ingenious Cycle of Epacts which, inserted in the calendar, always shows the new moon and so can be easily adapted to any length of the year, if only at the right moments the due adjustment is applied." The papal bull ("Inter gravissimas") was issued on 24 February 1582, ordering Catholic clergy to adopt the new calendar, and exhorting Catholic sovereigns to do the same.
The year 2010 was the 500th year since the Lilius' birth; several activities were organized by Italian astronomers in order to recognize the great work performed by him. In particular, in Torretta di Crucoli (Crotone, Italy), a new astronomical group was created and dedicated to Luigi Lilio: 

</doc>
<doc id="39127" url="http://en.wikipedia.org/wiki?curid=39127" title="The Times">
The Times

The Times is a British daily national newspaper based in London. It began in 1785 under the title "The Daily Universal Register" and became "The Times" on 1 January 1788. "The Times" and its sister paper "The Sunday Times" (founded in 1821) are published by Times Newspapers, since 1981 a subsidiary of News UK, itself wholly owned by the News Corp group headed by Rupert Murdoch. "The Times" and "The Sunday Times" do not share editorial staff, were founded independently and have only had common ownership since 1967.
In 1959, historian of journalism Allan Nevins analysed the importance of "The Times" in shaping London's elite views of events:
For much more than a century "The Times" has been an integral and important part of the political structure of Great Britain. Its news and its editorial comment have in general been carefully coordinated, and have at most times been handled with an earnest sense of responsibility. While the paper has admitted some trivia to its columns, its whole emphasis has been on important public affairs treated with an eye to the best interests of Britain. To guide this treatment, the editors have for long periods been in close touch with 10 Downing Street.
"The Times" is the first newspaper to have borne that name, lending it to numerous other papers around the world, including "The Times of India" (founded in 1838), "The Straits Times" (1845), "The New York Times" (1851), "The Irish Times" (1859), the "Cape Times" (South Africa) (1872), the "Los Angeles Times" (1881), "The Seattle Times" (1891), "The Manila Times" (1898), "The Daily Times" (Malawi) (1900), "The Canberra Times" (1926), and "The Times" (Malta) (1935). In these countries and others, the newspaper is often referred to as The London Times or The Times of London.
"The Times" is the originator of the widely used Times Roman typeface, originally developed by Stanley Morison of "The Times" in collaboration with the Monotype Corporation for its legibility in low-tech printing. In November 2006 "The Times" began printing headlines in a new font, Times Modern. "The Times" was printed in broadsheet format for 219 years, but switched to compact size in 2004 in an attempt to appeal more to younger readers and commuters using public transport. "The Sunday Times" remains a broadsheet.
Though traditionally a moderate newspaper and sometimes a supporter of the Conservative Party, it supported the Labour Party in the 2001 and 2005 general elections. In 2004, according to MORI, the voting intentions of its readership were 40% for the Conservative Party, 29% for the Liberal Democrats, and 26% for Labour. "The Times" had an average daily circulation of 394,448 in March 2014; in the same period, "The Sunday Times" had an average daily circulation of 839,077. An American edition of "The Times" has been published since 6 June 2006.
History.
1785 to 1890.
"The Times" was founded by publisher John Walter on 1 January 1785 as The Daily Universal Register, with Walter in the role of editor. Walter had lost his job by the end of 1784 after the insurance company where he was working went bankrupt because of the complaints of a Jamaican hurricane. Being unemployed, Walter decided to set a new business up. It was in that time when Henry Johnson invented the logography, a new typography that was faster and more precise (three years later, it was proved that it was not as efficient as had been said). Walter bought the logography's patent and to use it, he decided to open a printing house, where he would daily produce an advertising sheet. The first publication of the newspaper "The Daily Universal Register in Great Britain" was 1 January 1785. Unhappy because people always omitted the word "Universal", Ellias changed the title after 940 editions on 1 January 1788 to "The Times". In 1803, Walter handed ownership and editorship to his son of the same name. Walter Sr had spent sixteen months in Newgate Prison for libel printed in "The Times", but his pioneering efforts to obtain Continental news, especially from France, helped build the paper's reputation among policy makers and financiers.
"The Times" used contributions from significant figures in the fields of politics, science, literature, and the arts to build its reputation. For much of its early life, the profits of "The Times" were very large and the competition minimal, so it could pay far better than its rivals for information or writers. Beginning in 1814, the paper was printed on the new steam-driven cylinder press developed by Friedrich Koenig. In 1815, "The Times" had a circulation of 5,000.
Thomas Barnes was appointed general editor in 1817. In the same year, the paper's publisher James Lawson, died and passed the business onto his son John Joseph Farrell (1802–1852). Under the editorship of Barnes and his successor in 1841, John Thadeus Delane, the influence of "The Times" rose to great heights, especially in politics and amongst the City of London. Peter Fraser and Edward Sterling were two noted journalists, and gained for "The Times" the pompous/satirical nickname 'The Thunderer' (from "We thundered out the other day an article on social and political reform."). The increased circulation and influence of the paper was based in part to its early adoption of the steam-driven rotary printing press. Distribution via steam trains to rapidly growing concentrations of urban populations helped ensure the profitability of the paper and its growing influence.
"The Times" was the first newspaper to send war correspondents to cover particular conflicts. W. H. Russell, the paper's correspondent with the army in the Crimean War, was immensely influential with his dispatches back to England.
In other events of the nineteenth century, "The Times" opposed the repeal of the Corn Laws until the number of demonstrations convinced the editorial board otherwise, and only reluctantly supported aid to victims of the Irish Potato Famine. It enthusiastically supported the Great Reform Bill of 1832, which reduced corruption and increased the electorate from 400,000 people to 800,000 people (still a small minority of the population). During the American Civil War, "The Times" represented the view of the wealthy classes, favouring the secessionists, but it was not a supporter of slavery.
The third John Walter, the founder's grandson, succeeded his father in 1847. The paper continued as more or less independent, but from the 1850s "The Times" was beginning to suffer from the rise in competition from the penny press, notably "The Daily Telegraph" and "The Morning Post".
During the 19th century, it was not infrequent for the Foreign Office to approach "The Times" and ask for continental intelligence, which was often superior to that conveyed by official sources.
1890 to 1981.
"The Times" faced financial extinction in 1890 under Arthur Fraser Walter, but it was rescued by an energetic editor, Charles Frederic Moberly Bell. During his tenure (1890–1911), "The Times" became associated with selling the "Encyclopædia Britannica" using aggressive American marketing methods introduced by Horace Everett Hooper and his advertising executive, Henry Haxton. Due to legal fights between the "Britannica's" two owners, Hooper and Walter Montgomery Jackson, "The Times" severed its connection in 1908 and was bought by pioneering newspaper magnate, Alfred Harmsworth, later Lord Northcliffe.
In editorials published on 29 and 31 July 1914, Wickham Steed, the "Times's" Chief Editor, argued that the British Empire should enter World War I. On 8 May 1920, also under the editorship of Steed, "The Times" in an editorial endorsed the anti-Semitic fabrication "The Protocols of the Learned Elders of Zion" as a genuine document, and called Jews the world's greatest danger. In the leader entitled "The Jewish Peril, a Disturbing Pamphlet: Call for Inquiry", Steed wrote about "The Protocols of the Elders of Zion": What are these 'Protocols'? Are they authentic? If so, what malevolent assembly concocted these plans and gloated over their exposition? Are they forgery? If so, whence comes the uncanny note of prophecy, prophecy in part fulfilled, in part so far gone in the way of fulfillment?". The following year, when Philip Graves, the Constantinople (modern Istanbul) correspondent of "The Times", exposed "The Protocols" as a forgery, "The Times" retracted the editorial of the previous year.
In 1922, John Jacob Astor, son of the 1st Viscount Astor, bought "The Times" from the Northcliffe estate. The paper gained a measure of notoriety in the 1930s with its advocacy of German appeasement; then-editor Geoffrey Dawson was closely allied with those in the government who practised appeasement, most notably Neville Chamberlain.
Kim Philby, a Soviet double agent, was a correspondent for the newspaper in Spain during the Spanish Civil War of the late 1930s. Philby was admired for his courage in obtaining high-quality reporting from the front lines of the bloody conflict. He later joined MI6 during World War II, was promoted into senior positions after the war ended, then eventually defected to the Soviet Union in 1963.
Between 1941 and 1946, the left-wing British historian E.H. Carr was Assistant Editor. Carr was well known for the strongly pro-Soviet tone of his editorials. In December 1944, when fighting broke out in Athens between the Greek Communist ELAS and the British Army, Carr in a "Times" editorial sided with the Communists, leading Winston Churchill to condemn him and that leader in a speech to the House of Commons. As a result of Carr's editorial, "The Times" became popularly known during that stage of World War II as the threepenny "Daily Worker" (the price of the "Daily Worker" being one penny)
On 3 May 1966 it resumed printing news on the front page - previously the front page featured small advertisements, usually of interest to the moneyed classes in British society. In 1967, members of the Astor family sold the paper to Canadian publishing magnate Roy Thomson. His Thomson Corporation brought it under the same ownership as "The Sunday Times" to form Times Newspapers Limited.
An industrial dispute prompted the management to shut the paper for nearly a year (1 December 1978 – 12 November 1979).
The Thomson Corporation management were struggling to run the business due to the 1979 Energy Crisis and union demands. Management were left with no choice but to find a buyer who was in a position to guarantee the survival of both titles, and also one who had the resources and was committed to funding the introduction of modern printing methods.
Several suitors appeared, including Robert Maxwell, Tiny Rowland and Lord Rothermere; however, only one buyer was in a position to meet the full Thomson remit, Australian media magnate Rupert Murdoch.
1981 to present.
In 1981, "The Times" and "The Sunday Times" were bought from Thomson by Rupert Murdoch's News International. The acquisition followed three weeks of intensive bargaining with the unions by company negotiators, John Collier and Bill O'Neill.
After 14 years as editor, William Rees-Mogg resigned the post upon completion of the change of ownership. Murdoch began to make his mark on the paper by appointing Harold Evans as his replacement. One of his most important changes was the introduction of new technology and efficiency measures. In March–May 1982, following agreement with print unions, the hot-metal Linotype printing process used to print "The Times" since the 19th century was phased out and replaced by computer input and photo-composition. This allowed print room staff at "The Times" and "The Sunday Times" to be reduced by half. However, direct input of text by journalists ("single stroke" input) was still not achieved, and this was to remain an interim measure until the Wapping dispute of 1986, when "The Times" moved from New Printing House Square in Gray's Inn Road (near Fleet Street) to new offices in Wapping.
Robert Fisk, seven times British International Journalist of the Year, resigned as foreign correspondent in 1988 over what he saw as "political censorship" of his article on the shooting-down of Iran Air Flight 655 in July 1988. He wrote in detail about his reasons for resigning from the paper due to meddling with his stories, and the paper's pro-Israel stance.
In June 1990, "The Times" ceased its policy of using courtesy titles ("Mr", "Mrs", or "Miss" prefixes) for living persons before full names on first reference, but it continues to use them before surnames on subsequent references. The more formal style is now confined to the "Court and Social" page, though "Ms" is now acceptable in that section, as well as before surnames in news sections.
In November 2003, News International began producing the newspaper in both broadsheet and tabloid sizes. On 13 September 2004, the weekday broadsheet was withdrawn from sale in Northern Ireland. Since 1 November 2004, the paper has been printed solely in tabloid format.
On 6 June 2005, "The Times" redesigned its Letters page, dropping the practice of printing correspondents' full postal addresses. Published letters were long regarded as one of the paper's key constituents. Author/solicitor David Green of Castle Morris Pembrokeshire has had more letters published on the main letters page than any other known contributor – 158 by 31 January 2008. According to its leading article, "From Our Own Correspondents", removal of full postal addresses was in order to fit more letters onto the page.
In a 2007 meeting with the House of Lords Select Committee on Communications, which was investigating media ownership and the news, Murdoch stated that the law and the independent board prevented him from exercising editorial control.
In May 2008 printing of "The Times" switched from Wapping to new plants at Broxbourne on the outskirts of London, and Merseyside and Glasgow, enabling the paper to be produced with full colour on every page for the first time.
On 26 July 2012, to coincide with the official start of the London 2012 Olympics and the issuing of a series of souvenir front covers, "The Times" added the suffix "of London" to its masthead.
Content.
"The Times" features news for the first half of the paper with the leading articles on the second page, the Opinion/Comment section begins after the first news section with world news normally following this. The business pages begin on the centre spread, and are followed by The Register, containing obituaries, Court & Social section, and related material. The sport section is at the end of the main paper. "The Times" current prices are £1.20 for the daily edition and £1.50 for the Saturday edition.
"Times2".
"The Times"'s main supplement, every day, is the "times2", featuring various lifestyle columns. It was discontinued on 1 March 2010 but reintroduced on 11 October 2010 after negative feedback. Its regular features include a puzzles section called "Mind Games". Its previous incarnation began on 5 September 2005, before which it was called "T2" and previously "Times 2". Regular features include columns by a different columnist each weekday. There was a column by Marcus du Sautoy each Wednesday, for example. The back pages are devoted to puzzles and contain sudoku, "Killer Sudoku", "KenKen", word polygon puzzles, and a crossword simpler and more concise than the main "Times Crossword".
The supplement contains arts and lifestyle features, TV and radio listings and reviews.
"The Game".
"The Game" is included in the newspaper on Mondays, and details all the weekend's football activity (Premier League and Football League Championship, League One and League Two.) The Scottish edition of "The Game" also includes results and analysis from Scottish Premier League games.
Saturday supplements.
The Saturday edition of "The Times" contains a variety of supplements. These supplements were relaunched in January 2009 as: "Sport", "Weekend" (including travel and lifestyle features), "Saturday Review" (arts, books, and ideas), "The Times Magazine" (columns on various topics), and "Playlist" (an entertainment listings guide).
"Saturday Review" is the first regular supplement published in broadsheet format since the paper switched to a compact size in 2004.
At the beginning of summer 2011 "Saturday Review" switched to the tabloid format
"The Times Magazine" features columns touching on various subjects such as celebrities, fashion and beauty, food and drink, homes and gardens or simply writers' anecdotes. Notable contributors include Giles Coren, Food and Drink Writer of the Year in 2005.
Online presence.
"The Times" and "The Sunday Times" have had an online presence since March 1999, originally at "the-times.co.uk" and "sunday-times.co.uk", and later at "timesonline.co.uk". There are now two websites: "thetimes.co.uk" is aimed at daily readers, and the "thesundaytimes.co.uk" site at providing weekly magazine-like content. There are also iPad and Android editions of both newspapers. Since July 2010, News UK has required readers who do not subscribe to the print edition to pay £2 per week to read "The Times" and "The Sunday Times" online.
"The Times" Digital Archive (1785–2008) is freely accessible via Gale databases to readers affiliated with subscribing academic, public, and school libraries.
Visits to the websites have decreased by 87% since the paywall was introduced, from 21 million unique users per month to 2.7 million. In April 2009, the "timesonline" site had a readership of 750,000 readers per day. As of October 2011, there were around 111,000 subscribers to "The Times"‍ '​ digital products.
Ownership.
"The Times" has had the following eight owners since its foundation in 1785:
Readership.
At the time of Harold Evans' appointment as editor in 1981, "The Times" had an average daily sale of 282,000 copies in comparison to the 1.4 million daily sales of its traditional rival "The Daily Telegraph". By November 2005 "The Times" sold an average of 691,283 copies per day, the second-highest of any British "quality" newspaper (after "The Daily Telegraph", which had a circulation of 903,405 copies in the period), and the highest in terms of full-rate sales. By March 2014, average daily circulation of "The Times" had fallen to 394,448 copies, compared to "The Daily Telegraph"'s 523,048, with the two retaining respectively the second-highest and highest circulations among British "quality" newspapers. In contrast "The Sun", the highest-selling "tabloid" daily newspaper in the United Kingdom, sold an average of 2,069,809 copies in March 2014, and the "Daily Mail", the highest-selling "middle market" British daily newspaper, sold an average of 1,708,006 copies in the period.
"The Sunday Times" has a significantly higher circulation than "The Times", and sometimes outsells "The Sunday Telegraph". As of January 2013, "The Times" has a circulation of 399,339 and "The Sunday Times" of 885,612.
In a 2009 national readership survey "The Times" was found to have the highest number of ABC1 25–44 readers and the largest numbers of readers in London of any of the "quality" papers.
Typeface.
In 1908, "The Times" started using the "Monotype Modern" typeface.
"The Times" commissioned the serif typeface "Times New Roman", created by Victor Lardent at the English branch of Monotype, in 1931. It was commissioned after Stanley Morison had written an article criticizing "The Times" for being badly printed and typographically antiquated. The font was supervised by Morison and drawn by Victor Lardent, an artist from the advertising department of "The Times". Morison used an older font named Plantin as the basis for his design, but made revisions for legibility and economy of space. "Times New Roman" made its debut in the issue of 3 October 1932. After one year, the design was released for commercial sale. "The Times" stayed with "Times New Roman" for 40 years, but new production techniques and the format change from broadsheet to tabloid in 2004 have caused the newspaper to switch font five times since 1972. However, all the new fonts have been variants of the original New Roman font:
Political allegiance.
"The Times" adopted a stance described as "peculiarly detached" at the 1945 general election; although it was increasingly critical of the Conservative Party's campaign, it did not advocate a vote for any one party. However, the newspaper reverted to the Tories for the next election five years later. It supported the Conservatives for the subsequent three elections, followed by support for both the Conservatives and the Liberal Party for the next five elections, expressly supporting a Con-Lib coalition in 1974. The paper then backed the Conservatives solidly until 1997, when it declined to make any party endorsement but supported individual (primarily Eurosceptic) candidates.
For the 2001 general election "The Times" declared its support for Tony Blair's Labour government, which was re-elected by a landslide. It supported Labour again in 2005, when Labour achieved a third successive win, though with a reduced majority. For the 2010 general election, however, the newspaper declared its support for the Tories once again; the election ended in the Tories taking the most votes and seats but having to form a coalition with the Liberal Democrats in order to form a government as they had failed to gain an overall majority.
On the day of the 2015 general election, the leader in The Times suggested that readers consider tactical voting in order to avoid a Labour / SNP coalition. The Times broadly endorsed a continuation of the Conservative / Liberal Democrat coalition as the preferred outcome, though in contrast to The Independent (which has also expressed this preference but would like a stronger liberal voice within the coalition), The Times leader seemed to suggest that the Liberal Democrat party were simply a necessity in order to return the Conservative party to government. 
This makes it the most varied newspaper in terms of political support in British history. Some columnists in "The Times" are connected to the Conservative Party such as Daniel Finkelstein, Tim Montgomerie, Matthew Parris and Matt Ridley, but there are also columnists connected to the Labour Party such as David Aaronovitch, Phil Collins, Oliver Kamm and Jenni Russell.
"The Times" occasionally makes endorsements for foreign elections. In November 2012, it endorsed a second term for Barack Obama although it also expressed reservations about his foreign policy.
Sponsorships.
"The Times", along with the British Film Institute, sponsors the "The Times" "bfi" London Film Festival. It also sponsors the Cheltenham Literature Festival and the Asia House Festival of Asian Literature at Asia House, London.
Related publications.
"Times Literary Supplement".
"The Times Literary Supplement" ("TLS") first appeared in 1902 as a supplement to "The Times", becoming a separately paid-for weekly literature and society magazine in 1914. "The Times" and the "TLS" have continued to be co-owned, and as of 2012 the "TLS" is also published by News International and cooperates closely with "The Times", with its online version hosted on "The Times" website, and its editorial offices based in Times House, Pennington Street, London.
"The Times Science Review".
Between 1951 and 1966 "The Times" published a separately paid-for quarterly science review, "The Times Science Review".
"The Times" started a new, free, monthly science magazine, "Eureka", in October 2009.
Times Atlases.
Times Atlases have been produced since 1895. They are currently produced by the Collins Bartholomew imprint of HarperCollins Publishers. The flagship product is The Times Comprehensive Atlas of the World.
"The Sunday Times Travel Magazine".
This 164-page monthly magazine is sold separately from the newspaper and is Britain's best-selling travel magazine. The first issue of "The Sunday Times Travel Magazine" was in 2003, and it includes news, features and insider guides.
In fiction.
In the dystopian future world of George Orwell's "Nineteen Eighty-Four", "The Times" has been transformed into the organ of the totalitarian ruling party, its editorials—of which several are quoted in the book—reflecting Big Brother's pronouncements.
Rex Stout's fictional detective Nero Wolfe is described as fond of solving the London "Times"‍ '​ crossword puzzle at his New York home, in preference to those of American papers.
In the James Bond series by Ian Fleming, James Bond, reads "The Times". As described by Fleming in "From Russia, with Love": ""The Times" was the only paper that Bond ever read."
In "The Wombles", Uncle Bulgaria read "The Times" and asked for the other Wombles to bring him any copies that they found amongst the litter. The newspaper played a central role in the episode "Very Behind the Times" (Series 2, Episode 12).

</doc>
<doc id="39129" url="http://en.wikipedia.org/wiki?curid=39129" title="OS/390">
OS/390

OS/390 is an IBM operating system for the System/390 IBM mainframe computers.
OS/390 was introduced in late 1995 in an effort, led by the late Randy Stelman, to simplify the packaging and ordering for the key, entitled elements needed to complete a fully functional MVS operating system package. These elements included, but were not limited to:
An additional benefit of the OS/390 packaging concept was to improve reliability, availability and serviceability (RAS) for the operating system, as the number of different combinations of elements that a customer could order and run was drastically reduced. This reduced the overall time required for customers to test and deploy the operating system in their environments, as well as reducing the number of customer reported problems (PMRs), errors (APARs) and fixes (PTFs) arising from the variances in element levels.
In December 2001 IBM extended OS/390 to include support for 64-bit zSeries processors and added various other improvements, and the result is now named z/OS. IBM ended support for the older OS/390-branded versions in late 2004.
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="39132" url="http://en.wikipedia.org/wiki?curid=39132" title="Faunus">
Faunus

In ancient Roman religion and myth, Faunus was the horned god of the forest, plains and fields; when he made cattle fertile he was called Inuus. He came to be equated in literature with the Greek god Pan.
Faunus was one of the oldest Roman deities, known as the "di indigetes". According to the epic poet Virgil, he was a legendary king of the Latins who came with his people from Arcadia. His shade was consulted as a god of prophecy under the name of Fatuus, with oracles in the sacred grove of Tibur, around the well Albunea, and on the Aventine Hill in ancient Rome itself.
Marcus Terentius Varro asserted that the oracular responses were given in Saturnian verse. Faunus revealed the future in dreams and voices that were communicated to those who came to sleep in his precincts, lying on the fleeces of sacrificed lambs. W. Warde Fowler suggested that Faunus is identical with Favonius, one of the Roman wind gods (compare the Anemoi).
Etymology.
Faunus is the Latin outcome of a PIE *"dhau-no-" meaning "the strangler" and denotes the wolf.
According to D. Briquel ("Le problème des Dauniens" in "MEFRA" 1974) it is likely that the "Luceres", one of the three tribes of Rome, were Daunians from Ardea, as well as the characters of the "Aeneis" Mezentius, Messapus and Metabus, who show a Daunian origin. A. Pasqualini agrees on the presence of a Daunian connection in the towns of Latium claiming a Diomedean descent. Moreover it would seem that there is a sizable presence of Daunians in Latium and Campania (Liternum, Nola). Festus 106 L records a king Lucerus who helped Romulus against Titus Tatius. Moreover Oscan epithet Leucesius (present also in the "Carmen Saliare") and Lucetius (Servius Aen. IX 570 "a luce") should be interpreted as related to the Luceres. He also lists the Leucaria mother of Romos (Dionysius of Halicarnassus I 72), Jupiter Lucetius, toponyms Leucasia /Leucaria (Pliny III 8 (13) 85; Dion. Hal. I 53) near Paestum, the ethnonym Lucani. Though Briquel is apparently unaware that the etymology of both "Luceres", Lucera, Leucaria, Lucani and Dauni is from a word meaning "wolf" and therefore different from that of Leucesius/ Lucetius, i.e. from IE from *"luq" (wolf), not from *"leuk" light: compare also "Hirpini" and "Dauni". Daunos according to Walde Hoffmann is from IE root *"dhau" to strangle, meaning the strangler, epithet of the wolf: cfr. Greek "thaunos, thērion" Hes., Phrygian "dáos, lykos" Hes., Latin "F(f)aunus". According to Alessio Latins and Umbrians both did not name the wolf because of a religious taboo, thence their use of loanwords such as "lupus" in Latin (which is Sabine, instead of the expected *"luquos") and the Umbrians "hirpos" (cfr. Hirpini) originally male goat instead of expected "*lupos", whence also "herpex" for "hirpex" tool in the shape wolf teeth.
Consorts and family.
In fable Faunus appears as an old king of Latium, grandson of Saturnus, son of Picus, and father of Latinus by the nymph Marica (who was also sometimes Faunus' mother). After his death he is raised to the position of a tutelary deity of the land, for his many services to agriculture and cattle-breeding.
A goddess of like attributes, called Fauna and Fatua, was associated in his worship. She was regarded as his daughter, wife, or sister. The female deity Bona Dea was often equated with Fauna.
As Pan was accompanied by the "Paniskoi", or little Pans, so the existence of many Fauni was assumed besides the chief Faunus. Fauns are place-spirits ("genii") of untamed woodland. Educated, Hellenizing Romans connected their fauns with the Greek satyrs, who were wild and orgiastic drunken followers of Dionysus, with a distinct origin.
Equivalence with Pan.
With the increasing Hellenization of literate upper-class Roman culture in the 3rd and 2nd–centuries BC, the Romans tried to equate their own deities with one of the Greeks', applying in reverse the Greeks' own "interpretatio graeca". Faunus was naturally equated with the god Pan, who was a pastoral god of shepherds who was said to reside in Arcadia. Pan had always been depicted with horns and as such many depictions of Faunus also began to display this trait. However, the two deities were also considered separate by many, for instance, the epic poet Virgil, in his "Aeneid", made mention of both Faunus and Pan independently.
Festivals.
The Christian writer Justin Martyr identified him as "Lupercus" ("he who wards off the wolf"), the protector of cattle, following Livy, who named his aspect of Inuus as the god who was originally worshiped at the Lupercalia, celebrated on the anniversary of the founding of his temple, February 15, when his priests ("Luperci") wore goat-skins and hit onlookers with goat-skin belts.
Two festivals, called Faunalia, were celebrated in his honour—one on the 13th of February, in the temple of Faunus on the island in the Tiber, the other on the 5th of December, when the peasants brought him rustic offerings and amused themselves with dancing.
A euhemeristic account made Faunus a Latin king, son of Picus and Canens. He was then revered as the god Fatuus after his death, worshipped in a sacred forest outside what is now Tivoli, but had been known since Etruscan times as Tibur, the seat of the Tiburtine Sibyl. His numinous presence was recognized by wolf skins, with wreaths and goblets.
In Nonnos' "Dionysiaca", Faunus/Phaunos accompanied Dionysus when the god campaigned in India.
Later worship.
Faunus was worshipped across the Roman Empire for many centuries. An example of this was a set of thirty-two 4th-century spoons found near Thetford in England in 1979. They had been engraved with the name "Faunus", and each also had a different epithet after the god's name. The spoons also bore Christian symbols, and it has been suggested that these were initially Christian but later taken and devoted to Faunus by pagans. The 4th century was a time of large scale Christianisation, and the discovery provides us with evidence that even during the decline of traditional Roman religion, the god Faunus was still worshipped.
In Gaul, Faunus was identified with the Celtic Dusios.

</doc>
<doc id="39134" url="http://en.wikipedia.org/wiki?curid=39134" title="Pearl Harbour, New Zealand">
Pearl Harbour, New Zealand

"For the site of the 1941 attack, go to Pearl Harbor (Hawaii)."
Pearl Harbour is a small harbour at the head of the Waiau River, in the town of Manapouri on the South Island of New Zealand. The harbour and town are located at the south-east corner of Lake Manapouri.
Pearl Harbour is used mainly by ferry and water taxi operators transporting Meridian Energy staff 35 km across Lake Manapouri to the Manapouri Hydroelectric Power Station, on the West Arm of the lake. Ferries also carry tourists traveling to the power station and onwards to Doubtful Sound, located on the coast 10 km beyond the West Arm. It also provides easy access to Lake Manapouri for recreational boat users. 

</doc>
<doc id="39136" url="http://en.wikipedia.org/wiki?curid=39136" title="Accelerating universe">
Accelerating universe

The accelerating universe is the observation that the universe appears to be expanding at an increasing rate. In formal terms, this means that the cosmic scale factor formula_1 has a positive second derivative, so that the velocity at which a distant galaxy is receding from us should be continuously increasing with time. 
In 1998 two independent projects, the Supernova Cosmology Project and the High-Z Supernova Search Team simultaneously obtained results suggesting a totally unexpected acceleration in the expansion of the universe by using distant type Ia supernovae as standard candles. Three members of these two groups have subsequently been awarded Nobel Prizes for their discovery. Confirmatory evidence has been found in baryon acoustic oscillations and other new results about the clustering of galaxies.
Background.
Since Hubble's discovery of the expansion of the universe in 1929, the Big Bang model has become the accepted explanation for the origin of our universe. The Friedmann equation defines how the energy in the universe drives its expansion.
where the pressure P is defined by the cosmological model chosen. (see explanatory models below)
Physicists at one time were so assured of the deceleration of the universe's expansion that they introduced a so-called deceleration parameter formula_3. Current observations point towards this deceleration parameter being negative.
Evidence for acceleration.
To learn about the rate of expansion of the universe we look at the magnitude-redshift relationship of astronomical objects using standard candles, or their distance-redshift relationship using standard rulers. We can also look at the growth of large-scale structure, and find that the observed values of the cosmological parameters are best described by models which include an accelerating expansion.
Supernova observation.
The first evidence for acceleration came from the observation of Type Ia supernovae, which are exploding white dwarfs who have exceeded their stability limit. Because they all have similar masses, their intrinsic luminosity is standardizable. Repeated imaging of selected areas of sky is used to discover the supernovae, then followup observations give their peak brightness, which is converted into a quantity known as luminosity distance (see distance measures in cosmology for details). Spectral lines of their light can be used to determine their redshift.
For supernovae at redshift less than around 0.1, or light travel time less than 10 percent of the age of the universe, this gives a nearly linear distance-redshift relation due to Hubble's law. At larger distances, since the expansion rate of the universe has changed over time, the distance-redshift relation deviates from linearity, and this deviation depends on how the expansion rate has changed over time. The full calculation requires integration of the Friedmann equation, but a simple derivation can be given as follows: the redshift z directly gives the cosmic scale factor at the time the supernova exploded.
So a supernova with a measured redshift z = 0.5 implies the universe was 1/(1+0.5) = 2/3 of its present size when the supernova exploded. In an accelerating universe, the universe was expanding more slowly in the past than it is today, which means it took a longer time to expand from 2/3 to 1.0 times its present size compared to a non-accelerating universe. This results in a larger light-travel time, larger distance and fainter supernovae, which corresponds to the actual observations. Riess found that "the distances of the high-redshift SNe Ia were, on average, 10% to 15% farther than expected in a low mass density formula_5 universe without a cosmological constant". This means that the measured high-redshift distances were too large, compared to nearby ones, for a decelerating universe.
Baryon acoustic oscillations.
In the early universe before recombination and decoupling took place, photons and matter existed in a primordial plasma. Points of higher density in the photon-baryon plasma would contract, being compressed by gravity until the pressure became too large and they expanded again. This contraction and expansion created vibrations in the plasma analogous to sound waves. Since dark matter only interacts gravitationally it stayed at the centre of the sound wave, the origin of the original overdensity. When decoupling occurred, approximately 380,000 years after the Big Bang, photons separated from matter and were able to stream freely through the universe, creating the cosmic microwave background as we know it. This left shells of baryonic matter at a fixed radius from the overdensities of dark matter, a distance known as the sound horizon. As time passed and the universe expanded, it was at these anisotropies of matter density where galaxies started to form. So by looking at the distances at which galaxies at different redshifts tend to cluster, it is possible to determine a standard angular diameter distance and use that to compare to the distances predicted by different cosmological models.
Peaks have been found in the correlation function (the probability that two galaxies will be a certain distance apart) at 100hformula_6 Mpc, indicating that this is the size of the sound horizon today, and by comparing this to the sound horizon at the time of decoupling (using the CMB), we can confirm that the expansion of the universe is accelerating.
Clusters of galaxies.
Measuring the mass functions of galaxy clusters, which describe the number density of the clusters above a threshold mass, also provides evidence for dark energy. By comparing these mass functions at high and low redshifts to those predicted by different cosmological models, values for formula_7 and formula_8 are obtained which confirm a low matter density and a non zero amount of dark energy.
Age of the universe.
Given a cosmological model with certain values of the cosmological density parameters, it is possible to integrate the Friedmann equations and derive the age of the universe.
By comparing this to actual measured values of the cosmological parameters, we can confirm the validity of a model which is accelerating now, and had a slower expansion in the past.
Explanatory models.
Dark energy.
The most important property of dark energy is that it has negative pressure which is distributed relatively homogeneously in space.
where c is the speed of light, formula_11 is the energy density. Different theories of dark energy suggest different values of w, with w < -1/3 for cosmic acceleration (this leads to a positive value of formula_12 in the acceleration equation above).
The simplest explanation for dark energy is that it is a cosmological constant or vacuum energy; in this case w = -1. This leads to the Lambda-CDM model, which has generally been known as the Standard Model of Cosmology from 2003 through the present, since it is the simplest model in good agreement with a variety of recent observations. Riess found that their results from supernovae observations favoured expanding models with positive cosmological constant (formula_13 > 0) and a current acceleration of the expansion (formula_3 < 0).
Phantom energy.
Current observations allow the possibility of a cosmological model containing a dark energy component with equation of state:
This phantom energy density would become infinite in finite time, causing such a huge gravitational repulsion that the universe would lose all structure and end in a Big Rip. For example, for w = -3/2 and formula_16 = 70 km·s−1·Mpc−1, the time remaining before the universe ends in this "Big Rip" is 22 billion years.
Alternative theories.
Other explanations for the accelerating universe include quintessence, a proposed form of dark energy with a non-constant state equation, whose density decreases with time. Dark fluid is an alternative explanation for accelerating expansion which attempts to unite dark matter and dark energy into a single framework. 
Alternatively, some authors have argued that the universe expansion acceleration could be due to a repulsive gravitational interaction of antimatter.
Another type of model, the backreaction conjecture, was proposed by cosmologist Syksy Räsänen: the rate of expansion is not homogenous, but we are coincidentally in a region where expansion is faster than the background. In this model, inhomogeneities in the early universe cause the formation of walls and bubbles, where the inside of a bubble has less matter than on average. According to general relativity, space is less curved than on the walls, and thus appears to have more volume and a higher expansion rate. If we live inside such a bubble, then it would appear that the universe is expanding at an accelerating rate. The benefit is that it does not require any new physics such as dark energy. Räsänen does not consider the model likely, but without any falsification, it must remain a possibility.
Theories for the consequences to the universe.
As the universe expands, the density of radiation and ordinary and dark matter declines more quickly than the density of dark energy (see equation of state) and, eventually, dark energy dominates. Specifically, when the scale of the universe doubles, the density of matter is reduced by a factor of 8, but the density of dark energy is nearly unchanged (it is exactly constant if the dark energy is a cosmological constant).
In models where dark energy is a cosmological constant, the universe will expand exponentially with time from now on, coming closer and closer to a de Sitter spacetime. This will eventually lead to all evidence for the Big Bang disappearing, as the cosmic microwave background is redshifted to lower intensities and longer wavelengths. Eventually its frequency will be small enough that it will be absorbed by the interstellar medium, and so be screened from any observer within the galaxy. This will occur when the universe is less than 50 times its current age, leading to the end of cosmology as we know it as the distant universe turns dark.
Alternatives for the ultimate fate of the universe include the Big Rip mentioned above, a Big Bounce, Big Freeze, or Big Crunch.

</doc>
<doc id="39137" url="http://en.wikipedia.org/wiki?curid=39137" title="Quintessence (physics)">
Quintessence (physics)

In physics, quintessence is a hypothetical form of dark energy postulated as an explanation of the observation of an accelerating rate of expansion of the Universe announced in 1998. 
It has been proposed by some physicists to be a fifth fundamental force. Quintessence differs from the cosmological constant explanation of dark energy in that it is dynamic, that is, it changes over time, unlike the cosmological constant which always stays constant. 
It is suggested that quintessence can be either attractive or repulsive depending on the ratio of its kinetic and potential energy. Specifically, it is thought that quintessence became repulsive about ten billion years ago (the universe is approximately 13.8 billion years old).
Scalar field.
Quintessence is a scalar field with an equation of state where "w""q", the ratio of pressure "p""q" and density formula_1"q", is given by the potential energy formula_2 and a kinetic term:
Hence, quintessence is dynamic, and generally has a density and "w""q" parameter that varies with time. By contrast, a cosmological constant is static, with a fixed energy density and "w""q" = −1.
Tracker behavior.
Many models of quintessence have a "tracker" behavior, which according to Paul Steinhardt "et al." (1999) partly solves the cosmological constant problem. In these models, the quintessence field has a density which closely tracks (but is less than) the radiation density until matter-radiation equality, which triggers quintessence to start having characteristics similar to dark energy, eventually dominating the Universe. This naturally sets the low scale of the dark energy. When comparing the predicted expansion rate of the Universe as given by the tracker solutions with cosmological data, a main feature of tracker solutions is that one needs four parameters to properly describe the behavior of their equation of state, whereas it has been shown that at most a two-parameter model can optimally be constrained by mid-term future data (horizon 2015-2020).
Specific models.
Some special cases of quintessence are phantom energy, in which "w""q" < −1, and k-essence (short for kinetic quintessence), which has a non-standard form of kinetic energy. If this type of energy were to exist, it would cause a big rip in the Universe due to the growing energy density of dark energy which would cause the expansion of the Universe to increase at a faster-than-exponential rate.
Quintom scenario.
In 2004, when scientists fit the evolution of dark energy with the cosmological data, they found that the equation of state had possibly crossed the cosmological constant boundary (w=-1) from above to below. A proven no-go theorem indicates this situation, called the Quintom scenario, requires at least two degrees of freedom for dark energy models.
Terminology.
The name comes from the classical elements in ancient Greece. The aether, a pure "fifth element" ("quinta essentia" in Latin), was thought to fill the Universe beyond Earth. Similarly, modern quintessence would be the fifth known contribution to the overall mass-energy content of the Universe. (The other four in the modern interpretation, different from the ancient ideas, are: baryonic matter; radiation – photons and the highly relativistic neutrinos, which may be considered hot dark matter; cold dark matter; and the term due to spatial curvature – loosely, gravitational self-energy.)

</doc>
<doc id="39139" url="http://en.wikipedia.org/wiki?curid=39139" title="Proso millet">
Proso millet

Panicum miliaceum (Kannada:ಬರಗು), with many common names including proso millet, common millet, broomtail millet, hog millet, red millet,and white millet, is a grass species used as a crop. Both the wild ancestor and the location of domestication of proso millet are unknown, but it first appears as a crop in both Transcaucasia and China about 7,000 years ago, suggesting it may have been domesticated independently in each area. It is still extensively cultivated in India, Russia, Ukraine, the Middle East, Turkey and Romania. In the United States, proso is mainly grown for birdseed. It is sold as health food, and due to its lack of gluten, it can be included in the diets of people who cannot tolerate wheat.
The name comes from the pan-Slavic general and generic name for millet (Russian, Serbian, Macedonian, Bulgarian: просо and Polish, Czech, Slovakian, Slovenian, Croatian: "proso").
Proso is well adapted to many soil and climatic conditions; it has a short growing season, and needs little water. The water requirement of proso is probably the lowest of any major cereal. It is an excellent crop for dryland and no-till farming. Proso millet is an annual grass whose plants reach an average height of 100 cm (4 feet.). Like corn, it has a C4 photosynthesis. The seedheads grow in bunches. The seeds are small (2–3 mm or 0.1 inch) and can be cream, yellow, orange-red, or brown in colour.
Proso is an annual grass like all other millets, but it is not closely related to pearl millet, foxtail millet, finger millet, or the barnyard millets.
History and domestication.
Unlike the foxtail millet, the wild ancestor of the proso millet has not yet been satisfactorily identified. Weedy forms of this grain are found in central Asia, covering a widespread area from the Caspian Sea east to Xinjiang and Mongolia, and it may be that these semiarid areas may harbor "genuinely wild "P. miliaceum" forms." This millet has been reportedly found in Neolithic sites in Georgia (dated to the fifth and fourth millennia BC), as well as excavated Yangshao culture farming villages east in China. Proso millet appears to have reached Europe not long after its appearance in Georgia, first appearing in east and central Europe; however, the grain needed a few thousand more years to cross into Italy, Greece, and Iran, and the earliest evidence for its cultivation in the Near East is a find in the ruins of Nimrud, Iraq dated to about 700 BC.
While proso millet is not a member of the Neolithic Near East crop assemblage, it arrived in Europe no later than the time these introductions did, and proso millet as an independent domestication could predate the arrival of the Near East grain crops.
Cultivation.
Proso millet is a relatively low demanding crop and diseases aren’t known. That’s why Proso millet is often used in organic farming systems in Europe. In the United States it is often used as an intercrop. Thereby, proso millet can help to avoid a summer fallow, and continuous crop rotation can be achieved. Its superficial root system and its resistance to atrazine residue make proso millet a good intercrop between two water and pesticide demanding crops. The stubbles of the last crop, by allowing more heat into the soil, result in a faster and earlier millet growth. While millet occupies the ground, because of its superficial root system, the soil can replenish in water for the next crop. The later, for example a winter wheat, can in turn benefits from the millet stubbles, which can notably act as snow accumulators.
Climate and soil requirements.
Due to its C4 photosynthetic system, Proso millet is thermophilic like maize. Therefore shady locations of the field should be avoided. It is sensitive to cold temperatures lower than 10 to 13 degrees Celsius. Proso millet is highly drought resistant which makes it of interest to regions with low water availability and longer periods without rain.
The soil should be lightly or medium-heavy. Due to its flat root systems, soil compaction must be prohibited. Furthermore Proso millet doesn’t tolerate wetness due to dammed-up water.
Seedbed & Sowing.
The seedbed should be fine crumbled like for sugar beet and rapeseed. In Europe proso millet is sowed between mid April and the end of May. 500g/are of seeds are required which comes up to 500 grains/m2. In organic farming this amount should be increased if a harrow weeder is used. For sowing the normal sowing machines could be used similar they are used for other crops like wheat. A distance between the rows among 16 to 25 centimeters is recommended if the farmer uses an inter row cultivator. The sowing depth should be 1.5 up to 2 cm in optimal soil or 3 to 4 cm in dry soil. Rolling of the ground after sowing is helpful for further cultivation. Cultivation in no till farming systems is also possible and often done in the US. The sowing then could be done two weeks later.
Field management.
Only a few diseases and pests are known but they aren’t very important. Bigger problems are weeds. The critical phase is the juvenile development. The formation of the grains happens in the 3 up to 5 leaf stadium. After that all nutrients should be available for the proso millet. Therefore it is necessary to oppress the development of weeds. In conventional farming herbicides could be used. In organic farming it’s possible to use harrow weeders and inter row cultivators. For that special sowing parameters described in the chapter above are needed.
For a good development of the plant fertilization with 50 to 75 kg nitrogen per hectare is recommended. Planting proso millet in a crop rotation after maize should be avoided due to its same weed spectrum. Because proso millet is an undemanding crop, it also could be at the end of the crop rotation.
Harvesting & postharvest treatments.
Harvest time is at the end of August until mid September. To determine the best harvest time isn’t that easy because the ripeness of the grains isn’t synchronized. The grains on the top of the panicle are ripe first while the grains in the lower parts need more time. That’s why it’s necessary to find a compromise and catch the date when the yield is highest.
Harvesting could be done with a conventional combine harvester at moisture of the grains about 15-20%. Usually proso millet is mowed at windrows first because the plants aren’t dry like wheat. There they could wither which makes the threshing easier. Then the harvest is done with a pickup attached to the combine.
The possible yield is between 2.5 and 4.5 tons per hectare under optimal conditions. Studies in Germany showed that even a higher yield could be reached.
Uses.
Proso millet is one of the few types of millet not cultivated in Africa.
In the United States, former Soviet Union, and some South American countries, it is primarily grown for livestock feed. As a grain fodder, it is very deficient in lysine and needs complementation.
Proso millet is also a poor fodder due to its low leaf:stem ratio and a possible irritant effect due to its hairy stem. Foxtail millet, having a higher leaf:stem ratio and less hairy stems, is preferred as fodder, particularly the variety called moha, which is a high quality fodder.
In order to promote millet cultivation, other potential uses have been considered recently. For example, starch derived from millets has been shown to be a good substrate for fermentation and malting with grains having similar starch contents as wheat grains. A recently published study suggested that starch derived from Proso millet can be converted to ethanol with an only moderately lower efficiency than starch derived from corn. The development of varieties with highly fermentable characteristics could improve ethanol yield to that of highly fermentable corn. Since Proso Millet is compatible with low input agriculture, cultivation on marginal soils for biofuel production could represent an important new market, for example for farmers in the High Plains of the US.
The demand for more diverse and healthier cereal-based foods is increasing, particularly in affluent countries. This could create new markets for proso millet products in human nutrition. Protein content in proso millet grains is comparable with that of wheat, but the share of essential amino acids (leucine, isoleucine and methionine) is substantially higher in proso millet. In addition, health-promoting phenolic compounds contained in the grains are readily bioaccessible and high Calcium contents favor bone strengthening and dental health. Among the most commonly consumed products are ready-to-eat breakfast cereals made purely from millet flour as well as a variety of noodles and bakery products, which are, however, often produced from mixtures with wheat flour to improve their sensory quality.

</doc>
<doc id="39140" url="http://en.wikipedia.org/wiki?curid=39140" title="Dyson's eternal intelligence">
Dyson's eternal intelligence

Dyson's eternal intelligence concept (the Dyson Scenario) states that intelligent beings would be able to think an infinite number of thoughts in an open universe.
The intelligent beings would begin by storing a finite amount of energy. They then use half (or any fraction) of this energy to power their thought. When the energy gradient created by unleashing this fraction of the stored fuel was exhausted, the beings would enter a state of zero-energy-consumption until the universe cooled. Once the universe had cooled sufficiently, half of the remaining half (one quarter of the original energy) of the intelligent beings' fuel reserves would once again be released, powering a brief period of thought once more. This would continue, with smaller and smaller amounts of energy being released. As the universe cooled, the thoughts would be slower and slower, but there would still be an infinite number of them. The idea was published in a scientific paper and in a popular book by Freeman Dyson.
Two recent observations have presented problems for Dyson's scenario. The first is that the expansion of the universe appears to be accelerating rather than decelerating due to a positive cosmological constant, implying that any two regions of the universe will eventually become permanently separated from one another. The second is that there appears to be a lower bound for the temperature of a vacuum, meaning that the universe would not continue to cool indefinitely .
Also, many grand unification theories predict that protons are unstable, albeit with a very long half-life. Thus the material base for intelligence could eventually disappear (in which case another type of matter could possibly be utilized). No evidence for proton decay has yet been detected, however.
However, even if intelligence cannot continue its own survival indefinitely in an ever-expanding Universe, it may be able to create a `baby universe' via a wormhole in spacetime, add some DNA , and hope that life would eventually replicate itself there. Alternatively, futurist and inventor Ray Kurzweil predicts that a vastly superior race of posthumans or artificial intelligence may be able to evolve to the point when they can control the Universe, thus invalidating the heat death and continuing forever.

</doc>
<doc id="39143" url="http://en.wikipedia.org/wiki?curid=39143" title="Prince Igor">
Prince Igor

Prince Igor (Russian: Князь Игорь, "Knyaz' Igor'") is an opera in four acts with a prologue, written and composed by Alexander Borodin. The composer adapted the libretto from the Ancient Russian epic "The Lay of Igor's Host", which recounts the campaign of Rus prince Igor Svyatoslavich against the invading Cuman ("Polovtsian") tribes in 1185. He also incorporated material drawn from two medieval Kievan chronicles. The opera was left unfinished upon the composer's death in 1887 and was edited and completed by Nikolai Rimsky-Korsakov and Alexander Glazunov. It was first performed in St. Petersburg, Russia, in 1890.
Composition history.
Original Composition: 1869–1887
After briefly considering Lev Mei's "The Tsar's Bride" as a subject (later taken up in 1898 by Nikolai Rimsky-Korsakov, his 9th opera), Borodin began looking for a new project for his first opera. Vladimir Stasov, critic and advisor to The Mighty Handful, suggested "The Lay of Igor's Host", a 12th-century epic prose poem, and sent Borodin a scenario for a three-act opera on 30 April 1869. Initially, Borodin found the proposition intriguing, but daunting:
Your outline is so complete that everything seems clear to me and suits me perfectly. But will I manage to carry out my own task to the end? Bah! As they say here, 'He who is afraid of the wolf doesn't go into the woods!' So I shall give it a try...—Alexander Borodin, reply to Stasov's proposal
After collecting material from literary sources, Borodin began composition in September 1869 with initial versions of Yaroslavna's arioso and Konchakovna's cavatina, and sketched the Polovtsian Dances and March of the Polovtsy. He soon began to have doubts and ceased composing. He expressed his misgivings in a letter to his wife: "There is too little drama here, and no movement... To me, opera without drama, in the strict sense, is unnatural." This began a period of about four years in which he proceeded no further on "Prince Igor", but began diverting materials for the opera into his other works, the Symphony No 2 in B minor (1869–76) and the collaborative opera-ballet "Mlada" (1872).
The "Mlada" project was soon aborted, and Borodin, like the other members of The Mighty Handful who were involved—César Cui, Modest Mussorgsky, and Rimsky-Korsakov—thought about ways to recycle the music he contributed. Of the eight numbers he had composed for Act 4 of "Mlada", those that eventually found their way into (or back into) "Prince Igor" included No. 1 (Prologue: The opening C major chorus), No. 2 (material for Yaroslavna's arioso and Igor's aria), No. 3 (Prologue: The eclipse), No. 4 (Act 3: The trio), and No. 8 (Act 4: The closing chorus).
Borodin returned to "Prince Igor" in 1874, inspired by the success of his colleagues Rimsky-Korsakov and Mussorgsky in the staging of their historical operas, "The Maid of Pskov" (1873) and "Boris Godunov" (1874). This period also marks the creation of two new characters, the deserters Skula and Yeroshka, who have much in common with the rogue monks Varlaam and Misail in "Boris Godunov".
In his memoirs, Rimsky-Korsakov mentions an 1876 concert at which Borodin's "closing chorus" was performed, the first public performance of any music from "Prince Igor" identified by him:
...Borodin's closing chorus ["Glory to the beautiful Sun"]..., which, in the epilogue of the opera (subsequently done away with) extolled Igor's exploits, was shifted by the author himself to the prologue of the opera, of which it now forms a part. At present this chorus extolls Igor as he starts on his expedition against the Polovtsy. The episodes of the solar eclipse, of the parting from Yaroslavna, etc., divide it into halves which fringe the entire prologue. In those days this whole middle part was non-existent, and the chorus formed one unbroken number of rather considerable dimensions.—Nikolai Rimsky-Korsakov, "Chronicle of My Musical Life", 1909
The idea of a choral epilogue in the original scenario was no doubt inspired by the example of "A Life for the Tsar" by Mikhail Glinka, to whose memory "Prince Igor" is dedicated.
Borodin's primary occupation was chemistry, including research and teaching. However, he also spent much time in support of women's causes, much to the consternation of his fellow composers, who felt he should devote his time and talent to music. In 1876, a frustrated Stasov gave up hope that Borodin would ever finish "Prince Igor", and offered his scenario to Rimsky-Korsakov. Rimsky-Korsakov instead assisted Borodin in orchestrating important numbers in preparation for concert performance—for example, the "Polovtsian Dances" in 1879:
There was no end of waiting for the orchestration of the "Polovtsian Dances", and yet they had been announced and rehearsed by me with the chorus. It was high time to copy out the parts. In despair I heaped reproaches on Borodin. He, too, was none too happy. At last, giving up all hope, I offered to help him with the orchestration. Thereupon he came to my house in the evening, bringing with him the hardly touched score of the Polovtsian Dances; and the three of us—he, Anatoly Lyadov, and I—took it apart and began to score it in hot haste. To gain time, we wrote in pencil and not in ink. Thus we sat at work until late at night. The finished sheets of the score Borodin covered with liquid gelatine, to keep our pencil marks intact; and in order to have the sheets dry the sooner, he hung them out like washing on lines in my study. Thus the number was ready and passed on to the copyist. The orchestration of the closing chorus I did almost single-handed..."—Nikolai Rimsky-Korsakov, "Chronicle of My Musical
Life", 1909
Borodin worked on "Prince Igor", off and on, for almost 18 years.
Posthumous Completion and Orchestration: 1887 – 1888
Borodin died suddenly in 1887, leaving "Prince Igor" incomplete. Rimsky-Korsakov and Stasov went to Borodin's home, collected his scores, and brought them to Rimsky-Korsakov's house.
Glazunov and I together sorted all the manuscripts ... In the first place there was the unfinished "Prince Igor". Certain numbers of the opera, such as the first chorus, the dance of the Polovtsy, Yaroslavna's Lament, the recitative and song of Vladimir Galitsky, Konchak's aria, the arias of Konchakovna and Prince Vladimir Igorevich, as well as the closing chorus, had been finished and orchestrated by the composer. Much else existed in the form of finished piano sketches; all the rest was in fragmentary rough draft only, while a good deal simply did not exist. For Acts II and III (in the camp of the Polovtsy) there was no adequate libretto—no scenario, even—there were only scattered verses and musical sketches, or finished numbers that showed no connection between them. The synopsis of these acts I knew full well from talks and discussions with Borodin, although in his projects he had been changing a great deal, striking things out and putting them back again. The smallest bulk of composed music proved to be in Act III. Glazunov and I settled the matter as follows between us: he was to fill in all the gaps in Act III and write down from memory the Overture played so often by the composer, while I was to orchestrate, finish composing, and systematize all the rest that had been left unfinished and unorchestrated by Borodin.—Nikolai Rimsky-Korsakov, "Chronicle of My Musical Life", 1909
The often-repeated account that Glazunov reconstructed and orchestrated the overture from memory after hearing the composer play it at the piano is true only in part. The following statement by Glazunov himself clarifies the matter:
The overture was composed by me roughly according to Borodin's plan. I took the themes from the corresponding numbers of the opera and was fortunate enough to find the canonic ending of the second subject among the composer's sketches. I slightly altered the fanfares for the overture ... The bass progression in the middle I found noted down on a scrap of paper, and the combination of the two themes (Igor's aria and a phrase from the trio) was also discovered among the composer's papers. A few bars at the very end were composed by me.—Alexander Glazunov, memoir, 1891, published in the "Russkaya muzikalnaya gazeta", 1896
Performance history.
During the season of 1888–9 the Directorate of Imperial Theatres began to lead us a fine dance with the production of Prince Igor, which had been finished, published, and forwarded to the proper authorities. We were led by the nose the following season as well, with constant postponements of production for some reason or other." "On October 23, 1890, "Prince Igor" was produced at last, rehearsed fairly well by K. A. Kuchera, as Nápravník had declined the honor of conducting Borodin's opera. Both Glazunov and I were pleased with our orchestration and additions. The cuts later introduced by the Directorate in Act 3 of the opera did it considerable harm. The unscrupulousness of the Mariinsky Theatre subsequently went to the length of omitting Act 3 altogether. Taken all in all, the opera was a success and attracted ardent admirers, particularly among the younger generation.—Nikolai Rimsky-Korsakov, "Chronicle of My Musical Life", 1909
The world premiere was given in St. Petersburg on 4 November (23 October O.S.), 1890 at the Mariinsky Theatre. Set designers were Yanov, Andreyev, and Bocharov, while Lev Ivanov was balletmaster.
Moscow premieres followed later. The first was given in 1892 by the Russian Opera Society, conducted by Iosif Pribik. The Bolshoi Theatre premiere was given in 1898 and was conducted by Ulrikh Avranek
Other notable premieres were given in Prague in 1899, and in Paris in 1909, with a Sergei Diaghilev production featuring Feodor Chaliapin as Galitsky and Maria Kuznetsova as Yaroslavna. London saw the same production in 1914 conducted by Thomas Beecham, again with Chaliapin as Galitsky. In 1915 the United States premiere took place at the Metropolitan Opera, but staged in Italian and conducted by Giorgio Polacco. The first performance in English was at Covent Garden on 26 July 1919, with Miriam Licette as Yaroslavna.
In January and February 2009 there was a production at the Aalto Theatre by the Essen Opera. While some aspects of the production may have been unusual, one critic noted that "placing the (Polovtsian) Dances as a Finale is an elegant idea, [...] the director Andrejs Zagars and the conductor Noam Zur have thus presented a musically and dramaturgically coherent "Prince Igor". Heartfelt applause for a worthwhile evening at the opera.
In 2011 there was a concert performance in Moscow by Helikon Opera, based on Pavel Lamm's reconstruction. A new edition based on 92 surviving manuscripts by Borodin was completed by musicologist Anna Bulycheva and published in 2012.
In 2014, the Metropolitan Opera in New York City staged a reconceived version, sung in Russian for the first time there. Director Dmitri Tcherniakov and conductor Giandrea Noseda removed most of the melodies contributed by Rimsky-Korsakov and Glazunov, although they retained the composers' orchestrations. They added many fragments by Borodin that Rimsky-Korsakov and Glazunov had omitted, basing their work on many decades of musicological research. They rearranged the order in which some of the material appeared, in some cases taking account of notes left by Borodin. The overall conception made the opera more of a psychological drama about Prince Igor and his state of mind, given the deep depression he went into following his soldiers' loss to the Polovtsians. The entire opera was reordered: after the prologue, in which the solar eclipse was taken as a bad omen, Act 1 presented a dream sequence dealing with the relation of Igor and his son with the Polovtsian general and his daughter in the Polovtsian camp. The second act largely dealt with the antics of Prince Galitsky in Putivyl and ended with the destruction of the city. The third act ended with Prince Igor coming out of his depression to begin the rebuilding of the destroyed city. This production starred Russian bass Ildar Abdrazakov in the title role with Ukrainian sopano Oksana Dyka as Yaroslavna. The performances in New York included a worldwide HD broadcast.
 The production was jointly produced with De Nederlandse Opera of Amsterdam.
At the beginning of the Opening Ceremony of the Winter Olympics in Sochi, Russia, in February 2014, some of Borodin's music from this opera was played while an eclipsed sun, crescent-shaped, drifted across the upper levels of the center of the stadium, showing the basis of Russian history in the Prince Igor story.
Roles.
"Note:"
Synopsis.
"Time": The year 1185
"Place": The city of Putivl (prologue, Acts 1 and 4); a Polovtsian camp (Acts 2 and 3)
Note: "As discussed in this article, Borodin's final decision on the order of the first two acts is unclear. The traditional grouping presented here is that of the Rimsky-Korsakov-Glazunov edition. In many productions, Act 3 is omitted."
Prologue.
"The cathedral square in Putivl"
Prince Igor is about to set out on a campaign against the Cumans/Polovtsy and their Khans who have previously attacked the Russian lands. The people sing his praise and that of his son, the other leaders and the army ("Chorus: "Glory to the beautiful Sun""). A solar eclipse takes place to general consternation. Two soldiers Skula and Yeroshka desert feeling sure that Vladimir Yaroslavich, Prince Galitsky, will offer them work more to their liking. Although Yaroslavna, Igor's wife, takes the eclipse for a bad omen, Igor insists that honour demands that he go to war. He leaves her to the care of her brother, Prince Galitsky, who tells of his gratitude to Igor for sheltering him after he was banished from his own home by his father and brothers. The people sing a great chorus of praise ("Chorus: "Glory to the multitude of stars"") as the host sets out on their campaign against the Polovtsy.
Act 1.
"Scene 1: Vladimir Galitsky's court in Putivl"
Galitsky's followers sing his praise. Skula and Yeroshka are now working as gudok-players. They entertain the followers and all sing of how Galitsky and his men abducted a young woman and how she pleaded to be allowed to return to her father without being dishonoured. The prince arrives and sings of how, if he were Prince of Putivl, he would drink and feast all day while dispensing judgment and have the prettiest maidens with him all night ("Galitsky's Song"). The treasury would be spent on himself and his men while his sister would be praying in a monastery. A group of young women beg the prince to restore their abducted friend. He threatens them and drives them away, saying how she now lives in luxury in his quarters and does not have to work. The prince returns to his rooms having sent for wine for his followers. The gudok players and the prince's followers mock the women. They wonder what might happen if Yaroslavna hears of what happens, but then realise she would be helpless with all her men gone to war. They sing of how they are all drunkards and are supported by Galitsky. The men decide to go to the town square to declare Galitsky the Prince of Putivl, leaving just the two drunk musicians behind.
"Scene 2: A room in Yaroslavna's palace"
Yaroslavna is alone worrying about why she has not heard from Igor and his companions ("Yaroslavna's Arioso"). She sings of her tearful nights and nightmares and reminisces about when she was happy with Igor by her side. The nurse brings in the young women who tell Yaroslavna of their abducted friend. They are reluctant at first to reveal the culprit but eventually name Galitsky and talk of how he and his drunken followers cause trouble around Putivl. Galitsky enters and the women run away. Yaroslavna questions him as to the truth of their story and he mocks her saying she should treat him as a guest in her house. She threatens him with what Igor will do on his return, but Galitsky replies that he can seize the throne whenever he wants. Yaroslavna accuses him of repeating the betrayal that he carried out against their father, but he replies that he was only joking and asks if she has a lover now her husband is away. She threatens him with sending him back to their father. He replies that he will return the girl but will take another later and leaves. The council of boyars arrive to inform Yaroslavna that the Polovtsy under Khan Gzak are about to attack Putivl. Igor's army has been utterly destroyed and he has been wounded and captured with his son and brother. After a moment of faintness, Yaroslavna orders messengers sent to the city's allies, but the Boyars report that the roads are cut, some towns are in revolt and their princes will be captured. The Boyars say that they will organise the defence but Galitsky returns with his followers to demand that a new Prince be chosen. His retinue say it should be him as he is Yaroslavna's brother and Igor's brother-in-law. The boyars refuse. The argument is interrupted by the sight of flames and the sound of crying women. Some of the boyars flee; some join the battle, others guard the Princess. They call the attack God's judgment.
Act 2.
"Evening in the Polovtsian Camp"
Polovtsian maidens sing comparing love to a flower that droops in the heat of the day and is revived by night. They dance together ("Dance of the Polovtsian Maidens"). Konchakovna joins in the singing hoping that her own lover will join here soon ("Konchakovna's Cavatina"). The Russian prisoners arrive from their day's work and express their gratitide when fed by Konchakovna and the maidens. Their guards retire for the night leaving just Ovlur, a Christian, in charge. Vladimir, son of Igor, sings of his hope that his love will soon join him now that the day is fading ("Vladimir's Cavatina"). His love is Konchakovna. She comes and the two sing of their love and their desire to marry ("Love Duet"). While her father will consent to the marriage, they know that his will not. They part when they hear Igor coming. He sings of his disgrace and torment at being captured with his followers dead ("Prince Igor's Aria"). Only his wife, he feels, will be loyal. He hopes for the chance to regain his honour. Ovlur urges Igor to escape and the prince agrees to think about it. Khan Konchak asks him if all is well ("Konchak's Aria") and he replies that the falcon cannot live in captivity. Konchak says that as Igor did not ask for mercy he is not a prisoner but an honoured guest equal to a Khan. Igor reminds him that he too knows what it is to be a captive. Konchak offers Igor freedom if he will promise not to wage war on him again, but he refuses saying he cannot lie. Konchak regrets that they were not born to be allies. They would then have captured all of Russia. He summons the Polovtsian slaves to entertain Igor and himself and offers Igor his choice of them. As the slaves dance the Polovtsy sing of Konchak's glory ("Polovtsian Dances").
Act 3.
"The Polovtsian camp"
The Polovtsian army returns in triumph singing the praise of Khan Gzak ("Polovtsian March"). Konchak sings of the sack of Putivl and other victories and confidently predicts that they will soon capture all of Russia. Igor and his son Vladimir have their worst fears confirmed by the new captives. Vladimir and the other prisoners urge Igor to escape, but he is at first reluctant, singing of his shame and saying that it is the duty of the other Russian princes to save the homeland ("Igor's Monologue", Mariinsky edition only). Ovlur now arrives to say that he has prepared horses for Igor and Vladimir and Igor now agrees to escape. The distressed Konchakovna comes, challenging Vladimir to show his love by either taking her with him or by staying. Igor urges his son to come, but Vladimir feels unable to leave Konchakovna who threatens to wake the camp. Eventually Igor flees alone and Konchakovna sounds the alarm. She and her father refuse to let the Polovtsy kill Vladimir. Instead Konchak orders the death of the guards and marries Vladimir to his daughter. As for Igor, Konchak thinks more of him for his escape.
Act 4.
"Dawn in Putivl"
Yaroslavna weeps at her separation from Igor and the defeat of his army, blaming the very elements themselves for helping the enemy ("Yaroslavna's Lament"). Peasant women blame not the wind but Khan Gzak for the devastation. As Yaroslavna looks around to acknowledge the destruction, she sees two riders in the distance who turn out to be Igor and Ovlur. The two lovers sing of their joy of being reunited and of the expectation that Ivan will lead the Russians to victory against the Khan. Unaware of Igor's return, Skula and Yeroshka, the drunken gudok players, sing a song that mocks him. Then they notice him in the distance. After a moment of panic about what will happen to them, Skula says that they should rely on their cunning and decides on a plan that will save them. They ring the church bells to summon a crowd. Although people at first treat them with suspicion, the gudok players manage to convince the crowd that Igor has returned and the boyars that they are loyal followers of the true prince and not Galitsky. All joyously celebrate Igor's return.
Principal arias and numbers.
Prologue
Act 1
Act 2
Act 3
Act 4
Both the Overture to "Prince Igor" and the "Polovtsian Dances" (from Act II) are well-known concert standards. Together with the "Polovtsian March", they form the so-called "suite" from the opera.
Critical analysis.
"Prince Igor" is a staple of Russian opera, but has not travelled well abroad. One obvious reason is the Russian language, although translation into Italian was once a solution.
Another explanation for the failure to gain acceptance is its lack of unity resulting from its unfinished state. Despite the skill and efforts of editors Rimsky-Korsakov and Glazunov, the opera is still episodic and dramatically static, a problem of which the composer himself was aware when he embarked on composition (see quote above in "Composition History"). This is partly a consequence of Borodin's failure to complete a libretto before beginning composition of the music—the same problem that plagued his colleague Mussorgsky in the composition of "Khovanshchina". Both composers wrote their librettos piece by piece while composing the music, both lost sight of the overall narrative thread of their operas, and both wound up with pages and pages of music that needed to be sacrificed to assemble a cohesive whole. Also, both died before finishing their operas, leaving the task of completion, editing, and orchestration to Rimsky-Korsakov.
Performance practice.
One of the main considerations when performing "Prince Igor" is the question of whether to include Act 3, much of which was composed by Glazunov. The practice of omitting it was mentioned as early as 1909 in Rimsky-Korsakov's memoirs. Many productions leave Act 3 out because it "fails to carry conviction both musically and dramatically."
On the other hand, maintaining the act has certain benefits. It contains some fine pages (e.g., the "Polovtsian March"), provides an important link in the narrative (Igor's escape, Vladimir's fate), and is the origin of some of the memorable themes first heard in the overture (the trio, brass fanfares). Fortunately, the option of omitting the fine overture, also known to have been composed by Glazunov, is seldom considered.
Recently, the question of the best sequence of scenes in which to perform the opera has gained some prominence. Borodin did not complete a libretto before composing the music to "Prince Igor". The opera has traditionally been performed in the edition made by Rimsky-Korsakov and Glazunov. It will be obvious that the positions to which they assigned the Prologue, Act 3, and Act 4 cannot be changed if the story is to make sense. However, because the events of Act 1 and Act 2 overlap and are independent of one another, Act 2 may just as well precede Act 1 without any loss of coherence. Soviet musicologists Pavel Lamm and Arnold Sokhor reported the existence of a written plan (now in Glinka's Musical Culture Museum, Moscow), in Borodin's hand, that specified this sequence of scenes:
Sokhor assessed the plan as not written later than 1883.
The 1993 recording of "Prince Igor" by Valery Gergiev with the Kirov Opera features a new edition of the score with additions commissioned from composer Yuri Faliek for a production at the Mariinsky Theatre, adopting this hypothetical original sequence. The authors of the notes to the recording assert that this order better balances the musical structure of the score by alternating the acts in the Russian and Polovtsian settings with their distinctive musical atmospheres.
Despite this justification, there is reason to maintain the traditional sequence. Act II contains most of the numbers for which the work is known today, with Igor's brooding and impassioned aria ("Oh give me freedom") at the center, flanked by Vladimir's cavatina and Konchak's aria, not to mention the rousing conclusion provided by the "Polovtsian Dances." Moving its wealth of arias and dances from the center of the work to near the beginning may weaken the opera's structure.
The "Mariinsky edition" makes other important changes and additions to the score. Although much of the material composed or orchestrated by Glazunov and Rimsky-Korsakov is retained, there are additions culled from the unpublished vocal score by Pavel Lamm, orchestrated and linked by Faliek. The changes include:
In the West, the opera has often been given in languages other than Russian. For example, the 1960 recording under Lovro von Matačić is sung in German, the 1964 recording under Armando La Rosa Parodi is in Italian and the 1982 David Lloyd-Jones recording is in English. On the other hand, the 1990 Bernard Haitink and the 1962 Oscar Danon recordings are Western performances sung in Russian.
Recordings.
This is a list of studio recordings. A comprehensive list of all recordings of "Prince Igor" may be found at 
Audio
Video
Popular culture.
In the American musical "Kismet" (1953), most of the score was adapted from works by Borodin. "Prince Igor" was no exception; the "Dance of the Polovtsians" was used extensively and the "Gliding Dance of the Maidens" provided the melody for the popular hit song "Stranger in Paradise"
In "The Simpsons" episode "Simpson Tide", the Boyar's Chorus (Act 1, Scene 2) plays while tanks emerge from parade floats during a peace parade on Red Square in front of Saint Basil's, soldiers walk out of a building, the Berlin Wall re-erects itself out of the ground, and Lenin rises from his grave, saying "Rrr! Must Crush Capitalism, Rrr!"
References.
Notes
Sources

</doc>
<doc id="39145" url="http://en.wikipedia.org/wiki?curid=39145" title="The Abduction of Figaro">
The Abduction of Figaro

The Abduction of Figaro is a comic opera, described as "A Simply Grand Opera by P. D. Q. Bach," which is actually the work of composer Peter Schickele. It is a parody of opera in general, and the title is a play on two operas by Wolfgang Amadeus Mozart: "The Abduction from the Seraglio" and "The Marriage of Figaro". Those two operas, "Così fan tutte", and "Don Giovanni", as well as Gilbert and Sullivan's "The Pirates of Penzance" are among the core inspirations for the piece.
Schickele was commissioned to "discover" this opera by the Minnesota Opera, where the piece premiered on April 27 and 28, 1984. In addition to parodying Mozart, the music incorporates diverse influences and musical quotes, from traditional camp songs like "Found a Peanut" to popular songs like "Macho Man" by the Village People. The opera has been released on VHS and DVD.
Synopsis of Scenes.
<poem>
1. Introductory remarks by Prof. Peter Schickele
2. Opening credits
3. Overture
Act One: A town on the seacoast of Spain or Italy or somewhere
Scene One: Figaro's bedroom in the palace of Count Almamater
4. Introduction: "Found a peanut!"
5. Recitative: "Ah, dear husband"
    Aria: "Stay with me"
6. Recitative: "Suzanna"
7. Recitative: "Dog!"
    Aria: "Perfidy, thy name is Donald"
8. Recitative: "I am distraught"
9. Quartet: "Love is gone"
Scene Two: A courtyard of the palace
10. Recitative: "Well, here we are"
11. Aria: "Behold, fair maiden"
12. Recitative: "Just a moment"
      Duet: "Thy lofty tree"
Scene Three: Figaro's bedroom
13. Recitative: "And here is my husband"
14. Recitative: "Hold it!"
      Aria: "My name is Captain Kadd"
15. Recitative: "Now that you've heard"
16. Sextet: "What a downer!"
Scene Four: The courtyard
17. Recitative: "Schlepporello"
      Aria: "No man"
Scene Five: At the dock
18. Recitative: "What a strange turn of events"
19. Quintet and chorus: "Ah, though we must part"
20. Act I Finale
21. Introduction to Act II
Act Two: Somewhere in the Turkish Empire
Scene One: At the seashore
22. Duet: "God be praised"
Scene Two: In front of the Pasha's palace
23. Aria: "Fish gotta swim"
24. Chorus: "Hey, make way"
25. Dance of the Seven Pails
26. Recitative: "Your immenseness"
27. Duet, chorus, and dialogue: "Who is the highest"
28. Quartet: "May I introduce"
Scene Three: A courtyard of the palace
29. Dialogue and recitative: "Why?"
      Aria and dialogue: "Macho, macho"
30. Cavatina and dialogue: "You can beat me"
31. Act II Finale
Act Three: A tropical forest
32. Ballet
33. Trio and dialogue: "A magic forest"
34. Duet and dialogue: "I am a swineherd"
35. Finale (Part I) and dialogue
36. Aria and dialogue: "Why, oh why"
37. Finale (Part II)
38. Curtain calls and closing credits
39. Closing remarks by Prof. Schickele
</poem>
DVD Bonus Selections

</doc>
<doc id="39147" url="http://en.wikipedia.org/wiki?curid=39147" title="Finite difference">
Finite difference

A finite difference is a mathematical expression of the form "f"("x" + "b") − "f"("x" + "a"). If a finite difference is divided by "b" − "a", one gets a difference quotient. The approximation of derivatives by finite differences plays a central role in finite difference methods for the numerical solution of differential equations, especially boundary value problems.
Certain recurrence relations can be written as difference equations by replacing iteration notation with finite differences.
Today, the term "finite difference" is often taken as synonymous with finite difference approximations of derivatives, especially in the context of numerical methods. Finite difference approximations are finite difference quotients in the terminology employed above.
Finite differences have also been the topic of study as abstract self-standing mathematical objects, e.g. in works by George Boole (1860), L. M. Milne-Thomson (1933), and (1939), tracing its origins back to Isaac Newton. In this viewpoint, the formal calculus of finite differences is an alternative to the calculus of infinitesimals.
Forward, backward, and central differences.
Three forms are commonly considered: forward, backward, and central differences.
A forward difference is an expression of the form
Depending on the application, the spacing "h" may be variable or constant. When omitted, "h" is taken to be 1: formula_2.
A backward difference uses the function values at "x" and "x" − "h", instead of the values at "x" + "h" and "x":
Finally, the central difference is given by
Relation with derivatives.
The derivative of a function f at a point x is defined by the limit
If h has a fixed (non-zero) value instead of approaching zero, then the right-hand side of the above equation would be written
Hence, the forward difference divided by h approximates the derivative when h is small. The error in this approximation can be derived from Taylor's theorem. Assuming that f is differentiable, we have
The same formula holds for the backward difference:
However, the central (also called centered) difference yields a more accurate approximation. If f is twice differentiable,
The main problem with the central difference method, however, is that oscillating functions can yield zero derivative. If for n odd, and for n even, then "f ' " ("nh")=0 if it is calculated with the central difference scheme. This is particularly troublesome if the domain of f is discrete.
Authors for whom finite differences mean finite difference approximations define the forward/backward/central differences as the quotients given in this section (instead of employing the definitions given in the previous section).
Higher-order differences.
In an analogous way one can obtain finite difference approximations to higher order derivatives and differential operators. For example, by using the above central difference formula for "f ' "("x"+"h"/2) and "f ' "("x"−"h"/2) and applying a central difference formula for the derivative of f ' at x, we obtain the central difference approximation of the second derivative of f:
2nd order central
Similarly we can apply other differencing formulas in a recursive manner.
2nd order forward
More generally, the n-th order forward, backward, and central differences are given by, respectively,
Forward
or for h=1,
Backward
Central
These equations are using binomial coefficients after the summation sign shown as formula_16. Each row of Pascal's triangle provides the coefficient for each value of i.
Note that the central difference will, for odd n, have h multiplied by non-integers. This is often a problem because it amounts to changing the interval of discretization. The problem may be remedied taking the average of formula_17 and formula_18.
Forward differences applied to a sequence are sometimes called the binomial transform of the sequence, and have a number of interesting combinatorial properties.
Forward differences may be evaluated using the Nörlund–Rice integral. The integral representation for these types of series is interesting, because the integral can often be evaluated using asymptotic expansion or saddle-point techniques; by contrast, the forward difference series can be extremely hard to evaluate numerically, because the binomial coefficients grow rapidly for large n.
The relationship of these higher-order differences with the respective derivatives is straightforward,
Higher-order differences can also be used to construct better approximations. As mentioned above, the first-order difference approximates the first-order derivative up to a term of order h. However, the combination
approximates "f"'("x") up to a term of order "h"2. This can be proven by expanding the above expression in Taylor series, or by using the calculus of finite differences, explained below.
If necessary, the finite difference can be centered about any point by mixing forward, backward, and central differences.
Arbitrarily sized kernels.
Using a little linear algebra, one can fairly easily construct approximations, which sample an arbitrary number of points to the left and a (possibly different) number of points to the right of the center point, for any order of derivative. This involves solving a linear system such that the Taylor expansion of the sum of those points, around the center point, well approximates the Taylor expansion of the desired derivative.
This is useful for differentiating a function on a grid, where, as one approaches the edge of the grid, one must sample fewer and fewer points on one side.
The details are outlined in these .
Finite difference methods.
An important application of finite differences is in numerical analysis, especially in numerical differential equations, which aim at the numerical solution of ordinary and partial differential equations respectively. The idea is to replace the derivatives appearing in the differential equation by finite differences that approximate them. The resulting methods are called finite difference methods.
Common applications of the finite difference method are in computational science and engineering disciplines, such as thermal engineering, fluid mechanics, etc.
Newton's series.
The Newton series consists of the terms of the Newton forward difference equation, named after Isaac Newton; in essence, it is the Newton interpolation formula, first published in his "Principia Mathematica" in 1687, namely the discrete analog of the continuum Taylor expansion,
which holds for any polynomial function "f" and for most (but not all) analytic functions. Here, the expression
is the binomial coefficient, and
is the "falling factorial" or "lower factorial", while the empty product ("x")0 is defined to be 1. In this particular case, there is an assumption of unit steps for the changes in the values of "x", "h" = 1 of the generalization below.
Note also the formal correspondence of this result to Taylor's theorem. Historically, this, as well as the Chu–Vandermonde identity, 
(following from it, and corresponding to the binomial theorem), are included in the observations which matured to the system of the umbral calculus.
To illustrate how one may use Newton's formula in actual practice, consider the first few terms of doubling the Fibonacci sequence f = 2, 2, 4, ... One can find a polynomial that reproduces these values, by first computing a difference table, and then substituting the differences which correspond to "x"0 (underlined) into the formula as follows,
For the case of nonuniform steps in the values of "x", Newton computes the divided differences,
the series of products,
and the resulting polynomial is the scalar product, formula_29 .
In analysis with p-adic numbers, Mahler's theorem states that the assumption that "f" is a polynomial function can be weakened all the way to the assumption that "f" is merely continuous.
Carlson's theorem provides necessary and sufficient conditions for a Newton series to be unique, if it exists. However, a Newton series will not, in general, exist.
The Newton series, together with the Stirling series and the Selberg series, is a special case of the general difference series, all of which are defined in terms of suitably scaled forward differences.
In a compressed and slightly more general form and equidistant nodes the formula reads
Calculus of finite differences.
The forward difference can be considered as a difference operator, which maps the function "f" to Δ"h"["f" ]. This operator amounts to
where "T""h" is the shift operator with step "h", defined by "T""h"["f" ]("x") = "f"("x"+"h"), and "I" is the identity operator.
The finite difference of higher orders can be defined in recursive manner as Δ"h""n" ≡ Δ"h" (Δ"h""n"−1). Another equivalent definition is .
The difference operator Δ"h" is a linear operator and it satisfies a special Leibniz rule indicated above,
. Similar statements hold for the backward and central differences.
Formally applying the Taylor series with respect to "h", yields the formula
where "D" denotes the continuum derivative operator, mapping "f" to its derivative "f"'. The expansion is valid when both sides act on analytic functions, for sufficiently small "h". Thus, , and formally inverting the exponential yields 
This formula holds in the sense that both operators give the same result when applied to a polynomial.
Even for analytic functions, the series on the right is not guaranteed to converge; it may be an asymptotic series. However, it can be used to obtain more accurate approximations for the derivative. For instance, retaining the first two terms of the series yields the second-order approximation to "f’"("x") mentioned at the end of the section "Higher-order differences".
The analogous formulas for the backward and central difference operators are
The calculus of finite differences is related to the umbral calculus of combinatorics. This remarkably systematic correspondence is due to the identity of the commutators of the umbral quantities to their continuum analogs ("h"→0 limits),
A large number of formal differential relations of standard calculus involving 
functions "f"("x") thus "map systematically to umbral finite-difference analogs" involving "f"("xT"h−1).
For instance, the umbral analog of a monomial "x"n is a generalization of the above falling factorial (Pochhammer k-symbol), 
so that 
hence the above Newton interpolation formula (by matching coefficients in the expansion of an arbitrary function "f"("x") in such symbols), and so on.
For example, the umbral sine is
As in the continuum limit, the eigenfunction of Δ"h" /"h" also happens "to be an exponential",
and hence "Fourier sums of continuum functions are readily mapped to umbral Fourier sums faithfully", i.e., involving the same Fourier coefficients multiplying these umbral basis exponentials. This umbral exponential thus amounts to the exponential generating function of the Pochhammer symbols.
Thus, for instance, the Dirac delta function maps to its umbral correspondent, the cardinal sine function,
and so forth. Difference equations can often be solved with techniques very similar to those for solving differential equations.
The inverse operator of the forward difference operator, so then the umbral integral, is the indefinite sum or antidifference operator.
Rules for calculus of finite difference operators.
Analogous to rules for finding the derivative, we have:
All of the above rules apply equally well to any difference operator, including formula_42 as to formula_43.
Generalizations.
where formula_52 is its coefficients vector. An infinite difference is a further generalization, where the finite sum above is replaced by an infinite series. Another way of generalization is making coefficients formula_53 depend on point formula_54 : formula_55, thus considering weighted finite difference. Also one may make step formula_56 depend on point formula_54 : formula_58. Such generalizations are useful for constructing different modulus of continuity.
Finite difference in several variables.
Finite differences can be considered in more than one variable. They are analogous to partial derivatives in several variables.
Some partial derivative approximations are (using central step method):
Alternatively, for applications in which the computation of f is the most costly step, and both first and second derivatives must be computed, a more efficient formula for the last case is
since the only values to be computed which are not already needed for the previous four equations are "f"("x"+"h", "y"+"k") and "f"("x"−"h", "y"−"k").

</doc>
<doc id="39149" url="http://en.wikipedia.org/wiki?curid=39149" title="Frederic W. H. Myers">
Frederic W. H. Myers

Frederic William Henry Myers (6 February 1843, in Keswick, Cumberland – 17 January 1901, in Rome) was a poet, classicist, philologist, and a founder of the Society for Psychical Research. Myers' work on psychical research and his ideas about a "subliminal self" were influential in his time, but have not been accepted by the scientific community.
Early life.
Myers was the son of Revd Frederic Myers (1811–1851) and his second wife Susan Harriet Myers "nee" Marshall (1811–1896). He was a brother of poet Ernest Myers (1844–1921) and of Dr. Arthur Thomas Myers (1851–1894). His maternal grandfather was the wealthy industrialist John Marshall (1765–1845).
Myers was educated at Cheltenham College and at Trinity College, Cambridge, where he received a B.A. in 1865, and university prizes, including the Bell, Craven, Camden and Chancellor's Medal, though he was forced to resign the Camden medal for 1863 after accusations of plagiarism. He was a Fellow of Trinity College from 1865 to 1874 and college lecturer in classics from 1865 to 1869. In 1872 be became an Inspector of schools.
In 1867, Myers published a long poem, "St Paul", which became popular. The poem included the words of the hymn "Hark what a sound, and too divine for hearing". This was followed in 1882 by "The Renewal of Youth and Other Poems". He also wrote books of literary criticism, in particular "Wordsworth" (1881) and "Essays, Classical and Modern" (in two volumes, 1883), which included an essay on Virgil.
Personal life.
As a young man, Myers was a homosexual. He was involved in homosexual relationships with Arthur Sidgwick and the poet John Addington Symonds. He later fell in love with the medium Annie Eliza, the wife of his cousin Walter James Marshall and they had an affair. Myer's relationship with his cousin's wife was described as sexual. Annie committed suicide in September 1876 by drowning.
The British occult writer Richard Cavendish wrote "According to his own statement, he [Myers] had very strong sexual inclinations, which he indulged. These would seem to have been mainly homosexual in his youth, but in later life he was wholly heterosexual." In 1880, Myers married Eveleen Tennant (1856–1937), daughter of Charles Tennant and Gertrude Tennant. They had two sons, the elder the novelist Leopold Hamilton Myers (1881–1944), and a daughter. English author Ronald Pearsall wrote that Myers had sexual interests in the young lady mediums that he investigated. The researcher Trevor H. Hall argued that Myers had an affair with the medium Ada Goodrich Freer. However, Trevor Hamilton dismissed this and suggested that Freer was simply using her acquaintance with Myers to gain status in the psychical research movement.
Psychical research.
Myers was interested in psychical research and was one of the founder members of the Society for Psychical Research (SPR) in 1883. He became the President in 1900. Myers has been described as an "important early depth psychologist" who influenced William James, Pierre Janet, Théodore Flournoy and Carl G. Jung.
In the late 19th century Douglas Blackburn and George Albert Smith were endorsed as genuine psychics by Myers and Edmund Gurney. Smith even became an SPR member himself and the private secretary to the Honorary Secretary Gurney from 1883 to 1888. However, Blackburn later confessed to fraud. Blackburn called Gurney and Myers a "couple of credulous spiritualists" and wrote "we resolved that we should be doing the world a service by fooling them to the top of their bent, and then showing how easy a matter it was to 'take in' scientific observers."
Myers' 1884 essay "Visible Apparitions" with Gurney claimed a "personal experience" by a retired Judge Edmund Hornby involving a visitation from a spirit was true, but Joseph McCabe wrote that the story was a "jumble of inaccuracies" and "Sir E. Hornby was compelled to admit, that the story was entirely untrue."
Myers was the co-author of the two-volume "Phantasms of the Living" (London: Trübner, 1886) with Gurney and Frank Podmore which listed alleged sightings of apparitions. Myers speculated on the existence of a deep region of the unconscious (collective unconscious) or what he termed the “subliminal self”, which he believed could account for paranormal events. He also proposed the existence of a “metetherial world,” a world of images lying beyond the physical world. He wrote that apparitions are not hallucinations but have a real existence in the metetherial world which he described as a dream-like world. Myers’ belief that apparitions occupied regions of physical space and had an objective existence was in opposition to the views of his co-authors Gurney and Podmore who wrote apparitions were telepathic hallucinations.
The two-volume "Phantasms of the Living" was criticized by scholars for the lack of written testimony and the time elapsed between the occurrence and the report of it being made. Some of the reports were analyzed by the German hallucination researcher Edmund Parish (1861–1916) who concluded they were evidence for a dream state of consciousness, not the paranormal. Charles Sanders Peirce wrote a long criticism of the book arguing that no scientific conclusion could be reached from anecdotes and stories of unanalyzed phenomena. Alexander Taylor Innes attacked the book due to the stories lacking evidential substantiation in nearly every case. According to Innes the alleged sightings of apparitions were unreliable as they rested upon the memory of the witnesses and no contemporary documents had been produced, even in cases where such documents were alleged to exist. The psychologist C. E. M. Hansel noted that the stories in "Phantasms of the Living" were not backed up by any corroborating evidence. Hansel concluded "none of the stories investigated has withstood critical examination."
In 1893 Myers wrote a small collection of essays, "Science and a Future Life". In 1903, after Myers's death, "Human Personality and Its Survival of Bodily Death" was compiled and published. This work comprises two large volumes at 1,360 pages in length and presents an overview of Myers's research into the unconscious mind. Myers believed that a theory of consciousness must be part of a unified model of mind which derives from the full range of human experience, including not only normal psychological phenomena but also a wide variety of abnormal and "supernormal" phenomena.
Eusapia Palladino.
Myers with Charles Richet, Oliver Lodge, and Julian Ochorowicz investigated the medium Eusapia Palladino in the summer of 1894 at Richet's house in the Ile Roubaud in the Mediterranean. Myers and Richet claimed furniture moved during the séance and that some of the phenomena was the result of a supernatural agency. However, Richard Hodgson claimed there was inadequate control during the séances and the precautions described did not rule out trickery. Hodgson wrote all the phenomena "described could be accounted for on the assumption that Eusapia could get a hand or foot free." Lodge, Myers and Richet disagreed, but Hodgson was later proven correct in the Cambridge sittings as Palladino was observed to have used tricks exactly the way he had described them.
In July 1895, Palladino was invited to England to Myers' house in Cambridge for a series of investigations into her mediumship. According to reports by Hodgson, Myers and Oliver Lodge, all the phenomena observed in the Cambridge sittings were the result of trickery. Her fraud was so clever, according to Myers, that it "must have needed long practice to bring it to its present level of skill."
In the Cambridge sittings the results proved disastrous for her mediumship. During the séances Palladino was caught cheating in order to free herself from the physical controls of the experiments. Palladino was found liberating her hands by placing the hand of the controller on her left on top of the hand of the controller on her right. Instead of maintaining any contact with her, the observers on either side were found to be holding each other's hands and this made it possible for her to perform tricks. Richard Hodgson had observed Palladino free a hand to move objects and use her feet to kick pieces of furniture in the room. Because of the discovery of fraud, the British SPR investigators such as Henry Sidgwick and Frank Podmore considered Palladino's mediumship to be permanently discredited and because of her fraud she was banned from any further experiments with the SPR in Britain.
In the "British Medical Journal" on November 9, 1895 an article was published titled "Exit Eusapia!". The article questioned the scientific legitimacy of the SPR for investigating Palladino a medium who had a reputation of being a fraud and imposture. Part of the article read "It would be comic if it were not deplorable to picture this sorry Egeria surrounded by men like Professor Sidgwick, Professor Lodge, Mr. F. H. Myers, Dr. Schiaparelli, and Professor Richet, solemnly receiving her pinches and kicks, her finger skiddings, her sleight of hand with various articles of furniture as phenomena calling for serious study." This caused Henry Sidgwick to respond in a published letter to the "British Medical Journal", November 16, 1895. According to Sidgwick SPR members had exposed the fraud of Palladino at the Cambridge sittings, Sidgwick wrote "Throughout this period we have continually combated and exposed the frauds of professional mediums, and have never yet published in our Proceedings, any report in favour of the performances of any of them." The response from the Journal questioned why the SPR wastes time investigating phenomena that are the "result of jugglery and imposture" and not urgently concerning the welfare of mankind.
In 1898, Myers was invited to a series of séances in Paris with Charles Richet. In contrast to the previous séances in which he had observed fraud he claimed to have observed convincing phenomena. Sidgwick reminded Myers of Palladino's trickery in the previous investigations as "overwhelming" but Myers did not change his position. This enraged Richard Hodgson, then editor of SPR publications to ban Myers from publishing anything on his recent sittings with Palladino in the SPR journal. Hodgson was convinced Palladino was a fraud and supported Sidgwick in the "attempt to put that vulgar cheat Eusapia beyond the pale." It wasn't until the 1908 sittings in Naples that the SPR reopened the Palladino file.

</doc>
<doc id="39155" url="http://en.wikipedia.org/wiki?curid=39155" title="ActiveX Data Objects">
ActiveX Data Objects

In computing, Microsoft's ActiveX Data Objects (ADO) comprises a set of Component Object Model (COM) objects for accessing data sources. A part of MDAC (Microsoft Data Access Components), it provides a middleware layer between programming languages and OLE DB (a means of accessing data stores, whether databases or not, in a uniform manner). ADO allows a developer to write programs that access data without knowing how the database is implemented; developers must be aware of the database for connection only. No knowledge of SQL is required to access a database when using ADO, although one can use ADO to execute SQL commands directly (with the disadvantage of introducing a dependency upon the type of database used).
Microsoft introduced ADO in October 1996, positioning the software as a successor to Microsoft's earlier object layers for accessing data sources, including RDO (Remote Data Objects) and DAO (Data Access Objects).
ADO is made up of four collections and twelve objects.
Basic usage.
Some basic steps are required in order to be able to access and manipulate data using ADO :
ASP example.
Here is an ASP example using ADO to select the "Name" field, from a table called "Phonebook", where a "PhoneNumber" was equal to "555-5555". 
This is equivalent to the following ASP code, which uses plain SQL instead of the functionality of the Recordset object:
Software support.
ADO is supported in ASP, Delphi, PowerBuilder, and in Visual Basic for Applications (VBA). ADO support has now been added to dBase Plus 8 (With ADO)
Legacy.
ADO.NET has replaced ADO in the same way that C#/.NET replaced C/Win32 as the primary mode for targeting Windows application development. ADO.NET follows the same design pattern as ADO, enabling an ADO developer an easy path forward when moving to the .NET framework.

</doc>
<doc id="39158" url="http://en.wikipedia.org/wiki?curid=39158" title="Superior">
Superior

Superior may refer to:
Superiority.
Superiority may refer to:

</doc>
<doc id="39159" url="http://en.wikipedia.org/wiki?curid=39159" title="Mace (club)">
Mace (club)

A mace is a blunt weapon, a type of club or virge that uses a heavy head on the end of a handle to deliver powerful blows. A mace typically consists of a strong, heavy, wooden or metal shaft, often reinforced with metal, featuring a head made of stone, copper, bronze, iron, or steel.
The head of a military mace can be shaped with flanges or knobs to allow greater penetration of plate armour. The length of maces can vary considerably. The maces of foot soldiers were usually quite short (two or three feet, or seventy to ninety centimetres). The maces of cavalrymen were longer and thus better suited for blows delivered from horseback. Two-handed maces could be even larger.
Maces are rarely used today for actual combat, but a large number of government bodies (for instance the British House of Commons, the U.S. Congress), universities and other institutions have ceremonial maces and continue to display them as symbols of authority. They are often paraded in academic, parliamentary or civic rituals and processions.
Prehistory.
The mace was developed during the Upper Paleolithic from the simple club, by adding sharp spikes of flint or obsidian.
In Europe, an elaborately carved ceremonial flint mace head was one of the artifacts discovered in excavations of the Neolithic mound of Knowth in Ireland, and Bronze Age archaeology cites numerous finds of perforated mace heads.
In ancient Ukraine, stone mace heads were first used nearly eight millennia ago. The others known were disc maces with oddly formed stones mounted perpendicularly to their handle. The Narmer Palette shows a king swinging a mace. See the articles on the Narmer Macehead and the Scorpion Macehead for examples of decorated maces inscribed with the names of kings.
The problem with early maces was that their stone heads shattered easily and it was difficult to fix the head to the wooden handle reliably. The Egyptians attempted to give them a disk shape in the predynastic period (about 3850–3650 B.C.) in order to increase their impact and even provide some cutting capabilities, but this seems to have been a short-lived improvement.
A rounded pear form of mace head known as a "piriform" replaced the disc mace in the Naqada II period of pre-dynastic Upper Egypt (3600–3250 B.C.) and was used throughout the Naqada III period (3250-3100 B.C.). Similar mace heads were also used in Mesopotamia around 2450–1900 B.C. The Assyrians used maces probably about nineteenth century B.C. and in their campaigns; the maces were usually made of stone or marble and furnished with gold or other metals, but were rarely used in battle unless fighting heavily armoured infantry. 
An important, later development in mace heads was the use of metal for their composition. With the advent of copper mace heads, they no longer shattered and a better fit could be made to the wooden club by giving the eye of the mace head the shape of a cone and using a tapered handle.
The Shardanas or warriors from Sardinia who fought for Ramses II against the Hittities were armed with maces consisting of wooden sticks with bronze heads. Many bronze statuettes of the times show Sardinian warriors carrying swords, bows and original maces.
Antiquity.
The usage of maces in warfare is also described in the ancient Indian epics "Ramayana" and "Mahabarata". Unique types of maces known as "Gada" were used extensively in ancient Indian warfare, and the enchanted talking mace Sharur made its first appearance in Sumerian/Akkadian mythology during the epic of Ninurta.
The ancient Romans did not make wide use of maces, probably because of the influence of armour, and due to the nature of the Roman infantry fighting style which involved the pilum (or spear) and the gladius (short sword used in a stabbing fashion). The use of a heavy swinging-arc weapon in the well-disciplined tight formations of the Roman infantry would not have been practical.
Persians used a variety of maces. One simple explanation is the mode of Persian warfare. Unlike Romans, Persians fielded large numbers of heavily armoured and armed cavalry (see cataphracts). For a heavily armed Persian knight, a mace was as effective as a sword or battle axe. In fact, Shahnameh has countless references to heavily armoured knights facing each other using mace, axe, or swords.
European Middle Ages.
During the Middle Ages metal armour such as mail protected against the blows of edged weapons and blocked arrows and other projectiles. Solid metal maces and war hammers proved able to inflict damage on well armoured knights, as the force of a blow from a mace is great enough to cause damage without penetrating the armour. Though iron became increasingly common, copper and bronze were also used, especially in iron poor areas. The Sami for example continued to use bronze for maces as a cheaper alternative to iron or steel swords.
One example of a mace capable of penetrating armour is the flanged mace. The flanges (protruding edges of metal) allow it to dent or penetrate even the thickest armour. This variation of the mace did not become popular until significantly after knobbed maces. Although there are some references to flanged maces (bardoukion) as early as the Byzantine empire c. 900 it is commonly accepted that the flanged mace did not become popular in Europe until the 12th century, when it was simultaneously developed in Russia and Mid-West Asia.
Maces, being simple to make, cheap, and straightforward in application, were quite common weapons. Examples found in museums are often highly decorated.
It is popularly believed that maces were employed by the clergy in warfare to avoid shedding blood ("sine effusione sanguinis"). The evidence for this is sparse and appears to derive almost entirely from the depiction of Bishop Odo of Bayeux wielding a club-like mace at the Battle of Hastings in the Bayeux Tapestry, the idea being that he did so to avoid either shedding blood or bearing the arms of war. The fact that his brother Duke William carries a similar item suggests that, in this context, the mace may have been simply a symbol of authority. Certainly, other Bishops were depicted bearing the arms of a knight without comment, such as Archbishop Turpin who bears both a spear and a sword named "Almace" in The Song of Roland or Bishop Adhemar of Le Puy, who also appears to have fought as a knight during the First Crusade, an expedition that Odo also joined.
Eastern Europe.
Maces were very common in eastern Europe, especially medieval Poland, Ukraine. Eastern European maces often had pear shaped heads. These maces were also used by the Moldavian king Stephen the Great in some of his wars (see Bulawa).
The Pernach was a type of flanged mace developed since the 8th century BC in the region of Kievan Rus', and later widely used throughout the whole of Europe. The name comes from the Slavic word "pero" ("перо") meaning feather, reflecting the form of pernach that resembled a fletched arrow. Pernachs were the first form of the flanged mace to enjoy a wide usage. It was well suited to penetrate plate armour and chain mail. In the later times it was often used as a symbol of power by the military leaders in Eastern Europe.
Pre-Columbian America.
The cultures of pre-Columbian America used clubs and maces extensively. The warriors of the Moche state and the Inca Empire used maces with bone, stone or copper heads and wooden shafts.
Indian Subcontinent.
Mace as exercise equipment.
Indian akharas (combat training gymnasiums) often use use a heavy stone gada as a part of their training.
Modern era.
Trench raiding clubs used during World War I were modern variations on the medieval mace. They were homemade mêlée weapons used by both the Allies and the Central Powers. Clubs were used during nighttime trench raiding expeditions as a quiet and effective way of killing or wounding enemy soldiers.
Makeshift maces were also found in the possession of some football hooligans in the 1980s.
Ceremonial use.
Maces have had a role in ceremonial practices over time, including some still in use today.
Parliamentary maces.
Ceremonial maces are important in many parliaments following the Westminster system. They are carried in by the sergeant-at-arms or some other mace-bearers and displayed on the clerks' table while parliament is in session to show that a parliament is fully constituted. They are removed when the session ends. The mace is also removed from the table when a new speaker is being elected to show that parliament is not ready to conduct business.
Ecclesiastical maces.
The ceremonial mace is a short, richly ornamented staff often made of silver, the upper part of which is furnished with a knob or other head-piece and decorated with a coat of arms. The ceremonial mace was commonly borne before eminent ecclesiastical corporations, magistrates, and academic bodies as a mark and symbol of jurisdiction.
Parade maces.
Maces are also used as a parade item, rather than a tool of war, notably in military bands. Specific movements of the mace from the Drum Major will signal specific orders to the band he or she leads. The mace can signal anything from a step-off to a halt, from the commencement of playing to the cut off.
University maces.
University maces are employed in a manner similar to Parliamentary maces. They symbolize the authority and independence of a chartered university and the authority vested in the provost. They are typically carried in at the beginning of a convocation ceremony and are often less than half a meter high.
Heraldic use.
Like many weapons from feudal times, maces have been used in heraldic military blazons as either a charge on the shield or as external ornamentation.
Thus, in France:

</doc>
<doc id="39163" url="http://en.wikipedia.org/wiki?curid=39163" title="Toktar Aubakirov">
Toktar Aubakirov

Toktar Ongarbayuly Aubakirov (Kazakh: Тоқтар Оңғарбайұлы Әубәкіров, Russian: Токтар Онгарбаевич Аубакиров, born on 27 July 1946) is a retired Kazakh Air Force officer and a former cosmonaut.
Early life.
Toktar Aubakirov was born in Karkaraly region, Karagandinskaya Oblast, Kazakh SSR, which is now Kazakhstan. After graduating from the 8th grade of a secondary school he started working as a metal turner at the Temirtau foundry, whilst attending an evening school. In 1965 he joined the Armavir Military Aviation Institute of the Anti-Air Defence Pilots. He served as a fighter pilot in the Soviet Air Force on the Far East borders of the USSR until his acceptance into the M. Gromov Test Pilot's School in 1975.
Test pilot career.
Between 1976 and 1991 he served as a test pilot at the Mikoyan Experimental Design Bureau (MiG aircraft). During this time he tested over 50 types of aircraft. The first in the Soviet Union to make a nonstop flight crossing the North Pole and with two in-flight refueling, the first in the Soviet Union who took off of the aircraft-carrier "Tbilisi" on a MiG 29K.
Spaceflight experience.
In 1991, in accordance with an agreement between the governments of the USSR and the Kazakh SSR, started training at the Gagarin Cosmonaut Training Centern. On 2 October 1991 he launched with Russian cosmonaut Alexander Volkov as flight commander, and the Austrian research cosmonaut Franz Viehböck in Soyuz TM-13 from the Baikonur Cosmodrome spaceport, and spent over eight days in space.
Career.
Since 1993, he has been the general director of the National Aerospace Agency of Republic of Kazakhstan. He was a member of the Kazakhstan parliament. Now he is a pensioner and consultant.
Family.
Toktar Aubakirov is married to Tatyana Aubakirova. They have two children: Timur (born in 1977) and Mikhail (born in 1982).

</doc>
<doc id="39170" url="http://en.wikipedia.org/wiki?curid=39170" title="Flash">
Flash

Flash may refer to:

</doc>
<doc id="39172" url="http://en.wikipedia.org/wiki?curid=39172" title="Clustering">
Clustering

Clustering can refer to the following:
In demographics:
In graph theory:
In statistics and data mining:
In computing:

</doc>
<doc id="39173" url="http://en.wikipedia.org/wiki?curid=39173" title="Commonwealth Games">
Commonwealth Games

The Commonwealth Games (known as the British Empire Games from 1930–1950, the British Empire and Commonwealth Games from 1954–1966, and British Commonwealth Games from 1970–1974) is an international, multi-sport event involving athletes from the Commonwealth of Nations. The event was first held in 1930, and, with the exception of 1942 and 1946, which were cancelled due to World War II, has taken place every four years since then.
The games are overseen by the Commonwealth Games Federation (CGF), which also controls the sporting programme and selects the host cities. A host city is selected for each edition. 18 cities in seven countries have hosted the event. Apart from many Olympic sports, the games also include some sports that are played predominantly in Commonwealth countries, such as lawn bowls and netball.
Although there are 53 members of the Commonwealth of Nations, 71 teams participate in the Commonwealth Games, as a number of dependent territories compete under their own flag. The four Home Nations of the United Kingdom—England, Scotland, Wales, and Northern Ireland—also send separate teams. Only six countries have attended every Commonwealth Games: Australia, Canada, England, New Zealand, Scotland, and Wales. Australia has been the highest achieving team for twelve games, England for seven, and Canada for one.
History of the Games.
A sporting competition bringing together the members of the British Empire was first proposed by the John Astley Cooper in 1891, when he wrote an article in "The Times" suggesting a "Pan-Britannic-Pan-Anglican Contest and Festival every four years as a means of increasing goodwill and good understanding of the British Empire". The John Astley Cooper Committees worldwide (e.g. Australia) helped Pierre de Coubertin to get his international Olympic Games off the ground fast. In 1911, the Festival of the Empire was held at The Crystal Palace in London to celebrate the coronation of King George V. As part of the festival, an Inter-Empire Championships was held in which teams from Australia, Canada, South Africa, and the United Kingdom competed in events such as boxing, wrestling, swimming, and athletics.
In 1928, Melville Marks Robinson of Canada was asked to organise the first British Empire Games; these were held in 1930, in Hamilton, Ontario, and women competed in the swimming events only. From 1934, women also competed in some athletics events.
The first Commonwealth Paraplegic Games were held alongside the Commonwealth Games from 1962 to 1974. Athletes with a disability were then first included in exhibition events at the 1994 Commonwealth Games in Victoria, British Columbia, and, at the 2002 Commonwealth Games, they were included as full members of their national teams, making them the first fully inclusive international multi-sport games. This meant that results were included in the medal count.
The Empire Games flag was donated in 1931 by the British Empire Games Association of Canada. The year and location of subsequent games were added until the 1950 games. The name of the event was changed to the British Empire and Commonwealth Games and the flag was retired as a result.
Editions of the Games.
The first edition of the event was the 1930 British Empire Games in which 11 nations participated. The quadrennial schedule of the games was interrupted by the Second World War and the 1942 Games (set to be held in Montreal) and the 1946 Games were abandoned. The games were revived in 1950 and underwent a name change four years later with the first British Empire and Commonwealth Games in 1954. Over 1000 athletes participated in the 1958 Games as over thirty teams took part for the first time.
The Edmonton event marked a new high as almost 1500 athletes from 46 countries took part.
Nigeria was the first country to boycott the Commonwealth Games in 1978 in protest over New Zealand's sporting contacts with South Africa. Participation at the 1986 Games was affected by a boycott by 32 African and Caribbean nations in protest to British Prime Minister Margaret Thatcher's refusal to condemn sporting contacts of Apartheid era South Africa in 1985, but the Games rebounded and continued to grow thereafter. The 1998 Commonwealth Games in Kuala Lumpur, Malaysia saw the sporting programme grow from 10 to 15 sports as team sports were allowed for the first time. Participation also reached new levels as over 3500 athletes represented 70 teams at the event. At the Games in Melbourne in 2006, over 4000 athletes took part in sporting competitions.
The three nations to have hosted the games the most times are Australia (5), Canada (4) and New Zealand (3). Furthermore, six editions have taken place in the countries within the United Kingdom (Scotland 3, England 2 and Wales 1), twice in Asia (Malaysia 1 and India 1) and once in the Caribbean (Jamaica 1). Two cities have held the games on multiple occasions: Auckland (1950 and 1990), and Edinburgh (1970, 1986 and some events in 2014).
1
2
Approved sports.
There are a total of 22 sports (with two multi-disciplinary sports) and a further seven para-sports which are approved by the Commonwealth Games Federation. They are categorised into three types. Core sports must be included on each programme. A number of optional sports may be picked by the host nation, which may include some team sports such as basketball. Recognised sports are sports which have been approved by the CGF but which are deemed to need expansion; host nations may not pick these sports for their programme until the CGF's requirements are fulfilled.
Participation.
Only six teams have attended every Commonwealth Games: Australia, Canada, England, New Zealand, Scotland and Wales. Australia has been the highest scoring team for twelve games, England for seven and Canada for one.
"Notes:"
Commonwealth nations/dependencies/disputed territories yet to send teams.
Very few Commonwealth dependencies and nations have yet to take part:
Notable competitors.
Lawn bowler Willie Wood from Scotland was the first competitor to have competed in seven Commonwealth Games, from 1974 to 2002, a record equalled in 2014 by Isle of Man cyclist Andrew Roche. Also, Greg Yelavich, a sports shooter from New Zealand, has won 12 medals in seven games from 1986 to 2010.
Nauruan weightlifter Marcus Stephen won twelve medals at the Games between 1990 and 2002, of which seven gold, and was elected President of Nauru in 2007. His performance has helped place Nauru (the smallest independent state in the Commonwealth, at 21 km2 and with a population of fewer than 9,400 in 2011) in nineteenth place on the all-time Commonwealth Games medal table.

</doc>
<doc id="39176" url="http://en.wikipedia.org/wiki?curid=39176" title="Hartford (disambiguation)">
Hartford (disambiguation)

Hartford is the state capital of Connecticut in the USA and the largest city named "Hartford".
Hartford may also refer to:

</doc>
<doc id="39181" url="http://en.wikipedia.org/wiki?curid=39181" title="Cuneiform (disambiguation)">
Cuneiform (disambiguation)

Cuneiform (from the Latin word for "wedge-shaped") can refer to:

</doc>
<doc id="39183" url="http://en.wikipedia.org/wiki?curid=39183" title="ReiserFS">
ReiserFS

ReiserFS is a general-purpose, journaled computer file system formerly designed and implemented by a team at Namesys led by Hans Reiser. ReiserFS is currently supported on Linux (without quota support). Introduced in version 2.4.1 of the Linux kernel, it was the first journaling file system to be included in the standard kernel. ReiserFS is the default file system on the Elive, Xandros, Linspire, GoboLinux, and Yoper Linux distributions. ReiserFS was the default file system in Novell's SUSE Linux Enterprise until Novell decided to move to ext3 on October 12, 2006 for future releases.
Namesys considered ReiserFS (now occasionally referred to as Reiser3) stable and feature-complete and, with the exception of security updates and critical bug fixes, ceased development on it to concentrate on its successor, Reiser4. Namesys went out of business in 2008 after Hans Reiser was charged with the murder of his wife (and later convicted and sent to prison). However, volunteers continue to work on the open source project.
Features.
At the time of its introduction, ReiserFS offered features that had not been available in existing Linux file systems:
Performance.
Compared with ext2 and ext3 in version 2.4 of the Linux kernel, when dealing with files under 4 KiB and with tail packing enabled, ReiserFS may be faster. This was said to be of great benefit in Usenet news spools, HTTP caches, mail delivery systems and other applications where performance with small files is critical. However, in practice news spools use a feature called cycbuf, which holds articles in one large file; fast HTTP caches and several revision control systems use similar approach, nullifying these performance advantages. For email servers, ReiserFS was problematic due to semantic problems explained below. Also, ReiserFS had a problem with very fast filesystem aging when compared to other filesystems – in several usage scenarios filesystem performance decreased dramatically with time.
Before Linux 2.6.33, ReiserFS heavily used the big kernel lock (BKL) — a global kernel-wide lock — which does not scale very well for systems with multiple cores, as the critical code parts are only ever executed by one core at a time.
Criticism.
Some directory operations (including unlink(2)) are not synchronous on ReiserFS, which can result in data corruption with applications relying heavily on file-based locks (such as mail transfer agents qmail and Postfix) if the machine halts before it has synchronized the disk.
There are no programs to specifically defragment a ReiserFS file system, although tools have been written to automatically copy the contents of fragmented files hoping that more contiguous blocks of free space can be found. However, a "repacker" tool was planned for the next Reiser4 file system to deal with file fragmentation.
fsck.
The tree rebuild process of ReiserFS's fsck has attracted much criticism: if the file system becomes so badly corrupted that its internal tree is unusable, performing a tree rebuild operation may further corrupt existing files or introduce new entries with unexpected contents, but this action is not part of normal operation or a normal file system check and has to be explicitly initiated and confirmed by the administrator.
ReiserFS v3 images should not be stored on a ReiserFS v3 partition (e.g. backups or disk images for emulators) without transforming them (e.g., by compressing or encrypting) in order to avoid confusing the rebuild. Reformatting an existing ReiserFS v3 partition can also leave behind data that could confuse the rebuild operation and make files from the old system reappear. This also allows malicious users to intentionally store files that will confuse the rebuilder. As the metadata is always in a consistent state after a file system check, "corruption" here means that contents of files are merged in unexpected ways with the contained file system's metadata. The ReiserFS successor, Reiser4, fixes this problem.
Earlier issues.
ReiserFS in versions of the Linux kernel before 2.4.16 were considered unstable by Namesys and not recommended for production use, especially in conjunction with NFS.
Early implementations of ReiserFS (prior to that in Linux 2.6.2) were also susceptible to out-of-order write hazards. But the current journaling implementation in ReiserFS is now on par with that of ext3's "ordered" journaling level.
Move away from ReiserFS to ext3.
Jeff Mahoney of SUSE wrote a post on Sep 14 2006 proposing to move from ReiserFS to ext3 for the default installation file system. Some reasons he mentioned were scalability, "performance problems with extended attributes and ACLs", "a small and shrinking development community", and that "Reiser4 is not an incremental update and requires a reformat, which is unreasonable for most people." On October 4 he wrote a response comment on a blog in order to clear up some issues. He wrote that his proposal for the switch was unrelated to Reiser's "legal troubles" (i.e., Hans Reiser murdering his wife) Mahoney wrote he "was concerned that people would make a connection where none existed" and that "the timing is entirely coincidental and the motivation is unrelated."
On Oct 12, 2006, Novell officially announced that SUSE Linux Enterprise would switch from ReiserFS to ext3.
Design.
ReiserFS stores file metadata ("stat items"), directory entries ("directory items"), inode block lists ("indirect items"), and tails of files ("direct items") in a single, combined B+ tree keyed by a universal object ID. Disk blocks allocated to nodes of the tree are "formatted internal blocks". Blocks for leaf nodes (in which items are packed end-to-end) are "formatted leaf blocks". All other blocks are "unformatted blocks" containing file contents. Directory items with too many entries or indirect items which are too long to fit into a node spill over into the right leaf neighbour. Block allocation is tracked by free space bitmaps in fixed locations.
By contrast, ext2 and other Berkeley FFS-like file systems of that time simply used a fixed formula for computing inode locations, hence limiting the number of files they may contain. Most such file systems also store directories as simple lists of entries, which makes directory lookups and updates linear time operations and degrades performance on very large directories. The single B+ tree design in ReiserFS avoids both of these problems due to better scalability properties.

</doc>
<doc id="39184" url="http://en.wikipedia.org/wiki?curid=39184" title="NTFS">
NTFS

NTFS (New Technology File System) is a proprietary file system developed by Microsoft. Starting with Windows NT 3.1, it is the default file system of Windows NT family.
NTFS has several technical improvements over FAT and HPFS (High Performance File System), the file systems that it superseded, such as improved support for metadata, and the use of advanced data structures to improve performance, reliability, and disk space utilization, plus additional extensions, such as security access control lists (ACL) and file system journaling.
History.
In the mid-1980s, Microsoft and IBM formed a joint project to create the next generation of graphical operating system. The result of the project was OS/2, but Microsoft and IBM disagreed on many important issues and eventually separated: OS/2 remained an IBM project and Microsoft worked on Windows NT. The OS/2 file system HPFS contained several important new features. When Microsoft created their new operating system, they borrowed many of these concepts for NTFS. Probably as a result of this common ancestry, HPFS and NTFS use the same disk partition identification type code (07). Using the same ID is unusual, since there were dozens of available codes, and other major file systems have their own code. FAT has more than nine (one each for FAT12, FAT16, FAT32, etc.). Algorithms identifying the file system in a partition type 07 must perform additional checks.
Developers.
NTFS developers include:
Versions.
Microsoft has released five versions of NTFS:
The NTFS.sys version number (e.g. v5.0 in Windows 2000) should not be confused with the NTFS format version number (v3.1 since Windows XP).
Windows Vista implemented Transactional NTFS, NTFS symbolic links, partition shrinking and self-healing. All except NTFS symbolic links are operating system's features. Windows Vista also introduced persistent shadow copies for use with System Restore and Previous Versions features. Persistent shadow copies, however, are deleted when an older operating system mounts that NTFS volume. This happens because the older operating system does not understand the newer format of persistent shadow copies.
Features.
NTFS v3.0 includes several new features over its predecessors: sparse file support, disk usage quotas, reparse points, distributed link tracking, and file-level encryption, also known as the Encrypting File System (EFS).
Scalability.
In theory, the maximum NTFS volume size is 264−1 clusters. However, the maximum NTFS volume size as implemented in Windows XP Professional is 232−1 clusters partly due to partition table limitations. For example, using 64 kB clusters, the maximum Windows XP NTFS volume size is 256 TBs minus 64 KBs. Using the default cluster size of 4 kB, the maximum NTFS volume size is 16 TB minus 4 kB. (Both of these are vastly higher than the 128 GB limit lifted in Windows XP SP1.) Because partition tables on master boot record (MBR) disks only support partition sizes up to 2 TB, dynamic or GPT volumes must be used to create NTFS volumes over 2 TB. Booting from a GPT volume to a Windows environment requires a system with UEFI and 64-bit support.
The maximum theoretical file size on NTFS is 16 EB (16 × 10246 or 264 bytes) minus 1 kB or 18,446,744,073,709,550,592 bytes. With Windows 8 and Windows Server 2012, the maximum file size implemented is 256 TB minus 64 KB or 281,474,976,645,120 bytes.
NTFS supports a maximum cluster size of 64 kB.
Journaling.
NTFS is a journaling file system and uses the NTFS Log ($LogFile) to record metadata changes to the volume. It is a critical functionality of NTFS (a feature that FAT/FAT32 does not provide) for ensuring that its internal complex data structures (notably the volume allocation bitmap), or data moves performed by the defragmentation API, the modifications to MFT records (such as moves of some variable-length attributes stored in MFT records and attribute lists), and indices (for directories and security descriptors) will remain consistent in case of system crashes, and allow easy rollback of uncommitted changes to these critical data structures when the volume is remounted.
The USN Journal (Update Sequence Number Journal) is a system management feature that records (in $Extend$UsnJrnl) changes to files, streams and directories on the volume, as well as their various attributes and security settings. The journal is made available for applications to track changes to the volume. This journal can be enabled or disabled on non-system volumes.
Hard links.
Hard links allows different file names to refer to the same file contents.
Hard links are similar to directory junctions, but refer to files instead. Hard links may link to files in the same volume only because each volume has its own MFT. Hard links have their own file metadata, so a change in file size or attributes under one hard link may not update the others until they are opened.
Hard links were originally included to support the POSIX subsystem in Windows NT.
Windows uses hard links to support Short (8.3) filenames in NTFS. Operating system support is needed because there are legacy applications that can work only with 8.3 filenames. In this case, an additional filename record and directory entry is added, but both 8.3 and long file name are linked and updated together, unlike a regular hard link.
Alternate data streams (ADS).
Alternate data streams allow more than one data stream to be associated with a filename, using the format "filename:streamname" (e.g., "text.txt:extrastream").
NTFS Streams were introduced in Windows NT 3.1, to enable Services for Macintosh (SFM) to store resource forks. Although current versions of Windows Server no longer include SFM, third-party Apple Filing Protocol (AFP) products (such as GroupLogic's ExtremeZ-IP) still use this feature of the file system. Very small ADS (called Zone.Identifier) are added by Internet Explorer and recently by other browsers to mark files downloaded from external sites as possibly unsafe to run; the local shell would then require user confirmation before opening them. When the user indicates that they no longer want this confirmation dialog, this ADS is deleted.
Alternate streams are not listed in Windows Explorer, and their size is not included in the file's size. They are ignored when the file is copied or moved to another file system without ADS support, attached to an e-mail, or uploaded to a website. Thus, using alternate streams for critical data may cause problems. Microsoft provides a tool called Streams to view streams on a selected volume. Starting with Windows PowerShell 3.0, it is possible to manage ADS natively with seven cmdlets: Add-Content, Clear-Content, Get-Content, Get-Item, Out-String, Remove-Item, Set-Content.
Malware has used alternate data streams to hide code. As a result, malware scanners and other special tools now check for alternate data streams.
File compression.
NTFS can compress files using LZNT1 algorithm (a variant of the LZ77). Files are compressed in 16-cluster chunks. With 4 kB clusters, files are compressed in 64 kB chunks. The compression algorithms in NTFS are designed to support cluster sizes of up to 4 kB. When the cluster size is greater than 4 kB on an NTFS volume, NTFS compression is not available. If the compression reduces 64 kB of data to 60 kB or less, NTFS treats the unneeded 4 kB pages like empty sparse file clusters—they are not written. This allows for reasonable random-access times as the OS just has to follow the chain of fragments. However, large compressible files become highly fragmented since every chunk < 64KB becomes a fragment. Single-user systems with limited hard disk space can benefit from NTFS compression for small files, from 4 kB to 64 kB or more, depending on compressibility. Files less than 900 bytes or so are stored within the directory entry at the MFT.
Flash memory, such as SSD drives do not have the head movement delays of hard disk drives, so fragmentation has only small effects. Users of fast multi-core processors will find improvements in application speed by compressing their applications and data as well as a reduction in space used. Note that SSDs with Sandforce controllers already compress data. However, since less data is transferred, there is a reduction in I/Os.
The best use of compression is for files that are repetitive, seldom written, usually accessed sequentially, and not themselves compressed. Log files are an ideal example.
Compressing system files needed at boot time, like drivers, NTLDR, winload.exe, or BOOTMGR may prevent the system from booting correctly, as compression filters are not available then. However, in later editions of Windows, compression of important system files is disallowed.
Files may be compressed or decompressed individually (via changing the advanced attributes) for a drive, directory, or directory tree, becoming a default for the files inside.
Although read–write access to compressed files is mostly transparent, Microsoft recommends avoiding compression on server systems and/or network shares holding roaming profiles because it puts a considerable load on the processor. Since many fragments are created for compressible files, defragmentation may take longer.
Sparse files.
Sparse files are files interspersed with empty segments for which no actual storage space is used. To the applications, the file looks like an ordinary file with empty regions seen as regions filled with zeros.
Database applications, for instance, may use sparse files. As with compressed files, the actual sizes of sparse files are not taken into account when determining quota limits.
Volume Shadow Copy.
The Volume Shadow Copy Service (VSS) keeps historical versions of files and folders on NTFS volumes by copying old, newly overwritten data to shadow copy via copy-on-write technique. The user may later requests an earlier version to be recovered. This also allows data backup programs to archive files currently in use by the file system. On heavily loaded systems, Microsoft recommends setting up a shadow copy volume on a separate disk.
Transactions.
As of Windows Vista, applications can use Transactional NTFS to group changes to files together into a transaction. The transaction will guarantee that all changes happen, or none of them do, and it will guarantee that applications outside the transaction will not see the changes until they are committed.
It uses similar techniques as those used for Volume Shadow Copies (i.e. copy-on-write) to ensure that overwritten data can be safely rolled back, and a CLFS log to mark the transactions that have still not been committed, or those that have been committed but still not fully applied (in case of system crash during a commit by one of the participants).
Transactional NTFS does not restrict transactions to just the local NTFS volume, but also includes other transactional data or operations in other locations such as data stored in separate volumes, the local registry, or SQL databases, or the current states of system services or remote services. These transactions are coordinated network-wide with all participants using a specific service, the DTC, to ensure that all participants will receive same commit state, and to transport the changes that have been validated by any participant (so that the others can invalidate their local caches for old data or rollback their ongoing uncommitted changes). Transactional NTFS allows, for example, the creation of network-wide consistent distributed filesystems, including with their local live or offline caches.
Encryption.
Encrypting File System (EFS) provides strong and user-transparent encryption of any file or folder on an NTFS volume. EFS works in conjunction with the EFS service, Microsoft's CryptoAPI and the EFS File System Run-Time Library (FSRTL).
EFS works by encrypting a file with a bulk symmetric key (also known as the File Encryption Key, or FEK), which is used because it takes a relatively small amount of time to encrypt and decrypt large amounts of data than if an asymmetric key cipher is used. The symmetric key that is used to encrypt the file is then encrypted with a public key that is associated with the user who encrypted the file, and this encrypted data is stored in an alternate data stream of the encrypted file. To decrypt the file, the file system uses the private key of the user to decrypt the symmetric key that is stored in the file header. It then uses the symmetric key to decrypt the file. Because this is done at the file system level, it is transparent to the user. Also, in case of a user losing access to their key, support for additional decryption keys has been built into the EFS system, so that a recovery agent can still access the files if needed. NTFS-provided encryption and NTFS-provided compression are mutually exclusive; however, NTFS can be used for one and a third-party tool for the other.
The support of EFS is not available in Basic, Home and MediaCenter versions of Windows, and must be activated after installation of Professional, Ultimate and Server versions of Windows or by using enterprise deployment tools within Windows domains.
Quotas.
Disk quotas were introduced in NTFS v3. They allow the administrator of a computer that runs a version of Windows that supports NTFS to set a threshold of disk space that users may use. It also allows administrators to keep track of how much disk space each user is using. An administrator may specify a certain level of disk space that a user may use before they receive a warning, and then deny access to the user once they hit their upper limit of space. Disk quotas do not take into account NTFS's transparent file-compression, should this be enabled. Applications that query the amount of free space will also see the amount of free space left to the user who has a quota applied to them.
Reparse points.
NTFS reparse points, introduced in NTFS v3, are used by associating a reparse tag in the user space attribute of a file or directory. When the object manager (see Windows NT line executive) parses a file system name lookup and encounters a reparse attribute, it will "reparse" the name lookup, passing the user controlled reparse data to every file system filter driver that is loaded into Windows. Each filter driver examines the reparse data to see whether it is associated with that reparse point, and if that filter driver determines a match, then it intercepts the file system call and executes its special functionality.
Resizing.
Starting with Windows Vista Microsoft added the built-in ability to shrink or expand a partition, but this capability is limited because it will not relocate page file fragments or files that have been marked as unmovable. So shrinking will often require relocating or disabling any page file, the index of Windows Search, and any Shadow Copy used by System Restore. Various third-party tools are capable of resizing NTFS partitions.
Internals.
Internally, NTFS uses B+ trees to index file system data. Although complex to implement, this allows faster file look up times in most cases. A file system journal is used to guarantee the integrity of the file system metadata but not individual files' content. Systems using NTFS are known to have improved reliability compared to FAT file systems.
NTFS allows any sequence of 16-bit values for name encoding (file names, stream names, index names, etc.) except 0x0000. This means UTF-16 code units are supported, but the file system does not check whether a sequence is valid UTF-16 (it allows any sequence of short values, not restricted to those in the Unicode standard). File names are limited to 255 UTF-16 code units. Certain names are reserved in the volume root directory and cannot be used for files. These are codice_1, codice_2, codice_3, codice_4, codice_5, codice_6 (dot), codice_7, codice_8, codice_9, codice_10, codice_11, and codice_12. (dot) and $Extend are both directories; the others are files. The NT kernel limits full paths to 32,767 UTF-16 code units. There are some additional restrictions on code points and file names.
Partition Boot Sector.
The OS first looks at the 8 bytes at 0x30 to find the cluster number of the $MFT, then multiplies that number by the number of sectors per cluster (1 byte found at 0x0D) and the number of bytes per sector (2 bytes found at 0x0b). This value is the byte offset to the $MFT, which is described below.
Master File Table.
In NTFS, all file, directory and metafile data—file name, creation date, access permissions (by the use of access control lists), and size—are stored as metadata in the Master File Table (MFT). This abstract approach allowed easy addition of file system features during Windows NT's development—an example is the addition of fields for indexing used by the Active Directory software. This also enables fast file search software such as Everything or Ultrasearch to locate named local files and folders included in the MFT very fast, without requiring any other index.
The MFT structure supports algorithms which minimize disk fragmentation. A directory entry consists of a filename and a "file ID", which is the record number representing the file in the Master File Table. The file ID also contains a reuse count to detect stale references. While this strongly resembles the W_FID of Files-11, other NTFS structures radically differ.
Two copies of the MFT are stored in case of corruption. If the first record is corrupted, NTFS reads the second record to find the MFT mirror file. Locations for both files are stored in the boot sector.
Metafiles.
NTFS contains several files that define and organize the file system. In all respects, most of these files are structured like any other user file ($Volume being the most peculiar), but are not of direct interest to file system clients. These metafiles define files, back up critical file system data, buffer file system changes, manage free space allocation, satisfy BIOS expectations, track bad allocation units, and store security and disk space usage information. All content is in an unnamed data stream, unless otherwise indicated.
These metafiles are treated specially by Windows and are difficult to directly view: special purpose-built tools are needed. One such tool is the nfi.exe ("NTFS File Sector Information Utility") that is freely distributed as part of the Microsoft "OEM Support Tools". For example to obtain information on the "$MFT"-Master File Table Segment the following command is used: codice_13
Attribute lists, attributes, and streams.
For each file (or directory) described in the MFT record, there's a linear repository of stream descriptors (also named "attributes"), packed together in one or more MFT records (containing the so-called "attributes list"), with extra padding to fill the fixed 1 KB size of every MFT record, and that fully describes the effective streams associated with that file.
Each attribute has an attribute type (a fixed-size integer mapping to an attribute definition in file $AttrDef), an optional attribute name (for example, used as the name for an alternate data stream), and a value, represented in a sequence of bytes. For NTFS, the standard data of files, the alternate data streams, or the index data for directories are stored as attributes.
According to $AttrDef, some attributes can be either resident or non-resident. The $DATA attribute, which contains file data, is such an example. When the attribute is resident (which is represented by a flag), its value is stored directly in the MFT record. Otherwise, clusters are allocated for the data, and the cluster location information is stored as data runs in the attribute.
All attributes of a given file may be displayed by using the nfi.exe ("NTFS File Sector Information Utility") that is freely distributed as part of the Microsoft "OEM Support Tools".
Windows system calls may handle alternate data streams. Depending on the operating system, utility and remote file system, a file transfer might silently strip data streams. A safe way of copying or moving files is to use the BackupRead and BackupWrite system calls, which allow programs to enumerate streams, to verify whether each stream should be written to the destination volume and to knowingly skip unwanted streams.
Resident vs. non-resident attributes.
To optimize the storage and reduce the I/O overhead for the very common case of attributes with very small associated value, NTFS prefers to place the value within the attribute itself (if the size of the attribute does not then exceed the maximum size of an MFT record), instead of using the MFT record space to list clusters containing the data; in that case, the attribute will not store the data directly but will just store an allocation map (in the form of "data runs") pointing to the actual data stored elsewhere on the volume. When the value can be accessed directly from within the attribute, it is called "resident data" (by computer forensics workers). The amount of data that fits is highly dependent on the file's characteristics, but 700 to 800 bytes is common in single-stream files with non-lengthy filenames and no ACLs.
The allocation map is stored in a form of "data runs" with compressed encoding. Each data run represents a contiguous group of clusters that store the attribute value. For files on a multi-GB volume, each entry can be encoded as 5 to 7 bytes, which means a 1 KB MFT record can store about 100 such data runs. However, as the $ATTRIBUTE_LIST also has a size limit, it is dangerous to have more than 1 million fragments of a single file on an NTFS volume, which also implies that it is in general not a good idea to use NTFS compression on a file larger than 10 GB.
The NTFS filesystem driver will sometimes attempt to relocate the data of some of the attributes that can be made non-resident into the clusters, and will also attempt to relocate the data stored in clusters back to the attribute inside the MFT record, based on priority and preferred ordering rules, and size constraints.
Since resident files do not directly occupy clusters ("allocation units"), it is possible for an NTFS volume to contain more files on a volume than there are clusters. For example, a 74.5 GB partition NTFS formats with 19,543,064 clusters of 4 KB. Subtracting system files (a 64 MB log file, a 2,442,888-byte Bitmap file, and about 25 clusters of fixed overhead) leaves 19,526,158 clusters free for files and indices. Since there are four MFT records per cluster, this volume theoretically could hold almost 4 × 19,526,158 = 78,104,632 resident files.
Opportunistic locks.
Opportunistic locks (oplocks) allow clients to alter their buffering strategy for a given file or stream in order to increase performance and reduce network use. Oplocks apply to the given open stream of a file and do not affect oplocks on a different stream.
Oplocks can be used to transparently access files in the background. A network client may avoid writing information into a file on a remote server if no other process is accessing the data, or it may buffer read-ahead data if no other process is writing data.
Windows supports four different types of oplocks:
Opportunistic locks have been enhanced in Windows 7 and Windows Server 2008 R2 with per-client oplock keys.
Time.
Windows NT and its descendants keep internal timestamps as UTC and make the appropriate conversions for display purposes. Therefore, NTFS timestamps are in UTC.
For historical reasons, the versions of Windows that do not support NTFS all keep time internally as local zone time, and therefore so do all file systems other than NTFS that are supported by current versions of Windows. This means that when files are copied or moved between NTFS and non-NTFS partitions, the OS needs to convert timestamps on the fly. But if some files are moved when daylight saving time (DST) is in effect, and other files are moved when standard time is in effect, there can be some ambiguities in the conversions. As a result, especially shortly after one of the days on which local zone time changes, users may observe that some files have timestamps that are incorrect by one hour. Due to the differences in implementation of DST in different jurisdictions, this can result in a potential timestamp error of up to 4 hours in any given 12 months.
Interoperability.
While the different NTFS versions are for the most part fully forward- and backward-compatible, there are technical considerations for mounting newer NTFS volumes in older versions of Microsoft Windows. This affects dual-booting, and external portable hard drives. For example, attempting to use an NTFS partition with "Previous Versions" (a.k.a. Volume Shadow Copy) on an operating system that does not support it will result in the contents of those previous versions being lost. A Windows command-line utility called convert.exe can convert supporting file systems to NTFS, including HPFS (only on Windows NT 3.1, 3.5, and 3.51), FAT16 and FAT32 (on Windows 2000 and later).
Mac OS X 10.3 and later include read-only support for NTFS-formatted partitions. The GPL-licensed NTFS-3G also works on Mac OS X through FUSE and allows reading and writing to NTFS partitions. A performance enhanced commercial version, called "Tuxera NTFS for Mac", is also available from the NTFS-3G developers. Paragon Software Group sells a read-write driver named "NTFS for Mac OS X", which is also included on some models of Seagate hard drives. Native NTFS write support has been discovered in Mac OS X 10.6 and later, but is not activated by default, although workarounds do exist to enable the functionality. However, user reports indicate the functionality is unstable and tends to cause kernel panics, probably the reason why write support has not been enabled or advertised.
Linux kernel versions 2.2.0 and later include the ability to read NTFS partitions; kernel versions 2.6.0 and later contain a driver written by Anton Altaparmakov (University of Cambridge) and Richard Russon which supports file read, overwrite and resize. Three userspace drivers (NTFSMount, NTFS-3G and Captive NTFS, a 'wrapping' driver that uses Windows' own driver, ntfs.sys) exist for NTFS support. They are built on the Filesystem in Userspace (FUSE), a Linux kernel module tasked with bridging userspace and kernel code to save and retrieve data. All three are licensed under the terms of the GNU General Public License (GPL). Due to the complexity of internal NTFS structures, both the built-in 2.6.14 kernel driver and the FUSE drivers disallow changes to the volume that are considered unsafe, to avoid corruption. Two proprietary solutions also exist: 
eComStation, and FreeBSD offer read-only NTFS support (there is a beta NTFS driver that allows write/delete for eComStation, but is generally considered unsafe). A free third-party tool for BeOS, which was based on NTFS-3G, allows full NTFS read and write. NTFS-3G also works on Mac OS X, FreeBSD, NetBSD, Solaris, QNX and Haiku, in addition to Linux, through FUSE. A free for personal use read/write driver for MS-DOS called "NTFS4DOS" also exists. Ahead Software developed a "NTFSREAD" driver (version 1.200) for DR-DOS 7.0x between 2002 and 2004. It was part of their Nero Burning ROM software. OpenBSD offer native read-only NTFS support by default on i386 and amd64 platforms as of version 4.9 released 1 May 2011. Read/write support through NTFS-3G are possible in OpenBSD -current as of 1 November 2013 (first release is OpenBSD 5.5 on 1 May 2014) as OpenBSD henceforth has its own FUSE implementation and NTFS-3G is available from ports.
Further reading.
</dl>

</doc>
<doc id="39185" url="http://en.wikipedia.org/wiki?curid=39185" title="Johan August Arfwedson">
Johan August Arfwedson

Johan August Arfwedson (12 January 1792 – 28 October 1841) was a Swedish chemist who discovered the chemical element lithium in 1817 by isolating it as a salt.
Life and work.
Arfwedson belonged to a wealthy bourgeois family, the son of the wholesale merchant and factory owner Jacob Arfwedson and his spouse, Anna Elisabeth Holtermann. The younger Arfwedson matriculated as a student at the University of Uppsala in 1803 (at the time, matriculating at a young age was common for aristocratic and wealthy students), completed a degree in Law in 1809 and a second degree in mineralogy in 1812. In the latter year, he received an unpaid position in the Royal Board of Mines, where he advanced to the position of notary (still without a salary) in 1814.
In Stockholm, Arfwedson knew the chemist Jöns Jakob Berzelius and received access to his private laboratory, where he discovered the element lithium in 1817, during analysis of the mineral petalite. The actual isolation of lithium metal would be done by others.
In 1818 and 1819, Arfwedson made a European journey, partly in the society of Berzelius. After coming home, Arfwedson built his own laboratory on his estate. He spent the larger part of his remaining life administering and multiplying his inherited wealth.
He was elected a member of the Royal Swedish Academy of Sciences in 1821.
The rare mineral arfvedsonite was named after him.

</doc>
<doc id="39186" url="http://en.wikipedia.org/wiki?curid=39186" title="Semi-automatic rifle">
Semi-automatic rifle

A semi-automatic rifle is a rifle that fires a single round each time the trigger is pulled. 
Operation.
Semi-automatic weapons use gas, blowforward, blowback, or recoil energy to eject the spent cartridge after the round has traveled down the barrel, chambers a new cartridge from its magazine, and resets the action; enabling another round to be fired once the trigger is depressed again.
The self-loading design was a successor to earlier rifles that required manual-cycling of the weapon after each shot, such as the bolt-action rifle or repeating rifles, which required the operator to manual cycle the action before each shot. The ability to automatically load the next round allowed for an increase in the rounds per minute the operator could fire. 
These rifles are also known as self-loading rifles ('SLR') or auto-loading rifles and are often mistaken for automatic rifles or machine guns. Self-loading rifles were one of the most revolutionary designs in the history of warfare. To name one example, semi-automatic weapons gave the United States an important edge in World War II, as the M1 Garand was a semi-automatic rifle issued to most soldiers, whereas the Axis powers only had bolt action weapons and limited quantities of semi-automatic rifles. Semi-automatic rifles are versatile designs. They can be efficiently fed by en-bloc clip and internal magazine, detachable magazine or a combination of stripper clip and internal magazine.

</doc>
<doc id="39189" url="http://en.wikipedia.org/wiki?curid=39189" title="Racquetball">
Racquetball

Racquetball is a racquet sport played with a hollow rubber ball in an indoor or outdoor court. Joseph Sobek is credited with inventing the modern sport of racquetball in 1950 in the US (the outdoor, one-wall game goes back to at least 1910 in N.Y.C.), adding a stringed racquet to paddleball in order to increase velocity and control. Unlike most racquet sports, such as tennis and badminton, there is no net to hit the ball over, and, unlike squash, no tin (out of bounds area at the bottom of front wall) to hit the ball above. Also, the court's walls, floor, and ceiling are legal playing surfaces, with the exception of court-specific designated hinders being out-of-bounds. 
Racquetball is very similar to 40×20 American handball. It is also very similar to the British sport racketball (see below for a comparison).
Joe Sobek is credited with inventing the sport of racquetball in the New Britain Connecticut YMCA, though not with naming it. A professional tennis and American handball player, Sobek sought a fast-paced sport that was easy to learn and play. He designed the first strung paddle, devised a set of rules, based on those of squash, handball, and paddleball, and named his game "paddle rackets".
In February 1952 Sobek founded the National Paddle Rackets Association (NPRA), codified the rules, and had them printed as a booklet. The new sport was rapidly adopted and became popular through Sobek's continual promotion of it; he was aided by the existence of some 40,000 handball courts in the country's YMCAs and Jewish Community Centers, wherein racquetball could be played.
In 1969, aided by Robert W. Kendler, the president-founder of the U.S. Handball Association (USHA), the International Racquetball Association (IRA) was founded using the name coined by Bob McInerney, a professional tennis player. That same year, the IRA assumed the national championship from the NPRA. In 1973, after a dispute with the IRA board of directors, Kendler formed two other racquetball organizations, yet the IRA remains the sport's dominant organization, recognized by the United States Olympic Committee as the American national racquetball governing body.
In 1974, the IRA organized the first professional tournament, and is a founding member of the International Racquetball Federation (IRF). Eventually, the IRA became the American Amateur Racquetball Association (AARA); in late 1995, it renamed itself as the United States Racquetball Association (USRA). In 2003, the USRA again renamed itself to USA Racquetball (USAR), to mirror other Olympic sports associations, even if Racquetball is not an Olympic sport.
Kendler used his publication "ACE" to promote both handball and racquetball. Starting in the 1970s, and aided by the fitness boom of that decade, the sport's popularity increased to an estimated 3.1 million players by 1974. Consequent to increased demand, racquetball clubs and courts were founded and built, and sporting goods manufacturers began producing racquetball-specific equipment. This growth continued until the early 1980s, and declining in the decade's latter part when racquet clubs converted to physical fitness clubs, in service to a wider clientele, adding aerobics exercise classes and physical fitness and bodybuilding machines. Since then, the number of racquetball players has remained steady, an estimated 5.6 million players.
United Kingdom.
In 1976, Ian D.W. Wright created the sport of racketball based on U.S. racquetball. British racketball is played in a 32 ft long by 21 ft wide squash court (8 ft shorter and 1 ft wider than the U.S. racquetball court), using a smaller, less dynamic ball than the American racquetball. In racketball, the ceiling is out-of-bounds. The racketball is served after a bounce on the floor then struck into play with the racket. Scoring is like squash with point-a-rally scoring of up to 11 points. The British Racketball Association was formed on 13 February 1984, and confirmed by the English Sports Council as the sport's governing body on 30 October 1984. The first National Racketball Championship was held in London on 1 December 1984. The sport is now played in countries where squash is played, Australia, Bermuda, France, Germany, Malaysia, the Netherlands, New Zealand, South Africa, Argentina, Ireland and Sweden. Currently, racketball also is played in parts of North America.
In 1988, the British Racketball Association merged with the English Squash Rackets Association. England Squash & Racketball is now recognised by Sport England as the English national governing body of the sports of squash and racketball. There is now an established UK Racketball Tournament Series consisting of 8 events around the UK, which forms the basis of the national rankings along with the National Racketball championships held annually at The Edgbaston Priory Club.
Governing bodies.
The International Racquetball Federation (IRF) governs the World Racquetball Championships, which were first held in 1981 in conjunction with the first World Games. The second World Championships were played in 1984, and since then have been held biennially in August. Players from the United States have won the most World Championship titles.
The IRF also runs the World Junior Racquetball Championships that occur annually in either late October, or early to mid November, as well as the annual World Senior Racquetball Championships for players who are 35 years of age or older.
Racquetball has been included in the World Games on five occasions: 1981, 1989, 1993, 2009 and 2013. Racquetball has been included in the Pan American Games in 1995, 1999, 2003, and 2011. Toronto will host the 2015 Pan American Games, and racquetball is on the program.
There are three professional racquetball organizations. The International Racquetball Tour (IRT) is the men's professional organization. IRT tournaments occur usually in the USA, with some events occurring in Mexico and Canada. The Ladies Professional Racquetball Tour (LPRT) is the women's professional organization, while the Classic Pro Racquetball Tour (CPRT) is for players 40 and older many of whom were pro players at younger ages.
Equipment.
This court and equipment are required for playing racquetball:
The "service box" is formed by the "short line" (a solid red line running the court's width parallel to the front and back walls at a distance of 20 feet) and the "service line" (which runs parallel to the short line and is 15 feet from the front wall). Within the service box there are two sets of lines perpendicular to the short and service lines.
One set of lines is 18 inches from, and parallel to, the side walls. Along with the short line, service line, and side wall these lines define the doubles box, where the non-serving doubles partner stands during the serve; 36 inches from the side wall is another set of lines which, along with the short line and the service line, define an area that the server must not enter if he wishes to hit a drive serve between himself and the nearest side wall. The "receiving line" is a parallel dashed line 5 feet behind the short line.
Other equipment needed:
Racquetball differs from other racquet sports as most competitive players wear a glove on their racquet hand for the purpose of getting a better grip on the racquet (similar to golfers using a glove when driving), but gloves are optional equipment. Also, players usually wear a comfortable short sleeved shirt and shorts, as well as racquetball court shoes designed for enabling quick lateral as well as forward and backward movement.
Ball Colors.
Racquetballs are manufactured in a variety of colors such as blue, green, purple, black, red, and pink, and some are for specific purposes (e.g., outdoor play and indoor play), but the differences are unlikely to make much difference for recreational play. Beginners are recommended to use a blue ball by Penn, Ektelon, or Wilson. The blue ball is the most commonly used and it is the most neutral ball for average speed and accuracy of contact. Green balls are similar to blue balls. In the USA the main choices of ball are blue and green for tournament play. In some cases the International Pro Racquetball Tour (IRT) will use a purple Penn HD ball as the official ball. A black ball is often used in tournaments for senior players because the ball is designed to be slower moving and allows for longer rallies. The red ball is the fastest in production and they are known as Red Ektelon Fireballs. This ball is heavier and allows for a quicker pace. 
Balls do break occasionally, and will lose their bounce over time even without breaking. To keep balls around for a long time it is best to keep them in a room temperature setting and keep them out of extreme cold or heat because this will cause the balls to become less effective and lose their bounce.
Rules.
Play begins with the serve. The serving player must bounce the ball on the floor once and hit it directly to the front wall, making the ball hit the floor beyond the short line; otherwise the serve counts as a fault. The ball may touch one side wall, but not two, prior to hitting the floor; hitting both side walls after the front wall (but before the floor) is a "three wall serve," and a fault. Also, serving the ball into the front wall so that it rebounds to the back wall without hitting the floor first is a long serve, and a fault.
Other fault serves include a ceiling serve, in which the ball touches the ceiling after the front wall, and serving before the receiving player is ready. Also, the server must wait until the ball passes the short line before stepping out of the service box, otherwise it is a fault serve.
If the server hits the ball directly to any surface other than the front wall the server immediately loses serve regardless of whether it was first or second serve.
After the ball bounces behind the short line, or passes the receiving line, the ball is in play and the opposing player(s) may play it.
Usually, the server is allowed two opportunities (called first serve and second serve) to put the ball into play (two serve rule), although elite level competitions often allow the server only one opportunity (one serve rule).
After a successful serve, players alternate hitting the ball against the front wall. The player returning the hit may allow the ball to bounce once on the floor or hit the ball on the fly. However, once the player returning the shot has hit the ball, it must strike the front wall before striking the floor. Unlike during the serve, a ball in play may touch as many walls, including the ceiling, as necessary so long as it reaches the front wall without striking the floor.
Hinders.
Due to the nature of the game, players often occupy the space their opponent(s) want(s) to occupy. This may result in a player blocking his opponent's ability to play the ball. Such occurrences are termed either hinders or penalty hinders. A hinder is a replay of the current rally (the server resumes play at the first serve), while a penalty hinder results in the player who caused the avoidable obstruction to lose the rally. A type of hinder is a screen in which the player is unable to see the ball prior to it passing the opponent.
The difference between a hinder and a penalty hinder (or formerly an avoidable hinder) is that in the latter case a player has missed out on a clear opportunity to make a rally-winning shot due to the obstruction by the player's opponent, while in the former case the opportunity missed would not clearly have led to a winning shot. This difference is almost always a judgment call by the referee (if available).
There is also a "court" hinder in which some part of the playing field caused the ball to bounce untrue. Often this is the door frame or (recessed) handle or a flaw in the floor or walls. In this case, the rally is a re-serve.
Under USA Racquetball rules, matches are best of three games with the first two games to 15 points and a third game to 11 points, if necessary. USA Racquetball rules do not require players to win by two, so a match score line could read 15–14, 14–15, 11–10. Racquetball Canada matches are also the best of three format, but require a winning margin of at least two points.
International competitions run by the International Racquetball Federation are like the USA Racquetball scoring system: two games to 15 with a tie-breaker to 11, if necessary, and win by one. However, the men's and women's pro tours play matches that are the best-of-five games to 11 points, requiring a two-point margin for victory.
Game variations.
Racquetball games can be played with two, three or four players, with doubles or singles matches being most common. Two player games are called singles or "one-up" (one vs. one for the entire game), while four player games are doubles with two pairs playing against each other (two vs. two for the entire game). Tournament competitions have divisions for singles or doubles or both.
Three-player games are most commonly called "Cut-throat" and sometimes "Ironman" (two-on-one for the entire game) where each player takes turns serving to the other two, who play as a team against the serving player. Another three-player game is "California", "In-and-Out", or "King of the Court" where play is 1 vs. 1 with the third player remaining in the back court out of play while the other two play a rally; the rally winner then serves to the player who was sitting out, and the rally loser stays out of play. Another three-player variation is "Sevens" in which one player plays against two players as a team, with the game being played to seven points; if the two player team gets to seven first, the game is over, but if the solo player gets to seven first then the game continues to 14; if the solo player again reaches 14 first, then the game continues to 21, where the game ends regardless of whether the solo player or the two player team reach 21 first.
Shots of the game.
Service.
Serve style varies drastically from player to player. Generally, they are divided into two types: offensive and defensive. Most players use an offensive serve for the first serve, and a defensive serve if they need to hit a second serve. Of the offensive serves, the most common is the drive. The intention with this serve is for the ball to travel low and fast towards either back corner, and to bounce twice before striking either side wall or the back wall. If the opponent is adjusting to the drive serve, the server will throw in any variety of jam serves.
A "jam serve" is an offensive serve which attempts to catch the opponent off balance by making use of difficult angles and unfrequented play space. The most common jam serve is the Z-serve, which strikes the front wall close to a side wall. The ball bounces quickly off the side wall, then strikes the floor and then the opposite side wall about 30–35 feet back. Depending upon the spin the server gives the Z-serve, the resulting carom may prove unpredictable and difficult to return. Side spin may cause the ball to bounce parallel to the back wall.
A "pinch serve" is similar to a drive serve; however, the ball strikes a side wall very low and close to the serving box. With the appropriate spin, the ball has little bounce, and is difficult to return. It is possible that a successful serve would strike the sidewall before the short line, and land on the floor after the short line.
If the player faults on the first serve, they will usually hit a defensive serve. Defensive serves do not usually garner aces, but they are designed to generate a weak return by the opponent, thereby setting up the server to win the point. Most defensive serves are any variety of lob serves. A plain lob serve is a ball hit with a long, high arch into either back corner. The goal is to hit the ball so that it lands as close as possible to the back wall, giving the opponent very little room to hit a solid return. A junk lob takes a shallower arch, and lands close to the side wall somewhere between the dotted line and the back wall. This lob is intended to deceive the opponent into thinking he has an easy kill. However, since the ball is in the deep zone, it will more likely set up the server for an offensive shot.
Offensive shots.
Straight-in shots are usually meant to hit the front wall as low as possible. If the ball contacts the front wall so low as to bounce twice before it reaches the service line it is called a "kill" shot. Straight-in shots are normally attempted with the idea of hitting toward the area of the court the opponent cannot cover. Straight-in shots hit where the opponent can't return them are called down-the-line and cross court passing shots. Often kill shots are returned very close to the back wall as the ball is moving towards the front wall.
Pinches and splats are shots that strike the side wall before the front wall. This often makes the ball bounce twice quickly to end the rally. Pinches normally strike the side wall towards the front part of the court, often within a few inches from the front wall.
The "splat" shot is an elongated pinch that strikes the side wall towards the back part of the court. It often makes a distinctive splatting sound. A very disorienting shot named the "rayjay splat" after Ray Johnson, a Wyoming state champion, who consistently used this shot by smashing the ball into the sidewall at such an angle that it would "Z" into the opposite front wall, arriving with such minimum momentum that it would "die" at the front wall and not rebound as expected. The best defense is to listen for the splat and anticipate the ball action. The advantage to a splat shot, beyond an unpredictable angle, is that it creates a longer distance to travel forward for the opponent who is held between a tension of going forward and staying back because of velocity of passing shots.
Pinches are classified as frontside or reverse. A right-handed player shooting a forehand shot to the right front corner is shooting a frontside pinch. A right-handed player shooting to the left front corner is a reverse pinch. A right-handed player shoots a backhand frontside pinch to the left corner and a reverse double pinch to the right corner. Everything for a left-handed player would be the opposite.
The dink is another very effective offensive shot designed to end the point. It is a shot very low to the front wall hit very softly so as to bounce twice before the opponent can get to it. Dinks are most effective when the opponent is positioned deep in the court.
Another important shot type is the "Z" shot. This shot is effective at confusing and tiring out your opponent. To hit a "Z" shot one hits the side wall hard and up high causing the ball to hit the front then the other side wall then back to the original side wall. If done correctly, the path of the ball will be Z shaped. This shot can have confusing bounces which can frustrate opponents. If done correctly, a "Z" shot will apply spin to the ball as well on the final bounce, causing it to rebound perpendicular to the second wall and fall parallel to the back wall, the closer the better. This makes the "Z" shot very difficult to return.
An interesting and surprise attack shot is the "CB Pinch", named after Charlie Beram, a Colorado state champion who is credited with this unique style. The CB Pinch occurs where the player responds to a ceiling shot quickly, in front of the service line and right after the floor bounce. The shot is basically a redirected floor bounce (knee to waist high), where the ball is directed very softly but quickly to either corner from a position in front of the service line. The fact that the shot is taken right after a ceiling shot-floor bounce usually leaves the opponent in the rear of the court defenseless against the CB Pinch, when the player has suddenly rushed forward to take the shot.
Defensive shots.
Defensive shots are defined as shots which are not returned low to the front wall.
The ceiling ball shot is the primary defensive shot. This is a shot that strikes the ceiling at or near the front wall. The ball will bounce once in the forecourt and should then travel in a high arc to arrive as close to, and as vertical to, the back wall as possible. Often this is aimed at the corner which would require a backhand return by the opponent. This makes it difficult for the opponent to return the ball as he cannot make a full arc of the racquet. However, if the ball comes down too long or too short of the back wall, this can allow the opponent a kill shot.
Another defensive shot is the high Z. Used when the defensive player is near the front wall, the high Z is hit ten feet high or higher into the front wall near a corner. The ball then bounces from the side wall all the way to the opposite side wall, usually traveling over the top of the opponent, hitting the opposite side wall with spin. The spin will cause the ball to leave the opposite wall almost perpendicular to it. This may confuse inexperienced opponents but importantly, if very close to and parallel to the rear wall, makes for a difficult return shot.
The "around the world" or "3 wall" defensive shot is hit like a pinch shot but high on the wall toward the ceiling. It travels around the court in a high trajectory and is an alternative to hitting a ceiling ball.
Two other defensive shots are used but are less effective. If the defensive player is in the backcourt but unable to position himself for a non-defensive shot, he may need to hit the ball off of the back wall. The ball often returns without much force and is easily returned. The round-the-world shot is hit high into the side wall first so the ball then hits the front wall and then the other side wall, effectively circling the court. It can be easily cut off and is rarely used anymore.
Strategy.
The primary strategy of racquetball is to command the center of the court just at or behind the dashed receiving line. This allows the player to move as quickly as possible to all areas of the court and limit open court areas which are difficult to defend. After a shot, return quickly to center court. The antithesis of this is to be against a wall which severely limits the player's movement and allows the opponent an open court.
Keep an eye on the opponent by glancing sideways to anticipate their return shot and move appropriately in the court. Learn the typical return shots of the opponent and move appropriately in the court for a return shot. Attempt to not be predictable with your return shots.
Other more obvious strategies are to keep the returned ball as low on the front wall as possible, keeping the ball moving fast (limiting reaction time) and to keep your opponent moving away from center court by the use of lobs, cross court shots, and dinks.
Major competitions and players.
Organized competitive racquetball began in the 1970s. The best male players of that era were Charlie Brumfield and Marty Hogan, as well as Bud Muehleisen, Jerry Hilecher, Steve Keeley, Davey Bledsoe, Steve Serot, and Steve Strandemo. Hogan continued to be a dominant player into the 1980s, and was rivaled on the scene by Brett Harnett, Dave Peck, and Mike Yellen.
In the 1990s, Ruben Gonzalez, Cliff Swain and Sudsy Monchik dominated pro tournaments, and other great players like Andy Roberts, John Ellis, and Drew Kachtik were often left out of the winner's circle. In the 2000s, Kane Waselenchuk, Jack Huczek, Jason Mannino, Ben Croft, and Rocky Carson have all excelled, but Waselenchuk has been dominant the last two seasons losing only once since September 2008.
The first great woman player was Peggy Steding in the 1970s. She was succeeded by Shannon Wright, who was then rivaled by Heather McKay, a great Australian squash player who made the transition to racquetball when living in Canada. McKay then developed a great rivalry with Lynn Adams, and after McKay moved back to Australia, Adams dominated women's racquetball for the better part of the 1980s.
The 1990s belonged to Michelle Gould (née Gilman) whose drive serve was a huge weapon against her opponents. In the late 1990s and into the 2000s, Jackie Paraiso and then Cheryl Gudinas were the dominant players. Then in the mid-2000s, Christie Van Hees and Rhonda Rajsich were the dominant players, but Paola Longoria finished #1 at the end of the 2008–2009 and 2009–2010 seasons.
US Open.
Held annually in October, the US Open is the most prestigious professional racquetball event. First held in 1996, the US Open was in Memphis, Tennessee until 2010, when it moved to Minneapolis, Minnesota. In men's play, Kane Waselenchuk (Canada) has won the most US Open titles with ten ahead of Sudsy Monchik (USA) with four, while Jason Mannino (USA) and Cliff Swain (USA) have both won the title twice, and Rocky Carson (USA) once.
In women's play, Paola Longoria (Mexico) has the most US Open titles with five, Rhonda Rajsich (USA) has four, one more than Christie Van Hees (Canada) with three. Michelle Gould (USA), Cheryl Gudinas (USA), and Jackie Paraiso (USA) have each won two US Open titles. Kerri Wachtel (USA) won the title once.
Other championships.
Racquetball is included in the Pan American Games, World Games and Central American and Caribbean Games. Also, the regional associations of the International Racquetball Federation organize their own continental championships: Asian Championships, European Championships and Pan American Championships.
Comparison to racketball.
Racquetball is very similar to the British sport of 'racketball', which was patterned on it in 1976. The main differences are that the British ball is smaller, denser, and less bouncy; the British sport's court is a squash court, which is substantially shorter and somewhat wider; and the ceiling in the British game is out of bounds.

</doc>
<doc id="39190" url="http://en.wikipedia.org/wiki?curid=39190" title="William Thurston">
William Thurston

William Paul Thurston (October 30, 1946 – August 21, 2012) was an American mathematician. He was a pioneer in the field of low-dimensional topology. In 1982, he was awarded the Fields Medal for his contributions to the study of 3-manifolds. From 2003 until his death he was a professor of mathematics and computer science at Cornell University.
Mathematical contributions.
Foliations.
His early work, in the early 1970s, was mainly in foliation theory, where he had a dramatic impact. His more significant results include:
In fact, Thurston resolved so many outstanding problems in foliation theory in such a short period of time that it led to a kind of exodus from the field, where advisors counselled students against going into foliation theory because Thurston was "cleaning out the subject" (see "On Proof and Progress in Mathematics", especially section 6 ).
The geometrization conjecture.
His later work, starting around the mid-1970s, revealed that hyperbolic geometry played a far more important role in the general theory of 3-manifolds than was previously realised. Prior to Thurston, there were only a handful of known examples of hyperbolic 3-manifolds of finite volume, such as the Seifert–Weber space. The independent and distinct approaches of Robert Riley and Troels Jørgensen in the mid-to-late 1970s showed that such examples were less atypical than previously believed; in particular their work showed that the figure-eight knot complement was hyperbolic. This was the first example of a hyperbolic knot.
Inspired by their work, Thurston took a different, more explicit means of exhibiting the hyperbolic structure of the figure eight knot complement. He showed that the figure eight knot complement could be decomposed as the union of two regular ideal hyperbolic tetrahedra whose hyperbolic structures matched up correctly and gave the hyperbolic structure on the figure eight knot complement. By utilizing Haken's normal surface techniques, he classified the incompressible surfaces in the knot complement. Together with his analysis of deformations of hyperbolic structures, he concluded that all but 10 Dehn surgeries on the figure eight knot resulted in irreducible, non-Haken non-Seifert-fibered 3-manifolds. These were the first such examples; previously it had been believed that except for certain Seifert fiber spaces, all irreducible 3-manifolds were Haken. These examples were actually hyperbolic and motivated his next revolutionary theorem.
Thurston proved that in fact most Dehn fillings on a cusped hyperbolic 3-manifold resulted in hyperbolic 3-manifolds. This is his celebrated hyperbolic Dehn surgery theorem.
To complete the picture, Thurston proved a hyperbolization theorem for Haken manifolds. A particularly important corollary is that many knots and links are in fact hyperbolic. Together with his hyperbolic Dehn surgery theorem, this showed that closed hyperbolic 3-manifolds existed in great abundance.
The geometrization theorem has been called "Thurston's Monster Theorem," due to the length and difficulty of the proof. Complete proofs were not written up until almost 20 years later. The proof involves a number of deep and original insights which have linked many apparently disparate fields to 3-manifolds.
Thurston was next led to formulate his geometrization conjecture. This gave a conjectural picture of 3-manifolds which indicated that all 3-manifolds admitted a certain kind of geometric decomposition involving eight geometries, now called Thurston model geometries. Hyperbolic geometry is the most prevalent geometry in this picture and also the most complicated. The conjecture was proved by Grigori Perelman in 2002–2003.
Orbifold theorem.
In his work on hyperbolic Dehn surgery, Thurston realized that orbifold structures naturally arose. Such structures had been studied prior to Thurston, but his work, particularly the next theorem, would bring them to prominence. In 1981, he announced the orbifold theorem, an extension of his geometrization theorem to the setting of 3-orbifolds. Two teams of mathematicians around 2000 finally finished their efforts to write down a complete proof, based mostly on Thurston's lectures given in the early 1980s in Princeton. His original proof relied partly on Hamilton's work on the Ricci flow.
Education and career.
Thurston was born in Washington, D.C. to a homemaker and an aeronautical engineer. He received his bachelor's degree from New College (now New College of Florida) in 1967. For his undergraduate thesis he developed an intuitionist foundation for topology. Following this, he earned a doctorate in mathematics from the University of California, Berkeley, in 1972. His Ph.D. advisor was Morris W. Hirsch and his dissertation was on "Foliations of Three-Manifolds which are Circle Bundles".
After completing his Ph.D., he spent a year at the Institute for Advanced Study, then another year at MIT as Assistant Professor. In 1974, he was appointed Professor of Mathematics at Princeton University. In 1991, he returned to UC-Berkeley as Professor of Mathematics and in 1993 became Director of the Mathematical Sciences Research Institute. In 1996, his wife Julian, who had earlier been his Ph.D. student at Princeton University, made a career switch to veterinary medicine, and began her studies at the UC Davis School of Veterinary Medicine. Bill and Julian moved to Davis, California, where Bill became Professor of Mathematics at UC Davis. In 2000, their first child Jade was born, and in 2003 their second child Liam was born. Bill and Julian had visited Ithaca in 1997 for a family celebration for his mother's 80th birthday. They were enchanted by the beauty of Ithaca, and in 2003 the family moved to Ithaca, NY, where Bill became Professor of Mathematics at Cornell University.
His Ph.D. students include Martin Bridgeman, Danny Calegari, Richard Canary, Suhyoung Choi, Renaud Dreyer, David Gabai, William Goldman, Benson Farb, Sergio Fenley, Detlef Hardorp, Craig Hodgson, Christopher Jerdonek, Richard Kenyon, Steven Kerckhoff, Silvio Levy, Robert Meyerhoff, Yair Minsky, Lee Mosher, Igor Rivin, Nicolau Saldanha, Oded Schramm, Richard Schwartz, William Floyd, Biao Wang and Jeffrey Weeks. His son Dylan Thurston is an associate professor of mathematics at Indiana University.
In later years Thurston widened his attention to include mathematical education and bringing mathematics to the general public. He has served as mathematics editor for Quantum Magazine, a youth science magazine, and was one of the founders of The Geometry Center. As director of Mathematical Sciences Research Institute from 1992 to 1997, he initiated a number of programs designed to increase awareness of mathematics among the public.
In 2005 Thurston won the first AMS Book Prize, for "Three-dimensional Geometry and Topology".
The prize "recognizes an outstanding research book that makes a seminal contribution to the research literature".
In 2012, Thurston was awarded the Leroy P Steele Prize by the AMS for seminal contribution to research. The citation described his work as having "revolutionized 3-manifold theory".
He died on August 21, 2012 in Rochester, New York, of a sinus mucosal melanoma that was diagnosed in 2011.
Thurston and his family had been in the process of moving back to Davis, CA, where he was to rejoin the mathematics faculty at UC Davis while his wife completed her veterinary medical degree. Thurston died before he could make the move to California. He had remained with his brother George in Rochester, NY, while his family went ahead of him to California to get settled, waiting for him to gain better physical strength for making the cross-country trip to California to join them. Bill's health declined rapidly, and the family returned to Rochester to be with him during his final days.
In Thurston's last days, he sometimes used American Sign Language to communicate with his children, Liam and Jade. Bill and Julian had spent a year studying ASL when Jade was an infant, and the family had become somewhat fluent. He also communicated by writing on one of his many pads of paper. One of his last written messages was, "Treasure Island," and this reference remains mysterious to his family.
Thurston has an Erdős number of 2, via John Horton Conway. Paths of length 3 are many; for example, Allan R. Wilks is a co-author, and Ronald Graham is a co-author with Wilks. Kenneth Steiglitz is a co-author, and Daniel J. Kleitman is a co-author with Steiglitz. Joel Hass is a co-author, and Laszlo Lovasz is a co-author with Hass. Graham, Lovasz and Kleitman are all co-authors with Erdős.
Selected works.
"William P. Thurston, . Notices of the AMS 37:7 (September 1990) pp 844–850

</doc>
<doc id="39191" url="http://en.wikipedia.org/wiki?curid=39191" title="Leisure Suit Larry">
Leisure Suit Larry

Leisure Suit Larry is a controversial/adult-themed video game series created by Al Lowe. It was published by Sierra from 1987 to 2009, then by Codemasters from 2009. The games follow Larry Laffer, a balding, double entendre-speaking, leisure suit-wearing man in his 40s. The game play revolves around him attempting, usually unsuccessfully, to seduce attractive women.
Series.
The series had its origins in Sierra's earlier "Softporn Adventure", a 1981 text adventure created by designer Chuck Benton; the story and basic structure from that game were reused for the first "Larry" game. The "Larry" games were one of Sierra's most popular game series during the genre's heyday when it was first released in the mid-1980s. The series contains the only games produced by Sierra that contain significant sexual themes.
In general, the games follow the escapades of Larry Laffer as he attempts to convince a variety of nubile women to sleep with him. A common link between the games are Larry's explorations of luxurious and cosmopolitan hotels, ships, beaches, resorts, and casinos. The character of Larry Laffer was voiced by Jan Rabson.
After the first game, the series – despite becoming known for its lewd content – gained a reputation for not featuring as much sexual material as expected; this was particularly true of the middle games in the series, which were released at the same time as more explicit games, like "Cobra Mission". At most, the raunchier moments were usually hidden as Easter eggs. Things became racier again toward the end of the series, particularly in the (original) final installment, "Love for Sail!"
', a spin-off version of the series, featured a different protagonist and style of game play. It was developed by High Voltage Software and released by Sierra in 2004. In 2007, Vivendi Universal Games announced a mobile remake of "Love for Sail!" In 2008, Sierra Entertainment announced plans to release '. It was developed by Team 17, and the publishing rights were offered to Codemasters, who published the game in 2009. Larry Lovage, the protagonist of "Magna Cum Laude" and "Box Office Bust", was voiced by Tim Dadabo.
In June 2011, Replay Games announced on their blog that they had acquired a license for the "Leisure Suit Larry" series. Replay Games planned to re-release the titles it licensed. It was also announced that the series' creator, Al Lowe, would be involved with the development of the new releases. In April 2012, Replay Games initiated a Kickstarter project that finished funding on May 2, 2012 and secured a total of $674,598, for which the developers promised to add more story, additional dialogue, and one more character.
The "HD" game, titled "" was originally announced to be released in late 2012, but it was delayed to mid-2013. In April 2013, Lowe said that early development had begun on a remake of "Larry 2"; Josh Mandel later confirmed the game's early development in a Kickstarter update.
Larry Lovage games.
Neither of these featured Larry Laffer as a starring character or had any involvement from Al Lowe and the original Sierra staff.
Unreleased games.
"Leisure Suit Larry 8: Lust in Space" ("Leisure Suit Larry Explores Uranus").
"Leisure Suit Larry 8", tentatively subtitled "Lust in Space" (as well as "Explores Uranus" in some references), was in full development in 1998 until funding was cut. Shortly afterwards, Sierra's adventure games department was disbanded, and Al Lowe left Sierra on February 22, 1999. Like the canned "Space Quest" sequel, "Larry 8" was to feature 3D computer graphics, but no more than a few test renders now survive. The game "Leisure Suit Larry: Explores Uranus", as well as its teaser, was referenced in "Leisure Suit Larry: Love for Sail!" triggered with an Easter egg, as well as a teaser after completing the game. In 2013, Al Lowe pointed out that this title is still being considered, with support from series co-writer Josh Mandel. Lowe stated that even though he would like to complete the "Reloaded" series first, "Leisure Suit Larry 8" is "absolutely" still in the works.
"Leisure Suit Larry: Pocket Party".
"Leisure Suit Larry: Pocket Party" was the canceled game in the series that was supposed to be released in the second half of 2005 for the N-Gage. The publishers were Vivendi and Nokia, while the developer was TKO-Software. In the game, players would explore a 3D college campus, while solving puzzles and engaging in risque activities. As they search for the ultimate good time, gamers bump into Rosie Palmer, the head cheerleader at Larry's college. Attempting to win over Rosie's heart, Larry is thoroughly embarrassed by her jock boyfriend Chuck Rockwell, but humiliation has never stopped Larry before and he is determined to do anything to be with Rosie. In addition to single-player game play, players could also wirelessly square off against an opponent in four different turn-based mini-games.
"Leisure Suit Larry: Cocoa Butter".
In late 2005, Target department stores (through online vendor Amazon.com) began accepting pre-orders for a sequel to "Leisure Suit Larry: Magna Cum Laude" titled "Leisure Suit Larry: Cocoa Butter". This new game was being developed for the PC, PS2, Xbox, and PSP systems, but has since been canceled.
Collections.
Several "Larry" collections have been compiled:
Reception.
The series was ranked as the 85th top game of all time by "Next Generation" in 1996, for how "the designers have managed to work in enough campy humor and bad puns to keep the game going through five more installments over nine years."

</doc>
<doc id="39194" url="http://en.wikipedia.org/wiki?curid=39194" title="Ext2">
Ext2

The ext2 or second extended filesystem is a file system for the Linux kernel. It was initially designed by Rémy Card as a replacement for the extended file system (ext). Its metadata structure was inspired by the earlier Unix File System (UFS).
The canonical implementation of ext2 is the "ext2fs" filesystem driver in the Linux kernel. Other implementations (of varying quality and completeness) exist in GNU Hurd, MINIX 3, Mac OS X (third-party), Darwin (same third-party as Mac OS X but untested), some BSD kernels, in Atari MiNT, and as third-party Microsoft Windows drivers.
ext2 was the default filesystem in several Linux distributions, including Debian and Red Hat Linux, until supplanted more recently by ext3, which is almost completely compatible with ext2 and is a journaling file system. ext2 is still the filesystem of choice for flash-based storage media (such as SD cards, and USB flash drives), since its lack of a journal increases performance and minimizes the number of writes, and flash devices have a limited number of write cycles. Recent kernels, however, support a journal-less mode of ext4, which would offer the same benefit, along with a number of ext4-specific benefits.
History.
The early development of the Linux kernel was made as a cross-development under the Minix operating system. Naturally, it was obvious that the MINIX file system would be used as Linux's first file system. The Minix file system was mostly free of bugs, but used 16-bit offsets internally and thus had a maximum size limit of only 64 megabytes. There was also a filename length limit of 14 characters. Because of these limitations, work began on a replacement native file system for Linux.
To ease the addition of new file systems and provide a generic file API, VFS, a virtual file system layer, was added to the Linux kernel. The extended file system (ext), was released in April 1992 as the first file system using the VFS API and was included in Linux version 0.96c. The ext file system solved the two major problems in the Minix file system (maximum partition size and filename length limitation to 14 characters), and allowed 2 gigabytes of data and filenames of up to 255 characters. But it still had problems: there was no support of separate timestamps for file access, inode modification, and data modification.
As a solution for these problems, two new filesystems were developed in January 1993 for Linux kernel 0.99: xiafs and the second extended file system (ext2), which was an overhaul of the extended file system incorporating many ideas from the Berkeley Fast File System. ext2 was also designed with extensibility in mind, with space left in many of its on-disk data structures for use by future versions.
Since then, ext2 has been a testbed for many of the new extensions to the VFS API. Features such as POSIX ACLs and extended attributes were generally implemented first on ext2 because it was relatively simple to extend and its internals were well-understood.
On Linux kernels prior to 2.6.17, restrictions in the block driver mean that ext2 filesystems have a maximum file size of 2 TiB.
ext2 is still recommended over journaling file systems on bootable USB flash drives and other solid-state drives. ext2 performs fewer writes than ext3, since there is no journaling. As the major aging factor of a flash chip is the number of erase cycles, and as erase cycles happen frequently on writes, decreasing writes increases the life span of the solid-state device. Another good practice for filesystems on flash devices is the use of the "noatime" mount option, for the same reason.
ext2 data structures.
The space in ext2 is split up into blocks. These blocks are grouped into block groups, analogous to cylinder groups in the Unix File System. There are typically thousands of blocks on a large file system. Data for any given file is typically contained within a single block group where possible. This is done to minimize the number of disk seeks when reading large amounts of contiguous data.
Each block group contains a copy of the superblock and block group descriptor table, and all block groups contain a block bitmap, an inode bitmap, an inode table and finally the actual data blocks.
The superblock contains important information that is crucial to the booting of the operating system. Thus backup copies are made in multiple block groups in the file system. However, typically only the first copy of it, which is found at the first block of the file system, is used in the booting.
The group descriptor stores the location of the block bitmap, inode bitmap and the start of the inode table for every block group. These, in turn, are stored in a group descriptor table.
Inodes.
Every file or directory is represented by an inode. The term "inode" comes from "index node" (over the time, it became i-node and then inode). The inode includes data about the size, permission, ownership, and location on disk of the file or directory.
Example of ext2 inode structure:
Quote from the Linux kernel documentation for ext2:
"There are pointers to the first 12 blocks which contain the file's data in the inode. There is a pointer to an indirect block (which contains pointers to the next set of blocks), a pointer to a doubly indirect block and a pointer to a trebly indirect block."
So, there is a structure in ext2 that has 15 pointers. Pointers 1 to 12 point to direct blocks, pointer 13 points to an indirect block, pointer 14 points to a doubly indirect block, and pointer 15 points to a trebly indirect block.
Directories.
Each directory is a list of directory entries. Each directory entry associates one file name with one inode number, and consists of the inode number, the length of the file name, and the actual text of the file name. To find a file, the directory is searched front-to-back for the associated filename. For reasonable directory sizes, this is fine. But for very large directories this is inefficient, and ext3 offers a second way of storing directories (HTree) that is more efficient than just a list of filenames.
The root directory is always stored in inode number two, so that the file system code can find it at mount time. Subdirectories are implemented by storing the name of the subdirectory in the name field, and the inode number of the subdirectory in the inode field. Hard links are implemented by storing the same inode number with more than one file name. Accessing the file by either name results in the same inode number, and therefore the same data.
The special directories "." (current directory) and ".." (parent directory) are implemented by storing the names "." and ".." in the directory, and the inode number of the current and parent directories in the inode field. The only special treatment these two entries receive is that they are automatically created when any new directory is made, and they cannot be deleted.
Allocating Data.
When a new file or directory is created, ext2 must decide where to store the data. If the disk is mostly empty, then data can be stored almost anywhere. However, clustering the data with related data will minimize seek times and maximize performance.
ext2 attempts to allocate each new directory in the group containing its parent directory, on the theory that accesses to parent and children directories are likely to be closely related. ext2 also attempts to place files in the same group as their directory entries, because directory accesses often lead to file accesses. However, if the group is full, then the new file or new directory is placed in some other non-full group.
The data blocks needed to store directories and files can be found by looking in the data allocation bitmap. Any needed space in the inode table can be found by looking in the inode allocation bitmap.
File system limits.
The reason for some limits of ext2 are the file format of the data and the operating system's kernel. Mostly these factors will be determined once when the file system is built. They depend on the block size and the ratio of the number of blocks and inodes.
In Linux the block size is limited by the architecture page size.
There are also some userspace programs that can't handle files larger than 2 GiB.
If "b" is the block size, the maximum file size is limited to "min"( (("b"/4)3+("b"/4)2+"b"/4+12)*"b", (232-1)*512 ) due to the i_block structure (an array of direct/indirect EXT2_N_BLOCKS) and i_blocks (32-bit integer value) representing the number of 512-byte "blocks" in the file.
The max number of sublevel-directories is 31998, due to the link count limit. Directory indexing is not available in ext2, so there are performance issues for directories with a large number of files (10,000+). The theoretical limit on the number of files in a directory is 1.3 × 1020, although this is not relevant for practical situations.
Note: In Linux 2.4 and earlier, block devices were limited to 2 TiB, limiting the maximum size of a partition, regardless of block size.
Compression extension.
e2compr is a modification to the ext2 driver in the Linux kernel to support compression and decompression of files by the file system, without any support by user applications. e2compr is a small patch against ext2.
e2compr compresses only regular files; the administrative data (superblock, inodes, directory files etc.) are not compressed (mainly for safety reasons). Access to compressed blocks is provided for read and write operations. The compression algorithm and cluster size is specified on a per-file basis. Directories can also be marked for compression, in which case every newly created file in the directory will be automatically compressed with the same cluster size and the same algorithm that was specified for the directory.
e2compr is not a new file system. It is only a patch to ext2 made to support the EXT2_COMPR_FL flag. It does not require user to make a new partition, and will continue to read or write existing ext2 file systems. One can consider it as simply a way for the read and write routines to access files that could have been created by a simple utility similar to gzip or compress. Compressed and uncompressed files coexist nicely on ext2 partitions.
The latest e2compr-branch is available for current releases of Linux 2.4, 2.6, and 3.0. The latest patch for Linux 3.0 was released in August 2011 and provides multicore and High memory support. There are also branches for Linux 2.0 and 2.2.
References.
</dl>

</doc>
<doc id="39195" url="http://en.wikipedia.org/wiki?curid=39195" title="Ext3">
Ext3

ext3, or third extended filesystem, is a journaled file system that is commonly used by the Linux kernel. It is the default file system for many popular Linux distributions. Stephen Tweedie first revealed that he was working on extending ext2 in "Journaling the Linux ext2fs Filesystem" in a 1998 paper, and later in a February 1999 kernel mailing list posting. The filesystem was merged with the mainline Linux kernel in November 2001 from 2.4.15 onward. Its main advantage over ext2 is journaling, which improves reliability and eliminates the need to check the file system after an unclean shutdown. Its successor is ext4.
Advantages.
The performance (speed) of ext3 is less attractive than competing Linux filesystems, such as ext4, JFS, ReiserFS and XFS. But ext3 has a significant advantage in that it allows in-place upgrades from ext2 without having to back up and restore data. Benchmarks suggest that ext3 also uses less CPU power than ReiserFS and XFS. It is also considered safer than the other Linux file systems, due to its relative simplicity and wider testing base.
ext3 adds the following features to ext2:
Without these features, any ext3 file system is also a valid ext2 file system. This situation has allowed well-tested and mature file system maintenance utilities for maintaining and repairing ext2 file systems to also be used with ext3 without major changes. The ext2 and ext3 file systems share the same standard set of utilities, e2fsprogs, which includes an fsck tool. The close relationship also makes conversion between the two file systems (both forward to ext3 and backward to ext2) straightforward.
ext3 lacks "modern" filesystem features, such as dynamic inode allocation and extents. This situation might sometimes be a disadvantage, but for recoverability, it is a significant advantage. The file system metadata is all in fixed, well-known locations, and data structures have some redundancy. In significant data corruption, ext2 or ext3 may be recoverable, while a tree-based file system may not.
Size limits.
The max number of blocks for ext3 is 232. The size of a block can vary, affecting the max number of files and the max size of the file system:
Journaling levels.
There are three levels of journaling available in the Linux implementation of ext3:
In all three modes, the internal structure of file system is assured to be consistent even after a crash. In any case, only the data content of files or directories which were being modified when the system crashed will be affected; the rest will be intact after recovery.
Disadvantages.
Functionality.
Since ext3 aims to be backwards compatible with the earlier ext2, many of the on-disk structures are similar to those of ext2. Consequently, ext3 lacks recent features, such as extents, dynamic allocation of inodes, and block suballocation. A directory can have at most 31998 subdirectories, because an inode can have at most 32000 links.
ext3, like most current Linux filesystems, cannot be fsck-ed while the filesystem is mounted for writing. Attempting to check a file system that is already mounted may detect bogus errors where changed data has not reached the disk yet, and corrupt the file system in an attempt to "fix" these errors.
Defragmentation.
There is no online ext3 defragmentation tool that works on the filesystem level. There is an offline ext2 defragmenter, codice_1, but it requires that the ext3 filesystem be converted back to ext2 first. But codice_1 may destroy data, depending on the feature bits turned on in the filesystem; it does not know how to treat many of the newer ext3 features.
There are userspace defragmentation tools, like Shake and defrag. Shake works by allocating space for the whole file as one operation, which will generally cause the allocator to find contiguous disk space. If there are files which are used at the same time, Shake will try to write them next to one another. Defrag works by copying each file over itself. However, this strategy works only if the file system has enough free space. A true defragmentation tool does not exist for ext3.
However, as the Linux System Administrator Guide states, "Modern Linux filesystem(s) keep fragmentation at a minimum by keeping all blocks in a file close together, even if they can't be stored in consecutive sectors. Some filesystems, like ext3, effectively allocate the free block that is nearest to other blocks in a file. Therefore it is not necessary to worry about fragmentation in a Linux system."
While ext3 is more resistant to file fragmentation than the FAT filesystem, ext3 can get fragmented over time or for specific usage patterns, like slowly writing large files. Consequently, ext4 (the successor to ext3) has an online filesystem defragmentation utility e4defrag and currently supports extents (contiguous file regions).
Undelete.
ext3 does not support the recovery of deleted files. The ext3 driver actively deletes files by wiping file inodes, for crash safety reasons.
There are still several techniques and some free and commercial software for recovery of deleted or lost files using file system journal analysis; however, they do not guarantee any specific file recovery.
Compression.
e3compr is an unofficial patch for ext3 that does transparent compression. It is a direct port of e2compr and still needs further development. It compiles and boots well with upstream kernels, but journaling is not implemented yet.
Lack of snapshots support.
Unlike a number of modern file systems, ext3 does not have native support for snapshots—the ability to quickly capture the state of the filesystem at arbitrary times. Instead, it relies on less-space-efficient, volume-level snapshots provided by the Linux LVM. The Next3 file system is a modified version of ext3 which offers snapshots support, yet retains compatibility with the ext3 on-disk format.
No checksumming in journal.
ext3 does not do checksumming when writing to the journal. On a storage device with extra cache, if "barrier=1" is not enabled as a mount option (in /etc/fstab), and if the hardware is doing out-of-order write caching, one runs the risk of severe filesystem corruption during a crash. This is because storage devices with write caches report to the system that the data has been completely written, even if it was written to the (volatile) cache.
Consider the following scenario: If hard disk writes are done out-of-order (due to modern hard disks caching writes in order to amortize write speeds), it is likely that one will write a commit block of a transaction before the other relevant blocks are written. If a power failure or unrecoverable crash should occur before the other blocks get written, the system will have to be rebooted. Upon reboot, the file system will replay the log as normal, and replay the "winners" (transactions with a commit block, including the invalid transaction above, which happened to be tagged with a valid commit block). The unfinished disk write above will thus proceed, but using corrupt journal data. "The file system will thus mistakenly overwrite normal data with corrupt data while replaying the journal." There is a available to trigger the problematic behavior. If checksums had been used, where the blocks of the "fake winner" transaction were tagged with a mutual checksum, the file system could have known better and not replayed the corrupt data onto the disk. Journal checksumming has been added to ext4.
Filesystems going through the device mapper interface (including software RAID and LVM implementations) may not support barriers, and will issue a warning if that mount option is used. There are also some disks that do not properly implement the write cache flushing extension necessary for barriers to work, which causes a similar warning. In these situations, where barriers are not supported or practical, reliable write ordering is possible by turning off the disk's write cache and using the data=journal mount option. Turning off the disk's write cache may be required even when barriers are available.
Applications like databases expect that a call to fsync() will flush pending writes to disk, and the barrier implementation doesn't always clear the drive's write cache in response to that call. There is also a potential issue with the barrier implementation related to error handling during events, such as a drive failure. It is also known that sometimes some virtualization technologies do not properly forward fsync or flush commands to the underlying devices (files, volumes, disk) from a guest operating system. Similarly, some hard disks or controllers implement cache flushing incorrectly or not at all, but still advertise that it is supported, and do not return any error when it is used. There are so many ways to handle fsync and write cache handling incorrectly, it is safer to assume that cache flushing does not work unless it is explicitly tested, regardless of how reliable individual components are believed to be.
ext4.
On June 28, 2006, Theodore Ts'o, the principal developer of ext3, announced an enhanced version, called ext4. On October 11, 2008, the patches that mark ext4 as stable code were merged in the Linux 2.6.28 source code repositories, marking the end of the development phase and recommending its adoption.
In 2008, Ts'o stated that although ext4 has improved features, it is not a major advance, it uses old technology, and is a stop-gap; Ts'o believes that Btrfs is the better direction, because "it offers improvements in scalability, reliability, and ease of management". Btrfs also has "a number of the same design ideas that reiser3/4 had".

</doc>
<doc id="39197" url="http://en.wikipedia.org/wiki?curid=39197" title="Corporate haven">
Corporate haven

A corporate haven is a jurisdiction with laws friendly to corporations
thereby encouraging them to choose that jurisdiction as a legal domicile.
History.
"Corporate Havens" are a post 1970s global economic phenomenon. This decade more or less marked the end of colonialism and the formation of EU in the form of the EEC. Many legal and taxation processes culminated in this decade that led to the formation of Tax Havens and Corporate Havens throughout the developed world.
Geographical distribution.
North America.
Within the United States, Delaware is considered the pre-eminent corporate haven for both domestic and foreign large public corporations, while Nevada, Wyoming, Alaska, Puerto Rico, and the U.S. Virgin Islands are corporate havens for small closed corporations.
Delaware, through its developed legal system and laws protecting shareholder rights, is geared toward the large complex public corporation, whereas Nevada and Wyoming are more attractive to the small privately held corporation. Delaware law tends to protect the rights of boards of directors and shareholders, while Nevada and Wyoming tend to favor management. In the U.S. Virgin Islands, aside from the fact that there are no property and import taxes in the territory. It has been successful in luring small businesses in investing in the economy with generous tax breaks. Companies receiving a 90% tax and a personal income tax cut. Puerto Rico recently opened up as a corporate tax haven for American citizens. Non-Puerto Rican citizens who move to Puerto Rico from anywhere else in the U.S. will receive automatic tax incenitives and a 100 percent tax break. Same with Small corporations that are not Puerto Rican, but move their office(s) there.
Commonwealth and Western Europe.
Corporate havens outside the United States include British Overseas Territories such as
and the British Crown Dependencies:
Other havens include
Increasingly, multinational corporations are using the leading corporate havens, either by incorporating subsidiary corporations in these locations or by moving their corporate domicile (the home company of the corporation) there.
Corporate havens are often not beneficial to small corporations operating in one legal jurisdiction because of the complexity of creating a corporation elsewhere and then having to re-register in the local area as a foreign corporation.
North American legal issues.
United States tax law (since the 1980s) has made these offshore corporate havens very unattractive to individual citizens. US citizens are statutorily taxed by the U.S. government on their worldwide income, unlike the citizens of many other nations.
The Canada Revenue Agency loses billions each year to the Tax Haven phenomena.

</doc>
<doc id="39199" url="http://en.wikipedia.org/wiki?curid=39199" title="Green Mountain Boys">
Green Mountain Boys

The Green Mountain Boys were a militia organization first established in the late 1760s in the territory between the British provinces of New York and New Hampshire, known as the New Hampshire Grants (which later became the state of Vermont). Headed by Ethan Allen and members of his extended family, they were instrumental in resisting New York's attempts to control the territory, over which it had won "de jure" control in a territorial dispute with New Hampshire.
Some companies served in the American Revolutionary War, including notably when the Green Mountain Boys led by Ethan Allen captured Fort Ticonderoga on Lake Champlain on May 10, 1775; and invaded Canada later in 1775. In early June of 1775, Ethan Allen and his then subordinate, Seth Warner, induced the Continental Congress at Philadelphia to create a Continental Army ranger regiment from the then New Hampshire Grants. Having no treasury, the Congress directed that New York's revolutionary Congress pay for the newly authorized regiment. In July of 1775, Allen's militia was granted support from the New York revolutionary Congress.
The Green Mountain Boys disbanded more than a year before Vermont declared its independence in 1777 from Great Britain "as a separate, free and independent jurisdiction or state". The Vermont Republic operated for 14 years, before being admitted in 1791 to the United States as the 14th state.
The remnants of the Green Mountain Boys militia were largely reconstituted as the Green Mountain Continental Rangers. Command of the newly formed regiment passed from Allen to Seth Warner. Allen joined the staff of the Northern Army of New York's Major General Philip Schuyler and was given the rank of lieutenant colonel. Under Warner the regiment fought at the battles of Hubbardton and Bennington in 1777. The regiment was disbanded in 1779.
The Green Mountain Boys mustered again during the War of 1812, the Civil War, and the Spanish–American War. Today it is the informal name of the Vermont National Guard, which comprises both the Army and Air National Guards.
Historical unit.
The original "Green Mountain Boys" were a militia organized in what is now southwestern Vermont in the decade prior to the American Revolutionary War. They comprised settlers and land speculators who held New Hampshire titles to lands between the Connecticut River and Lake Champlain, an area then known as the New Hampshire Grants, that is now modern Vermont. New York was given legal control of the area by a decision of the British crown and refused to respect the New Hampshire titles and town charters. Although a few towns with New York land titles, notably Brattleboro on the Connecticut River, supported the change, the vast majority of the settlers in the sparsely populated frontier region rejected the authority of New York.
With several hundred members, the Green Mountain Boys effectively controlled the area where New Hampshire grants had been issued. They were led by Ethan Allen, his brother Ira Allen, and their cousins Seth Warner and Remember Baker. They were based at the Catamount Tavern in Bennington. By the 1770s, the Green Mountain Boys had become an armed military force and "de facto" government that prevented New York from exercising its authority in the northeast portion of the Province of New York. New York authorities had standing warrants for the arrest of the leaders of the rebellious Vermonters, but were unable to exercise them. New York surveyors and other officials attempting to exercise their authority were prevented from doing so and in some cases were severely beaten, and settlers arriving to clear and work land under New York–issued grants were forced off their land, and sometimes had their possessions destroyed. At the same time, New York sought to extend its authority over the territory. During an event once known as the Westminster massacre, anti-Yorkers occupied the courthouse in Westminster to prevent a New York judge from holding court, and two men were killed in the ensuing standoff. Ethan Allen then went to Westminster with a band of Boys, and organized a convention calling for the territory's independence from New York.
When the American Revolutionary War started in 1775, Ethan Allen and a troop of his men, along with Connecticut Colonel Benedict Arnold, marched up to Lake Champlain and captured the strategically important military posts at Fort Ticonderoga, Crown Point, and Fort George, all in New York. The Boys also briefly held St. John's in Québec, but retreated on word of arriving British regulars. The Green Mountain Boys later formed the basis of the Vermont militia that selected Seth Warner as its leader. Some of the Green Mountain Boys preferred to stick with Ethan Allen and were captured along with Allen in August 1775 in a bungled attempt to capture the city of Montreal. Some members of this unit were Congressman Matthew Lyon and Lieutenant Benjamin Tucker. Benjamin Tucker joined the British Military during his capture; because of this, his name was rebuked by Ethan Allen and his men.
Vermont eventually declared itself an independent nation in January 1777, and organized a government based in Windsor. The army of the Vermont Republic was based upon the Green Mountain Boys. Although Vermont initially supported the American Revolutionary War and sent troops to fight John Burgoyne's British invasion from Quebec in battles at Hubbardton and Bennington in 1777, Vermont eventually adopted a more neutral stance and became a haven for deserters from both the British and colonial armies. George Washington, who had more than sufficient difficulties with the British, brushed off Congressional demands that he subdue Vermont. During the Haldimand Affair, some members of the Green Mountain Boys became involved in secret negotiations with British officials about restoring the Crown's rule over the territory.
The "Vermont Army" version of the Green Mountain Boys faded away after Vermont joined the United States as the 14th U.S. state in 1791, although the Green Mountain Boys mustered for the War of 1812, The Civil War, the Spanish–American War, and following World War I as the Vermont National Guard.
Flag.
A remnant of a Green Mountain Boys flag, believed to belong to John Stark, is owned by the Bennington Museum. It still exists as one of the few regimental flags from the American Revolution. Although Stark was at the Battle of Bennington and likely flew this flag, the battle has become more commonly associated with the Bennington flag, which is believed to be a 19th-century banner.
Vermont National Guard.
Today, the Vermont Army National Guard and Vermont Air National Guard are collectively known as the Vermont National Guard or the "Green Mountain Boys", even though women have served in both branches since the mid-twentieth century. Both units use the original Green Mountain Boys battle flag as their banner.

</doc>
<doc id="39201" url="http://en.wikipedia.org/wiki?curid=39201" title="Mark the Evangelist">
Mark the Evangelist

Mark the Evangelist (Latin: "Mārcus"; Greek: Μᾶρκος; Coptic: Μαρκοϲ; Hebrew: מרקוס‎) is the traditionally ascribed author of the Gospel of Mark. Mark said to have founded the Church of Alexandria, one of the most important episcopal sees of Early Christianity. His feast day is celebrated on April 25, and his symbol is the winged lion.
Mark's identity.
According to William Lane (1974), an "unbroken tradition" identifies Mark the Evangelist with John Mark, and John Mark as the cousin of Barnabas. However, Hippolytus of Rome in "On the Seventy Apostles" distinguishes Mark the Evangelist (2 Tim 4:11), John Mark (Acts 12:12, 25; 13:5, 13; 15:37), and Mark the cousin of Barnabas (Col 4:10; Phlm 1:24). According to Hippolytus, they all belonged to the "Seventy Disciples" who were sent out by Jesus to saturate Judea with the gospel (Luke 10:1ff.). However, when Jesus explained that his flesh was "real food" and his blood was "real drink", many disciples left him (John 6:44–6:66), presumably including Mark. He was later restored to faith by the apostle Peter; he then became Peter’s interpreter, wrote the Gospel of Mark, founded the church of Africa, and became the bishop of Alexandria.
According to Eusebius of Caesarea ("Eccl. Hist." 2.9.1–4), Herod Agrippa I in his first year of reign over the whole Judea (AD 41) killed James, son of Zebedee and arrested Peter, planning to kill him after the Passover. Peter was saved miraculously by angels, and escaped out of the realm of Herod (Acts 12:1–19). Peter went to Antioch, then through Asia Minor (visiting the churches in Pontus, Galatia, Cappadocia, Asia, and Bithynia, as mentioned in 1 Pet 1:1), and arrived in Rome in the second year of Emperor Claudius (AD 42; Eusebius, Eccl, Hist. 2.14.6). Somewhere on the way, Peter picked up Mark and took him as travel companion and interpreter. Mark the Evangelist wrote down the sermons of Peter, thus composing the Gospel according to Mark ("Eccl. Hist." 15–16), before he left for Alexandria in the third year of Claudius (43).
In AD 49, about 19 years after the Ascension of Jesus, Mark traveled to Alexandria [cf. c. 49 [cf. Acts 15:36–41] and founded the Church of Alexandria - today, both the Coptic Orthodox Church and the Greek Orthodox Church of Alexandria claim to be successors to this original community. Aspects of the Coptic liturgy can be traced back to Mark himself. He became the first bishop of Alexandria and he is honored as the founder of Christianity in Africa.
According to Eusebius ("Eccl. Hist." 2.24.1), Mark was succeeded by Annianus as the bishop of Alexandria in the eighth year of Nero (62/63), probably, but not definitely, due to his coming death. Later Coptic tradition says that he was martyred in 68.
The Gospel of Mark was written by an anonymous author. The Gospel wasn't written and does not claim to be written by direct witnesses to the reported events.
Biblical and traditional information.
Evidence for Mark the Evangelist's authorship of the Gospel that bears his name originates with Papias. Scholars of the Trinity Evangelical Divinity School are "almost certain" that Papias refers to John Mark. However, Catholic scholars have argued that identifying Mark the Evangelist with John Mark and Mark the Cousin of Barnabas has led to the downgrading of the character of Barnabas from truly a "Son of Comfort" to one who favored his blood relative over principles.
Identifying Mark the Evangelist with John Mark also led to identifying him as the man who carried water to the house where the Last Supper took place (Mark 14:13), or as the young man who ran away naked when Jesus was arrested (Mark 14:51–52).
The Coptic Church accords with identifying Mark the Evangelist with John Mark, as well as that he was one of the Seventy Disciples sent out by Christ (Luke 10:1), as Hippolytus confirmed. Coptic tradition also holds that Mark the Evangelist hosted the disciples in his house after Jesus' death, that the resurrected Jesus Christ came to Mark's house (John 20), and that the Holy Spirit descended on the disciples at Pentecost in the same house. Furthermore, Mark is also believed to have been among the servants at the Marriage at Cana who poured out the water that Jesus turned to wine (John 2:1–11).
According to the Coptic tradition, Saint Mark was born in Cyrene, a city in the Pentapolis of North Africa (now Libya). This tradition adds that Mark returned to Pentapolis later in life, after being sent by Paul to Colossae (Colossians 4:10; Philemon 24. Some, however, think these actually refer to Mark the Cousin of Barnabas), and serving with him in Rome (2 Tim 4:11); from Pentapolis he made his way to Alexandria. When Mark returned to Alexandria, the pagans of the city resented his efforts to turn the Alexandrians away from the worship of their traditional gods. In AD 68 they placed a rope around his neck and dragged him through the streets until he was dead.
Where Saint John Mark (son of Mary) is distinguished from Saint Mark, the composer of the earliest Gospel that we have, Saint John Mark is celebrated on September 27 (as in the Roman Martyrology) and the writer of the Gospel on April 25. In addition to Saint John Mark's in Jerusalem, the Parish Church of Chester Hill with Sefton in the Diocese of Sydney (Anglican Church of Australia) is Saint John Mark's and it celebrated its patronal festival on September 27. An icon of Saint John Mark on Cyprus, painted by a Russian Orthodox monk at Walsingham, was formerly in that church and is now in Christ Church Saint Laurence in Sydney.
Relics of St. Mark.
In 828, relics believed to be the body of St. Mark were stolen from Alexandria (at the time controlled by the Abbasid Caliphate) by two Venetian merchants with the help of two Greek monks and taken to Venice. A mosaic in St Mark's Basilica depicts sailors covering the relics with a layer of pork and cabbage leaves. Since Muslims are not permitted to touch pork, this was done to prevent the guards from inspecting the ship's cargo too closely. 
Donald Nicol explained this act as "motivated as much by politics as by piety", and "a calculated stab at the pretensions of the Patriarchate of Aquileia." But instead of being used to adorn the church of Grado, which claimed to possess the throne of St. Mark, but was kept secretly by Doge Giustiniano Participazio in his modest palace. Possession of St. Mark's remains was, in Nicol's words, "the symbol not of the Patriarchate of Grado, nor of the bishopric of Olivolo, but of the city of Venice." In his will Doge Giustininao asked his widow to build a basilica dedicated to St. Mark, which was erected between the palace and the chapel of St. Theodore Stratelates, who until then had been patron saint of Venice.
In 1063, during the construction of a new basilica in Venice, St. Mark's relics could not be found. However, according to tradition, in 1094 the saint himself revealed the location of his remains by extending an arm from a pillar. The newfound remains were placed in a sarcophagus in the basilica.
Copts believe that the head of St. Mark remains in a church named after him in Alexandria, and parts of his relics are in Saint Mark's Coptic Orthodox Cathedral, Cairo. The rest of his relics are in the San Marco Cathedral in Venice, Italy. Every year, on the 30th day of the month of Paopi, the Coptic Orthodox Church celebrates the commemoration of the consecration of the church of St. Mark, and the appearance of the head of the saint in the city of Alexandria. This takes place inside St. Mark Coptic Orthodox Cathedral in Alexandria, where the saint's head is preserved.
In June 1968, Pope Cyril VI of Alexandria sent an official delegation to Rome to receive a relic of St. Mark from Pope Paul VI. The delegation consisted of ten metropolitans and bishops, seven of whom were Coptic and three Ethiopian, and three prominent Coptic lay leaders.
The relic was said to be a small piece of bone that had been given to the Roman pope by Giovanni Cardinal Urbani, Patriarch of Venice. Pope Paul, in an address to the delegation, said that the rest of the relics of the saint remained in Venice.
The delegation received the relic on June 22, 1968. The next day, the delegation celebrated a pontifical liturgy in the Church of Saint Athanasius the Apostolic in Rome. The metropolitans, bishops, and priests of the delegation all served in the liturgy. Members of the Roman papal delegation, Copts who lived in Rome, newspaper and news agency reporters, and many foreign dignitaries attended the liturgy.
In a 2011 episode of the National Geographic Channel television series "Mystery Files," historian Andrew Chugg suggests that Alexander the Great's body was stolen from Alexandria, Egypt by Venetian merchants who believed it to be that of St. Mark the Evangelist. They smuggled the remains to Venice, which were then venerated as St. Mark the Evangelist in the Basilica Cattedrale Patriarcale di San Marco.
In art.
Mark the Evangelist is most often depicted writing or holding his gospel. In Christian tradition, Mark the Evangelist, the author of the second gospel is symbolized by a lion – a figure of courage and monarchy.
Some Christian legends refer to Saint Mark as "Saint Mark The Lionhearted". These legends say that he was thrown to the Lions and the animals refused to attack or eat him. Instead the Lions slept at his feet, while he petted them. When the Romans saw this, they released him impressed by this sight.
Mark the Evangelist attributes are the Lion in the desert; he can be depicted as a bishop on a throne decorated with lions; as a man helping Venetian sailors.
Mark the Evangelist is often depicted holding a book with "pax tibi Marce" written on it or holding a palm and book. Mark the Evangelist attributes are the Lion in the desert. Other depictions of Mark show him as a man with a book or scroll accompanied by a winged lion. the lion might also be associated with Jesus' Resurrection because lions were believed to sleep with open eyes, thus a comparison with Christ in his tomb, and Christ as king.
Mark the Evangelist can be depicted as a man with a halter around his neck and as Mark the Evangelist rescuing Christian slaves from Saracens.

</doc>
<doc id="39202" url="http://en.wikipedia.org/wiki?curid=39202" title="Telugu language">
Telugu language

Telugu (; తెలుగు "telugu", ]) is a Dravidian language and is the only language other than Hindi, English and Bengali that is predominantly spoken in more than one Indian state, being the primary language in Andhra Pradesh and Telangana, as well as in the town of Yanam where it is also an official language. It is also spoken by significant minorities in the Andaman and Nicobar, Chhattisgarh, Karnataka, Maharashtra, Odisha, Tamil Nadu, and Puducherry, and by the Sri Lankan Gypsy people. It is one of six languages designated a classical language of India. Telugu ranks third by the number of native speakers in India (74 million) (2001 Census), thirteenth in the Ethnologue list of most-spoken languages worldwide and is the most widely spoken Dravidian language. It is one of the twenty-two scheduled languages of the Republic of India.
Telugu retains some features of Sanskrit that have subsequently been lost in some of Sanskrit's daughter languages such as Hindi and Bengali, especially in the pronunciation of some vowels and consonants.
Etymology.
The etymology of Telugu is not known for certain. It is thought to have been derived from "trilinga", as in "Trilinga Desa", "the country of the three lingas". According to a Hindu legend, Shiva descended as a linga on three mountains: Kaleswaram in Telangana, Srisailam in Rayalaseema and Bhimeswaram in Coastal Andhra; in the legend, these marked the boundaries of the Telugu country. According to Marepalli Ramachandra Sastry, "Telu" means white and "unga" designates a plural in Gondi.
History.
According to the Russian linguist Andronov, Telugu split from Proto-Dravidian languages between 1500–1000 BC.
Earliest records.
Some Telugu words appear in the Maharashtri Prakrit anthology of poems (the "Gāhā Sattasaī") collected during the Satavahana dynasty.
Inscriptions containing Telugu words can be dated back to 400 BC to 100 BC. They were discovered in Bhattiprolu in Guntur district. The English translation of one inscription reads, "gift of the slab by venerable Midikilayakha".
Post-Ikshvaku period.
The period from 575 AD to 1022 AD corresponds to the second phase of Telugu history, after the Andhra Ikshvaku period. This is evidenced by the first inscription that is entirely in Telugu, dated 575 AD, which was found in the Rayalaseema region and is attributed to the Renati Cholas, who broke with the prevailing custom of using Sanskrit and began writing royal proclamations in the local language. During the next fifty years, Telugu inscriptions appeared in Anantapuram and other neighboring regions.
Telugu was more influenced by Sanskrit and Prakrit during this period, which corresponded to the advent of Telugu literature. Telugu literature was initially found in inscriptions and poetry in the courts of the rulers, and later in written works such as Nannayya's "Mahabharatam" (1022 AD). During the time of Nannayya, the literary language diverged from the popular language. It was also a period of phonetic changes in the spoken language.
Middle Ages.
The third phase is marked by further stylization and sophistication of the literary language. Ketana (13th century) in fact prohibited the use of spoken words in poetic works.
During this period the split of the Telugu and Kannada alphabets took place. Tikkana wrote his works in this script.
Vijayanagara Empire.
The Vijayanagara Empire gained dominance from 1336 to the late 17th century, reaching its peak during the rule of Krishnadevaraya in the 16th century, when Telugu literature experienced what is considered its Golden Age. "Pada kavita pitamaha", Annamacharya, contributed many Telugu songs to this language.
Muslim rule.
With the exception of Coastal Andhra, a distinct dialect developed in the Telangana State and the Rayalaseema region due to Muslim influence: Sultanate rule under the Tughlaq dynasty had been established earlier in the northern Deccan during the 14th century. In the latter half of the 17th century, Muslim rule extended further south, culminating in the establishment of the princely state of Hyderabad by the Asaf Jah dynasty in 1724. This heralded an era of Persian/Arabic influence on the Telugu language, especially among the people of Hyderabad. The effect is also evident in the prose of the early 19th century, as in the "Kaifiyats".
In the princely state of Nizam, Andhra Jana Sangham was started in 1921 with the main intention of promoting Telugu language, literature, its books and historical research led by Suravaram Pratapareddy, Madapati Hanumantha Rao, Komarraju Venkata Lakshmana Rao and others.
Colonial period.
The 16th-century Venetian explorer Niccolò de' Conti visited the Vijayanagara Empire and described it as "Italian of the east"; a saying which has been widely repeated.
In the period of the late 19th and the early 20th centuries saw the influence of the English language and modern communication/printing press as an effect of the British rule, especially in the areas that were part of the Madras Presidency. Literature from this time had a mix of classical and modern traditions and included works by scholars like Kandukuri Veeresalingam, Gurazada Apparao and Panuganti Lakshminarasimha Rao.
Since the 1930s, what was considered an elite literary form of the Telugu language, has now spread to the common people with the introduction of mass media like movies, television, radio and newspapers. This form of the language is also taught in schools and colleges as a standard.
Dialects.
Waddar, Chenchu, and Manna-Dora are all closely related to Telugu. Dialects of Telugu are Berad, Dasari, Dommara, Golari, Kamathi, Komtao, Konda-Reddi, Salewari, Telaingani, Warangal, Mahaboobnagar (Palamuru), Gadwal (Rayalaseema mix), Narayanapeta (Kannada and Marathi influence), Vijayawada, Vadaga, Srikakulam, Visakhapatnam, Toorpu (East) Godavari, Paschima (West) Godavari, Kandula, Rayalaseema, Nellooru, Prakasam, Gunturu, Tirupati, Vadari and Yanadi (Yenadi).
In Tamil Nadu the Telugu dialect is classified into Salem, Coimbatore, Vellore, Tiruvannamalai and Madras Telugu dialects. It is also spoken in pockets of Virudhunagar, Tuticorin, Madurai, Madras and Thanjavur districts.
Geographic distribution.
Telugu is mainly spoken in the states of Telangana, Andhra Pradesh and Yanam district of Puducherry as well as in the neighboring states of Tamil Nadu, Puducherry, Karnataka, Maharashtra, Odisha, Chhattisgarh, some parts of Jharkhand and the Kharagpur region of West Bengal in India. It is also spoken in the United States, where the Telugu diaspora numbers more than 800,000, with the highest concentration in Central New Jersey; as well as in Australia, New Zealand, Bahrain, Canada, Fiji, Malaysia, Singapore, Mauritius, Ireland, South Africa, Trinidad and Tobago, the United Arab Emirates, United Kingdom, as well as other western European countries, where there is also a considerable Telugu diaspora. At 7.2% of the population, Telugu is the third-most-spoken language in the Indian subcontinent after Hindi and Bengali. In Karnataka, 7.0% of the population speak Telugu, and in Tamil Nadu, where it commonly known as "Telungu", 5.6%.
Phonology.
Telugu words generally end in vowels. In Old Telugu, this was absolute; in the modern language "m, n, y, w" may end a word. Atypically for a Dravidian language, voiced consonants were distinctive even in the oldest recorded form of the language. Sanskrit loans have introduced aspirated and murmured consonants as well.
Telugu does not have contrastive stress, and speakers vary on where they perceive stress. Most judge it to be on the penultimate or final syllable, depending on word and vowel length.
Vowels.
Telugu features a form of vowel harmony wherein the second vowel in disyllabic noun and adjective roots alters whether the first vowel is tense or lax.[] Also, if the second vowel is open (i.e. /aː/ or /a/), then the first vowel will be more open and centralized (e.g. [mɛːka] 'goat', as opposed to [meːku] 'nail'). Telugu words also have vowels in inflectional suffixes harmonized with the vowels of the preceding syllable.
/æː/ only occurs in loan words.
Telugu has two diphthongs: ఐ [ai] and ఔ [au].
Consonants.
The table below illustrates the articulation of the consonants.
Grammar.
The Telugu Grammar is called "vyākaranam" (వ్యాకరణం).
The first treatise on Telugu grammar, the "Andhra Sabda Chintamani" was written in Sanskrit by Nannayya, considered the first Telugu poet and translator, in the 11th century A.D. This grammar followed the patterns which existed in grammatical treatises like Aṣṭādhyāyī and Vālmīkivyākaranam but unlike Pāṇini, Nannayya divided his work into five chapters, covering "samjnā", "sandhi", "ajanta", "halanta" and "kriya". Every Telugu grammatical rule is derived from Pāṇinian concepts.
In the 19th century, Chinnaya Suri wrote a simplified work on Telugu grammar called "Bāla Vyākaranam" by borrowing concepts and ideas from Nannayya's grammar.
This sentence can also be interpreted as 'Ramu will go to school.' depending on the context. But it does not affect the SOV order.
Inflection.
Telugu nouns are inflected for number (singular, plural), gender (masculine, feminine, and neuter) and case (nominative, accusative, genitive, dative, vocative, instrumental, and locative).
Gender.
Telugu has three genders: masculine, feminine, and neutral.
Pronouns.
Telugu pronouns include personal pronouns (The persons speaking, the persons spoken to, or the persons or things spoken about). Indefinite pronouns, relative pronouns (connect parts of sentences) and reciprocal or reflexive pronouns (in which the object of a verb is being acted on by verb's subject).
Telugu uses the same forms for singular feminine and neutral gender—the third person pronoun (అది /ad̪ɪ/) is used to refer to animals and objects.
The nominative case (karta), object of a verb (karma) and the verb are somewhat in a sequence in Telugu sentence construction. "Vibhakti" (case of a noun) and "pratyayamulu" (an affix to roots and words forming derivs. and inflections) depict the ancient nature and progression of the language. The "Vibhaktis" of Telugu language " డు [Du], ము [mu], వు [vu], లు [lu]" etc. are different from those in Sanskrit and have been in the usage for a long time.
Vocabulary.
Sanskrit influenced Telugu of Telanganites, Andhras for about 1500 years, however, there are evidences which suggest older origin of the influence. During 1000–1100 AD, Nannaya's re-writing of the "Mahābhārata" in Telugu (మహాభారతము) re-established its use, and it dominated over the royal language, Sanskrit. Telugu absorbed "tatsama"s from Sanskrit.
The vocabulary of Telugu, especially in Telangana state, has a trove of Persian-Arabic borrowings, which have been modified to fit Telugu phonology. This was due to centuries of Muslim rule in these regions, such as the erstwhile kingdoms of Golkonda and Hyderabad. (e.g. కబురు, /kaburu/ for Urdu /xabar/, خبر or జవాబు, /dʒavaːbu/ for Urdu /dʒawɑːb/, جواب)
Modern Telugu vocabulary can be said to constitute a diglossia, because the formal, standardized version of the language, heavily influenced by Sanskrit, is taught in schools and used by the government and Hindu religious institutions. However, everyday Telugu varies depending upon region and social status.
Writing system.
Telugu script is written from left to right and consists of sequences of simple and/or complex characters. The script is syllabic in nature—the basic units of writing are syllables. Since the number of possible syllables is very large, syllables are composed of more basic units such as vowels (“achchu” or “swaram”) and consonants (“hallu” or “vyanjanam”). Consonants in consonant clusters take shapes that are very different from the shapes they take elsewhere. Consonants are presumed to be pure consonants, that is, without any vowel sound in them. However, it is traditional to write and read consonants with an implied 'a' vowel sound. When consonants combine with other vowel signs, the vowel part is indicated orthographically using signs known as vowel “maatras”. The shapes of vowel “maatras” are also very different from the shapes of the corresponding vowels.
The overall pattern consists of sixty symbols, of which 16 are vowels, three vowel modifiers, and forty-one consonants. Spaces are used between words as word separators.
The sentence ends with either a single bar । (“purna viramam”) or a double bar ॥ (“deergha viramam”). Traditionally, in handwriting, Telugu words were not separated by spaces. Modern punctuation (commas, semicolon, etc.) were introduced with the advent of print.
There is a set of symbols for numerals, though Arabic numbers are typically used.
Telugu is assigned Unicode codepoints: 0C00-0C7F (3072–3199).
Number system.
Telugu has its own digits, as shown below. However, these aren't used commonly.
Alphabet.
The Telugu alphabet consist of 60 symbols – 16 vowels, 3 vowel modifiers, and 41 consonants. Sanskrit and Telugu alphabets are similar and exhibit one-to-one correspondence. Telugu has complete set of letters which follows scientific system to express sounds. Some of them are introduced to express fine shades of difference in sounds.
Telugu has full-zero ("anusvāra") ( ం ), half-zero ("arthanusvāra" or "candrabindu") (ఁ) and "visarga" ( ః ) to convey various shades of nasal sounds. la and La, ra and Ra are differentiated.
Telugu has .CH and .JH which are not represented in Sanskrit. Their pronunciation is similar to the s sound in the word treasure and z sound in zebra respectively. Secondly S, SH, and KSH which are not found in Tamil.
Telugu script can reproduce the full range of Sanskrit phonetics without losing any of the text's originality. Telugu has made its letters expressive of all the sounds and hence it has to deal with significant borrowings from Sanskrit, Tamil and Hindustani.
Telugu Gunintalu:
క కా కి కీ కు కూ కృ కౄ కె కే కై కొ కో కౌ క్ కం కః
ఖ ఖా ఖి ఖీ ఖు ఖూ ఖృ ఖౄ ఖె ఖే ఖై ఖొ ఖో ఖౌ ఖ్ ఖం ఖః
Literature.
Telugu literature is generally divided into six periods:
In the Telugu literature Tikkana was given agraasana (top position) by many famous critics.
In the earliest period there were only inscriptions from 575 AD onwards. Nannaya's (1022–1063) translation of the Sanskrit Mahabharata into Telugu is the earliest piece of Telugu literature as yet discovered. After the demise of Nannaya, there was a kind of social and religious revolution in the Telugu country.
Tikkana (13th century) and Yerrapragada (14th century) continued the translation of the Mahabharata started by Nannaya. Telugu poetry also flourished in this period, especially in the time of Srinatha.
During this period, some Telugu poets translated Sanskrit poems and dramas, while others attempted original narrative poems. The popular Telugu literary form called the Prabandha evolved during this period. Srinatha (1365–1441) was the foremost poet, who popularized this style of composition (a story in verse having a tight metrical scheme). Srinatha's "Sringara Naishadham" is particularly well-known.
The Ramayana poets may also be referred in this context. The earliest Ramayana in Telugu is generally known as the "Ranganatha Ramayana", authored by the chief Gona Buddha Reddy. The works of Pothana (1450–1510), Jakkana (second half of the 14th century) and Gaurana (first half of the 15th century) formed a canon of religious poetry during this period. Padakavitha Pithamaha – Annamayya, contributed many original Telugu Paatalu (Songs) to the language.
The 16th and 17th centuries is regarded as the "golden age" of Telugu literature. Krishnadevaraya's "Amukthamalayadha", and Pedhdhana's "Manucharithra" are regarded as Mahaakaavyaas. Sri Krishnadeva Raya stated "Desa bhashalandu Telugu Lessa" meaning " Telugu is the best among the languages of the nation". Telugu literature flourished in the south in the traditional "samsthanas" (centres) of Southern literature, such as Madurai and Tanjore. This age is often referred to as the Southern Period. There were also an increasing number of poets in this period among the ruling class, women and working class who popularised indigenous (desi) meters.
With the conquest of the Deccan by the Mughals in 1687, Telugu literature entered a lull. Tyagaraja's compositions are some of the known works from this period. Then emerged a period of transition (1850–1910), followed by a long period of Renaissance. Europeans like C.P. Brown played an important role in the development of Telugu language and literature. In common with the rest of India, Telugu literature of this period was increasingly influenced by European literary forms like the novel, short story, prose and drama.
Paravastu Chinnayya Soori (1807–1861) is a well-known Telugu writer who dedicated his entire life to the progress and promotion of Telugu language and literature. Sri Chinnayasoori wrote the "Bala Vyakaranam" in a new style after doing extensive research on Telugu grammar. Other well-known writings by Chinnayasoori are "Neethichandrika", "Sootandhra Vyaakaranamu", "Andhra Dhatumoola", and "Neeti Sangrahamu".
Kandukuri Veeresalingam (1848–1919) is generally considered the father of modern Telugu literature. His novel "Rajasekhara Charitamu" was inspired by the Vicar of Wakefield. His work marked the beginning of a dynamic of socially conscious Telugu literature and its transition to the modern period, which is also part of the wider literary renaissance that took place in Indian culture during this period. Other prominent literary figures from this period are Gurajada Appa Rao, Viswanatha Satyanarayana, Gurram Jashuva, Rayaprolu Subba Rao, Devulapalli Krishnasastri and Srirangam Srinivasa Rao, popularly known as "Mahakavi" Sri Sri. Sri Sri was instrumental in popularising free verse in spoken Telugu ("vaaduka bhasha"), as opposed to the pure form of written Telugu used by several poets in his time. Devulapalli Krishnasastri is often referred to as the Shelley of Telugu literature because of his pioneering works in Telugu Romantic poetry.
Viswanatha Satyanarayana won India's national literary honour, the Jnanpith Award for his magnum opus "Ramayana Kalpavrikshamu". C. Narayana Reddy won the Jnanpith Award in 1988 for his poetic work, "Viswambara". Ravuri Bharadhwaja won the 3rd Jnanpith Award for Telugu literature in 2013 for "Paakudu Raallu", a graphic account of life behind the screen in film industry. "Kanyasulkam", the first social play in Telugu by Gurajada Appa Rao, was followed by the progressive movement, the free verse movement and the Digambara style of Telugu verse. Other modern Telugu novelists include Unnava Lakshminarayana ("Maalapalli"), Bulusu Venkateswarulu ("Bharatiya Tatva Sastram"), Kodavatiganti Kutumba Rao and Buchi Babu.
Telugu support on digital devices.
Telugu input, display, and support was initially provided on the Windows platform. Subsequently, various browsers, office applications, operating systems, and user interfaces were localized for Windows and Linux platforms by vendors and free and open source volunteers. Telugu-capable smart phones were also introduced by vendors in 2013.

</doc>
<doc id="39204" url="http://en.wikipedia.org/wiki?curid=39204" title="Battle of Bennington">
Battle of Bennington

The Battle of Bennington was a battle of the American Revolutionary War, part of the Saratoga campaign, that took place on August 16, 1777, in Walloomsac, New York, about 10 mi from its namesake Bennington, Vermont. A rebel force of 2,000 men, primarily composed of New Hampshire and Massachusetts militiamen, led by General John Stark, and reinforced by men led by Colonel Seth Warner and members of the Green Mountain Boys, decisively defeated a detachment of General John Burgoyne's army led by Lieutenant Colonel Friedrich Baum, and supported by additional men under Lieutenant Colonel Heinrich von Breymann.
Baum's detachment was a mixed force of 700 composed of dismounted Brunswick dragoons, Canadians, Loyalists, and Indians. He was sent by Burgoyne to raid Bennington in the disputed New Hampshire Grants area for horses, draft animals, and other supplies. Believing the town to be only lightly defended, Burgoyne and Baum were unaware that Stark and 1,500 militiamen were stationed there. After a rain-caused standoff, Stark's men enveloped Baum's position, taking many prisoners, and killing Baum. Reinforcements for both sides arrived as Stark and his men were mopping up, and the battle restarted, with Warner and Stark driving away Breymann's reinforcements with heavy casualties.
The battle was a decisive victory for the rebel cause, as it reduced Burgoyne's army in size by almost 1,000 men, led his Indian support to largely abandon him, and deprived him of needed supplies such as cavalry and draft horses and food, all factors that contributed to Burgoyne's eventual surrender at Saratoga. The victory also galvanized colonial support for the independence movement, and played a role in bringing France into the war on the rebel side. The battle anniversary is celebrated in the state of Vermont as Bennington Battle Day.
Background.
With the American Revolutionary War two years old, the British changed their plans. Giving up on the rebellious New England colonies, they decided to split the Thirteen Colonies and isolate New England from what the British believed to be the more loyal southern colonies. The British command devised a grand plan to divide the colonies via a three-way pincer movement. The western pincer, under the command of Barry St. Leger, was repulsed when the Siege of Fort Stanwix failed, and the southern pincer, which was to progress up the Hudson valley from New York City, never started since General William Howe decided instead to capture Philadelphia.
The northern pincer, proceeding southward from Montreal, enjoyed the most success. After the British victories at Hubbardton, Fort Ticonderoga, and Fort Anne, General John Burgoyne proceeded with the Saratoga campaign, with the goal of capturing Albany and gaining control of the Hudson River Valley, where Burgoyne's force could (as the plan went) meet the other pincers, dividing the colonies in two.
British forces.
Burgoyne's progress towards Albany had initially met with great success, including the scattering of Seth Warner's men in the Battle of Hubbardton. However, his advance had slowed to a crawl by late July, due to logistical difficulties, exacerbated by the American destruction of a key road, and the army's supplies began to dwindle. Burgoyne's concern over supplies was magnified in early August when he received word from Howe that he (Howe) was going to Philadelphia, and was not in fact going to advance up the Hudson River valley. In response to a proposal first made on July 22 by the commander of his German troops, Baron Riedesel, Burgoyne sent a detachment of about 800 troops under the command of Lieutenant Colonel Friedrich Baum from Fort Miller on a foraging mission to acquire horses for the German dragoons, draft animals to assist in moving the army, and to harass the enemy. Baum's detachment was primarily made up of dismounted Brunswick dragoons of the Prinz Ludwig regiment. Along the way it was joined by local companies of Loyalists, some Canadians and about 100 Indians, and a company of British sharpshooters. Baum was originally ordered to proceed to the Connecticut River valley where they believed horses could be procured for the dragoons. However, as Baum was preparing to leave, Burgoyne verbally changed the goal to be a supply depot at Bennington, which was believed to be guarded by the remnants of Warner's brigade, about 400 colonial militia.
American forces.
Unknown to Burgoyne, the citizens of the New Hampshire Grants territory (which was then disputed between New York and the Vermont Republic) had appealed to the states of New Hampshire and Massachusetts for protection from the invading army following the British capture of Ticonderoga. New Hampshire responded on July 18 by authorizing John Stark to raise a militia for the defense of the people "or the annoyance of the enemy". Using funds provided by John Langdon, Stark raised 1,500 New Hampshire militiamen in the space of six days, more than ten percent of New Hampshire's male population over the age of sixteen. They were first marched to the Fort at Number 4 (modern Charlestown, New Hampshire), then crossed the river border into the Grants and stopped at Manchester, where Stark conferred with Warner. While in Manchester, General Benjamin Lincoln, whose promotion in preference to Stark had been the cause for Stark's resignation from the Continental Army, attempted to assert Army authority over Stark and his men. Stark refused, stating that he was solely responsible to the New Hampshire authorities. Stark then went on to Bennington with Warner as a guide, while Warner's men remained in Manchester. Lincoln returned to the American camp at Stillwater, where he and General Philip Schuyler hatched a plan for Lincoln, with 500 men, to join with Stark and Warner in actions to harass Burgoyne's communications and supply lines at Skenesboro. Baum's movements significantly altered these plans.
Prelude.
Baum's Germans left Burgoyne's camp at Fort Edward on August 9 and marched to Fort Miller, where they waited until they were joined by the Indians and a company of British marksmen. The company marched off toward Bennington on August 11. In minor skirmishes along the way they learned from prisoners taken that a sizable force was in place at Bennington. On August 14 Baum's men encountered a detachment of Stark's men that had been sent out to investigate reports of Indians in the area. Stark's men retreated, destroying a bridge to delay Baum's advance. Stark, on receiving word of the approaching force, sent a request to Manchester for support, and then moved his troops out of Bennington toward Baum's force, setting up a defensive line. Baum sent a message to Burgoyne following the first contact indicating that the American force was larger than expected, but that it was likely to retreat before him. He then advanced a few miles further until he neared Stark's position. He then realized that at least part of his first message was incorrect, so he sent a second message to Burgoyne, requesting reinforcements.
It rained for the next day and a half, preventing battle. During this time, Baum's men constructed a small redoubt at the crest of the hill and hoped that the weather would prevent the Americans from attacking before reinforcements arrived. Stark sent out skirmishers to probe the German lines, and managed to kill thirty Indians in spite of the difficulties of keeping their gunpowder dry. Reinforcements for both sides marched out on the 15th; travel was quite difficult due to the heavy rains. Burgoyne sent 550 men under Heinrich von Breymann, while Warner's company of about 350 Green Mountain Boys came south from Manchester under Lieutenant Samuel Safford's command.
Late on the night of August 15, Stark was awakened by the arrival of Parson Thomas Allen and a band of Massachusetts militiamen from nearby Berkshire County who insisted on joining his force. In response to the minister's fiery threat that his men would never come out again if they were not allowed to participate, Stark is reported to have said, "Would you go now on this dark and rainy night? Go back to your people and tell them to get some rest if they can, and if the Lord gives us sunshine to-morrow and I do not give you fighting enough, I will never call on you to come again." Stark's forces again swelled the next day with the arrival of some Stockbridge Indians, bringing his force (excluding Warner's men) to nearly 2,000 men.
Stark was not the only beneficiary of unexpected reinforcements. Baum's force grew by almost 100 when a group of local Loyalists arrived in his camp on the morning of August 16.
Battle.
On the afternoon of August 16, the weather cleared, and Stark ordered his men to be ready to attack. Stark is reputed to have rallied his troops by saying, "There are your enemies, the Red Coats and the Tories. They are ours, or this night Molly Stark sleeps a widow." Upon hearing that the militia had melted away into the woods, Baum assumed that the Americans were retreating or redeploying. However, Stark had decided to capitalize on weaknesses in the German's widely distributed position, and had sent sizable flanking parties to either side of his lines. These movements were assisted by a ruse employed by Stark's men that enabled them to get closer safely without alarming the opposing forces. The Germans, most of whom spoke no English, had been told that soldiers with bits of white paper in their hat were Loyalists, and should not be fired on; Stark's men had also heard this and many of them had suitably adorned their hats.
When the fighting broke out around 3:00 PM the German position was immediately surrounded by gunfire, which Stark described as "the hottest engagement I have ever witnessed, resembling a continual clap of thunder." The Loyalists and Indian positions were overrun, causing many of them to flee or surrender. This left Baum and his Brunswick dragoons trapped alone on the high ground. The Germans fought valiantly even after running low on powder and the destruction of their ammunition wagon. In desperation the dragoons led a sabre charge in an attempt to break through the enveloping forces. Baum was mortally wounded in this final charge, and the remaining Germans surrendered.
After the battle ended, while Stark's militiamen were busy disarming the prisoners and looting their supplies, Breymann arrived with his reinforcements. Seeing the Americans in disarray, they immediately pressed their attack. After hastily regrouping, Stark's forces tried to hold their ground against the new German onslaught, but began to fall back. Before their lines collapsed, Warner's men arrived on the scene to reinforce Stark's troops. Pitched battle continued until dark, when both sides disengaged. Breymann began a hasty retreat; he had lost one quarter of his force and all of his artillery pieces.
Aftermath.
Total German and British losses at Bennington were recorded at 207 dead and 700 captured; American losses included 30 Americans dead and 40 wounded. The battle was at times particularly brutal when Loyalists met Patriots, as in some cases they came from the same communities. The prisoners, who were first kept in Bennington, were eventually marched to Boston.
Burgoyne's army was readying to cross the Hudson at Fort Edward on August 17 when the first word of the battle arrived. Believing that reinforcements might be necessary, Burgoyne marched the army toward Bennington until further word arrived that Breymann and the remnants of his force were returning. Stragglers continued to arrive throughout the day and night, while word of the disaster spread within the camp.
The effect on Burgoyne's campaign was significant. Not only had he lost nearly 1,000 men, of which half were regulars, but he also lost the crucial Indian support. In a council following the battle, many of the Indians (who had traveled with him from Quebec) decided to go home. This loss severely hampered Burgoyne's reconnaissance efforts in the days to come. The failure to bring in nearby supplies meant that he had to rely on supply lines that were already dangerously long, and that he eventually broke in September. The shortage of supplies was a significant factor in his decision to surrender at Saratoga, following which France entered the war.
American Patriots reacted to news of the battle with optimism. Especially after Burgoyne's Indian screen left him, small groups of local Patriots began to emerge to harass the fringes of British positions. Interestingly, a significant portion of Stark's force returned home and did not again become influential in the campaign until appearing at Saratoga on October 13 to complete the encirclement of Burgoyne's army.
John Stark's reward from the New Hampshire General Assembly for "the Memorable Battle of Bennington" was "a compleat suit of Clothes becoming his Rank". A reward that Stark likely valued the highest was a message of thanks from John Hancock, president of the Continental Congress, which included a commission as "brigadier in the army of the United States".
Order of battle.
The battle forces are generally described as in Morrissey. His numbers are generally consistent with other sources on the British units, although there is disagreement across a wide array of sources on the number of troops under Breymann, which are generally listed at either approximately 550 or 650. Morrissey is also incorrect in identifying some of the American units. He identifies William Gregg as having a separate command; Gregg apparently led several companies in Nichols' regiment. Morrissey also failed to include the Massachusetts militia, and misidentified Langdon's company, erroneously believing they may have been from Worcester, Massachusetts. (Militia companies from the Worcester area marched on Bennington, with some companies arriving the day after the battle.) Langdon originally raised his company in 1776, but it did not become a cavalry unit until 1778.
Commemorations.
August 16 is a legal holiday in Vermont, known as Bennington Battle Day. The battlefield, now a New York state historic site, was designated a National Historic Landmark on January 20, 1961, and added to the National Register of Historic Places on October 15, 1966. In the 1870s, the local historic society in Bennington commissioned the design and construction of the Bennington Battle Monument, which was complete in 1889 and dedicated in 1891 with ceremonies attended by President Benjamin Harrison. The Monument, an obelisk 306 ft high, is also listed on the National Register of Historic Places. Although the monument was not ready in time to mark the centennial of the battle, the 100th anniversary of the battle was marked by speeches attended by President Rutherford B. Hayes.
References.
</dl>

</doc>
<doc id="39205" url="http://en.wikipedia.org/wiki?curid=39205" title="Asian Games">
Asian Games

The Asian Games, also known as Asiad, is a Pancontinental multi-sport event held every four years among athletes from all over Asia. The Games were regulated by the Asian Games Federation (AGF) from the first Games in New Delhi, India, until the 1978 Games. Since the 1982 Games they have been organized by the Olympic Council of Asia (OCA), after the breakup of the Asian Games Federation. The Games are recognized by the International Olympic Committee (IOC) and are described as the second largest multi-sport event after the Olympic Games.
In its history, nine nations have hosted the Asian Games. Forty-six nations have participated in the Games, including Israel, which was excluded from the Games after their last participation in 1974.
The last Games was held in Incheon, South Korea from 19 September to 4 October 2014.
History.
Prior formation.
Before the Asian Games were held, a gathering known as the Far Eastern Championship Games existed which was first mooted in 1912 at a location set between the Empire of Japan, the Philippine Islands, and China. The Far Eastern Games were first held in Manila in 1913. Ten more Far Eastern Games were held until 1934. Against the backdrop of the second Sino-Japanese War in 1934, in the face of Japan's insistence on including Manchu Empire as a competitor nation in the Games, China announced its withdrawal from participation. Consequently, the Far Eastern Games scheduled for 1938 were cancelled. The organization was ultimately discontinued
Formation.
After World War II, a number of Asian countries became independent. Many of the newly independent Asian countries desired the formation of a new type of competition whereby Asian dominance was not expressed through violence, but instead strengthened through mutual understanding. During the 1948 Summer Olympics in London, a conversation between sportsmen from China and the Philippines raised the idea of restoring the Far Eastern Games. However, Guru Dutt Sondhi, the Indian International Olympic Committee representative, did not believe that restoration of the Far Eastern Games would sufficiently display the spirit of unity and level of achievement taking place in Asian sports. As a result, he proposed to sports leaders the idea of having a wholly new competition  - which came to be the Asian Games. This led to an agreement to form the Asian Athletic Federation. A preparatory committee was then set up to draft the charter for this new body. On 13 February 1949, the Asian Athletic Federation was formally inaugurated in New Delhi, alongside the name Asian Games Federation, with New Delhi announced as the first host city of the Asian Games which were scheduled to be held in 1950.
Crisis, reorganization, expansion.
Starting in 1962, the Games were hit by several crises. First, the host country Indonesia, refused to permit the participation of Israel and the Republic of China due to political and religious issues. As a result, the IOC removed its sponsorship of the Games and terminated Indonesia as one of the IOC members. The Asian Football Confederation (AFC), International Amateur Athletics Federation (IAAF) and International Weightlifting Federation (IWF), also removed their recognition of the Games.
In 1970, South Korea dropped its plan to host the Games allegedly due to national security crisis, however the main reason was due to financial crisis, forcing the previous host Thailand to administer the Games again in Bangkok using funds transferred from South Korea. Prior to the Games, Japan was asked to host the Games, but declined due to Expo '70 in Osaka. This edition also marked the first time the Games have a television broadcasting throughout the world. In 1974, the Games formally recognized the participation of China, North Korea and Mongolia. Israel was allowed to participate despite the opposition from Arab world, while Taiwan was permitted to continue taking part (as "Chinese Taipei" despite its status was abolished in general meeting on 16 November 1973 by Games Federation.
The last is 1978, Pakistan dropped its plan to host the Games in 1975 due to financial crisis and political issues. Thailand offered to help and the Games were once again held in Bangkok. However, once again, like in 1962, Taiwan and Israel were refused the participation by Games Federation, amid political issues and security fears. Several governing bodies protested against the ban, like IAAF, threatened to bar the participating players from 1980 Summer Olympics, this caused several teams to withdraw prior to the Games.
Following this series of crises, the National Olympic Committee in Asia decided to revise the constitution of the Asian Games Federation. A new association, named the Olympic Council of Asia, was created in November 1981 with the exclusion of Israel. India was already scheduled to host the 1982 Games and the OCA decided not to drop the old AGF timetable. The OCA formally supervised the Games starting with the 1986 Asian Games in South Korea. In the succeeding Games, Taiwan (Republic of China) was re-admitted, but was forced by the People's Republic of China to compete under the name "Chinese Taipei".
In 1994, the Games included the former republics of the Soviet Union: Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan and Uzbekistan for the first time. It was also the first time that the Games had been held outside the capital city of the host country. However, Iraq was suspended from the Games due to the Persian Gulf War in 1990, while North Korea boycotted the Games due to political issues. It was also marred by the death of Nepalese delegation Nareshkumar Adhikari during the Games' opening ceremony.
The 1998 Games marked the fourth time the Games had been held in Bangkok, Thailand. The fourth opening ceremony occurred on 6 December, compared to 9 December for the previous 3. All four games were opened by H.M.King Bhumibol Adulyadej. The date of the closing ceremony remained as 20 December for all 4 games hosted by Thailand.
Symbols.
Similar to the Olympic Games, the Olympic Council of Asia also uses symbols to represent the ideals of the Asian Games, namely:
Participation.
All 45 members affiliated to the Olympic Council of Asia (OCA) are eligible to take part in the Games. In history, 46 National Olympic Committees (NOCs) have sent competitors to the Games. Israel has been excluded from the Games since 1976, the reason cited as being due to security reasons. Israel requested to participate in the 1982 Games, but the request was rejected by the organizers due to incident in 1972 Summer Olympics. Israel is now a member of the European Olympic Committees (EOC).
Due to its continuing ambiguous political status, Taiwan has participated in the Games under the flag of Chinese Taipei since 1990. Macau is allowed to compete as one of the NOCs in Asian Games, despite not being recognized by the International Olympic Committee (IOC) for participation in the Olympic Games.
In 2007, the President of OCA, Sheikh Ahmed Al-Fahad Al-Ahmed Al-Sabah, rejected the proposal to allow Australia to participate in the Games. He stated that while Australia would add good value to the Asian Games, it would be unfair to the other NOCs in Oceania.
Only seven countries, namely India, Indonesia, Japan, the Philippines, Sri Lanka, Singapore and Thailand have competed in all editions of the games.
Sports.
Forty-four sports were presented in Asian Games history, including 2014 Games in Incheon.
Medal count.
Of the 46 National Olympic Committees participating throughout the history of the Games, 43 nations have won at least a single medal in the competition, leaving three nations: Bhutan, Maldives and Timor-Leste yet to win a single medal. 37 nations have won at least one gold medal (only Japan and India have done so at every Asian Games), while Japan and China became the only two nations in history to emerge as overall champions.
Samsung MVP award.
Samsung introduced the Most Valuable Player (MVP) award in Asian Games beginning in the 1998 Games in Bangkok, Thailand. Below is the list of winners:
Centennial Festival.
On 8 November 2012, the OCA decided at its 31st General Assembly in Macau to create a special multi-sport event called "Asian Games Centennial Festival" in celebration of the 100th anniversary of the Oriental Games (later became Far Eastern Championship Games). OCA awarded the Philippines the hosting rights as it was the same host 100 years ago. The event was originally scheduled to be held in Boracay Island, Malay, Aklan on 27 to 29 November 2013 but due to the events it was moved to January 2014.

</doc>
<doc id="39206" url="http://en.wikipedia.org/wiki?curid=39206" title="Business">
Business

A business, also known as an enterprise or a firm, is an organization involved in the trade of goods, services, or both to consumers. Businesses are prevalent in capitalist economies, where most of them are privately owned and provide goods and services to customers in exchange for other goods, services, or money. Businesses may also be not-for-profit or state-owned. A business owned by multiple individuals may be referred to as a company.
Business can refer to a particular organization or to an entire market sector, e.g. "the music business". Compound forms such as agribusiness represent subsets of the word's broader meaning, which encompasses all activity by suppliers of goods and services. The goal is for sales to be more than expenditures resulting in a profit.
Basic forms of ownership.
Forms of business ownership vary by jurisdiction, but several common forms exist:
Management.
The efficient and effective operation of a business, and study of this subject, is called management. The major branches of management are financial management, marketing management, human resource management, strategic management, production management, operations management, service management and information technology management. 
Owners may administer their businesses themselves, or employ managers to do this for them. Whether they are owners or employees, managers administer three primary components of the business' value: its financial resources, capital or tangible resources, and human resources. These resources are administered in at least five functional areas: legal contracting, manufacturing or service production, marketing, accounting, financing, and human resources.
Restructuring state enterprises.
In recent decades, various states modeled some of their assets and enterprises after business enterprises. In 2003, for example, the People's Republic of China modeled 80% of its state-owned enterprises on a company-type management system. Many state institutions and enterprises in China and Russia have transformed into joint-stock companies, with part of their shares being listed on public stock markets.
Business process management (BPM) is a holistic management approach focused on aligning all aspects of an organization with the wants and needs of clients. It promotes business effectiveness and efficiency while striving for innovation, flexibility, and integration with technology. BPM attempts to improve processes continuously. It can therefore be described as a "process optimization process." It is argued that BPM enables organizations to be more efficient, effective and capable of change than a functionally focused, traditional hierarchical management approach. 
Organization and government regulation.
Most legal jurisdictions specify the forms of ownership that a business can take, creating a body of commercial law for each type.
The major factors affecting how a business is organized are usually:
Many businesses are operated through a separate entity such as a corporation or a partnership (either formed with or without limited liability). Most legal jurisdictions allow people to organize such an entity by filing certain charter documents with the relevant Secretary of State or equivalent and complying with certain other ongoing obligations. The relationships and legal rights of shareholders, limited partners, or members are governed partly by the charter documents and partly by the law of the jurisdiction where the entity is organized. Generally speaking, shareholders in a corporation, limited partners in a limited partnership, and members in a limited liability company are shielded from personal liability for the debts and obligations of the entity, which is legally treated as a separate "person". This means that unless there is misconduct, the owner's own possessions are strongly protected in law if the business does not succeed.
Where two or more individuals own a business together but have failed to organize a more specialized form of vehicle, they will be treated as a general partnership. The terms of a partnership are partly governed by a partnership agreement if one is created, and partly by the law of the jurisdiction where the partnership is located. No paperwork or filing is necessary to create a partnership, and without an agreement, the relationships and legal rights of the partners will be entirely governed by the law of the jurisdiction where the partnership is located. A single person who owns and runs a business is commonly known as a "sole proprietor", whether that person owns it directly or through a formally organized entity.
A few relevant factors to consider in deciding how to operate a business include:
Commercial law.
A very detailed and well-established body of rules that evolved over a very long period of time applies to commercial transactions. The need to regulate trade and commerce and resolve business disputes helped shape the creation of law and courts. The Code of Hammurabi dates back to about 1772 BC for example, and contains provisions that relate, among other matters, to shipping costs and dealings between merchants and brokers. The word "corporation" derives from the Latin "corpus", meaning body, and the Maurya Empire in Iron-Age India accorded legal rights to business entities.
In many countries it is difficult to compile all the laws that can affect a business into a single reference source. Laws can govern treatment of labour and employee relations, worker protection and safety, discrimination on the basis of age, gender, disability, race, and in some jurisdictions, sexual orientation, and the minimum wage, as well as unions, worker compensation, and working hours and leave.
Some specialized businesses may also require licenses, either due to laws governing entry into certain trades, occupations or professions, that require special education, or to raise revenue for local governments. Professions that require special licenses include law, medicine, piloting aircraft, selling liquor, radio broadcasting, selling investment securities, selling used cars, and roofing. Local jurisdictions may also require special licenses and taxes just to operate a business.
Some businesses are subject to ongoing special regulation, for example, public utilities, investment securities, banking, insurance, broadcasting, aviation, and health care providers. Environmental regulations are also very complex and can affect many businesses.
Capital.
 When businesses need to raise money (called capital), they sometimes offer securities for sale.
Capital may be raised through private means, by an initial public offering or IPO on a stock exchange, or in other ways.
Major stock exchanges include the Shanghai Stock Exchange, Singapore Exchange, Hong Kong Stock Exchange, New York Stock Exchange and Nasdaq (USA), the London Stock Exchange (UK), the Tokyo Stock Exchange (Japan), and Bombay Stock Exchange (India). Most countries with capital markets have at least one.
Businesses that have gone public are subject to regulations concerning their internal governance, such as how executive officers' compensation is determined, and when and how information is disclosed to shareholders and to the public. In the United States, these regulations are primarily implemented and enforced by the United States Securities and Exchange Commission (SEC). Other Western nations have comparable regulatory bodies. The regulations are implemented and enforced by the China Securities Regulation Commission (CSRC) in China. In Singapore, the regulation authority is the Monetary Authority of Singapore (MAS), and in Hong Kong, it is the Securities and Futures Commission (SFC).
The proliferation and increasing complexity of the laws governing business have forced increasing specialization in corporate law. It is not unheard of for certain kinds of corporate transactions to require a team of five to ten attorneys due to sprawling regulation. Commercial law spans general corporate law, employment and labor law, health-care law, securities law, mergers and acquisitions, tax law, employee benefit plans, food and drug regulation, intellectual property law on copyrights, patents, trademarks and such, telecommunications law, financing.
Other types of capital sourcing includes crowd sourcing on the internet, venture capital, bank loans and debentures.
Intellectual property.
Businesses often have important "intellectual property" that needs protection from competitors for the company to stay profitable. This could require patents, copyrights, trademarks or preservation of trade secrets. Most businesses have names, logos and similar branding techniques that could benefit from trademarking. Patents and copyrights in the United States are largely governed by federal law, while trade secrets and trademarking are mostly a matter of state law. Because of the nature of intellectual property, a business needs protection in every jurisdiction in which they are concerned about competitors. Many countries are signatories to international treaties concerning intellectual property, and thus companies registered in these countries are subject to national laws bound by these treaties. In order to protect trade secrets, companies may require employees to sign non-compete clauses which will impose limitations on an employee's interactions with stakeholders, and competitors.

</doc>
<doc id="39208" url="http://en.wikipedia.org/wiki?curid=39208" title="Commerce">
Commerce

Commerce is the activity of buying and selling, especially on a large scale. The system includes legal, economic, political, social, cultural and technological systems that are in operation in any country or internationally. Thus, commerce is a system or an environment that affects the business prospects of economies. It can also be defined as a component of business which includes all activities, functions and institutions involved in transferring goods from producers to consumers.
History.
Some commentators trace the origins of commerce to the very start of communication in prehistoric times. Apart from traditional self-sufficiency, trading became a principal facility of prehistoric people, who bartered what they had for goods and services from each other. Historian Peter Watson dates the history of long-distance commerce from circa 150,000 years ago.
In historic times, the introduction of currency as a standardized money, facilitated a wider exchange of goods and services. Numismatists have collections of these monetary tokens, which include coins from some Ancient World large-scale societies, although initial usage involved unmarked lumps of precious metal.
The circulation of a standardized currency provides a method of overcoming the major disadvantage to commerce through use of a barter system, the "double coincidence of wants" necessary for barter trades to occur. For example, if a man (or woman) who makes pots for a living needs a new house, he/she may wish to hire someone to build it for him/her. But he/she cannot make an equivalent number of pots to equal this service done for him/her, because even if the builder could build the house, the builder might not want many or any pots. Currency solved this problem by allowing a society as a whole to assign values and thus to collect goods and services effectively and to store them for later use, or to split them among several providers.
Today[ [update]] commerce includes as a subset a complex system of companies which try to maximize their profits by offering products and services to the market (which consists both of individuals and other companies) at the lowest production cost. A system of international trade has helped to develop the world economy but, in combination with bilateral or multilateral agreements to lower tariffs or to achieve free trade, has sometimes harmed third-world markets for local products, it has been argued. ("See" Globalization.)

</doc>
<doc id="39209" url="http://en.wikipedia.org/wiki?curid=39209" title="Philipp Melanchthon">
Philipp Melanchthon

Philipp Melanchthon (; 16 February 1497 – 19 April 1560), born Philipp Schwartzerdt (]), was a German reformer, collaborator with Martin Luther, the first systematic theologian of the Protestant Reformation, intellectual leader of the Lutheran Reformation, and an influential designer of educational systems. He stands next to Luther and Calvin as a reformer, theologian, and molder of Protestantism. Along with Luther, he is the primary founder of Lutheranism. They both denounced what they believed was the exaggerated cult of the saints, asserted justification by faith, and denounced the coercion of the conscience in the sacrament of penance by the Catholic Church, that they believed could not offer certainty of salvation. In unison they rejected transubstantiation, the belief that the bread from the Lord's Supper becomes Christ's body when consumed. Melanchthon made the distinction between law and gospel the central formula for Lutheran evangelical insight. By the "law", he meant God's requirements both in Old and New Testament; the "gospel" meant the free gift of grace through faith in Jesus Christ.
Early life and education.
He was born Philipp Schwartzerdt (of which "Melanchthon" is a Greek translation) on 16 February 1497, at Bretten, near Karlsruhe, where his father Georg Schwarzerdt was armorer to Philip, Count Palatine of the Rhine. His birthplace, along with almost the whole city of Bretten, was burned in 1689 by French troops during the War of the Palatinate Succession. The town's Melanchthonhaus was built on its site in 1897.
In 1507 he was sent to the Latin school at Pforzheim, where the rector, Georg Simler of Wimpfen, introduced him to the Latin and Greek poets and Aristotle. He was influenced by his great-uncle Johann Reuchlin, brother of his maternal grandmother, a representative humanist. It was Reuchlin who suggested the change from "Schwartzerdt" (literally "black earth"), into the Greek equivalent "Melanchthon" (Μελάγχθων), a custom which was usual among humanists of that time.
Still young, he entered in 1509 the University of Heidelberg where he studied philosophy, rhetoric, and astronomy/astrology, and was known as a good Greek scholar. On being refused the degree of master in 1512 on account of his youth, he went to Tübingen, where he continued humanistic studies, but also worked on jurisprudence, mathematics, and medicine. While there, he was taught the technical aspects of astrology by Johannes Stöffler.
Having taken the degree of master in 1516, he began to study theology. Under the influence of men like Reuchlin and Erasmus he became convinced that true Christianity was something different from scholastic theology as it was taught at the university. He became a "conventor" (repentant) in the "contubernium" and instructed younger scholars. He also lectured on oratory, on Virgil and Livy.
His first publications were an edition of Terence (1516) and his Greek grammar (1518), but he had written previously the preface to the "Epistolae clarorum virorum" of Reuchlin (1514).
Professor at Wittenberg.
Opposed as a reformer at Tübingen, he accepted a call to the University of Wittenberg by Martin Luther, recommended by his great-uncle Johann Reuchlin. Melanchthon became professor of the Greek language in Wittenberg at the age of 21. He studied the Scripture, especially of Paul, and Evangelical doctrine. He was present at the disputation of Leipzig (1519) as a spectator, but participated by his comments. Johann Eck having attacked his views, Melanchthon replied based on the authority of Scripture in his "Defensio contra Johannem Eckium" (Wittenberg, 1519).
After lectures on the Gospel of Matthew and the Epistle to the Romans, together with his investigations into the doctrines of Paul, he was granted the degree of bachelor of theology, and was transferred to the theological faculty. He married Katharina Krapp, daughter of Wittenberg's mayor, on 25 November 1520.
Theological disputes.
In the beginning of 1521 in his "Didymi Faventini versus Thomam Placentinum pro M. Luthero oratio" (Wittenberg, n.d.), he defended Luther. He argued that Luther rejected only papal and ecclesiastical practises which were at variance with Scripture. But while Luther was absent at Wartburg Castle, during the disturbances caused by the Zwickau prophets, Melanchthon wavered.
The appearance of Melanchthon's "Loci communes rerum theologicarum seu hypotyposes theologicae" (Wittenberg and Basel, 1521) was of subsequent importance for Reformation. Melanchthon presented the new doctrine of Christianity under the form of a discussion of the "leading thoughts" of the Epistle to the Romans. "Loci communes" began the gradual rise of the Lutheran scholastic tradition, and the later theologians Martin Chemnitz, Mathias Haffenreffer, and Leonhard Hutter expanded upon it. Melanchthon continued to lecture on the classics.
On a journey in 1524 to his native town, he encountered the papal legate, Cardinal Lorenzo Campeggio, who tried to draw him from Luther's cause. In his "Unterricht der Visitatorn an die Pfarherrn im Kurfürstentum zu Sachssen" (1528) Melanchthon presented the evangelical doctrine of salvation as well as regulations for churches and schools.
In 1529 he accompanied the elector to the Diet of Speyer. His hopes of inducing the Imperial party to a recognition of the Reformation were not fulfilled. A friendly attitude towards the Swiss at the Diet was something he later changed, calling Zwingli's doctrine of the Lord's Supper "an impious dogma".
Augsburg Confession.
The composition now known as the "Augsburg Confession" was laid before the Diet of Augsburg in 1530, and would come to be considered perhaps the most significant document of the Protestant Reformation. While the confession was based on Luther's Marburg and Schwabach articles, it was mainly the work of Melanchthon. Although commonly thought of as a unified statement of doctrine by the two reformers, Luther did not conceal his dissatisfaction with the irenic tone of the confession. Indeed, some would criticize Melanchthon's conduct at the Diet as unbecoming of the principle he promoted, implying that faith in the truth of his cause would logically have inspired Melanchthon to a more firm and dignified posture. Others point out that he had not sought the part of a political leader, suggesting that he seemed to lack the requisite energy and decision for such a role and may simply have been a lackluster judge of human nature.
Melanchthon then settled into the comparative quiet of his academic and literary labors. His most important theological work of this period was the "Commentarii in Epistolam Pauli ad Romanos" (Wittenberg, 1532), noteworthy for introducing the idea that "to be justified" means "to be accounted just," whereas the Apology had placed side by side the meanings of "to be made just" and "to be accounted just." Melanchthon's increasing fame gave occasion for several honorable calls to Tübingen (Sept., 1534), to France, and to England, but consideration of the elector caused him to refuse them.
Discussions on Lord's Supper and Justification.
He took an important part in the discussions concerning the Lord's Supper which began in 1531. He approved fully of the Wittenberg Concord sent by Bucer to Wittenberg, and at the instigation of the Landgrave of Hesse discussed the question with Bucer in Kassel, at the end of 1534. He eagerly labored for an agreement, for his patristic studies and the Dialogue (1530) of Œcolampadius had made him doubt the correctness of Luther's doctrine. Moreover, after the death of Zwingli and the change of the political situation his earlier scruples in regard to a union lost their weight. Bucer did not go so far as to believe with Luther that the true body of Christ in the Lord's Supper is bitten by the teeth, but admitted the offering of the body and blood in the symbols of bread and wine. Melanchthon discussed Bucer's views with the most prominent adherents of Luther; but Luther himself would not agree to a mere veiling of the dispute. Melanchthon's relation to Luther was not disturbed by his work as a mediator, although Luther for a time suspected that Melanchthon was "almost of the opinion of Zwingli"; nevertheless he desired to "share his heart with him."
During his sojourn in Tübingen in 1536 Melanchthon was severely attacked by Cordatus, preacher in Niemeck, because he had taught that works are necessary for salvation. In the second edition of his Loci (1535) he abandoned his earlier strict doctrine of determinism which went even beyond that of Augustine, and in its place taught more clearly his so-called Synergism. He repulsed the attack of Cordatus in a letter to Luther and his other colleagues by stating that he had never departed from their common teachings on this subject, and in the Antinomian Controversy of 1537 Melanchthon was in harmony with Luther.
Controversies with Flacius.
The last eventful and sorrowful period of his life began with controversies over the Interims and the Adiaphora (1547). It is true, Melanchthon rejected the Augsburg Interim, which the emperor tried to force upon the defeated Protestants; but in the negotiations concerning the so-called Leipzig Interim he made concessions which many feel can in no way be justified, even if one considers his difficult position, opposed as he was to the elector and the emperor.
In agreeing to various Roman usages, Melanchthon started from the opinion that they are adiaphora if nothing is changed in the pure doctrine and the sacraments which Jesus instituted, but he disregarded the position that concessions made under such circumstances have to be regarded as a denial of Evangelical convictions.
Melanchthon himself perceived his faults in the course of time and repented of them, perhaps having to suffer more than was just in the displeasure of his friends and the hatred of his enemies. From now on until his death he was full of trouble and suffering. After Luther's death he became the "theological leader of the German Reformation," not indisputably, however; for the Lutherans with Matthias Flacius at their head accused him and his followers of heresy and apostasy. Melanchthon bore all accusations and calumnies with admirable patience, dignity, and self-control.
Disputes with Osiander and Flacius.
In his controversy on justification with Andreas Osiander Melanchthon satisfied all parties. Melanchthon took part also in a controversy with Stancari, who held that Christ was our justification only according to his human nature.
He was also still a strong opponent of the Roman Catholics, for it was by his advice that the elector of Saxony declared himself ready to send deputies to a council to be convened at Trent, but only under the condition that the Protestants should have a share in the discussions, and that the Pope should not be considered as the presiding officer and judge. As it was agreed upon to send a confession to Trent, Melanchthon drew up the "Confessio Saxonica" which is a repetition of the Augsburg Confession, discussing, however, in greater detail, but with moderation, the points of controversy with Rome. Melanchthon on his way to Trent at Dresden saw the military preparations of Maurice of Saxony, and after proceeding as far as Nuremberg, returned to Wittenberg in March 1552, for Maurice had turned against the emperor. Owing to his act, the condition of the Protestants became more favorable and were still more so at the Peace of Augsburg (1555), but Melanchthon's labors and sufferings increased from that time.
The last years of his life were embittered by the disputes over the Interim and the freshly-started controversy on the Lord's Supper. As the statement "good works are necessary for salvation" appeared in the Leipzig Interim, its Lutheran opponents attacked in 1551 Georg Major, the friend and disciple of Melanchthon, so Melanchthon dropped the formula altogether, seeing how easily it could be misunderstood.
But all his caution and reservation did not hinder his opponents from continually working against him, accusing him of synergism and Zwinglianism. At the Colloquy of Worms in 1557 which he attended only reluctantly, the adherents of Flacius and the Saxon theologians tried to avenge themselves by thoroughly humiliating Melanchthon, in agreement with the malicious desire of the Roman Catholics to condemn all heretics, especially those who had departed from the Augsburg Confession, before the beginning of the conference. As this was directed against Melanchthon himself, he protested, so that his opponents left, greatly to the satisfaction of the Roman Catholics who now broke off the colloquy, throwing all blame upon the Protestants. The Reformation in the sixteenth century did not experience a greater insult, as Nietzsche says.
Nevertheless, Melanchthon persevered in his efforts for the peace of the Church, suggesting a synod of the Evangelical party and drawing up for the same purpose the Frankfurt Recess, which he defended later against the attacks of his enemies.
More than anything else the controversies on the Lord's Supper embittered the last years of his life. The renewal of this dispute was due to the victory in the Reformed Church of the Calvinistic doctrine and its influence upon Germany. To its tenets Melanchthon never gave his assent, nor did he use its characteristic formulas. The personal presence and self-impartation of Christ in the Lord's Supper were especially important for Melanchthon; but he did not definitely state how body and blood are related to this. Although rejecting the physical act of mastication, he nevertheless assumed the real presence of the body of Christ and therefore also a real self-impartation. Melanchthon differed from Calvin also in emphasizing the relation of the Lord's Supper to justification.
Marian views.
Melanchthon viewed any veneration of saints rather critically but developed positive commentaries about Mary. In his "Annotations in Evangelia" commenting on Lk 2,52, he discusses the faith of Mary, "“she kept all things in her heart”" which to Melanchton is a call to the Church to follow her example. During the marriage at Cana, Melanchton points out that Mary went too far, asking for more wine, misusing her position. But she was not upset, when Jesus gently scolded her. Mary was negligent, when she lost her son in the temple, but she did not sin. Mary was conceived with original sin like every other human being, but she was spared the consequences of it. Consequently, Melanchton opposed the feast of the Immaculate Conception, which in his days, although not dogma, was celebrated in several cities and had been approved at the Council of Basel in 1439. He declared that the Immaculate Conception was an invention of monks. Mary is a representation (Typus) of the Church and in the Magnificat, Mary spoke for the whole Church. Standing under the cross, Mary suffered like no other human being. Consequently Christians have to unite with her under the Cross, in order to become Christ-like.
Views on natural philosophy.
In lecturing on the "Librorum de judiciis astrologicis" of Ptolemy in 1535–6, Melanchthon expressed to students his interest in Greek mathematics, astronomy and astrology. He considered that a purposeful God had reasons to exhibit comets and eclipses. He was the first to print a paraphrased edition of Ptolemy's Tetrabiblos in Basel, 1554. Natural philosophy, in his view, was directly linked to Providence, a point of view that was influential in curriculum change after the Protestant Reformation in Germany. In the period 1536–9 he was involved in three academic innovations: the refoundation of Wittenberg along Protestant lines, the reorganisation at Tübingen, and the foundation of the University of Leipzig.
Death.
But before these and other theological dissensions were ended, he died. A few days before his death he committed to writing his reasons for not fearing it. On the left were the words, "Thou shalt be delivered from sins, and be freed from the acrimony and fury of theologians"; on the right, "Thou shalt go to the light, see God, look upon his Son, learn those wonderful mysteries which thou hast not been able to understand in this life." The immediate cause of death was a severe cold which he had contracted on a journey to Leipzig in March, 1560, followed by a fever that consumed his strength, weakened by many sufferings. On 19 April 1560 he was announced dead.
The only care that occupied him until his last moment was the desolate condition of the Church. He strengthened himself in almost uninterrupted prayer, and in listening to passages of Scripture. Especially significant did the words seem to him, "His own received him not; but as many as received him, to them gave he power to become the sons of God." When Caspar Peucer, his son in-law, asked him if he wanted anything, he replied, "Nothing but heaven." His body was buried beside Luther's in the Schloßkirche in Wittenberg.
He is commemorated in the Calendar of Saints of the Lutheran Church–Missouri Synod on February 16 (the date of his birth) and of the Evangelical Lutheran Church in America on June 25 (The date of the presentation of the Augsburg Confession).
Estimation of his works and character.
Melanchthon's importance for the Reformation lay essentially in the fact that he systematized Luther's ideas, defended them in public, and made them the basis of a religious education. These two, by complementing each other, could be said to have harmoniously achieved the results of the Reformation. Melanchthon was impelled by Luther to work for the Reformation; his own inclinations would have kept him a student. Without Luther's influence Melanchthon would have been "a second Erasmus," although his heart was filled with a deep religious interest in the Reformation. While Luther scattered the sparks among the people, Melanchthon by his humanistic studies won the sympathy of educated people and scholars for the Reformation. Besides Luther's strength of faith, Melanchthon's many-sidedness and calmness, as well as his temperance and love of peace, had a share in the success of the movement.
Both were aware of their mutual position and they thought of it as a divine necessity of their common calling. Melanchthon wrote in 1520, "I would rather die than be separated from Luther," whom he afterward compared to Elijah, and called "the man full of the Holy Ghost." In spite of the strained relations between them in the last years of Luther's life, Melanchthon exclaimed at Luther's death, "Dead is the horseman and chariot of Israel who ruled the Church in this last age of the world!"
On the other hand, Luther wrote of Melanchthon, in the preface to Melanchthon's Commentary on the Galatians (1529), "I had to fight with rabble and devils, for which reason my books are very warlike. I am the rough pioneer who must break the road; but Master Philipp comes along softly and gently, sows and waters heartily, since God has richly endowed him with gifts." Luther also did justice to Melanchthon's teachings, praising one year before his death in the preface to his own writings Melanchthon's revised "Loci" above them and calling Melanchthon "a divine instrument which has achieved the very best in the department of theology to the great rage of the devil and his scabby tribe." It is remarkable that Luther, who vehemently attacked men like Erasmus and Bucer, when he thought that truth was at stake, never spoke directly against Melanchthon, and even during his melancholy last years conquered his temper.
The strained relation between these two men never came from external things, such as human rank and fame, much less from other advantages, but always from matters of Church and doctrine, and chiefly from the fundamental difference of their individualities; they repelled and attracted each other "because nature had not formed out of them one man." However, it can not be denied that Luther was the more magnanimous, for however much he was at times dissatisfied with Melanchthon's actions, he never uttered a word against his private character; however Melanchthon sometimes evinced a lack of confidence in Luther. In a letter to Carlowitz, before the Diet of Augsburg, he protested that Luther on account of his hot-headed nature exercised a personally humiliating pressure upon him.
His work as reformer.
As a reformer, Melanchthon was characterized by moderation, conscientiousness, caution, and love of peace; but these qualities were sometimes said to only be lack of decision, consistence, and courage. Often, however, his actions are shown stemming not from anxiety for his own safety, but from regard for the welfare of the community and for the quiet development of the Church.
Melanchthon was not said to lack personal courage, but rather he was said to be less of an aggressive than of a passive nature. When he was reminded how much power and strength Luther drew from his trust in God, he answered, "If I myself do not do my part, I can not expect anything from God in prayer." His nature was seen to be inclined to suffer with faith in God that he would be released from every evil rather than to act valiantly with his aid.
The distinction between Luther and Melanchthon is well brought out in Luther's letters to the latter (June, 1530): "To your great anxiety by which you are made weak, I am a cordial foe; for the cause is not ours. It is your philosophy, and not your theology, which tortures you so,-- as though you could accomplish anything by your useless anxieties. So far as the public cause is concerned, I am well content and satisfied; for I know that it is right and true, and, what is more, it is the cause of Christ and God himself. For that reason, I am merely a spectator. If we fall, Christ will likewise fall; and if he fall, I would rather fall with Christ than stand with the emperor."
Another trait of his character was his love of peace. He had an innate aversion to quarrels and discord; yet, often he was very irritable. His irenical character often led him to adapt himself to the views of others, as may be seen from his correspondence with Erasmus and from his public attitude from the Diet of Augsburg to the Interim. It was said not to be merely a personal desire for peace, but his conservative religious nature that guided him in his acts of conciliation. He never could forget that his father on his death-bed had besought his family "never to leave the Church." He stood toward the history of the Church in an attitude of piety and reverence that made it much more difficult for him than for Luther to be content with the thought of the impossibility of a reconciliation with the Roman Catholic Church. He laid stress upon the authority of the Fathers, not only of Augustine, but also of the Greek Fathers.
His attitude in matters of worship was conservative, and in the Leipsic Interim he was said by Cordatus and Schenk even to be Crypto-Catholic. He never strove for a reconciliation with Roman Catholicism at the price of pure doctrine. He attributed more value to the external appearance and organization of the Church than Luther did, as can be seen from his whole treatment of the "doctrine of the Church." The ideal conception of the Church, which the Reformers opposed to the organization of the Roman Church, which was expressed in his "Loci" of 1535, lost for him after 1537 its former prominence, when he began to emphasize the conception of the true visible Church as it may be found among the Evangelicals.
He believed that the relation of the Church to God was that the Church held the divine office of the ministry of the Gospel. The universal priesthood was for Melanchthon as for Luther no principle of an ecclesiastical constitution, but a purely religious principle. In accordance with this idea Melanchthon tried to keep the traditional church constitution and government, including the bishops. He did not want, however, a church altogether independent of the State, but rather, in agreement with Luther, he believed it the duty of the secular authorities to protect religion and the Church. He looked upon the consistories as ecclesiastical courts which therefore should be composed of spiritual and secular judges, for to him the official authority of the Church did not lie in a special class of priests, but rather in the whole congregation, to be represented therefore not only by ecclesiastics, but also by laymen. Melanchthon in advocating church union did not overlook differences in doctrine for the sake of common practical tasks.
The older he grew, the less he distinguished between the Gospel as the announcement of the will of God, and right doctrine as the human knowledge of it. Therefore he took pains to safeguard unity in doctrine by theological formulas of union, but these were made as broad as possible and were restricted to the needs of practical religion.
As scholar.
As a scholar Melanchthon embodied the entire spiritual culture of his age. At the same time he found the simplest, clearest, and most suitable form for his knowledge; therefore his manuals, even if they were not always original, were quickly introduced into schools and kept their place for more than a century.
Knowledge had for him no purpose of its own; it existed only for the service of moral and religious education, and so the teacher of Germany prepared the way for the religious thoughts of the Reformation. He is the father of Christian humanism, which has exerted a lasting influence upon scientific life in Germany. [But it is Erasmus who is called, "The Prince of the Humanists".]
His works were not always new and original, but they were clear, intelligible, and answered their purpose. His style is natural and plain, better, however, in Latin and Greek than in German. He was not without natural eloquence, although his voice was weak.
Melanchthon wrote numerous treatises dealing with education and learning that present some of his key thoughts on learning, including his views on the basis, method, and goal of reformed education. In his “Book of Visitation,” Melanchthon outlines a school plan that recommends schools to teach Latin only. Here he suggests children should be broken up into three distinct groups: children who are learning to read, children who know how to read and are ready to learn grammar, and children who are well-trained in grammar and syntax. Melanchthon also believed that the disciplinary system of the classical “seven liberal arts,” and the sciences studied in the higher faculties could not encompass the new revolutionary discoveries of the age in terms of either content or method. He expanded the traditional categorization of science in several directions, incorporating not only history, geography and poetry but also the new natural sciences in his system of scholarly disciplines.
As theologian.
As a theologian, Melanchthon did not show so much creative ability, but rather a genius for collecting and systematizing the ideas of others, especially of Luther, for the purpose of instruction. He kept to the practical, and cared little for connection of the parts, so his "Loci" were in the form of isolated paragraphs.
The fundamental difference between Luther and Melanchthon lies not so much in the latter's ethical conception, as in his humanistic mode of thought which formed the basis of his theology and made him ready not only to acknowledge moral and religious truths outside of Christianity, but also to bring Christian truth into closer contact with them, and thus to mediate between Christian revelation and ancient philosophy.
Melanchthon's views differed from Luther's only in some modifications of ideas. Melanchthon looked upon the law as not only the correlate of the Gospel, by which its effect of salvation is prepared, but as the unchangeable order of the spiritual world which has its basis in God himself. He furthermore reduced Luther's much richer view of redemption to that of legal satisfaction. He did not draw from the vein of mysticism running through Luther's theology, but emphasized the ethical and intellectual elements.
After giving up determinism and absolute predestination and ascribing to man a certain moral freedom, he tried to ascertain the share of free will in conversion, naming three causes as concurring in the work of conversion, the Word, the Spirit, and the human will, not passive, but resisting its own weakness. Since 1548 he used the definition of freedom formulated by Erasmus, "the capability of applying oneself to grace." 
His definition of faith lacks the mystical depth of Luther. In dividing faith into knowledge, assent, and trust, he made the participation of the heart subsequent to that of the intellect, and so gave rise to the view of the later orthodoxy that the establishment and acceptation of pure doctrine should precede the personal attitude of faith. To his intellectual conception of faith corresponded also his view that the Church also is only the communion of those who adhere to the true belief and that her visible existence depends upon the consent of her unregenerated members to her teachings.
Finally, Melanchthon's doctrine of the Lord's Supper, lacking the profound mysticism of faith by which Luther united the sensual elements and supersensual realities, demanded at least their formal distinction.
The development of Melanchthon's beliefs may be seen from the history of the "Loci". In the beginning Melanchthon intended only a development of the leading ideas representing the Evangelical conception of salvation, while the later editions approach more and more the plan of a text-book of dogma. At first he uncompromisingly insisted on the necessity of every event, energetically rejected the philosophy of Aristotle, and had not fully developed his doctrine of the sacraments.
In 1535 he treated for the first time the doctrine of God and that of the Trinity; rejected the doctrine of the necessity of every event and named free will as a concurring cause in conversion. The doctrine of justification received its forensic form and the necessity of good works was emphasized in the interest of moral discipline. The last editions are distinguished from the earlier ones by the prominence given to the theoretical and rational element.
As moralist.
In ethics Melanchthon preserved and renewed the tradition of ancient morality and represented the Evangelical conception of life. His books bearing directly on morals were chiefly drawn from the classics, and were influenced not so much by Aristotle as by Cicero. His principal works in this line were "Prolegomena" to Cicero's "De officiis" (1525); "Enarrationes librorum Ethicorum Aristotelis" (1529); "Epitome philosophiae moralis" (1538); and "Ethicae doctrinae elementa" (1550).
In his "Epitome philosophiae moralis" Melanchthon treats first the relation of philosophy to the law of God and the Gospel. Moral philosophy, it is true, does not know anything of the promise of grace as revealed in the Gospel, but it is the development of the natural law implanted by God in the heart of man, and therefore representing a part of the divine law. The revealed law, necessitated because of sin, is distinguished from natural law only by its greater completeness and clearness. The fundamental order of moral life can be grasped also by reason; therefore the development of moral philosophy from natural principles must not be neglected. Melanchthon therefore made no sharp distinction between natural and revealed morals.
His contribution to Christian ethics in the proper sense must be sought in the Augsburg Confession and its Apology as well as in his "Loci", where he followed Luther in depicting the Evangelical ideal of life, the free realization of the divine law by a personality blessed in faith and filled with the spirit of God.
As exegete.
Melanchthon's formulation of the authority of Scripture became the norm for the following time. The principle of his hermeneutics is expressed in his words: "Every theologian and faithful interpreter of the heavenly doctrine must necessarily be first a grammarian, then a dialectician, and finally a witness." By "grammarian" he meant the philologist in the modern sense who is master of history, archaeology, and ancient geography. As to the method of interpretation, he insisted with great emphasis upon the unity of the sense, upon the literal sense in contrast to the four senses of the scholastics. He further stated that whatever is looked for in the words of Scripture, outside of the literal sense, is only dogmatic or practical application.
His commentaries, however, are not grammatical, but are full of theological and practical matter, confirming the doctrines of the Reformation, and edifying believers. The most important of them are those on Genesis, Proverbs, Daniel, the Psalms, and especially those on the New Testament, on Romans (edited in 1522 against his will by Luther), Colossians (1527), and John (1523). Melanchthon was the constant assistant of Luther in his translation of the Bible, and both the books of the Maccabees in Luther's Bible are ascribed to him. A Latin Bible published in 1529 at Wittenberg is designated as a common work of Melanchthon and Luther.
As historian and preacher.
In the sphere of historical theology the influence of Melanchthon may be traced until the seventeenth century, especially in the method of treating church history in connection with political history. His was the first Protestant attempt at a history of dogma, "Sententiae veterum aliquot patrum de caena domini" (1530) and especially "De ecclesia et auctoritate verbi Dei" (1539).
Melanchthon exerted a wide influence in the department of homiletics, and has been regarded as the author, in the Protestant Church, of the methodical style of preaching. He himself keeps entirely aloof from all mere dogmatizing or rhetoric in the "Annotationes in Evangelia" (1544), the "Conciones in Evangelium Matthaei" (1558), and in his German sermons prepared for George of Anhalt. He never preached from the pulpit; and his Latin sermons "(Postilla)" were prepared for the Hungarian students at Wittenberg who did not understand German. In this connection may be mentioned also his "Catechesis puerilis" (1532), a religious manual for younger students, and a German catechism (1549), following closely Luther's arrangement.
From Melanchthon came also the first Protestant work on the method of theological study, so that it may safely be said that by his influence every department of theology was advanced even if he was not always a pioneer.
As professor and philosopher.
As a philologist and pedagogue Melanchthon was the spiritual heir of the South German Humanists, of men like Reuchlin, Wimpheling, and Rodolphus Agricola, who represented an ethical conception of the humanities. The liberal arts and a classical education were for him only a means to an ethical and religious end. The ancient classics were for him in the first place the sources of a purer knowledge, but they were also the best means of educating the youth both by their beauty of form and by their ethical content. By his organizing activity in the sphere of educational institutions and by his compilations of Latin and Greek grammars and commentaries, Melanchthon became the founder of the learned schools of Evangelical Germany, a combination of humanistic and Christian ideals. In philosophy also Melanchthon was the teacher of the whole German Protestant world. The influence of his philosophical compendia ended only with the rule of the Leibniz-Wolff school.
He started from scholasticism; but with the contempt of an enthusiastic Humanist he turned away from it and came to Wittenberg with the plan of editing the complete works of Aristotle. Under the dominating religious influence of Luther his interest abated for a time, but in 1519 he edited the "Rhetoric" and in 1520 the "Dialectic."
The relation of philosophy to theology is characterized, according to him, by the distinction between law and Gospel. The former, as a light of nature, is innate; it also contains the elements of the natural knowledge of God which, however, have been obscured and weakened by sin. Therefore, renewed promulgation of the law by revelation became necessary and was furnished in the Decalogue; and all law, including that in the scientific form of philosophy, contains only demands, shadowings; its fulfilment is given only in the Gospel, the object of certainty in theology, by which also the philosophical elements of knowledge—experience, principles of reason, and syllogism—receive only their final confirmation. As the law is a divinely ordered pedagogue that leads to Christ, philosophy, its interpreter, is subject to revealed truth as the principal standard of opinions and life.
Besides Aristotle's "Rhetoric" and "Dialectic" he published
"De dialecta libri iv" (1528);
"Erotemata dialectices" (1547);
"Liber de anima" (1540);
"Initia doctrinae physicae" (1549); and
"Ethicae doctrinae elementa" (1550).
Personal appearance and character.
There have been preserved original portraits of Melanchthon by three famous painters of his time—by Holbein in various versions, one of them in the Royal Gallery of Hanover, by Albrecht Dürer (made in 1526, meant to convey a spiritual rather than physical likeness and said to be eminently successful in doing so), and by Lucas Cranach.
Melanchthon was dwarfish, misshapen, and physically weak, although he is said to have had a bright and sparkling eye, which kept its color till the day of his death. He was never in perfectly sound health, and managed to perform as much work as he did only by reason of the extraordinary regularity of his habits and his great temperance. He set no great value on money and possessions; his liberality and hospitality were often misused in such a way that his old faithful Swabian servant had sometimes difficulty in managing the household.
His domestic life was happy. He called his home "a little church of God," always found peace there, and showed a tender solicitude for his wife and children. To his great astonishment a French scholar found him rocking the cradle with one hand, and holding a book in the other.
His noble soul showed itself also in his friendship for many of his contemporaries; "there is nothing sweeter nor lovelier than mutual intercourse with friends," he used to say. His most intimate friend was Joachim Camerarius, whom he called the half of his soul. His extensive correspondence was for him not only a duty, but a need and an enjoyment. His letters form a valuable commentary on his whole life, as he spoke out his mind in them more unreservedly than he was wont to do in public life. A peculiar example of his sacrificing friendship is furnished by the fact that he wrote speeches and scientific treatises for others, permitting them to use their own signature. But in the kindness of his heart he was said to be ready to serve and assist not only his friends, but everybody.
He was an enemy to jealousy, envy, slander, and sarcasm. His whole nature adapted him especially to the intercourse with scholars and men of higher rank, while it was more difficult for him to deal with the people of lower station. He never allowed himself or others to exceed the bounds of nobility, honesty, and decency. He was very sincere in the judgment of his own person, acknowledging his faults even to opponents like Flacius, and was open to the criticism even of such as stood far below him. In his public career he sought not honor or fame, but earnestly endeavored to serve the Church and the cause of truth.
His humility and modesty had their root in his personal piety. He laid great stress upon prayer, daily meditation on the Word, and attendance of public service. In Melanchthon is found not a great, impressive personality, winning its way by massive strength of resolution and energy, but a noble character hard to study without loving and respecting.

</doc>
<doc id="39211" url="http://en.wikipedia.org/wiki?curid=39211" title="Albury–Wodonga">
Albury–Wodonga

Albury–Wodonga is the broad settlement incorporating the twin Australian cities of Albury and Wodonga, which are separated geographically by the Murray River and politically by a state border: Albury on the north of the river is part of New South Wales while Wodonga on the south bank is in Victoria.
Although in many senses the centre operates as one community, it has parallel municipal governments and state government services. However, the fact that Melbourne is significantly closer than Sydney and the fact that Victorian television broadcasts in the region, resulting (among other things) in the predominance of Australian rules football in the local media outlets, gives Albury close cultural and psychological links to Victoria, despite its location in southern New South Wales.
Albury–Wodonga was selected as the primary focus of the federal Whitlam government's scheme to arrest the uncontrolled growth of Australia's large coastal cities (Sydney and Melbourne in particular) by encouraging decentralisation. Grand plans were made to turn Albury–Wodonga into a major inland city. Some industries were enticed to move there, and a certain amount of population movement resulted. However, due to the subsequent Fraser Government's repudiation of Labor's decentralisation policies, the plan to populate inland areas and cities other than the State capitals was abandoned. No other Commonwealth Government since, either conservative or Labor, has made any attempt at repopulating inland areas. Thus the current Albury–Wodonga population of approximately 104,609 residents is far below the 300,000 projected by Whitlam in the 1970s.
The industrial employment sector has meant that Albury–Wodonga, unusually for an Australian inland city, is not dependent on agriculture. According to the most recently available figures from the Australian Bureau of Statistics, the average income of the Albury area is $36K per year, below the $42K average for the state of New South Wales.

</doc>
<doc id="39212" url="http://en.wikipedia.org/wiki?curid=39212" title="Moog (surname)">
Moog (surname)

Moog is a surname. People bearing the name include:

</doc>
<doc id="39216" url="http://en.wikipedia.org/wiki?curid=39216" title="Bangers and mash">
Bangers and mash

Bangers and mash, also known as sausages and mash, is a traditional British Isles dish made of mashed potatoes and sausages, the latter of which may consist of a variety of flavoured sausage made of pork or beef or a Cumberland sausage. It is sometimes served with onion gravy, fried onions, baked beans, and peas. It is mostly eaten in the United Kingdom, Canada, Australia, and New Zealand.
This dish, even when cooked at home, may be thought of as an example of pub grub—relatively quick and easy to make in large quantities. More up-market varieties, with exotic sausages and mashes, are sold in gastropubs, as well as less alternatives being available in regular public houses.
Etymology.
Although it is sometimes stated that the term "bangers" has its origins in World War II, the term was actually in use at least as far back as 1919. The term "bangers" is attributed (in common usage in the UK) to the fact that sausages, particularly the kind made during World War II under rationing, were made with water so they were more likely to explode under high heat if not cooked carefully; modern sausages do not have this attribute.
In popular culture.
Peter Sellers recorded a song with Sophia Loren, "Bangers and Mash" (1961), extolling their virtues: "No wonder you're so bony Joe, and skinny as a rake. Well then, give us a bash at the bangers and mash me mother used to make".
References.
Notes

</doc>
<doc id="39217" url="http://en.wikipedia.org/wiki?curid=39217" title="Donnybrook, Dublin">
Donnybrook, Dublin

Donnybrook (Irish: "Domhnach Broc", meaning "The Church of Saint Broc") is a district of Dublin, Ireland. It is situated on the southside of the city, in the Dublin 4 postal district, and is home to the Irish public service broadcaster Raidió Teilifís Éireann (RTÉ). It was once part of the Pembroke Township. Its neighbouring suburbs are Ballsbridge, Sandymount, Ranelagh and Clonskeagh. It is also a civil parish mainly situated in the old barony of Dublin.
History.
Donnybrook Fair dates from a charter of King John of England in 1204 and was held annually until 1866. It began as a fair for livestock and agricultural produce but later declined, growing into a more of a carnival and fun fair. Drunkenness and fighting became common place and the people of Donnybrook were anxious that it should cease. After a good deal of local fund raising, the patent was bought by a group of prominent residents and clergy, bringing about its demise. The Fair took place on lands now occupied by Donnybrook Rugby Ground and the Ever Ready Garage. The word "donnybrook" has since entered the English language to describe a rowdy brawl. 
Donnybrook Castle was an Elizabethan mansion and residence of the Ussher family. James Ussher was appointed Archbishop of Armagh in the Church of Ireland by Queen Elizabeth I of England. The mansion was replaced in 1795 by the existing Georgian house. It is now occupied by the Religious Sisters of Charity.
Donnybrook Graveyard dates back to the 8th century and was once the location of a church founded by St Broc. It was also the site of Catholic and Protestant churches, bot called St Mary's. Those buried in it include Dr. Bartholomew Mosse, the founder of the Rotunda Hospital, Sir Edward Lovett Pearce, architect of the Irish Houses of Parliament on College Green and Dr. Richard Madden, biographer of the United Irishmen. It is possible that the wall on the south side of the cemetery is the oldest man made structure still existing in Donnybrook. The brick chimney behind the cemetery was built on the site of a former marble works and later served as a Magdalene laundry.
Geography.
The river Dodder runs through Donnybrook and at one time there was a ford here. It is subject to periodic serious flooding and in 1628 one of the Usshers of Donnybrook Castle was drowned while trying to cross.
It is a civil parish consisting of sixteen townlands. All but four of these townlands are situated in the Barony of Dublin. Donnybrook is the single biggest parish in that barony. The most southerly townlands, Annefield, Simmonscourt and Priesthouse, belong to the barony of Rathdown. The smallest of these, Annefield, is itself an enclave of Simmonscourt which gives its name to a pavilion of the Royal Dublin Society. Today, the majority of Priesthouse is occupied by Elm Park Golf Club and the studios of RTE. The remaining townland of Sallymount - the parish's most westerly point - is in the barony of Uppercross.
Donnybrook today.
The television and radio studios of the national broadcaster, RTE, are located in Priesthouse, Donnybrook.
Politics.
Donnybrook is in the Dáil Éireann constituency of Dublin South–East and the Pembroke-Rathmines local electoral area of Dublin City Council.
Sport.
Donnybrook is the traditional home of rugby union in Leinster. The headquarters of the Irish Rugby Football Union Leinster Branch is located opposite Donnybrook Stadium, where the professional Leinster team played their home games until recently. Kiely's pub in Donnybrook village is a traditional social point for rugby fans.
Rugby clubs Bective Rangers and Old Wesley have their home ground in Donnybrook Stadium. During the school year secondary schools such as St Conleth's College, Blackrock, Belvedere College, Clongowes, St. Michaels and many more play rugby in Donnybrook Stadium. 
There are several tennis clubs in Donnybrook, (Lawn Tennis Club), , and .
Belmont Football Club has its home ground in Herbert Park. 
Merrion Cricket Club is located in Donnybrook, off Anglesea Road and backing on to the Dodder.

</doc>
<doc id="39218" url="http://en.wikipedia.org/wiki?curid=39218" title="Ballsbridge">
Ballsbridge

Ballsbridge or Ball's Bridge (Irish: "Droichead na Dothra", meaning "Dodder bridge") is a bridge in Dublin, Ireland. Situated in "Pembroke" which spans the River Dodder on the south side of the city, it is a three-arch stone bridge and was built in 1791.
Pembroke is an exclusive and very affluent area and is known to contain very expensive property. However, this name is little known and the area is instead more commonly known as Ballsbridge, named after the bridge in the area. The sign on the bridge still proclaims it as "Ball's Bridge" in recognition of the fact that the original bridge in this location was built and owned by a Mr. Ball.
Ballsbridge was once part of the Pembroke Township.
Buildings and structures.
The bridge itself forms the centre of the Ballsbridge suburb which extends northwards towards the Grand Canal along Northumberland Road up to Haddington Road and Shelbourne Road, extends southwards along the Merrion Road towards Merrion and along Anglesea Road towards Donnybrook, and westwards to encompass the area around Pembroke Road, Clyde Road, Elgin Road, and Herbert Park. The park also forms part of Ballsbridge's nebulous border with Donnybrook.
The RDS has its grounds here, and the Lansdowne Road headquarters of the Irish Rugby Football Union (IRFU) is on the boundary between Ballsbridge and Irishtown. The corporate headquarters of Allied Irish Banks (AIB) are also located in Ballsbridge.
Ailesbury Road, along with adjacent Shrewsbury Road comprise the blue (most expensive) properties in the Dublin edition of the board-game "Monopoly". Shrewsbury Road was the sixth most expensive street in the world in 2007.
The bulk of Dublin's embassies and many diplomatic residences are located in the southern part of Ballsbridge on and around Ailesbury Road. The British, American, Spanish, Dutch and Israeli embassies are all located in the Ballsbridge area of Dublin.
Transport.
The DART train passes nearby, stopping at Lansdowne Road and Sandymount stations.
Ballsbridge is serviced by the following bus routes: 
A number of services have been withdrawn due to Dublin Bus' "Network Direct" route restructuring programme.

</doc>
<doc id="39219" url="http://en.wikipedia.org/wiki?curid=39219" title="IS-IS">
IS-IS

Intermediate System to Intermediate System (IS-IS) is a routing protocol designed to move information efficiently within a computer network, a group of physically connected computers or similar devices.
It accomplishes this by determining the best route for datagrams through a packet-switched network. The protocol was defined in ISO/IEC 10589:2002 as an international standard within the Open Systems Interconnection (OSI) reference design. Though originally an ISO standard, the IETF republished the protocol as an Internet Standard in RFC 1142. IS-IS has been called "the "de facto" standard for large service provider network backbones."
Description.
IS-IS "(pronounced "i-s i-s")" is an interior gateway protocol, designed for use within an administrative domain or network. This is in contrast to Exterior Gateway protocols, primarily Border Gateway Protocol (BGP), which is used for routing between autonomous systems (RFC 1930).
IS-IS is a link-state routing protocol, operating by reliably flooding link state information throughout a network of routers. Each IS-IS router independently builds a database of the network's topology, aggregating the flooded network information. Like the OSPF protocol, IS-IS uses Dijkstra's algorithm for computing the best path through the network. Packets (datagrams) are then forwarded, based on the computed ideal path, through the network to the destination.
History.
The IS-IS protocol was developed by Digital Equipment Corporation as part of DECnet Phase V. It was standardized by the ISO in 1992 as ISO 10589 for communication between network devices which are termed Intermediate Systems (as opposed to end systems or hosts) by the ISO. The purpose of IS-IS was to make possible the routing of datagrams using the ISO-developed OSI protocol stack called CLNS.
IS-IS was developed at roughly the same time that the Internet Engineering Task Force IETF was developing a similar protocol called OSPF. IS-IS was later extended to support routing of "datagrams" in the Internet Protocol (IP), the Network Layer protocol of the global Internet. This version of the IS-IS "routing" protocol was then called "Integrated IS-IS" ()
Comparison with OSPF.
Both IS-IS and OSPF are link state protocols, and both use the same Dijkstra algorithm for computing the best path through the network. As a result, they are conceptually similar. Both support variable length subnet masks, can use multicast to discover neighboring routers using "hello packets", and can support authentication of routing updates.
While OSPF was natively built to route IP and is itself a Layer 3 protocol that runs on top of IP, IS-IS is an OSI Layer 3 protocol (it is at the same layer as CLNS). The widespread adoption of IP worldwide may have contributed to OSPF's popularity. IS-IS does not use IP to carry routing information messages.
IS-IS is neutral regarding the type of network addresses for which it can route. OSPF version 2, on the other hand, was designed for IPv4. This allowed IS-IS to be easily used to support IPv6. To operate with IPv6 networks, the OSPF protocol was rewritten in OSPF v3 (as specified in RFC 2740).
IS-IS routers build a topological representation of the network. This map indicates the subnets which each IS-IS router can reach, and the lowest-cost (shortest) path to a subnet is used to forward traffic.
IS-IS differs from OSPF in the way that "areas" are defined and routed between. IS-IS routers are designated as being: Level 1 (intra-area); Level 2 (inter area); or Level 1-2 (both). Level 2 routers are inter area routers that can only form relationships with other Level 2 routers. Routing information is exchanged between Level 1 routers and other Level 1 routers, and Level 2 routers only exchange information with other Level 2 routers. Level 1-2 routers exchange information with both levels and are used to connect the inter area routers with the intra area routers.
In OSPF, areas are delineated on the interface such that an area border router (ABR) is actually in two or more areas at once, effectively creating the borders between areas inside the ABR, whereas in IS-IS area borders are in between routers, designated as Level 2 or Level 1-2. The result is that an IS-IS router is only ever a part of a single area.
IS-IS also does not require Area 0 (Area Zero) to be the backbone area through which all inter-area traffic must pass. The logical view is that OSPF creates something of a spider web or star topology of many areas all attached directly to Area Zero and IS-IS by contrast creates a logical topology of a backbone of Level 2 routers with branches of Level 1-2 and Level 1 routers forming the individual areas.
IS-IS also differs from OSPF in the methods by which it reliably floods topology and topology change information through the network. However, the basic concepts are similar.
OSPF has a larger set of extensions and optional features specified in the protocol standards. However IS-IS is more easy to expand: its use of type-length-value data allows engineers to implement support for new techniques without redesigning the protocol. For example, in order to support IPv6, the IS-IS protocol was extended to support a few additional TLVs, whereas OSPF required a new protocol draft (OSPFv3). In addition to that, IS-IS is less "chatty" and can scale to support larger networks. Given the same set of resources, IS-IS can support more routers in an area than OSPF. This has contributed to IS-IS as an ISP-scale protocol.
The TCP/IP implementation, known as "Integrated IS-IS" or "Dual IS-IS", is described in RFC 1195.
Other Uses.
IS-IS is also used as the control plane for IEEE 802.1aq Shortest Path Bridging (SPB). SPB allows for shortest-path forwarding in an Ethernet mesh network context utilizing multiple equal cost paths. This permits SPB to support large Layer 2 topologies, with fast convergence, and improved use of the mesh topology. Combined with this is single point provisioning for logical connectivity membership. IS-IS is therefore augmented with a small number of TLVs and sub-TLVs, and supports two Ethernet encapsulating data paths, 802.1ad Provider Bridges and 802.1ah Provider Backbone Bridges. SPB requires no state machine or other substantive changes to IS-IS, and simply requires a new Network Layer Protocol Identifier (NLPID) and set of TLVs. This extension to IS-IS is defined in the IETF proposed standard .

</doc>
<doc id="39220" url="http://en.wikipedia.org/wiki?curid=39220" title="Amaterasu">
Amaterasu

Amaterasu (天照), Amaterasu-ōmikami (天照大神／天照大御神) or Ōhirume-no-muchi-no-kami (大日孁貴神) is a part of the Japanese myth cycle and also a major deity of the Shinto religion. She is the goddess of the sun, but also of the universe. The name Amaterasu derived from Amateru meaning "shining in heaven." The meaning of her whole name, Amaterasu-ōmikami, is "the great august kami (god) who shines in the heaven". Based on the mythological stories in "Kojiki" and "Nihon Shoki", the Emperors of Japan are considered to be direct descendants of Amaterasu.
History.
Amaterasu appears to be the Japanese expression of a historical panasiatic solar goddess. Several similarities have been noticed between the Japanese solar goddess and the Korean solar goddess Hae-nim, particularly in regards to shamanistic worship, utilising the same symbols and practises. Another possible expression is the Chinese Xihe. Though historically probably venerated highly, only in Japan did this deity find continuous worship as a central figure, as elsewhere several newer religious movements such as Buddhism and Taoism discouraged the veneration of solar goddesses.
The oldest tales of Amaterasu come from the ca. AD 680 "Kojiki" and ca. AD 720 "Nihon Shoki", the oldest records of Japanese history. In Japanese mythology, Amaterasu, the goddess of the sun, is the sister of Susanoo, the god of storms and the sea, and of Tsukuyomi, the god of the moon. It was written that Amaterasu had painted the landscape with her siblings to create ancient Japan . All three were born from Izanagi when he was purifying himself after entering Yomi, the underworld, after failing to save Izanami. Amaterasu was born when Izanagi washed out his left eye, Tsukuyomi was born from the washing of the right eye, and Susanoo from the washing of the nose.
She became the ruler of the sun and the heavens along with her brother, Tsukuyomi, the god of the moon and ruler of the night. Originally, Amaterasu shared the sky with Tsukuyomi, her husband and brother until, out of disgust, he killed the goddess of food, Uke Mochi, when she pulled "food from her rectum, nose, and mouth". This killing upset Amaterasu causing her to label Tsukuyomi an evil god and split away from him; separating night from day.
The texts also tell of a long-standing rivalry between Amaterasu and her other brother, Susanoo. When he was to leave Heaven by orders of Izanagi, he went to bid his sister goodbye. Amaterasu was suspicious, but when Susanoo proposed a challenge to prove his sincerity, she accepted. Each of them took an object of the other's and from it birthed gods and goddesses. Amaterasu birthed three women from Susano's sword while he birthed five men from her necklace. Claiming the gods were hers because they were born of her necklace, and the goddesses were his, she decided that she had won the challenge, as his item produced women. The two were content for a time, but her brother became restless and went on a rampage, destroying Amaterasu's rice fields, hurling a flayed pony at her loom, and killing one of her attendants in a fit of rage. Amaterasu, who was in fury and grief, hid inside the Ama-no-Iwato ("heavenly rock cave"), thus effectively hiding the sun for a long period of time. Though she was persuaded to leave the cave, Susanoo was punished by being banished from Heaven. Both later amended their conflict when Susanoo gave her the Kusanagi-no-Tsurugi sword as a reconciliation gift.
According to legend, Amaterasu bequeathed to her descendant Ninigi: the mirror, Yata no Kagami; the jewel, Yasakani no Magatama; and the sword, Kusanagi-no-Tsurugi. This sacred mirror, jewel, and sword collectively became the three Imperial Regalia of Japan.
Worship.
The Ise Shrine located in Ise, Honshū, Japan houses the inner shrine, Naiku, dedicated to Amaterasu. Her sacred mirror, Yata no Kagami, is said to be kept at this shrine as one of the Imperial Regalia of Japan. At this shrine, a ceremony known as Shikinen Sengu is held every 20 years to honor Amaterasu. The main shrine buildings are destroyed and rebuilt at a location adjacent to the site. New clothing and food is then offered to the goddess. This practice is a part of the Shinto faith and has been practiced since the year 690.
The worship of Amaterasu to the exclusion of other kami has been described as "the cult of the sun". This phrase can also refer to the early pre-archipelagoan worship of the sun itself.

</doc>
<doc id="39221" url="http://en.wikipedia.org/wiki?curid=39221" title="Thermodynamic free energy">
Thermodynamic free energy

The thermodynamic free energy is the amount of work that a thermodynamic system can perform. The concept is useful in the thermodynamics of chemical or thermal processes in engineering and science. The free energy is the internal energy of a system minus the amount of energy that cannot be used to perform work. This unusable energy is given by the entropy of a system multiplied by the temperature of the system.
Like the internal energy, the free energy is a thermodynamic state function. Energy is a generalization of free energy.
Overview.
Free energy is that portion of any first-law energy that is available to perform thermodynamic work; "i.e.", work mediated by thermal energy. Free energy is subject to irreversible loss in the course of such work. Since first-law energy is always conserved, it is evident that free energy is an expendable, second-law kind of energy that can perform work within finite amounts of time. Several free energy functions may be formulated based on system criteria. Free energy functions are Legendre transformations of the internal energy. For processes involving a system at constant pressure "p" and temperature "T", the Gibbs free energy is the most useful because, in addition to subsuming any entropy change due merely to heat, it does the same for the "p"d"V" work needed to "make space for additional molecules" produced by various processes. (Hence its utility to solution-phase chemists, including biochemists.) The Helmholtz free energy has a special theoretical importance since it is proportional to the logarithm of the partition function for the canonical ensemble in statistical mechanics. (Hence its utility to physicists; and to gas-phase chemists and engineers, who do not want to ignore "p"d"V" work.)
The historically earlier Helmholtz free energy is defined as "A" = "U" − "TS", where "U" is the internal energy, "T" is the absolute temperature, and "S" is the entropy. Its change is equal to the amount of reversible work done on, or obtainable from, a system at constant "T". Thus its appellation "work content", and the designation "A" from "Arbeit", the German word for work. Since it makes no reference to any quantities involved in work (such as "p" and "V"), the Helmholtz function is completely general: its decrease is the maximum amount of work which can be done "by" a system, and it can increase at most by the amount of work done "on" a system.
The Gibbs free energy is given by "G" = "H" − "TS", where "H" is the enthalpy. ("H" = "U" + "pV", where "p" is the pressure and "V" is the volume.)
Historically, these energy terms have been used inconsistently. In physics, "free energy" most often refers to the Helmholtz free energy, denoted by "A", while in chemistry, "free energy" most often refers to the Gibbs free energy. Since both fields use both functions, a compromise has been suggested, using "A" to denote the Helmholtz function and "G" for the Gibbs function. While "A" is preferred by IUPAC, "G" is sometimes still in use, and the correct free energy function is often implicit in manuscripts and presentations.
Meaning of "free".
In the 18th and 19th centuries, the theory of heat, i.e., that heat is a form of energy having relation to vibratory motion, was beginning to supplant both the caloric theory, i.e., that heat is a fluid, and the four element theory, in which heat was the lightest of the four elements. In a similar manner, during these years, heat was beginning to be distinguished into different classification categories, such as “free heat”, “combined heat”, “radiant heat”, specific heat, heat capacity, “absolute heat”, “latent caloric”, “free” or “perceptible” caloric ("calorique sensible"), among others.
In 1780, for example, Laplace and Lavoisier stated: “In general, one can change the first hypothesis into the second by changing the words ‘free heat, combined heat, and heat released’ into ‘vis viva, loss of vis viva, and increase of vis viva.’” In this manner, the total mass of caloric in a body, called "absolute heat", was regarded as a mixture of two components; the free or perceptible caloric could affect a thermometer, whereas the other component, the latent caloric, could not. The use of the words “latent heat” implied a similarity to latent heat in the more usual sense; it was regarded as chemically bound to the molecules of the body. In the adiabatic compression of a gas, the absolute heat remained constant but the observed rise in temperature implied that some latent caloric had become “free” or perceptible.
During the early 19th century, the concept of perceptible or free caloric began to be referred to as “free heat” or heat set free. In 1824, for example, the French physicist Sadi Carnot, in his famous “Reflections on the Motive Power of Fire”, speaks of quantities of heat ‘absorbed or set free’ in different transformations. In 1882, the German physicist and physiologist Hermann von Helmholtz coined the phrase ‘free energy’ for the expression "E − TS", in which the change in "F" (or "G") determines the amount of energy ‘free’ for work under the given conditions.
Thus, in traditional use, the term “free” was attached to Gibbs free energy, i.e., for systems at constant pressure and temperature, or to Helmholtz free energy, i.e., for systems at constant volume and temperature, to mean ‘available in the form of useful work.’ With reference to the Gibbs free energy, we add the qualification that it is the energy free for non-volume work.
An increasing number of books and journal articles do not include the attachment “free”, referring to G as simply Gibbs energy (and likewise for the Helmholtz energy). This is the result of a 1988 IUPAC meeting to set unified terminologies for the international scientific community, in which the adjective ‘free’ was supposedly banished. This standard, however, has not yet been universally adopted, and many published articles and books still include the descriptive ‘free’.
Application.
The experimental usefulness of these functions is restricted to conditions where certain variables ("T", and "V" or "external" "p") are held constant, although they also have theoretical importance in deriving Maxwell relations. Work other than "p"d"V" may be added, e.g., for electrochemical cells, or formula_1 work in elastic materials and in muscle contraction. Other forms of work which must sometimes be considered are stress-strain, magnetic, as in adiabatic demagnetization used in the approach to absolute zero, and work due to electric polarization. These are described by tensors.
In most cases of interest there are internal degrees of freedom and processes, such as chemical reactions and phase transitions, which create entropy. Even for homogeneous "bulk" materials, the free energy functions depend on the (often suppressed) composition, as do all proper thermodynamic potentials (extensive functions), including the internal energy.
"N""i" is the number of molecules (alternatively, moles) of type "i" in the system. If these quantities do not appear, it is impossible to describe compositional changes. The differentials for reversible processes are (assuming only "pV" work):
where μ"i" is the chemical potential for the "i"-th component in the system. The second relation is especially useful at constant "T" and "p", conditions which are easy to achieve experimentally, and which approximately characterize living creatures.
Any decrease in the Gibbs function of a system is the upper limit for any isothermal, isobaric work that can be captured in the surroundings, or it may simply be dissipated, appearing as "T" times a corresponding increase in the entropy of the system and/or its surrounding.
An example is surface free energy, the amount of increase of free energy when the area of surface increases by every unit area.
The path integral Monte Carlo method is a numerical approach for determining the values of free energies, based on quantum dynamical principles.
History.
The quantity called "free energy" is a more advanced and accurate replacement for the outdated term "affinity", which was used by chemists in previous years to describe the "force" that caused chemical reactions. The term affinity, as used in chemical relation, dates back to at least the time of Albertus Magnus in 1250.
From the 1998 textbook "Modern Thermodynamics" by Nobel Laureate and chemistry professor Ilya Prigogine we find: "As motion was explained by the Newtonian concept of force, chemists wanted a similar concept of ‘driving force’ for chemical change. Why do chemical reactions occur, and why do they stop at certain points? Chemists called the ‘force’ that caused chemical reactions affinity, but it lacked a clear definition."
During the entire 18th century, the dominant view with regard to heat and light was that put forth by Isaac Newton, called the "Newtonian hypothesis", which states that light and heat are forms of matter attracted or repelled by other forms of matter, with forces analogous to gravitation or to chemical affinity.
In the 19th century, the French chemist Marcellin Berthelot and the Danish chemist Julius Thomsen had attempted to quantify affinity using heats of reaction. In 1875, after quantifying the heats of reaction for a large number of compounds, Berthelot proposed the "principle of maximum work", in which all chemical changes occurring without intervention of outside energy tend toward the production of bodies or of a system of bodies which liberate heat.
In addition to this, in 1780 Antoine Lavoisier and Pierre-Simon Laplace laid the foundations of thermochemistry by showing that the heat given out in a reaction is equal to the heat absorbed in the reverse reaction. They also investigated the specific heat and latent heat of a number of substances, and amounts of heat given out in combustion. In a similar manner, in 1840 Swiss chemist Germain Hess formulated the principle that the evolution of heat in a reaction is the same whether the process is accomplished in one-step process or in a number of stages. This is known as Hess' law. With the advent of the mechanical theory of heat in the early 19th century, Hess’s law came to be viewed as a consequence of the law of conservation of energy.
Based on these and other ideas, Berthelot and Thomsen, as well as others, considered the heat given out in the formation of a compound as a measure of the affinity, or the work done by the chemical forces. This view, however, was not entirely correct. In 1847, the English physicist James Joule showed that he could raise the temperature of water by turning a paddle wheel in it, thus showing that heat and mechanical work were equivalent or proportional to each other, i.e., approximately, formula_5. This statement came to be known as the mechanical equivalent of heat and was a precursory form of the first law of thermodynamics.
By 1865, the German physicist Rudolf Clausius had shown that this equivalence principle needed amendment. That is, one can use the heat derived from a combustion reaction in a coal furnace to boil water, and use this heat to vaporize steam, and then use the enhanced high-pressure energy of the vaporized steam to push a piston. Thus, we might naively reason that one can entirely convert the initial combustion heat of the chemical reaction into the work of pushing the piston. Clausius showed, however, that we must take into account the work that the molecules of the working body, i.e., the water molecules in the cylinder, do on each other as they pass or transform from one step of or state of the engine cycle to the next, e.g., from ("P"1,"V"1) to ("P"2,"V"2). Clausius originally called this the “transformation content” of the body, and then later changed the name to entropy. Thus, the heat used to transform the working body of molecules from one state to the next cannot be used to do external work, e.g., to push the piston. Clausius defined this "transformation heat" as d"Q" = "T"d"S".
In 1873, Willard Gibbs published "A Method of Geometrical Representation of the Thermodynamic Properties of Substances by Means of Surfaces", in which he introduced the preliminary outline of the principles of his new equation able to predict or estimate the tendencies of various natural processes to ensue when bodies or systems are brought into contact. By studying the interactions of homogeneous substances in contact, i.e., bodies, being in composition part solid, part liquid, and part vapor, and by using a three-dimensional volume-entropy-internal energy graph, Gibbs was able to determine three states of equilibrium, i.e., "necessarily stable", "neutral", and "unstable", and whether or not changes will ensue. In 1876, Gibbs built on this framework by introducing the concept of chemical potential so to take into account chemical reactions and states of bodies that are chemically different from each other. In his own words, to summarize his results in 1873, Gibbs states:
In this description, as used by Gibbs, "ε" refers to the internal energy of the body, "η" refers to the entropy of the body, and "ν" is the volume of the body.
Hence, in 1882, after the introduction of these arguments by Clausius and Gibbs, the German scientist Hermann von Helmholtz stated, in opposition to Berthelot and Thomas’ hypothesis that chemical affinity is a measure of the heat of reaction of chemical reaction as based on the principle of maximal work, that affinity is not the heat given out in the formation of a compound but rather it is the largest quantity of work which can be gained when the reaction is carried out in a reversible manner, e.g., electrical work in a reversible cell. The maximum work is thus regarded as the diminution of the free, or available, energy of the system ("Gibbs free energy" "G" at "T" = constant, "P" = constant or "Helmholtz free energy" "F" at "T" = constant, "V" = constant), whilst the heat given out is usually a measure of the diminution of the total energy of the system (Internal energy). Thus, "G" or "F" is the amount of energy “free” for work under the given conditions.
Up until this point, the general view had been such that: “all chemical reactions drive the system to a state of equilibrium in which the affinities of the reactions vanish”. Over the next 60 years, the term affinity came to be replaced with the term free energy. According to chemistry historian Henry Leicester, the influential 1923 textbook "Thermodynamics and the Free Energy of Chemical Reactions" by Gilbert N. Lewis and Merle Randall led to the replacement of the term “affinity” by the term “free energy” in much of the English-speaking world.

</doc>
<doc id="39223" url="http://en.wikipedia.org/wiki?curid=39223" title="Coins of the pound sterling">
Coins of the pound sterling

The standard circulating coinage of the United Kingdom is denominated in pounds sterling (symbol "£"), and, since the introduction of the two-pound coin in 1994 (to celebrate the 300th Anniversary of the Bank of England 1694-1994), ranges in value from one penny to two pounds. Since decimalisation, on 15 February 1971, the pound has been divided into 100 (new) pence. From the 16th century until decimalisation, the pound was divided into 20 shillings, each of 12 (old) pence. British coins are minted by the Royal Mint in Llantrisant, Wales. The Royal Mint also commissions the coins' designs.
As of 30 March 2010, there were an estimated 28 billion coins circulating in the United Kingdom.
The first decimal coins were circulated in 1968. These were the five pence (5p) and ten pence (10p), and had values of one shilling (1/-) and two shillings (2/-), respectively, under the pre-decimal £sd system. The decimal coins are minted in copper-plated steel (previously bronze), nickel-plated steel, cupro-nickel and nickel-brass. The two-pound coin is bimetallic. The coins are discs, except for the twenty pence and fifty-pence pieces, both of which are heptagonal curves of constant width. All the circulating coins have an effigy of Queen Elizabeth II on the obverse, and various national and regional designs, and the denomination, on the reverse. The circulating coins, excepting the two-pound coin, were redesigned in 2008, keeping the sizes and compositions unchanged, but introducing reverse designs that each depict a part of the Royal Shield of Arms and form the whole shield when they are placed together in the appropriate arrangement. The exception, the 2008 one-pound coin, depicts the entire shield of arms on the reverse. All current coins carry a Latin inscription whose full form is , meaning "Elizabeth II, by the grace of God, Queen and Defender of the Faith".
In addition to the circulating coinage, the UK also mints commemorative decimal coins (crowns) in the denomination of five pounds (previously 25p, i.e. five shillings). Ceremonial Maundy money and bullion coinage of gold sovereigns, half sovereigns, and gold and silver Britannia coins are also produced. Some territories outside the United Kingdom, that use the pound sterling, produce their own coinage, with the same denominations and specifications as the UK coinage but local designs.
In the years just prior to decimalisation, the circulating British coins were the half crown (2/6), two shillings or florin (2/-), shilling (1/-), sixpence (6d), threepence (3d), penny (1d) and halfpenny (½d). The farthing (¼d) had been withdrawn in 1960. There was also the Crown (5/-), which was (and still is) legal tender but only minted on special occasions and not normally circulated.
All modern coins feature a profile of the current monarch's head. The direction in which they face changes with each successive monarch, a pattern that began with the Stuarts. For the Tudors and pre-Restoration Stuarts, both left and right-facing portrait images were minted within the reign of a single monarch. In the Middle Ages, portrait images tended to be full face.
From a very early date, British coins have been inscribed with the name of the ruler of the kingdom in which they were produced, and a longer or shorter title, always in Latin; among the earliest distinctive English coins are the silver pennies of Offa of Mercia, which were inscribed with the legend , "King Offa". The English silver penny was derived from another silver coin, the sceat, of 20 troy grains weight, which was in general circulation in Europe during the Middle Ages. In the 12th century, Henry II established the sterling silver standard for English coinage, of 92.5% silver and 7.5% copper, replacing the earlier use of fine silver in the Middle Ages. The coinage reform of 1816 set up a weight/value ratio and physical sizes for silver coins. Silver was eliminated from coins, except Maundy coins, in 1947.
History.
Manufacture.
The history of the Royal Mint stretches back to AD 886. For many centuries production took place in London, initially at the Tower of London, and then at premises nearby in Tower Hill. In the 1970s production was transferred to Llantrisant in South Wales. Historically Scotland and England had separate coinage; the last Scottish coins were struck in 1709 shortly after union with England.
Coins were originally hand-hammered — an ancient technique in which two dies are struck together with a blank coin between them. This was the traditional method of manufacturing coins in the Western world from the classical Greek era onwards, in comparison with Asia, where coins were traditionally cast. The first milled (that is, machine-made) coins were produced during the reign of Elizabeth I (1558–1603) and periodically during the subsequent reigns of James I and Charles I, but there was initially opposition to mechanisation from the moneyers who ensured that most coins continued to be produced by hammering. All British coins produced since 1662 have been milled.
Origins of the penny.
The English penny first appeared in Anglo-Saxon times, as a silver coin. It was derived from another silver coin, the sceat, of 20 troy grains weight, which was in general circulation in Europe during the Middle Ages. The weight of the English penny was fixed at 22.5 troy grains (about 1.46 grams) by Offa of Mercia, an 8th-century contemporary of Charlemagne. The coin's designated value, however, was that of 24 troy grains of silver (one pennyweight, or 1⁄240 of a troy pound, or about 1.56 grams), with the difference being a premium attached by virtue of the minting into coins. Thus 240 pennyweights made one troy pound of silver in weight, and the monetary value of 240 pennies also became known as a "pound". (240 actual pennies, however, weighed only 5400 troy grains, known as tower pound, a unit used only by mints. The tower pound was abolished in the 16th century.) The silver penny remained the primary unit of coinage for about 500 years.
The purity of 92.5% silver ("i.e.," sterling silver) was instituted by Henry II in 1158 with the "Tealby Penny" — a hammered coin.
Over the years, the penny was gradually debased until by the 16th century it contained about a third the silver content of a proper troy 24 grain pennyweight.
By 1915, a penny was worth around 1/6 what it had been worth during the Middle Ages. British government sources suggest that prices have risen over 61-fold since 1914, so a medieval sterling silver penny might have had purchasing power equivalent to £4.50 today, and a farthing (a quarter penny) would have the value of slightly more than today's pound (about £1.125).
Silver content.
From the time of Charlemagne until the 12th century, the silver currency of England was made from the highest purity silver available. Unfortunately there were drawbacks to minting currency of fine silver, notably the level of wear it suffered, and the ease with which coins could be "clipped", or trimmed, by those dealing in the currency.
In the 12th century a new standard for English coinage was established by Henry II — the Sterling Silver standard of 92.5% silver and 7.5% copper. This was a harder-wearing alloy, yet it was still a rather high grade of silver. It went some way towards discouraging the practice of "clipping", though this practice was further discouraged and largely eliminated with the introduction of the milled edge we see on coins today.
During the reign of Henry VIII, the silver content was gradually debased, reaching a low of 1/3 silver. However, in Edward VI's reign, silver purity was increased to sterling again and the first crowns and half crowns were produced dated 1551. From this point onwards till 1920, sterling was the rule.
By 1696, the currency had been seriously weakened by an increase in clipping during the Nine Years' War to the extent that it was decided to recall and replace all hammered silver coinage in circulation. The exercise came close to disaster due to fraud and mismanagement, but was saved by the personal intervention of Isaac Newton after his appointment as Warden of the Mint, a post which was intended to be a sinecure, but which he took seriously. Newton was subsequently given the post of Master of the Mint in 1699. Following the 1707 union between the Kingdom of England and the Kingdom of Scotland, Newton used his previous experience to direct the 1707–1710 Scottish recoinage, resulting in a common currency for the new Kingdom of Great Britain. After 15 September 1709 no further silver coins were ever struck in Scotland.
As a result of a report written by Newton on 21 September 1717 to the Lords Commissioners of His Majesty's Treasury the bimetallic relationship between gold coins and silver coins was changed by
Royal proclamation on 22 December 1717, forbidding the exchange of gold guineas for more than 21 silver shillings. Due to differing valuations in other European countries this inadvertently resulted in a silver shortage as silver coins were used to pay for imports, while exports were paid for in gold, effectively moving Britain from the silver standard to its first gold standard, rather than the bimetallic standard implied by the proclamation.
The coinage reform of 1816 set up a weight/value ratio and physical sizes for silver coins.
In 1920, the silver content of all British coins was reduced from 92.5% to 50%, with a portion of the remainder consisting of manganese, which caused the coins to tarnish to a very dark colour after they had been in circulation for a significant period. Silver was eliminated altogether in 1947, except for Maundy coinage, which returned to the pre-1920 92.5% silver composition.
The 1816 weight/value ratio and size system survived the debasement of silver in 1920, and the adoption of token coins of cupro-nickel in 1947. It even persisted after decimalisation for those coins which had equivalents and continued to be minted with their values in new pence. The UK finally abandoned it in 1992 when smaller, more convenient, "silver" coins were introduced.
Monarch's head.
All coins since the 17th century have featured a profile of the current monarch's head. The direction in which they face changes with each successive monarch, a pattern that began with the Stuarts, as shown in the table below:
For the Tudors and pre-Restoration Stuarts, both left- and right-facing portrait images were minted within the reign of a single monarch (left-facing images were more common). In the Middle Ages, portrait images tended to be full face.
There was a small quirk in this alternating pattern when Edward VIII ascended to the throne in January 1936 and was portrayed facing left, the same as his predecessor George V. This was because Edward thought his left to be his best side. However, Edward VIII abdicated in December 1936 and his coins were never put into general circulation. When George VI came to the throne, he had his coins struck with him facing the left, as if Edward VIII's coins had faced right (as they should have done according to tradition). Thus, in a timeline of circulating British coins, George V and VI's coins both feature left-facing portraits, although they follow directly chronologically.
<span id="Post-decimalisation coinage" />
Currently circulating coinage.
Production and distribution.
All UK coins are produced by the Royal Mint. The same coinage is used across the United Kingdom – Unlike banknotes, local issues of coins are not produced for different parts of the UK. The pound coin until 2008 has been produced in regional designs, but these circulate equally in all parts of the UK (see UK designs, below).
Every year, newly minted coins are checked for size, weight, and composition at a Trial of the Pyx. Essentially the same procedure has been used since the 13th century. Assaying is now done by the Worshipful Company of Goldsmiths on behalf of HM Treasury.
The 1p and 2p coins from 1971 are the oldest standard-issue coins still in circulation.
Coins from the British dependencies and territories that use the pound as their currency are sometimes found in change in other jurisdictions. Strictly, they are not legal tender in the United Kingdom; however, since they have the same specifications as UK coins, they are sometimes tolerated in commerce, and can readily be used in vending machines.
UK-issued coins are, on the other hand, generally fully accepted and freely mixed in other British dependencies and territories that use the pound.
An extensive coinage redesign was commissioned by the Royal Mint in 2005, and new designs were gradually introduced into the circulating British coinage from summer 2008. The pre-2008 coins will remain legal tender and are expected to stay in circulation for the foreseeable future.
Coins in circulation.
Estimated as at March 2014
Dimensions.
Stated as largest value
UK decimal coinage history.
Decimalisation.
Since decimalisation on 15 February 1971 the pound (symbol "£") has been divided into 100 pence. (Prior to decimalisation the pound was divided into 20 shillings, each of 12 (old) pence; thus there were 240 (old) pence to the pound. The value of the pound itself was unchanged by decimalisation.)
The first decimal coins – the five pence (5p) and ten pence (10p) — were introduced in 1968 in the run-up to decimalisation in order to familiarise the public with the new system. These initially circulated alongside the pre-decimal coinage and had the same size and value as the existing one shilling and two shilling coins respectively. The fifty pence (50p) coin followed in 1969, replacing the old ten shilling note. The remaining decimal coins – at the time, the half penny (½p), penny (1p) and two pence (2p) — were issued in 1971 at decimalisation. A quarter-penny coin, to be struck in aluminium, was proposed at the time decimalisation was being planned, but was never minted.
The new coins were initially marked with the wording (singular) or (plural). The word "new" was dropped in 1982. The symbol "p" was adopted to distinguish the new pennies from the old, which used the symbol "d" (from the Latin "denarius," a coin used in the Roman Empire).
Post 1982.
In the years since decimalisation a number of changes have been made to the coinage. The twenty pence (20p) coin was introduced in 1982 to fill the gap between the 10p and 50p coins. The pound coin (£1) was introduced in 1983 to replace the Bank of England £1 banknote which was discontinued in 1984 (although the Scottish banks continued producing them for some time afterwards; the last of them, the Royal Bank of Scotland £1 note, is still in production as of 2013). The designs on the one pound coin change annually in a largely five-year cycle.
The decimal half penny coin was demonetised in 1984 as its value was by then too small to be useful. The pre-decimal sixpence, shilling and two shilling coins, which had continued to circulate alongside the decimal coinage with values of 2½p, 5p and 10p respectively, were finally withdrawn in 1980, 1990 and 1993 respectively.
In the 1990s the Royal Mint reduced the sizes of the 5p, 10p and 50p coins. As a consequence, the oldest 5p coins in circulation date from 1990, the oldest 10p coins from 1992 and the oldest 50p coins come from 1997. Since 1997, many special commemorative designs of 50p have been issued. Some of these are found fairly frequently in circulation and some are rare. They are all legal tender.
The specifications and dates of introduction of the 5p, 10p and 50p coins refer to the current versions. These coins were originally issued in larger sizes in 1968 and 1969 respectively.
With their high copper content (97%), the intrinsic value of pre-1992 1p and 2p coins increased with the surge in metal prices of the mid-2000s, until by 2006 the coins would, if melted down, have been worth about 50% more than their face value. (To do this, however, would be illegal, and they would have had to be melted in huge quantities to achieve significant gain.) In later years the price of copper fell considerably. Copper plated steel coins were introduced to replace them.
A circulating bimetallic two pound (£2) coin was introduced in 1998 (first minted in, and dated, 1997). There had previously been unimetallic commemorative £2 coins which did not normally circulate. This tendency to use the two pound coin for commemorative issues has continued since the introduction of the bimetallic coin, and a few of the older unimetallic coins have since entered circulation.
There are also commemorative issues of crowns. Before 1990 these had a face value of twenty-five pence (25p), equivalent to the five shilling crown used in pre-decimal Britain. However, in 1990 crowns were redenominated with a face value of five pounds (£5) as the previous value was considered not sufficient for such a high-status coin. The size and weight of the coin remained exactly the same. Decimal crowns are generally not found in circulation as their market value is likely to be higher than their face value, but they remain legal tender.
2008 redesign<span id="Reverse designs (from 2008)" />.
In 2008, UK coins underwent an extensive redesign, which changed the reverse designs, and some other details, of all coins except the £2. The original intention was to exclude both the £1 and £2 coins from the redesign because they were "relatively new additions" to the coinage, but it was later decided to include the £1 coin. This was the first wholesale change to British coinage since the first decimal coins were introduced in April 1968.
The new coins were initially to be put into circulation in early 2008, although they did not actually start to appear until mid-2008.
The major design feature was the introduction of a reverse design shared across six coins (1p, 2p, 5p, 10p, 20p, 50p), that can be pieced together to form an image of the Royal Shield. This was the first time a coin design had been featured across multiple coins in this way. Completing the set, the new £1 reverse features the Shield in its entirety. The effigy of the Queen, by Ian Rank-Broadley, continues to appear on the obverse of all the coins.
On all coins, the beading (ring of small dots) around the edge of the obverses has been removed. The obverse of the 20p coin has also been amended to incorporate the year, which had been on the reverse of the coin since its introduction in 1982 (giving rise to an unusual issue of a mule version without any date at all). The orientation of both sides of the 50p coin has been rotated through 180 degrees, meaning the bottom of the coin is now a corner rather than a flat edge. The numerals showing the decimal value of each coin, previously present on all coins except £2 and £1, have been removed, leaving the values spelled out in words only.
The redesign was the result of a competition launched by the Royal Mint in August 2005, which closed on 14 November 2005. The competition was open to the public and received over 4,000 entries. The winning entry was unveiled on 2 April 2008, designed by Matthew Dent. The Royal Mint stated the new designs were "reflecting a twenty-first century Britain". An advisor to the Royal Mint described the new coins as "post-modern" and said that this was something that could not have been done 50 years previously.
The redesign was criticised by some for having no specifically Welsh symbol (such as the Welsh Dragon), because the Royal Shield does not include a specifically Welsh symbol. Wrexham MP Ian Lucas, who was also campaigning to have the Welsh Dragon included on the Union Flag, called the omission "disappointing", and stated that he would be writing to the Queen to request that the Royal Standard be changed to include Wales. The Royal Mint stated that "the Shield of the Royal Arms is symbolic of the whole of the United Kingdom and as such, represents Wales, Scotland, England and Northern Ireland." Designer Dent stated "I am a Welshman and proud of it, but I never thought about the fact we did not have a dragon or another representation of Wales on the design because as far as I am concerned Wales is represented on the Royal Arms. This was never an issue for me."
The designs were also criticised for not including a portrayal of Britannia, the female personification of Britain whose image has appeared on British coinage continuously since 1672. In response to the concern over the loss of Britannia, the chairman of the Royal Mint Advisory Committee stated "There are 806 million Britannias in circulation at the moment [on the old 50p coin]. They will remain in circulation. They will see all of us out, until they die a natural death. So whatever happens, Britannia stays around".
The Royal Mint's choice of an inexperienced coin designer to produce the new coinage was criticised by Virginia Ironside, daughter of Christopher Ironside who designed the previous UK coins. She stated that the new designs were "totally unworkable as actual coins", due to the loss of a numerical currency identifier, and the smaller typeface used.
The German news magazine "Der Spiegel" claimed that the redesign signalled the UK's intention "not to join the euro any time soon".
Steel 5p and 10p coins.
As of 2012, 5p and 10p coins have been issued in nickel-plated steel, and much of the remaining cupro-nickel types withdrawn, in order to retrieve more expensive metals. The new coins are 11% thicker to maintain the same weight.
There are heightened nickel allergy concerns over the new coins. Studies commissioned by the Royal Mint found no increased discharge of nickel from the coins when immersed in artificial sweat. However, an independent study found that the friction from handling results in four times as much nickel exposure as from the older-style coins. Sweden already plans to desist from using nickel in coins from 2015.
Specifications.
†The specification and date of introduction of the £2 coin refers to the current version. This coin was originally issued in a smaller size in a single metal in 1986 for special issues only.
With their high copper content (97%), the intrinsic value of pre-1992 1p and 2p coins increased with the surge in metal prices of the mid-2000s, until by 2006 the coins, would, if melted down, have been worth about 50% more than their face value. (To do this, however, would be illegal, and they would have had to be melted in huge quantities to achieve significant gain.) In subsequent years the price of copper fell considerably from these peaks.
UK designs.
Obverse.
All modern British coins feature a profile of the current monarch's head on the obverse. There has been only one monarch since decimalisation, Queen Elizabeth II, so her head appears on all decimal coins, facing to the right (see also Monarch's head, above). However, five different effigies have been used, reflecting the Queen's changing appearance as she has aged. These are the effigies by Mary Gillick (until 1968), Arnold Machin (1968–1984), Raphael Maklouf (1985–1997), Ian Rank-Broadley (1998–2015), and Jody Clark (from 2015).
All current coins carry a Latin inscription whose full form is , meaning "Elizabeth II, by the grace of God, Queen and Defender of the Faith". The inscription appears on the coins in any of several abbreviated forms, typically .
From 2008 the circle of dots between the lettering and the rim was removed from the 1p, 2p, 5, and 10p and £1. It was never on the 20p and 50p, and is retained on the £2.
Original reverse designs.
The original standard-issue decimal coinage reverse designs are as follows:
Up until the 2008 redesign, the reverse designs of the one pound coin have followed a five-year cycle. This cycle successively represents, by using royal heraldic badges, each of the four constituent countries of the United Kingdom, namely Scotland, Wales, Northern Ireland and England, with the Royal Coat of Arms used in every fifth year:
Royal Shield reverse.
The 1p, 2p, 5p, 10p, 20p and 50p coin designs post 2008 each depict a part of the Royal Shield, and form the whole shield when they are placed together in the appropriate arrangement. The Royal Shield is seen in its entirety on the £1 coin.
Edge designs.
The 1p, 2p, 20p and 50p coins have smooth edges. The 5p, 10p, £1 and £2 coins have milled edges. The milling, in combination with the non-circular shape of the 20p and 50p, serve as the primary means of identification and differentiation between coinage for blind or visually impaired people. Historically, milling also served to discourage coin clipping.
The £1 coin and £2 coins have, inscribed into the milling, words or a decoration related to their face design. Many issues of the £1 coin carry one of the following edge inscriptions:
The standard-issue £2 coin carries the edge inscription . Other designs of the coin are issued from time to time to commemorate special events or anniversaries. These may have special edge inscriptions relevant to the theme, or the edge inscription may be replaced by a decorative motif.
Commemorative designs.
Circulating fifty pence and two pound coins have been issued with various commemorative reverse designs, typically to mark the anniversaries of historical events or the births of notable people.
Three commemorative designs were issued of the large version of the 50p: in 1973 (the EEC), 1992–3 (EC presidency) and 1994 (D-Day anniversary). Commemorative designs of the smaller 50p coin have been issued (alongside the Britannia standard issue) in 1998 (two designs), 2000, and from 2003 to 2007 yearly (two designs in 2006). For a complete list, see Fifty pence (British decimal coin).
Prior to 1997, the two pound coin was minted in commemorative issues only – in 1986, 1989, 1994, 1995 and 1996. Commemorative £2 coins have been regularly issued since 1999, alongside the standard-issue bi-metallic coins which were introduced in 1997. One or two designs have been minted each year, with the exception of none in 2000, and four regional 2002 issues marking the 2002 Commonwealth Games in Manchester. As well as a distinct reverse design, these coins have an edge inscription relevant to the subject. The anniversary themes are continued until at least 2009, with two designs announced. For a complete list, see Two pounds (British decimal coin).
Non-UK coinage.
Outside the United Kingdom, the British Crown Dependencies of Jersey and Guernsey use the pound sterling as their currencies. However, they produce local issues of coinage in the same denominations and specifications, but with different designs. These circulate freely alongside UK coinage and English, Northern Irish and Scottish banknotes within these territories, but must be converted in order to be used in the UK. The island of Alderney also produces occasional commemorative coins. "(See coins of the Jersey pound, coins of the Guernsey pound, and Alderney pound for details.)". The Isle of Man is a unique case among the Crown Dependencies, issuing its own currency, the Manx pound. While the Isle of Man recognises the Pound Sterling as a secondary currency, coins of the Manx pound are not legal tender in the UK.
The pound sterling is also the official currency of the British overseas territories of South Georgia and the South Sandwich Islands, British Antarctic Territory and Tristan da Cunha. South Georgia and the South Sandwich Islands produces occasional special collectors' sets of coins. In 2008, British Antarctic Territory issued a £2 coin commemorating the centenary of Britain's claim to the region.
The currencies of the British overseas territories of Gibraltar, The Falkland Islands and Saint Helena/Ascension — namely the Gibraltar pound, Falkland Islands pound and Saint Helena pound — are pegged one-to-one to the pound sterling but are technically separate currencies. These territories issue their own coinage, again with the same denominations and specifications as the UK coinage but with local designs, as coins of the Gibraltar pound, coins of the Falkland Islands pound and coins of the Saint Helena pound.
The other British overseas territories do not use the pound as their official currency.
Non-circulating coins.
25p and £5 coins.
Although these coins are in practice very rarely found in circulation, they are for convenience described with the circulating coins, above.
Maundy money.
Maundy money is a ceremonial coinage traditionally given to the poor, and nowadays awarded annually to deserving senior citizens. There are Maundy coins in denominations of one, two, three and four pence. They bear dates from 1822 to the present and are minted in very small quantities. Though they are legal tender in the UK, they are never encountered in circulation. The pre-decimal Maundy pieces have the same legal tender status and value as post-decimal ones, and effectively increased in face value by 140% upon decimalisation. Their numismatic value is much greater.
Maundy coins still bear the original portrait of the Queen as used in the circulating coins of the first years of her reign.
Bullion coinage.
The traditional bullion coin issued by Britain is the gold sovereign, formerly a circulating coin with a face value of one pound. The Royal Mint continues to produce gold sovereigns and half sovereigns, with 2013 list prices of, respectively, £495 and £250.
Between 1987-2012 a series of bullion coins, the Britannia, was issued, containing 1 ozt, 1/2 ozt, 1/4 ozt, and 1/10 ozt of fine gold at a millesimal fineness of 917 (22 carat) and with face values of £100, £50, £25, and £10.
Since 2013 Britannia bullion contains 1 ozt of fine gold at a millesimal fineness of 999 (24 carat).
Between 1997-2012 silver bullion coins have also been produced under the name “Britannias”. The alloy used was Britannia silver (millesimal fineness 958). The silver coins were available in 1 ozt, 1/2 ozt, 1/4 ozt, and 1/10 ozt sizes.
Since 2013 the alloy used is silver at a (millesimal fineness 999). The silver coins are available in 1 ozt sizes.
The Royal Mint also issues silver, gold and platinum proof sets of the circulating coins, as well as gift products such as gold coins set into jewellery.
Pre-decimal coinage.
System.
Before decimalisation in 1971, the pound was divided into 240 pence rather than 100, though it was rarely expressed in this way. Rather it was expressed in terms of pounds, shillings and pence, where:
Thus: £1 = 240 pence. The penny was further subdivided at various times, though these divisions vanished as inflation made them irrelevant:
Using the example of five shillings and sixpence, the standard ways of writing shillings and pence were:
The sum of 5/6 would be spoken as "five shillings and sixpence" or "five and six".
The abbreviation for the old penny, d, was derived from the Roman "denarius", and the abbreviation for the shilling, s, from the Roman "solidus". The shilling was also denoted by the slash symbol, also called a solidus for this reason, which was originally an adaptation of the long s. The symbol "£", for the pound, is derived from the first letter of the Latin word for pound, "libra".
A similar pre-decimal system operated in France, also based on the Roman currency, consisting of the "livre" (L), "sol" or "sou" (s) and "denier" (d). Until 1816 another similar system was used in the Netherlands, consisting of the "gulden" (G), "stuiver" (s; 1/20 G) and "duit", (d; 1/8 s or 1/160 G).
Denominations.
In the years just prior to decimalisation, the circulating British coins were:
The farthing (¼d) had been demonetised on 1 January 1961, while the crown (5/-) was issued periodically as a commemorative coin but rarely found in circulation.
The crown, half crown, florin, shilling and sixpence were cupro-nickel coins (in historical times silver or silver alloy); the penny, halfpenny and farthing were bronze; and the threepence was a twelve-sided nickel-brass coin (historically it was a small silver coin).
Some of the pre-decimalisation coins with exact decimal equivalent values continued in use after 1971 alongside the new coins, albeit with new names (the shilling became equivalent to the 5p coin, with the florin equating to 10p), and the others were withdrawn almost immediately. The use of florins and shillings as legal tender in this way ended in 1990 and 1992 when the 5p and 10p coins were replaced with smaller versions. Indeed, while pre-decimalisation shillings were used as 5p coins, for a while after decimalisation many people continued to call the new 5p coin a shilling, since it remained 1/20 of a pound, but was now worth 5p instead of 12d. The pre-decimalisation sixpence, also known as a sixpenny bit or sixpenny piece, was rated at 2½p, but was demonetised in 1980.
Slang and everyday usage.
Some pre-decimalisation coins or denominations became commonly known by colloquial and slang terms, perhaps the most well known being "bob" for a shilling, and "quid" for a pound. A farthing was a "mag", a silver threepence was a "joey" and the later nickel-brass threepence was called a "threepenny bit" ( or bit, i.e. thrup'ny or threp'ny bit – the apostrophe was pronounced on a scale from full "e" down to complete omission); a sixpence was a "tanner", the two-shilling coin or florin was a "two-bob bit". Bob is still used in phrases such as "earn/worth a bob or two", and "bob‐a‐job week". The two shillings and sixpence coin or half-crown was a "half dollar", also sometimes referred to as "two and a kick". A value of two pence was universally pronounced "tuppence", a usage which is still heard today, especially among older people. The unaccented suffix "-pence", pronounced , was similarly appended to the other numbers up to twelve; thus "fourpence", "sixpence-three-farthings", "twelvepence-ha'penny", but "eighteen pence" would usually be said "one-and-six".
"Quid" remains as popular slang for one or more pounds to this day in Britain in the form "a quid" and then "two quid", and so on. Similarly, in some parts of the country, "bob" continued to represent one-twentieth of a pound, that is five new pence, and "two bob" is 10p.
The introduction of decimal currency caused a new casual usage to emerge, where any value in pence is spoken using the suffix "pee": e.g. "twenty-three pee" or, in the early years, "two-and-a-half pee" rather than the previous "tuppence-ha'penny". Amounts over a pound are normally spoken thus: "five pounds forty". A value with less than ten pence over the pound is sometimes spoken like this: "one pound and a penny", "three pounds and fourpence". The slang term "bit" has almost disappeared from use completely, although in Scotland a fifty pence is sometimes referred to as a "ten bob bit". Decimal denomination coins are generally described using the terms "piece" or coin, for example "a fifty-pee piece", a "ten-pence coin".
Coins in the colonies.
A 1½ "d." coin was circulated in Jamaica in the nineteenth century. Jamaicans referred to the coin as a "quatty".
Minting errors reaching circulation.
Coins with errors in the minting process that reach circulation are often seen as valuable items by coin collectors.
In 1983 the Royal Mint mistakenly produced some two pence pieces with the old wording "New Pence" on the reverse (tails) side, when the design had been changed from 1982 to "Two Pence".
In June 2009 the Royal Mint estimated that between 50,000 and 200,000 dateless 20 pence coins had entered circulation, the first undated British coin to enter circulation in more than 300 years. It resulted from the accidental combination of old and new face tooling in a production batch, creating what is known as a mule, following the 2008 redesign which moved the date from the reverse (tails) to the obverse (heads) side.
Titles.
From a very early date, British coins have been inscribed with the name of the ruler of the kingdom in which they were produced, and a longer or shorter title, always in Latin; among the earliest distinctive English coins are the silver pennies of Offa of Mercia, which were inscribed with the legend "King Offa". As the legends became longer, words in the inscriptions were often abbreviated so that they could fit on the coin; identical legends have often been abbreviated in different ways depending upon the size and decoration of the coin. Inscriptions which go around the edge of the coin generally have started at the center of the top edge and proceeded in a clockwise direction. A very lengthy legend would be continued on the reverse side of the coin.
More recent legends include the following (the full unabbreviated text is given here):
Mottos.
In addition to the title, a Latin or French motto might be included, generally on the reverse side of the coin. These varied between denominations and issues; some were personal to the monarch, others were more general. Some of the mottos were:

</doc>
<doc id="39225" url="http://en.wikipedia.org/wiki?curid=39225" title="Melvin Kranzberg">
Melvin Kranzberg

Melvin Kranzberg (November 22, 1917 – December 6, 1995) was a professor of history at Case Western Reserve University from 1952 until 1971. He was a Callaway professor of the history of technology at Georgia Tech from 1972 to 1988.
Born in St. Louis, Missouri, Kranzberg graduated from Amherst College, received a master's and a PhD from Harvard College and served in the Army in Europe during World War II. He received a Bronze Star for interrogating captured German prisoners and learning the location of Nazi gun emplacements. He was one of two interrogators out of nine in Patton's army that were not killed during the conflict.
Kranzberg is known for his laws of technology, the first of which states "Technology is neither good nor bad; "nor is it neutral"."
He was one of the founders of the Society for the History of Technology in the US and long-time editor of its journal "Technology and Culture". Kranzberg served as president of the society from 1983 to 1984, and edited the society's journal from 1959 to 1981, when he turned it over to Robert C. Post of the Smithsonian Institution. The society awards a yearly $4000 fellowship named after Kranzberg to doctoral students engaged in the preparation of dissertations on the history of technology. The award is available to students all over the world.
Kranzberg's laws of technology.
Melvin Kranzberg's six laws of technology state:
References.
Taken from Kranzberg, Melvin (1986) Technology and History: "Kranzberg's Laws", "Technology and Culture", Vol. 27, No. 3, pp. 544–560.
There are two biographical articles by Robert C. Post in Technology and Culture: "Back at the Start: History and Technology and Culture," T&C 51 (2010): 961-94, and "Chance and Contingency: Putting Mel Kranzberg in Context," T&C 50 (2009): 839-72. Kranzberg's role in founding ICOHTEC is addressed by Post in "Our Mel Kranzberg: Risks He Took, Stumbles, and Sometimes a Second Thought," ICON 20 (2014): 6-16.

</doc>
<doc id="39229" url="http://en.wikipedia.org/wiki?curid=39229" title="Open system (computing)">
Open system (computing)

Open systems are computer systems that provide some combination of interoperability, portability, and open software standards. (It can also refer to specific installations that are configured to allow unrestricted access by people and/or other computers; this article does not discuss that meaning).
The term was popularized in the early 1980s, mainly to describe systems based on Unix, especially in contrast to the more entrenched mainframes and minicomputers in use at that time. Unlike older legacy systems, the newer generation of Unix systems featured standardized programming interfaces and peripheral interconnects; third party development of hardware and software was encouraged, a significant departure from the norm of the time, which saw companies such as Amdahl and Hitachi going to court for the right to sell systems and peripherals that were compatible with IBM's mainframes.
The definition of "open system" can be said to have become more formalized in the 1990s with the emergence of independently administered software standards such as The Open Group's Single UNIX Specification.
Although computer users today are used to a high degree of both hardware and software interoperability, in the 20th century the open systems concept could be promoted by Unix vendors as a significant differentiator. IBM and other companies resisted the trend for decades, exemplified by a now-famous warning in 1991 by an IBM account executive that one should be "careful about getting locked into open systems".
However, in the first part of the 21st century many of these same legacy system vendors, particularly IBM and Hewlett-Packard, began to adopt Linux as part of their overall sales strategy, with "open source" marketed as trumping "open system". Consequently an IBM mainframe with Linux on zSeries is marketed as being more of an open system than commodity computers using closed-source Microsoft Windows—or even those using Unix, despite its open systems heritage. In response, more companies are opening the source code to their products, with a notable example being Sun Microsystems and their creation of the OpenOffice.org and OpenSolaris projects, based on their formerly closed-source StarOffice and Solaris software products.

</doc>
<doc id="39230" url="http://en.wikipedia.org/wiki?curid=39230" title="Charles Pierce (female impersonator)">
Charles Pierce (female impersonator)

Charles Pierce (July 14, 1926 – May 31, 1999) was one of the 20th century's foremost female impersonators, particularly noted for his impersonation of Bette Davis.
Life and career.
Born in Watertown, New York, he began his show business career playing the organ and acting in radio dramas at station WWNY. He branched out into a comedy routine, attired in tuxedo, yet managing to evoke eerily convincing imitations of popular movie actresses. Eschewing the term "drag queen", which he hated, he billed himself as a "male actress".
Initially playing in small gay clubs, his fame spread. He took up residence in San Francisco, where his act became well known to Hollywood stars. As he toured, his costuming became more elaborate, initially adding small props, later full costume and makeup changes. His imitations were imitated by other female impersonators; and his roles included Bette Davis, Mae West, Tallulah Bankhead, Gloria Swanson, Carol Channing, Katharine Hepburn, and Joan Crawford, which became the drag queen canon. His act was centered on wit rather than accurate mimicry, though it was often said that he looked more like Joan Collins than did Joan Collins herself.
Carol Channing was the only Hollywood celebrity Charles "impersonated" who actually saw his act. She went backstage after a show at Gold Street in San Francisco (c. 1972) and said, "Cheee-yarles: you do me better than I do!”
He performed at many clubs in New York, including The Village Gate, Ted Hook's OnStage, The Ballroom, and Freddy's Supper Club. His numerous San Francisco venues included the Gilded Cage, Cabaret/After Dark, Gold Street, Bimbo's 365 Club, Olympus, The Plush Room, the Venetian Room at the Fairmont Hotel, Louise M. Davies Symphony Hall, and the War Memorial Opera House. 
He was a guest actor on an episode of TV's "Wonder Woman" and played a cross-dressing villain in an episode of "Laverne & Shirley" ("Murder on the Moose Jaw Express"). Pierce also appeared on an early episode of the hit 80's TV sitcom "Designing Women" as a steward on a cruise ship. During the episode, he imitates Joan Collins (as the ship's waitress) and Bette Davis (as the ship's lounge entertainment). As Davis he quips: "Was that Joan Collins hustling the tables? One bitch on this boat is enough!"
He died in North Hollywood, California, aged 72, and was cremated. His memorial service at Forest Lawn Memorial Park was carefully planned and scripted by Pierce before his death. Among those attending his memorial was his friend Bea Arthur, who closed with “I’ll See You Again.” His ashes were interred in the Columbarium of Providence, Forest Lawn - Hollywood Hills Cemetery in Los Angeles, California.
In Bea Arthur's Tony Award-nominated one-woman show, "Just Between Friends", which she played on Broadway and in London's West End, Arthur performs Pierce's favorite joke, "A Mother's Ingenuity". It can be heard in the CD cast recording.
References.
it was noted that carol channing was the only hollywood actor to recognize charles pierce by seeing him backstage but pierce played the old Cicro's nightclub in west hollywood in 1974(?). The show was produced by Les Natalie and had a venu of dancers, comedian(Peter Pitt) and a husband and wife team of singers(---& Renee Campos. Charles did a bit from Coco on broadway--they were lines spoken by Sir John Guilguld and one night he came to see the show and pierce introduced him to the audience.
I was a bartender at the club in the early seventies(most of the time it was "Art Laboe's Oldies but Goodies Niteclub
Jack Wentzel

</doc>
<doc id="39231" url="http://en.wikipedia.org/wiki?curid=39231" title="MFT">
MFT

MFT may refer to:

</doc>
<doc id="39235" url="http://en.wikipedia.org/wiki?curid=39235" title="Albrecht III Achilles, Elector of Brandenburg">
Albrecht III Achilles, Elector of Brandenburg

Albert III (German: "Albrecht III.") (9 November 1414 – 11 March 1486), often known simply as Albert Achilles ("Albrecht Achilles"), was a Prince-elector of the Margraviate of Brandenburg. He received the nickname "Achilles" because of his knightly qualities. He also ruled the Principality of Ansbach.
Early life.
Albert was born in Tangermünde as the third son of Elector Frederick I and his wife, Elisabeth of Bavaria-Landshut. After passing some time at the court of Emperor Sigismund, Albert took part in the war against the Hussites, and afterwards distinguished himself whilst assisting the German king, Albert II, against Poland.
Reign.
On the division of territory which followed his father's death in 1440, Albert received the Principality of Ansbach. Although his resources were meager, he soon took a leading place among the German princes and was especially prominent in resisting the attempts of the towns to obtain self-government.
In 1443, Albert formed a league directed mainly against Nuremberg, over which members of his family had formerly exercised the rights of burgrave. It was not until 1448, however, that he found a pretext for attack. After initial military successes in the First Margrave War, he was defeated at the Battle of Pillenreuther Weiher, resulting in the Treaty of Bamberg (22 June 1450),which forced Albert to return all of the conquered territory and to recognize the independence of Nuremberg and its associated towns.
Albert supported Holy Roman Emperor Frederick III in his struggle with the princes who desired reforms in the Holy Roman Empire, and in return for this loyalty received many marks of favour from Frederick, including extensive judicial rights which aroused considerable irritation among neighbouring rulers.
In 1457, Albert arranged a marriage between his eldest son John, and Margaret, daughter of William III, Landgrave of Thuringia, who inherited the claims upon Hungary and Bohemia of her mother, a granddaughter of Emperor Sigismund. The attempt to secure these thrones for the Hohenzollerns through this marriage failed, and a similar fate befell Albert's efforts to revive in his own favour the disused title of duke of Franconia.
The sharp dissensions which existed among the princes over the question of reform culminated in open warfare in 1460, when Albert was confronted with a league under the leadership of the Count Palatine, Frederick I, and Louis IX, Duke of Bavaria-Landshut. Defeated in this struggle, which was concluded in 1462, Albert made an alliance with his former enemy, George of Poděbrady, King of Bohemia, a step which caused Pope Paul II to place him under the ban.
In 1470, Albert, who had inherited Bayreuth on the death of his brother John in 1464, became Margrave of Brandenburg, owing to the abdication of his remaining brother, Elector Frederick II. He was soon actively engaged in its administration, and by the Treaty of Prenzlau in 1472 he brought Pomerania also under his supremacy. Having established his right to levy a tonnage on wines in the mark, he issued in February 1473 the "Dispositio Achillea", which decreed that the Margraviate of Brandenburg should descend in its entirety to the eldest son, while the younger sons should receive the Franconian possessions of the family.
After treating in vain for a marriage between one of his sons and Mary, daughter and heiress of Charles the Bold, Duke of Burgundy, Albert handed over the government of Brandenburg to his eldest son John, and returned to his Franconian possessions.
Albert's main attention afterwards was claimed by the business of the empire. Soon after taking part in the election of Maximilian as King of the Romans, Albert died at Frankfurt in March 1486. He left a considerable amount of treasure.
Dynastic marriages of his children.
In 1474, Albert married his daughter Barbara to Duke Henry XI of Głogów, who left his possessions on his death in 1476 to his widow with reversion to her family, an arrangement which was resisted by Henry's kinsman, Duke Jan II of Żagań. Aided by King Matthias Corvinus of Hungary, Jan of Żagań invaded Brandenburg, and the Pomeranians seized the opportunity to revolt. Under these circumstances Albert returned to Brandenburg in 1478, compelled the Pomeranians to recognize his supremacy, and, after a stubborn struggle, secured a part of Duke Henry's lands for his daughter in 1482. 
Family and children.
Albert was married twice. First, he married 12 November 1446 Margaret of Baden, daughter of Margrave Jakob I of Baden and Catherine of Lorraine. From this marriage he had following children:
Margaret died 24 October 1457 and in 1458 Albert married Anna, daughter of Frederick II, Elector of Saxony and Margarete of Austria. Their children were:
References.
Mario Müller (Ed.): Kurfürst Albrecht Achilles (1414-1486). Kurfürst von Brandenburg, Burggraf von Nürnberg (Jahrbuch des Historischen Vereins für Mittelfranken, vol. 102), Ansbach 2014. ISSN: 0341-9339.

</doc>
