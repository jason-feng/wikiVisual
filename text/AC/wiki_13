<doc id="27646" url="http://en.wikipedia.org/wiki?curid=27646" title="Spy fiction">
Spy fiction

Spy fiction, a genre of literature involving espionage as an important context or plot device, emerged in the early twentieth century, inspired by rivalries and intrigues between the major powers, and the establishment of modern intelligence agencies. It was given new impetus by the development of fascism and communism in the lead-up to World War II, continued to develop during the Cold War, and received a fresh impetus from the emergence of rogue states, international criminal organizations, Muslim fundamentalism, global terrorist networks, maritime piracy and technological sabotage and espionage as potent threats to Western societies.
As a genre, spy fiction is thematically related to the novel of adventure ("The Prisoner of Zenda", 1894, "The Scarlet Pimpernel", 1905), the thriller (such as the works of Edgar Wallace) and the politico–military thriller ("The Schirmer Inheritance", 1953, "The Quiet American", 1955).
History.
Nineteenth century.
Early examples of the espionage novel are "The Spy" (1821) and "The Bravo" (1831), by American novelist James Fenimore Cooper. "The Bravo" attacks European anti-republicanism, by depicting Venice as a city-state where a ruthless oligarchy wears the mask of the "serene republic".
In nineteenth-century France, the Dreyfus Affair (1894–99) contributed much to public interest in espionage. For some twelve years (ca. 1894–1906), the Affair, which involved elements of international espionage, treason, and anti-Semitism, dominated French politics. The details were reported by the world press: an Imperial German penetration agent betraying to Germany the secrets of the General Staff of the French Army; the French counter-intelligence riposte of sending a charwoman to rifle the trash in the German Embassy in Paris, were news that inspired successful spy fiction.
The major themes of spy in the lead-up to the First World War were the continuing rivalry between the European colonial powers for control of Asia, the growing threat of conflict in Europe, the domestic threat of revolutionaries and anarchists, and historical romance.
"Kim" (1901) by Rudyard Kipling concerns the Anglo–Russian Great Game of imperial and geopolitical rivalry and strategic warfare for supremacy in Central Asia, usually in Afghanistan. "The Secret Agent" (1907) by Joseph Conrad examines the psychology and ideology motivating the socially marginal men and women of a revolutionary cell determined to provoke revolution in Britain with a terrorist bombing of the Greenwich Observatory. G. K. Chesterton's "The Man Who Was Thursday" (1908) is a metaphysical thriller ostensibly based on the infiltration of an anarchist organisation by detectives; but the story is actually a vehicle for exploring society's power structures and the nature of suffering.
The fictional detective Sherlock Holmes, created by Arthur Conan Doyle, served as a spyhunter for Britain in the stories "The Adventure of the Second Stain" (1904), and "The Adventure of the Bruce-Partington Plans" (1912). In "His Last Bow" (1917), he served Crown and country as a double agent, transmitting false intelligence to Imperial Germany on the eve of the Great War.
"The Scarlet Pimpernel" (1905) by Baroness Orczy chronicled an English aristocrat's derring-do in rescuing French aristocrats from the Reign of Terror of the populist French Revolution (1789–99).
But the term "spy novel" was defined by "The Riddle of the Sands" (1903) by Irish author Robert Erskine Childers. It described amateur spies discovering a German plan to invade Britain. Its success created a market for the invasion literature subgenre, which was flooded by imitators. William Le Queux and E. Phillips Oppenheim became the most widely read and most successful British writers of spy fiction, especially of invasion literature. Their prosaic style and formulaic stories, produced voluminously from 1900 to 1914, proved of low literary merit.
During the First World War.
During the War, the propagandist John Buchan became the pre-eminent British spy novelist. His well written stories portray the Great War as a "clash of civilisations" between Western civilization and barbarism. His notable novels are "The Thirty-nine Steps" (1915), "Greenmantle" (1916) and sequels, all featuring the heroic Scotsman Richard Hannay. In France Gaston Leroux published the spy thriller "Rouletabille chez Krupp" (1917), in which a detective, Joseph Rouletabille, engages in espionage.
Inter-war period.
After the successful Russian Revolution (1917), the quality of spy fiction declined, perhaps because the Bolshevik enemy had won the Russian Civil War (1917–23). Thus, the inter-war spy story usually concerns combating the Red Menace, which was then perceived as another "clash of civilizations".
Spy fiction was dominated by British authors during this period, initially former intelligence officers and agents writing from inside the trade. Examples include "" (1928) by W. Somerset Maugham, about counter-revolutionary British espionage against Bolshevik Russia, and "The Mystery of Tunnel 51" (1928) by Alexander Wilson whose novels conveyed an uncanny portrait of the first head of the Secret Intelligence Service, Mansfield Smith-Cumming, the original 'C'.
At a more popular level, Leslie Charteris' popular and long-running "Saint" series began, featuring Simon Templar, with "Meet the Tiger" (1928). "Water on the Brain" (1933) by former intelligence officer Compton Mackenzie was the first successful spy novel satire. Prolific author Dennis Wheatley also wrote his first spy novel, "The Eunuch of Stamboul" (1935) during this period.
Second World War.
The growing threat of fascism in Germany, Italy and Spain, and the imminence of war, attracted quality writers back to spy fiction.
British author Eric Ambler brought a new realism to spy fiction. "The Dark Frontier" (1936), "Epitaph for a Spy" (1938), "The Mask of Dimitrios" (US: "A Coffin for Dimitrios", 1939), and "Journey into Fear" (1940) feature amateurs entangled in espionage. The politics and ideology are secondary to the personal story that involved the hero or heroine. Ambler's Popular Front–period "œuvre" has a left-wing perspective about the personal consequences of "big picture" politics and ideology, which was notable, given spy fiction's usual right-wards tilt in defence of the Establishment attitudes underpinning empire and imperialism. Ambler's early novels "Uncommon Danger" (1937) and "Cause for Alarm" (1938), in which NKVD spies help the amateur protagonist survive, are especially remarkable among English-language spy fiction.
"Above Suspicion" (1939) by Helen MacInnes, about an anti-Nazi husband and wife spy team, features literate writing and fast-paced, intricate, and suspenseful stories occurring against contemporary historical backgrounds. MacInnes wrote many other spy novels in the course of a long career, including "Assignment in Brittany" (1942), "Decision at Delphi" (1961), and "Ride a Pale Horse" (1984).
Manning Coles published "Drink to Yesterday" (1940), a grim story occurring during the Great War, which introduces the hero Thomas Elphinstone Hambledon. However, later novels featuring Hambledon were lighter-toned, despite being set either in Nazi Germany or Britain during the Second World War (1939–45). After the War, the Hambledon adventures fell to formula, losing critical and popular interest.
The events leading up to the Second World War, and the War itself, continue to be fertile ground for authors of spy fiction. Notable examples include Ken Follett, "Eye of the Needle" (1978); Alan Furst, "Night Soldiers" (1988); and David Downing, the Station series, beginning with "Zoo Station" (2007).
The early Cold War.
The metamorphosis of the Second World War (1939–45) into the Soviet–American Cold War (1945–91) gave new impetus to spy novelists.
British.
With "Secret Ministry" (1951), Desmond Cory introduced Johnny Fedora, the secret agent with a licence to kill, the government-sanctioned assassin. Ian Fleming, a former member of MI5, followed swiftly with the glamorous James Bond, secret agent 007 of the British Secret Service, a mixture of counter-intelligence officer, assassin and playboy. Perhaps the most famous fictional spy, Bond was introduced in "Casino Royale" (1953). After Fleming's death the franchise continued under other British and American authors, including Kingsley Amis, Christopher Wood, John Gardner, Raymond Benson, Sebastian Faulks, Jeffery Deaver and William Boyd.
Despite the commercial success of Fleming's extravagant anti-Communist novels, John le Carré, himself a former spy, created anti-heroic protagonists who struggled with the ethical issues involved in espionage, and sometimes resorted to immoral tactics. Le Carré's middle-class George Smiley is a middle-aged spy burdened with an unfaithful, upper-class wife who publicly cuckolds him for sport.
Like Le Carré, former British Intelligence officer Graham Greene also examined the morality of espionage in left-leaning, anti-imperialist novels such as "The Heart of the Matter" (1948), set in Sierra Leone, the seriocomic "Our Man in Havana" (1959) occurring in the Cuba of dictator Fulgencio Batista before his deposition by Fidel Castro's popular Cuban Revolution (1953–59), and "The Human Factor" (1978) about British support for the apartheid National Party government of South Africa, against the Red Menace.
Other novelists followed a similar path. Len Deighton's anonymous spy, protagonist of "The IPCRESS File" (1962), "Horse Under Water" (1963), "Funeral in Berlin" (1964), and others, is a working-class man with a negative view of the Establishment.
Other notable examples of espionage fiction during this period were also built around recurring characters. These include James Mitchell's 'John Craig' series, written under his pseudonym 'James Munro', beginning with "The Man Who Sold Death" (1964); and Trevor Dudley-Smith's Quiller spy novel series written under the pseudonym 'Adam Hall', beginning with "The Berlin Memorandum" (US: "The Quiller Memorandum", 1965), a hybrid of glamour and dirt, Fleming and Le Carré; and William Garner's fantastic Michael Jagger in "Overkill" (1966), "The Deep, Deep Freeze" (1968), "The Us or Them War" (1969) and "A Big Enough Wreath" (1974).
Other important British writers who first became active in spy fiction during this period include Padraig Manning O'Brine, "Killers Must Eat" (1951); Michael Gilbert, "Be Shot for Sixpence" (1956); Alistair MacLean, "The Last Frontier" (1959); Brian Cleeve, "Assignment to Vengeance" (1961); Jack Higgins, "The Testament of Caspar Schulz" (1962); and Desmond Skirrow, "It Won't Get You Anywhere" (1966). Dennis Wheatley's 'Gregory Sallust' (1934-1968) and 'Roger Brook' (1947-1974) series were also largely written during this period.
American.
US spy novelists began to achieve a measure of parity in a genre dominated by British writers.
During the war E. Howard Hunt wrote his first spy novel, "East of Farewell" (1943). In 1949 he joined the recently created CIA, and continued to write spy fiction for many years. In 1955, Edward S. Aarons began publishing the Sam Durell CIA "Assignment" series, which began with "Assignment to Disaster" (1955). Donald Hamilton published "Death of a Citizen" (1960) and "The Wrecking Crew" (1960), beginning the series featuring Matt Helm, a CIA assassin and counter-intelligence agent.
The Nick Carter-Killmaster series of spy novels, initiated by Michael Avallone and Valerie Moolman, but authored anonymously, ran to over 260 separate books between 1964 and the early 1990s and invariably pitted American, Soviet and Chinese spies against each other. With the proliferation of male protagonists in the spy fiction genre, writers and book packagers also started bringing out spy fiction with a female as the protagonist. One notable spy series is "The Baroness", featuring a sexy female superspy, with the novels being more action-oriented, in the mould of Nick Carter-Killmaster.
Other important American authors who became active in spy fiction during this period include Ross Thomas, "The Cold War Swap" (1966).
The later Cold War.
The June 1967 Six Day War between Israel and its neighbours introduced new themes to espionage fiction - the conflict between Israel and the Palestinians, against the backdrop of continuing Cold War tensions, and the increasing use of terrorism as a political tool.
British.
Notable recurring characters from this era include Adam Diment's Philip McAlpine is a long-haired, hashish-smoking fop in the novels "The Dolly Dolly Spy" (1967), "The Great Spy Race" (1968), "The Bang Bang Birds" (1968) and "Think, Inc." (1971); James Mitchell's 'David Callan' series, written in his own name, beginning with "Red File for Callan" (1969); William Garner's John Morpurgo in "Think Big, Think Dirty" (1983), "Rats' Alley" (1984), and "Zones of Silence" (1986); and Joseph Hone's 'Peter Marlow' series, beginning with "The Private Sector" (1971), set during Israel's Six Day War (1967) against Egypt, Jordan and Syria. In all of these series the writing is literary and the tradecraft believable.
Noteworthy examples of the journalistic style and successful integration of fictional characters with historical events were the politico–military novels "The Day of the Jackal" (1971) by Frederick Forsyth and "Eye of the Needle" (1978) by Ken Follett. With the explosion of technology, Craig Thomas, launched the techno-thriller with "Firefox" (1977), describing the Anglo–American theft of a superior Soviet jet aeroplane.
Other important British writers who first became active in spy fiction during this period include Ian Mackintosh, "A Slaying in September" (1967); Kenneth Benton, "Twenty-Fourth Level" (1969); Desmond Bagley, "Running Blind" (1970); Anthony Price, "The Labyrinth Makers" (1971); Gerald Seymour, "Harry's Game" (1975); Brian Freemantle, "Charlie M" (1977); Bryan Forbes, "Familiar Strangers" (1979); Reginald Hill, "The Spy's Wife" (1980); and Raymond Harold Sawkins, writing as Colin Forbes, "Double Jeopardy" (1982).
American.
"The Scarlatti Inheritance" (1971) by Robert Ludlum is usually considered the first American modern (glamour and dirt) spy thriller weighing action and reflection. In the 1970s, former CIA man Charles McCarry began the Paul Christopher series with "The Miernik Dossier" (1973) and "The Tears of Autumn" (1978), which were well-written, with believable tradecraft.
The first American techno-thriller was "The Hunt for Red October" (1984) by Tom Clancy. It introduced CIA deskman (analyst) Jack Ryan as a field agent; he reprised the role in the sequel "The Cardinal of the Kremlin" (1987).
Other important American authors who became active in spy fiction during this period include Robert Littell, "The Defection of A. J. Lewinter" (1973); James Grady, "Six Days of the Condor" (1974); William F. Buckley Jr., "Saving the Queen" (1976); Nelson DeMille, "The Talbot Odyssey" (1984); W. E. B. Griffin, the "Men at War" series (1984-); Stephen Coonts, "Flight of the Intruder" (1986); Canadian-American author David Morrell, "The League of Night and Fog" (1987); David Hagberg, "Without Honor" (1989); Noel Hynd, "False Flags" (1990); and Richard Ferguson, "Oiorpata" (1990).
Writers of other nationalities.
French journalist Gérard de Villiers began to write his "SAS" series in 1965. The franchise now extends to 200 titles and 150 million books.
Julian Semyonov was an influential spy novelist, writing in the Eastern Bloc, whose range of novels and novel series featured a White Russian spy in the USSR; Max Otto von Stierlitz, a Soviet mole in the Nazi High Command, and Felix Dzerzhinsky, founder of the Cheka. In his novels, Semyonov covered much Soviet intelligence history, ranging from the Russian Civil War (1917–1923), through the Great Patriotic War (1941–45), to the Russo–American Cold War (1945–91).
Swedish author Jan Guillou also began to write his "Coq Rouge" series, featuring Swedish spy Carl Hamilton, during this period, beginning in 1986.
Post–Cold War.
The end of the Cold War in 1991 mooted the USSR, Russia and other Iron Curtain countries as credible enemies of democracy, and the US Congress even considered disestablishing the CIA. Espionage novelists found themselves at a temporary loss for obvious nemeses. "The New York Times" ceased publishing a spy novel review column. Nevertheless, counting on the aficionado, publishers continued to issue spy novels by writers popular during the Cold War era, among them "Harlot's Ghost" (1991) by Norman Mailer.
In the US, the new novels "Moscow Club" (1991) by Joseph Finder, "Coyote Bird" (1993) by Jim DeFelice, "Masquerade" (1996) by Gayle Lynds, and "The Unlikely Spy" (1996) by Daniel Silva maintained the spy novel in the post–Cold War world. Other important American authors who first became active in spy fiction during this period include David Ignatius, "Agents of Innocence" (1997); David Baldacci, "Saving Faith" (1999); and Vince Flynn, with "Term Limits" (1999) and a series of novels featuring counter-terrorism expert Mitch Rapp.
In the UK, Robert Harris entered the spy genre with "Enigma" (1995). Other important British authors who became active during this period include Hugh Laurie, "The Gun Seller" (1996); Andy McNab, "Remote Control" (1998); Henry Porter, "Remembrance Day" (2000); and Charles Cumming, "A Spy By Nature" (2001).
Post–9/11.
The terrorist attacks against the US on 11 September 2001, and the subsequent War on Terror, reawakened interest in the peoples and politics of the world beyond its borders. Espionage genre elders such as John le Carré, Frederick Forsyth, Robert Littell, and Charles McCarry resumed work, and many new authors emerged.
Important British writers who wrote their first spy novels during this period include Stephen Leather, "Hard Landing" (2004); and William Boyd, "Restless" (2006).
New American writers include Brad Thor, "The Lions of Lucerne" (2002); Ted Bell, "Hawke" (2003); Alex Berenson, with John Wells appearing for the first time in "The Faithful Spy" (2006); Brett Battles, "The Cleaner" (2007); Ellis Goodman, "Bear Any Burden" (2008); Olen Steinhauer, "The Tourist" (2009); and Richard Ferguson, "Oiorpata" (2012). A number of other established writers began to write spy fiction for the first time, including Kyle Mills, "Fade" (2005) and James Patterson, "Private" (2010).
Swede Stieg Larsson, who died in 2004, was the world's second best-selling author for 2008 due to his "Millennium series", featuring Lisbeth Salander, published posthumously between 2005 and 2007. Other authors of note include Australian James Phelan, beginning with "Fox Hunt" (2010).
Recognising the importance of the thriller genre, including spy fiction, International Thriller Writers (ITW) was established in 2004, and held its first conference in 2006.
Insider spy fiction.
Many authors of spy fiction have themselves been intelligence officers working for British agencies such as MI5 or MI6, or American agencies such as the OSS or its successor, the CIA. 'Insider' spy fiction has a special claim to authenticity, and overlaps with biographical and other documentary accounts of secret service.
The first insider fiction emerged after World War 1 as the thinly disguised reminiscences of former British intelligence officers such as W. Somerset Maugham, Alexander Wilson, and Compton Mackenzie. The tradition continued during World War II with Helen MacInnes and Manning Coles.
Notable British examples from the Cold War period and beyond include Ian Fleming, John le Carré, Graham Greene, Brian Cleeve, Ian Mackintosh, Kenneth Benton, Bryan Forbes, Andy McNab and Chris Ryan. Notable American examples include Charles McCarry, William F. Buckley Jr., W. E. B. Griffin and David Hagberg.
Many post-attack period novels are written by insiders. At the CIA, the number of manuscripts submitted for pre-publication vetting doubled between 1998 and 2005. American examples include Barry Eisler, "A Clean Kill in Tokyo" (2002); Charles Gillen, "Saigon Station" (2003); R J Hillhouse, "Rift Zone" (2004); Gene Coyle, "The Dream Merchant of Lisbon" (2004) and "No Game For Amateurs" (2009); Thomas F. Murphy, "Edge of Allegiance" (2005); Mike Ramsdell, "A Train to Potevka" (2005); T. H. E. Hill, "" (2008); Duane Evans, "North from Calcutta" (2009); and Jason Matthews, "Red Sparrow" (2013).
British examples include "The Code Snatch" (2001) by Alan Stripp, formerly a cryptographer at Bletchley Park; "At Risk" (2004), "Secret Asset" (2006), "Illegal Action" (2007), and "Dead Line" (2008), by Dame Stella Rimington (Director General of MI5 from 1992 to 1996); and Matthew Dunn's "Spycatcher" (2011) and sequels.
Spy television and cinema.
Cinema.
Much spy fiction was adapted as spy films in the 1960s, ranging from the fantastical James Bond series to the realistic "The Spy Who Came in from the Cold" (1965), and the hybrid "The Quiller Memorandum" (1966). While Hamilton's Matt Helm novels were adult and well-written, their cinematic interpretations were adolescent parody.
English-language spy films of the 2000s include "The Bourne Identity" (2002), "" (1996); "Munich" (2005), "Syriana" (2005), "The Constant Gardener" (2005) and "Casino Royale" (2006), a relaunching of the James Bond series.
Among the comedy films focusing on espionage are 1974's "S*P*Y*S" and 1985's "Spies Like Us".
In March 2015, filming of Howard Kaplan's best selling "The Damascus Cover" wrapped in Casablanca starring Jonathan Rhys Meyers, John Hurt, Jurgen Prochnow and Olivia Thirlby. It is set in Damascus and Jerusalem circa 1989 at the time of the Berlin Wall falling.
Television.
The American adaptation of "Casino Royale" (1954) featured Jimmy Bond in an episode of the "Climax!" anthology series. The narrative tone of television espionage ranged from the drama of "Danger Man" (1960–68) to the sardonicism of "The Man from U.N.C.L.E" (1964–68) and the flippancy of "I Spy" (1965–68) until the exaggeration, akin to that of William Le Queux and E. Phillips Oppenheim before the First World War (1914–18), degenerated to the parody of "Get Smart" (1965–70).
In 1973, Semyonov's novel "Seventeen Moments of Spring" (1968) was adapted to television as a twelve-part mini-series about the Soviet spy Maksim Isaev operating in wartime Nazi Germany as Max Otto von Stierlitz, charged with preventing a separate peace between Nazi Germany and America which would exclude the USSR. The programme "TASS Is Authorized to Declare..." also derives from his work.
However, the circle closed in the late 1970s when "The Sandbaggers" (1978–80) presented the grit and bureaucracy of espionage.
In the 1980s, US television featured the light espionage programmes "Airwolf" (1984–87) and "MacGyver" (1985–92), each rooted in the Cold War yet reflecting American citizens' distrust of their government, after the crimes of the Nixon Government (the internal, political espionage of the Watergate Scandal and the Vietnam War) were exposed. The spy heroes were independent of government; MacGyver, in later episodes and post-DXS employment, works for a non-profit, private think tank, and aviator Hawke and two friends work free-lance adventures. Although each series features an intelligence agency, the DXS in "MacGyver", and the FIRM, in "Airwolf", its agents could alternately serve as adversaries as well as allies for the heroes.
Television espionage programmes of the late 1990s to the early 2010s include "La Femme Nikita" (1997–2001), "Alias" (2001–2006), "24" (2001-2010, 2014), "Spooks" in the UK (release as "MI-5" in the USA and Canada) (2002-2011), NBC's "Chuck" (2007-2012), FX's "Archer" (2009–present), "Burn Notice", "Covert Affairs" and "Homeland".
For children and adolescents.
In every medium, spy thrillers introduce children and adolescents to deception and espionage at earlier ages. The genre ranges from action adventure, such as Chris Ryan's "Alpha Force" series, through the historical espionage dramas of Y. S. Lee, to the girl orientation of Ally Carter's "Gallagher Girls" series, beginning with "I'd Tell You I Love You, But Then I'd Have to Kill You".
Leading examples include the "Agent Cody Banks" film, the Alex Rider adventure novels by Anthony Horowitz, and the CHERUB series, by Robert Muchamore. Ben Allsop, one of England's youngest novelists, also writes spy fiction. His titles include "Sharp" and "The Perfect Kill".
Other authors writing for adolescents include A. J. Butcher, Joe Craig, Charlie Higson, Andy McNab and Francine Pascal.
Video games and theme parks.
In contemporary digital video games, the player can be a vicarious spy, as in the "Metal Gear series", especially in the series' third installment, "Metal Gear Solid", unlike the games of the Third-Person Shooter genre, "Syphon Filter", and "Splinter Cell". The games feature complex stories and cinematic images. Games such as "No One Lives Forever" and the sequel "No One Lives Forever 2: A Spy in H.A.R.M.'s Way" humorously combine espionage and 1960s design. "Evil Genius (game)", contemporary to NOLF series, allows the player to be the villain and its strategy occurs real time.
The "Spyland" espionage theme park, in the Gran Scala pleasure dome, in Zaragoza province, Spain, opened in 2012.
References.
</dl>

</doc>
<doc id="27647" url="http://en.wikipedia.org/wiki?curid=27647" title="Star height problem">
Star height problem

The star height problem in formal language theory is the question whether all regular languages can be expressed using regular expressions of limited star height, i.e. with a limited nesting depth of Kleene stars. Specifically, is a nesting depth of one always sufficient? If not, is there an algorithm to determine how many are required? The problem was raised by . 
Families of regular languages with unbounded star height.
The first question was answered in the negative when in 1963, Eggan gave examples of regular languages of star height "n" for every "n". Here, the star height "h"("L") of a regular language "L" is defined as the minimum star height among all regular expressions representing "L". The first few languages found by are described in the following, by means of giving a regular expression for each language:
The construction principle for these expressions is that expression formula_2 is obtained by concatenating two copies of formula_3, appropriately renaming the letters of the second copy using fresh alphabet symbols, concatenating the result with another fresh alphabet symbol, and then by surrounding the resulting expression with a Kleene star. The remaining, more difficult part, is to prove that for formula_3 there is no equivalent regular expression of star height less than "n"; a proof is given in .
However, Eggan's examples use a large alphabet, of size 2"n"-1 for the language with star height "n". He thus asked whether we can also find examples over binary alphabets. This was proved to be true shortly afterwards by . 
Their examples can be described by an inductively defined family of regular expressions over the binary alphabet formula_5 as follows–cf. :
Again, a rigorous proof is needed for the fact that formula_3 does not admit an equivalent regular expression of lower star height. Proofs are given by and by .
Computing the star height of regular languages.
In contrast, the second question turned out to be much more difficult, and the question became a famous open problem in formal language theory for over two decades . For years, there was only little progress. The pure-group languages were the first interesting family of regular languages for which the star height problem was proved to be decidable . But the general problem remained open for more than 25 years until it was settled by Hashiguchi, who in 1988 published an algorithm to determine the star height of any regular language. The algorithm wasn't at all practical, being of non-elementary complexity. To illustrate the immense resource consumptions of that algorithm, Lombardy and Sakarovitch (2002) give some actual numbers:
\right)^{\left(10^{10^{10}}\right)^{\left(10^{10^{10}}\right)}}.</math>
Notice that alone the number formula_8 has 10 billion zeros when written down in decimal notation, and is already "by far" larger than the number of atoms in the observable universe. 
A much more efficient algorithm than Hashiguchi's procedure was devised by Kirsten in 2005. This algorithm runs, for a given nondeterministic finite automaton as input, within double-exponential space. Yet the resource requirements of this algorithm still greatly exceed the margins of what is considered practically feasible.

</doc>
<doc id="27648" url="http://en.wikipedia.org/wiki?curid=27648" title="William Crookes">
William Crookes

Sir William Crookes, OM, FRS (17 June 1832 – 4 April 1919) was a British chemist and physicist who attended the Royal College of Chemistry, London, and worked on spectroscopy. He was a pioneer of vacuum tubes, inventing the Crookes tube which was made in 1875. Crookes was the inventor of the Crookes radiometer, which today is made and sold as a novelty item.
Biography.
Crookes made a career of being a meteorologist and lecturer at multiple places. Crookes worked in chemistry and physics. His experiments were notable for the originality of their design. He executed them skillfully. His interests, ranging over pure and applied science, economic and practical problems, and psychiatric research, made him a well-known personality. He received many public and academic honors. Crookes's life was one of unbroken scientific activity.
Early years.
William Crookes (later Sir William Crookes) was born in London, the eldest of 16 siblings. His father, Joseph Crookes, was a tailor of north-country origin, at that time living with his second wife, Mary Scott Lewis Rutherford Johnson.
From 1850 to 1854 he filled the position of assistant in the college, and soon embarked upon original work. It wasn't in organic chemistry which the focus of his teacher, August Wilhelm von Hofmann, might have been expected to lead him towards, but into new compounds of selenium. These were the subject of his first published papers, 1851. He worked at the department at the Radcliffe Observatory in Oxford in 1854. In 1855 he was appointed lecturer in chemistry at the Chester Diocesan Training College. In 1856 he married Ellen, daughter of William Humphrey of Darlington. They had three sons and a daughter. Married and living in London, he was devoted mainly to independent work. In 1859, he founded the "Chemical News", a science magazine which he edited for many years and conducted on much less formal lines than was usual for the journals of scientific societies.
Middle years.
In 1861, Crookes discovered a previously unknown element with a bright green emission line in its spectrum and named the element thallium, from the Greek "thallos", a green shoot. Crookes wrote a standard treatise on "Select Methods in Chemical Analysis" in 1871. Crookes was effective in experimentation. The method of spectral analysis, introduced by Bunsen and Kirchhoff, was received by Crookes with great enthusiasm and to great effect. His first important discovery was that of the element thallium, announced in 1861, and made with the help of spectroscopy. By this work his reputation became firmly established, and he was elected a fellow of the "Royal Society" in 1863.
He developed the Crookes tubes, investigating cathode rays. He published numerous papers on spectroscopy and conducted research on a variety of minor subjects. In his investigations of the conduction of electricity in low pressure gases, he discovered that as the pressure was lowered, the negative electrode (cathode) appeared to emit rays (the so-called "cathode rays", now known to be a stream of free electrons, and used in cathode ray display devices). As these examples indicate, he was a pioneer in the construction and use of vacuum tubes for the study of physical phenomena. He was, as a consequence, one of the first scientists to investigate what is now called a plasma and identified it as the fourth state of matter in 1879. He also devised one of the first instruments for the studying nuclear radioactivity, the spinthariscope.
Crookes investigated the properties of cathode rays, showing that they travel in straight lines, cause fluorescence when they fall on some substances, and that their impact can produce great heat. He believed that he had discovered a fourth state of matter, which he called "radiant matter", but his theoretical views on the nature of "radiant matter" were to be superseded. He believed the rays to consist of streams of particles of ordinary molecular magnitude. It remained for Sir J. J. Thomson to expound on the subatomic nature of cathode rays (consisting of streams of negative electrons). Nevertheless, Crookes's experimental work in this field was the foundation of discoveries which eventually changed the whole of chemistry and physics.
Crookes' attention had been attracted to the vacuum balance in the course of his research into thallium. He soon discovered the phenomenon which drives the movement in a Crookes radiometer, in which a set of vanes, each blackened on one side and polished on the other, rotate when exposed to radiant energy. Crookes did not, however, provide the true explanation of this apparent "attraction and repulsion resulting from radiation".
After 1880, he lived at 7 Kensington Park Gardens where all his later work was done, in his private laboratory.
Later years.
Crookes identified the first known sample of helium, in 1895. Crookes was knighted in 1897.
In 1903, Crookes turned his attention to the newly discovered phenomenon of radioactivity, achieving the separation from uranium of its active transformation product, "uranium-X" (later established to be protactinium). Crookes observed the gradual decay of the separated transformation product, and the simultaneous reproduction of a fresh supply in the original uranium. At about the same time as this important discovery, he observed that when "p-particles", ejected from radio-active substances, impinge upon zinc sulfide, each impact is accompanied by a minute scintillation, an observation which forms the basis of one of the most useful methods in the technique of radioactivity.
Crookes wrote a small book on diamonds in 1909. In 1910, Crookes received the Order of Merit. He died in London on 4 April 1919, two years after his wife. He is buried in London's Brompton Cemetery.
Spiritualism.
Crookes became interested in spiritualism in the late 1860s. In this he was possibly influenced by the untimely death of his younger brother Philip in 1867 at age 21 from yellow fever contracted while on an expedition to lay a telegraph cable from Cuba to Florida. In 1867, Crookes from the influence of Cromwell Fleetwood Varley attended a séance to try and get in touch with his brother.
Between 1871 and 1874, Crookes studied the mediums Kate Fox, Florence Cook, and Daniel Dunglas Home. After his investigation he believed that the mediums could produce genuine paranormal phenomena and communicate with spirits. Psychologists Leonard Zusne and Warren H. Jones have described Crookes as gullible as he endorsed fraudulent mediums as genuine.
Edward Clodd claimed Crookes had a poor eyesight that may have explained his belief in spiritualist phenomena and quoted William Ramsay as saying Crookes is "so shortsighted that, despite his unquestioned honesty, he cannot be trusted in what he tells you he has seen." Biographer William Hodson Brock noted that Crookes was "evidently short-sighted, but did not wear spectacles until the 1890s. Until then he may have used a monocle or pocket magnifying glass when necessary. What limitations this imposed upon his psychic investigations we can only imagine."
After studying the reports of Florence Cook the science historian Sherrie Lynne Lyons wrote that the alleged spirit "Katie King" was Cook herself and at other times an accomplice. Regarding Crookes, Lyons wrote "Here was a man with a flawless scientific reputation, who discovered a new element, but could not detect a real live maiden who was masquerading as a ghost." Cook was repeatedly exposed as a fraudulent medium but she had been "trained in the arts of the séance" which managed to trick Crookes. Some researchers such as Trevor H. Hall suspected that Crookes had an affair with Cook.
In a series of experiments in London at the house of Crookes in February 1875, the medium Anna Eva Fay managed to fool Crookes into believing she had genuine psychic powers. Fay later confessed to her fraud and revealed the tricks she had used. Regarding Crookes and his experiments with mediums, the magician Harry Houdini suggested that Crookes had been deceived. The physicist Victor Stenger wrote that the experiments were poorly controlled and "his desire to believe blinded him to the chicanery of his psychic subjects."
In his quest for proof of psychic phenomena, Cookes created a physicist's experiment to reveal a "hitherto undetected force" involving a vessel filled with water sitting on a balance beam. This led to correspondence with William Carpenter and Charles Wheatstone, among others, which Crookes published, in an attempt to explain away the objections of the other scientists... with some almost farcical elements arising along the way: "You must allow me to protest against (my experiments) being ignored" (Crookes to Wheatstone, March 27, 1872, reproduced in the pamphlet published by Crookes). "Dr W.B. Carpenter...(spoke of) an experiment... It was not my experiment, but an unjustifiable misrepresentation of it". (Preface of pamphlet.)
In 1906, William Hope tricked Crookes with a fake spirit photograph of his wife. Oliver Lodge revealed there had been obvious signs of double exposure, the picture of Lady Crookes had been copied from a wedding anniversary photograph, however, Crookes was a convinced spiritualist and claimed it was genuine evidence for spirit photography.
Crookes joined the Society for Psychical Research, becoming its president in the 1890s: he also joined the Theosophical Society and the Ghost Club, of which he was president from 1907 to 1912. In 1890 he was initiated into the Hermetic Order of the Golden Dawn.

</doc>
<doc id="27650" url="http://en.wikipedia.org/wiki?curid=27650" title="September 16">
September 16

September 16 is the day of the year in the Gregorian calendar.

</doc>
<doc id="27651" url="http://en.wikipedia.org/wiki?curid=27651" title="September 23">
September 23

September 23 is the day of the year in the Gregorian calendar. It is frequently the day of the autumnal equinox in the Northern Hemisphere and the day of the vernal equinox in the Southern Hemisphere.

</doc>
<doc id="27655" url="http://en.wikipedia.org/wiki?curid=27655" title="Sonny Bono">
Sonny Bono

Salvatore Phillip "Sonny" Bono (; February 16, 1935 – January 5, 1998) was an American recording artist and producer, who came to fame in partnership with his second wife Cher, as the popular singing duo Sonny & Cher. He was also mayor of Palm Springs, California, from 1988 to 1992, and congressman for California's 44th district from 1994 until his death in a skiing accident while still in office.
Early life.
Bono was born in Detroit, to Santo Bono (born in Montelepre, Palermo, Italy) and Zena "Jean" (née La Valle). His mother gave him the nickname "Sonny", which lasted his lifetime. Sonny was the youngest of three siblings; he had two older sisters, Fran and Betty. The family moved to Inglewood, California when he was seven. He attended Inglewood High School, but did not graduate.
Career.
Entertainment career.
Bono began his music career working at Specialty Records where his song "Things You Do to Me" was recorded by Sam Cooke, and went on to work for the record producer Phil Spector in the early 1960s as a promotion man, percussionist and "gofer". One of his earliest songwriting efforts was "Needles and Pins" which he co-wrote with Jack Nitzsche, another member of Spector's production team. Later in the same decade, he achieved commercial success, along with his then-wife Cher, as part of the singing duo Sonny and Cher. Bono wrote, arranged, and produced a number of hit records with singles like "I Got You Babe" and "The Beat Goes On", although Cher received more attention as a performer. He also played a major part in Cher's early solo career with recordings such as "Bang Bang" and "You Better Sit Down Kids".
Bono co-wrote the song "She Said Yeah", which was covered by The Rolling Stones on their 1965 LP "December's Children" Bono also recorded as a solo artist under the name of Sonny. He had only one hit single as a solo artist, "Laugh at Me". "Laugh at Me" was released in 1965 and peaked at number 10 on the "Billboard" Hot 100. In live concerts, Bono would sing the song with an introduction of, "I'd like to sing a medley of my hit." His only other single as a solo artist was a follow-up release, "The Revolution Kind", which reached number 70 on the "Billboard" Hot 100 later that same year. Bono also recorded an unsuccessful Sonny album titled "Inner Views" in 1967.
Sonny continued to work with Cher through the early and mid-1970s starring in a popular television variety show, "The Sonny and Cher Show," which ran on CBS from 1971 to 1974. From 1976 to 1977, the couple returned to performing together on "The Sonny and Cher Show" despite being divorced. Their last appearance together was on "Late Night with David Letterman" on November 13, 1987, when they sang "I Got You Babe".
Bono continued his acting career, doing bit roles in such shows as "Fantasy Island" and "The Love Boat". He played the part of mad bomber Joe Selucci in ' (1982) and in the 1986 horror movie "Troll". Bono also played the part of Franklin Von Tussle in the 1988 John Waters film "Hairspray". In the 1997 film "Men in Black", Bono is one of several oddball celebrities seen on a wall of video screens that monitor extraterrestrials living among us. He also appeared in several episodes of "P.S. I Luv U" starring Connie Sellecca and Greg Evigan during the 1991–92 TV season as the Mayor of Palm Springs [which he really was at the time]. His last acting role was in the television series ' (Season 1, Episode 9, originally aired on November 21, 1993), in which he played the Mayor Frank Berkowitz. He also made a minor appearance as himself in the 1996 film "First Kid".
Sonny poked a little fun at himself when he guest-starred on "The Golden Girls", in the episode "Mrs. George Devereaux", aired November 17, 1990, as himself vying with Lyle Waggoner for Dorothy's (Bea Arthur) affection in a dream, where Blanche (Rue McClanahan) dreams her husband is still alive. In the dream, Sonny uses his power as mayor of Palm Springs, California to have Waggoner falsely arrested just so he can have Dorothy to himself. Later on, after Blanche awakens from the dream, Dorothy is thrilled to learn she picked Sonny this time.
Political career.
Bono entered politics after experiencing great frustration with local government bureaucracy in trying to open a restaurant in Palm Springs, California. Bono placed a successful bid to become the new mayor of Palm Springs. He served four years (1988 to 1992). He was instrumental in spearheading the creation of the Palm Springs International Film Festival, which is held each year in Bono's memory.
Bono ran for the Republican nomination for United States Senate in 1992, but the nomination went to the more conservative Bruce Herschensohn, and the election to the Democrat Barbara Boxer. Bono and Herschensohn became close friends after the campaign. Bono was elected to the United States House of Representatives in 1994 to represent California's 44th congressional district. He was one of twelve co-sponsors of a House bill extending copyright. Although that bill was never voted on in the Senate, a similar Senate bill was passed after his death and named the Sonny Bono Copyright Term Extension Act in his memory.
He championed the restoration of the Salton Sea, bringing the giant lake's plight to national attention. Speaker of the House Newt Gingrich made a public appearance and speech at the shore of the lake on Bono's behalf.
In their book "Tell Newt to Shut Up", David Maraniss and Michael Weisskopf credit Bono with being the first person to recognize Gingrich's public relations problems in 1995. Drawing on his long experience as a celebrity and entertainment producer, Bono (according to Maraniss and Weisskopf) recognized that Gingrich's status had changed from politician to celebrity, and that Gingrich was not making allowances for that change:
You're a celebrity now, ... The rules are different for celebrities. I know it. I've been there. I've been a celebrity. I used to be a bigger celebrity. But let me tell you, you're not being handled right. This is not political news coverage. This is celebrity status. You need handlers. You need to understand what you're doing. You need to understand the attitude of the media toward celebrities.
Bono remains the only member of Congress to have scored a number one pop single on the US "Billboard" Hot 100 chart.
Personal life.
Bono married his first wife, Donna Rankin, on November 3, 1954. Their daughter Christine ("Christy") was born on June 24, 1958. They divorced in 1962. In 1964, Bono married singer and actress Cher. They had one child, Chastity (later Chaz Bono), born March 4, 1969. In 1975 the duo divorced. Bono then married Susie Coelho, but divorced her in 1984. He wed the much-younger Mary Whitaker in 1986 and they had two children, son Chesare Elan on April 25, 1988 and daughter Chianna Maria on February 2, 1991. He became interested in Scientology partly because of the influence of Mimi Rogers, but stated that he was a Roman Catholic on all official documents, campaign materials, web sites, etc. His wife Mary also took Scientology courses.
Bono was named a godparent of Anthony Kiedis who would go on to become a musical artist with his band, Red Hot Chili Peppers. Sonny was a close friend of Anthony's father, Blackie Dammett, and would often take the boy on weekend trips.
Bono was a champion of the Salton Sea in southeastern California, where a park was named in his honor. The 2005 documentary film "Plagues & Pleasures on the Salton Sea" (narrated by John Waters) features Bono and documented the lives of the inhabitants of Bombay Beach, Niland, and Salton City, as well as the ecological issues associated with the Sea.
In 1996, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.
Death.
Bono died on January 5, 1998, of injuries sustained when he hit a tree while skiing on the Nevada side of Heavenly Ski Resort near South Lake Tahoe, California. His death came just a little less than a week after Michael Kennedy, a son of Robert F. Kennedy, died in a similar skiing accident in Aspen, Colorado. After Bono's death, Mary Bono told an interviewer from "TV Guide" that Sonny had been addicted to prescription drugs, mainly Vicodin and Valium. Though Mary claimed that Sonny's drug use caused the accident, the autopsy performed by the Douglas County Coroner showed no indication of any substances or alcohol. Sonny's mother and several friends disputed Mary's account.
Bono was survived by his wife, Mary Bono, and children, Christy, Chianna, Chesare and Chastity (later Chaz). His mother, Jean Bono, also survived him, and died on January 15, 2005, at the age of 90. At Mary Bono's request, Cher gave a eulogy at Sonny's funeral. His remains were buried at Desert Memorial Park in Cathedral City, California. The epitaph on Bono's headstone reads: "And the Beat Goes On", written in uppercase.
Mary Bono was elected to fill the remainder of his Congressional term. She was elected in her own right seven more times and was defeated for re-election in 2012.

</doc>
<doc id="27656" url="http://en.wikipedia.org/wiki?curid=27656" title="Single market">
Single market

A common market is usually referred to as the first stage towards the creation of a single market. It usually is built upon a free trade area with relatively free movement of capital and of services, but not so advanced in reduction of the rest of the trade barriers.
A single market is a type of trade bloc in which most trade barriers have been removed (for goods) with some common policies on product regulation, and freedom of movement of the factors of production (capital and labour) and of enterprise and services. The goal is that the movement of capital, labour, goods, and services between the members is as easy as within them. The physical (borders), technical (standards) and fiscal (taxes) barriers among the member states are removed to the maximum extent possible. These barriers obstruct the freedom of movement of the four factors of production.
A unified market is the last stage and ultimate goal of a single market. It requires the total free movement of goods, services (including financial services), capital and people without regard to national boundaries.
Integration phases.
A common market allows for the free movement of capital and services but large amounts of trade barriers remain. It eliminates all quotas and “tariffs” – duties on imported goods – from trade in goods within it. However “non-tariff barriers” remain such as differences between the Member States’ safety, packaging requirements and national administrative procedures. They prevent for example manufacturers from marketing the same goods in all member states. The objective of a common market is most often economic convergence and the creation of an integrated single market. It is sometimes considered as the first stage of a single market. The European Economic Community was the first example of a common market. It was an economic union since it had a customs union in addition to its internal market. Today, existing common markets include Mercosur and ASEAN, which plans to create a common market by December 2015.
A single market (sometimes called 'internal market') allows for people, goods, services and capital to move around a union as freely as they do within a single country – instead of being obstructed by national borders and barriers as they were in the past. Citizens can study, live, shop, work and retire in any member state. Consumers enjoy a vast array of products from all member states and businesses have unrestricted access to more consumers. A single market is commonly described as "frontier-free". However, several barriers remain such as differences in national tax systems, differences in parts of the services sector and different requirements for e-commerce. In addition separate national markets still exist for financial services, energy and transport. Laws concerning the recognition of professional qualifications also may not be fully harmonized.
The Eurasian Economic Union, the Gulf Cooperation Council, CARICOM and the European Union are current examples of single markets. Although the GCC's single market has been described as "malfunctioning" in 2014. The European Union is the only economic union whose objective is "completing the single market."
A completed, unified market usually refers to the complete removal of barriers and integration of the remaining national markets. Complete economic integration can be seen within many countries, whether in a single unitary state with a single set of economic rules, or among the members of a strong national federation. For example, the sovereign states of the United States do to some degree have different local economic regulations (e.g. licensing requirements for professionals, rules and pricing for utilities and insurance, consumer safety laws, environmental laws, minimum wage) and taxes, but are subordinate to the federal government on any matter of interstate commerce the national government chooses to assert itself. Movement of people and goods among the states is unrestricted and without tariffs.
Benefits and costs.
A single market has many benefits. With full freedom of movement for all the factors of production between the member countries, the factors of production become more efficiently allocated, further increasing productivity.
For both business within the market and consumers, a single market is a very competitive environment, making the existence of monopolies more difficult. This means that inefficient companies will suffer a loss of market share and may have to close down. However, efficient firms can benefit from economies of scale, increased competitiveness and lower costs, as well as expecting profitability to increase as a result. Consumers are benefited by the single market in the sense that the competitive environment brings them cheaper products, more efficient providers of products and also increased choice of products. What is more, businesses in competition will innovate to create new products; another benefit for consumers.
Transition to a single market can have a negative impact on some sectors of a national economy due to increased international competition. Enterprises that previously enjoyed national market protection and national subsidy (and could therefore continue in business despite falling short of international performance benchmarks) may struggle to survive against their more efficient peers, even for its traditional markets. Ultimately, if the enterprise fails to improve its organization and methods, it will fail. The consequence may be unemployment or migration.
List of single markets.
Every economic union and economic and monetary union includes a common market, but these are not listed below.
Additionally the autonomous and dependent territories, such as some of the EU member state special territories, are sometimes treated as separate customs territory from their mainland state or have varying arrangements of formal or de facto customs union, common market and currency union (or combinations thereof) with the mainland and in regards to third countries through the trade pacts signed by the mainland state.
Unified market.
A unified market is the economic term for a single market where goods, services, capital and people can move freely without regard to national boundaries. These "four freedoms" are implemented by, among other things, removal of tariffs on the transfer of goods and services among the member nations, imposition of uniform product standards, revision of laws to permit "market-wide" financial services, and the restructuring of most government procurement practices, so as not to favour local businesses over other member states' businesses.

</doc>
<doc id="27658" url="http://en.wikipedia.org/wiki?curid=27658" title="Special administrative region">
Special administrative region

The Special Administrative Regions of the People's Republic of China (SAR; Portuguese: RAE) are autonomous territories that fall within the sovereignty of the People's Republic of China, yet do not form part of Mainland China. The legal basis for the establishment of SARs, unlike the administrative divisions of Mainland China, is provided for by Article 31, rather than Article 30, of the Constitution of the People's Republic of China of 1982. Article 31 reads: "The state may establish special administrative regions when necessary. The systems to be instituted in special administrative regions shall be prescribed by law enacted by the National People's Congress in the light of the specific conditions".
At present, there are two SARs, namely the Hong Kong SAR and the Macau SAR, former British and Portuguese dependencies respectively, transferred to China in 1997 and 1999 pursuant to the Sino-British Joint Declaration of 1984 and the Sino-Portuguese Joint Declaration of 1987 respectively. Pursuant to their Joint Declarations, which are binding inter-state treaties registered with the United Nations, and their Basic laws, the Chinese SARs "shall enjoy a high degree of autonomy."
The provision to establish special administrative regions appeared in the constitution in 1982, in anticipation of the talks with the United Kingdom over the question of the sovereignty over Hong Kong. It was envisioned as the model for the eventual reunification with Taiwan and other islands, where the Republic of China has resided since 1949. Special administrative regions should not be confused with special economic zones, which are areas in which special economic laws apply to promote trade and investments.
Under the One China, Two Systems principle, the two SARs continue to possess their own governments, multi-party legislatures, legal systems, police forces, monetary systems, separate customs territory, immigration policies, national sports teams, official languages, postal systems, academic and educational systems, and substantial competence in external relations that are different or independent from the People's Republic of China.
Characteristics.
The two special administrative regions of Hong Kong and Macau (created in 1997 and 1999 respectively) each have a codified constitution called Basic Law. The law provides the regions with a high degree of autonomy, a separate political system, and a capitalist economy under the principle of "one country, two systems" proposed by Deng Xiaoping.
High degree of autonomy.
Currently, the two SARs of Hong Kong and Macau are responsible for all affairs except those regarding diplomatic relations and national defense. Consequently, the National People's Congress authorizes the SAR to exercise a high degree of autonomy and enjoy executive, legislative and independent judicial power, and each with their own Courts of Final Appeal.
External affairs.
Special administrative regions are empowered to contract a wide range of agreements with other countries and territories such as mutual abolition of visa requirement, mutual legal aid, air services, extradition, handling of double taxation and others. In diplomatic talks involving a SAR, the SAR concerned may send officials to be part of the Chinese delegation. In sporting events the SARs participate under the respective names of "Hong Kong, China" and "Macau, China", and compete as different entities.
Defense and military.
The People's Liberation Army is garrisoned in both SARs. PRC authorities have said the PLA will not be allowed to interfere with the local affairs of Hong Kong and Macau, and must abide by its laws. In 1988, scholar Chen Fang of the Academy of Military Science even tried to propose the "One military, two systems" concept to separate the defence function and public functions in the army. The PLA does not participate in the governance of the SAR but the SAR may request them for civil-military participation, in times of emergency such as natural disasters. Defence is the responsibility of the PRC government.
A 1996 draft PRC law banned People's Liberation Army-run businesses in HK, but loopholes allow them to operate while the profits are ploughed back into the military. There are many PLA-run corporations in Hong Kong. The PLA also have sizable land-holdings in Hong Kong worth billions of dollars.
Immigration and nationality.
Each of the SARs issues passports on its own to its permanent residents who are concurrently nationals of the PRC. PRC nationals must also satisfy one of the following conditions:
Apart from affording the holder consular protection by the People's Republic of China, these passports also specify that the holder has right of abode in the issuing SAR.
The National People's Congress has also put each SAR in charge of administering the PRC's Nationality Law in its respective realms, namely naturalization, renunciation and restoration of PRC nationality and issuance of proof of nationality.
Due to their colonial past, many inhabitants of the SARs hold some form of non-Chinese nationality (e.g. British National (Overseas) status, United Kingdom citizenship or Portuguese citizenship), however residents of Chinese descent have always been considered as Chinese citizens by the PRC. Special interpretation of the Nationality Law, while not recognizing dual nationality, has allowed Chinese citizens to keep their foreign "right of abode" and use travel documents issued by the foreign country. However, such travel documents cannot be used to travel to mainland China and the holder may not enjoy consular protection while in mainland China. Chinese citizens who also have foreign citizenship may declare a change of nationality at the Immigration Department of the respective SARs, and upon approval, would no longer be considered Chinese citizens.
Offer to Taiwan and other ROC-controlled areas.
The status of a special administrative region for Taiwan and other areas controlled by the Republic of China was first proposed in 1981. The 1981 proposal was put forth by Ye Jianying called "Ye's nine points" (葉九條). A series of different offers have since appeared. On 25 June 1983 Deng Xiaoping appeared at Seton Hall University in the US to propose "Deng's six points" (鄧六條), which called for a "Taiwan Special Administrative Region" (台灣特別行政區). It was envisioned that after Taiwan's unification with the PRC as an SAR, the PRC would become the sole representative of China. Under this proposal, Taiwan would be guaranteed its own military, its own administrative and legislative powers, an independent judiciary and the right of adjudication, although it would not be considered a separate government of China. While there would be minimal interference by the PRC in Taiwan's political system, there may be representatives from the Taiwan SAR that would be appointed to the central government in Beijing by the Taiwan SAR.
In 2005 the Anti-Secession Law of the PRC was enacted. It promises the lands currently ruled by the authorities of Taiwan a high degree of autonomy, among other things. The PRC can also employ non-peaceful means and other necessary measures to defend its claims to sovereignty over the ROC's territories from Taiwanese independence activists.
See also.
Listen to this article ()
This audio file was created from a revision of the "Special administrative region" article dated 2006-07-23, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="27660" url="http://en.wikipedia.org/wiki?curid=27660" title="Seattle Mariners">
Seattle Mariners

The Seattle Mariners are an American professional baseball team based in Seattle, Washington. Enfranchised in 1977, the Mariners are a member of the Western Division of Major League Baseball's American League. Since July 1999, the Mariners' home ballpark has been Safeco Field, located south of downtown Seattle. From the team's inception in 1977 until June 1999, the club's home ballpark was the Kingdome. Through the 2014 season, the franchise has finished with a losing record in 26 of 38 seasons.
The "Mariners" name originates from the prominence of marine culture in the city of Seattle. They are nicknamed "the M's", a title featured in their primary logo from 1987–1992. The current team colors are Navy blue, Northwest green, and Silver, after having been Royal blue and Gold from 1977–1992; on January 23, 2015 the Mariners revealed their new alternative uniforms with the colors of royal blue and gold. Their mascot is the Mariner Moose.
The organization did not field a winning team until 1991, and any real success eluded them until 1995 when they won their first division championship and defeated the New York Yankees in the American League Division Series. The game-winning hit in Game 5, in which Edgar Martínez drove home Ken Griffey, Jr. to win the game in the 11th inning, clinched a series win for the Mariners, and has since become an iconic moment in team history.
The Mariners won 116 games in , which set the American League record for most wins in a single season and tied the 1906 Chicago Cubs for the Major League record for most wins in a single season.
The Mariners are one of eight Major League Baseball teams without a World Series title, and one of two (along with the Washington Nationals) to never have appeared in a World Series.
Owned by Nintendo of America, the Mariners are one of three Major League Baseball teams under corporate ownership; the other two are the Atlanta Braves and the Toronto Blue Jays.
History.
The Mariners were created as a result of a lawsuit. In , in the aftermath of the Seattle Pilots' purchase and relocation to Milwaukee (as the Milwaukee Brewers) by future Commissioner of Baseball Bud Selig, the city of Seattle, King County, and the state of Washington (represented by then-state attorney general and later U.S. Senator Slade Gorton) sued the American League for breach of contract. Confident that Major League Baseball would return to Seattle within a few years, King County built the multi-purpose Kingdome, which would become home to the NFL's expansion Seattle Seahawks in 1976. The name "Mariners" was chosen by club officials in August 1976 from over 600 names submitted by 15,000 entrants in a name-the-team contest.
The Mariners played their first game on April 6, 1977, to a sold-out crowd of 57,762 at the Kingdome, losing 7–0 to the California Angels. The first home run in team history was hit on April 10, 1977, by designated hitter Juan Bernhardt.
That year, star pitcher Diego Seguí, in his last major league season, became the only player to play for both the Pilots and the Mariners. The Mariners finished with a 64–98 record, echoing the record the 1969 Pilots once held. In 1979, Seattle hosted the 50th Major League Baseball All-Star Game. After the 1981 season, the Mariners were sold to California businessman and future U.S. Ambassador to Spain George Argyros.
During the 1992–93 offseason, the Mariners hired manager Lou Piniella, who had led the Cincinnati Reds to victory in the 1990 World Series. Mariner fans embraced Piniella, and he would helm the team from 1993 through 2002, winning two American League Manager of the Year Awards along the way.
The Mariners club finished with a record of 116-46, leading all of Major League Baseball in winning percentage for the duration of the season and easily winning the American League West division championship. In doing so, the team broke the 1998 Yankees American League single-season record of 114 wins and matched the all-time MLB single-season record for wins set by the 1906 Chicago Cubs. At the end of the season, Ichiro Suzuki won the AL MVP, AL Rookie of the Year, and one of three outfield Gold Glove Awards, becoming the first player since the 1975 Boston Red Sox's Fred Lynn to win all three in the same season.
On October 22, the Mariners announced the hiring of Jack Zduriencik, formerly scouting director of the Milwaukee Brewers, as their general manager. Weeks later, on November 18, the team named Oakland Athletics bench coach Don Wakamatsu as its new field manager. Wakamatsu and Zduriencik hired an entirely new coaching staff for 2009, which included former World Series MVP John Wetteland as bullpen coach. The off-season also saw a litany of roster moves, headlined by a 12-player, 3-team trade that included sending All-Star closer J. J. Putz to the New York Mets and brought 5 players—including prospect Mike Carp and outfielder Endy Chávez from New York and outfielder Franklin Gutierrez from the Cleveland Indians—to Seattle. Many of the moves, like the free agent signing of Mike Sweeney, were made in part with the hope of squelching the clubhouse infighting that plagued the Mariners in 2008. It also saw the return of Seattle favorite Ken Griffey, Jr. The 2009–10 offseason was highlighted by the trade for 2008 American League Cy Young Award winner Cliff Lee from the Philadelphia Phillies, the signing of third baseman Chone Figgins and the contract extension of star pitcher "King" Félix Hernández.
On June 2, 2010 Ken Griffey, Jr. announced his retirement after 22 MLB seasons.
On August 9, 2010 the Mariners fired field manager Don Wakamatsu along with bench coach Ty Van Burkleo, pitching coach Rick Adair and performance coach Steve Hecht. Daren Brown, the manager of the AAA affiliate Tacoma Rainiers, took over as interim field manager. Roger Hansen, the former Minor League catching coordinator, was promoted to bench coach. Carl Willis, the former Minor League pitching coordinator, was promoted to pitching coach.
The Mariners hired former Cleveland Indians manager Eric Wedge as their new manager on October 19, 2010.
On November 10, 2010, Dave Niehaus, the Mariners' play-by-play announcer since the team's inception, died of a heart attack at the age of 75. In memory of Niehaus, Seattle rapper Macklemore wrote a tribute song called "My Oh My" in December 2010. He performed the song at the Mariners' Opening Day game on April 8, .
On April 21, 2012, Philip Humber of the Chicago White Sox threw the third perfect game in Chicago White Sox history against the Mariners at Safeco Field in Seattle. It was the 21st perfect game in MLB history. On June 8, 2012, the Mariners starting pitcher Kevin Millwood and five other pitchers combined to throw the tenth combined no-hitter in MLB history and the first in team history. The last combined one occurred in 2003, when six Houston Astros no-hit the New York Yankees in New York. The six pitchers used in a no-hitter is a major league record. On August 15, 2012, Félix Hernández pitched the first perfect game in team history, shutting down the Tampa Bay Rays 1-0 at Safeco Field. It was the 23rd perfect game in Major League Baseball history.
Uniforms.
The Mariners donned their current uniform in (with a slight change in 2015). For home games, they wear white jerseys and pants, except on Fridays, when they don "Northwest Green" teal jerseys, a throwback to their 1990s color scheme. The Friday jerseys were introduced in 2011. The alternate road uniform is dark blue.
In January 2015 the team announced a return to the color of their original 1977 uniform with a new alternative uniform to be worn for Sunday home games only. This uniform is cream-colored with royal blue and gold outlining, and blue stirrups. Numbers without names are on the back of the jerseys. The cap is royal blue with the updated Mariners logo. Additionally, the uniform script on both the primary home and away jerseys were tweaked.
Spring training.
The team mainly plays spring training games in Peoria, Arizona at the Peoria Sports Complex. They share the complex and stadium with the San Diego Padres. On March 25, 2013, in a 16-0 victory over the Cincinnati Reds, the Mariners broke the team record for total home runs during a spring training season with 52.
Season records.
"This is a partial list listing the past 15 completed regular seasons. For the full season records, see here."
Achievements.
Baseball Hall of Famers.
The following elected members of the Baseball Hall of Fame spent part of their careers with the Mariners. None is depicted on his plaque wearing a Mariners cap insignia.
Seattle Mariners Hall of Fame.
Seattle Mariners former chairman and CEO John Ellis announced on June 14, 1997 the creation of a Mariners Hall of Fame. It is operated by the Seattle Mariners organization. It honors the players, staff and other individuals that greatly contributed to the history and success of the Mariners franchise. It is located at the Baseball Museum of the Pacific Northwest in Safeco Field. On January 13, 2015 pitcher Jamie Moyer (1996-2006) will become the ninth Mariner to be inducted into the Mariner Hall of Fame. The induction ceremony will be on August 8, 2015.
The current members of the Mariners Hall of Fame are:
Retired numbers.
The Seattle Mariners have not retired any uniform numbers. Official team policy states that a number may only be retired for a player in the National Baseball Hall of Fame who played for at least five years with the Mariners, or a player who comes close to such election having spent "substantially his entire career with the Mariners."
Despite not officially retiring any numbers, the team has not reissued the numbers 11 (Edgar Martínez), 14 (Lou Piniella), 19 (Jay Buhner) or 24 (Ken Griffey, Jr.) to any uniformed staff since the last player to have worn the number left the team. Under current team policy, Martinez, who played his entire major-league career in Seattle and first appeared on the Hall of Fame ballot in #redirect , is the only player who may be eligible to have his number retired. His best Hall of Fame voting figures came in 2012, when he received 36.5% of the vote (75% is required for induction). Whether this constitutes coming "close" to Hall of Fame election is unclear.
Jackie Robinson's number 42 was retired throughout Major League Baseball on April 15, 1997.
Number 51, worn by Randy Johnson, was withheld from players from 1998 until 2001, when it was issued to Ichiro Suzuki upon his request after wearing it for his entire career in Japan. It has presumably been taken out of circulation again, following Ichiro's 2012 trade to the Yankees coupled with Johnson's 2015 election into the Baseball Hall-of-Fame.
The number 24 was not issued from the time Ken Griffey, Jr. left the team after the season until it was re-issued to him when he returned in and has not been reissued since his retirement in .
Uniform number 00 is presumed off-limits, as it has been worn by the Mariner Moose since 1997 (outfielder Jeffrey Leonard was the last player to don 00 for the M's, in 1990). From 1990 to 1996, the Moose wore the last 2 digits of the year of the current season.
Culture.
Rally Fries.
Rally Fries are a baseball tradition started by Mariners broadcaster Mike Blowers in 2007. During a game against the Cincinnati Reds, a fan tried to catch a foul ball along the right-field line but in turn spilled his tray of french fries along the track. While chatting on the air and seeing the mishap, Blowers' partner, Dave Sims, suggested that he should send a new tray of fries to the fan. Blowers agreed, and sent his intern to deliver a plate of fries to the man.
However, on the next game, fans made signs and boards, asking Blowers for free fries as well. Coincidentally, every time the fries were delivered, the Mariners seem to score or rally from a deficit, and thus the "Rally Fries" were created. This became so popular with the fans that signs were even seen when the Mariners were on the road, though on August 1, 2009, Blowers mentioned he doesn't award winners on the road.
Generally, Blowers will select a person or a group of people that appeals to him, whether it is through fans wearing elaborate costumes or waving funny signs and boards. The fries are usually delivered from Ivar's, a Seattle-based seafood restaurant with a location at Safeco Field. The amount of fries given out varies with the size of the winning group of fans. The winners are generally selected around the 5th or 6th inning, although potential candidates are shown in almost every inning beforehand.
King's Court.
As the 2011 season progressed, the Mariners marketing staff came up with an idea to encourage the growing fanbase of Cy Young-winning pitcher "King" Félix Hernández. Every Hernandez start at Safeco Field is now accompanied by a King's Court section—a place for his fans to sing, dance and cheer while donning custom-made shirts and K cards.
The King's Court is both a personal rooting section for Hernandez and trend-setter for Safeco Field. The team encouraged fans to dress like Larry Bernandez, Hernandez's alter ego from a Mariners TV Commercial, or show up in wacky costumes, rewarding the best with a ceremonial turkey leg.
Supreme Court The Supreme Court is when every game attendee is given a free King's Court T-shirt and a K card, and only occurs at special events. The first Supreme Court was Félix's first home game following his perfect game in 2012. Since then it has occurred each year at Félix's first home game.
Radio and television.
The Mariners' flagship radio station is KIRO-AM 710 (ESPN Radio), which previously broadcast Mariners contests from 1985–2002. Former flagship stations include KOMO 1000 AM (2003–2008), and KVI 570 AM (1977–1984). Television rights are held by Root Sports Northwest. In years past, Mariners games have also appeared in Seattle on over-the-air stations KING-TV, KIRO-TV, KTZZ-TV, and KSTW-TV. Selected Mariners games are also available on Canadian television, due to an agreement between Root Sports Northwest and Rogers Sportsnet.
The Mariners' broadcast team for 2010 featured Dave Niehaus and Rick Rizzs—back for their 32nd and 23rd seasons with the club, respectively—as well as veteran broadcaster Dave Sims and former infielder Mike Blowers. For the first three innings of each game, Niehaus worked the television broadcast with Blowers, and Rizzs and Sims handled radio duties; after the third inning, Niehaus and Sims traded places. Niehaus, who had broadcast for the Mariners since their inaugural season of 1977, died on November 10, 2010. For the 2011 season, Dave Niehaus' duties in the broadcast booth were filled by a collection of former Mariners broadcasters such as Ron Fairly, Ken Levine, and Ken Wilson; and former Mariners' players such as Dave Valle, Dan Wilson, Jay Buhner, and Dave Henderson. Since 2013, Aaron Goldsmith joins Rizzs calling games on the radio, while Sims and Blowers anchor the television broadcasting. Seattle radio personality Matt Pitman hosts the post-game show on the Mariners' radio network, along with clubhouse reporter Shannon Drayer. Spanish-language radio broadcast duties are handled by Alex Rivera on play-by-play and former second baseman Julio Cruz providing color commentary.
Tom Hutyler has been the Mariners' public address announcer since 1987, first at the Kingdome, and presently at Safeco Field. While KOMO 1000 AM was the Mariners' flagship radio station, Hutyler occasionally hosted the post-game radio show.

</doc>
<doc id="27661" url="http://en.wikipedia.org/wiki?curid=27661" title="Source code">
Source code

In computing, source code is any collection of computer instructions (possibly with comments) written using some human-readable computer language, usually as text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by a compiler program into low-level machine code understood by the computer. The machine code might then be stored for execution at a later time. Alternatively, an interpreter can be used to analyze and perform the outcomes of the source code program directly on the fly.
Most computer applications are distributed in a form that includes executable files, but not their source code. If the source code were included, it would be useful to a user, programmer, or system administrator, who may wish to modify the program or to understand how it works.
Aside from its machine-readable forms, source code also appears in books and other media; often in the form of small code snippets, but occasionally complete code bases; a well-known case is the source code of PGP.
Definitions.
The notion of source code may also be taken more broadly, to include machine code and notations in graphical languages, neither of which are textual in nature. An example from an article presented on the annual IEEE conference and on Source Code Analysis and Manipulation:
For the purpose of clarity ‘source code’ is taken to mean any fully executable description of a software system. It is therefore so construed as to include machine code, very high level languages and executable graphical representations of systems.
Organization.
The source code which constitutes a program is usually held in one or more text files stored on a computer's hard disk ; usually these files are carefully arranged into a directory tree, known as a source tree. Source code can also be stored in a database (as is common for stored procedures) or elsewhere.
The source code for a particular piece of software may be contained in a single file or many files. Though the practice is uncommon, a program's source code can be written in different programming languages. For example, a program written primarily in the C programming language, might have portions written in assembly language for optimization purposes. It is also possible for some components of a piece of software to be written and compiled separately, in an arbitrary programming language, and later integrated into the software using a technique called library linking. This is the case in some languages, such as Java: each class is compiled separately into a file and linked by the interpreter at runtime.
Yet another method is to make the main program an interpreter for a programming language, either designed specifically for the application in question or general-purpose, and then write the bulk of the actual user functionality as macros or other forms of add-ins in this language, an approach taken for example by the GNU Emacs text editor.
The code base of a computer programming project is the larger collection of all the source code of all the computer programs which make up the project. It has become common practice to maintain code bases in version control systems.
Moderately complex software customarily requires the compilation or assembly of several, sometimes dozens or even hundreds, of different source code files. In these cases, instructions for compilations, such as a Makefile, are included with the source code. These describe the relationships among the source code files, and contain information about how they are to be compiled.
The revision control system is another tool frequently used by developers for source code maintenance.
Purposes.
Source code is primarily used as input to the process that produces an executable program (i.e., it is compiled or interpreted). It is also used as a method of communicating algorithms between people (e.g., code snippets in books).
Computer Programmers often find it helpful to review existing source code to learn about programming techniques. The sharing of source code between developers is frequently cited as a contributing factor to the maturation of their programming skills. Some people consider source code an expressive artistic medium.
Porting software to other computer platforms is usually prohibitively difficult without source code. Without the source code for a particular piece of software, portability is generally computationally expensive. Possible porting options include binary translation and emulation of the original platform.
Decompilation of an executable program can be used to generate source code, either in assembly code or in a high-level language.
Programmers frequently adapt source code from one piece of software to use in other projects, a concept known as software reusability.
Licensing.
Software, and its accompanying source code, typically falls within one of two licensing paradigms: open source and proprietary software.
Generally speaking, software is "open source" if the source code is free to use, distribute, modify and study, and "proprietary" if the source code is kept secret, or is privately owned and restricted. The first software license to be published and to explicitly grant these freedoms was the GNU General Public License in 1989. The GNU GPL was originally intended to be used with the GNU operating system.
For proprietary software, the provisions of the various copyright laws, trade secrecy and patents are used to keep the source code closed. Additionally, many pieces of retail software come with an end-user license agreement (EULA) which typically prohibits decompilation, reverse engineering, analysis, modification, or circumventing of copy protection. Types of source code protection – beyond traditional compilation to object code – include code encryption, code obfuscation or code morphing.
Legal issues in the United States.
In a 2003 court case in the United States, it was ruled that source code should be considered a constitutionally protected form of free speech. Proponents of free speech argued that because source code conveys information to programmers, is written in a language, and can be used to share humor and other artistic pursuits, it is a protected form of communication.
One of the first court cases regarding the nature of source code as free speech involved University of California mathematics professor Dan Bernstein, who had published on the Internet the source code for an encryption program that he created. At the time, encryption algorithms were classified as munitions by the United States government; exporting encryption to other countries was considered an issue of national security, and had to be approved by the State Department. The Electronic Frontier Foundation sued the U.S. government on Bernstein's behalf; the court ruled that source code was free speech, protected by the First Amendment.
Quality.
The way a program is written can have important consequences for its maintainers. Coding conventions, which stress readability and some language-specific conventions, are aimed at the maintenance of the software source code, which involves debugging and updating. Other priorities, such as the speed of the program's execution, or the ability to compile the program for multiple architectures, often make code readability a less important consideration, since code "quality" generally depends on its "purpose".

</doc>
<doc id="27667" url="http://en.wikipedia.org/wiki?curid=27667" title="Space">
Space

Space is the boundless three-dimensional extent in which objects and events have relative position and direction. Physical space is often conceived in three linear dimensions, although modern physicists usually consider it, with time, to be part of a boundless four-dimensional continuum known as spacetime. In mathematics, "spaces" are examined with different numbers of dimensions and with different underlying structures. The concept of space is considered to be of fundamental importance to an understanding of the physical universe. However, disagreement continues between philosophers over whether it is itself an entity, a relationship between entities, or part of a conceptual framework.
Debates concerning the nature, essence and the mode of existence of space date back to antiquity; namely, to treatises like the "Timaeus" of Plato, or Socrates in his reflections on what the Greeks called "khôra" (i.e. "space"), or in the "Physics" of Aristotle (Book IV, Delta) in the definition of "topos" (i.e. place), or in the later "geometrical conception of place" as "space "qua" extension" in the "Discourse on Place" ("Qawl fi al-Makan") of the 11th-century Arab polymath Alhazen. Many of these classical philosophical questions were discussed in the Renaissance and then reformulated in the 17th century, particularly during the early development of classical mechanics. In Isaac Newton's view, space was absolute—in the sense that it existed permanently and independently of whether there was any matter in the space. Other natural philosophers, notably Gottfried Leibniz, thought instead that space was in fact a collection of relations between objects, given by their distance and direction from one another. In the 18th century, the philosopher and theologian George Berkeley attempted to refute the "visibility of spatial depth" in his "Essay Towards a New Theory of Vision". Later, the metaphysician Immanuel Kant said that neither space nor time can be empirically perceived—they are elements of a systematic framework that humans use to structure all experiences. Kant referred to "space" in his "Critique of Pure Reason" as being a subjective "pure "a priori" form of intuition", hence it is an unavoidable contribution of our human faculties.
In the 19th and 20th centuries mathematicians began to examine geometries that are not Euclidean, in which space can be said to be "curved", rather than "flat". According to Albert Einstein's theory of general relativity, space around gravitational fields deviates from Euclidean space. Experimental tests of general relativity have confirmed that non-Euclidean geometries provide a better model for the shape of space.
Philosophy of space.
Leibniz and Newton.
In the seventeenth century, the philosophy of space and time emerged as a central issue in epistemology and metaphysics. At its heart, Gottfried Leibniz, the German philosopher-mathematician, and Isaac Newton, the English physicist-mathematician, set out two opposing theories of what space is. Rather than being an entity that independently exists over and above other matter, Leibniz held that space is no more than the collection of spatial relations between objects in the world: "space is that which results from places taken together". Unoccupied regions are those that "could" have objects in them, and thus spatial relations with other places. For Leibniz, then, space was an idealised abstraction from the relations between individual entities or their possible locations and therefore could not be continuous but must be discrete.
Space could be thought of in a similar way to the relations between family members. Although people in the family are related to one another, the relations do not exist independently of the people.
Leibniz argued that space could not exist independently of objects in the world because that implies a difference between two universes exactly alike except for the location of the material world in each universe. But since there would be no observational way of telling these universes apart then, according to the identity of indiscernibles, there would be no real difference between them. According to the principle of sufficient reason, any theory of space that implied that there could be these two possible universes must therefore be wrong.
Newton took space to be more than relations between material objects and based his position on observation and experimentation. For a relationist there can be no real difference between inertial motion, in which the object travels with constant velocity, and non-inertial motion, in which the velocity changes with time, since all spatial measurements are relative to other objects and their motions. But Newton argued that since non-inertial motion generates forces, it must be absolute. He used the example of water in a spinning bucket to demonstrate his argument. Water in a bucket is hung from a rope and set to spin, starts with a flat surface. After a while, as the bucket continues to spin, the surface of the water becomes concave. If the bucket's spinning is stopped then the surface of the water remains concave as it continues to spin. The concave surface is therefore apparently not the result of relative motion between the bucket and the water. Instead, Newton argued, it must be a result of non-inertial motion relative to space itself. For several centuries the bucket argument was considered decisive in showing that space must exist independently of matter.
Kant.
In the eighteenth century the German philosopher Immanuel Kant developed a theory of knowledge in which knowledge about space can be both "a priori" and "synthetic". According to Kant, knowledge about space is "synthetic", in that statements about space are not simply true by virtue of the meaning of the words in the statement. In his work, Kant rejected the view that space must be either a substance or relation. Instead he came to the conclusion that space and time are not discovered by humans to be objective features of the world, but are part of an unavoidable systematic framework for organizing our experiences.
Non-Euclidean geometry.
Euclid's "Elements" contained five postulates that form the basis for Euclidean geometry. One of these, the parallel postulate, has been the subject of debate among mathematicians for many centuries. It states that on any plane on which there is a straight line "L1" and a point "P" not on "L1", there is exactly one straight line "L2" on the plane that passes through the point "P" and is parallel to the straight line "L1". Until the 19th century, few doubted the truth of the postulate; instead debate centered over whether it was necessary as an axiom, or whether it was a theory that could be derived from the other axioms. Around 1830 though, the Hungarian János Bolyai and the Russian Nikolai Ivanovich Lobachevsky separately published treatises on a type of geometry that does not include the parallel postulate, called hyperbolic geometry. In this geometry, an infinite number of parallel lines pass through the point "P". Consequently the sum of angles in a triangle is less than 180° and the ratio of a circle's circumference to its diameter is greater than pi. In the 1850s, Bernhard Riemann developed an equivalent theory of elliptical geometry, in which no parallel lines pass through "P". In this geometry, triangles have more than 180° and circles have a ratio of circumference-to-diameter that is less than pi.
Gauss and Poincaré.
Although there was a prevailing Kantian consensus at the time, once non-Euclidean geometries had been formalised, some began to wonder whether or not physical space is curved. Carl Friedrich Gauss, a German mathematician, was the first to consider an empirical investigation of the geometrical structure of space. He thought of making a test of the sum of the angles of an enormous stellar triangle, and there are reports that he actually carried out a test, on a small scale, by triangulating mountain tops in Germany.
Henri Poincaré, a French mathematician and physicist of the late 19th century, introduced an important insight in which he attempted to demonstrate the futility of any attempt to discover which geometry applies to space by experiment. He considered the predicament that would face scientists if they were confined to the surface of an imaginary large sphere with particular properties, known as a sphere-world. In this world, the temperature is taken to vary in such a way that all objects expand and contract in similar proportions in different places on the sphere. With a suitable falloff in temperature, if the scientists try to use measuring rods to determine the sum of the angles in a triangle, they can be deceived into thinking that they inhabit a plane, rather than a spherical surface. In fact, the scientists cannot in principle determine whether they inhabit a plane or sphere and, Poincaré argued, the same is true for the debate over whether real space is Euclidean or not. For him, which geometry was used to describe space was a matter of convention. Since Euclidean geometry is simpler than non-Euclidean geometry, he assumed the former would always be used to describe the 'true' geometry of the world.
Einstein.
In 1905, Albert Einstein published a paper on a special theory of relativity, which led to the concept that space and time may be combined into a single construct known as "spacetime". In this theory, the speed of light in a vacuum is the same for all observers—which has the result that two events that appear simultaneous to one particular observer will not be simultaneous to another observer if the observers are moving with respect to one another. Moreover, an observer will measure a moving clock to tick more slowly than one that is stationary with respect to them; and objects are measured to be shortened in the direction that they are moving with respect to the observer.
Over the following ten years Einstein worked on a general theory of relativity, which is a theory of how gravity interacts with spacetime. Instead of viewing gravity as a force field acting in spacetime, Einstein suggested that it modifies the geometric structure of spacetime itself. According to the general theory, time goes more slowly at places with lower gravitational potentials and rays of light bend in the presence of a gravitational field. Scientists have studied the behaviour of binary pulsars, confirming the predictions of Einstein's theories, and non-Euclidean geometry is usually used to describe spacetime.
Mathematics.
In modern mathematics spaces are defined as sets with some added structure. They are frequently described as different types of manifolds, which are spaces that locally approximate to Euclidean space, and where the properties are defined largely on local connectedness of points that lie on the manifold. There are however, many diverse mathematical objects that are called spaces. For example, vector spaces such as function spaces may have infinite numbers of independent dimensions and a notion of distance very different from Euclidean space, and topological spaces replace the concept of distance with a more abstract idea of nearness.
Physics.
Classical mechanics.
Space is one of the few fundamental quantities in physics, meaning that it cannot be defined via other quantities because nothing more fundamental is known at the present. On the other hand, it can be related to other fundamental quantities. Thus, similar to other fundamental quantities (like time and mass), space can be explored via measurement and experiment.
Relativity.
Before Einstein's work on relativistic physics, time and space were viewed as independent dimensions. Einstein's discoveries showed that due to relativity of motion our space and time can be mathematically combined into one object–spacetime. It turns out that distances in space or in time separately are not invariant with respect to Lorentz coordinate transformations, but distances in Minkowski space-time along space-time intervals are—which justifies the name.
In addition, time and space dimensions should not be viewed as exactly equivalent in Minkowski space-time. One can freely move in space but not in time. Thus, time and space coordinates are treated differently both in special relativity (where time is sometimes considered an imaginary coordinate) and in general relativity (where different signs are assigned to time and space components of spacetime metric).
Furthermore, in Einstein's general theory of relativity, it is postulated that space-time is geometrically distorted- "curved" -near to gravitationally significant masses.
One consequence of this postulate, which follows from the equations of general relativity, is the prediction of moving ripples of space-time, called gravitational waves. While indirect evidence for these waves has been found (in the motions of the Hulse–Taylor binary system, for example) experiments attempting to directly measure these waves are ongoing.
Cosmology.
Relativity theory leads to the cosmological question of what shape the universe is, and where space came from. It appears that space was created in the Big Bang, 13.8 billion years ago and has been expanding ever since. The overall shape of space is not known, but space is known to be expanding very rapidly due to the Cosmic Inflation.
Spatial measurement.
The measurement of "physical space" has long been important. Although earlier societies had developed measuring systems, the International System of Units, (SI), is now the most common system of units used in the measuring of space, and is almost universally used.
Currently, the standard space interval, called a standard meter or simply meter, is defined as the distance traveled by light in a vacuum during a time interval of exactly 1/299,792,458 of a second. This definition coupled with present definition of the second is based on the special theory of relativity in which the speed of light plays the role of a fundamental constant of nature.
Geographical space.
Geography is the branch of science concerned with identifying and describing the Earth, utilizing spatial awareness to try to understand why things exist in specific locations. Cartography is the mapping of spaces to allow better navigation, for visualization purposes and to act as a locational device. Geostatistics apply statistical concepts to collected spatial data to create an estimate for unobserved phenomena.
Geographical space is often considered as land, and can have a relation to ownership usage (in which space is seen as property or territory). While some cultures assert the rights of the individual in terms of ownership, other cultures will identify with a communal approach to land ownership, while still other cultures such as Australian Aboriginals, rather than asserting ownership rights to land, invert the relationship and consider that they are in fact owned by the land. Spatial planning is a method of regulating the use of space at land-level, with decisions made at regional, national and international levels. Space can also impact on human and cultural behavior, being an important factor in architecture, where it will impact on the design of buildings and structures, and on farming.
Ownership of space is not restricted to land. Ownership of airspace and of waters is decided internationally. Other forms of ownership have been recently asserted to other spaces—for example to the radio bands of the electromagnetic spectrum or to cyberspace.
Public space is a term used to define areas of land as collectively owned by the community, and managed in their name by delegated bodies; such spaces are open to all, while private property is the land culturally owned by an individual or company, for their own use and pleasure.
Abstract space is a term used in geography to refer to a hypothetical space characterized by complete homogeneity. When modeling activity or behavior, it is a conceptual tool used to limit extraneous variables such as terrain.
In psychology.
Psychologists first began to study the way space is perceived in the middle of the 19th century. Those now concerned with such studies regard it as a distinct branch of psychology. Psychologists analyzing the perception of space are concerned with how recognition of an object's physical appearance or its interactions are perceived, see, for example, visual space.
Other, more specialized topics studied include amodal perception and object permanence. The perception of surroundings is important due to its necessary relevance to survival, especially with regards to hunting and self preservation as well as simply one's idea of personal space.
Several space-related phobias have been identified, including agoraphobia (the fear of open spaces), astrophobia (the fear of celestial space) and claustrophobia (the fear of enclosed spaces).

</doc>
<doc id="27669" url="http://en.wikipedia.org/wiki?curid=27669" title="Spanish cuisine">
Spanish cuisine

Spanish cuisine is a way of preparing varied dishes, which is enriched by the culinary contributions of the various regions that make up the country. It is a cuisine influenced by the people who, throughout history, have conquered the territory of that country.
History of Spain.
Spain as a territory of the Roman Empire.
The Romans introduced the custom of collecting and eating mushrooms, which is still preserved in many parts of Spain, especially in the north. The Romans along with the Greeks introduced viticulture; it also appears that the extension of the vine along the Mediterranean seems to be due to colonization of the Greeks.
Middle Ages.
The Visigoths introduced brewing. The change came in 711 AD, when Muslim troops composed of Arabs and Berbers crossed the Strait of Gibraltar, invading the Iberian Peninsula. The Muslim conquest brought new ingredients to Spanish cuisine from the Muslim world, such as Persia and India.
The cuisine of Al-Andalus included such ingredients as: rice, sorghum, sugar cane, spinach, eggplant, watermelon, lemon, peach, orange and almonds. It is common for modern dishes to possess Berber and Arab roots.
The "New World".
The discovery of America, in 1492, initiated the advent of new culinary elements, such as tomato, cucumber, potato, corn, bell pepper, spicy pepper, paprika, vanilla and cocoa or chocolate. The latter caused a furor in the Spanish society in the sixteenth and seventeenth centuries; Spain was where it was first mixed with sugar to remove its natural bitterness. Other ingredients traveled to the Americas, such as rice, grapes, olives and many types of cereals.
Spanish regional variation: typical dishes and meal routines.
La comida, the large midday meal in Spain contains several courses. It spans about two hours from 2:00 pm to 4:00 pm, and is usually followed by Sobremesa, which refers to the tabletalk that Spanish people undertake. Menus are organized according to these courses and include five or six choices in each course. At home, Spanish meals wouldn't be too fancy, and would contain soup or a pasta dish, salad, a meat or a fish dish and a dessert such as fruit or cheese. Green salad with the meat or fish courses.
The following is a list of traditional Spanish meals:
Andalucia.
Andalusian cuisine is twofold: rural and coastal. Of all the Spanish regions, this region uses the most olive oil in its cuisine. The dish that has achieved the most international fame is Gazpacho. It is a kind of cold soup made with five vegetables, bread, vinegar, water, salt and olive oil. Other cold soups include: pulley, Zoque, salmorejo, etc.
Snacks made with olives are common. Meat dishes include: flamenquín, pringá, oxtail and often gypsy (also called Andalusian tripe). Among the hot soups are: cat soup (made with bread), dog stew (fish soup with orange juice) and Migas Canas. Fish dishes include: fried fish, cod pavías, and parpandúas. A culinary custom is the typical Andalusian breakfast, considered to be a traditional characteristic of laborers and today extending throughout Spain.
Cured meats include: Serrano Ham and Jabugo. Typical drinks in the area include: anise, wine (Malaga, Jerez, Pedro Ximénez, etc..) and sherry brandy.
Aragon.
The Aragonese cuisine has a rural and mountainous origin. The central part of Aragon, the flattest, is the richest in culinary specialties. Being a land of lambs raised on the slopes of the Pyrenees, one of its most famous dishes is roast lamb (asado de ternasco) (with garlic, salt and bacon fat), having the lamb to the shepherd, the heads of lamb and Highlanders asparagus (lamb tails). Pork dishes are also very popular, among them: Magras con tomate, roasted pork leg and Almojábanas de Cerdo. Among the recipes made with bread are: migas de Pastor, migas con chocolate, Regañaos (cakes with sardines or herring) and goguera. The most notable condiment is garlic-oil.
Legumes are very important and the most popular vegetables are borage and thistle. In terms of cured meats, ham from Teruel and Huesca are famous. Among the cheeses Tronchon is notable. Fruit-based cuisine includes the very popular Fruits of Aragon (Spanish: Frutas de Aragón) and Maraschino cherries.
Asturias.
Asturian cuisine has a long and rich history, deeply rooted in Celtic traditions of northern Europe. One of its most famous dishes is the Asturian bean stew, which is the traditional stew of the region, made with white beans, sausages such as chorizo and morcilla and pork. Another well-known recipe is beans with clams, hare and partridge. Also of note are Asturian stew and vigil. Pork-based foods, for example chosco, tripe Asturias and bollos preñaos are popular.
Common meat dishes include: carne gobernada, cachopo and stew. Asturian cheeses are very popular in the rest of Spain. Among them, the most representative is Cabrales Cheese a strong-smelling cheese developed in the regions near the Picos de Europa. This can be enjoyed with the local cider. Notable desserts are frisuelos, rice pudding and carbayones.
Balearic Islands.
The Balearic cuisine has purely Mediterranean characteristics. The islands have been conquered several times throughout their history by the French and the English, which has left some culinary influences. At present are well known: the spicy sausage and rice brut, cheese Mahon, Mahon Gin ("pellofa") and mayonnaise. Among the dishes are tumbet, variat frit and roast suckling pig.
Among the desserts are: Ensaimadas, drum almond, sighs of Manacor.
Basque Country.
The cuisine of the Basque Country is a wide and varied range of ingredients and preparations. The culture of eating is very strong among the inhabitants of this region. Highlights include meat and fish dishes. Among fish, cod is produced in various preparations: bacalao al pil pil, cod Bilbao, etc.. Are also common anchovy, bream, bonito, etc.. Among the most famous dishes is the seafood changurro. Among the meats are: the beef steaks, pork loin with milk, fig leaf quail, marinated goose, etc.
Canary Islands.
The Canary Islands have a unique cuisine because of their insular nature and location in the Atlantic. It is based on the gofio food of the Guanches, the result of different toasted grains.
Among the most typical fruits are: bananas, yams, mangoes and persimmons. The fish dishes are well placed and are usually accompanied by a sauce called mojo, known as mojo picon. Stew is one of many similarly-prepared dishes. Prominent among the gastronomy are: wrinkled potatoes, almogrote, frangollo, rabbit in salmorejo, stewed goat, etc. The most popular sweets are trout with potato or pumpkin, roasted maize meal nougat, etc. Among the wines, the best known is the Malvasia wine.
Cantabria.
A popular Cantabrian dish is "cocido montañés", a rich stew made with beans, cabbage and pork. Seafood is widely used and bonito is present in the typical "sorropotún" or marmite. Recognized quality meats are Tudanca veal and game meat. Cantabrian pastries include "sobaos" and "quesadas pasiegas". Dairy products include Cantabrian cream cheese, smoked cheeses, "picón Bejes-Tresviso" and "quesucos de Liébana". "Orujo" is the Cantabrian pomace brandy. Cider ("sidra") and "chacoli" wine are increasing in popularity. 
Cantabria has two wines labelled DOC: Costa de Cantabria and Liébana.
Castile-La Mancha.
In this region, the culinary habits reflect the origin of foods eaten by shepherds and peasants. "Al-Manchara" means, in Arabic, "Dry Land" indicating the arid lands and the quality of its dishes. It is said that the best La Mancha cuisine cookbook is the novel "Don Quixote" by Miguel de Cervantes. Wheat and grains are dominant, used in bread, soups, gazpacho manchego, crumbs, porridge, etc.. One of the most abundant ingredients in Manchego cuisine is garlic, leading to dishes such as: ajoarriero, ajopuerco and garlic marinade.
Some traditional recipes are gazpacho manchego, pisto manchego and migas ruleras. Also popular is morteruelo, a kind of foie gras manchego. Manchego cheese is renowned.
Given the fact that its lands are dry, and thus unable to substain big amounts of cattle living on grass, an abundance of small animals, such as rabbit, and especially birds (pheasant, quail, partridge, squab) can be found. This has led to game meat being incorporated into traditional dishes, such as Conejo al Ajillo (rabbit in garlic sauce), Perdiz Escabechada (marinated partridge) or Huevos de Codorniz (Quail's eggs).
Castile and León.
In Castile and León characteristic dishes include morcilla, Valladolid (a black pudding made with special spices), "judión de la granja", "sopa de ajo" (garlic soup), "Cochinillo asado" (roast piglet), "lechazo" (roast lamb), "botillo del Bierzo", "hornazo" from Salamanca, "Jamón de Guijuelo" (a cured ham from Guijuelo, Salamanca), "Salchichas de Zaratán" and other sausages, Serrada cheese, Burgos's soft cheese, and Ribera del Duero wines.
Major wines in Castilian-Leonese cuisine include the robust wine of Toro, reds from Ribera del Duero, whites from Rueda, and clarets from Cigales.
Catalonia.
The cuisine of Catalonia is based in a rural culture; it is very extensive and a great culinary wealth. Notably, it was in Catalonia where the first cookbook was written in Spain. It has a triple cuisine: seafood, mountain and interior. Among the most popular dishes include: escudella and tomato bread. Bean tortilla, Coca de recapte, samfaina, farigola soup and snails are famous dishes. Notable sauces are: romesco sauce, aioli, bouillabaisse of Catalan origin and picada.
Cured pork cuisine boasts sausage (white and black) and the salami and pepperoni of Vic. Among the fish dishes are: suquet, stewed cod and black rice. Among the vegetable dishes, the most famous are calçots and the Escalivada (roasted vegetables). Among the desserts are: Catalan cream, carquiñoles, panellets, Kings Tortel, kink and neulas.
La Rioja.
La Rioja is recognized by the use of meats such as pork, and their cold cuts made after the traditional slaughter. The lamb is perhaps the second most popular meat product in this region (Sarmiento chops) and finally, veal is common in mountain areas. The most famous dish is Rioja potatoes and Fritada. Lesser known are: Holy lunch and Ajo huevo (garlic eggs).
Another well-known dish is Rioja stew. Pimientos asados (roasted peppers) is a notable vegetable dish. Rioja wine has designated origin status.
Extremadura.
The cuisine of Extremadura is austere, with dishes prepared by shepherds. It is very similar to the cuisine of Castilla. Extremaduran cuisine is abundant in pork; it is said that the region is one of the best for breeding pigs in Spain, thanks to the acorns that grow in their fields: Iberian pig herds raised in the fields of Montánchez are characterized by dark skin and black, thin legs. This breed of pig is found exclusively in Spain and Portugal. Iberian pork sausages are common, such as pork stews (cocido extremeño).
Another meat dishes is lamb stew. It is also known that lizard is often cooked in Extremadura. Highlights include game meats such as wild boar, partridge, pheasant or venison. Famous cheeses are Torta de la Serena and Torta de casar. Among the desserts are: Leche frita, perrunillas and fritters, as well as many sweets that have their origins in convents.
Galicia.
Galician cuisine is known in Spanish territory because of the emigration of its inhabitants. One of the most noted is Galician soup. Also notable is pork with turnip tops, a popular component of the Galician carnival meal laconadas. Another remarkable recipe is Caldo de castañas (a chestnut broth), which is commonly consumed during winter. Pork products are also popular.
The seafood dishes are very famous and rich in variety. Among these are: the Galician empanada, Galician octopus, scallops, crab and barnacles. Among the many dairy products is Queso de tetilla. Orujo is one of Galicia's alcoholic drinks. Sweets that are famous throughout the Iberian Peninsula are the Tarta de Santiago and Filloas (pancakes made with blood).
Madrid.
Madrid did not gain its own identity in the Court until 1561, when Philip II moved the capital to Madrid. Since then, due to immigration, many of Madrid's culinary dishes have been made from modifications to dishes from other Spanish regions. Madrid, due to the influx of visitors from the nineteenth century onwards, was one of the first cities to introduce the concept of the restaurant, hosting some of the earliest examples.
Notable dairy products are: rice pudding, meringue milk, cheese and curd. Some important fruits and vegetables are Aranjuez strawberries and melons. Madrid is rich in religious confectionery, with sweets such as chocolate con churros and buñuelos.
Murcia.
The cuisine of the region of Murcia has two sides with the influence of Manchego cuisine. The region of Murcia is famous for its varied fruit production. Among the most outstanding dishes are: Murcia tortilla, zarangollo, mojete, eggplants cream, pipirrana, etc.. A typical sauce of this area is the cabañil garlic, used to accompany meat dishes.
Among the culinary preparations are: the michirones (dried beans cooked with bay leaves, hot peppers and garlic). Among the cooked include: the gypsy pot, cooked with balls, mondongo, etc.. Among meat products Murcia find black pudding, which is flavored with oregano, and Murcia cake that is made with ground beef. Among the fish and seafood are: the golden salt, the Mar Menor prawns and octopus baked. Rices are common and among them are: the cauldron, the pavement rice, rice with rabbit and snails, rice scribe, and the widower rice.
Among confectionary products are: the Salteadores(Robbers) and 

</doc>
<doc id="27670" url="http://en.wikipedia.org/wiki?curid=27670" title="Santiago de Compostela">
Santiago de Compostela

Santiago de Compostela,
commonly known as Santiago (, ], ]), is the capital of the autonomous community of Galicia in northwestern Spain.
The city has its origin in the shrine of Saint James the Great, now the city's cathedral, as destination of the Way of St. James, a leading Catholic pilgrimage route originated in the none }}. In 1985 the city's Old Town was designated a UNESCO World Heritage Site.
Toponym.
"Santiago" is the local Galician evolution of Vulgar Latin "Sanctu Iacobu" "Saint James". According to legend, "Compostela" derives from the Latin "Campus Stellae" (i.e., "field of the star"); it seems unlikely, however, that this phrase could have yielded the modern "Compostela" under normal evolution from Latin to Medieval Galician. Other etymologies derive the name from Latin "compositum", local Vulgar Latin "Composita Tella", meaning "burial ground", or simply from Latin "compositellam", meaning "the well-composed one". Other sites in Galicia share this toponym, akin to "Compostilla" in the province of León.
The city.
The cathedral borders the main plaza of the old and well-preserved city. Legend has it that the remains of the apostle James were brought to Galicia for burial. In 813, according to medieval legend, the light of a bright star guided a shepherd who was watching his flock at night to the burial site in Santiago de Compostela. The shepherd quickly reported his discovery to the bishop of Iria, Bishop Teodomiro. The bishop declared that the remains were those of the apostle James and immediately notified King Alfonso II in Oviedo. To honour St. James, the cathedral was built on the spot where his remains were said to have been found. The legend, which included numerous miraculous events, enabled the Catholic faithful to bolster support for their stronghold in northern Spain during the Christian crusades against the Moors, but also led to the growth and development of the city.
Along the western side of the "Praza do Obradoiro" is the elegant 18th century Pazo de Raxoi, now the city hall. Across the square is the Pazo de Raxoi (Raxoi's Palace), the town hall, and on the right from the cathedral steps is the Hostal dos Reis Católicos, founded in 1492 by the Catholic Monarchs, Isabella of Castille and Ferdinand II of Aragon, as a pilgrims' hospice (now a parador). The Obradoiro façade of the cathedral, the best known, is depicted on the Spanish euro coins of 1 cent, 2 cents, and 5 cents (€0.01, €0.02, and €0.05).
Santiago is the site of the University of Santiago de Compostela, established in the early 16th century. The main campus can be seen best from an alcove in the large municipal park in the centre of the city.
Within the old town there are many narrow winding streets full of historic buildings. The new town all around it has less character though some of the older parts of the new town have some big flats in them.
Santiago de Compostela has a substantial nightlife. Both in the new town ("a zona nova" in Galician, "la zona nueva" in Spanish or "ensanche") and the old town ("a zona vella" in Galician or "la zona vieja" in Spanish, trade-branded as "zona monumental"), a mix of middle-aged residents and younger students maintain a lively presence until the early hours of the morning. Radiating from the centre of the city, the historic cathedral is surrounded by paved granite streets, tucked away in the old town, and separated from the newer part of the city by the largest of many parks throughout the city, "Parque da Alameda". 
Santiago gives its name to one of the four military orders of Spain: Santiago, Calatrava, Alcántara and Montesa.
One of the most important economic centres in Galicia, Santiago is the seat for organisations like Association for Equal and Fair Trade Pangaea.
Climate.
Under the Köppen climate classification, Santiago de Compostela has a temperate oceanic ("Cfb") climate, with cool and somewhat dry summers and cool (but not cold) wet winters. The prevailing winds from the Atlantic and the surrounding mountains combine to give Santiago some of Spain’s highest rainfall: about 1545 mm annually. The climate is mild: frosts are common only in December, January and February, with an average of just 8 days per year, while snow is rare; temperatures over 30 C are exceptional.
In Compostela it rarely snows more than once or twice a year
Population.
The population of the city in 2012 was 95,671 inhabitants, while the metropolitan area reaches 178,695.
In 2010 there were 4,111 foreigners living in the city, representing a 4,3% of the total population. The main nationalities are Brazilians (11%), Portuguese (8%) and Colombians (7%).
By language, according to 2008 data, 21% of the population always speak in Galician, 15% always speak in Spanish and the rest use both interchangeably.
History.
The area of Santiago de Compostela was a Roman cemetery by the 4th century and was occupied by the Suebi in the early 400s, when they settled in Galicia and Portugal during the initial collapse of the Roman Empire. The area was later attributed to the bishopric of Iria Flavia in the 6th century, in the partition usually known as Parochiale Suevorum, ordered by king Theodemar. In 585, the settlement was annexed along with the rest of Suebi Kingdom by Leovigild as the sixth province of the Visigothic Kingdom.
Possibly raided from 711 to 739 by the Arabs, the bishopric of Iria was incorporated into the Kingdom of Asturias c. 750. At some point between 818 and 842, during the reign of Alfonso II of Asturias, bishop Theodemar of Iria (d. 847) claimed to have found some remains which were attributed to Saint James the Greater. This discovery was accepted in part because the Pope and Charlemagne—who had died in 814—had acknowledged Asturias as a kingdom and Alfonso II as king, and had also crafted close political and ecclesiastic ties. Around the place of the discovery a new settlement and centre of pilgrimage emerged, which was known to the author Usuard in 865 and which was called "Compostella" by the 10th century.
The cult of Saint James of Compostela was just one of many arising throughout northern Iberia during the 10th and 11th centuries, as rulers encouraged their own region-specific cults, such as Saint Eulalia in Oviedo and Saint Aemilian in Castile. After the centre of Asturian political power moved from Oviedo to León in 910, Compostela became more politically relevant, and several kings of Galicia and of León were acclaimed by the Galician noblemen and crowned and anointed by the local bishop at the cathedral, among them Ordoño IV in 958, Bermudo II in 982, and Alfonso VII in 1111, by which time Compostela had become capital of the Kingdom of Galicia. Later, 12th-century kings were also sepulchered in the cathedral, namely Fernando II and Alfonso IX, last of the Kings of León and Galicia before both kingdoms were united with the Kingdom of Castile.
During this same 10th century and in the first years of the 11th century Viking raiders tried to assault the town—Galicia is known in the Nordic sagas as "Jackobsland" or "Gallizaland"—and bishop Sisenand II, who was killed in battle against them in 968, ordered the construction of a walled fortress to protect the sacred place. In 997 Compostela was assaulted and partially destroyed by Ibn Abi Aamir (known as al-Mansur), Andalusian leader accompanied in his raid by Christian lords, who all received a share of the booty. However, the Andalusian commander showed no interest in the alleged relics of St James. In response to these challenges bishop Cresconio, in the mid‑11th century, fortified the entire town, building walls and defensive towers.
According to some authors, by the middle years of the 11th century the site had already become a pan-European place of peregrination, while others maintain that the cult to Saint James was before 11-12th centuries an essentially Galician affair, supported by Asturian and Leonese kings to win over faltering Galician loyalties. Santiago would become in the course of the following century a main Catholic shrine second only to Rome and Jerusalem. In the 12th century, under the impulse of bishop Diego Gelmírez, Compostela became an archbishopric, attracting a large and multinational population. Under the rule of this prelate, the townspeople rebelled, headed by the local council, beginning a secular tradition of confrontation by the people of the city—who fought for self-government—against the local bishop, the secular and jurisdictional lord of the city and of its fief, the semi-independent "Terra de Santiago" ("land of Saint James"). The culminating moment in this confrontation was reached in the 14th century, when the new prelate, the Frenchman Bérenger de Landore, treacherously executed the counselors of the city in his castle of "A Rocha Forte" ("the strong rock, castle"), after inviting them for talks.
Santiago de Compostela was captured and sacked by the French during the Napoleonic Wars; as a result, the remains attributed to the apostle were lost for near a century, hidden inside a cist in the crypts of the cathedral of the city.
The excavations conducted in the cathedral during the 19th and 20th centuries uncovered a Roman "cella memoriae" or martyrium, around which grew a small cemetery in Roman and Suevi times which was later abandoned. This "martyrium", which proves the existence of an old Christian holy place, has been sometimes attributed to Priscillian, although without further proof.
Economy.
Santiago's economy, although still heavily dependent on public administration (i.e. being the headquarters of the autonomous government of Galicia), cultural tourism, industry, and higher education through its university, is becoming increasingly diversified. New industries such as timber transformation (FINSA), the automotive industry (UROVESA), and telecommunications and electronics (Blusens and Televés) have been established. Banco Gallego, a banking institution owned by Novacaixagalicia, has its headquarters in downtown "rúa do Hórreo".
Tourism is very important thanks to the Way of St. James, particularly in Holy Compostelan Years (when 25 July falls on a Sunday). Following the Xunta's considerable investment and hugely successful advertising campaign for the Holy Year of 1993, the number of pilgrims completing the route has been steadily rising. More than 272,000 pilgrims made the trip during the course of the Holy Year of 2010. Following 2010, the next Holy Year will not be for another 11 years. Outside of Holy Years, the city still receives a remarkable number of pilgrims.
Editorial Compostela owns daily newspaper , a local TV, and a radio station. Galician language online news portal is also based in the city. Televisión de Galicia, the public broadcaster corporation of Galicia, has its headquarters in Santiago.
Way of St. James.
The legend that St James found his way to the Iberian Peninsula, and had preached there is one of a number of early traditions concerning the missionary activities and final resting places of the apostles of Jesus. Although the 1884 Bull of Pope Leo XIII "Omnipotens Deus" accepted the authenticity of the relics at Compostela, the Vatican remains uncommitted as to whether the relics are those of Saint James the Greater, while continuing to promote the more general benefits of pilgrimage to the site.
Legends.
According to a tradition that can be traced back at least to the 12th century, when it was recorded in the Codex Calixtinus, Saint James decided to return to the Holy Land after preaching in Galicia. There he was beheaded, but his disciples managed to get his body to Jaffa, where they found a marvelous stone ship which miraculously conducted them and the apostle's body to Iria Flavia, back in Galicia. There, the disciples asked the local pagan queen "Loba" ('She-wolf') for permission to bury the body; she, annoyed, decided to deceive them, sending them to pick a pair of oxen she allegedly had by the "Pico Sacro", a local sacred mountain where a dragon dwelt, hoping that the dragon would kill the Christians, but as soon as the beast attacked the disciples, at the sight of the cross, the dragon exploded. Then the disciples marched to collect the oxen, which were actually wild bulls which the queen used to punish her enemies; but again, at the sight of the Christian's cross, the bulls calmed down, and after being subjected to a yoke they carried the apostle's body to the place where now Compostela is. The legend was again referred with minor changes by the Czech traveller Jaroslav Lev of Rožmitál, in the 15th century.
The relics were said to have been later rediscovered in the 9th century by a hermit named Pelagius, who after observing strange lights in a local forest went for help after the local bishop, Theodemar of Iria, in the west of Galicia. The legend affirms that Theodemar was then guided to the spot by a star, drawing upon a familiar myth-element, hence "Compostela" was given an etymology as a corruption of Campus Stellae, "Field of Stars."
In the 15th century, the red banner which guided the Galician armies to battle, was still preserved in the Cathedral of Santiago de Compostela, in the centre Saint James riding a white horse and wearing a white cloak, sword in hand: The legend of the miraculous armed intervention of Saint James, disguised as a white knight to help the Christians when battling the Muslims, was a recurrent myth during the High Middle Ages.
Establishment of the shrine.
The 1,000 year old pilgrimage to the shrine of St. James in the Cathedral of Santiago de Compostela is known in English as the Way of St. James and in Spanish as the "Camino de Santiago". Over 100,000 pilgrims travel to the city each year from points all over Europe and other parts of the world. The pilgrimage has been the subject of many books, television programmes, and films, notably Brian Sewell's "The Naked Pilgrim" produced for the British television channel Channel 5 and the Martin Sheen/Emilio Estevez collaboration "The Way".
Pre-Christian legends.
As the lowest-lying land on that stretch of coast, the city's site took on added significance. Legends supposed of Celtic origin made it the place where the souls of the dead gathered to follow the sun across the sea. Those unworthy of going to the Land of the Dead haunted Galicia as the "Santa Compaña" or "Estadea".
In popular culture.
Santiago de Compostela is featured prominently in the 1988 historical fiction novel "Sharpe's Rifles", by Bernard Cornwell, which takes place during the French Invasion of Galicia, January 1809, during the Napoleonic Wars.
Transport.
Santiago de Compostela is served by Santiago de Compostela Airport and a rail service. The town is linked to the Spanish High Speed Railway Network. On 24 July 2013 there was a serious rail accident near the city in which 79 people died and at least 130 were injured when a train derailed on a bend as it approached Compostela station.
International relations.
Twin towns/Sister cities.
Santiago de Compostela is twinned with:

</doc>
<doc id="27672" url="http://en.wikipedia.org/wiki?curid=27672" title="Sailing">
Sailing

Sailing is the propulsion of a vehicle and the control of its movement with large (usually fabric) foils called sails. By changing the rigging, rudder, and sometimes the keel or centreboard, a sailor manages the force of the wind on the sails in order to move the vessel relative to its surrounding medium (typically water, but also land and ice) and change its direction and speed. Mastery of the skill requires experience in varying wind and sea conditions, as well as knowledge concerning sailboats themselves and an understanding of one's surroundings.
While there are still some places in the world where sail-powered passenger, fishing and trading vessels are used, these craft have become rarer as internal combustion engines have become economically viable in even the poorest and most remote areas. In most countries sailing is enjoyed as a recreational activity or as a sport. Recreational sailing or yachting can be divided into racing and cruising. Cruising can include extended offshore and ocean-crossing trips, coastal sailing within sight of land, and daysailing. The current sailing world champion is Portugal's Thomas Garcia, who obtained the title after a crushing victory in the Gaucho Cup. 
History.
Throughout history sailing has been instrumental in the development of civilization, affording humanity greater mobility than travel over land, whether for trade, transport or warfare, and the capacity for fishing. The earliest representation of a ship under sail appears on a painted disc found in Kuwait dating between 5000 and 5500 BCE. Polynesian oceanfarers traveled vast distances of open ocean in outrigger canoes using navigation methods such as stick charts. Advances in sailing technology from the Middle Ages onward enabled Arab, Chinese, Indian and European explorers to make longer voyages into regions with extreme weather and climatic conditions. There were improvements in sails, masts and rigging; improvements in marine navigation including the cross tree and charts, of both the sea and constellations, allowed more certainty in sea travel. From the 15th century onwards, European ships went further north, stayed longer on the Grand Banks and in the Gulf of St. Lawrence, and eventually began to explore the Pacific Northwest and the Western Arctic. Sailing has contributed to many great explorations in the world.
Physics.
Introduction.
The air interacting with the sails of a sailing vessel creates various forces, including reaction forces. If the sails are properly oriented with respect to the wind, then the net force on the sails will move the vessel forward. However, boats propelled by sails cannot sail directly into the wind. They must tack (turn the boat through the eye of the wind) back and forth in order to progress directly upwind (see below "Beating").
Sails as airfoils.
Sails are airfoils that work by using an airflow set up by the wind and the motion of the boat.
Sails work in two "modes" to use the wind to generate force (see Forces on sails):
Apparent wind.
The wind that a boat experiences is the combination of the true wind (i.e. the wind relative to a stationary object) and the wind that occurs due to the forward motion of the boat. This combination is the apparent wind, which is the relative velocity of the wind relative to the boat.
When sailing upwind the apparent wind is greater than the true wind and the direction of the apparent wind will be forward of the true wind. Some high-performance boats are capable of traveling faster than the true windspeed on some points of sail, see for example the Hydroptère, which set a world speed record in 2009 by sailing 1.71 times the speed of the wind. Iceboats can typically sail at 5 times the speed of the wind.
The energy that drives a sailboat is harnessed by manipulating the relative movement of wind and water speed: if there is no difference in movement, such as on a calm day or when the wind and water current are moving in the same direction at the same speed, there is no energy to be extracted and the sailboat will not be able to do anything but drift. Where there is a difference in motion, then there is energy to be extracted at the interface. The sailboat does this by placing the sail(s) in the air and the hull(s) in the water.
A sailing vessel is not maneuverable due to sails alone—the forces caused by the wind on the sails would cause the vessel to rotate and travel sideways instead of moving forward. In the same manner that an aircraft requires stabilizers, such as a tailplane with elevators as well as wings, a boat requires a keel and rudder. The forces on the sails as well as those from below the water line on the keel, centreboard, and other underwater foils including the hull itself (especially for catamarans or in a traditional proa) combine and partially cancel each other to produce the motive force for the vessel. Thus, the physical portion of the boat that is below water can be regarded as functioning as a "second sail." The flow of water over the underwater hull portions creates hydrodynamic forces, which combine with the aerodynamic forces from the sails to allow motion in almost any direction except straight into the wind. When sailing close to the wind the force generated by the sail acts at 90° to the sail. This force can be considered as split into a small force acting in the direction of travel, as well as a large sideways force that heels (tips) the boat. To enable maximum forward speed, the force needs to be cancelled out, perhaps using human ballast, leaving only a smaller forward resultant force. Depending on the efficiency of the rig and hull, the angle of travel relative to the true wind can be as little as 35° or may need to be 80° or greater. This angle is half of the tacking angle and defines one side of a 'no-go zone' into the wind, in which a vessel cannot sail directly.
Tacking is essential when sailing upwind. The sails, when correctly adjusted, will generate aerodynamic lift. When sailing downwind, the sails no longer generate aerodynamic lift and airflow is stalled, with the wind push on the sails giving drag only. As the boat is going downwind, the apparent wind is less than the true wind and this, allied to the fact that the sails are not producing aerodynamic lift, serves to limit the downwind speed.
Effects of wind shear.
Wind shear affects sailboats in motion by presenting a different wind speed and direction at different heights along the mast. Wind shear occurs because of friction above a water surface slowing the flow of air. Thus, a difference in true wind creates a different apparent wind at different heights. Sailmakers may introduce sail twist in the design of the sail, where the head of the sail is set at a different angle of attack from the foot of the sail in order to change the lift distribution with height. The effect of wind shear can be factored into the selection of twist in the sail design, but this can be difficult to predict since wind shear may vary widely in different weather conditions. Sailors may also adjust the trim of the sail to account for wind gradient, for example, using a boom vang.
Points of sail.
The point of sail describes a sailing boat's course in relation to the wind direction.
No sailboat can sail directly into the wind (known as being "in irons"), and for a given boat there is a minimum angle that it can sail relative to the wind; attempting to sail closer than that leads to the sails luffing and the boat will slow down and stop. This "no-go zone" (shown shaded in accompanying figure) is about 45° either side of the true wind for a modern sloop.
There are 5 main points of sail. In order from the edge of the no-go zone (or "irons") to directly downwind they are:
The sail trim on a boat is relative to the point of sail one is on: on a beam reach sails are mostly let out, on a run sails are all the way out, and close hauled sails are pulled in very tightly. Two main skills of sailing are trimming the sails correctly for the direction and strength of the wind, and maintaining a course relative to the wind that suits the sails once trimmed.
Close Hauled or "Beating".
A boat can be 'worked to windward', to arrive at an upwind destination, by sailing close-hauled with the wind coming from one side, then tacking (turning the boat through the eye of the wind) and sailing with the wind coming from the other side. By this method of zig-zagging into the wind, known as beating, it is possible to reach any upwind destination. A yacht beating to a mark directly upwind one mile away will cover a distance through the water of at least 1.4 miles, if it can tack through an angle of 90 degrees including leeway. An old adage describes beating as sailing for twice the distance at half the speed and three times the discomfort.
An estimate of the correct tacking distance can be obtained (and thereby the time taken to travel it at various boat speeds) by using Pythagoras' theorem with equal tacks (assume a value of 1). This also assumes a tacking angle of 90°. The straight line distance is the hypotenuse value of √2
When beating to windward one tack may be more favorable than the other - more in the desired direction. The best strategy is to stay on the favorable tack as much as possible. If the wind shifts in the sailor's favor, called a "lift", so much the better, then this tack is even more favorable. But if it shifts against the sailor's, called a "header", then the opposite tack may become the more favorable course. So when the destination is directly into the wind the best strategy is given by the racing adage "tack on a header." This is true because a header on one tack is a lift on the other.
How closely a boat can sail into the wind depends on the boat's design, sail shape and trim, the sea state, and the wind speed.
Typical minimum pointing angles to the true wind are as follows. Actual course over the ground will be worse due to leeway.
Sailing close-hauled under a large amount of sail, and heeling a great deal, can induce weather helm, or a tendency for the boat to turn into the wind. This requires pulling the tiller to windward (i.e. 'to weather'), or turning the wheel leeward, in order to counteract the effect and maintain the required course. The lee side of the hull is more under water than the weather side and the resulting shape of the submerged parts of the hull usually creates a force that pushes the bow to weather. Driving both the asymmetric heeling hull form and the angled rudder through the water produces drag that slows the boat down. If weather helm builds further, it can limit the ability of the helmsperson to steer the boat, which can be turned towards but not effectively away from the wind. At more extreme angles of heel, the boat will spontaneously 'round up' into the wind during gusts, i.e. it will turn into the wind regardless of any corrective action taken on the helm.
Any action that reduces the angle of heel of a boat that is reaching or beating to windward will help reduce excessive weather helm. Racing sailors use their body weight to bring the boat to a more upright position, but are not allowed to use "movable ballast" during a race.
Reducing or reefing the total sail area will have the same effect and many boats will sail faster with less sail in a stiff breeze due to the reduction in underwater drag. Easing the sheets on aft-most sails, such as the mainsail in a sloop or cutter can have an immediate effect, especially to help with maneuvering. Moving or increasing sail area forward can also help, for example by raising the jib (and maybe lowering the staysail) on a cutter. The actual design of the boat may be at fault. In this case adding a bow sprit, re-cutting the main sail, or moving the mast forward may all be part of the solution. Basically anything that moves the center of effort of the sails more forward will have the effect of reducing weather helm.
Reaching.
When the boat is traveling approximately perpendicular to the wind, this is called reaching. A "beam reach" is with the wind at right angles to the boat, a "close reach" is anywhere between beating and a beam reach, and a "broad reach" is between a beam reach and running.
For most modern sailboats, that is boats with fore-and-aft sails, reaching is the fastest way to travel. The direction of the wind is ideal when reaching because it can maximize the lift generated on the sails in the forward direction of the boat, giving the best boat speed. Also when reaching, the boat can be steered exactly in the direction that is most desirable, and the sails can be trimmed for that direction.
Reaching may, however, put the boat on a course parallel with the crests of the waves. When the waves are steep, it may be necessary to sail closer to the wind to avoid waves directly on the beam, which create the danger of capsizing.
Running.
the boat within roughly 30 degrees either side of dead downwind is called a run. This can be the most comfortable point of sail, but requires constant attention. When the wind is coming directly behind the boat, the sailor must sail "wing on wing," one sail on port the other on starboard. Loss of attention by the helmsperson can lead to an accidental jibe, causing injury to the boat or crew. All on deck must be aware of, and if possible avoid, the potential arc of the boom, mainsheet and other gear in case an accidental jibe occurs during a run. A preventer can be rigged to reduce danger and damage from accidental jibes.
Another technique used while running is "sailing by the lee". Here the main sail is placed on the windward side of the boat, leading to a heightened risk of gybing. With the main placed perpendicular to the boat to windward, and then pulled in slightly, the leech is allowed to act as the leading edge of an airfoil. (Usually, the luff is the leading edge, such as when close-hauled.) This position, though unstable to accidental gybes, allows the sail to generate some force from lift, just as when sailing on a broad-reach. In fact, because there is no mast to generate turbulence around the sail's leading edge (as happens on the broad reach) the lift generated is somewhat stronger than might be expected for such an oblique profile.
Another technique often used by cruisers is to set two head sails, one to port and one to starboard. Depending on the sails, this can often give as much sail area as a spinnaker, but is easier to control. It is also easier to handle than going wing and wing, as the main sail is not set and does not disturb the air flow to the head sails. The main boom then can be rigged as a whisker pole too, to stabilize one of the head sails.
Running is generally the most unstable point of sail, but the easiest for a novice to grasp conceptually, making it a common downfall for beginners. In stronger winds, rolling increases as there is less rolling resistance provided by the sails, as they are eased out. Also, having the sails and boom(s) perpendicular to the boat throws weight and some wind force to that side, making the boat harder to balance. In smaller boats, death rolls can build up and lead to capsize.
Also on a run an inexperienced or inattentive sailor can easily misjudge the real wind strength since the boat speed subtracts directly from the true wind speed and makes the apparent wind less. In addition sea conditions can also falsely seem milder than they are as the waves ahead are being viewed from behind making white caps less apparent. When changing course from this point of sail to a reach or a beat, a sailboat that seemed under control can instantly become over-canvassed and in danger. Any boat over-canvassed on a run can round up, heel excessively and stop suddenly in the water. This is called broaching and it can lead to capsize, possible crew injury and loss of crew into the water.
Options for maneuvering are also reduced. On other points of sail, it is easy to stop or slow the boat by heading into the wind; there may be no such easy way out when running, especially in close quarters or when a spinnaker (including an Asymmetrical spinnaker), whisker pole or preventer are set.
Basic sailing techniques.
Trim.
An important aspect of sailing is keeping the boat in "trim".
Together, these points are known as 'The Five Essentials' and constitute the central aspects of sailing.
Tacking and Jibing.
There are two ways to change from port tack to starboard tack (or vice versa): either by turning the bow through the eye of the wind, "tacking" or the stern, "jibing". In general sailing, tacking is the safer method and preferred especially when sailing upwind; in windsurfing, Jibing is preferred as this involves much less manoeuvring for the sailor.
For general sailing, during such course changes, there is work that needs to be done. Just before tacking the command "Ready about" is given, at which point the crew must man the sheet lines which need to be changed over to the other tack and the helmsperson gets ready. To execute the tack the command "Lee-ho" or "Hard-a-lee" is given. The latter is a direct order to the helmsperson to push the tiller hard to the leeward side of the boat making the bow of the boat come up and quickly turn through the eye of the wind to prevent the boat being caught in irons. As the boat turns through the eye of the wind, some sails such as those with a boom and a single sheet may self-tack and need only small adjustments of sheeting points, but for jibs and other sails with separate sheets on either side, the original sheet must be loosened and the opposite sheet lines hauled in and set quickly and properly for the new point of sail.
Jibing is often necessary to change course when sailing off the wind or downwind. It is a more dangerous manoeuvre because the boom has further to travel (because the sails are let further out to the side of the boat when travelling downwind) in the same amount of time and therefore must be controlled as the sails catch the new wind direction from astern. An uncontrolled jibe can happen suddenly by itself when sailing downwind if the helmsperson is not paying attention to the wind direction and can be very dangerous as the main boom will sweep across the cockpit very quickly and with great force. Before jibing the command "Ready to jibe" is given. The crew gets ready at their positions. If any sails are constrained with preventers or whisker poles these are taken down. The command "Jibe-ho" is given to execute the turn. The boomed sails must be hauled in and made fast before the stern reaches the eye of the wind, so that they are amidship and controlled as the stern passes through the eye of the wind, and then let out quickly under control and adjusted to the new point of sail.
The choice of which strategy to use (coming-about or jibing) depends on the conditions, sail configuration, and the craft. For light craft such as a Hobie Cat (which has little mass) coming into the wind should only be attempted when moving very quickly such as >8 knots. Of course this happens under strong wind. The timing of the crew shift is also critical when coming into the wind. If in light wind, a jibe is the better choice as there's less danger of the wind tipping the boat. A phrase to help remember this is: "light jibe, hard tack" (light/hard referring to wind strength) Of course being caught in irons near shore/structures in strong wind can be catastrohphic.
Reducing sail.
An important safety aspect of sailing is to adjust the amount of sail to suit the wind conditions. As the wind speed increases the crew should progressively reduce the amount of sail. On a small boat with only jib and mainsail this is done by furling the jib and by partially lowering the mainsail, a process called 'reefing the main'.
Reefing means reducing the area of a sail without actually changing it for a smaller sail. Ideally reefing does not only result in a reduced sail area but also in a lower centre of effort from the sails, reducing the heeling moment and keeping the boat more upright.
There are three common methods of reefing the mainsail:
Mainsail furling systems have become increasingly popular on cruising yachts, as they can be operated shorthanded and from the cockpit, in most cases. However, the sail can become jammed in the mast or boom slot if not operated correctly. Mainsail furling is almost never used while racing because it results in a less efficient sail profile. The classical slab-reefing method is the most widely used. Mainsail furling has an additional disadvantage in that its complicated gear may somewhat increase weight aloft. However, as the size of the boat increases, the benefits of mainsail roller furling increase dramatically.
An old saying goes, "The first time you think of reducing sail you should," and correspondingly, "When you think you are ready to take out a reef, have a cup of tea first."
Sail trimming.
The most basic control of the sail consists of setting its angle relative to the wind. The control line that accomplishes this is called a "sheet." If the sheet is too loose the sail will flap in the wind, an occurrence that is called "luffing." Optimum sail angle can be approximated by pulling the sheet in just so far as to make the luffing stop, or by using of tell-tales - small ribbons or yarn attached each side of the sail that both stream horizontally to indicate a properly trimmed sail. Finer controls adjust the overall shape of the sail.
Two or more sails are frequently combined to maximize the smooth flow of air. The sails are adjusted to create a smooth laminar flow over the sail surfaces. This is called the "slot effect". The combined sails fit into an imaginary aerofoil outline, so that the most forward sails are more in line with the wind, whereas the more aft sails are more in line with the course followed. The combined efficiency of this sail plan is greater than the sum of each sail used in isolation.
More detailed aspects include specific control of the sail's shape, e.g.:
Hull trim.
Hull trim is the adjustment of a boat's loading so as to change its fore-and-aft attitude in the water. In small boats, it is done by positioning the crew. In larger boats the weight of a person has less effect on the hull trim, but it can be adjusted by shifting gear, fuel, water, or supplies. Different hull trim efforts are required for different kinds of boats and different conditions. Here are just a few examples: In a lightweight racing dinghy like a Thistle, the hull should be kept level, on its designed water line for best performance in all conditions. In many small boats, weight too far aft can cause drag by submerging the transom, especially in light to moderate winds. Weight too far forward can cause the bow to dig into the waves. In heavy winds, a boat with its bow too low may capsize by pitching forward over its bow (pitch-pole) or dive under the waves (submarine). On a run in heavy winds, the forces on the sails tend to drive a boat's bow down, so the crew weight is moved far aft.
Heeling.
When a ship or boat leans over to one side, from the action of waves or from the centrifugal force of a turn or under wind pressure or from amount of exposed topsides, it is said to 'heel'. A sailing boat that is over-canvassed and therefore heeling, may sail less efficiently depending on fundamental or opportunistic factors such as temporary nature of the feature (e.g. wind gust), use (e.g. racing), crew ability, point of sail, hull size & design.
When a vessel is subject to a heeling force (such as wind pressure), vessel buoyancy & beam of the hull will counter-act the heeling force. A weighted keel provides additional means to right the boat. In some high-performance racing yachts, water ballast or the angle of a canting keel can be changed to provide additional righting force to counteract heeling. The crew may move their personal weight to the high (upwind) side of the boat, this is called "hiking", which also changes the centre of gravity & produces a righting lever to reduce the degree of heeling. Incidental benefits include faster vessel speed caused by more efficient action of the hull & sails. Other options to reduce heeling include reducing exposed sail area & efficiency of the sail setting & a variant of hiking called "trapezing". This can only be done if the vessel is designed for this, as in dinghy sailing. A sailor can (usually involuntarily) try turning upwind in gusts (it is known as "rounding up"). This can lead to difficulties in controlling the vessel if over-canvassed. Wind can be spilled from the sails by 'sheeting out', or loosening them. The number of sails, their size and shape can be altered. Raising the dinghy centreboard can reduce heeling by allowing more leeway.
The increasingly asymmetric underwater shape of the hull matching the increasing angle of heel may generate an increasing directional turning force into the wind. The sails' centre of effort will also increase this turning effect or force on the vessel's motion due to increasing lever effect with increased heeling which shows itself as increased human effort required to steer a straight course. Increased heeling reduces exposed sail area relative to the wind direction, so leading to an equilibrium state. As more heeling force causes more heel, weather helm may be experienced. This condition has a braking effect on the vessel but has the safety effect in that an excessively hard pressed boat will try and turn into the wind therefore reducing the forces on the sail. Small amounts (≤5 degrees) of weather helm are generally considered desirable because of the consequent aerofoil lift effect from the rudder. This aerofoil lift produces helpful motion to windward & the corollary of the reason why lee helm is dangerous. Lee helm, the opposite of weather helm, is generally considered to be dangerous because the vessel turns away from the wind when the helm is released, thus increasing forces on the sail at a time when the helmsperson is not in control.
Sailing hulls and hull shapes.
Sailing boats with one hull are "monohulls", those with two are "catamarans", those with three are "trimarans". A boat is turned by a rudder, which itself is controlled by a tiller or a wheel, while at the same time adjusting the sheeting angle of the sails. Smaller sailing boats often have a stabilizing, raisable, underwater fin called a centreboard, daggerboard, or leeboard; larger sailing boats have a fixed (or sometimes canting) keel. As a general rule, the former are called dinghies, the latter keelboats. However, up until the adoption of the Racing Rules of Sailing, any vessel racing under sail was considered a yacht, be it a multi-masted ship-rigged vessel (such as a sailing frigate), a sailboard (more commonly referred to as a windsurfer) or remote-controlled boat, or anything in between. (See Dinghy sailing.)
Multihulls use flotation and/or weight positioned away from the centre line of the sailboat to counter the force of the wind. This is in contrast to heavy ballast that can account for up to 90% (in extreme cases like AC boats) of the weight of a monohull sailboat. In the case of a standard catamaran, there are two similarly-sized and -shaped slender hulls connected by beams, which are sometimes overlaid by a deck superstructure. Another catamaran variation is the proa. In the case of trimarans, which have an unballasted centre hull similar to a monohull, two smaller amas are situated parallel to the centre hull to resist the sideways force of the wind. The advantage of multihulled sailboats is that they do not suffer the performance penalty of having to carry heavy ballast, and their relatively lesser draft reduces the amount of drag, caused by friction and inertia, when moving through the water.
One of the most common dinghy hulls in the world is the Laser hull. It was designed by Bruce Kirby in 1969 and unveiled at the New York boat show (1971). It was designed with speed and simplicity in mind. The Laser is 13 feet 10.5 inches long and a 12.5 foot water line and 76 sqft of sail.
Types of sails and layouts.
A traditional modern yacht is technically called a "Bermuda sloop" (sometimes a "Bermudan sloop"). A sloop is any boat that has a single mast and usually a single headsail (generally a jib) in addition to the mainsail (Bermuda rig but c.f. Friendship sloop). A cutter (boat) also has a single mast, set further aft than a sloop and more than one headsail. Additionally, Bermuda sloops only have a single sail behind the mast. Other types of sloops are gaff-rigged sloops and lateen sloops. Gaff-rigged sloops have quadrilateral mainsails with a gaff (a small boom) at their upper edge (the "head" of the sail). Gaff-rigged vessels may also have another sail, called a topsail, above the gaff. Lateen sloops have triangular sails with the upper edge attached to a gaff, and the lower edge attached to the boom, and the boom and gaff are attached to each other via some type of hinge. It is also possible for a sloop to be square rigged (having large square sails like a Napoleonic Wars-era ship of the line). Note that a "sloop of war", in the naval sense, may well have more than one mast, and is not properly a sloop by the modern meaning.
If a boat has two masts, it may be a schooner, a ketch, or a yawl, if it is rigged fore-and-aft on all masts. A schooner may have any number of masts provided the second from the front is the tallest (called the "main mast"). In both a ketch and a yawl, the foremost mast is tallest, and thus the main mast, while the rear mast is shorter, and called the mizzen mast. The difference between a ketch and a yawl is that in a ketch, the mizzen mast is forward of the rudderpost (the axis of rotation for the rudder), while a yawl has its mizzen mast behind the rudderpost. In modern parlance, a brigantine is a vessel whose forward mast is rigged with square sails, while her after mast is rigged fore-and-aft. A brig is a vessel with two masts both rigged square.
As one gets into three or more masts the number of combinations rises and one gets barques, barquentines, and full rigged ships.
A spinnaker is a large, full sail that is only used when sailing off wind either reaching or downwind, to catch the maximum amount of wind.
Sailing by high altitude wind power.
SkySails is sailing freighter ships. Speedsailor Dave Culp strongly introduced his OutLeader kite sail for speedsailing. Malcolm Phillips invents an advanced sailing technique using high altitude kites and kytoon.
Rigid foils.
With modern technology, "wings", that is rigid sails, may be used in place of fabric sails. An example of this would be the International C-Class Catamaran Championship and the yacht USA 17 that won the 2010 America's Cup. Such rigid sails are typically made of thin plastic fabric held stretched over a frame. See also AC72 wing-sail catamarans which competed in the 2013 America's Cup.
Alternative wind-powered vessels.
Some non-traditional rigs capture energy from the wind in a different fashion and are capable of feats that traditional rigs are not, such as sailing directly into the wind. One such example is the wind turbine boat, also called the windmill boat, which uses a large windmill to extract energy from the wind, and a propeller to convert this energy to forward motion of the hull. A similar design, called the autogyro boat, uses a wind turbine without the propellor, and functions in a manner similar to a normal sail. A more recent (2010) development is a cart that uses wheels linked to a propeller to "sail" dead downwind at speeds exceeding wind speed.
Kitesurfing and windsurfing.
Kitesurfing and windsurfing are other forms of sailing.
Sailing terminology.
Sailors use traditional nautical terms for the parts of or directions on a vessel: starboard (right), port or larboard (left), forward or fore (front), aft or abaft (rearward), bow (forward part of the hull), stern (aft part of the hull), beam (the widest part). Vertical spars are masts, horizontal spars are booms (if they can hit the sailor), yards, gaffs (if they are too high to reach) or poles (if they cannot hit the sailor).==Further 
Rope and lines.
In most cases, "rope" is the term used only for raw material. Once a section of rope is designated for a particular purpose on a vessel, it generally is called a "line," as in "outhaul line" or "dock line". A very thick line is considered a "cable." Lines that are attached to sails to control their shapes are called "sheets", as in "mainsheet". If a rope is made of wire, it maintains its rope name as in 'wire rope' halyard.
Lines (generally steel cables) that support masts are stationary and are collectively known as a vessel's standing rigging, and individually as "shrouds" or "stays". The stay running forward from a mast to the bow is called the "forestay" or "headstay". Stays running aft are backstays or after stays.
Moveable lines that control sails or other equipment are known collectively as a vessel's running rigging. Lines that raise sails are called "halyards" while those that strike them are called "downhauls". Lines that adjust (trim) the sails are called "sheets". These are often referred to using the name of the sail they control (such as "main sheet", or "jib sheet"). Sail trim may also be controlled with smaller lines attached to the forward section of a boom such as a cunningham; a line used to hold the boom down is called a "vang", or a "kicker" in the United Kingdom. A "topping lift" is used to hold a boom up in the absence of sail tension. "Guys" are used to control the ends of other spars such as spinnaker poles.
Lines used to tie a boat up when alongside are called "docklines", "docking cables" or "mooring warps". In dinghies the single line from the bow is referred to as the "painter". A "rode" is what attaches an anchored boat to its anchor. It may be made of chain, rope, or a combination of the two.
Some lines are referred to as ropes:
Other terms.
Walls are called "bulkheads" or "ceilings", while the surfaces referred to as ceilings on land are called 'overheads'or 'deckheads'. Floors are called 'soles' or "decks". "Broken up" was the fate of a ship that hit a "rocky point" or was simply no longer wanted. The toilet is traditionally called the 'head', the kitchen is the "galley". When lines are tied off, this may be referred to as 'made fast' or 'belayed.' Sails in different sail plans have unchanging names, however. For the naming of sails, see sail-plan.
Knots and line handling.
The tying and untying of knots and hitches as well as the general handling of ropes and lines are fundamental to the art of sailing. The RYA basic 'Start Yachting' syllabus lists the following knots and hitches:
It also lists securing a line around a cleat and the use of winches and jamming cleats.
The RYA Competent Crew syllabus adds the following to the list above, as well as knowledge of the correct use of each: 
In addition it requires competent crewmembers to understand 'taking a turn' around a cleat and to be able to make cleated lines secure. Lines and halyards need to be coiled neatly for stowage and reuse. Dock lines need to be thrown and handled safely and correctly when coming alongside, up to a buoy, and when anchoring, as well as when casting off and getting under way.
Rules and regulations.
Every vessel in coastal and offshore waters is subject to the International Regulations for Preventing Collisions at Sea (the COLREGS). On inland waterways and lakes other similar regulations, such as CEVNI in Europe, may apply. In some sailing events, such as the Olympic Games, which are held on closed courses where no other boating is allowed, specific racing rules such as the Racing Rules of Sailing (RRS) may apply. Often, in club racing, specific club racing rules, perhaps based on RRS, may be "superimposed" onto the more general regulations such as COLREGS or CEVNI.
In general, regardless of the activity, every sailor must
The stand-on vessel must hold a steady course and speed but be prepared to take late avoiding action to prevent an actual collision if the other vessel does not do so in time. The give-way vessel must take early, positive and obvious avoiding action, without crossing ahead of the other vessel.(Rules 16-17)
The COLREGS go on to describe the lights to be shown by vessels under way at night or in restricted visibility. Specifically, for sailing boats, red and green sidelights and a white sternlight are required, although for vessels under 7 metres (23.0 ft) in length, these may be substituted by a torch or white all-round lantern. (Rules 22 & 25)
Sailors are required to be aware not only of the requirements for their own boat, but of all the other lights, shapes and flags that may be shown by other vessels, such as those fishing, towing, dredging, diving etc., as well as sound signals that may be made in restricted visibility and at close quarters, so that they can make decisions under the COLREGS in good time, should the need arise. (Rules 32 - 37)
In addition to the COLREGS, CEVNI and/or any specific racing rules that apply to a sailing boat, there are also
Licensing.
Licensing regulations vary widely across the world. While boating on international waters does not require any license, a license may be required to operate a vessel on coastal waters or inland waters. Some jurisdictions require a license when a certain size is exceeded (e.g., a length of 20 meters), others only require licenses to pilot passenger ships, ferries or tugboats. For example, the European Union issues the International Certificate of Competence, which is required to operate pleasure craft in most inland waterways within the union. The United States in contrast has no licensing, but instead has voluntary certification organizations such as the American Sailing Association. These US certificates are often required to charter a boat, but are not required by any federal or state law.
Sailboat racing.
Sailboat racing generally fits into one of two categories:
Class racing can be further subdivided. Each class has its own set of class rules, and some classes are more restrictive than others.
In a strict one-design class the boats are essentially identical. Examples include the 29er, J/24, Laser, and RS Feva. 
At the other end of the extreme are the development classes based on a box-rule. The box-rule might specify only a few parameters such as maximum length, minimum weight, and maximum sail area, thus allowing creative engineering to develop the fastest boat within the constraints. Examples include the Moth (dinghy), the A Class Catamaran, and the boats used in the America's Cup, Volvo Ocean Race, and Barcelona World Race.
Many classes lie somewhere in between strict one-design and box rule. These classes allows some variation, but the boats are still substantially similar. For instance, both wood and fiberglass hulls are allowed in the Albacore, Wayfarer, and Fireball classes, but the hull shape, weight, and sail area are tightly constrained.
Sailboat racing ranges from single person dinghy racing to large boats with 10 or 20 crew and from small boats costing a few thousand dollars to multi-million dollar America's Cup or Sydney to Hobart Yacht Race campaigns. The costs of participating in the high end large boat competitions make this type of sailing one of the most expensive sports in the world. However, there are inexpensive ways to get involved in sailboat racing, such as at community sailing clubs, classes offered by local recreation organizations and in some inexpensive dinghy and small catamaran classes. Additionally high schools and colleges may offer sailboat racing programs through the Interscholastic Sailing Association (in the USA) and the Intercollegiate Sailing Association (in the USA and some parts of Canada). Under these conditions, sailboat racing can be comparable to or less expensive than sports such as golf and skiing. Sailboat racing is one of the few sports in which people of all ages and genders can regularly compete with and against each other.
Most sailboat and yacht racing is done in coastal or inland waters. However, in terms of endurance and risk to life, ocean races such as the Volvo Ocean Race, the solo VELUX 5 Oceans Race, and the non-stop solo Vendée Globe, rate as some of the most extreme and dangerous sporting events. Not only do participants compete for days with little rest, but an unexpected storm, a single equipment failure, or collision with an ice floe could result in the sailboat being disabled or sunk hundreds or thousands of miles from search and rescue.
The sport of Sailboat racing is governed by the International Sailing Federation, and the rules under which competitors race are the Racing Rules of Sailing, which can be found on the ISAF web site.
Recreational sailing.
Sailing for pleasure can involve short trips across a bay, day sailing, coastal cruising, and more extended offshore or 'blue-water' cruising. These trips can be singlehanded or the vessel may be crewed by families or groups of friends. Sailing vessels may proceed on their own, or be part of a flotilla with other like-minded voyagers. Sailing boats may be operated by their owners, who often also gain pleasure from maintaining and modifying their craft to suit their needs and taste, or may be rented for the specific trip or cruise. A professional skipper and even crew may be hired along with the boat in some cases. People take cruises in which they crew and 'learn the ropes' aboard craft such as tall ships, classic sailing vessels and restored working boats.
Cruising trips of several days or longer can involve a deep immersion in logistics, navigation, meteorology, local geography and history, fishing lore, sailing knowledge, general psychological coping, and serendipity. Once the boat is acquired it is not all that expensive an endeavor, often much less expensive than a normal vacation on land. It naturally develops self-reliance, responsibility, economy, and many other useful skills. Besides improving sailing skills, all the other normal needs of everyday living must also be addressed. There are work roles that can be done by everyone in the family to help contribute to an enjoyable outdoor adventure for all.
A style of casual coastal cruising called gunkholing is a popular summertime family recreational activity. It consists of taking a series of day sails to out of the way places and anchoring overnight while enjoying such activities as exploring isolated islands, swimming, fishing, etc. Many nearby local waters on rivers, bays, sounds, and coastlines can become great natural cruising grounds for this type of recreational sailing. Casual sailing trips with friends and family can become lifetime bonding experiences.
Passagemaking.
Long-distance voyaging, such as that across oceans and between far-flung ports, can be considered the near-absolute province of the cruising sailboat. Most modern yachts of 25–55 feet long, propelled solely by mechanical powerplants, cannot carry the fuel sufficient for a point-to-point voyage of even 250–500 miles without needing to resupply; but a well-prepared sail-powered yacht of similar length is theoretically capable of sailing anywhere its crew is willing to guide it. Even considering that the cost benefits are offset by a much reduced cruising speed, many people traveling distances in small boats come to appreciate the more leisurely pace and increased time spent on the water. Since the solo circumnavigation of Joshua Slocum in the 1890s, long-distance cruising under sail has inspired thousands of otherwise normal people to explore distant seas and horizons. The important voyages of Robin Lee Graham, Eric Hiscock, Don Street and others have shown that, while not strictly racing, ocean voyaging carries with it an inherent sense of competition, especially that between man and the elements. Such a challenging enterprise requires keen knowledge of sailing in general as well as maintenance, navigation (especially celestial navigation), and often even international diplomacy (for which an entire set of protocols should be learned and practiced). But one of the great benefits to sailboat ownership is that one may at least imagine the type of adventure that the average affordable powerboat could never accomplish.

</doc>
<doc id="27675" url="http://en.wikipedia.org/wiki?curid=27675" title="Simple Mail Transfer Protocol">
Simple Mail Transfer Protocol

Simple Mail Transfer Protocol (SMTP) is an Internet standard for electronic mail (e-mail) transmission. First defined by RFC 821 in 1982, it was last updated in 2008 with the Extended SMTP additions by RFC 5321 - which is the protocol in widespread use today.
SMTP by default uses TCP port 25. The protocol for mail submission is the same, but uses port 587. SMTP connections secured by SSL, known as SMTPS, default to port 465 (nonstandard, but sometimes used for legacy reasons).
Although electronic mail servers and other mail transfer agents use SMTP to send and receive mail messages, user-level client mail applications typically use SMTP only for sending messages to a mail server for relaying. For receiving messages, client applications usually use either POP3 or IMAP.
Although proprietary systems (such as Microsoft Exchange and Lotus Notes/Domino) and webmail systems (such as Hotmail, Gmail and Yahoo! Mail) use their own non-standard protocols to access mail box accounts on their own mail servers, all use SMTP when sending or receiving email from outside their own systems.
History.
Various forms of one-to-one electronic messaging were used in the 1960s. People communicated with one another using systems developed for specific mainframe computers. As more computers were interconnected, especially in the US Government's ARPANET, standards were developed to allow users of different systems to e-mail one another. SMTP grew out of these standards developed during the 1970s.
SMTP can trace its roots to two implementations described in 1971: the Mail Box Protocol, whose implementation has been disputed, but is discussed in RFC 196 and other RFCs, and the SNDMSG program, which, according to RFC 2235, Ray Tomlinson of BBN invented for TENEX computers to send mail messages across the ARPANET. Fewer than 50 hosts were connected to the ARPANET at this time.
Further implementations include FTP Mail and Mail Protocol, both from 1973. Development work continued throughout the 1970s, until the ARPANET transitioned into the modern Internet around 1980. Jon Postel then proposed a Mail Transfer Protocol in 1980 that began to remove the mail's reliance on FTP. SMTP was published as RFC 788 in November 1981, also by Postel.
The SMTP standard was developed around the same time as Usenet, a one-to-many communication network with some similarities.
SMTP became widely used in the early 1980s. At the time, it was a complement to Unix to Unix Copy Program (UUCP) mail, which was better suited for handling e-mail transfers between machines that were intermittently connected. SMTP, on the other hand, works best when both the sending and receiving machines are connected to the network all the time. Both use a store and forward mechanism and are examples of push technology. Though Usenet's newsgroups are still propagated with UUCP between servers, UUCP as a mail transport has virtually disappeared along with the "bang paths" it used as message routing headers.
Sendmail, released with 4.1cBSD, right after RFC 788, was one of the first mail transfer agents to implement SMTP. Over time, as BSD Unix became the most popular operating system on the Internet, sendmail became the most common MTA (mail transfer agent). Some other popular SMTP server programs include Postfix, qmail, Novell GroupWise, Exim, Novell NetMail, Microsoft Exchange Server, Sun Java System Messaging Server.
Message submission (RFC 2476) and SMTP-AUTH (RFC 2554) were introduced in 1998 and 1999, both describing new trends in e-mail delivery. Originally, SMTP servers were typically internal to an organization, receiving mail for the organization "from the outside", and relaying messages from the organization "to the outside". But as time went on, SMTP servers (mail transfer agents), in practice, were expanding their roles to become message submission agents for Mail user agents, some of which were now relaying mail "from the outside" of an organization. (e.g. a company executive wishes to send e-mail while on a trip using the corporate SMTP server.) This issue, a consequence of the rapid expansion and popularity of the World Wide Web, meant that SMTP had to include specific rules and methods for relaying mail and authenticating users to prevent abuses such as relaying of unsolicited e-mail (spam). Work on message submission (RFC 2476) was originally started because popular mail servers would often rewrite mail in an attempt to fix problems in it, for example, adding a domain name to an unqualified address. This behavior is helpful when the message being fixed is an initial submission, but dangerous and harmful when the message originated elsewhere and is being relayed. Cleanly separating mail into submission and relay was seen as a way to permit and encourage rewriting submissions while prohibiting rewriting relay. As spam became more prevalent, it was also seen as a way to provide authorization for mail being sent out from an organization, as well as traceability. This separation of relay and submission quickly became a foundation for modern email security practices.
As this protocol started out purely ASCII text-based, it did not deal well with binary files, or characters in many non-English languages. Standards such as Multipurpose Internet Mail Extensions (MIME) were developed to encode binary files for transfer through SMTP. Mail transfer agents (MTAs) developed after Sendmail also tended to be implemented 8-bit-clean, so that the alternate "just send eight" strategy could be used to transmit arbitrary text data (in any 8-bit ASCII-like character encoding) via SMTP. Mojibake was still a problem due to differing character set mappings between vendors, although the email addresses themselves still allowed only ASCII. 8-bit-clean MTAs today tend to support the 8BITMIME extension, permitting binary files to be transmitted almost as easily as plain text. Recently the SMTPUTF8 extension was created to support UTF-8 text, allowing international content and addresses in non-Latin scripts like Cyrillic or Chinese.
Many people contributed to the core SMTP specifications, among them Jon Postel, Eric Allman, Dave Crocker, Ned Freed, Randall Gellens, John Klensin, and Keith Moore.
Mail processing model.
Email is submitted by a mail client (MUA, mail user agent) to a mail server (MSA, mail submission agent) using SMTP on TCP port 587. Most mailbox providers still allow submission on traditional port 25. From there, the MSA delivers the mail to its mail transfer agent (MTA, mail transfer agent). Often, these two agents are just different instances of the same software launched with different options on the same machine. Local processing can be done either on a single machine, or split among various appliances; in the former case, involved processes can share files; in the latter case, SMTP is used to transfer the message internally, with each host configured to use the next appliance as a smart host. Each process is an MTA in its own right; that is, an SMTP server.
The boundary MTA has to locate the target host. It uses the Domain name system (DNS) to look up the mail exchanger record (MX record) for the recipient's domain (the part of the email address on the right of @). The returned MX record contains the name of the target host. The MTA next connects to the exchange server as an SMTP client. (The article on MX record discusses many factors in determining which server the sending MTA connects to.)
Once the MX target accepts the incoming message, it hands it to a mail delivery agent (MDA) for local mail delivery. An MDA is able to save messages in the relevant mailbox format. Again, mail reception can be done using many computers or just one —the picture displays two nearby boxes in either case. An MDA may deliver messages directly to storage, or forward them over a network using SMTP, or any other means, including the Local Mail Transfer Protocol (LMTP), a derivative of SMTP designed for this purpose.
Once delivered to the local mail server, the mail is stored for batch retrieval by authenticated mail clients (MUAs). Mail is retrieved by end-user applications, called email clients, using Internet Message Access Protocol (IMAP), a protocol that both facilitates access to mail and manages stored mail, or the Post Office Protocol (POP) which typically uses the traditional mbox mail file format or a proprietary system such as Microsoft Exchange/Outlook or Lotus Notes/Domino. Webmail clients may use either method, but the retrieval protocol is often not a formal standard.
SMTP defines message "transport", not the message "content". Thus, it defines the mail "envelope" and its parameters, such as the envelope sender, but not the header (except "trace information") nor the body of the message itself. STD 10 and RFC 5321 define SMTP (the envelope), while STD 11 and RFC 5322 define the message (header and body), formally referred to as the Internet Message Format.
Protocol overview.
SMTP is a connection-oriented, text-based protocol in which a mail sender communicates with a mail receiver by issuing command strings and supplying necessary data over a reliable ordered data stream channel, typically a Transmission Control Protocol (TCP) connection. An "SMTP session" consists of commands originated by an SMTP client (the initiating agent, sender, or transmitter) and corresponding responses from the SMTP server (the listening agent, or receiver) so that the session is opened, and session parameters are exchanged. A session may include zero or more SMTP transactions. An "SMTP transaction" consists of three command/reply sequences (see example below.) They are:
Besides the intermediate reply for DATA, each server's reply can be either positive (2xx reply codes) or negative. Negative replies can be permanent (5xx codes) or transient (4xx codes). A reject is a permanent failure by an SMTP server; in this case the SMTP client should send a bounce message. A drop is a positive response followed by message discard rather than delivery.
The initiating host, the SMTP client, can be either an end-user's email client, functionally identified as a mail user agent (MUA), or a relay server's mail transfer agent (MTA), that is an SMTP server acting as an SMTP client, in the relevant session, in order to relay mail. Fully capable SMTP servers maintain queues of messages for retrying message transmissions that resulted in transient failures.
A MUA knows the "outgoing mail" SMTP server from its configuration. An SMTP server acting as client, i.e. "relaying", typically determines which SMTP server to connect to by looking up the MX (Mail eXchange) DNS resource record for each recipient's domain name. Conformant MTAs (not all) fall back to a simple A record in case no MX record can be found. Relaying servers can also be configured to use a smart host.
An SMTP server acting as client initiates a TCP connection to the server on the "well-known port" designated for SMTP: port 25. MUAs should use port 587 to connect to an MSA. The main difference between an MTA and an MSA is that SMTP Authentication is mandatory for the latter only.
SMTP vs mail retrieval.
SMTP is a delivery protocol only. In normal use, mail is "pushed" to a destination mail server (or next-hop mail server) as it arrives. Mail is routed based on the destination server, not the individual user(s) to which it is addressed. Other protocols, such as the Post Office Protocol (POP) and the Internet Message Access Protocol (IMAP) are specifically designed for use by individual users retrieving messages and managing mail boxes. To permit an intermittently-connected mail server to "pull" messages from a remote server on demand, SMTP has a feature to initiate mail queue processing on a remote server (see Remote Message Queue Starting below). POP and IMAP are unsuitable protocols for relaying mail by intermittently-connected machines; they are designed to operate after final delivery, when information critical to the correct operation of mail relay (the "mail envelope") has been removed.
Remote Message Queue Starting.
Remote Message Queue Starting is a feature of SMTP that permits a remote host to start processing of the mail queue on a server so it may receive messages destined to it by sending the TURN command. This feature however was deemed insecure and was extended in RFC 1985 with the ETRN command which operates more securely using an authentication method based on Domain Name System information.
On-Demand Mail Relay.
On-Demand Mail Relay (ODMR) is an SMTP extension standardized in RFC 2645 that allows an intermittently-connected SMTP server to receive email queued for it when it is connected.
Internationalization.
Many users whose native script is not Latin based have had difficulty with the Latin email address requirement. Often this leads to meaningless, but easy to type, locale addresses.
RFC 6531 was created to solve that problem, providing internationalization features for SMTP, the SMTPUTF8 extension. RFC 6531 provides support for multi-byte and non-ASCII characters in email addresses, such as Pelé@live.com (simple diacritic), δοκιμή@παράδειγμα.δοκιμή, and 测试@测试.测试. Current support is limited, but there is strong interest in broad adoption of RFC 6531 and the related RFCs in countries like China that have a large user base where Latin (ASCII) is a foreign script.
Outgoing mail SMTP server.
An e-mail client needs to know the IP address of its initial SMTP server and this has to be given as part of its configuration (usually given as a DNS name). This server will deliver outgoing messages on behalf of the user.
Outgoing mail server access restrictions.
Server administrators need to impose some control on which clients can use the server. This enables them to deal with abuse, for example spam. Two solutions have been in common use:
Restricting access by location.
Under this system, an ISP's SMTP server will not allow access by users who are outside the ISP's network. More precisely, the server may only allow access to users with an IP address provided by the ISP, which is equivalent to requiring that they are connected to the Internet using that same ISP. A mobile user may often be on a network other than that of their normal ISP, and will then find that sending email fails because the configured SMTP server choice is no longer accessible.
This system has several variations. For example, an organisation's SMTP server may only provide service to users on the same network, enforcing this by firewalling to block access by users on the wider Internet. Or the server may perform range checks on the client's IP address. These methods were typically used by corporations and institutions such as universities which provided an SMTP server for outbound mail only for use internally within the organisation. However, most of these bodies now use client authentication methods, as described below.
By restricting access to certain IP addresses, server administrators can readily recognise the IP address of any abuser. As it will be a meaningful address to them, the administrators can deal with the rogue machine or user.
Where a user is mobile, and may use different ISPs to connect to the internet, this kind of usage restriction is onerous, and altering the configured outbound email SMTP server address is impractical. It is highly desirable to be able to use email client configuration information that does not need to change.
Client authentication.
Modern SMTP servers typically require authentication of clients by credentials before allowing access, rather than restricting access by location as described earlier. This more flexible system is friendly to mobile users and allows them to have a fixed choice of configured outbound SMTP server.
Open relay.
A server that is accessible on the wider Internet and does not enforce these kinds of access restrictions is known as an open relay. This is now generally considered a bad practice worthy of blacklisting.
Ports.
Server administrators choose whether clients use TCP port 25 (SMTP) or port 587 (Submission), as formalized in RFC 6409 (previously RFC 2476), for relaying outbound mail to an initial mail server. The specifications and many servers support both. Although some servers support port 465 for legacy "secure SMTP" in violation of the specifications, it is preferable to use standard ports and standard ESMTP commands according to RFC 3207 if a secure session needs to be used between the client and the server.
Some servers are set up to reject all relaying on port 25, but valid users authenticating on port 587 are allowed to relay mail to any valid address.
Some Internet service providers intercept port 25, redirecting traffic to their own SMTP server regardless of the destination address. This means that it is not possible for their users to access an SMTP server outside the ISP's network using port 25.
Some SMTP servers support authenticated access on an additional port other than 587 or 25 to allow users to connect to them even if port 25 is blocked, but 587 is the standardized and widely-supported port for users to submit new mail.
Microsoft Exchange Server 2013 SMTP can listen on ports 25, 587, 465, 475, and 2525, depending on server role and whether roles are combined on a single server. Ports 25 and 587 are used to provide client connectivity to the front end transport service on the client access server (CAS) role. Ports 25, 465, and 475 are used by the mailbox transport service. However, when the mailbox role is combined with the CAS role on a single server, port 2525 is used by the mailbox role for SMTP from the CAS front end transport service, while CAS continues to use port 25. Port 465 is used by the mailbox transport service to receive client connections proxied by the CAS role. Port 475 is used by the mailbox role to communicate directly with other mailbox roles, transferring mail between the mailbox transport submission service and the mailbox transport delivery service.
SMTP transport example.
A typical example of sending a message via SMTP to two mailboxes ("alice" and "theboss") located in the same mail domain ("example.com" or "localhost.com") is reproduced in the following session exchange. (In this example, the conversation parts are prefixed with "S:" and "C:", for "server" and "client", respectively; these labels are not part of the exchange.)
After the message sender (SMTP client) establishes a reliable communications channel to the message receiver (SMTP server), the session is opened with a greeting by the server, usually containing its fully qualified domain name (FQDN), in this case "smtp.example.com". The client initiates its dialog by responding with a codice_1 command identifying itself in the command's parameter with its FQDN (or an address literal if none is available).
 S: 220 smtp.example.com ESMTP Postfix
 C: HELO relay.example.org
 S: 250 Hello relay.example.org, I am glad to meet you
 C: MAIL FROM:<bob@example.org>
 S: 250 Ok
 C: RCPT TO:<alice@example.com>
 S: 250 Ok
 C: RCPT TO:<theboss@example.com>
 S: 250 Ok
 C: DATA
 S: 354 End data with <CR><LF>.<CR><LF>
 C: From: "Bob Example" <bob@example.org>
 C: To: "Alice Example" <alice@example.com>
 C: Cc: theboss@example.com
 C: Date: Tue, 15 January 2008 16:02:43 -0500
 C: Subject: Test message
 C:
 C: Hello Alice.
 C: This is a test message with 5 header fields and 4 lines in the message body.
 C: Your friend,
 C: Bob
 C: .
 S: 250 Ok: queued as 12345
 C: QUIT
 S: 221 Bye
The client notifies the receiver of the originating email address of the message in a codice_2 command. In this example, the email message is sent to two mailboxes on the same SMTP server: one for each recipient listed in the To and Cc header fields. The corresponding SMTP command is codice_3. Each successful reception and execution of a command is acknowledged by the server with a result code and response message (e.g., 250 Ok).
The transmission of the body of the mail message is initiated with a codice_4 command after which it is transmitted verbatim line by line and is terminated with an end-of-data sequence. This sequence consists of a new-line (<CR><LF>), a single full stop (period), followed by another new-line. Since a message body can contain a line with just a period as part of the text, the client sends "two" periods every time a line starts with a period; correspondingly, the server replaces every sequence of two periods at the beginning of a line with a single one. Such escaping method is called "dot-stuffing".
The server's positive reply to the end-of-data, as exemplified, implies that the server has taken the responsibility of delivering the message. A message can be doubled if there is a communication failure at this time, e.g. due to a power shortage: Until the sender has received that 250 reply, it must assume the message was not delivered. On the other hand, after the receiver has decided to accept the message, it must assume the message has been delivered to it. Thus, during this time span, both agents have active copies of the message that they will try to deliver. The probability that a communication failure occurs exactly at this step is directly proportional to the amount of filtering that the server performs on the message body, most often for anti-spam purposes. The limiting timeout is specified to be 10 minutes.
The codice_5 command ends the session. If the email has other recipients located elsewhere, the client would codice_5 and connect to an appropriate SMTP server for subsequent recipients after the current destination(s) had been queued. The information that the client sends in the codice_1 and codice_2 commands are added (not seen in example code) as additional header fields to the message by the receiving server. It adds a codice_9 and codice_10 header field, respectively.
Some clients are implemented to close the connection after the message is accepted (codice_11), so the last two lines may actually be omitted. This causes an error on the server when trying to send the codice_12 reply.
Optional extensions.
Although optional and not shown in this example, many clients ask the server for the SMTP extensions that the server supports, by using the codice_13 greeting of the Extended SMTP specification (RFC 1870). Clients fall back to codice_1 only if the server does not respond to codice_13.
Modern clients may use the ESMTP extension keyword codice_16 to query the server for the maximum message size that will be accepted. Older clients and servers may try to transfer excessively sized messages that will be rejected after consuming network resources, including connect time to network links that is paid by the minute.
Users can manually determine in advance the maximum size accepted by ESMTP servers. The client replaces the codice_1 command with the codice_13 command.
 S: 220 smtp2.example.com ESMTP Postfix
 C: EHLO bob.example.org
 S: 250-smtp2.example.com Hello bob.example.org [192.0.2.201]
 S: 250-SIZE 14680064
 S: 250-PIPELINING
 S: 250 HELP
Thus "smtp2.example.com" declares that it will accept a fixed maximum message size no larger than 14,680,064 octets (8-bit bytes). Depending on the server's actual resource usage, it may be currently unable to accept a message this large.
In the simplest case, an ESMTP server will declare a maximum codice_16 immediately after receiving an codice_13. According to RFC 1870, however, the numeric parameter to the codice_16 extension in the codice_13 response is optional. Clients may instead, when issuing a codice_2 command, include a numeric estimate of the size of the message they are transferring, so that the server can refuse receipt of overly-large messages.
Security and spamming.
The original SMTP specification did not include a facility for authentication of senders. Subsequently, the SMTP-AUTH extension was defined by RFC 2554. ESMTP provides a mechanism for email clients to specify a security mechanism to a mail server, authenticate the exchange, and negotiate a security profile (Simple Authentication and Security Layer, SASL) for subsequent message transfers.
Microsoft products implement the proprietary Secure Password Authentication (SPA) protocol through the use of the SMTP-AUTH extension.
However, the impracticality of widespread SMTP-AUTH implementation and management means that E-mail spamming is not and cannot be addressed by it.
Modifying SMTP extensively, or replacing it completely, is not believed to be practical, due to the network effects of the huge installed base of SMTP. Internet Mail 2000 was one such proposal for replacement.
Spam is enabled by several factors, including vendors implementing MTAs that are not standards-compliant, which make it difficult for other MTAs to enforce standards, security vulnerabilities within the operating system (often exacerbated by always-on broadband connections) that allow spammers to remotely control end-user PCs and cause them to send spam, and a lack of "intelligence" in many MTAs.
There are a number of proposals for sideband protocols that will assist SMTP operation. The Anti-Spam Research Group (ASRG) of the Internet Research Task Force (IRTF) is working on a number of Email authentication and other proposals for providing simple source authentication that is flexible, lightweight, and scalable. Recent Internet Engineering Task Force (IETF) activities include MARID (2004) leading to two approved IETF experiments in 2005, and DomainKeys Identified Mail in 2006.
In 2012, a group of organizations proposed a new specification, called DMARC to reduce email abuse and spoofing.

</doc>
<doc id="27676" url="http://en.wikipedia.org/wiki?curid=27676" title="Shuttlecock">
Shuttlecock

A shuttlecock (previously called Shuttlecork) (also called a bird or birdie) is a high-drag projectile used in the sport of badminton. It has an open conical shape: the cone is formed from 16 or so overlapping feathers, usually goose or duck, embedded into a rounded cork base. The cork is covered with thin leather. The shuttlecock's shape makes it extremely aerodynamically stable. Regardless of initial orientation, it will turn to fly cork first, and remain in the cork-first orientation. The name "shuttlecock" is frequently shortened to shuttle. The "shuttle" part of the name was probably derived from its back-and-forth motion during the game, resembling the shuttle of a loom; the "cock" part of the name was probably derived from the resemblance of the feathers to those on a cockerel.
Feathered vs. synthetic shuttlecocks.
The feathers are brittle; shuttlecocks break easily and often need to be replaced several times during a game. For this reason, synthetic shuttlecocks have been developed that replace the feathers with a plastic skirt. Players often refer to synthetic shuttlecocks as "plastics" and feathered shuttlecocks as "feathers". 
The cost of good quality feathers is similar to that of good quality plastics, but plastics are far more durable, typically lasting many matches without any impairment to their flight. 
Most experienced and skillful players greatly prefer feathers, and serious tournaments or leagues are always played using feather shuttlecocks of the highest quality. Experienced players generally prefer the "feel" of feathered shuttlecocks and assert that they are able to control the flight of feathers better than of plastics. In Asia, where feather shuttlecocks are more affordable than in Europe and North America, plastic shuttlecocks are hardly used at all.
The playing characteristics of plastics and feathers are substantially different. Plastics fly more slowly on initial impact, but slow down less towards the end of their flight. While feathers tend to drop straight down on a clear shot, plastics never quite return to a straight drop, falling more on a diagonal. Feather shuttles may come off the strings at speeds in excess of 320 km/h (200 mph) but slow down faster as they drop. For this reason, the feather shuttle makes the game seem faster, but also allows more time to play strokes. Because feather shuttles fly more quickly off the racquet face they also tend to cause less shoulder impact and injury.
Specifications.
A shuttlecock weighs around 4.75 to. It has 16 feathers with each feather 70 mm in length. The diameter of the cork is 25 to and the diameter of the circle that the feathers make is around 54 mm.

</doc>
<doc id="27679" url="http://en.wikipedia.org/wiki?curid=27679" title="Soldering iron">
Soldering iron

A soldering iron is a hand tool used in soldering. It supplies heat to melt solder so that it can flow into the joint between two workpieces.
A soldering iron is composed of a heated metal tip and an insulated handle. Heating is often achieved electrically, by passing an electric current (supplied through an electrical cord or battery cables) through a resistive heating element. Cordless irons can be heated by combustion of gas stored in a small tank, often using a catalytic heater rather than a flame. Simple irons less commonly used than in the past were simply a large copper bit on a handle, heated in a flame.
Soldering irons are most often used for installation, repairs, and limited production work in electronics assembly. High-volume production lines use other soldering methods. Large irons may be used for soldering joints in sheet metal objects. Less common uses include pyrography (burning designs into wood) and plastic welding.
Types.
Simple iron.
For electrical and electronics work, a low-power iron, a power rating between 15 and 35 watts, is used. Higher ratings are available, but do not run at higher temperature; instead there is more heat available for making soldered connections to things with large thermal capacity, for example, a metal chassis. Some irons are temperature-controlled, running at a fixed temperature in the same way as a soldering station, with higher power available for joints with large heat capacity. Simple irons run at an uncontrolled temperature determined by thermal equilibrium; when heating something large their temperature drops a little, possibly too much to melt solder.
Cordless iron.
Small irons heated by a battery, or by combustion of a gas such as butane in a small self-contained tank, can be used when electricity is unavailable or cordless operation is required. The operating temperature of these irons is not regulated directly; gas irons may change power by adjusting gas flow. Gas-powered irons may have interchangeable tips including different size soldering tips, hot knife for cutting plastics, miniature blow-torch with a hot flame, and small hot air blower for such applications as shrinking heat shrink tubing.
Temperature-controlled soldering iron.
Simple irons reach a temperature determined by thermal equilibrium, dependent upon power input and cooling by the environment and the materials it comes into contact with. The iron temperature will drop when in contact with a large mass of metal such as a chassis; a small iron will lose too much temperature to solder a large connection. More advanced irons for use in electronics have a mechanism with a temperature sensor and method of temperature control to keep the tip temperature steady; more power is available if a connection is large. Temperature-controlled irons may be free-standing, or may comprise a head with heating element and tip, controlled by a base called a soldering station, with control circuitry and temperature adjustment and sometimes display.
A variety of means are used to control temperature. The simplest of these is a variable power control, much like a light dimmer, which changes the equilibrium temperature of the iron without automatically measuring or regulating the temperature. Another type of system uses a thermostat, often inside the iron's tip, which automatically switches power on and off to the element. A thermal sensor such as a thermocouple may be used in conjunction with circuitry to monitor the temperature of the tip and adjust power delivered to the heating element to maintain a desired temperature.
Another approach is to use magnetized soldering tips which lose their magnetic properties at a specific temperature, the Curie point. As long as the tip is magnetic, it closes a switch to supply power to the heating element. When it exceeds the design temperature it opens the contacts, cooling until the temperature drops enough to restore magnetisation. More complex Curie-point irons circulate a high-frequency AC current through the tip, using magnetic physics to direct heating only where the surface of the tip drops below the Curie point.
Soldering station.
A soldering station, invariably temperature-controlled, consists of an electrical power supply, control circuitry with provision for user adjustment of temperature and display, and a soldering iron or soldering head with a tip temperature sensor. The station will normally have a stand for the hot iron when not in use, and a wet sponge for cleaning. It is most commonly used for soldering electronic components. Other functions may be combined; for example a rework station, mainly for surface-mount components may have a hot air gun, vacuum pickup tool, and a soldering head; a desoldering station will have a desoldering head with vacuum pump for desoldering through-hole components, and a soldering iron head.
Soldering tweezers.
For soldering and desoldering small surface-mount components with two terminals, such as some links, resistors, capacitors, and diodes, soldering tweezers can be used; they can be either free-standing or controlled from a soldering station. The tweezers have two heated tips mounted on arms whose separation can be manually varied by squeezing gently against spring force, like simple tweezers; the tips are applied to the two ends of the component. The main purpose of the soldering tweezers is to melt solder in the correct place; components are usually moved by simple tweezers or vacuum pickup.
Stands.
A soldering iron stand keeps the iron away from flammable materials, and often also comes with a cellulose sponge and flux pot for cleaning the tip. Some soldering irons for continuous and professional use come as part of a "soldering station," which allows the exact temperature of the tip to be adjusted, kept constant, and sometimes displayed.
Tips.
Most soldering irons for electronics have interchangeable tips, also known as "bits", that vary in size and shape for different types of work. Pyramid tips with a triangular flat face and chisel tips with a wide flat face are useful for soldering sheet metal. Fine conical or tapered chisel tips are typically used for electronics work. Tips may be straight or have a bend. Concave or wicking tips with a chisel face with a concave well in the flat face to hold a small amount of solder are available. Tip selection depends upon the type of work and access to the joint; soldering of 0.5mm pitch surface-mount ICs, for example, is quite different from soldering a through-hole connection to a large area. A concave tip well is said to help prevent bridging of closely spaced leads; different shapes are recommended to correct bridging that has occurred. Due to patent restrictions not all manufacturers offer concave tips everywhere; in particular there are restrictions in the USA.
Older and very cheap irons typically use a bare copper tip, which is shaped with a file or sandpaper. This dissolves gradually into the solder, suffering pitting and erosion of the shape. Copper tips are sometimes filed when worn down. Iron-plated copper tips have become increasingly popular since the 1980s. Because iron is not readily dissolved by molten solder, the plated tip is more durable than a bare copper one, though it will eventually wear out and need replacing. This is especially important when working at the higher temperatures needed for modern lead-free solders. Solid iron and steel tips are seldom used because they store less heat, and rusting can break the heating element.
Cleaning.
When the iron tip oxidises and burnt flux accumulates on it, solder no longer wets the tip, impeding heat transfer and making soldering difficult or impossible; tips must be periodically cleaned in use. Such problems happen with all kinds of solder, but are much more severe with the lead-free solders which have become widespread in electronics work, which require higher temperatures than solders containing lead. Exposed iron plating oxidises; if the tip is kept tinned with molten solder oxidation is inhibited. A clean unoxidised tip is tinned by applying a little solder and flux.
A wetted small sponge, often supplied with soldering equipment, can be used to wipe the tip. For lead-free solder a slightly more aggressive cleaning, with brass shavings, can be used. Soldering flux will help to remove oxide; the more active the flux the better the cleaning, although acidic flux used on circuit boards and not carefully cleaned off will cause corrosion. A tip which is cleaned but not retinned is susceptible to oxidation, particularly if wet.
Soldering iron tips are made of copper core plated with iron. The copper is used for heat transfer and the iron plating is used for durability. Copper is very easily corroded, eating away the tip, particularly in lead-free work; iron is not. Cleaning tips requires the removal of oxide without damaging the iron plating and exposing the copper to rapid corrosion. The use of solder already containing a small amount of copper can slow corrosion of copper tips.
In cases of severe oxidation not removable by gentler methods, abrasion with something hard enough to remove oxide but not so hard as to scratch the iron plating can be used. A brass wire scourer, brush, or wheel on a bench grinder, can be used with care. Sandpaper and other tools may be used but are likely to damage the plating.

</doc>
<doc id="27680" url="http://en.wikipedia.org/wiki?curid=27680" title="Supernova">
Supernova

A supernova is a stellar explosion that briefly outshines an entire galaxy, radiating as much energy as the Sun or any ordinary star is expected to emit over its entire life span, before fading from view over several weeks or months. The extremely luminous burst of radiation expels much or all of a star's material at a velocity of up to (10% of the speed of light), driving a shock wave into the surrounding interstellar medium. This shock wave sweeps up an expanding shell of gas and dust called a supernova remnant. Supernovae are potentially strong galactic sources of gravitational waves. A great proportion of primary cosmic rays comes from supernovae.
Supernovae are more energetic than a nova. "Nova" means "new" in Latin, referring to what appears to be a very bright new star shining in the celestial sphere; the prefix "super-" distinguishes supernovae from ordinary novae, which are far less luminous. The word "supernova" was coined by Walter Baade and Fritz Zwicky in 1931. It is pronounced with the plural supernovae or supernovas (abbreviated SN, plural SNe after "supernovae").
Supernovae can be triggered in one of two ways: by the sudden re-ignition of nuclear fusion in a degenerate star; or by the gravitational collapse of the core of a massive star. In the first case, a degenerate white dwarf may accumulate sufficient material from a companion, either through accretion or via a merger, to raise its core temperature, ignite carbon fusion, and trigger runaway nuclear fusion, completely disrupting the star. In the second case, the core of a massive star may undergo sudden gravitational collapse, releasing gravitational potential energy that can create a supernova explosion.
The last directly observed supernova in the Milky Way was Kepler's Star of 1604 (SN 1604); remnants of two more recent supernovae have been found retrospectively. Observations in other galaxies indicate that supernovae should occur on average about three times every century in the Milky Way, and that any galactic supernova would almost certainly be observable in modern astronomical equipment. They play a significant role in enriching the interstellar medium with higher mass elements. Furthermore, the expanding shock waves from supernova explosions can trigger the formation of new stars.
Observation history.
Hipparchus' interest in the fixed stars may have been inspired by the observation of a supernova (according to Pliny). The earliest recorded supernova, SN 185, was viewed by Chinese astronomers in 185 AD. The brightest recorded supernova was the SN 1006, which was described in detail by Chinese and Islamic astronomers. The widely observed supernova SN 1054 produced the Crab Nebula. Supernovae SN 1572 and SN 1604, the latest to be observed with the naked eye in the Milky Way galaxy, had notable effects on the development of astronomy in Europe because they were used to argue against the Aristotelian idea that the universe beyond the Moon and planets was immutable. Johannes Kepler began observing SN 1604 at its peak on October 17, 1604, and continued to make estimates of its brightness until it faded from naked eye view a year later. It was the second supernova to be observed in a generation (after SN 1572 seen by Tycho Brahe in Cassiopeia).
Since the development of the telescope, the field of supernova discovery has extended to other galaxies, starting with the 1885 observation of supernova S Andromedae in the Andromeda galaxy. American astronomers Rudolph Minkowski and Fritz Zwicky developed the modern supernova classification scheme beginning in 1941. In the 1960s, astronomers found that the maximum intensities of supernova explosions could be used as standard candles, hence indicators of astronomical distances. Some of the most distant supernovae recently observed appeared dimmer than expected. This supports the view that the expansion of the universe is accelerating. Techniques were developed for reconstructing supernova explosions that have no written records of being observed. The date of the Cassiopeia A supernova event was determined from light echoes off nebulae, while the age of supernova remnant RX J0852.0-4622 was estimated from temperature measurements and the gamma ray emissions from the decay of titanium-44. In 2009, nitrates were discovered in Antarctic ice deposits that matched the times of past supernova events.
Discovery.
Early work on what was originally believed to be simply a new category of novae was performed during the 1930s by Walter Baade and Fritz Zwicky at Mount Wilson Observatory. The name "super-novae" was first used during 1931 lectures held at Caltech by Baade and Zwicky, then used publicly in 1933 at a meeting of the American Physical Society. By 1938, the hyphen had been lost and the modern name was in use. Because supernovae are relatively rare events within a galaxy, occurring about three times a century in the Milky Way, obtaining a good sample of supernovae to study requires regular monitoring of many galaxies.
Supernovae in other galaxies cannot be predicted with any meaningful accuracy. Normally, when they are discovered, they are already in progress. Most scientific interest in supernovae—as standard candles for measuring distance, for example—require an observation of their peak luminosity. It is therefore important to discover them well before they reach their maximum. Amateur astronomers, who greatly outnumber professional astronomers, have played an important role in finding supernovae, typically by looking at some of the closer galaxies through an optical telescope and comparing them to earlier photographs.
Toward the end of the 20th century astronomers increasingly turned to computer-controlled telescopes and CCDs for hunting supernovae. While such systems are popular with amateurs, there are also professional installations such as the Katzman Automatic Imaging Telescope. Recently the Supernova Early Warning System (SNEWS) project has begun using a network of neutrino detectors to give early warning of a supernova in the Milky Way galaxy. Neutrinos are particles that are produced in great quantities by a supernova explosion, and they are not significantly absorbed by the interstellar gas and dust of the galactic disk.
Supernova searches fall into two classes: those focused on relatively nearby events and those looking for explosions farther away. Because of the expansion of the universe, the distance to a remote object with a known emission spectrum can be estimated by measuring its Doppler shift (or redshift); on average, more distant objects recede with greater velocity than those nearby, and so have a higher redshift. Thus the search is split between high redshift and low redshift, with the boundary falling around a redshift range of "z" = 0.1–0.3—where "z" is a dimensionless measure of the spectrum's frequency shift.
High redshift searches for supernovae usually involve the observation of supernova light curves. These are useful for standard or calibrated candles to generate Hubble diagrams and make cosmological predictions. Supernova spectroscopy, used to study the physics and environments of supernovae, is more practical at low than at high redshift. Low redshift observations also anchor the low-distance end of the Hubble curve, which is a plot of distance versus redshift for visible galaxies. (See also Hubble's law).
Naming convention.
Supernova discoveries are reported to the International Astronomical Union's Central Bureau for Astronomical Telegrams, which sends out a circular with the name it assigns to that supernova. The name is the marker "SN" followed by the year of discovery, suffixed with a one or two-letter designation. The first 26 supernovae of the year are designated with a capital letter from "A" to "Z". Afterward pairs of lower-case letters are used: "aa", "ab", and so on. Hence, for example, "SN 2003C" designates the third supernova reported in the year 2003. The last supernova of 2005 was SN 2005nc, indicating that it was the 367th supernova found in 2005. Since 2000, professional and amateur astronomers have been finding several hundreds of supernovae each year (572 in 2007, 261 in 2008, 390 in 2009; 231 in 2013).
Historical supernovae are known simply by the year they occurred: SN 185, SN 1006, SN 1054, SN 1572 (called "Tycho's Nova") and SN 1604 ("Kepler's Star"). Since 1885 the additional letter notation has been used, even if there was only one supernova discovered that year (e.g. SN 1885A, SN 1907A, etc.) — this last happened with SN 1947A. "SN", for SuperNova, is a standard prefix. Until 1987, two-letter designations were rarely needed; since 1988, however, they have been needed every year.
Classification.
As part of the attempt to understand supernovae, astronomers have classified them according to their light curves and the absorption lines of different chemical elements that appear in their spectra. The first element for division is the presence or absence of a line caused by hydrogen. If a supernova's spectrum contains lines of hydrogen (known as the Balmer series in the visual portion of the spectrum) it is classified "Type II"; otherwise it is "Type I". In each of these two types there are subdivisions according to the presence of lines from other elements or the shape of the light curve (a graph of the supernova's apparent magnitude as a function of time).
Type I.
The type I supernovae are subdivided on the basis of their spectra, with type Ia showing a strong ionised silicon absorption line. Type I supernovae without this strong line are classified as types Ib and Ic, with type Ib showing strong neutral helium lines and type Ic lacking them. The light curves are all similar, although type Ia are generally brighter at peak luminosity, but the light curve is not important for classification of type I supernovae.
A small number of type Ia supernovae exhibit unusual features such as non-standard luminosity or broadened light curves, and these are typically classified by referring to the earliest example showing similar features. For example the sub-luminous SN 2008ha is often referred to as SN 2002cx-like or class Ia-2002cx.
Type II.
The supernovae of Type II can also be sub-divided based on their spectra. While most Type II supernovae show very broad emission lines which indicate expansion velocities of many thousands of kilometres per second, some, such as SN 2005gl, have relatively narrow features in their spectra. These are called Type IIn, where the 'n' stands for 'narrow'.
A few supernovae, such as SN 1987K and SN 1993J, appear to change types: they show lines of hydrogen at early times, but, over a period of weeks to months, become dominated by lines of helium. The term "Type IIb" is used to describe the combination of features normally associated with Types II and Ib.
Type II supernovae with normal spectra dominated by broad hydrogen lines that remain for the life of the decline are classified on the basis of their light curves. The most common type shows a distinctive "plateau" in the light curve shortly after peak brightness where the visual luminosity stays relatively constant for several months before the decline resumes. These are called type II-P referring to the plateau. Less common are type II-L supernovae that lack a distinct plateau. The "L" signifies "linear" although the light curve is not actually a straight line.
Supernovae that do not fit into the normal classifications are designated peculiar, or 'pec'.
Types III, IV, and V.
Fritz Zwicky defined additional supernovae types, although based on a very few examples that didn't cleanly fit the parameters for a type I or type II supernova. SN 1961i in NGC 4303 was the prototype and only member of the type III supernova class, noted for its broad light curve maximum and broad hydrogen Balmer lines that were slow to develop in the spectrum. SN 1961f in NGC 3003 was the prototype and only member of the type IV class, with a light curve similar to a type II-P supernova, with hydrogen absorption lines but weak hydrogen emission lines. The type V class was coined for SN 1961V in NGC 1058, an unusual faint supernova or supernova imposter with a slow rise to brightness, a maximum lasting many months, and an unusual emission spectrum. The similarity to of SN 1961V to the Eta Carinae Great Outburst was noted. Supernovae in M101 (1909) and M83 (1923 and 1957) were also suggested as possible type IV or type V supernovae.
These types would now all be treated as peculiar type II supernovae, of which many more examples have been discovered, although it is still debated whether SN 1961V was a true supernova following an LBV outburst or an imposter.
Current models.
The type codes described above that astronomers give to supernovae are "taxonomic" in nature: the type number describes the light observed from the supernova, not necessarily its cause. For example, type Ia supernovae are produced by runaway fusion ignited on degenerate white dwarf progenitors while the spectrally similar type Ib/c are produced from massive Wolf-Rayet progenitors by core collapse. The following summarizes what astronomers currently believe are the most plausible explanations for supernovae.
Thermal runaway.
A white dwarf star may accumulate sufficient material from a stellar companion to raise its core temperature enough to ignite carbon fusion, at which point it undergoes runaway nuclear fusion, completely disrupting it. There are three avenues by which this detonation is theorized to happen: stable accretion of material from a companion, the collision of two white dwarfs, or accretion that causes ignition in a shell that then ignites. The dominant mechanism by which Type Ia supernovae are produced remains unclear. Despite this uncertainty in how Type Ia supernovae are produced, Type Ia supernovae have very uniform properties, and are useful standard candles over intergalactic distances. Some calibrations are required to compensate for the gradual change in properties or different frequencies of abnormal luminosity supernovae at high red shift, and for small variations in brightness identified by light curve shape or spectrum.
Normal Type Ia.
There are several means by which a supernova of this type can form, but they share a common underlying mechanism. If a carbon-oxygen white dwarf accreted enough matter to reach the Chandrasekhar limit of about 1.44 solar masses (M☉) (for a non-rotating star), it would no longer be able to support the bulk of its plasma through electron degeneracy pressure and would begin to collapse. However, the current view is that this limit is not normally attained; increasing temperature and density inside the core ignite carbon fusion as the star approaches the limit (to within about 1%), before collapse is initiated.
Within a few seconds, a substantial fraction of the matter in the white dwarf undergoes nuclear fusion, releasing enough energy (1–2 × 1044 joules) to unbind the star in a supernova explosion. An outwardly expanding shock wave is generated, with matter reaching velocities on the order of 5,000–20,000 km/s, or roughly 3% of the speed of light. There is also a significant increase in luminosity, reaching an absolute magnitude of −19.3 (or 5 billion times brighter than the Sun), with little variation.
The model for the formation of this category of supernova is a closed binary star system. The larger of the two stars is the first to evolve off the main sequence, and it expands to form a red giant. The two stars now share a common envelope, causing their mutual orbit to shrink. The giant star then sheds most of its envelope, losing mass until it can no longer continue nuclear fusion. At this point it becomes a white dwarf star, composed primarily of carbon and oxygen. Eventually the secondary star also evolves off the main sequence to form a red giant. Matter from the giant is accreted by the white dwarf, causing the latter to increase in mass. Despite widespread acceptance of the basic model, the exact details of initiation and of the heavy elements produced in the explosion are still unclear.
Type Ia supernovae follow a characteristic light curve—the graph of luminosity as a function of time—after the explosion. This luminosity is generated by the radioactive decay of nickel-56 through cobalt-56 to iron-56. The peak luminosity of the light curve is extremely consistent across normal Type Ia supernovae, having a maximum absolute magnitude of about −19.3. This allows them to be used as a secondary standard candle to measure the distance to their host galaxies.
Non-standard Type Ia.
Another model for the formation of a Type Ia explosion involves the merger of two white dwarf stars, with the combined mass momentarily exceeding the Chandrasekhar limit. There is much variation in this type of explosion, and in many cases there may be no supernova at all, but it is expected that they will have a broader and less luminous light curve than the more normal type Ia explosions.
Abnormally bright type Ia supernovae are expected when the white dwarf already has a mass higher than the Chandrasekhar limit, possibly enhanced further by asymmetry, but the ejected material will have less than normal kinetic energy.
There is no formal sub-classification for the non-standard type Ia supernovae. It has been proposed that a group of sub-luminous supernovae that occur when helium accretes onto a white dwarf should be classified as type Iax. This type of supernova may not always completely destroy the white dwarf progenitor and could leave behind a zombie star.
One specific type of non-standard type Ia supernova develops hydrogen, and other, emission lines and gives the appearance of mixture between a normal type Ia and a type IIn supernova. Examples are SN 2002ic and SN 2005gj. These supernova have been dubbed type Ia/IIn, type Ian, type IIa and type IIan.
Core collapse.
Very massive stars can undergo core collapse when nuclear fusion suddenly becomes unable to sustain the core against its own gravity; this is the cause of all types of supernova except type Ia. The collapse may cause violent expulsion of the outer layers of the star resulting in a supernova, or the release of gravitational potential energy may be insufficient and the star may collapse into a black hole or neutron star with little radiated energy.
Core collapse can be caused by several different mechanisms: electron capture; exceeding the Chandrasekhar limit; pair-instability; or photodisintegration. When a massive star develops an iron core larger than the Chandrasekhar mass it will no longer be able to support itself by electron degeneracy pressure and will collapse further to a neutron star or black hole. Electron capture by magnesium in a degenerate O/Ne/Mg core causes gravitational collapse followed by explosive oxygen fusion, with very similar results. Electron-positron pair production in a large post-helium burning core removes thermodynamic support and causes initial collapse followed by runaway fusion, resulting in a pair-instability supernova. A sufficiently large and hot stellar core may generate gamma-rays energetic enough to initiate photodisintegration directly, which will cause a complete collapse of the core.
The table below lists the known reasons for core collapse in massive stars, the types of star that they occur in, their associated supernova type, and the remnant produced. The metallicity is the proportion of elements other than hydrogen or helium, as compared to the Sun. The initial mass is the mass of the star prior to the supernova event, given in multiples of the Sun's mass, although the mass at the time of the supernova may be much lower.
Type IIn supernovae are not listed in the table. They can potentially be produced by various types of core collapse in different progenitor stars, possibly even by type Ia white dwarf ignitions, although it seems that most will be from iron core collapse in luminous supergiants or hypergiants (including LBVs). The narrow spectral lines for which they are named occur because the supernova is expanding into a small dense cloud of circumstellar material. It appears that a significant proportion of supposed type IIn supernovae are actually supernova imposters, massive eruptions of LBV-like stars similar to the Great Eruption Eta Carinae. In these events, material previously ejected from the star creates the narrow absorption lines and causes a shock wave through interaction with the newly ejected material.
When a stellar core is no longer supported against gravity it collapses in on itself with velocities reaching 70,000 km/s (0.23c), resulting in a rapid increase in temperature and density. What follows next depends on the mass and structure of the collapsing core, with low mass degenerate cores forming neutron stars, higher mass degenerate cores mostly collapsing completely to black holes, and non-degenerate cores undergoing runaway fusion.
The initial collapse of degenerate cores is accelerated by beta decay, photodisintegration and electron capture, which causes a burst of electron neutrinos. As the density increases, neutrino emission is cut off as they become trapped in the core. The inner core eventually reaches typically 30 km diameter and a density comparable to that of an atomic nucleus, and neutron degeneracy pressure tries to halt the collapse. If the core mass is more than about 15 M☉ then neutron degeneracy is insufficient to stop the collapse and a black hole forms directly with no supernova explosion.
In lower mass cores the collapse is stopped and the newly formed neutron core has an initial temperature of about 100 billion kelvin, 6000 times the temperature of the sun's core. 'Thermal' neutrinos form as neutrino-antineutrino pairs of all flavors, and total several times the number of electron-capture neutrinos. About 1046 joules, approximately 10% of the star's rest mass, is converted into a ten-second burst of neutrinos which is the main output of the event. The suddenly halted core collapse rebounds and produces a shock wave that stalls within milliseconds in the outer core as energy is lost through the dissociation of heavy elements. A process that is not clearly understood[ [update]] is necessary to allow the outer layers of the core to reabsorb around 1044 joules (1 foe) from the neutrino pulse, producing the visible explosion, although there are also other theories on how to power the explosion.
Some material from the outer envelope falls back onto the neutron star, and for cores beyond about 8 M☉ there is sufficient fallback to form a black hole. This fallback will reduce the kinetic energy of the explosion and the mass of expelled radioactive material, but in some situations it may also generate relativistic jets that result in a gamma-ray burst or an exceptionally luminous supernova.
Collapse of massive non-degenerate cores will ignite further fusion. When the core collapse is initiated by pair instability, oxygen fusion begins and the collapse may be halted. For core masses of 40–60 M☉, the collapse halts and the star remains intact, but core collapse will occur again when a larger core has formed. For cores of around 60–130 M☉, the fusion of oxygen and heavier elements is so energetic that the entire star is disrupted, causing a supernova. At the upper end of the mass range, the supernova is unusually luminous and extremely long-lived due to many solar masses of ejected Ni56. For even larger core masses, the core temperature becomes high enough to allow photodisintegration and the core collapses completely into a black hole.
Type II.
Stars with initial masses less than about eight times the sun never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least 9 M☉ (possibly as much as 12 M☉) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.
If core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a type II supernova. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.
Stars with an initial mass up to about 90 times the sun, or a little less at high metallicity, are expected to result in a type II-P supernova which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a type II-L supernova. At very low metallicity, stars of around 140–250 M☉ will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with type II characteristics but a very large mass of ejected Ni56 and high luminosity.
Type Ib and Ic.
These supernovae, like those of Type II, are massive stars that undergo core collapse. However the stars which become Types Ib and Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf-Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass loss rates. Observations of type Ib/c supernova do not match the observed or expected occurrence of Wolf Rayet stars and alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed. Since a supernova explosion can occur whenever the mass of the star at the time of core collapse is low enough not to cause complete fallback to a black hole, any massive star may result in a supernova if it loses enough mass before core collapse occurs.
Type Ib supernovae are the more common and result from Wolf-Rayet stars of type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining and these are the progenitors of type Ic supernovae.
A few percent of the Type Ic supernovae are associated with gamma ray bursts (GRB), though it is also believed that any hydrogen-stripped Type Ib or Ic supernova could produce a GRB, depending on the geometry of the explosion. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell of the explosion to produce a super-luminous supernova.
Ultra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (~0.1 Msun). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be an observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve.
The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core.
Light curves.
The visual light curves of the different supernova types vary in shape and amplitude, based on the underlying mechanisms of the explosion, the way that visible radiation is produced, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at UV and shorter wavelengths there is an extremely luminous peak lasting just a few hours, corresponding to the shock breakout of the initial explosion, which is hardly detectable at longer wavelengths.
The light curves for type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. The energy output is driven by radioactive decay of nickel-56 (half life 6 days), which then decays to radioactive cobalt-56 (half life 77 days). These radioisotopes from material ejected in the explosion excite surrounding material to incandescence. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission becomes dominant from the remaining cobalt-56, although this portion of the light curve has been little-studied.
Type Ib and Ic light curves are basically similar to type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of nickel-56 produced in these types of explosion. The peak luminosity varies considerably and there are even occasional type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increases peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.
The light curves for type II supernovae are characterised by a much slower decline than type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial explosion this hydrogen becomes heated and ionised. The majority of type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in type I supernovae, due to the efficiency of conversion into light by all the hydrogen.
In type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a type I supernova and the hydrogen even disappears from the spectrum after several weeks.
Type IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a hypernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.
Large numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.
Notes:
Asymmetry.
A long-standing puzzle surrounding Type II supernovae is why the compact object remaining after the explosion is given a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an asymmetry in the explosion, but the mechanism by which momentum is transferred to the compact object remains[ [update]] a puzzle. Proposed explanations for this kick include convection in the collapsing star and jet production during neutron star formation.
One possible explanation for the asymmetry in the explosion is large-scale convection above the core. The convection can create variations in the local abundances of elements, resulting in uneven nuclear burning during the collapse, bounce and resulting explosion.
Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova explosion. (A similar model is now favored for explaining long gamma ray bursts.)
Initial asymmetries have also been confirmed in Type Ia supernova explosions through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the explosion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarization of the emitted light.
Energy output.
Although we are used to thinking of supernovae primarily as luminous visible events, the electromagnetic radiation they produce is almost a minor side-effect of the explosion. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total event energy.
There is a fundamental difference between the balance of energy production in the different types of supernova. In type Ia white dwarf detonations, most of the explosion energy is directed into heavy element synthesis and kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the main explosion 99%+ of the neutrinos escape in the first few minutes following the start of the collapse.
Type Ia supernovae derive their energy from runaway nuclear fusion of a carbon-oxygen white dwarf. Details of the energetics are still not fully modelled, but the end result is the ejection of the entire mass of the original star with high kinetic energy. Around half a solar mass of this is Ni56 generated from silicon burning. Ni56 is radioactive and generates Co56 by beta plus decay with a half life of six days, plus gamma rays. Co56 itself decays by the beta plus path with a half life of 77 days to stable Fe56. These two processes are responsible for the electromagnetic radiation from type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.
Core collapse supernovae are on average visually fainter than type Ia supernovae, but the total energy released is far higher. This is driven by gravitational potential energy from the core collapse, initially producing electron neutrinos from disintegrating nucleons, followed by all flavours of thermal neutrinos from the super-heated neutron star core. Around 1% of these neutrinos are thought to deposit sufficient energy into the outer layers of the star to drive the resulting explosion, but again the details cannot be reproduced exactly in current models. Kinetic energies and nickel yields are somewhat lower than type Ia supernovae, hence the reduced visual luminosity, but energy from the ionisation of the many solar masses of remaining hydrogen can contribute to a much slower decline in luminosity and produce the plateau phase seen in the majority of core collapse supernovae.
In some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high luminosity supernovae and is thought to be the cause of type Ic hypernovae and long duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.
When a supernova occurs inside a small dense cloud of circumstellar material then it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial explosion energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause type IIn hypernovae.
Although pair-instability supernovae are core collapse supernovae with spectra and light curves similar to type II-P, the nature of the explosion following core collapse is more like a giant type Ia with runaway fusion of carbon, oxygen, and silicon. The total energy released by the highest mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected can be orders of magnitude higher, with consequently high visual luminosity.
Progenitor.
The supernova classification type is closely tied to the type of star at the time of the explosion. The occurrence of each type of supernova depends dramatically on the metallicity and hence the age of the host galaxy.
Type Ia supernovae are produced from white dwarf stars in binary systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.
Type Ib/c and II-L, and possibly most type IIn, supernovae are only thought to be produced from stars having near-solar metallicity levels that result in high mass loss from massive stars, hence they are less common in older more distant galaxies. The table shows the expected progenitor for the main types of core collapse supernova, and the approximate proportions of each in the local neighbourhood.
There are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the expected progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉ respectively. Most progenitors of type II supernovae are not detected and must be considerably fainter, and presumably less massive. It is now proposed that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for type IIb supernovae, and almost all type IIb supernovae near enough to observe have shown such progenitors.
Until just a few decades ago, hot supergiants were not considered likely to explode, but observations have shown otherwise. Blue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a type IIn supernova.
The expected progenitors of type Ib supernovae, luminous WC stars, are not observed at all. Instead WC stars are found at lower luminosities, apparently post-red supergiant stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Several examples of hot luminous progenitors of type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disupted.
Interstellar impact.
Source of heavy elements.
Supernovae are a key source of elements heavier than oxygen. These elements are produced by nuclear fusion (for iron-56 and lighter elements), and by nucleosynthesis during the supernova explosion for elements heavier than iron. Supernovae are the most likely, although not undisputed, candidate sites for the r-process, which is a rapid form of nucleosynthesis that occurs under conditions of high temperature and high density of neutrons. The reactions produce highly unstable nuclei that are rich in neutrons. These forms are unstable and rapidly beta decay into more stable forms.
The r-process reaction, which is likely to occur in type II supernovae, produces about half of all the element abundance beyond iron, including plutonium and uranium. The only other major competing process for producing elements heavier than iron is the s-process in large, old red giant stars, which produces these elements much more slowly, and which cannot produce elements heavier than lead.
Role in stellar evolution.
The remnant of a supernova explosion consists of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up the surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.
The Big Bang produced hydrogen, helium, and traces of lithium, while all heavier elements are synthesized in stars and supernovae. Supernovae tend to enrich the surrounding interstellar medium with "metals"—elements other than hydrogen and helium.
These injected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star's life, and may decisively influence the possibility of having planets orbiting it.
The kinetic energy of an expanding supernova remnant can trigger star formation due to compression of nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.
Evidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system. Supernova production of heavy elements over astronomic periods of time ultimately made the chemistry of life on Earth possible.
Effect on Earth.
A near-Earth supernova is a supernova close enough to the Earth to have noticeable effects on its biosphere. Depending upon the type and energy of the supernova, it could be as far as 3000 light-years away. Gamma rays from a supernova would induce a chemical reaction in the upper atmosphere converting molecular nitrogen into nitrogen oxides, depleting the ozone layer enough to expose the surface to harmful solar radiation. This has been proposed as the cause of the Ordovician–Silurian extinction, which resulted in the death of nearly 60% of the oceanic life on Earth.
In 1996 it was theorized that traces of past supernovae might be detectable on Earth in the form of metal isotope signatures in rock strata. Iron-60 enrichment was later reported in deep-sea rock of the Pacific Ocean. In 2009, elevated levels of nitrate ions were found in Antarctic ice, which coincided with the 1006 and 1054 supernovae. Gamma rays from these supernovae could have boosted levels of nitrogen oxides, which became trapped in the ice.
Type Ia supernovae are thought to be potentially the most dangerous if they occur close enough to the Earth. Because these supernovae arise from dim, common white dwarf stars, it is likely that a supernova that can affect the Earth will occur unpredictably and in a star system that is not well studied. The closest known candidate is IK Pegasi (see below). Recent estimates predict that a Type II supernova would have to be closer than eight parsecs (26 light-years) to destroy half of the Earth's ozone layer.
Milky Way candidates.
Several large stars within the Milky Way have been suggested as possible supernovae within the next million years. These include Rho Cassiopeiae, Eta Carinae, RS Ophiuchi, U Scorpii, VY Canis Majoris, Betelgeuse, and Antares. Many Wolf–Rayet stars, such as Gamma Velorum, WR 104, and those in the Quintuplet Cluster, are also considered possible precursor stars to a supernova explosion in the 'near' future.
The nearest supernova candidate is IK Pegasi (HR 8210), located at a distance of 150 light-years. This closely orbiting binary star system consists of a main sequence star and a white dwarf 31 million kilometres apart. The dwarf has an estimated mass 1.15 times that of the Sun. It is thought that several million years will pass before the white dwarf can accrete the critical mass required to become a Type Ia supernova.

</doc>
<doc id="27681" url="http://en.wikipedia.org/wiki?curid=27681" title="Sergei Prokofiev">
Sergei Prokofiev

Sergei Sergeyevich Prokofiev (; Russian: Сергей Сергеевич Прокофьев, "Sergej Sergeevič Prokof'ev"; April 27, 1891 [O.S. 15 April];– March 5, 1953) was a Russian composer, pianist and conductor. As the creator of acknowledged masterpieces across numerous musical genres, he is regarded as one of the major composers of the 20th century. His works include such widely heard works as the March from "The Love for Three Oranges," the suite "Lieutenant Kijé", the ballet "Romeo and Juliet" – from which "Dance of the Knights" is taken – and "Peter and the Wolf." Of the established forms and genres in which he worked, he created – excluding juvenilia – seven completed operas, seven symphonies, eight ballets, five piano concertos, two violin concertos, a cello concerto, and nine completed piano sonatas.
A graduate of the St Petersburg Conservatory, Prokofiev initially made his name as an iconoclastic composer-pianist, achieving notoriety with a series of ferociously dissonant and virtuosic works for his instrument, including his first two piano concertos. In 1915 Prokofiev made a decisive break from the standard composer-pianist category with his orchestral "Scythian Suite", compiled from music originally composed for a ballet commissioned by Sergei Diaghilev of the Ballets Russes. Diaghilev commissioned three further ballets from Prokofiev – "Chout," "Le pas d'acier" and "The Prodigal Son" – which at the time of their original production all caused a sensation among both critics and colleagues. Prokofiev's greatest interest, however, was opera, and he composed several works in that genre, including "The Gambler" and "The Fiery Angel". Prokofiev's one operatic success during his lifetime was "The Love for Three Oranges," composed for the Chicago Opera and subsequently performed over the following decade in Europe and Russia.
After the Revolution, Prokofiev left Russia with the official blessing of the Soviet minister Anatoly Lunacharsky, and resided in the United States, then Germany, then Paris, making his living as a composer, pianist and conductor. During that time he married a Spanish singer, Carolina Codina, with whom he had two sons. In the early 1930s, the Great Depression diminished opportunities for Prokofiev's ballets and operas to be staged in America and western Europe. Prokofiev, who regarded himself as composer foremost, resented the time taken by touring as a pianist, and increasingly turned to Soviet Russia for commissions of new music; in 1936 he finally returned to his homeland with his family. He enjoyed some success there – notably with "Lieutenant Kijé," "Peter and the Wolf," "Romeo and Juliet," and perhaps above all with "Alexander Nevsky." The Nazi invasion of the USSR spurred him to compose his most ambitious work, an operatic version of Leo Tolstoy's "War and Peace". In 1948 Prokofiev was criticized for "anti-democratic formalism" and, with his income severely curtailed, was forced to compose Stalinist works, such as "On Guard for Peace". However, he also enjoyed personal and artistic support from a new generation of Russian performers, notably Sviatoslav Richter and Mstislav Rostropovich: for the latter, he composed his Symphony-Concerto, whilst for the former he composed his ninth piano sonata.
Biography.
Early childhood and first compositions.
Prokofiev was born in 1891 in Sontsovka (now Krasne, Krasnoarmiisk Raion, Donetsk Oblast, eastern Ukraine), a remote rural estate in the Yekaterinoslav Governorate of the Russian Empire. His father, Sergei Alexeyevich Prokofiev, was an agronomist. Prokofiev's mother, Maria (née Zhitkova), came from a family of former serfs who had been owned by the Sheremetev family, under whose patronage serf-children were taught theatre and arts from an early age. She was described by Reinhold Glière (Prokofiev's first composition teacher) as "a tall woman with beautiful, clever eyes ... who knew how to create an atmosphere of warmth and simplicity about her." After their wedding in the summer of 1877, the Prokofievs had moved to a small estate in the Smolensk district. Eventually Sergei Alexeyevich found employment as a soil engineer, employed by one of his former fellow-students, Dmitri Sontsov, on whose estate in the Ukrainian steppes the Prokofievs moved to.
By the time of Prokofiev's birth Maria, having previously lost two daughters, had devoted her life to music; during her son's early childhood she spent two months a year in Moscow or St Petersburg taking piano lessons. Sergei Prokofiev was inspired by hearing his mother practicing the piano in the evenings – mostly works by Chopin and Beethoven – and composed his first piano composition at the age of five, an 'Indian Gallop', which was written down by his mother: this was in the F Lydian mode (a major scale with a raised 4th scale degree) as the young Prokofiev felt 'reluctance to tackle the black notes'. By seven, he had also learned to play chess. Much like music, chess would remain a passion, and he became acquainted with world chess champions José Raúl Capablanca, whom he beat in a simultaneous exhibition match in 1914, and Mikhail Botvinnik, with whom he played several matches in the 1930s. At the age of nine he was composing his first opera, "The Giant", as well as an overture and various other pieces.
Formal education and controversial early works.
In 1902, Prokofiev's mother met Sergei Taneyev, director of the Moscow Conservatory, who initially suggested that Prokofiev should start lessons in piano and composition with Alexander Goldenweiser. When Taneyev was unable to arrange this, he instead organised that composer and pianist Reinhold Glière should spend the summer of 1902 in Sontsovka teaching Prokofiev. This first series of lessons culminated, at the 11-year-old Prokofiev's insistence, with the budding composer making his first attempt to write a symphony. The following summer Glière revisited Sontsovka to give further tuition. When decades later Prokofiev wrote about his lessons with Glière, he gave due credit to his teacher's sympathetic method but complained that Glière had introduced him to "square" phrase structure and conventional modulations which he subsequently had to unlearn. Nonetheless, equipped with the necessary theoretical tools, Prokofiev started experimenting with dissonant harmonies and unusual time signatures in a series of short piano pieces which he called "ditties" (after the so-called "song form" – more accurately ternary form – they were based on), laying the basis for his own musical style.
Despite his growing talent, Prokofiev's parents hesitated over starting their son on a musical career at such an early age, and considered the possibility of his attending a quality high school in Moscow. By 1904, his mother had decided instead on Saint Petersburg, and she and Prokofiev visited the (then) capital to explore the possibility of their moving there for his education. They were introduced to composer Alexander Glazunov, a professor at the Conservatory, who asked to see Prokofiev and his music; Glazunov was so impressed that he urged Prokofiev's mother that her son apply to the Saint Petersburg Conservatory. By this point, Prokofiev had composed two more operas, "Desert Islands" and "The Feast during the Plague", and was working on his fourth, "Undina". He passed the introductory tests and entered the Conservatory that same year.
Several years younger than most of his class, he was viewed as eccentric and arrogant, and he annoyed a number of his classmates by keeping statistics on the errors made by fellow students. During this period, he studied under, among others, Alexander Winkler for piano, Anatoly Lyadov for harmony and counterpoint, Nikolai Tcherepnin for conducting, and Nikolai Rimsky-Korsakov for orchestration (though when Rimsky-Korsakov died in 1908, Prokofiev noted that he had only studied with him "after a fashion" – he was just one of many students in a heavily attended class—and regretted that he otherwise "never had the opportunity to study with him"). He also shared classes with the composers Boris Asafyev and Nikolai Myaskovsky, the latter becoming a relatively close and lifelong friend.
As a member of the Saint Petersburg music scene, Prokofiev developed a reputation as a musical rebel, while getting praise for his original compositions, which he performed himself on the piano. In 1909, he graduated from his class in composition with unimpressive marks. He continued at the Conservatory, studying piano under Anna Yesipova and continuing his conducting lessons under Tcherepnin.
In 1910, Prokofiev's father died and Sergei's financial support ceased. Fortunately he had started making a name for himself as a composer and pianist outside the Conservatory, making appearances at the St Petersburg Evenings of Contemporary Music. There he performed several of his more adventurous piano works, such as his highly chromatic and dissonant Etudes, Op. 2 (1909). His performance of this impressed the organizers of Evenings sufficiently for them to invite Prokofiev to give the Russian premiere of Arnold Schoenberg's Drei Klavierstücke, Op. 11. Prokofiev's harmonic experimentation continued with "Sarcasms" for piano, Op. 17 (1912), which makes extensive use of polytonality. He composed his first two piano concertos around this time, the latter of which caused a scandal at its premiere (23 August 1913, Pavlovsk). According to one account, the audience left the hall with exclamations of "'To hell with this futuristic music! The cats on the roof make better music!'", but the modernists were in rapture.
In 1911, help arrived from renowned Russian musicologist and critic Alexander Ossovsky, who wrote a supportive letter to music publisher Boris P. Jurgenson (son of publishing-firm founder Peter Jurgenson [1836–1904]); thus a contract was offered to the composer. Prokofiev made his first foreign trip in 1913, travelling to Paris and London where he first encountered Sergei Diaghilev's Ballets Russes.
The first ballets.
In 1914, Prokofiev finished his career at the Conservatory by entering the so-called 'battle of the pianos', a competition open to the five best piano students for which the prize was a Schreder grand piano: Prokofiev won by performing his own Piano Concerto No. 1. Soon afterwards, he journeyed to London where he made contact with the impresario Sergei Diaghilev. Diaghilev commissioned Prokofiev's first ballet, "Ala and Lolli"; but when Prokofiev brought the work in progress to him in Italy in 1915 he rejected it as "non-Russian". Urging Prokofiev to write "music that was national in character", Diaghilev then commissioned the ballet "Chout" ("The Fool", the original Russian-language full title was Сказка про шута, семерых шутов перешутившего (Skazka pro shuta, semerykh shutov pereshutivshavo), meaning "The Tale of the Buffoon who Outwits Seven Other Buffoons"). Under Diaghilev's guidance, Prokofiev chose his subject from a collection of folktales by the ethnographer Alexander Afanasyev; the story, concerning a buffoon and a series of confidence tricks, had been previously suggested to Diaghilev by Igor Stravinsky as a possible subject for a ballet, and Diaghilev and his choreographer Léonide Massine helped Prokofiev to shape this into a ballet scenario. Prokofiev's inexperience with ballet led him to revise the work extensively in the 1920s, following Diaghilev's detailed critique, prior to its first production. The ballet's premiere in Paris on 17 May 1921 was a huge success and was greeted with great admiration by an audience that included Jean Cocteau, Igor Stravinsky and Maurice Ravel. Stravinsky called the ballet "the single piece of modern music he could listen to with pleasure," while Ravel called it "a work of genius."
First World War and Revolution.
During World War I, Prokofiev returned to the Conservatory and studied organ in order to avoid conscription. He composed "The Gambler" based on Fyodor Dostoyevsky's novel of the same name, but rehearsals were plagued by problems and the scheduled 1917 première had to be canceled because of the February Revolution. In the summer of that year, Prokofiev composed his first symphony, the "Classical". This was his own name for the symphony, which was written in the style that, according to Prokofiev, Joseph Haydn would have used if he had been alive at the time. It is more or less classical in style but incorporates more modern musical elements (see Neoclassicism). This symphony was also an exact contemporary of Prokofiev's Violin Concerto No. 1 in D major, Op. 19, which was scheduled to premiere in November 1917. The first performances of both works had to wait until 21 April 1918 and 18 October 1923, respectively. He stayed briefly with his mother in Kislovodsk in the Caucasus. After completing the score of "Seven, They Are Seven", a "Chaldean invocation" for chorus and orchestra, Prokofiev was "left with nothing to do and time hung heavily on my hands". Believing that Russia "had no use for music at the moment", Prokofiev decided to try his fortunes in America until the turmoil in his homeland had passed. He set out for Moscow and Petersburg in March 1918 to sort out financial matters and to arrange for his passport. In May he headed for the USA, having obtained official permission to do so from Anatoly Lunacharsky, the People's Commissar for Education, who told him: "You are a revolutionary in music, we are revolutionaries in life. We ought to work together. But if you want to go to America I shall not stand in your way."
Life abroad.
Arriving in San Francisco after having been released from questioning by immigration officials on Angel Island on 11 August 1918, Prokofiev was soon compared to other famous Russian exiles (such as Sergei Rachmaninoff). His debut solo concert in New York led to several further engagements. He also received a contract from the music director of the Chicago Opera Association, Cleofonte Campanini, for the production of his new opera "The Love for Three Oranges"; however, due to Campanini's illness and death, the premiere was postponed. This delay was another example of Prokofiev's bad luck in operatic matters. The failure also cost him his American solo career, since the opera took too much time and effort. He soon found himself in financial difficulties, and, in April 1920, he left for Paris, not wanting to return to Russia as a failure.
In Paris Prokofiev reaffirmed his contacts with Diaghilev's Ballets Russes. He also completed some of his older, unfinished works, such as the Third Piano Concerto. "The Love for Three Oranges" finally premièred in Chicago, under the composer's baton, on 30 December 1921. Diaghilev became sufficiently interested in the opera to request Prokofiev play the vocal score to him in June 1922, while they were both in Paris for a revival of "Chout", so he could consider it for a possible production. Stravinsky, who was present at the audition, refused to listen to more than the first act. When he then accused Prokofiev of "wasting time composing operas", Prokofiev retorted that Stravinsky "was in no position to lay down a general artistic direction, since he is himself not immune to error". According to Prokofiev, Stravinsky "became incandescent with rage" and "we almost came to blows and were separated only with difficulty". As a result, "our relations became strained and for several years Stravinsky's attitude toward me was critical."
In March 1922, Prokofiev moved with his mother to the town of Ettal in the Bavarian Alps, where for over a year he concentrated on an opera project, "The Fiery Angel", based on the novel by Valery Bryusov. By this time his later music had acquired a following in Russia, and he received invitations to return there, but he decided to stay in Europe. In 1923, Prokofiev married the Spanish singer Carolina Codina (1897–1989, whose stage name was Lina Llubera) before moving back to Paris.
In Paris, several of his works (for example the Second Symphony) were performed, but the audiences' reception was now lukewarm and Prokofiev sensed that he "was evidently no longer a sensation". However the Symphony appeared to prompt Diaghilev to commission "Le pas d'acier" ("The Steel Step"), a 'modernist' ballet score intended to portray the industrialisation of the Soviet Union. It was enthusiastically received by Parisian audiences and critics.
In around 1924, Prokofiev was introduced to Christian Science. He began to practice its teachings, which he believed to be beneficial to his health and to his fiery temperament, and to which, according to biographer Simon Morrison, he remained faithful for the rest of his life.
Prokofiev and Stravinsky restored their friendship, though Prokofiev particularly disliked Stravinsky's "stylization of Bach" in such recent works as the Octet and the Concerto for Piano and Wind Instruments. However, Stravinsky himself described Prokofiev as the greatest Russian composer of his day, after himself.
First visits to the Soviet Union.
In 1927, Prokofiev made his first concert tour in the Soviet Union. Over the course of more than two months, he spent time in Moscow and Leningrad (as Saint Petersburg had been renamed), where he enjoyed a very successful staging of "The Love for Three Oranges" in the Mariinsky Theatre. In 1928, Prokofiev completed his Third Symphony, which was broadly based on his unperformed opera "The Fiery Angel". The conductor Serge Koussevitzky characterized the Third as "the greatest symphony since Tchaikovsky's Sixth."
In the meantime, however, Prokofiev, under the influence of the teachings of Christian Science, had turned against the expressionist style and the subject matter of "The Fiery Angel". He now preferred what he called a "new simplicity", which he believed more sincere than the "contrivances and complexities" of so much modern music of the 1920s. During 1928–29, Prokofiev composed what was to be the last ballet for Diaghilev, "The Prodigal Son". When first staged in Paris on 21 May 1929, with Serge Lifar in the title role, both audience and critics were particularly struck by the final scene in which the prodigal son drags himself across the stage upon his knees to be welcomed by his father. Diaghilev had recognised that in the music to this scene, Prokofiev had "never been more clear, more simple, more melodious, and more tender." Only months later, Diaghilev was dead.
That summer, Prokofiev completed the Divertimento, Op. 43 (which he had started in 1925) and revised his Sinfonietta, Op. 5/48, a work started in his days at the Conservatory. In October that year, he had a car crash while driving his family back to Paris from their holiday: as the car turned over, Prokofiev pulled some muscles on his left hand. Prokofiev was therefore unable to perform in Moscow during his tour shortly after the accident, but he was able to enjoy watching performances of his music from the audience. Prokofiev also attended the Bolshoi Theatre's "audition" of his ballet "Le pas d'acier", and was interrogated by members of the Russian Association of Proletarian Musicians (RAPM) about the work: he was asked whether the factory portrayed "a capitalist factory, where the worker is a slave, or a Soviet factory, where the worker is the master? If it is a Soviet factory, when and where did Prokofiev examine it, since from 1918 to the present he has been living abroad and came here for the first time in 1927 for two weeks [sic]?" Prokofiev replied, "That concerns politics, not music, and therefore I won't answer." The RAPM condemned the ballet as a "flat and vulgar anti-Soviet anecdote, a counter-revolutionary composition bordering on Fascism". The Bolshoi had no option but to reject the ballet.
With his left hand healed, Prokofiev toured the United States successfully at the start of 1930, propped up by his recent European success. That year Prokofiev began his first non-Diaghilev ballet "On the Dnieper", Op. 51, a work commissioned by Serge Lifar, who had been appointed "maitre de ballet" at the Paris Opéra. In 1931 and 1932, he completed his fourth and fifth piano concertos. The following year saw the completion of the Symphonic Song, Op. 57, which Prokofiev's friend Myaskovsky – thinking of its potential audience in the Soviet Union – told him "isn't quite for us ... it lacks that which we mean by monumentalism – a familiar simplicity and broad contours, of which you are extremely capable, but temporarily are carefully avoiding."
By the early 1930s, both Europe and America were suffering from the Great Depression, which inhibited both new opera and ballet productions, though audiences for Prokofiev's appearances as a pianist were - in Europe at least - undiminished. However Prokofiev, who saw himself as a composer first and foremost, increasingly resented the amount of time that was lost to composition through his appearances as a pianist. Having been homesick for some time, Prokofiev began to build substantial bridges with the Soviet Union. Following the dissolution of the RAPM in 1932, he acted increasingly as a musical ambassador between his homeland and western Europe, and his premieres and commissions were increasingly under the auspices of the Soviet Union. One such was "Lieutenant Kijé", which was commissioned as the score to a Soviet film. Another commission, from the Kirov Theatre (as the Mariinsky had now been renamed) in Leningrad, was the ballet "Romeo and Juliet", composed to a scenario created by Adrian Piotrovsky and Sergei Radlov following the precepts of "drambalet" (dramatised ballet, officially promoted at the Kirov to replace works based primarily on choreographic display and innovation). Following Radlov's acrimonious resignation from the Kirov in June 1934, a new agreement was signed with the Bolshoi Theatre in Moscow on the understanding that Piotrovsky would remain involved. However, the ballet's original happy ending (contrary to Shakespeare) provoked controversy among Soviet cultural officials; the ballet's production was then postponed indefinitely when the staff of the Bolshoi was overhauled at the behest of the chairman of the Committee on Arts Affairs, Platon Kerzhentsev.
Return to Russia.
In 1936, Prokofiev and his family settled permanently in Moscow. In that year he composed one of his most famous works, "Peter and the Wolf", for Natalya Sats's Central Children's Theatre. Sats also persuaded Prokofiev to write two songs for children – "Sweet Song", and "Chatterbox"; these were eventually joined by "The Little Pigs", published as "Three Children's Songs", Op. 68. Prokofiev also composed the gigantic "Cantata for the 20th Anniversary of the October Revolution," originally intended for performance during the anniversary year but effectively blocked by Kerzhentsev, who demanded at the work's audition before the Committee on Arts Affairs, "Just what do you think you're doing, Sergey Sergeyevich, taking texts that belong to the people and setting them to such incomprehensible music?" The Cantata had to wait until 5 April 1966 for a partial premiere (just over 13 years after the composer's death).
Forced to adapt to the new circumstances (whatever misgivings he had about them in private), Prokofiev wrote a series of "mass songs" (Opp. 66, 79, 89), using the lyrics of officially approved Soviet poets. In 1938, Prokofiev collaborated with Eisenstein on the historical epic "Alexander Nevsky". For this he composed some of his most inventive and dramatic music. Although the film had a very poor sound recording, Prokofiev adapted much of his score into a large-scale cantata for mezzo-soprano, orchestra and chorus, which was extensively performed and recorded. In the wake of "Alexander Nevsky"'s success, Prokofiev composed his first Soviet opera "Semyon Kotko", which was intended to be produced by the director Vsevolod Meyerhold. However the première of the opera was postponed because Meyerhold was arrested on 20 June 1939 by the NKVD (Joseph Stalin's Secret Police), and shot on 2 February 1940. Only months after Meyerhold's arrest, Prokofiev was 'invited' to compose "Zdravitsa" (literally translated 'Cheers!', but more often given the English title "Hail to Stalin") (Op. 85) to celebrate Joseph Stalin's 60th birthday.
Later in 1939, Prokofiev composed his Piano Sonatas Nos. 6, 7, and 8, Opp. 82–84, widely known today as the "War Sonatas." Premiered respectively by Prokofiev (No. 6: 8 April 1940), Sviatoslav Richter (No. 7: Moscow, 18 January 1943) and Emil Gilels (No. 8: Moscow, 30 December 1944), they were subsequently championed in particular by Richter. Biographer Daniel Jaffé argued that Prokofiev, "having forced himself to compose a cheerful evocation of the nirvana Stalin wanted everyone to believe he had created" (i.e. in "Zdravitsa") then subsequently, in these three sonatas, "expressed his true feelings". As evidence of this, Jaffé has pointed out that the central movement of Sonata No. 7 opens with a theme based on a Robert Schumann lied, 'Wehmut' ('Sadness', which appears in Schumann's Liederkreis, Op. 39): the words to this translate "I can sometimes sing as if I were glad, yet secretly tears well and so free my heart. Nightingales ... sing their song of longing from their dungeon's depth ... everyone delights, yet no one feels the pain, the deep sorrow in the song." Ironically (because, it appears, no one had noticed his allusion) Sonata No. 7 received a Stalin Prize (Second Class), and No. 8 a Stalin Prize First Class.
In the meantime, "Romeo and Juliet" was finally staged by the Kirov ballet, choreographed by Leonid Lavrovsky, on 11 January 1940. To the surprise of all its participants, the dancers having struggled to cope with the music's syncopated rhythms and almost having boycotted the production, the ballet was an instant success, and became recognised as the crowning achievement of Soviet dramatic ballet.
War years.
Prokofiev had been considering making an opera out of Leo Tolstoy's epic novel "War and Peace", when news of the German invasion of Russia on 22 June 1941 made the subject seem all the more timely. Prokofiev took two years to compose his original version of "War and Peace". Because of the war he was evacuated together with a large number of other artists, initially to the Caucasus where he composed his Second String Quartet. By this time his relationship with the 25-year-old writer and librettist Mira Mendelson (1915–1968) had finally led to his separation from his wife Lina, although they were never technically divorced: indeed Prokofiev had tried to persuade Lina and their sons to accompany him as evacuees out of Moscow, but Lina opted to stay.
During the war years, restrictions on style and the demand that composers should write in a 'socialist realist' style were slackened, and Prokofiev was generally able to compose in his own way. The Violin Sonata No. 1, Op. 80, "The Year 1941", Op. 90, and the "Ballade for the Boy Who Remained Unknown", Op. 93 all came from this period. In 1943 Prokofiev joined Eisenstein in Alma-Ata, the largest city in Kazakhstan, to compose more film music ("Ivan the Terrible"), and the ballet "Cinderella" (Op. 87), one of his most melodious and celebrated compositions. Early that year he also played excerpts from "War and Peace" to members of the Bolshoi Theatre collective. However, the Soviet government had opinions about the opera which resulted in many revisions. In 1944, Prokofiev spent time at a composer's colony outside Moscow in order to compose his Fifth Symphony (Op. 100). Prokofiev conducted its first performance on 13 January 1945, just a fortnight after the triumphant premieres on 30 December 1944 of his Eighth Piano Sonata and, on the same day, the first part of Eisenstein's "Ivan the Terrible". With the premiere of his Fifth Symphony, which was programmed alongside "Peter and the Wolf" and the "Classical" Symphony (these conducted by Nikolai Anosov), Prokofiev appeared to reach the peak of his celebrity as a leading composer of the Soviet Union. Shortly afterwards, he suffered a concussion after a fall due to chronic high blood pressure. He never fully recovered from this injury, and was forced on medical advice to restrict his composing activity.
Post-war.
Prokofiev had time to write his postwar Sixth Symphony and his Ninth Piano Sonata (for Sviatoslav Richter) before the so-called "Zhdanov Decree". In early 1948, following a meeting of Soviet composers convened by Andrei Zhdanov, the Politburo issued a resolution denouncing Prokofiev, Dmitri Shostakovich, Myaskovsky, and Khachaturian of the crime of "formalism", described as a "renunciation of the basic principles of classical music" in favour of "muddled, nerve-racking" sounds that "turned music into cacophony". Eight of Prokofiev's works were banned from performance: "The Year 1941", "Ode to the End of the War", "Festive Poem", "Cantata for the Thirtieth Anniversary of October", "Ballad of an Unknown Boy", the 1934 piano cycle "Thoughts", and Piano Sonatas Nos 6 and 8. Such was the perceived threat behind the banning of these works that even works that had avoided censure were no longer programmed: by August 1948, Prokofiev was in severe financial straits, his personal debt amounting to 180,000 rubles.
Meanwhile, on 20 February 1948, Prokofiev's wife Lina was arrested for 'espionage', as she had tried to send money to her mother in Spain. After nine months of interrogation, she was sentenced by a three-member Military Collegium of the Supreme Court of the USSR to 20 years of hard labour. She was eventually released after Stalin's death in 1953 and in 1974 left the Soviet Union.
Prokofiev's latest opera projects, among them his desperate attempt to appease the cultural authorities, "The Story of a Real Man", were quickly cancelled by the Kirov Theatre. This snub, in combination with his declining health, caused Prokofiev progressively to withdraw from public life and from various activities, even his beloved chess, and increasingly he devoted himself exclusively to his own work. After a serious relapse in 1949, his doctors ordered him to limit his activities, limiting him to composing for only an hour a day.
In spring 1949 he wrote his Cello Sonata in C, Op. 119, for the 22-year old Mstislav Rostropovich, who gave the first performance in 1950, with Sviatoslav Richter. For Rostropovich, Prokofiev also extensively recomposed his Cello Concerto, transforming it into a Symphony-Concerto, his last major masterpiece and a landmark in the cello and orchestra repertory today. The last public performance he attended was the première of the Seventh Symphony in 1952. The music was written for the Children's Radio Division.
Death.
Prokofiev died at the age of 61 on 5 March 1953, the same day as Joseph Stalin. He had lived near Red Square, and for three days the throngs gathered to mourn Stalin, making it impossible to carry Prokofiev's body out for the funeral service at the headquarters of the Soviet Composer's Union. He is buried in the Novodevichy Cemetery in Moscow.
The leading Soviet musical periodical reported Prokofiev's death as a brief item on page 116. The first 115 pages were devoted to the death of Stalin. Usually Prokofiev's death is attributed to cerebral hemorrhage. He had been chronically ill for the prior eight years; the precise nature of Prokofiev's terminal illness remains uncertain.
Lina Prokofiev outlived her estranged husband by many years, dying in London in early 1989. Royalties from her late husband's music provided her with a modest income, and she acted as storyteller for a recording of her husband's "Peter and the Wolf" (currently released on CD by Chandos Records) with Neeme Järvi conducting the Scottish National Orchestra. Their sons Sviatoslav (1924–2010), an architect, and Oleg (1928–1998), an artist, painter, sculptor and poet, dedicated a large part of their lives to the promotion of their father's life and work.
Posthumous reputation.
Arthur Honegger proclaimed that Prokofiev would "remain for us the greatest figure of contemporary music," and the American scholar Richard Taruskin has recognised Prokofiev's "gift, virtually unparalleled among 20th-century composers, for writing distinctively original diatonic melodies." Yet for some time Prokofiev's reputation in the West suffered as a result of cold-war antipathies, and his music has never won from Western academics and critics the kind of esteem currently enjoyed by Igor Stravinsky and Arnold Schoenberg, composers purported to have a greater influence on a younger generation of musicians.
Today Prokofiev may well be the most popular composer of 20th-century music. His orchestral music alone is played more frequently in the United States than that of any other composer of the last hundred years, save Richard Strauss, while his operas, ballets, chamber works, and piano music appear regularly throughout the major concert halls world-wide.
The composer received honours in his native Donetsk Oblast, when the Donetsk International Airport was renamed to be "Donetsk Sergey Prokofiev International Airport," and when the Donetsk Musical and Pedagogical Institute was renamed in 1988 to "S.S. Prokofiev State Music Academy of Donetsk."
Works.
Important works include (in chronological order):
Recordings.
Prokofiev was a soloist with the London Symphony Orchestra, conducted by Piero Coppola, in the first recording of his Piano Concerto No. 3, recorded in London by His Master's Voice in June 1932. Prokofiev also recorded some of his solo piano music for HMV in Paris in February 1935; these recordings were issued on CD by Pearl and Naxos. In 1938, he conducted the Moscow Philharmonic Orchestra in a recording of the second suite from his "Romeo and Juliet" ballet; this performance was later released on LP and CD. Another reported recording with Prokofiev and the Moscow Philharmonic was of the First Violin Concerto with David Oistrakh as soloist; Everest Records later released this recording on an LP. Despite the attribution, the conductor was Aleksandr Gauk. A short sound film of Prokofiev playing some of the music from his opera "War and Peace" and then explaining the music has been discovered.
Bibliography.
Autobiography and diaries.
</dl>
Memoirs, essays, etc..
</dl>
Biographies.
</dl>
Other monographs.
</dl>
Dictionary articles.
</dl>
Notes and references.
Notes
References

</doc>
<doc id="27683" url="http://en.wikipedia.org/wiki?curid=27683" title="Satellite">
Satellite

In the context of spaceflight, a satellite is an artificial object which has been intentionally placed into orbit. Such objects are sometimes called artificial satellites to distinguish them from natural satellites such as the Moon.
The world's first artificial satellite, the Sputnik 1, was launched by the Soviet Union in 1957. Since then, thousands of satellites have been launched into orbit around the Earth. Some satellites, notably space stations, have been launched in parts and assembled in orbit. Artificial satellites originate from more than 40 countries and have used the satellite launching capabilities of ten nations. A few hundred satellites are currently operational, whereas thousands of unused satellites and satellite fragments orbit the Earth as space debris. A few space probes have been placed into orbit around other bodies and become artificial satellites to the Moon, Mercury, Venus, Mars, Jupiter, Saturn, Vesta, Eros, Ceres, and the Sun.
Satellites are used for a large number of purposes. Common types include military and civilian Earth observation satellites, communications satellites, navigation satellites, weather satellites, and research satellites. Space stations and human spacecraft in orbit are also satellites. Satellite orbits vary greatly, depending on the purpose of the satellite, and are classified in a number of ways. Well-known (overlapping) classes include low Earth orbit, polar orbit, and geostationary orbit.
About 6,600 satellites have been launched. The latest estimates are that 3,600 remain in orbit. Of those, about 1,000 are operational; the rest have lived out their useful lives and are part of the space debris. Approximately 500 operational satellites are in low-Earth orbit, 50 are in medium-Earth orbit (at 20,000 km), the rest are in geostationary orbit (at 36,000 km).
Satellites are propelled by rockets to their orbits. Usually the launch vehicle itself is a rocket lifting off from a launch pad on land. In a minority of cases satellites are launched at sea (from a submarine or a mobile maritime platform) or aboard a plane (see air launch to orbit).
Satellites are usually semi-independent computer-controlled systems. Satellite subsystems attend many tasks, such as power generation, thermal control, telemetry, attitude control and orbit control.
History.
Early conceptions.
"Newton's cannonball", presented as a "thought experiment" in "A Treatise of the System of the World", was the first published mathematical study of the possibility of an artificial satellite.
The first fictional depiction of a satellite being launched into orbit is a short story by Edward Everett Hale, "The Brick Moon". The story is serialized in "The Atlantic Monthly", starting in 1869. The idea surfaces again in Jules Verne's "The Begum's Fortune" (1879).
In 1903, Konstantin Tsiolkovsky (1857–1935) published "Exploring Space Using Jet Propulsion Devices" (in Russian: "Исследование мировых пространств реактивными приборами"), which is the first academic treatise on the use of rocketry to launch spacecraft. He calculated the orbital speed required for a minimal orbit around the Earth at 8 km/s, and that a multi-stage rocket fuelled by liquid propellants could be used to achieve this. He proposed the use of liquid hydrogen and liquid oxygen, though other combinations can be used.
In 1928, Slovenian Herman Potočnik (1892–1929) published his sole book, "The Problem of Space Travel — The Rocket Motor" (German: "Das Problem der Befahrung des Weltraums — der Raketen-Motor"), a plan for a breakthrough into space and a permanent human presence there. He conceived of a space station in detail and calculated its geostationary orbit. He described the use of orbiting spacecraft for detailed peaceful and military observation of the ground and described how the special conditions of space could be useful for scientific experiments. The book described geostationary satellites (first put forward by Tsiolkovsky) and discussed communication between them and the ground using radio, but fell short of the idea of using satellites for mass broadcasting and as telecommunications relays.
In a 1945 "Wireless World" article, the English science fiction writer Arthur C. Clarke (1917–2008) described in detail the possible use of communications satellites for mass communications. Clarke examined the logistics of satellite launch, possible orbits and other aspects of the creation of a network of world-circling satellites, pointing to the benefits of high-speed global communications. He also suggested that three geostationary satellites would provide coverage over the entire planet.
The US military studied the idea of what was referred to as the "earth satellite vehicle" when Secretary of Defense James Forrestal made a public announcement on December 29, 1948, that his office was coordinating that project between the various services.
Artificial satellites.
The first artificial satellite was Sputnik 1, launched by the Soviet Union on October 4, 1957, and initiating the Soviet Sputnik program, with Sergei Korolev as chief designer (there is a crater on the lunar far side which bears his name). This in turn triggered the Space Race between the Soviet Union and the United States.
Sputnik 1 helped to identify the density of high atmospheric layers through measurement of its orbital change and provided data on radio-signal distribution in the ionosphere. The unanticipated announcement of "Sputnik 1"'s success precipitated the Sputnik crisis in the United States and ignited the so-called Space Race within the Cold War.
"Sputnik 2" was launched on November 3, 1957 and carried the first living passenger into orbit, a dog named Laika.
In May, 1946, Project RAND had released the Preliminary Design of an Experimental World-Circling Spaceship, which stated, "A satellite vehicle with appropriate instrumentation can be expected to be one of the most potent scientific tools of the Twentieth Century."
The United States had been considering launching orbital satellites since 1945 under the Bureau of Aeronautics of the United States Navy. The United States Air Force's Project RAND eventually released the above report, but did not believe that the satellite was a potential military weapon; rather, they considered it to be a tool for science, politics, and propaganda. In 1954, the Secretary of Defense stated, "I know of no American satellite program." In February 1954 Project RAND released "Scientific Uses for a Satellite Vehicle," written by R.R. Carhart. This expanded on potential scientific uses for satellite vehicles and was followed in June 1955 with "The Scientific Use of an Artificial Satellite," by H.K. Kallmann and W.W. Kellogg.
In the context of activities planned for the International Geophysical Year (1957–58), the White House announced on July 29, 1955 that the U.S. intended to launch satellites by the spring of 1958. This became known as Project Vanguard. On July 31, the Soviets announced that they intended to launch a satellite by the fall of 1957.
Following pressure by the American Rocket Society, the National Science Foundation, and the International Geophysical Year, military interest picked up and in early 1955 the Army and Navy were working on Project Orbiter, two competing programs: the army's which involved using a Jupiter C rocket, and the civilian/Navy Vanguard Rocket, to launch a satellite. At first, they failed: initial preference was given to the Vanguard program, whose first attempt at orbiting a satellite resulted in the explosion of the launch vehicle on national television. But finally, three months after Sputnik 2, the project succeeded; Explorer 1 became the United States' first artificial satellite on January 31, 1958.
In June 1961, three-and-a-half years after the launch of Sputnik 1, the Air Force used resources of the United States Space Surveillance Network to catalog 115 Earth-orbiting satellites.
Early satellites were constructed as "one-off" designs. With growth in geosynchronous (GEO) satellite communication, multiple satellites began to be built on single model platforms called satellite buses. The first standardized satellite bus design was the HS-333 GEO commsat, launched in 1972.
The largest artificial satellite currently orbiting the Earth is the International Space Station.
Space Surveillance Network.
The United States Space Surveillance Network (SSN), a division of The United States Strategic Command, has been tracking objects in Earth's orbit since 1957 when the Soviets opened the space age with the launch of Sputnik I. Since then, the SSN has tracked more than 26,000 objects. The SSN currently tracks more than 8,000 man-made orbiting objects. The rest have re-entered Earth's atmosphere and disintegrated, or survived re-entry and impacted the Earth. The SSN tracks objects that are 10 centimeters in diameter or larger; those now orbiting Earth range from satellites weighing several tons to pieces of spent rocket bodies weighing only 10 pounds. About seven percent are operational satellites (i.e. ~560 satellites), the rest are space debris. The United States Strategic Command is primarily interested in the active satellites, but also tracks space debris which upon reentry might otherwise be mistaken for incoming missiles.
A search of the NSSDC Master Catalog at the end of October 2010 listed 6,578 satellites launched into orbit since 1957, the latest being Chang'e 2, on 1 October 2010.
Non-military satellite services.
There are three basic categories of non-military satellite services:
Fixed satellite services.
Fixed satellite services handle hundreds of billions of voice, data, and video transmission tasks across all countries and continents between certain points on the Earth's surface.
Mobile satellite systems.
Mobile satellite systems help connect remote regions, vehicles, ships, people and aircraft to other parts of the world and/or other mobile or stationary communications units, in addition to serving as navigation systems.
Scientific research satellites (commercial and noncommercial).
Scientific research satellites provide meteorological information, land survey data (e.g. remote sensing), Amateur (HAM) Radio, and other different scientific research applications such as earth science, marine science, and atmospheric research.
Orbit types.
The first satellite, Sputnik 1, was put into orbit around Earth and was therefore in geocentric orbit. By far this is the most common type of orbit with approximately 2,465 artificial satellites orbiting the Earth. Geocentric orbits may be further classified by their altitude, inclination and eccentricity.
The commonly used altitude classifications of geocentric orbit are Low Earth orbit (LEO), Medium Earth orbit (MEO) and High Earth orbit (HEO). Low Earth orbit is any orbit below 2,000 km. Medium Earth orbit is any orbit between 2,000 km-35,786 km. High Earth orbit is any orbit higher than 35,786 km.
Centric classifications.
The general structure of a satellite is that it is connected to the earth stations that are present on the ground and connected through terrestrial links.
Satellite subsystems.
The satellite's functional versatility is imbedded within its technical components and its operations characteristics. Looking at the "anatomy" of a typical satellite, one discovers two modules. Note that some novel architectural concepts such as Fractionated Spacecraft somewhat upset this taxonomy.
Spacecraft bus or service module.
The bus module consists of the following subsystems:
The structural subsystem provides the mechanical base structure with adequate stiffness to withstand stress and vibrations experienced during launch, maintain structural integrity and stability while on station in orbit, and shields the satellite from extreme temperature changes and micro-meteorite damage.
The telemetry subsystem monitors the on-board equipment operations, transmits equipment operation data to the earth control station, and receives the earth control station's commands to perform equipment operation adjustments.
The power subsystem consists of solar panels to convert solar energy into electrical power, regulation and distribution functions, and batteries that store power and supply the satellite when it passes into the Earth's shadow. Nuclear power sources (Radioisotope thermoelectric generator have also been used in several successful satellite programs including the Nimbus program (1964–1978).
The thermal control subsystem helps protect electronic equipment from extreme temperatures due to intense sunlight or the lack of sun exposure on different sides of the satellite's body (e.g. Optical Solar Reflector)
The attitude and orbit control subsystem consists of sensors to measure vehicle orientation; control laws embedded in the flight software; and 
actuators (reaction wheels, thrusters) to apply the torques and forces needed to re-orient the vehicle to a desired attitude, keep the satellite in the correct orbital position and keep antennas positioning in the right directions.
Communication payload.
The second major module is the communication payload, which is made up of transponders. A transponder is capable of :
End of life.
When satellites reach the end of their mission, satellite operators have the option of de-orbiting the satellite, leaving the satellite in its current orbit or moving the satellite to a graveyard orbit. Historically, due to budgetary constraints at the beginning of satellite missions, satellites were rarely designed to be de-orbited. One example of this practice is the satellite Vanguard 1. Launched in 1958, Vanguard 1, the 4th manmade satellite put in Geocentric orbit, was still in orbit as of August 2009.
Instead of being de-orbited, most satellites are either left in their current orbit or moved to a graveyard orbit. As of 2002, the FCC requires all geostationary satellites to commit to moving to a graveyard orbit at the end of their operational life prior to launch. In cases of uncontrolled de-orbiting, the major variable is the solar flux, and the minor variables the components and form factors of the satellite itself, and the gravitational perturbations generated by the Sun and the Moon (as well as those exercised by large mountain ranges, whether above or below sea level). The nominal breakup altitude due to aerodynamic forces and temperatures is 78 km, with a range between 72 and 84 km. Solar panels, however, are destroyed before any other component at altitudes between 90 and 95 km.
Launch-capable countries.
This list includes countries with an independent capability of states to place satellites in orbit, including production of the necessary launch vehicle. Note: many more countries have the capability to design and build satellites but are unable to launch them, instead relying on foreign launch services. This list does not consider those numerous countries, but only lists those capable of launching satellites indigenously, and the date this capability was first demonstrated. The list include multi-national state organization ESA but does not include private consortiums.
Launch capable private entities.
A few other private companies are capable of sub-orbital launches.
First satellites of countries.
While Canada was the third country to build a satellite which was launched into space, it was launched aboard a U.S. rocket from a U.S. spaceport. The same goes for Australia, who launched first satellite involved a donated U.S. Redstone rocket and U.S. support staff as well as a joint launch facility with the United Kingdom. The first Italian satellite San Marco 1 launched on 15 December 1964 on a U.S. Scout rocket from Wallops Island (VA, USA) with an Italian Launch Team trained by NASA. By similar occasions, almost all further first national satellites was launched by foreign rockets.
Attempted first satellites.
†-note: Both Chile and Belarus used Russian companies as principal contractors to build their satellites, they used Russian-Ukrainian manufactured rockets and launched either from Russia or Kazakhstan.
Attacks on satellites.
In recent times, satellites have been hacked by militant organizations to broadcast propaganda and to pilfer classified information from military communication networks.
For testing purposes, satellites in low earth orbit have been destroyed by ballistic missiles launched from earth. Russia, the United States and China have demonstrated the ability to eliminate satellites. In 2007 the Chinese military shot down an aging weather satellite, followed by the US Navy shooting down a defunct spy satellite in February 2008.
Jamming.
Due to the low received signal strength of satellite transmissions, they are prone to jamming by land-based transmitters. Such jamming is limited to the geographical area within the transmitter's range. GPS satellites are potential targets for jamming, but satellite phone and television signals have also been subjected to jamming.
Also, it is trivial to transmit a carrier radio signal to a geostationary satellite and thus interfere with the legitimate uses of the satellite's transponder. It is common for Earth stations to transmit at the wrong time or on the wrong frequency in commercial satellite space, and dual-illuminate the transponder, rendering the frequency unusable. Satellite operators now have sophisticated monitoring that enables them to pinpoint the source of any carrier and manage the transponder space effectively. 

</doc>
<doc id="27684" url="http://en.wikipedia.org/wiki?curid=27684" title="Steampunk">
Steampunk

Steampunk refers to a subgenre of science fiction and sometimes fantasy—also in recent years a fashion and lifestyle movement—that incorporates technology and aesthetic designs inspired by 19th-century industrial steam-powered machinery. Although its literary origins are sometimes associated with the cyberpunk genre, steampunk works are often set in an alternative history of the 19th century's British Victorian era or American "Wild West", in a post-apocalyptic future during which steam power has maintained mainstream usage, or in a fantasy world that similarly employs steam power. It may, therefore, be described as neo-Victorian. Steampunk perhaps most recognisably features anachronistic technologies or retro-futuristic inventions as people in the 19th century might have envisioned them, and is likewise rooted in the era's perspective on fashion, culture, architectural style, and art. Such technology may include fictional machines like those found in the works of H. G. Wells and Jules Verne, or the modern authors Philip Pullman, Scott Westerfeld, Stephen Hunt and China Miéville. Other examples of steampunk contain alternative history-style presentations of such technology as lighter-than-air airships, analogue computers, or such digital mechanical computers as Charles Babbage's Analytical Engine.
Steampunk may also incorporate additional elements from the genres of fantasy, horror, historical fiction, alternate history, or other branches of speculative fiction, making it often a hybrid genre. The term "steampunk"'s first known appearance was in 1987, though it now retroactively refers to many works of fiction created even as far back as the 1950s or 1960s.
Steampunk also refers to any of the artistic styles, clothing fashions, or subcultures, that have developed from the aesthetics of steampunk fiction, Victorian-era fiction, art nouveau design, and films from the mid-20th century. Various modern utilitarian objects have been modded by individual artisans into a pseudo-Victorian mechanical "steampunk" style, and a number of visual and musical artists have been described as steampunk.
History.
Precursors.
Steampunk is influenced by and often adopts the style of the 19th-century scientific romances of Jules Verne, H.G. Wells, and Mary Shelley. Several works of art and fiction significant to the development of the genre were produced before the genre had a name. Perhaps the first steampunk short story is "The Aerial Burglar" (1844) by Percival Leigh. The oldest precursor of this genre in film, Fritz Lang's masterpiece, "Metropolis" (1927), may be the single most important early film to represent steampunk as an emerging stylistic genre. "Titus Alone" (1959), by Mervyn Peake, anticipated many of the tropes of steampunk, and the film "Brazil" (1985) was an important early cinematic influence toward creating the genre.
In fine art, Remedios Varo's paintings combine elements of Victorian dress, fantasy, and technofantasy imagery. In television, one of the earliest mainstream manifestations of the steampunk ethos was the original CBS television series "The Wild Wild West" (1965–69), which inspired the film "Wild Wild West" (1999). In print, the "A Nomad of the Time Streams" trilogy by Michael Moorcock, which began in 1971 with "The Warlord of the Air", was also an influential precursor.
Origin of the term.
Although many works now considered seminal to the genre were published in the 1960s and 1970s, the term "steampunk" originated in the late 1980s as a tongue-in-cheek variant of "cyberpunk". It was coined by science fiction author K. W. Jeter, who was trying to find a general term for works by Tim Powers ("The Anubis Gates", 1983); James Blaylock ("Homunculus", 1986); and himself ("Morlock Night", 1979, and "Infernal Devices", 1987)—all of which took place in a 19th-century (usually Victorian) setting and imitated conventions of such actual Victorian speculative fiction as H. G. Wells' "The Time Machine". In a letter to science fiction magazine "Locus", printed in the April 1987 issue, Jeter wrote:
Dear Locus,
Enclosed is a copy of my 1979 novel "Morlock Night"; I'd appreciate your being so good as to route it Faren Miller, as it's a prime piece of evidence in the great debate as to who in "the Powers/Blaylock/Jeter fantasy triumvirate" was writing in the "gonzo-historical manner" first. Though of course, I did find her review in the March Locus to be quite flattering.
Personally, I think Victorian fantasies are going to be the next big thing, as long as we can come up with a fitting collective term for Powers, Blaylock and myself. Something based on the appropriate technology of the era; like 'steam-punks', perhaps.—K.W. Jeter
Modern steampunk.
While Jeter's "Morlock Night" and "Infernal Devices", Powers' "The Anubis Gates", and Blaylock's "Lord Kelvin's Machine" were the first novels to which Jeter's neologism would be applied, the three authors gave the term little thought at the time. They were far from the first modern science fiction writers to speculate on the development of steam-based technology or alternative histories. Keith Laumer's "Worlds of the Imperium" (1962) and Ronald W. Clark's "Queen Victoria's Bomb" (1967) apply modern speculation to past-age technology and society. Michael Moorcock's "Warlord of the Air" (1971) is another early example. Harry Harrison's novel "A Transatlantic Tunnel, Hurrah!" (1973) portrays a British Empire of an alternative year 1973, full of atomic locomotives, coal-powered flying boats, ornate submarines, and Victorian dialogue. In February 1980 Richard A. Lupoff and Steve Stiles published the first "chapter" of their 10-part comic strip "The Adventures of Professor Thintwhistle and His Incredible Aether Flyer".
The first use of the word in a title was in Paul Di Filippo's 1995 "Steampunk Trilogy", consisting of three short novels: "Victoria", "Hottentots", and "Walt and Emily", which, respectively, imagine the replacement of Queen Victoria by a human/newt clone, an invasion of Massachusetts by Lovecraftian monsters, and a love affair between Walt Whitman and Emily Dickinson.
Relationship to Retrofuturism.
Superficially, steampunk may resemble retrofuturism. Indeed, both sensibilities recall "the older but still modern eras in which technological change seemed to anticipate a better world, one remembered as relatively innocent of industrial decline." But, where retrofuturism is primarily backward looking and relies on stylistic pastiche, steampunk embraces a broader lifestyle and creative vision. One of steampunk’s most significant contributions is the way in which it mixes the digital with the handmade. As scholars Rachel Bowser and Brian Croxall put it, “the tinkering and tinker-able technologies within steampunk invite us to roll up our sleeves and get to work re-shaping our contemporary world.” In this respect, steampunk bears more in common with DIY craft and making.
Art, entertainment, and media.
Art and design.
Many of the visualisations of steampunk have their origins with, among others, Walt Disney's film "20,000 Leagues Under the Sea" (1954), including the design of the story's submarine the "Nautilus", its interiors, and the crew's underwater gear; and George Pal's film "The Time Machine" (1960), with the design of the time machine itself. This theme is also carried over to Disney's theme parks in the designs of The Mysterious Island section of Tokyo DisneySea theme park and Disneyland Paris' Discoveryland area.
Aspects of steampunk design emphasise a balance between the form and function. So too is it like the Arts and Crafts Movement. But John Ruskin, William Morris, and the other reformers in the late nineteenth century rejected machines and industrial production. On the other hand, steampunk enthusiasts present a “non-luddite critique of technology.”
Various modern utilitarian objects have been modified by enthusiasts into a pseudo-Victorian mechanical "steampunk" style. Example objects include computer keyboards and electric guitars. The goal of such redesigns is to employ appropriate materials (such as polished brass, iron, wood, and leather) with design elements and craftsmanship consistent with the Victorian era, rejecting the aesthetic of industrial design.
In 1994, the Paris Metro station at Arts et Métiers was redesigned by Belgian artist Francois Schuiten in steampunk style to honor the works of Jules Verne. The station is reminiscent of a submarine, sheathed in brass with giant cogs in the ceiling and portholes that look out onto fanciful scenes.
The artist group "Kinetic Steam Works" brought a working steam engine to the Burning Man festival in 2006 and 2007. The group's founding member, Sean Orlando, created a Steampunk Tree House (in association with a group of people who would later form the "Five Ton Crane Arts Group") that has been displayed at a number of festivals. The Steampunk Tree House is now permanently installed at the Dogfish Head Brewery in Milton, Delaware.
In May–June 2008, multimedia artist and sculptor Paul St George exhibited outdoor interactive video installations linking London and Brooklyn, New York, in a Victorian era-styled telectroscope. Utilising this device, New York promoter Evelyn Kriete organised a transatlantic wave between steampunk enthusiasts from both cities, briefly prior to White Mischief's "Around the World in 80 Days" steampunk-themed event.
In 2009, artist Tim Wetherell created a large wall piece for Questacon representing the concept of the clockwork universe. This steel artwork contains moving gears, a working clock, and a movie of the moon's terminator in action. The 3D moon movie was created by Antony Williams.
From October 2009 through February 2010, the Museum of the History of Science, Oxford hosted the first major exhibition of steampunk art objects, curated and developed by New York artist and designer, who also exhibited his own "electro-futuristic" lighting sculptures and presented by Dr. Jim Bennett, museum director. From redesigned practical items to fantastical contraptions, this exhibition showcased the work of eighteen steampunk artists from across the globe. The exhibition proved to be the most successful and highly attended in the museum's history and attracted more than eighty thousand visitors. The event was detailed in the official artist's journal, "The Art of Steampunk" by curator Donovan.
In November 2010, The Libratory Steampunk Art Gallery was opened by Damien McNamara in Oamaru, New Zealand. Created from papier-mâché to resemble a large subterranean cave and filled with industrial equipment from yesteryear, rayguns and general steampunk quirks. Its purpose is to provide a place for steampunkers in the region to display artwork for sale all year long. A year later, a more permanent gallery, Steampunk HQ, was opened in the former Meeks Grain Elevator Building across the road from The Woolstore, and has since become a notable tourist attraction for Oamaru.
In 2012, the "Mobilis in Mobili: An Exhibition of Steampunk Art and Appliance" art exhibit made its debut. Originally located at New York City's Wooster Street Social Club (itself the subject of the television series "NY Ink"), the exhibit featured working steampunk tattoo systems designed, respectively, by Bruce Rosenbaum of and owner of the Steampunk House, Joey "Dr. Grymm" Marsocci., and Christopher Conte showing very different approaches. "bicycles, cell phones, guitars, timepieces and entertainment systems" rounded out the display. The opening night exhibition featured a live performance by steampunk band Frenchy and the Punk.
In November 2014, Sky Harbor Airport in Phoenix, AZ opened a museum exhibit entitled "Steampunk: The Exquisite Adventure", featuring both local and nationally known Steampunk artisans. The displays were originally part of an exhibit at Scottsdale Public Library.
Fashion.
Steampunk fashion has no set guidelines but tends to synthesize modern styles with influences from the Victorian era. This may include bustles, corsets, gowns, and petticoats; suits with waistcoats, coats, top hats, tailcoats and spats; or military-inspired garments. Steampunk-influenced outfits are usually accented with several technological and "period" accessories: timepieces, parasols, flying/driving goggles, and ray guns. Modern accessories like cell phones or music players can be found in steampunk outfits, after being modified to give them the appearance of Victorian-made objects. Post-apocalyptic elements, such as gas masks, ragged clothing and tribal motifs, can also be included. Aspects of steampunk fashion have been anticipated by mainstream high fashion, the Lolita fashion and aristocrat styles, neo-Victorianism, and the romantic goth subculture.
In 2005, Kate Lambert, known as "Kato", founded the first steampunk clothing company, "Steampunk Couture", mixing Victorian and post-apocalyptic influences. In 2013, IBM predicted, based on an analysis of more than a half million public posts on message boards, blogs, social media sites and news sources, "that 'steampunk,' a subgenre inspired by the clothing, technology and social mores of Victorian society, will be a major trend to bubble up and take hold of the retail industry". Indeed, high fashion lines such as Prada, Dolce & Gabbana, Versace, Chanel and Christian Dior had already been introducing steampunk styles on the fashion runways. And in episode 7 of "Lifetime"'s "Project Runway: Under the Gunn" reality series, contestants were challenged to create "avant-garde" "steampunk chic" looks.
Literature.
The educational book, "Elementary BASIC - Learning to Program Your Computer in BASIC with Sherlock Holmes" (1981) by Henry Singer and Andrew Ledgar, may have been the first fictional work to depict the use of Charles Babbage's Analytical Engine in an adventure story. The instructional book, aimed at young programming students, depicts Holmes using the engine as an aid in his investigations, and offer program listings to perform simple data processing tasks required to solve the fictional cases. The book even describes a possible enhancement to Babbage's machine; a device that allows the engine to be used remotely, through telegraph lines. Companion volumes, "Elementary Pascal - Learning to Program Your Computer in Pascal with Sherlock Holmes" and "From Baker Street to Binary - An Introduction to Computers and Computer Programming with Sherlock Holmes", were also written.
In 1988, the first version of the science fiction roleplaying game "" was published. It is set in an alternative history in which certain discredited Victorian scientific theories were probable, thus leading to new technologies. Contributing authors included Frank Chadwick, Loren Wiseman, and Marcus Rowland.
William Gibson and Bruce Sterling's novel "The Difference Engine" (1990) is often credited with bringing widespread awareness of steampunk. This novel applies the principles of Gibson and Sterling's cyberpunk writings to an alternative Victorian era where Ada Lovelace and Charles Babbage's proposed steam-powered mechanical computer, which Babbage called a difference engine (a later, more general-purpose version was known as an analytical engine), was actually built, and led to the dawn of the information age more than a century "ahead of schedule". This setting was different from most steampunk settings in that it takes a dim and dark view of this future rather than the more prevalent utopian versions.
Nick Gevers's original anthology "Extraordinary Engines" (2008) features newer steampunk stories by some of the genre's writers, as well as other science fiction and fantasy writers experimenting with neo-Victorian conventions. A retrospective reprint anthology of steampunk fiction was released, also in 2008, by Tachyon Publications; edited by Ann and Jeff VanderMeer and appropriately entitled "Steampunk", it is a collection of stories by James Blaylock, whose "Narbondo" trilogy is typically considered steampunk; Jay Lake, author of the novel "Mainspring", sometimes labeled "clockpunk"; the aforementioned Michael Moorcock; as well as Jess Nevins, known for his annotations to "The League of Extraordinary Gentlemen" (first published in 1999).
Younger readers have also been targeted with steampunk themes by authors such as Philip Reeve and Scott Westerfeld. Reeve's quartet "Mortal Engines" is set far in Earth's future where giant moving cities consume each other in a battle for resources, a concept Reeve coined as "Municipal Darwinism". Westerfeld's "Leviathan" trilogy is set during the First World War, fought between the "clankers" (Central Powers), who use steam technology, and "darwinists" (Allied Powers), who use genetically engineered creatures instead of machines.
While most of the original steampunk works had a historical setting, later works often place steampunk elements in a fantasy world with little relation to any specific historical era. Historical steampunk tends to be science fiction that presents an alternative history; it also contains real locales and persons from history with alternative fantasy technology. "Fantasy-world steampunk", such as China Miéville's "Perdido Street Station", Alan Campbell's "Scar Night", and Stephen Hunt's Jackelian novels, on the other hand, presents steampunk in a completely imaginary fantasy realm, often populated by legendary creatures coexisting with steam-era and other anachronistic technologies. However, the works of China Miéville and similar authors are sometimes referred to as belonging to the 'New Weird' rather than steampunk.
Self-described author of "far-fetched fiction" Robert Rankin has increasingly incorporated elements of steampunk into narrative worlds, both Victorian and re-imagined contemporary. In 2009, he was made a Fellow of the Victorian Steampunk Society.
The comic book series "Hellboy" created by Mike Mignola, and the two Hellboy films featuring Ron Perlman and directed by Guillermo del Toro, all have steampunk elements. In the comic book and the first (2004) film, Karl Ruprecht Kroenen is a Nazi SS scientist who has an addiction to surgery and many mechanical prostheses, including a clockwork heart. The character Johann Krauss features in the comic and in the second film, "" (2008), as an ectoplasmic medium (a gaseous form in a partly mechanical suit). This second film also features the Golden Army itself, which is a collection of 4,900 mechanical steampunk warriors.
Steampunk settings.
Alternative world.
Since the 1990s, the application of the steampunk label has expanded beyond works set in recognisable historical periods, to works set in fantasy worlds that rely heavily on steam- or spring-powered technology. One of the earliest short stories relying on steam-powered flying machines is "The Aerial Burglar" of 1844. An example from juvenile fiction is The Edge Chronicles by Paul Stewart and Chris Riddell.
Fantasy steampunk settings abound in tabletop and computer role-playing games. Notable examples include "Skies of Arcadia", ', and '.
The gnomes and goblins in "World of Warcraft" also have technological societies that could be described as steampunk as they are vastly ahead of the technologies of men, but are not magical like those of the Elves.
The Dwarves of the Elder Scrolls series, described therein as a race of Elves called the Dwemer, also use steam powered machinery, with gigantic brass like gears throughout their underground cities. However, magical means are used to keep ancient devices in motion despite the Dwemer's ancient disappearance.
Amidst the historical and fantasy subgenres of steampunk is a type which takes place in a hypothetical future or a fantasy equivalent of our future, involving the domination of steampunk-style technology and aesthetics. Examples include Jean-Pierre Jeunet & Marc Caro's "The City of Lost Children" (1995), "Turn A Gundam" (1999–2000), "Trigun" and Disney's film "Treasure Planet" (2002). In 2011, musician Thomas Dolby heralded his return to music after a 20-year hiatus with an online steampunk alternate fantasy world called the Floating City, to promote his album, "A Map of the Floating City".
American West.
Another setting is "Western" steampunk, which overlaps with both the Weird West and Science fiction Western subgenres. Several other categories have arisen, sharing similar names, including dieselpunk, clockworkpunk, and others. Most of these terms were coined as supplements to the GURPS role playing game, and are not used in other contexts.
Fantasy and horror.
Kaja Foglio introduced the term "Gaslight Romance", gaslamp fantasy, which John Clute and John Grant define as "steampunk stories ... most commonly set in a romanticised, smoky, 19th-century London, as are Gaslight Romances. But the latter category focuses nostalgically on icons from the late years of that century and the early years of the 20th century--on Dracula, Jekyll and Hyde, Jack the Ripper, Sherlock Holmes and even Tarzan--and can normally be understood as combining supernatural fiction and recursive fantasy, though some gaslight romances can be read as fantasies of history." Some, such as author/artist James Richardson-Brown use the term "steamgoth" to refer to steampunk expressions of fantasy and horror with a "darker" bent.
Post-apocalyptic.
Mary Shelley's The Last Man, set near the end of the 21st century after a plague had brought down civilization, was probably the ancestor of post-apocalyptic steampunk literature. Post-apocalyptic steampunk is set in a world where some cataclysm has precipitated the fall of civilization and steam power once again gains ascendancy, such as in Hayao Miyazaki's post-apocalyptic anime "Future Boy Conan" (1978), where a war fought with superweapons has devastated the planet. Robert Brown's novel, "The Wrath of Fate" (as well as much of Abney Park's music) is set in A Victorianesque world where an apocalypse was set into motion by a time-traveling mishap. Cherie Priest's Boneshaker series is set in a world where a zombie apocalypse happened during the Civil War era. The Peshawar Lancers by S.M. Stirling is set in a post-apocalyptic future in which a meteor shower in 1878 caused the collapse of Industrialized civilization. The movie 9 (which might be better classified as "stitchpunk" but had a large influence on steampunk) is also set in a post-apocalyptic world after a self-aware war machine ran amok. Steampunk Magazine even published a book called "A Steampunk's Guide to the Apocalypse", about how steampunks could survive should such a thing actually happen.
Victorian.
In general, the category includes any recent science fiction that takes place in a recognizable historical period (sometimes an alternate history version of an actual historical period) in which the Industrial Revolution has already begun, but electricity is not yet widespread. It places an emphasis on steam- or spring-propelled gadgets. The most common historical steampunk settings are the Victorian and Edwardian eras, though some in this "Victorian steampunk" category can go as early as the beginning of the Industrial Revolution and as late as the end of World War I.
Some examples of this type include the novel "The Difference Engine", the comic book series "League of Extraordinary Gentlemen", the Disney animated film "", Scott Westerfeld's "Leviathan" trilogy, and the roleplaying game "." The anime film "Steamboy" (2004) is another good example of Victorian steampunk, taking place in an alternate 1866 where steam technology is "far" more advanced than it ever was in real life. Some, such as the comic series "Girl Genius", have their own unique times and places despite partaking heavily of the flavor of historic times and settings.
Karel Zeman's film "The Fabulous World of Jules Verne" (1958) is a very early example of cinematic steampunk. Based on Jules Verne novels, Zeman's film imagines a past based on those novels which never was. Another early example of historical steampunk in cinema includes Hayao Miyazaki's anime films such as "" (1986) and "Howl's Moving Castle" (2004), containing many archetypal anachronisms characteristic of the steampunk genre.
"Historical" steampunk usually leans more towards science fiction than fantasy, but a number of historical steampunk stories have incorporated magical elements as well. For example, "Morlock Night", written by K. W. Jeter, revolves around an attempt by the wizard Merlin to raise King Arthur to save the Britain in 1892 from an invasion of Morlocks from the future.
Paul Guinan's "Boilerplate", a 'biography' of a robot in the late 19th century, began as a website that garnered international press coverage when people began believing that Photoshop images of the robot with historic personages were real. The site was adapted into an illustrated hardbound book "Boilerplate: History's Mechanical Marvel," and published by Abrams in October 2009. Because the story was not set in an alternative history, and in fact contained accurate information about the Victorian era, some booksellers referred to the tome as "historical steampunk."
Music.
Steampunk music is very broadly defined, as Caroline Sullivan says in "The Guardian": "Internet debates rage about exactly what constitutes the steampunk sound." Abney Park’s lead singer, Robert Brown, very loosely defined it as, "mixing Victorian elements and modern elements."
Joshua Pfeiffer (of Vernian Process) is quoted as saying, “As for Paul Roland, if anyone deserves credit for spearheading Steampunk music, it is him. He was one of the inspirations I had in starting my project. He was writing songs about the first attempt at manned flight, and an Edwardian airship raid in the mid-80’s long before almost anyone else…”. Thomas Dolby is also considered one of the early pioneers of retro-futurist (i.e., steampunk and dieselpunk) music. Amanda Palmer was once quoted as saying, "Thomas Dolby is to Steampunk what Iggy Pop was to Punk!"
Since there is little consensus on the "sound" steampunk music should take, there is a broad range of musical styles and interpretations among steampunk musical acts, from industrial dance and world music to folk rock, Punk cabaret to straightforward punk, Carnatic to industrial, hip-hop to opera (and even industrial hip-hop opera), darkwave to progressive rock, barbershop to big band.
Steampunk has also appeared in the work of musicians who do not specifically identify as steampunk. For example, the music video of "Turn Me On", by David Guetta and featuring Nicki Minaj, takes place in a steampunk universe where Guetta creates human droids. In addition, the album "Clockwork Angels" (2012) and its supporting tour by progressive rock band Rush contain lyrics, themes, and imagery based around steampunk. Similarly, Thomas Dolby headlined the first "Steamstock" outdoor steampunk music festival in Richmond, California, which also featured Steampunk bands Abney Park, Frenchy and the Punk, Lee Presson and the Nails, Vernian Process, and others.
Television and films.
The 1965 television series "The Wild Wild West", as well as the eponymous 1999 film, featured many of the elements of advanced steam-powered technology set in the Wild West time period of the United States.
The 1979 film "Time After Time" has Herbert George "H.G." Wells following a surgeon named John Leslie Stevenson into the future, as John is suspected of being Jack the Ripper. Both use Wells' time machine separately to travel.
The 1982 American TV series "Q.E.D.", set in Edwardian England, starred Sam Waterston as Professor Quentin Everett Deverill (the series title is the character's initials, as well as the Latin phrase "quod erat demonstrandum", which translates as "which was to be demonstrated"). The Professor was an inventor and scientific detective, in the mold of Sherlock Holmes. In the show, the lead character was known primarily by his initials, Q.E.D.
The 1986 Japanese film by Hayao Miyazaki "Castle in the Sky", was heavily influenced by steampunk culture, featuring various air ships and steam-powered contraptions as well as the story line centering around a mysterious island which floats through the sky. This is accomplished not through magic as most stories would resort to but instead relies on massive propellers as is fitting for the Victorian motif.
"The Adventures of Brisco County, Jr.", a Fox Network 1993 TV science fiction-western set in the 1890s, featured elements of steampunk as represented by the character Professor Wickwire, whose inventions were described as "the coming thing".
The short-lived 1995 TV show "Legend" on UPN, set in 1876 Arizona, featured such classic inventions as a steam-driven "quadrovelocipede" and night-vision goggles, and starred John de Lancie as a thinly disguised Nikola Tesla. Alan Moore's and Kevin O'Neill's 1999 "The League of Extraordinary Gentlemen" graphic novel series (and the subsequent 2003 film adaption) greatly popularised the steampunk genre.
The 2007 Syfy miniseries "Tin Man" incorporates a considerable amount of steampunk-inspired themes into a re-imagining of L. Frank Baum's "The Wonderful Wizard of Oz".
The Syfy series "Warehouse 13" (which premiered July 7, 2009) features many steampunk-inspired objects and artifacts, including computer designs created by steampunk artisan Richard Nagy, aka "Datamancer".
The BBC series "Doctor Who" (which premiered in 1963) also incorporates steampunk elements. During season 14 of the show (in 1976), the formerly futuristic looking interior set was replaced with a Victorian-styled wood panel and brass affair. In the 1996 American co-production, the TARDIS interior was re-designed to resemble an almost Victorian library with the central control console made up of eclectic and anachronistic objects. Modified and streamlined for the 2005 revival of the series, the TARDIS console continued to incorporate steampunk elements, including a Victorian typewriter and gramophone. Several storylines can be classed as steampunk, for example: "The Evil of the Daleks" (1966), wherein Victorian scientists invent a time travel device.
Steampunk has begun to attract notice from "mainstream" American sources as well. For example, the episode of the TV series "Castle" entitled "Punked" (which aired on October 11, 2010) prominently featured the steampunk subculture and used Los Angeles-area steampunks (such as the League of STEAM) as extras
Video games.
A variety of styles of video games have used Steampunk settings. "The Chaos Engine" is a run and gun video game inspired by the Gibson/Sterling novel "The Difference Engine" (1990), set in a Victorian steampunk age. Developed by the Bitmap Brothers, it was first released on the Amiga in 1993; a sequel was released in 1996. Other steampunk-styled video games include the first-person shooter "BioShock Infinite" (2013), the "Dishonored" (2012) stealth game, the role-playing games "Final Fantasy VI" (1994), "Final Fantasy IX" (2000), "Dark Chronicle" (2002) and the late Middle Ages/Victorian age styled "Thief" series (1998). The graphic adventure puzzle video games "Myst" (1993), "Riven" (1997), and "" (2001) (all produced by Cyan Worlds) take place in an alternate steampunk universe, where elaborate infrastructures have been built to run on steam power. Many steampunk themes can be found within World of Warcraft particularly the 'Gnome' race within the game. Many of the items which can be created via the Engineering profession are of a steampunk nature and also named in a similar fashion.
Culture and community.
Because of the popularity of steampunk, there is a growing movement of adults that want to establish steampunk as a culture and lifestyle. Some fans of the genre adopt a steampunk aesthetic through fashion, home decor, music, and film. This may be described as neo-Victorianism, which is the amalgamation of Victorian aesthetic principles with modern sensibilities and technologies.
In September 2012, a panel was held at Stan Lee's Comikaze Expo, chaired by steampunk entertainer Veronique Chevalier and with panelists including magician Pop Hadyn and members of the steampunk performance group The League of STEAM, which suggested that because steampunk was inclusive of and incorporated ideas from various other subcultures such as goth, neo-Victorian, and cyberpunk as well as a growing number of fandoms, it was fast becoming a "super-culture" rather than a mere subculture. Other steampunk notables such as Professor Elemental have expressed similar views about steampunk's inclusive diversity.
Some have proposed a steampunk philosophy, sometimes with punk-inspired anti-establishment sentiments, and typically bolstered by optimism about human potential.
Steampunk became a common descriptor for homemade objects on the craft network Etsy between 2009 and 2011, though many of the objects and fashions bear little resemblance to earlier established steampunk descriptions. Thus the craft network may not strike observers as 'sufficiently steampunk' to warrant the description. Comedienne April Winchell, author of the book, "Regretsy: Where DIY meets WTF", cataloged some of the most egregious and humorous examples on her website, "Regretsy". The blog was popular among steampunks and even inspired a music video that went viral in the community and was acclaimed by steampunk "notables."
Social events.
2006 saw the first "SalonCon", a neo-Victorian/steampunk convention. It ran for three consecutive years and featured artists, musicians (Voltaire and Abney Park), authors (Catherynne M. Valente, Ekaterina Sedia, and G. D. Falksen), salons led by people prominent in their respective fields, workshops and panels on steampunk—as well as a seance, ballroom dance instruction, and the Chrononauts' Parade. The event was covered by MTV and "The New York Times". Since then a number of popular steampunk conventions have sprung up the world over, with names like Steamcon (Seattle, WA), the Steampunk World's Fair (Piscataway, NJ) and Up in the Aether: The Steampunk Convention (Dearborn, MI).
Steampunk has also become a regular feature at San Diego Comic-Con International in recent years, with the Saturday of the four-day event being generally known among steampunks as "Steampunk Day", and culminating with a photo-shoot for the local press. In 2010 this was recorded in the Guinness Book of World Records as the world's largest steampunk photo shoot. In 2013, Comic-Con announced four official 2013 T-shirts: one of them featured the official Rick Geary Comic-Con toucan mascot in steampunk attire. The Saturday steampunk "after-party" has also become a major event on the steampunk social calendar; in 2010 the headliners included The Slow Poisoner, Unextraordinary Gentlemen and Voltaire, with Veronique Chevalier as Mistress of Ceremonies and special appearance by the League of STEAM, and in 2011 UXG returned with Abney Park.
Steampunk also has sprung up recently at Renaissance Festivals and Renaissance Faires, nationwide. Some have organised events or a "Steampunk Day", while other Fests simply support an open environment for donning Steampunk attire. The Bristol Renaissance Faire in Kenosha, Wisconsin, on the Wisconsin/Illinois border, featured a Steampunk costume contest during the 2012 season. The previous two seasons featured increasing participation in the phenomenon.
Steampunk also has a growing following in the UK and Europe. The largest European event is "Weekend at the Asylum", held at The Lawn, Lincoln every September since 2009. Organised as a not-for-profit event by the Victorian Steampunk Society, the Asylum is a dedicated steampunk event which takes over much of the historical quarter of Lincoln, England, along with Lincoln Castle. In 2011 there were over 1000 steampunks in attendance. The event features the Empire Ball, Majors Review, Bazaar Eclectica and the international Tea Duelling final.

</doc>
<doc id="27686" url="http://en.wikipedia.org/wiki?curid=27686" title="Spreadsheet">
Spreadsheet

A spreadsheet is an interactive computer application program for organization, analysis and storage of data in tabular form. Spreadsheets developed as computerized simulations of paper accounting worksheets. The program operates on data represented as cells of an array, organized in rows and columns. Each cell of the array is a model–view–controller element that may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells.
Spreadsheet users may adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for "what-if" analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets, and can display data either as text and numerals, or in graphical form.
Besides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial and statistical operations. Such calculations as net present value or standard deviation can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.
Spreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.
LANPAR was the first electronic spreadsheet on mainframe and time sharing computers. VisiCalc was the first electronic spreadsheet on a microcomputer, and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system. Excel now has the largest market share on the Windows and Macintosh platforms. A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form.
Usage.
A spreadsheet consists of a table of "cells" arranged into rows and columns and referred to by the X and Y locations. X locations, the columns, are normally represented by letters, "A", "B", "C", etc., while rows are normally represented by numbers, 1, 2, 3, etc. A single cell can be referred to by addressing its row and column, "C10" for instance. Additionally, spreadsheets have the concept of a "range", a group of cells, normally contiguous. For instance, one can refer to the first ten cells in the first column with the range "A1:A10". This system of cell references was introduced in VisiCalc, and known as "A1 notation".
In modern spreadsheet applications, several spreadsheets, often known as "worksheets" or simply "sheets", are gathered together to form a "workbook". A workbook is physically represented by a file, containing all the data for the book, the sheets and the cells with the sheets. Worksheets are normally represented by tabs that flip between pages, each one containing one of the sheets, although Numbers changes this model significantly. Cells in a multi-sheet book add the sheet name to their reference, for instance, "Sheet 1!C10". Some systems extend this syntax to allow cell references to different workbooks.
Users interact with sheets primarily through the cells. A given cell can hold data by simply entering it in, or a formula, which is normally created by preceding the text with an equals sign. Data might include the string of text codice_1, the number codice_2 or the date codice_3. A formula would begin with the equals sign, codice_4, but this would normally be invisible because the display shows the "result" of the calculation, codice_5 in this case, not the formula itself. This may lead to confusion in some cases.
The key feature of spreadsheets is the ability for a formula to refer to the contents of other cells, which may in turn be the result of a formula. To make such a formula, one simply replaces a number with a cell reference. For instance, the formula codice_6 would produce the result of multiplying the value in cell C10 by the number 5. If C10 holds the value codice_7 the result will be codice_5. But C10 might also hold its own formula referring to other cells, and so on.
The ability to chain formulas together is what gives a spreadsheet its power. Many problems can be broken down into a series of individual mathematical step, and these can be assigned to individual formulas in cells. Some of these formulas can apply to ranges as well, like the codice_9 function that adds up all the numbers within a range.
Spreadsheets share many principles and traits of databases, but spreadsheets and databases are not the same thing. A spreadsheet is essentially just one table, whereas a database is a collection of many tables with machine-readable semantic relationships between them. While it is true that a workbook that contains three sheets is indeed a file containing multiple tables that can interact with each other, it lacks the relational structure of a database. Spreadsheets and databases are interoperable—sheets can be imported into databases to become tables within them, and database queries can be exported into spreadsheets for further analysis.
A spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands, instead of a graphical user interface.
History.
Paper spreadsheets.
The word "spreadsheet" came from "spread" in its sense of a newspaper or magazine item (text or graphics) that covers two facing pages, extending across the center fold and treating the two pages as one large one. The compound word "spread-sheet" came to mean the format used to present book-keeping ledgers—with columns for categories of expenditures across the top, invoices listed down the left margin, and the amount of each payment in the cell where its row and column intersect—which were, traditionally, a "spread" across facing pages of a bound ledger (book for keeping accounting records) or on oversized sheets of paper (termed "analysis paper") ruled into rows and columns in that format and approximately twice as wide as ordinary paper.
Early implementations.
Batch spreadsheet report generator.
A batch "spreadsheet" is indistinguishable from a batch compiler with added input data, producing an output report, "i.e.", a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper "Budgeting Models and System Simulation" by Richard Mattessich. The subsequent work by Mattessich (1964a, Chpt. 9, "Accounting and Analytical Methods") and its companion volume, Mattessich (1964b, "Simulation of the Firm through a Budget Computer Program") applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual "cells".
In 1962 this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled "Business Computer Language" written by Kimball, Stoffells and Walsh and both the book and program were copyrighted in 1966 and years later that copyright was renewed
Applied Data Resources had a FORTRAN preprocessor called Empires.
In the late 1960s Xerox used BCL to develop a more sophisticated version for their timesharing system.
LANPAR spreadsheet compiler.
A key invention in the development of electronic spreadsheets was made by Rene K. Pardo and Remy Landau, who filed in 1970 U.S. Patent on spreadsheet automatic natural order recalculation algorithm. While the patent was initially rejected by the patent office as being a purely mathematical invention, following 12 years of appeals, Pardo and Landau won a landmark court case at the CCPA (Predecessor Court of the Federal Circuit) overturning the Patent Office in 1983—establishing that "something does not cease to become patentable merely because the point of novelty is in an algorithm." However, in 1995 the United States Court of Appeals for the Federal Circuit ruled the patent unenforceable.
The actual software was called LANPAR—LANguage for Programming Arrays at Random. This was conceived and entirely developed in the summer of 1969 following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having computer calculating results in the right order ("Forward Referencing/Natural Order Calculation"). Pardo and Landau developed and implemented the software in 1969.
LANPAR was used by Bell Canada, AT&T and the 18 operating telcos nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first "non-procedural" computer languages) as opposed to left-to-right, top to bottom sequence for calculating the results in each cell that was used by VisiCalc, Supercalc, and the first version of Multiplan. Without forward referencing/natural order calculation, the user had to manually recalculate the spreadsheet as many times as necessary until the values in all the cells had stopped changing. Forward Referencing/Natural Order Calculation by a compiler was the cornerstone functionality required for any spreadsheet to be practical and successful.
The LANPAR system was implemented on GE400 and Honeywell 6000 online timesharing systems enabling users to program remotely via computer terminals and modems. Data could be entered dynamically either by paper tape, specific file access, on line, or even external data bases. Sophisticated mathematical expressions including logical comparisons and "if/then" statements could be used in any cell, and cells could be presented in any order.
Autoplan/Autotab spreadsheet programming language.
In 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. "AutoPlan" ran on GE’s Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name "AutoTab". (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.) AutoPlan/AutoTab was not a WYSIWYG interactive spreadsheet program, it was a simple scripting language for spreadsheets. The user defined the names and labels for the rows and columns, then the formulas that defined each row or column.
Works Records System.
The Works Records System was a spreadsheet system designed in 1974 at ICI in the UK. It was a company-internal system that ran on IBM mainframes, and was in use essentially unchanged for 27 years. It was intended for use by non-programmers and had a WYSIWIG interface.
IBM Financial Planning and Control System.
The IBM Financial Planning and Control System was developed in 1976, by Brian Ingham at IBM Canada. It was implemented by IBM in at least 30 countries. It ran on an IBM mainframe and was among the first applications for financial planning developed with APL that completely hid the programming language from the end-user. Through IBM's VM operating system, it was among the first programs to auto-update each copy of the application as new versions were released. Users could specify simple mathematical relationships between rows and between columns. Compared to any contemporary alternatives, it could support very large spreadsheets. It loaded actual financial data drawn from the legacy batch system into each user's spreadsheet on a monthly basis. It was designed to optimize the power of APL through object kernels, increasing program efficiency by as much as 50 fold over traditional programming approaches.
APLDOT modeling language.
An example of an early "industrial weight" spreadsheet was APLDOT, developed in 1976 at the United States Railway Association on an IBM 360/91, running at The Johns Hopkins University Applied Physics Laboratory in Laurel, MD. The application was used successfully for many years in developing such applications as financial and costing models for the US Congress and for Conrail. APLDOT was dubbed a "spreadsheet" because financial analysts and strategic planners used it to solve the same problems they addressed with paper spreadsheet pads.
VisiCalc.
Because of Dan Bricklin and Bob Frankston's implementation of VisiCalc on the Apple II in 1979 and the IBM PC in 1981, the spreadsheet concept became widely known in the late 1970s and early 1980s. VisiCalc was the first spreadsheet that combined all essential features of modern spreadsheet applications (except for forward referencing/natural order recalculation), such as WYSIWYG interactive user interface, automatic recalculation, status and formula lines, range copying with relative and absolute references, formula building by selecting referenced cells. Unaware of LANPAR at the time "PC World" magazine called VisiCalc the first electronic spreadsheet.
Bricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite a number of sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc, the first application that turned the personal computer from a hobby for computer enthusiasts into a business tool.
VisiCalc went on to become the first killer app, an application that was so compelling, people would buy a particular computer just to use it. VisiCalc was in no small part responsible for the Apple II's success. The program was later ported to a number of other early computers, notably CP/M machines, the Atari 8-bit family and various Commodore platforms. Nevertheless, VisiCalc remains best known as an Apple II program.
Lotus 1-2-3 and other MS-DOS spreadsheets.
The acceptance of the IBM PC following its introduction in August, 1981, began slowly, because most of the programs available for it were translations from other computer models. Things changed dramatically with the introduction of Lotus 1-2-3 in November, 1982, and release for sale in January, 1983. Since it was written especially for the IBM PC, it had good performance and became the killer app for this PC. Lotus 1-2-3 drove sales of the PC due to the improvements in speed and graphics compared to VisiCalc on the Apple II.
Lotus 1-2-3, along with its competitor Borland Quattro, soon displaced VisiCalc. Lotus 1-2-3 was released on January 26, 1983, started outselling then-most-popular VisiCalc the very same year, and for a number of years was the leading spreadsheet for DOS.
Microsoft Excel.
Microsoft released the first version of Excel for the Macintosh on September 30, 1985, and then ported it to Windows, with the first version being numbered 2.05 (to synchronize with the Macintosh version 2.2) and released in November 1987. The Windows 3.x platforms of the early 1990s made it possible for Excel to take market share from Lotus. By the time Lotus responded with usable Windows products, Microsoft had begun to assemble their Office suite. By 1995, Excel was the market leader, edging out Lotus 1-2-3, and in 2013, IBM discontinued Lotus-1-2-3 altogether.
Open source software.
Gnumeric is a free, cross-platform spreadsheet program that is part of the GNOME Free Software Desktop Project. OpenOffice.org Calc and the very closely related LibreOffice Calc (using the LGPL license) are free and open-source spreadsheets.
Web based spreadsheets.
With the advent of advanced web technologies such as Ajax circa 2005, a new generation of online spreadsheets has emerged. Equipped with a rich Internet application user experience, the best web based online spreadsheets have many of the features seen in desktop spreadsheet applications. Some of them such as Office Online, ZOHO, Google Spreadsheets, EditGrid or ZK Spreadsheet also have strong multi-user collaboration features and / or offer real time updates from remote sources such as stock prices and currency exchange rates.
Other products.
A number of companies have attempted to break into the spreadsheet market with programs based on very different paradigms. Lotus introduced what is likely the most successful example, Lotus Improv, which saw some commercial success, notably in the financial world where its powerful data mining capabilities remain well respected to this day. 
Quantrix is built on the same paradigm as the discontinued Lotus Improv, except has many powerful new features, making it the application of choice for many financial professionals worldwide. 
Spreadsheet 2000 attempted to dramatically simplify formula construction, but was generally not successful.
Concepts.
The main concepts are those of a grid of cells, called a sheet, with either raw data, called values, or formulas in the cells. Formulas say how to mechanically compute new values from existing values. Values are generally numbers, but can also be pure text, dates, months, etc. Extensions of these concepts include logical spreadsheets. Various tools for programming sheets, visualizing data, remotely connecting sheets, displaying cells' dependencies, etc. are commonly provided.
Cells.
A "cell" can be thought of as a box for holding data. A single cell is usually referenced by its column and row (A2 would represent the cell containing the value 10 in the example table below). Usually rows, representing the dependent variables, are referenced in decimal notation starting from 1, while columns representing the independent variables use 26-adic bijective numeration using the letters A-Z as numerals. Its physical size can usually be tailored to its content by dragging its height or width at box intersections (or for entire columns or rows by dragging the column- or row-headers).
An array of cells is called a "sheet" or "worksheet". It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its own containing cell).
A cell may contain a value or a formula, or it may simply be left empty.
By convention, formulas usually begin with = sign.
Values.
A value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.
The Spreadsheet "Value Rule"
Computer scientist Alan Kay used the term "value rule" to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell.
The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.
Automatic recalculation.
A standard of spreadsheets since the 1980s, this optional feature eliminates the need to manually request the spreadsheet program to recalculate values (nowadays typically the default option unless specifically 'switched off' for large spreadsheets, usually to improve performance). Some earlier spreadsheets required a manual request to recalculate, since recalculation of large or complex spreadsheets often reduced data entry speed. Many modern spreadsheets still retain this option.
Real-time update.
This feature refers to updating a cell's contents periodically with a value from an external source—such as a cell in a "remote" spreadsheet. For shared, Web-based spreadsheets, it applies to "immediately" updating cells another user has updated. All dependent cells must be updated also.
Locked cell.
Once entered, selected cells (or the entire spreadsheet) can optionally be "locked" to prevent accidental overwriting. Typically this would apply to cells containing formulas but might be applicable to cells containing "constants" such as a kilogram/pounds conversion factor (2.20462262 to eight decimal places). Even though individual cells are marked as locked, the spreadsheet data are not protected until the feature is activated in the file preferences.
Data format.
A cell or range can optionally be defined to specify how the value is displayed. The default display format is usually set by its initial content if not specifically previously set, so that for example "31/12/2007" or "31 Dec 2007" would default to the cell format of "date".
Similarly adding a % sign after a numeric value would tag the cell as a percentage cell format. The cell contents are not changed by this format, only the displayed value.
Some cell formats such as "numeric" or "currency" can also specify the number of decimal places.
This can allow invalid operations (such as doing multiplication on a cell containing a date), resulting in illogical results without an appropriate warning.
Cell formatting.
Depending on the capability of the spreadsheet application, each cell (like its counterpart the "style" in a word processor) can be separately formatted using the attributes of either the content (point size, color, bold or italic) or the cell (border thickness, background shading, color). To aid the readability of a spreadsheet, cell formatting may be conditionally applied to data; for example, a negative number may be displayed in red.
A cell's formatting does not typically affect its content and depending on how cells are referenced or copied to other worksheets or applications, the formatting may not be carried with the content.
Named cells.
In most implementations, a cell, or group of cells in a column or row, can be "named" enabling the user to refer to those cells by a name rather than by a grid reference. Names must be unique within the spreadsheet, but when using multiple sheets in a spreadsheet file, an identically named cell range on each sheet can be used if it is distinguished by adding the sheet name. One reason for this usage is for creating or running macros that repeat a command across many sheets. Another reason is that formulas with named variables are readily checked against the algebra they are intended to implement (they resemble Fortran expressions). Use of named variables and named functions also makes the spreadsheet structure more transparent.
Cell reference.
In place of a named cell, an alternative approach is to use a cell (or grid) reference. Most cell references indicate another cell in the same spreadsheet, but a cell reference can also refer to a cell in a different sheet within the same spreadsheet, or (depending on the implementation) to a cell in another spreadsheet entirely, or to a value from a remote application.
A typical cell reference in "A1" style consists of one or two case-insensitive letters to identify the column (if there are up to 256 columns: A–Z and AA–IV) followed by a row number (e.g. in the range 1–65536). Either part can be relative (it changes when the formula it is in is moved or copied), or absolute (indicated with $ in front of the part concerned of the cell reference). The alternative "R1C1" reference style consists of the letter R, the row number, the letter C, and the column number; relative row or column numbers are indicated by enclosing the number in square brackets. Most current spreadsheets use the A1 style, some providing the R1C1 style as a compatibility option.
When the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), cause the computer to fetch the value of the named cell(s).
A cell on the same "sheet" is usually addressed as:
 =A1
A cell on a different sheet of the same spreadsheet is usually addressed as:
 =SHEET2!A1 (that is; the first cell in sheet 2 of same spreadsheet).
Some spreadsheet implementations allow a cell references to another spreadsheet (not the current open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:
 ='C:\Documents and Settings\Username\My spreadsheets\[main sheet]Sheet1!A1
In a spreadsheet, references to cells automatically update when new rows or columns are inserted or deleted. Care must be taken, however, when adding a row immediately before a set of column totals to ensure that the totals reflect the additional rows values—which they often do not.
A circular reference occurs when the formula in one cell refers—directly, or indirectly through a chain of cell references—to another cell that refers back to the first cell. Many common errors cause circular references. However, some valid techniques use circular references. These techniques, after many spreadsheet recalculations, (usually) converge on the correct values for those cells.
Cell ranges.
Likewise, instead of using a named range of cells, a range reference can be used. Reference to a range of cells is typically of the form (A1:A6), which specifies all the cells in the range A1 through to A6. A formula such as "=SUM(A1:A6)" would add all the cells specified and put the result in the cell containing the formula itself.
Sheets.
In the earliest spreadsheets, cells were a simple two-dimensional grid. Over time, the model has expanded to include a third dimension, and in some cases a series of named grids, called sheets. The most advanced examples allow inversion and rotation operations which can slice and project the data set in various ways.
Formulas.
A formula identifies the calculation needed to place the result in the cell it is contained within. A cell containing a formula therefore has two display components; the formula itself and the resulting value. The formula is normally only shown when the cell is selected by "clicking" the mouse over a particular cell; otherwise it contains the result of the calculation.
A formula assigns values to a cell or range of cells, and typically has the format:
where the expression consists of:
When a cell contains a formula, it often contains references to other cells. Such a cell reference is a type of variable. Its value is the value of the referenced cell or some derivation of it. If that cell in turn references other cells, the value depends on the values of those. References can be relative (e.g., codice_13, or codice_14), absolute (e.g., codice_25, or codice_26) or mixed row– or column-wise absolute/relative (e.g., codice_27 is column-wise absolute and codice_28 is row-wise absolute).
The available options for valid formulas depends on the particular spreadsheet implementation but, in general, most arithmetic operations and quite complex nested conditional operations can be performed by most of today's commercial spreadsheets. Modern implementations also offer functions to access custom-build functions, remote data, and applications.
A formula may contain a condition (or nested conditions)—with or without an actual calculation—and is sometimes used purely to identify and highlight errors. In the example below, it is assumed the sum of a column of percentages (A1 through A6) is tested for validity and an explicit message put into the adjacent right-hand cell.
Further examples:
The best way to build up conditional statements is step by step composing followed by trial and error testing and refining code.
A spreadsheet does not, in fact, have to contain any formulas at all, in which case it could be considered merely a collection of data arranged in rows and columns (a database) like a calendar, timetable or simple list. Because of its ease of use, formatting and hyperlinking capabilities, many spreadsheets are used solely for this purpose.
Functions.
Spreadsheets usually contain a number of supplied functions, such as arithmetic operations (for example, summations, averages and so forth), trigonometric functions, statistical functions, and so forth. In addition there is often a provision for "user-defined functions". In Microsoft Excel these functions are defined using Visual Basic for Applications in the supplied Visual Basic editor, and such functions are automatically accessible on the worksheet. In addition, programs can be written that pull information from the worksheet, perform some calculations, and report the results back to the worksheet. In the figure, the name "sq" is user-assigned, and function "sq" is introduced using the "Visual Basic" editor supplied with Excel. "Name Manager" displays the spreadsheet definitions of named variables "x" & "y".
Subroutines.
Functions themselves cannot write into the worksheet, but simply return their evaluation. However, in Microsoft Excel, subroutines can write values or text found within the subroutine directly to the spreadsheet. The figure shows the Visual Basic code for a subroutine that reads each member of the named column variable "x", calculates its square, and writes this value into the corresponding element of named column variable "y". The "y" column contains no formula because its values are calculated in the subroutine, not on the spreadsheet, and simply are written in.
Remote spreadsheet.
Whenever a reference is made to a cell or group of cells that are not located within the current physical spreadsheet file, it is considered as accessing a "remote" spreadsheet. The contents of the referenced cell may be accessed either on first reference with a manual update or more recently in the case of web based spreadsheets, as a near real time value with a specified automatic refresh interval.
Charts.
Many spreadsheet applications permit charts, graphs or histograms to be generated from specified groups of cells that are dynamically re-built as cell contents change. The generated graphic component can either be embedded within the current sheet or added as a separate object.
Multi-dimensional spreadsheets.
In the late 1980s and early 1990s, first Javelin Software and later Lotus Improv appeared and unlike models in a conventional spreadsheet, they utilized models built on objects called variables, not on data in cells of a report. These multi-dimensional spreadsheets enabled viewing data and algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users of Javelin could move through the connections between variables on a diagram while seeing the logical roots and branches of each variable. This is an example of what is perhaps its primary contribution of the earlier Javelin—the concept of traceability of a user's logic or model structure through its twelve views. A complex model can be dissected and understood by others who had no role in its creation, and this remains unique even today. Javelin was used primarily for financial modeling, but was also used to build instructional models in college chemistry courses, to model the world's economies, and by the military in the early Star Wars project. It is still in use by institutions for which model integrity is mission critical.
In these programs, a time series, or any variable, was an object in itself, not a collection of cells that happen to appear in a row or column. Variables could have many attributes, including complete awareness of their connections to all other variables, data references, and text and image notes. Calculations were performed on these objects, as opposed to a range of cells, so adding two time series automatically aligns them in calendar time, or in a user-defined time frame. Data were independent of worksheets—variables, and therefore data, could not be destroyed by deleting a row, column or entire worksheet. For instance, January's costs are subtracted from January's revenues, regardless of where or whether either appears in a worksheet. This permits actions later used in pivot tables, except that flexible manipulation of report tables was but one of many capabilities supported by variables. Moreover, if costs were entered by week and revenues by month, Javelin's program could allocate or interpolate as appropriate. This object design enabled variables and whole models to reference each other with user-defined variable names, and to perform multidimensional analysis and massive, but easily editable consolidations.
Trapeze, a spreadsheet on the Mac, went further and explicitly supported
not just table columns, but also matrix operators.
Logical spreadsheets.
Spreadsheets that have a formula language based upon logical expressions, rather than arithmetic expressions are known as logical spreadsheets. Such spreadsheets can be used to reason deductively about their cell values.
Programming issues.
Just as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.
End-user development.
Spreadsheets are a popular End-user development tool. EUD denotes activities or techniques in which people who are not professional developers create automated behavior and complex data objects without significant knowledge of a programming language. Many people find it easier to perform calculations in spreadsheets than by writing the equivalent sequential program. This is due to several traits of spreadsheets.
Spreadsheet programs.
A "spreadsheet program" is designed to perform general computation tasks using spatial relationships rather than time as the primary organizing principle.
It is often convenient to think of a spreadsheet as a mathematical graph, where the nodes are spreadsheet cells, and the edges are references to other cells specified in formulas. This is often called the dependency graph of the spreadsheet. References between cells can take advantage of spatial concepts such as relative position and absolute position, as well as named locations, to make the spreadsheet formulas easier to understand and manage.
Spreadsheets usually attempt to automatically update cells when the cells they depend on change. The earliest spreadsheets used simple tactics like evaluating cells in a particular order, but modern spreadsheets calculate following a minimal recomputation order from the dependency graph. Later spreadsheets also include a limited ability to propagate values in reverse, altering source values so that a particular answer is reached in a certain cell. Since spreadsheet cells formulas are not generally invertible, though, this technique is of somewhat limited value.
Many of the concepts common to sequential programming models have analogues in the spreadsheet world. For example, the sequential model of the indexed loop is usually represented as a table of cells, with similar formulas (normally differing only in which cells they reference).
Spreadsheets have evolved to use scripting programming languages like VBA as a tool for extensibility beyond what the spreadsheet language makes easy.
Shortcomings.
While spreadsheets represented a major step forward in quantitative modeling, they have deficiencies. Their shortcomings include the perceived unfriendliness of alpha-numeric cell addresses.
Other problems associated with spreadsheets include:
While there are built-in and third-party tools for desktop spreadsheet applications that address some of these shortcomings, awareness and use of these is generally low. A good example of this is that 55% of Capital market professionals "don't know" how their spreadsheets are audited; only 6% invest in a third-party solution
Spreadsheet risk.
Spreadsheet risk is the risk associated with deriving a materially incorrect value from a spreadsheet application that will be utilised in making a related (usually numerically based) decision. Examples include the valuation of an asset, the determination of financial accounts, the calculation of medicinal doses or the size of load-bearing beam for structural engineering. The risk may arise from inputting erroneous or fraudulent data values, from mistakes (or incorrect changes) within the logic of the spreadsheet or the omission of relevant updates (e.g. out of date exchange rates). Some single-instance errors have exceeded US$1 billion. Because spreadsheet risk is principally linked to the actions (or inaction) of individuals it is defined as a sub-category of operational risk.
In the report into the 2012 JPMorgan Chase trading loss, a lack of control over spreadsheets used for critical financial functions was cited as a factor in the trading losses of more than six billion dollars which were reported as a result of derivatives trading gone bad.
Despite this, research carried out by ClusterSeven revealed that around half (48%) of c-level executives and senior managers at firms reporting annual revenues over £50m said there were either no usage controls at all or poorly applied manual processes over the use of spreadsheets at the firms.
In 2013 Thomas Herndon, a graduate student of economics at the University of Massachusetts Amherst found major coding flaws in the spreadsheet used by the economists Carmen Reinhart and Kenneth Rogoff in a very influential 2010 journal article. The Reinhart and Rogoff article was widely used as justification to drive 2010-13 European austerity programs.

</doc>
<doc id="27687" url="http://en.wikipedia.org/wiki?curid=27687" title="St. Louis">
St. Louis

St. Louis ( or ) is a city and port in the U.S. state of Missouri. The city developed along the western bank of the Mississippi River, which forms Missouri's border with Illinois. In 2010, St. Louis had a population of 319,294; a 2013 estimate put the population at 318,416, making it the 58th-most populous U.S. city and the second-largest city in the state in terms of city proper population, second only to Kansas City. The St. Louis metropolitan area includes the city as well as nearby areas in Missouri and Illinois; with a population of 2,905,893, it is the largest in Missouri and one of the largest in the United States.
St. Louis was founded in 1764 by Pierre Laclède and Auguste Chouteau and named after Louis IX of France. Claimed first by the French, who settled mostly east of the Mississippi River, the region in which the city stands was ceded to Spain following France's defeat in the Seven Years' War. Its territory east of the Mississippi was ceded to Great Britain, the victor. The area of present-day Missouri was part of Spanish Louisiana from 1762 until 1802.
After the United States acquired this territory in the Louisiana Purchase, St. Louis developed as a major port on the Mississippi River. In the late 19th century, St. Louis was ranked as the fourth-largest city in the United States. It separated from St. Louis County in 1877, becoming an independent city and limiting its political boundaries. In 1904, it hosted the Louisiana Purchase Exposition and the Summer Olympics. The city's population peaked in 1950; with restructuring of heavy industry and loss of jobs, plus postwar suburbanization, it began a long decline that continued into the 21st century. Immigration has increased, and the city is the center of the largest Bosnian population in the world outside their homeland.
The economy of St. Louis relies on service, manufacturing, trade, transportation of goods, and tourism. The city is home to several major corporations including Express Scripts, Peabody Energy, Ameren, Monsanto, Ralcorp, and Sigma-Aldrich, as well as a large medical and research community. St. Louis has three professional sports teams: the St. Louis Cardinals of Major League Baseball, the St. Louis Blues of the National Hockey League, and the St. Louis Rams of the National Football League. The city is commonly identified with the 630 ft tall Gateway Arch in downtown St. Louis.
History.
The area that would become St. Louis was a center of the Native American Mississippian culture, which built numerous temple and residential earthwork mounds on both sides of the Mississippi River. Their major regional center was at Cahokia Mounds, active from 900 AD to 1500 AD. Due to numerous major earthworks within St. Louis boundaries, the city was nicknamed as the "Mound City." These mounds were mostly demolished during the city's development. Historic Native American tribes in the area included the Siouan-speaking Osage people, whose territory extended west, and the Illiniwek.
European exploration of the area was recorded in 1673, when French explorers Louis Jolliet and Jacques Marquette traveled through the Mississippi River valley. Five years later, La Salle claimed the region for France as part of "La Louisiane."
The earliest European settlements in the area were built in Illinois Country (also known as Upper Louisiana) on the east side of the Mississippi River during the 1690s and early 1700s at Cahokia, Kaskaskia, and Fort de Chartres. Migrants from the French villages on the opposite side of the Mississippi River (e.g. Kaskaskia) founded Ste. Genevieve in the 1730s.
In early 1764, after France lost the Seven Years' War, Pierre Laclède and his stepson Auguste Chouteau founded what was to become the city of St. Louis. (French lands east of the Mississippi had been ceded to Great Britain and the lands west of the Mississippi to Spain; France and Spain were 18th-century allies and both were Catholic nations.) The early French families built the city's economy on the fur trade with the Osage, as well as with more distant tribes along the Missouri River. The Chouteau brothers gained a monopoly from Spain on the fur trade with Santa Fe. French colonists used African slaves as domestic servants and workers in the city.
From 1762 to 1803 European control of the area west of the Mississippi to the northernmost part of the Missouri River basin, called Louisiana, was assumed by the Spanish as part of the Viceroyalty of New Spain. In 1780 during the American Revolutionary War, St. Louis was attacked by British forces, mostly Native American allies, in the Battle of St. Louis.
19th century.
St. Louis was transferred to the French First Republic in 1800 (although all of the colonial lands continued to be administered by Spanish officials), then sold by the French to the U.S. in 1803 as part of the Louisiana Purchase. The town became the territorial capital and gateway to the western territory. Shortly after the official transfer of authority was made, the Lewis and Clark Expedition was commissioned by President Thomas Jefferson. The expedition departed from St. Louis in May 1804 along the Missouri River to explore the vast territory. There were hopes of finding a water route to the Pacific Ocean, but the party had to go overland in the Upper West. They reached the Pacific Ocean via the Columbia River in summer 1805. They returned, reaching St. Louis on September 23, 1806. Both Lewis and Clark lived in St. Louis after the expedition. Many other explorers, settlers, and trappers (such as Ashley's Hundred) would later take a similar route to the West.
The city elected its first municipal legislators (called trustees) in 1808. Steamboats first arrived in St. Louis in 1818, improving connections with New Orleans and eastern markets. Missouri was admitted as a state in 1821, in which slavery was legal. As the state gained settlers, the first temporary of capital of the state of Missouri was St. Louis then the capitol of Missouri moved to St. Charles, MO then moved to the more central location of Jefferson City, MO in 1826. St. Louis was incorporated as a city in 1822, and continued to develop largely due to its busy port and trade connections. Slaves worked in many jobs on the waterfront as well as on the riverboats. Given the city's location close to the free state of Illinois and others, some slaves escaped to freedom. Others, especially women with children, sued in court in freedom suits, and several prominent local attorneys aided slaves in these suits. About half the slaves achieved freedom in hundreds of suits before the American Civil War.
Immigrants from Ireland and Germany arrived in St. Louis in significant numbers starting in the 1840s, and the population of St. Louis grew from less than 20,000 in 1840, to 77,860 in 1850, to more than 160,000 by 1860. By the mid-1800s, St. Louis had a greater population than New Orleans. To this day, St. Louis is the largest city of the former French Louisiana territory.
Settled by many Southerners in a slave state, the city was split in political sympathies and became polarized during the American Civil War. In 1861, 28 civilians were killed in a clash with Union troops. The war hurt St. Louis economically, due to the Union blockade of river traffic to the south on the Mississippi River. The St. Louis Arsenal constructed ironclads for the Union Navy.
After the war, St. Louis profited via trade with the West, aided by the 1874 completion of the Eads Bridge, the first bridge so far downriver over the Mississippi. Industrial developments on both banks of the river were linked by the bridge.
The first bridge, in the mid-west over the Mississippi River, opened at St. Louis, and was named Eads Bridge after the architect. The bridge connects St. Louis, MO to East St. Louis, IL. The Eads Bridge, became an iconic image of the city of St. Louis, from the time of its erection until 1965 when the Gateway Arch Bridge was constructed, is still in use. The bridge crosses the St. Louis riverfront between Laclede's Landing, to the north, and the grounds of the Gateway Arch, to the south. Today the road deck has been restored, allowing vehicular and pedestrian traffic to cross the river. The St. Louis MetroLink light rail system has used the rail deck since 1993. An estimated 8,500 vehicles pass through it daily.
On August 22, 1876, the city of St. Louis voted to secede from St. Louis County and become an independent city. Industrial production continued to increase during the late 19th century. Major corporations such as the Anheuser-Busch brewery and Ralston-Purina company were established. St. Louis also was home to Desloge Consolidated Lead Company and several brass era automobile companies, including the Success Automobile Manufacturing Company; St. Louis is the site of the Wainwright Building, an early skyscraper built in 1892 by noted architect Louis Sullivan.
20th century.
In 1904, the city hosted the 1904 World's Fair and the 1904 Summer Olympics, becoming the first non-European city to host the Olympics. Permanent facilities and structures remaining from the fair are Forest Park and associated structures within its boundaries: the St. Louis Art Museum, the St. Louis Zoo and the Missouri History Museum.
In the aftermath of emancipation of slaves following the Civil War, social and racial discrimination in housing and employment were common in St. Louis. Starting in the 1910s, many property deeds included racial or religious restrictive covenants. During World War II, the NAACP campaigned to integrate war factories, and restrictive covenants were prohibited in 1948 by the "Shelley v. Kraemer" U.S. Supreme Court decision, which case originated as a lawsuit in St. Louis. However, "de jure" educational segregation continued into the 1950s, and "de facto" segregation continued into the 1970s, leading to a court challenge and interdistrict desegregation agreement.
St. Louis, like many Midwestern cities, expanded in the early 20th century due to the formation of many industrial companies, providing employment to new generations of immigrants. It reached its peak population of 856,796 at the 1950 census. Suburbanization from the 1950s through the 1990s dramatically reduced the city's population. The effects of suburbanization were exacerbated by the relatively small geographical size of St. Louis due to its earlier decision to become an independent city, and it lost much of its tax base. During the 19th and 20th century, most major cities aggressively annexed surrounding areas as they developed away from the central city; however, St. Louis was unable to do so. In the 21st century, the city of St. Louis contains only 11% of its total metropolitan population, while among the top 20 metro areas in the United States, the central cities contain an average of 24% of total metropolitan area population. Although small increases in population have taken place in St. Louis during the early 2000s, overall the city lost population from 2000 to 2010. Immigration has continued, with the city attracting Vietnamese, Latinos from Mexico and Central America, and Bosnians, the latter forming the largest Bosnian community outside their homeland.
Several urban renewal projects were built in the 1950s, as the city worked to replace old and substandard housing. Some of these were poorly designed and resulted in problems, of which Pruitt-Igoe became a symbol of failure. It was torn down.
Since the 1980s, several revitalization efforts have focused on downtown St. Louis.
21st century.
Urban revitalization continued in the new century. Gentrification has taken place in the Washington Avenue Historic District. In 2006, St. Louis received the World Leadership Award for urban renewal.
In 2014, St. Louis celebrated its 250th birthday with events throughout the year. These were coordinated by the Missouri History Museum through its nonprofit entity, stl250, with help from the Saint Louis Ambassadors volunteer organization and its U.S. Small Business Institute. Commemorations of the Arch's 50th birthday are planned for 2015.
Geography.
Topography.
According to the United States Census Bureau, St. Louis has a total area of 66 sqmi, of which 62 sqmi is land and 4.1 sqmi (6.2%) is water. (Not shown on simple maps of the city, the land at its airport is owned by the city, served by its fire department and others, and is an exclave of St. Louis.) The city is built primarily on bluffs and terraces that rise 100–200 feet above the western banks of the Mississippi River, in the Midwestern United States just south of the Missouri-Mississippi confluence. Much of the area is a fertile and gently rolling prairie that features low hills and broad, shallow valleys. Both the Mississippi River and the Missouri River have cut large valleys with wide flood plains.
Limestone and dolomite of the Mississippian epoch underlie the area, and parts of the city are karst in nature. This is particularly true of the area south of downtown, which has numerous sinkholes and caves. Most of the caves in the city have been sealed, but many springs are visible along the riverfront. Coal, brick clay, and millerite ore were once mined in the city. The predominant surface rock, known as "St. Louis limestone", is used as dimension stone and rubble for construction.
Near the southern boundary of the city of St. Louis (separating it from St. Louis County) is the River des Peres, practically the only river or stream within the city limits that is not entirely underground. Most of River des Peres was confined to a channel or put underground in the 1920s and early 1930s. The lower section of the river was the site of some of the worst flooding of the Great Flood of 1993.
The city's eastern boundary is the Mississippi River, which separates Missouri from Illinois. The Missouri River forms the northern line of St. Louis County, except for a few areas where the river has changed its course. The Meramec River forms most of its southern line.
Cityscape.
Westward view of St. Louis skyline, September 2008.
Neighborhoods.
The city is divided into 79 government-designated neighborhoods. The neighborhood divisions have no legal standing, although some neighborhood associations administer grants or hold veto power over historic-district development.
Climate.
St. Louis lies in the transitional zone between the humid continental climate type and the humid subtropical climate type (Köppen "Dfa" and "Cfa", respectively), with neither large mountains nor large bodies of water to moderate its temperature. The city experiences hot, humid summers and cold winters. It is subject to both cold Arctic air and hot, humid tropical air from the Gulf of Mexico. The average annual temperature recorded at nearby Lambert–St. Louis International Airport, is 57.1 °F. Both 100 and temperatures can be seen on an average 2 or 3 days per year. Average annual precipitation is about 41.0 in, but annual precipitation has ranged from 20.59 in in 1953 to 57.96 in in 2008. The city has four distinct seasons:
Flora and fauna.
Before the founding of the city, the area was mostly prairie and open forest. Native Americans maintained this environment, good for hunting, by burning underbrush. Trees are mainly oak, maple, and hickory, similar to the forests of the nearby Ozarks; common understory trees include eastern redbud, serviceberry, and flowering dogwood. Riparian areas are forested with mainly American sycamore.
Most of the residential areas of the city are planted with large native shade trees. The largest native forest area is found in Forest Park. In autumn, the changing color of the trees is notable. Most species here are typical of the eastern woodland, although numerous decorative non-native species are found. The most notable invasive species is Japanese honeysuckle, which officials are trying to manage because of its damage to native trees. It is removed from some parks.
Large mammals found in the city include urbanized coyotes and white-tailed deer. Eastern gray squirrel, cottontail rabbit, and other rodents are abundant, as well as the nocturnal Virginia opossum. Large bird species are abundant in parks and include Canada goose, mallard duck, as well as shorebirds, including the great egret and great blue heron. Gulls are common along the Mississippi River; these species typically follow barge traffic.
Winter populations of bald eagles are found along the Mississippi River around the Chain of Rocks Bridge. The city is on the Mississippi Flyway, used by migrating birds, and has a large variety of small bird species, common to the eastern US. The Eurasian tree sparrow, an introduced species, is limited in North America to the counties surrounding St. Louis. The city has special sites for birdwatching of migratory species, including Tower Grove Park.
Frogs are commonly found in the springtime, especially after extensive wet periods. Common species include the American toad and species of chorus frogs commonly called spring peepers, which are found in nearly every pond. Some years have outbreaks of cicadas or ladybugs. Mosquitos, no-see-ums, and houseflies are common insect nuisances, especially in July and August; because of this, windows are nearly universally fitted with screens, and screened-in porches are common in the older homes constructed before air-conditioning was common. Invasive populations of honeybees have sharply declined in recent years. Numerous native species of pollinator insects have recovered to fill their ecological niche. Due to a warming climate, armadillos have been regularly seen throughout the St. Louis area, especially since 2005.
Demographics.
St. Louis grew slowly until the American Civil War, when industrialization and immigration sparked a boom. After years of immigration and expansion, it reached its peak population in 1950. That year, the Census Bureau reported St. Louis' population as 82% White and 17.9% African American. After World War II, St. Louis began losing population to the suburbs, first because of increased demand for new housing, the ease of commuting by subsidized highways, and later, white flight. St. Louis has lost 62.7% of its population since the 1950 United States Census, the highest percent of any city that had a population of 100,000 or more at the time of the 1950 Census. Detroit and Youngstown, Ohio are the only other cities to have seen population declines of at least 60% in the same time frame. The population of the city of St. Louis has been in decline since the 1960 census although the population of the St. Louis Metropolitan Area has grown every year and continues to do so. A big factor responsible for this decline is the rapid increase in suburbanization.
According to the 2010 United States Census, St. Louis had 319,294 people living in 142,057 households, of which 67,488 households were families. The population density was 5,158.2 people per square mile (1,990.6/km²). About 24% of the population was 19 or younger, 9% were 20 to 24, 31% were 25 to 44, 25% were 45 to 64, and 11% were 65 or older. The median age was about 34 years.
The population was about 49.2% African American, 43.9% White (42.2% Non-Hispanic White), 2.9% Asian, 0.3% Native American/Alaska Native, and 2.4% reporting two or more races. Hispanic or Latino of any race were 3.5% of the population.
The African-American population is mostly centered in the north side of the city (the area north of Delmar Boulevard is 94.0% black, compared with 35.0% in the central corridor and 26.0% in the south side of St. Louis ). Among the Asian-American population in the city, the largest ethnic group is Vietnamese (0.9%), followed by Chinese (0.6%) and Asian Indians (0.5%). The Vietnamese community has concentrated in the Dutchtown neighborhood; Chinese are concentrated in the Central West End. People of Mexican descent are the largest Latino group, and make up 2.2% of St. Louis' population. They have the highest concentration in the Dutchtown, Benton Park West (Cherokee Street), and Gravois Park neighborhoods.
In 2000, the median income for a household in the city was $29,156, and the median income for a family was $32,585. Males had a median income of $31,106; females, $26,987. Per capita income was $18,108.
Some 19% of the city's housing units were vacant, and slightly less than half of these were vacant structures not for sale or rent.
In 2010, St. Louis' per-capita rate of online charitable donations and volunteerism were among the highest among major U.S. cities.
As of 2010, 91.05% (270,934) of St. Louis city residents age 5 and older spoke English at home as a primary language, while 2.86% (8,516) spoke Spanish, 0.91% (2,713) Bosnian, 0.74% (2,200) Vietnamese, 0.50% (1,495) African languages, 0.50% (1,481) Chinese, and French was spoken as a main language by 0.45% (1,341) of the population over the age of five. In total, 8.95% (26,628) of St. Louis' population age 5 and older spoke a mother language other than English.
Bosnian population.
About 15 families from Bosnia settled in St. Louis in 1960 and 1970. After the Bosnian War started in 1992, more Bosnian refugees began arriving and by 2000, tens of thousands of Bosnian refugees settled in St. Louis, with the help of Catholic aid societies. Many of them were professionals and skilled workers who had to take any job opportunity to be able to support their families. Most Bosnians refugees are Muslim (87%); they have settled primarily in south St. Louis and South County. Bosnian-Americans are well integrated into the city, developing many businesses and ethnic/cultural organizations.
An estimated 70,000 Bosnians live in the metro area, the largest population of Bosnians in the United States and the largest Bosnian population outside their homeland. The highest concentration of Bosnians is in the neighborhood of Bevo Mill, and in Affton, Mehlville and Oakville of south St. Louis County
Economy.
The 2014 Gross Metropolitan Product (GMP) of St. Louis was $145.958 billion up from $144.03 in 2013, $138.403 in 2012 and $133.1 in 2011 making it the 21st-highest in the country. The St. Louis Metropolitan Area had a Per capita GDP of $48,738 in 2014 up 1.6% from 2013.This signals the growth of the St. Louis economy. According to the 2007 Economic Census, manufacturing in the city conducted nearly $11 billion in business, followed by the health care and social service industry with $3.5 billion, professional or technical services with $3.1 billion, and the retail trade with $2.5 billion. The health care sector was the biggest employer in the area with 34,000 workers, followed by administrative and support jobs, 24,000; manufacturing, 21,000, and food service, 20,000.
Major companies and institutions.
As of 2013, St. Louis Metropolitan Area is home to 9 Fortune 500 companies, third-highest in the Mid-West. St. Louis is home to two Fortune 500 companies: Peabody Energy and Ameren. In addition, seven other Fortune 500 companies are headquartered in the MSA: Express Scripts, Emerson Electric, Monsanto, Reinsurance Group of America, Centene, Graybar Electric, and Edward Jones Investments.
Other notable corporations headquartered in the region include Arch Coal, Scottrade, Wells Fargo Advisors (formerly A.G. Edwards), Energizer Holdings, Heritage Home Group, Patriot Coal, Post Foods, United Van Lines and Mayflower Transit, Ralcorp, Hardee's, and Enterprise Holdings (parent company of several car rental companies). Notable corporations with operations in St. Louis include Cassidy Turley, Kerry Group, MasterCard, TD Ameritrade, and BMO Harris Bank. 
Health care and biotechnology institutions with operations in St. Louis include Pfizer, the Donald Danforth Plant Science Center, the Solae Company, Sigma-Aldrich, and Multidata Systems International. General Motors manufactures automobiles in Wentzville, while an earlier plant called the St. Louis Truck Assembly built GMC from 1920 until 1987. Chrysler closed its Saint Louis Assembly production facility in nearby Fenton, Missouri, and Ford closed the St. Louis Assembly Plant in Hazelwood.
Several once-independent pillars of the local economy have been purchased by other corporations. Among them are Anheuser-Busch, purchased by Belgium-based InBev; Missouri Pacific Railroad, which was headquartered in St. Louis, merged with Omaha, Nebraska-based Union Pacific Railroad in 1982; McDonnell Douglas, whose operations are now part of Boeing Defense, Space & Security; Mallinckrodt, purchased by Tyco International; and Ralston Purina, now a wholly owned subsidiary of Nestle. The May Department Stores Company (which owned Famous-Barr and Marshall Field's stores) was purchased by Federated Department Stores, which has its regional headquarters in the area. The Federal Reserve Bank of St. Louis in downtown is one of two federal reserve banks in Missouri.
St. Louis is a center of medicine and biotechnology. The Washington University School of Medicine is affiliated with Barnes-Jewish Hospital, the fifth-largest hospital in the world. The two institutions operate the Alvin J. Siteman Cancer Center. The School of Medicine also is affiliated with St. Louis Children's Hospital, one of the country's top pediatric hospitals. Both hospitals are owned by BJC HealthCare. The school's Genome Sequencing Center played a major role in the Human Genome Project. St. Louis University Medical School is affiliated with Tenet Healthcare's St. Louis University Hospitals and SSM Health Care's Cardinal Glennon Children's Hospital. It also has a cancer center, vaccine research center, geriatric center, and a bioethics institute. Several different organizations operate hospitals in the area, including BJC HealthCare, SSM Health Care, Tenet and Mercy Healthcare.
Boeing employs nearly 15,000 people in its north St. Louis campus, headquarters to its defense unit. In 2013, the company said it would move about 600 jobs from Seattle, where labor costs have risen, to a new IT center in St. Louis. Other companies, such as LaunchCode and LockerDome, see the city's potential to become the next major tech hub. Programs such as Arch Grants are attracting new startups to the region.
According to the "St. Louis Business Journal", the top employers in the St. Louis metropolitan area as of June 1, 2013, are as follows:
According to St. Louis' 2013 Comprehensive Annual Financial Report, the top employers in the City only for 2012 are:
Culture.
With its French past and waves of Catholic immigrants in the 19th and 20th centuries, from Ireland, Germany and Italy, St. Louis is a major center of Roman Catholicism in the United States. St. Louis also boasts the largest Ethical Culture Society in the United States, and consistently ranks as one of the most generous cities in the United States, ranking ninth in 2013. Several places of worship in the city are noteworthy, such as the Cathedral Basilica of St. Louis, home of the world's largest mosaic installation.
Other locally notable churches include the Basilica of St. Louis, King of France, the oldest Roman Catholic cathedral west of the Mississippi River and the oldest church in St. Louis; the St. Louis Abbey, whose distinctive architectural style garnered multiple awards at the time of its completion in 1962; and St. Francis de Sales Oratory, a neo-Gothic church completed in 1908 in South St. Louis and the second-largest church in the city.
The city is defined by music and the performing arts, especially its association with blues, jazz, and ragtime. St. Louis is home to the St. Louis Symphony, the second-oldest symphony orchestra in the United States, which has toured nationally and internationally to strong reviews. Until 2010, it was also home to KFUO-FM, one of the oldest classical music FM radio stations west of the Mississippi River.
The Gateway Arch marks downtown St. Louis and a historic center that includes the Federal courthouse where the Dred Scott case was first argued, a newly renovated and expanded public library, major churches and businesses, and retail. An increasing downtown residential population has taken to adapted office buildings and other historic structures. In nearby University City is the Delmar Loop, ranked by the American Planning Association as a "great American street" for its variety of shops and restaurants, and the Tivoli Theater, all within walking distance.
Unique city and regional cuisine reflecting various immigrant groups include toasted ravioli, gooey butter cake, provel cheese, the slinger, the Gerber sandwich, the St. Paul sandwich, and St. Louis-style pizza, featuring thin crust and provel cheese. Some St. Louis chefs have begun emphasizing use of local produce, meats and fish, and neighborhood farmers' markets have become increasingly popular, as well as one downtown. Artisan bakeries, salumeria, and chocolatiers also operate in the city.
Also unique to St. Louis is the Ted Drewes "Concrete," which is frozen custard blended with any combination of dozens of ingredients, served in a large yellow cup with a spoon and straw. The mixture is so thick that a spoon inserted into the custard does not fall if the cup is inverted. Ted Drewes owns and operates a pair of frozen custard shops in St. Louis, which have been highlighted in the national media on several occasions. In 2006, the Route 66 (Chippewa) location was featured on the Food Network show "Feasting on Asphalt", hosted by Alton Brown. In 2010, it was recommended by Bobby Flay on the "Sweet Tooth" episode of "The Best Thing I Ever Ate". In 2011, it was featured in a special "Route 66" episode of "Man v. Food Nation", hosted by Adam Richman.
Architecture.
The architecture of St. Louis exhibits a variety of commercial, residential, and monumental architecture. St. Louis is known for the Gateway Arch, the tallest monument constructed in the United States at 630 ft. The Arch pays homage to Thomas Jefferson and St. Louis' position as the gateway to the West. Architectural influences reflected in the area include French Colonial, German, early American, and modern architectural styles.
Skyscrapers.
Some notable post-modern commercial skyscrapers were built downtown in the 1970s and 1980s, including the One US Bank Plaza (1976), the AT&T Center (1986), and One Metropolitan Square (1989), which is the tallest building in St. Louis. One US Bank Plaza, the local headquarters for US Bancorp, was constructed for the Mercantile Bancorporation in the Structural expressionist style, emphasizing the steel structure of the building.
During the 1990s, St. Louis saw the construction of the largest United States courthouse by area, the Thomas F. Eagleton United States Courthouse (completed in 2000). The Eagleton Courthouse is home to the United States District Court for the Eastern District of Missouri and the United States Court of Appeals for the Eighth Circuit. The most recent high-rise buildings in St. Louis include two residential towers: the Park East Tower in the Central West End and the Roberts Tower located in downtown.
Landmarks and monuments.
Several examples of religious structures are extant from the pre-Civil War period, and most reflect the common residential styles of the time. Among the earliest is the Basilica of St. Louis, King of France (locally referred to as the "Old Cathedral"). The Basilica was built between 1831 and 1834 in the Federal style. Other religious buildings from the period include SS. Cyril and Methodius Church (1857) in the Romanesque Revival style and Christ Church Cathedral (completed in 1867, designed in 1859) in the Gothic Revival style.
Only a few civic buildings were constructed during the early 19th century. The original St. Louis courthouse was built in 1826 and featured a Federal style stone facade with a rounded portico. However, this courthouse was replaced during renovation and expansion of the building in the 1850s. The Old St. Louis County Courthouse (locally known as the "Old Courthouse") was completed in 1864 and was notable for having an early cast iron dome and for being the tallest structure in Missouri until 1894. Finally, a customs house was constructed in the Greek Revival style in 1852, but was demolished and replaced in 1873 by the U.S. Customhouse and Post Office.
Because much of the city's early commercial and industrial development was centered along the riverfront, many pre-Civil War buildings were demolished during construction of the Gateway Arch. The city's remaining architectural heritage of the era includes a multi-block district of cobblestone streets and brick and cast-iron warehouses called Laclede's Landing. Now popular for its restaurants and nightclubs, the district is located north of Gateway Arch along the riverfront. Other industrial buildings from the era include some portions of the Anheuser-Busch Brewery, which date to the early 1860s.
St. Louis saw a vast expansion in variety and number of religious buildings during the late 19th century and early 20th century. The largest and most ornate of these is the Cathedral Basilica of St. Louis, designed by Thomas P. Barnett and constructed between 1907 and 1914 in the Neo-Byzantine style. The St. Louis Cathedral, as it is known, has one of the largest mosaic collections in the world. Another landmark in religious architecture of St. Louis is the St. Stanislaus Kostka, which is an example of the Polish Cathedral style. Among the other major designs of the period were St. Alphonsus Liguori (locally known as "The Rock Church") (1867) in the Gothic Revival and Second Presbyterian Church of St. Louis (1900) in Richardsonian Romanesque. 
Early in the 20th century (and during the years before and after the 1904 World's Fair), several churches moved to the Central West End neighborhood, near Forest Park and the fairgrounds. The neighborhood features the Holy Corners Historic District, which is a concentration of several historic religious structures, such as the First Church of Christ, Scientist (1904).
By the 1900 census, St. Louis was the fourth largest city in the country. In 1904, the city hosted a world's fair at Forest Park called the Louisiana Purchase Exposition. Its architectural legacy is somewhat scattered. Among the fair-related cultural institutions in the park are the Saint Louis Art Museum designed by Cass Gilbert, part of the remaining lagoon at the foot of Art Hill, and the Flight Cage at the St. Louis Zoo. The Missouri History Museum was built afterward, with the profit from the fair. But 1904 left other assets to the city, like Theodore Link's 1894 St. Louis Union Station, and an improved Forest Park.
Louis Sullivan designed Charlotte Dickson Wainwright's tomb on the north side of Bellefontaine Cemetery, surrounded by a collection of similar tombs for the great old St. Louis families, interesting for their late-Gilded Age artwork.
Shortly after the Civil War, St. Louis rapidly increased its school system and hospital system. One of the earliest structures and the oldest extant hospital building in St. Louis is the St. Louis Insane Asylum (now the Metropolitan St. Louis Psychiatric Center). The asylum is built of brick in the Italianate style, complete with cast iron dome and cupola reminiscent of the Old Courthouse.
As St. Louis expanded, the city hall was moved further west of downtown to its present location in 1904 (construction began in 1892). St. Louis City Hall, still in use, was designed by Harvey Ellis in the Renaissance Revival style, reminiscent of the Hôtel de Ville (City Hall) in Paris, France.
Other significant civic buildings from the late 19th century and early 20th century include the U.S. Customhouse and Post Office by Alfred B. Mullett (1873) and the stately St. Louis Public Library by Cass Gilbert (1912). While the Old Post Office has been renovated, the St. Louis Public Library is slated for renovation as of 2010. In 1923 the city passed a $87 million bond issue for re-development of the Civic Plaza along the lines of the City Beautiful movement. This development resulted in some of St. Louis's major civic architecture: the Soldiers' Memorial, the Civil Courts Building, and Kiel Auditorium.
Then into the 1940s and 1950s a certain subgenre of St. Louis modernism emerged, with the locally important architect Harris Armstrong, and a series of daring modern civic landmarks like Gyo Obata's Planetarium, the geodesic-dome Climatron, and the main terminal building at Lambert-St. Louis International Airport. The Poplar Street Bridge, a 647 ft long (197m) deck girder bridge, was built in 1967 and continues to carry three Interstates and one U.S. route. St. Louis also was the headquarters for postwar modernist bank designer Wenceslaus Sarmiento, whose major work in St. Louis is the Chancery Building (1965) on the grounds of the Cathedral Basilica of St. Louis. The culmination of St. Louis modern architecture is Eero Saarinen's magnificent stainless-steel gesture, the Gateway Arch, centerpiece of the 91 acre riverside Jefferson National Expansion Memorial.
A panoramic view of the St. Louis skyline, dominated by the 630 ft Gateway Arch.
Sports.
St. Louis is home to professional Major League Baseball, National Football League, and National Hockey League teams, notable collegiate-level soccer teams, and has hosted several collegiate sports tournaments.
Professional sports.
St. Louis is home to three major league sports teams. The St. Louis Cardinals are one of the most successful franchises in Major League Baseball, and have won 19 National League titles, which are the most pennants for the league franchise in one city, and 11 World Series titles, with the most recent in 2011. They play at Busch Stadium. The St. Louis Rams, an American football team in the National Football League play at the Edward Jones Dome and have won three NFL championships, including one Super Bowl in 2000. The St. Louis Blues of the National Hockey League play at the Scottrade Center.
St. Louis also hosts several minor league sports teams. The Gateway Grizzlies and the River City Rascals of the Frontier League (which are not affiliated with Major League Baseball) play in the area. The St. Louis Trotters of the Independent Basketball Association play at Matthews Dickey. Saint Louis FC of the USL play at St. Louis Soccer Park. The region hosts NHRA drag racing and NASCAR events at the Gateway International Raceway in Madison, Illinois.
Amateur sports.
At the collegiate level, St. Louis has hosted the Final Four of both the women's and men's college basketball NCAA Division I championship tournaments, and the Frozen Four collegiate ice hockey tournament. Although the area does not currently support a National Basketball Association team, it hosts an American Basketball Association team called the St. Louis Phoenix. St. Louis University has won 10 NCAA Men's Soccer Championships, and the city has hosted the College Cup several times. In addition to collegiate soccer, many St. Louisans have played for the United States men's national soccer team, and 20 St. Louisans have been elected into the National Soccer Hall of Fame. St. Louis also is the origin of the sport of corkball, a type of baseball in which there is no base running. The St. Louis TV market is the largest in the nation without a Division I college football team.
Parks.
The city operates more than 100 parks, with amenities that include sports facilities, playgrounds, concert areas, picnic areas, and lakes. Forest Park, located on the western edge of city, is the largest, occupying 1,400 acres of land, making it almost twice as large as Central Park in New York City. The park is home to five major institutions, including the St. Louis Art Museum, the St. Louis Zoo, the St. Louis Science Center, the Missouri History Museum, and the Muny amphitheatre. Another significant park in the city is the Jefferson National Expansion Memorial, a National Memorial located on the riverfront in downtown St. Louis. The centerpiece of the park is the 630 ft tall Gateway Arch, designed by noted architect Eero Saarinen and completed on October 28, 1965. Also part of the historic park is the Old Courthouse, where the first two trials of "Dred Scott v. Sandford" were held in 1847 and 1850.
Other notable parks in the city include the Missouri Botanical Garden, Tower Grove Park, Carondelet Park and Citygarden. The Missouri Botanical Garden, a private garden and botanical research facility, is a National Historic Landmark and one of the oldest botanical gardens in the United States. The Garden features 79 acres of horticultural displays from around the world. This includes a Japanese strolling garden, Henry Shaw's original 1850 estate home and a geodesic dome called the Climatron. Immediately south of the Missouri Botanical Garden is Tower Grove Park, a gift to the City by Henry Shaw. Citygarden is an urban sculpture park located in downtown St. Louis, with art from Fernand Léger, Aristide Maillol, Julian Opie, Tom Otterness, Niki de Saint Phalle, and Mark di Suvero. The park is divided into three sections, each of which represent a different theme: river bluffs; flood plains; and urban gardens. The park also has a restaurant – Death in the Afternoon. Another downtown sculpture park is the Serra Sculpture Park, with the 1982 Richard Serra sculpture "Twain".
Government.
The city of St. Louis has a mayor-council government with legislative authority vested in the Board of Aldermen of the City of St. Louis and with executive authority in the Mayor of St. Louis and six other separately elected officials. The Board of Aldermen is made up of 28 members (one elected from each of the city's wards) plus a board president who is elected city-wide. 
The 2014 fiscal year budget topped $1 billion for the first time, a 1.9% increase over the $985.2 million budget in 2013. 238,253 registered voters lived in the city in 2012, down from 239,247 in 2010, and 257,442 in 2008.
Local and regional government.
Municipal elections in St. Louis are held in odd numbered years, with the primary elections in March and the general election in April. The mayor is elected in odd numbered years following the United States Presidential Election, as are the aldermen representing odd-numbered wards. The President of the Board of Aldermen and the aldermen from even-numbered wards are elected in the off-years. The Democratic Party has dominated St. Louis city politics for decades. The city has not had a Republican mayor since 1949 and the last time a Republican was elected to another city-wide office was in the 1970s. As of 2006, 27 of the city's 28 Aldermen are Democrats. Forty-five individuals have held the office of mayor of St. Louis, four of whom—William Carr Lane, John Fletcher Darby, John Wimer, and John How—served non-consecutive terms. The most terms served by a mayor was by Lane who served 8 full terms plus the unexpired term of Darby. The current mayor is Francis G. Slay, who took office April 17, 2001, and who won a fourth four-year term on March 5, 2013. As of April 27, 2013, he is the longest-serving mayor of St. Louis. The second-longest serving mayor was Henry Kiel, who took office April 15, 1913 and left office April 21, 1925, a total of 12 years and 9 days over three terms in office. Two others—Raymond Tucker, and Vincent C. Schoemehl—also served three terms as mayor, but served seven fewer days. The shortest serving mayor was Arthur Barret who died 11 days after taking office.
Although St. Louis separated from St. Louis County in 1876, some mechanisms have been put in place for joint funding management and funding of regional assets. The St. Louis Zoo-Museum district collects property taxes from residents of both St. Louis City and County and the funds are used to support cultural institutions including the St. Louis Zoo, St. Louis Art Museum and the Missouri Botanical Gardens. Similarly, the Metropolitan Sewer District provides sanitary and storm sewer service to the city and much of St. Louis County. The Bi-State Development Agency (now known as Metro) runs the region's MetroLink light rail system and bus system.
State and federal government.
St. Louis is split between 11 districts in the Missouri House of Representatives: all of the 76th, 77th, 78th, 79th, 80th, 81st, 82nd, and 84th, and parts of the 66th, 83rd, and 93rd, which are shared with St. Louis County. The 5th Missouri Senate district is entirely within the city, while the 4th is shared with St. Louis County.
At the federal level, St. Louis is the heart of 2=nd
 |3=rd
 |th
 }} congressional district, which also includes part of northern St. Louis County. A Republican has not represented a significant portion of St. Louis in the U.S. House since 1953.
The United States Court of Appeals for the Eighth Circuit and the United States District Court for the Eastern District of Missouri are based in the Thomas F. Eagleton United States Courthouse in downtown St. Louis. St. Louis is also home to a Federal Reserve System branch, the Federal Reserve Bank of St. Louis. The National Geospatial-Intelligence Agency (NGA) also maintains major facilities in the St. Louis area.
The Military Personnel Records Center (NPRC-MPR) located at 9700 Page Avenue in St. Louis, Missouri, USA, is a branch of the National Personnel Records Center and is the repository of over 56 million military personnel records and medical records pertaining to retired, discharged, and deceased veterans of the U.S. armed forces.
Crime.
St. Louis index crime rates have declined every year since 1995; since 2005, violent crime has declined by 20%, although rates of violent crime and property crime in the city remain about 2% higher than the state and 2.3% United States national average. In 2012, St. Louis was ranked the 8th-most dangerous city in America for violent crime, behind Chicago, Memphis, Baltimore, Flint, Oakland, Detroit, and Camden.
Education.
The St. Louis Public Schools (SLPS) operate more than 75 schools, attended by more than 25,000 students, including several magnet schools. SLPS operates under provisional accreditation from the state of Missouri and is under the governance of a state-appointed school board called the Special Administrative Board, although a local board continues to exist without legal authority over the district. Since 2000, charter schools have operated in the city of St. Louis using authorization from Missouri state law. These schools are sponsored by local institutions or corporations and take in students from kindergarten through high school. In addition, several private schools exist in the city, and the Archdiocese of St. Louis operates dozens of parochial schools in the city, including parochial high schools. The city also has several private high schools, including secular, Catholic and Lutheran schools.
The city is home to two national research universities, Washington University in St. Louis and St. Louis University, as classified under the Carnegie Classification of Institutions of Higher Education. Washington University School of Medicine in St. Louis has been ranked among the top 10 medical schools in the country by US News & World Report for as long as the list has been published, and as high as second, in 2003 and 2004.
In addition to Catholic theological institutions such as Kenrick-Glennon Seminary, St. Louis is home to three Protestant seminaries: Eden Theological Seminary of the United Church of Christ, Covenant Theological Seminary of the Presbyterian Church in America, and Concordia Seminary of the St. Louis-based Lutheran Church Missouri Synod.
Media.
Greater St. Louis commands the 21st-largest media market in the United States, a position roughly unchanged for over a decade. All of the major U.S. television networks have affiliates in St. Louis, including KTVI 2 (Fox), KMOV 4 (CBS), KSDK 5 (NBC), KETC 9 (PBS), KPLR-TV 11 (CW), KDNL 30 (ABC), WRBU 46 (Ion), and WPXS 51 Daystar Television Network. Among the area's most popular radio stations are KMOX (AM sports and talk, notable as the longtime flagship station for St. Louis Cardinals broadcasts), KLOU (FM oldies), WIL-FM (FM country), WARH (FM adult hits), and KSLZ (FM Top 40 mainstream). St. Louis also supports public radio's KWMU, an NPR affiliate, and community radio's KDHX. All-sports stations, such as KFNS 590 AM "The Fan", WXOS "101.1 ESPN", and KSLG are also popular.
The "St. Louis Post-Dispatch" is the region's major daily newspaper. Others in the region include the "Suburban Journals", which serve parts of St. Louis County, while the primary alternative newspaper is the "Riverfront Times". Three weeklies serve the African-American community: the "St. Louis Argus", the "St. Louis American", and the "St. Louis Sentinel". "St. Louis Magazine", a local monthly magazine, covers topics such as local history, cuisine, and lifestyles, while the weekly "St. Louis Business Journal" provides coverage of regional business news. St. Louis was served by an online newspaper, the "St. Louis Beacon", but that publication merged with KWMU in 2013. 
Transportation.
Road, rail, ship, and air transportation modes connect the city with surrounding communities in Greater St. Louis, national transportation networks, and international locations. St. Louis also supports a public transportation network that includes bus and light rail service.
Roads and highways.
Four interstate highways connect the city to a larger regional highway system. Interstate 70, an east-west highway, runs roughly from the northwest corner of the city to downtown St. Louis. The north-south Interstate 55 enters the city at the south near the Carondelet neighborhood and runs toward the center of the city, and both Interstate 64 and Interstate 44 enter the city on the west, running parallel to the east. Two of the four interstates (Interstates 55 and 64) merge south of the Jefferson National Expansion Memorial and leave the city on the Poplar Street Bridge into Illinois, while Interstate 44 terminates at Interstate 70 at its new interchange near N Broadway and Cass Ave.
The 563-mile Avenue of the Saints links St. Louis with St. Paul, Minnesota.
Major roadways include the north-south Memorial Drive, located on the western edge of the Jefferson National Expansion Memorial and parallel to Interstate 70, the north-south streets of Grand Boulevard and Jefferson Avenue, both of which run the length of the city, and Gravois Road, which runs from the southeastern portion of the city to downtown and formerly was signed as U.S. Route 66. An east-west roadway that connects the city with surrounding communities is Martin Luther King, Jr. Drive, which carries traffic from the western edge of the city to downtown.
Light rail and subway.
Light rail service consists of two lines operating on double track. They both serve all the stations in the city, and branch to different destinations beyond its limits. Both lines enter the city north of Forest Park on the western edge of the city or on the Eads Bridge in downtown St. Louis to Illinois. All of the system track is in independent right of way, with both surface level and underground subways track in the city. All stations are independent entry, while all platforms are flush-level with trains. Rail service is provided by the Bi-State Development Agency (also known as Metro), which is funded by a sales taxes levied in the city and other counties in the region. The Gateway Multimodal Transportation Center acts as the hub station in the city of St. Louis, linking the city's light rail system, local bus system, passenger rail service, and national bus service.
Airports.
Lambert St. Louis International Airport, owned and operated by the City of St. Louis, is 11 miles northwest of downtown along highway I-70 between I-170 and I-270 in St. Louis County. It is the largest and busiest airport in the state. In 2011, the airport saw 255 daily departures to about 90 domestic and international locations and a total of nearly 13 million passengers. The airport serves as a focus city for Southwest Airlines and was a former hub for Trans World Airlines and former focus-city for American Airlines and AmericanConnection. Air cargo transportation is available at Lambert International and at other nearby regional airports, including MidAmerica St. Louis Airport, Spirit of St. Louis Airport, and St. Louis Downtown Airport.
The airport has two terminals with a total of five concourses. International flights and passengers use Terminal 2, whose lower level holds the Immigration and Customs gates. Passengers can move between the terminals on complimentary buses that run continuously, or via MetroLink for a fee. It was possible to walk between the terminals until Concourse D was closed in 2008.
Port authority.
River transportation is available through the Port of St. Louis, which is 19.3 miles of riverbank on the Mississippi River that handles more than 32 million tons of freight annually. The Port is the 2nd largest inland port by trip-ton miles, and the 3rd largest by tonnage in the United States, with more than one hundred docking facilities for barge shipping and 16 public terminals on the river. The Port Authority added 2 new small fire and rescue craft in 2012 and 2013.
Railroad service.
Inter-city rail passenger train service in the city is provided by Amtrak. All Amtrak trains serving St. Louis use the Gateway Multimodal Transportation Center downtown. Amtrak trains terminating in the city include the "Lincoln Service" to Chicago, Illinois and the "Missouri River Runner" to Kansas City, Missouri. St. Louis is an intermediate stop on the "Texas Eagle" route which provides long-distance passenger service between San Antonio, Texas, and Chicago, Illinois.
St. Louis is the nation's third-largest freight rail hub, moving Missouri exports such as fertilizer, gravel, crushed stone, prepared foodstuffs, fats, oils, nonmetallic mineral products, grain, alcohol, tobacco products, automobiles, and automobile parts. Freight rail service in St. Louis is provided on tracks owned by Union Pacific Railroad, Norfolk Southern Railway, Foster Townsend Rail Logistics - formerly Manufacturers Railway (St. Louis), Terminal Railroad Association of St. Louis, Affton Trucking, and the BNSF Railway.
The Terminal Railroad Association of St. Louis (reporting mark: TRRA) is a switching and terminal railroad jointly owned by all the major rail carriers in St. Louis. The company operates 30 diesel-electric locomotives to move railcars around the classification yards, deliver railcars to local industries, and ready trains for departure. The TRRA processes and dispatches a significant portion of railroad traffic moving through the city and owns and operates a network of rail bridges and tunnels including the MacArthur Bridge (St. Louis) and the Merchants Bridge. This infrastructure is also used by inter-city rail and long-distance passenger trains serving St. Louis.
Primary train classification yards in St. Louis, their approximate location in the city, and the "main rail lines or major customers served":
Union Pacific Railroad
Norfolk Southern Railway
Foster Townsend Rail Logistics - formerly Manufacturers Railway (St. Louis)
Affton Trucking
BNSF Railway
Bus service.
Local bus service in the city of St. Louis is provided by the Bi-State Development Agency via MetroBus, with more than 75 routes connecting to MetroLink commuter rail transit and stops in the city and region. The city is also served by Madison County Transit, which connects downtown St. Louis to Madison County, Illinois. National bus service in the city is offered by Greyhound Lines and Amtrak Thruway Motorcoach, with a station at the Gateway Multimodal Transportation Center, and Megabus, with a stop at St. Louis Union Station.
Taxi.
Taxicab service in the city is provided by private companies regulated by the Metropolitan Taxicab Commission. Rates vary by vehicle type, size, passengers and distance, and by regulation all taxicab fares must be calculated using a taximeter and be payable in cash or credit card. Solicitation by a driver is prohibited, although a taxicab may be hailed on the street or at a stand.
Sister cities.
St. Louis has 16 sister cities.
 Brčko, Bosnia and Herzegovina
 Lyon, France
 São Luís, Maranhão, Brazil
With informal relations with Tuguegarao, Philippines.

</doc>
<doc id="27691" url="http://en.wikipedia.org/wiki?curid=27691" title="Social security">
Social security

Social security is a concept enshrined in Article 22 of the Universal Declaration of Human Rights, which states: 
Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality. 
In simple terms, the signatories agree that society in which a person lives should help them to develop and to make the most of all the advantages (culture, work, social welfare) which are offered to them in the country.
Social security may also refer to the action programs of government intended to promote the welfare of the population through assistance measures guaranteeing access to sufficient resources for food and shelter and to promote health and well-being for the population at large and potentially vulnerable segments such as children, the elderly, the sick and the unemployed. Services providing social security are often called social services.
Terminology in this area in the United States is somewhat different from in the rest of the English-speaking world. The general term for an action program in support of the well being of the population in the United States is "welfare program" and the general term for all such programs is simply "welfare". In American society, the term "welfare" arguably has negative connotations. The term "Social Security", in the United States, refers to a specific social insurance program for the retired and the disabled. Elsewhere the term is used in a much broader sense, referring to the economic security society offers when people are faced with certain risks. In its 1952 Social Security (Minimum Standards) Convention (nr. 102), the International Labour Organization (ILO) defined the traditional contingencies covered by social security as including:
People who cannot reach a guaranteed social minimum for other reasons may be eligible for "social assistance" (or welfare, in American English).
Modern authors often consider the ILO approach too narrow. In their view, social security is not limited to the provision of cash transfers, but also aims at security of work, health, and social participation; and new social risks (single parenthood, the reconciliation of work and family life) should be included in the list as well.
Social security may refer to:
A report published by the ILO in 2014 estimated that only 27% of the world's population has access to comprehensive social security.
History.
While several of the provisions to which the concept refers have a long history (especially in poor relief), the notion of "social security" itself is a fairly recent one. The earliest examples of use date from the 19th century. In a speech to mark the independence of Venezuela, Simón Bolívar (1819) pronounced: "El sistema de gobierno más perfecto es aquel que produce mayor suma de felicidad posible, mayor suma de "seguridad social" y mayor suma de estabilidad política" (which translates to "The most perfect system of government is that which produces the greatest amount of happiness, the greatest amount of social security and the greatest amount of political stability").
In the Roman Empire, social welfare to help the poor was enlarged by the Emperor Trajan. Trajan's program brought acclaim from many, including Pliny the Younger.
In Jewish tradition, charity (represented by tzedakah) is a matter of religious obligation rather than benevolence. Contemporary charity is regarded as a continuation of the Biblical Maaser Ani, or poor-tithe, as well as Biblical practices, such as permitting the poor to glean the corners of a field and harvest during the Shmita (Sabbatical year). Voluntary charity, along with prayer and repentance, is befriended to ameliorate the consequences of bad acts.
The Song dynasty (c.1000AD) government supported multiple forms of social assistance programs, including the establishment of retirement homes, public clinics, and pauper's graveyards
According to Robert Henry Nelson, "The medieval Roman Catholic Church operated a far-reaching and comprehensive welfare system for the poor..."
The concepts of welfare and pension were put into practice in the early Islamic law of the Caliphate as forms of "Zakat" (charity), one of the Five Pillars of Islam, since the time of the Rashidun caliph Umar in the 7th century. The taxes (including "Zakat" and "Jizya") collected in the treasury of an Islamic government were used to provide income for the needy, including the poor, elderly, orphans, widows, and the disabled. According to the Islamic jurist Al-Ghazali (Algazel, 1058–1111), the government was also expected to store up food supplies in every region in case a disaster or famine occurred. (See Bayt al-mal for further information.)
There is relatively little statistical data on transfer payments before the High Middle Ages. In the medieval period and until the Industrial Revolution, the function of welfare payments in Europe was principally achieved through private giving or charity. In those early times, there was a much broader group considered to be in poverty as compared to the 21st century.
Early welfare programs in Europe included the English Poor Law of 1601, which gave parishes the responsibility for providing poverty relief assistance to the poor. This system was substantially modified by the 19th-century Poor Law Amendment Act, which introduced the system of workhouses.
It was predominantly in the late 19th and early 20th centuries that an organized system of state welfare provision was introduced in many countries. Otto von Bismarck, Chancellor of Germany, introduced one of the first welfare systems for the working classes in 1883. In Great Britain the Liberal government of Henry Campbell-Bannerman and David Lloyd George introduced the National Insurance system in 1911, a system later expanded by Clement Attlee. The United States did not have an organized welfare system until the Great Depression, when emergency relief measures were introduced under President Franklin D. Roosevelt. Even then, Roosevelt's New Deal focused predominantly on a program of providing work and stimulating the economy through public spending on projects, rather than on cash payment.
Income maintenance.
This policy is usually applied through various programs designed to provide a population with income at times when they are unable to care for themselves. Income maintenance is based in a combination of five main types of program:
Social protection.
Social protection refers to a set of benefits available (or not available) from the state, market, civil society and households, or through a combination of these agencies, to the individual/households to reduce multi-dimensional deprivation. This multi-dimensional deprivation could be affecting less active poor persons (such as the elderly or the disabled) and active poor persons (such as the unemployed).
This broad framework makes this concept more acceptable in developing countries than the concept of social security. Social security is more applicable in the conditions, where large numbers of citizens depend on the formal economy for their livelihood. Through a defined contribution, this social security may be managed.
But, in the context of widespread informal economy, formal social security arrangements are almost absent for the vast majority of the working population. Besides, in developing countries, the state's capacity to reach the vast majority of the poor people may be limited because of its limited infrastructure and resources. In such a context, multiple agencies that could provide for social protection, including health care, is critical for policy consideration. The framework of social protection is thus holds the state responsible for providing for the poorest populations by regulating non-state agencies.
Collaborative research from the Institute of Development Studies debating Social Protection from a global perspective, suggests that advocates for social protection fall into two broad categories: "instrumentalists" and "activists". Instrumentalists argue that extreme poverty, inequality, and vulnerability is dysfunctional in the achievement of development targets (such as the MDGs). In this view, social protection is about putting in place risk management mechanisms that will compensate for incomplete or missing insurance (and other) markets, until a time that private insurance can play a more prominent role in that society. Activist arguments view the persistence of extreme poverty, inequality, and vulnerability as symptoms of social injustice and structural inequality and see social protection as a right of citizenship. Targeted welfare is a necessary step between humanitarianism and the ideal of a "guaranteed minimum income" where entitlement extends beyond cash or food transfers and is based on citizenship, not philanthropy.

</doc>
<doc id="27692" url="http://en.wikipedia.org/wiki?curid=27692" title="Steam engine">
Steam engine

A steam engine is a heat engine that performs mechanical work using steam as its working fluid.
Using boiling water to produce mechanical motion goes back over 2000 years, but early devices were not practical. The Spanish inventor Jerónimo de Ayanz y Beaumont patented in 1606 the first steam engine. In 1698 Thomas Savery patented a steam pump that used steam in direct contact with the water being pumped. Savery's steam pump used condensing steam to create a vacuum and draw water into a chamber, and then applied pressurized steam to further pump the water. Thomas Newcomen's "atmospheric engine" was the first commercial true steam engine using a piston, and was used in 1712 for pumping in a mine. 
In 1781 James Watt patented a steam engine that produced continuous rotary motion. Watt's ten-horsepower engines enabled a wide range of manufacturing machinery to be powered. The engines could be sited anywhere that water and coal or wood fuel could be obtained. By 1883, engines that could provide 10,000 hp had become feasible. Steam engines could also be applied to vehicles such as traction engines and the railway locomotives. The stationary steam engine was a key component of the Industrial Revolution, allowing factories to locate where water power was unavailable.
Steam engines are external combustion engines, where the working fluid is separate from the combustion products. Non-combustion heat sources such as solar power, nuclear power or geothermal energy may be used. The ideal thermodynamic cycle used to analyze this process is called the Rankine cycle. In the cycle, water is heated and transforms into steam within a boiler operating at a high pressure. When expanded through pistons or turbines, mechanical work is done. The reduced-pressure steam is then condensed and pumped back into the boiler.
In general usage, the term "steam engine" can refer to either the integrated steam plants (including boilers etc.) such as railway steam locomotives and portable engines, or may refer to the piston or turbine machinery alone, as in the beam engine and stationary steam engine. Specialized devices such as steam hammers and steam pile drivers are dependent on steam supplied from a separate boiler. Reciprocating piston type steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines in commercial usage, and the ascendancy of steam turbines in power generation. Considering that the great majority of worldwide electric generation is produced by turbine type steam engines, the "steam age" is continuing with energy levels far beyond those of the turn of the 19th century.
History.
Since the early 18th century, steam power has been applied to a variety of practical uses. At first it was applied to reciprocating pumps, but from the 1780s rotative engines (i.e. those converting reciprocating motion into rotary motion) began to appear, driving factory machinery such as spinning mules and power looms. At the turn of the 19th century, steam-powered transport on both sea and land began to make its appearance becoming ever more dominant as the century progressed.
Steam engines can be said to have been the moving force behind the Industrial Revolution and saw widespread commercial use driving machinery in factories, mills and mines; powering pumping stations; and propelling transport appliances such as railway locomotives, ships and road vehicles. Their use in agriculture led to an increase in the land available for cultivation.
The weight of boilers and condensors generally makes the power-to-weight ratio of a steam plant lower than for internal combustion engines. For mobile applications steam has been largely superseded by internal combustion engines or electric motors. However most electric power is generated using steam turbine plant, so that indirectly the world's industry is still dependent on steam power. Recent concerns about fuel sources and pollution have incited a renewed interest in steam both as a component of cogeneration processes and as a prime mover. This is becoming known as the Advanced Steam movement.
Early experiments.
The history of the steam engine stretches back as far as the first century AD; the first recorded rudimentary steam engine being the aeolipile described by Greek mathematician Hero of Alexandria. In the following centuries, the few steam-powered "engines" known were, like the aeolipile, essentially experimental devices used by inventors to demonstrate the properties of steam. A rudimentary steam turbine device was described by Taqi al-Din in 1551 and by Giovanni Branca in 1629. Jerónimo de Ayanz y Beaumont received patents in 1606 for fifty steam powered inventions, including a water pump for draining inundated mines. Denis Papin, a Huguenot refugee, did some useful work on the steam digester in 1679, and first used a piston to raise weights in 1690.
Pumping engines.
The first commercial steam-powered device was a water pump, developed in 1698 by Thomas Savery. It used a vacuum to raise water from below, then used steam pressure to raise it higher. Small engines were effective though larger models were problematic. They proved only to have a limited lift height and were prone to boiler explosions. It received some use in mines, pumping stations and for supplying water wheels used to power textile machinery. An attractive feature of the Savery engine was its low cost. Bento de Moura Portugal, , introduced an ingenious improvement of Savery's construction "to render it capable of working itself", as described by John Smeaton in the Philosophical Transactions published in 1751. It continued to be manufactured until the late 18th century. One engine was still known to be operating in 1820.
Piston steam engines.
The first commercially successful true engine, in that it could generate power and transmit it to a machine, was the atmospheric engine, invented by Thomas Newcomen around 1712. It made use of technologies discovered by Savery and Papin. Newcomen's engine was relatively inefficient, and in most cases was used for pumping water. It worked by creating a partial vacuum by condensing steam under a piston within a cylinder. It was employed for draining mine workings at depths hitherto impossible, and also for providing a reusable water supply for driving waterwheels at factories sited away from a suitable "head". Water that had passed over the wheel was pumped back up into a storage reservoir above the wheel.
In 1720 Jacob Leupold described a two-cylinder high-pressure steam engine. The invention was published in his major work "Theatri Machinarum Hydraulicarum". The engine used two lead-weighted pistons providing a continuous motion to a water pump. Each piston was raised by the steam pressure and returned to its original position by gravity. The two pistons shared a common four way rotary valve connected directly to a steam boiler.
The next major step occurred when James Watt developed (1763–1775) an improved version of Newcomen's engine, with a separate condenser. Boulton and Watt's early engines used half as much coal as John Smeaton's improved version of Newcomen's. Newcomen's and Watt's early engines were "atmospheric". They were powered by air pressure pushing a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam. The engine cylinders had to be large because the only usable force acting on them was due to atmospheric pressure.
Watt proceeded to develop his engine further, modifying it to provide a rotary motion suitable for driving factory machinery. This enabled factories to be sited away from rivers, and further accelerated the pace of the Industrial Revolution.
High-pressure engines.
Around 1800 Richard Trevithick and, separately, Oliver Evans in 1801 introduced engines using high-pressure steam; Trevithick obtained his high-pressure engine patent in 1802. These were much more powerful for a given cylinder size than previous engines and could be made small enough for transport applications. Thereafter, technological developments and improvements in manufacturing techniques (partly brought about by the adoption of the steam engine as a power source) resulted in the design of more efficient engines that could be smaller, faster, or more powerful, depending on the intended application.
The Cornish engine was developed by Trevithick and others in the 1810s. It was a compound cycle engine that used high-pressure steam expansively, then condensed the low-pressure steam, making it relatively efficient. The Cornish engine had irregular motion and torque though the cycle, limiting it mainly to pumping. Cornish engines were used in mines and for water supply until the late 19th century.
Horizontal stationary engine.
Early builders of stationary steam engines considered that horizontal cylinders would be subject to excessive wear. Their engines were therefore arranged with the piston axis vertical. In time the horizontal arrangement became more popular, allowing compact, but powerful engines to be fitted in smaller spaces.
The acme of the horizontal engine was the Corliss steam engine, patented in 1849, which was a four-valve counter flow engine with separate steam admission and exhaust valves and automatic variable steam cutoff. When Corliss was given the Rumford medal the committee said that "no one invention since Watt's time has so enhanced the efficiency of the steam engine". In addition to using 30% less steam, it provided more uniform speed due to variable steam cut off, making it well suited to manufacturing, especially cotton spinning.
Marine engines.
Near the end of the 19th century compound engines came into widespread use. Compound engines exhausted steam in to successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved efficiency. These stages were called expansions, with double and triple expansion engines being common, especially in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines, with shipping in the 20th-century relying upon the steam turbine.
Steam locomotives.
As the development of steam engines progressed through the 18th century, various attempts were made to apply them to road and railway use. In 1784, William Murdoch, a Scottish inventor, built a prototype steam road locomotive. An early working model of a steam rail locomotive was designed and constructed by steamboat pioneer John Fitch in the United States probably during the 1780s or 1790s.
His steam locomotive used interior bladed wheels guided by rails or tracks.
The first full-scale working railway steam locomotive was built by Richard Trevithick in the United Kingdom and, on 21 February 1804, the world's first railway journey took place as Trevithick's unnamed steam locomotive hauled a train along the tramway from the Pen-y-darren ironworks, near Merthyr Tydfil to Abercynon in south Wales. The design incorporated a number of important innovations that included using high-pressure steam which reduced the weight of the engine and increased its efficiency. Trevithick visited the Newcastle area later in 1804 and the colliery railways in north-east England became the leading centre for experimentation and development of steam locomotives.
Trevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can in 1808. Only four years later, the successful twin-cylinder locomotive "Salamanca" by Matthew Murray was used by the edge railed rack and pinion Middleton Railway. In 1825 George Stephenson built the "Locomotion" for the Stockton and Darlington Railway. This was the first public steam railway in the world and then in 1829, he built "The Rocket" which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in 1830 making exclusive use of steam power for both passenger and freight trains.
Steam locomotives continued to be manufactured until the late twentieth century in places such as China and the former East Germany.
Steam turbines.
The final major evolution of the steam engine design was the use of steam turbines starting in the late part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines (for outputs above several hundred horsepower), have fewer moving parts, and provide rotary power directly instead of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In the United States 90% of the electric power is produced in this way using a variety of heat sources. Steam turbines were extensively applied for propulsion of large ships throughout most of the 20th century.
Present development.
Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. 4 kg of steam per kWh.
Components and accessories of steam engines.
There are two fundamental components of a steam plant: the boiler or steam generator, and the "motor unit", referred to itself as a "steam engine". Stationary steam engines in fixed buildings may have the boiler and engine in separate buildings some distance apart. For portable or mobile use, such as steam locomotives, the two are mounted together.
The widely used reciprocating engine typically consisted of a cast iron cylinder, piston, connecting rod and beam or a crank and flywheel, and miscellaneous linkages. Steam was alternately supplied and exhausted by one or more valves. Speed control was either automatic, using a governor, or by a manual valve. The cylinder casting contained steam supply and exhaust ports.
Engines equipped with a condenser are a separate type than those that exhaust to the atmosphere.
Other components are often present; pumps (such as an injector) to supply water to the boiler during operation, condensers to recirculate the water and recover the latent heat of vaporisation, and superheaters to raise the temperature of the steam above its saturated vapour point, and various mechanisms to increase the draft for fireboxes. When coal is used, a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox. See: Mechanical stoker
Heat source.
The heat required for boiling the water and supplying the steam can be derived from various sources, most commonly from burning combustible materials with an appropriate supply of air in a closed space (called variously combustion chamber, firebox). In some cases the heat source is a nuclear reactor or geothermal energy.
Boilers.
Boilers are pressure vessels that contain water to be boiled, and some kind of mechanism for transferring the heat to the water so as to boil it.
The two most common methods of transferring heat to the water are:
Fire tube boilers were the main type used for early high-pressure steam (typical steam locomotive practice), but they were to a large extent displaced by more economical water tube boilers in the late 19th century for marine propulsion and large stationary applications.
Once turned to steam, many boilers raise the temperature of the steam further, turning 'wet steam' into 'superheated steam'. This use of superheating avoids the steam condensing within the engine, and allows significantly greater efficiency.
Motor units.
In a steam engine, a piston or steam turbine or any other similar device for doing mechanical work takes a supply of steam at high pressure and temperature and gives out a supply of steam at lower pressure and temperature, using as much of the difference in steam energy as possible to do mechanical work.
These "motor units" are often called 'steam engines' in their own right. They will also operate on compressed air or other gas.
Cold sink.
As with all heat engines, a considerable quantity of waste heat at relatively low temperature is produced and must be disposed of.
The simplest cold sink is to vent the steam to the environment. This is often used on steam locomotives, as the released steam is released in the chimney so as to increase the draw on the fire, which greatly increases engine power, but is inefficient.
Sometimes the waste heat is useful itself, and in those cases very high overall efficiency can be obtained. For example, combined heat and power (CHP) systems use the waste steam for district heating.
Where CHP is not used, steam turbines in power stations use surface condensers as a cold sink. The condensers are cooled by water flow from oceans, rivers, lakes, and often by cooling towers which evaporate water to provide cooling energy removal. The resulting condensed hot water output from the condenser is then put back into the boiler via a pump. A dry type cooling tower is similar to an automobile radiator and is used in locations where water is costly. Evaporative (wet) cooling towers use the rejected heat to evaporate water; this water is kept separate from the condensate, which circulates in a closed system and returns to the boiler. Such towers often have visible plumes due to the evaporated water condensing into droplets carried up by the warm air. Evaporative cooling towers need less water flow than "once-through" cooling by river or lake water; a 700 megawatt coal-fired power plant may use about 3600 cubic metres of make-up water every hour for evaporative cooling, but would need about twenty times as much if cooled by river water. 
Water pump.
The Rankine cycle and most practical steam engines have a water pump to recycle or top up the boiler water, so that they may be run continuously. Utility and industrial boilers commonly use multi-stage centrifugal pumps; however, other types are used. Another means of supplying lower-pressure boiler feed water is an injector, which uses a steam jet usually supplied from the boiler. Injectors became popular in the 1850s but are no longer widely used, except in applications such as steam locomotives.
Monitoring and control.
For safety reasons, nearly all steam engines are equipped with mechanisms to monitor the boiler, such as a pressure gauge and a sight glass to monitor the water level.
Many engines, stationary and mobile, are also fitted with a governor (see below) to regulate the speed of the engine without the need for human interference (similar to cruise control in some cars).
The most useful instrument for analyzing the performance of steam engines is the steam engine indicator. Early versions were in use by 1851, but the most successful indicator was developed for the high speed engine inventor and manufacturer Charles Porter by Charles Richard and exhibited at London Exhibition in 1862. The steam engine indicator traces on paper the pressure in the cylinder throughout the cycle, which can be used to spot various problems and calculate developed horsepower. It was routinely used by engineers, mechanics and insurance inspectors. The engine indicator can also be used on internal combustion engines. See image of indicator diagram below (in "Types of motor units" section).
Governor.
The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt’s partner Boulton saw one at a flour mill Boulton & Watt were building. The governor could not actually hold a set speed, because it would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped only with this governor were not suitable for operations requiring constant speed, such as cotton spinning. The governor was improved over time and coupled with variable steam cut off, good speed control in response to changes in load was attainable near the end of the 19th century.
Engine configuration.
Simple engine.
In a simple engine the charge of steam works only once in a cylinder. It is then exhausted directly into the atmosphere or into a condenser. As steam expands in a high-pressure engine its temperature drops because no heat is added to the system; this is known as adiabatic expansion and results in steam entering the cylinder at high temperature and leaving at low temperature. This causes a cycle of heating and cooling of the cylinder with every stroke, which is a source of inefficiency.
Compound engines.
A method to lessen the magnitude of this heating and cooling was invented in 1804 by British engineer Arthur Woolf, who patented his "Woolf high-pressure compound engine" in 1805. In the compound engine, high-pressure steam from the boiler expands in a high-pressure (HP) cylinder and then enters one or more subsequent lower-pressure (LP) cylinders. The complete expansion of the steam now occurs across multiple cylinders and as less expansion now occurs in each cylinder less heat is lost by the steam in each. This reduces the magnitude of cylinder heating and cooling, increasing the efficiency of the engine. By staging the expansion in multiple cylinders, torque variability can be reduced. To derive equal work from lower-pressure steam requires a larger cylinder volume as this steam occupies a greater volume. Therefore the bore, and often the stroke, are increased in low-pressure cylinders resulting in larger cylinders.
Double expansion (usually known as compound) engines expanded the steam in two stages. The pairs may be duplicated or the work of the large low-pressure cylinder can be split with one high-pressure cylinder exhausting into one or the other, giving a 3-cylinder layout where cylinder and piston diameter are about the same making the reciprocating masses easier to balance.
Two-cylinder compounds can be arranged as:
With two-cylinder compounds used in railway work, the pistons are connected to the cranks as with a two-cylinder simple at 90° out of phase with each other ("quartered").
When the double expansion group is duplicated, producing a 4-cylinder compound, the individual pistons within the group are usually balanced at 180°, the groups being set at 90° to each other. In one case (the first type of Vauclain compound), the pistons worked in the same phase driving a common crosshead and crank, again set at 90° as for a two-cylinder engine.
With the 3-cylinder compound arrangement, the LP cranks were either set at 90° with the HP one at 135° to the other two, or in some cases all three cranks were set at 120°.
The adoption of compounding was common for industrial units, for road engines and almost universal for marine engines after 1880; it was not universally popular in railway locomotives where it was often perceived as complicated. This is partly due to the harsh railway operating environment and limited space afforded by the loading gauge (particularly in Britain, where compounding was never common and not employed after 1930). However, although never in the majority, it was popular in many other countries.
Multiple expansion engines.
It is a logical extension of the compound engine (described above) to split the expansion into yet more stages to increase efficiency. The result is the multiple expansion engine. Such engines use either three or four expansion stages and are known as "triple" and "quadruple expansion engines" respectively. These engines use a series of cylinders of progressively increasing diameter. These cylinders are designed to divide the work into equal shares for each expansion stage. As with the double expansion engine, if space is at a premium, then two smaller cylinders may be used for the low-pressure stage. Multiple expansion engines typically had the cylinders arranged inline, but various other formations were used. In the late 19th century, the Yarrow-Schlick-Tweedy balancing 'system' was used on some marine triple expansion engines. Y-S-T engines divided the low-pressure expansion stages between two cylinders, one at each end of the engine. This allowed the crankshaft to be better balanced, resulting in a smoother, faster-responding engine which ran with less vibration. This made the 4-cylinder triple-expansion engine popular with large passenger liners (such as the Olympic class), but this was ultimately replaced by the virtually vibration-free turbine (see below).
The image to the right shows an animation of a triple expansion engine. The steam travels through the engine from left to right. The valve chest for each of the cylinders is to the left of the corresponding cylinder.
Land-based steam engines could exhaust much of their steam, as feed water was usually readily available. Prior to and during World War I, the expansion engine dominated marine applications where high vessel speed was not essential. It was however superseded by the British invention steam turbine where speed was required, for instance in warships, such as the dreadnought battleships, and ocean liners. HMS "Dreadnought" of 1905 was the first major warship to replace the proven technology of the reciprocating engine with the then-novel steam turbine.
Types of motor units.
Reciprocating piston.
In most reciprocating piston engines, the steam reverses its direction of flow at each stroke (counterflow), entering and exhausting from the cylinder by the same port. The complete engine cycle occupies one rotation of the crank and two piston strokes; the cycle also comprises four "events" – admission, expansion, exhaust, compression. These events are controlled by valves often working inside a "steam chest" adjacent to the cylinder; the valves distribute the steam by opening and closing steam "ports" communicating with the cylinder end(s) and are driven by valve gear, of which there are many types.
The simplest valve gears give events of fixed length during the engine cycle and often make the engine rotate in only one direction. Most however have a reversing mechanism which additionally can provide means for saving steam as speed and momentum are gained by gradually "shortening the cutoff" or rather, shortening the admission event; this in turn proportionately lengthens the expansion period. However, as one and the same valve usually controls both steam flows, a short cutoff at admission adversely affects the exhaust and compression periods which should ideally always be kept fairly constant; if the exhaust event is too brief, the totality of the exhaust steam cannot evacuate the cylinder, choking it and giving excessive compression ("kick back").
In the 1840s and 50s, there were attempts to overcome this problem by means of various patent valve gears with a separate, variable cutoff expansion valve riding on the back of the main slide valve; the latter usually had fixed or limited cutoff. The combined setup gave a fair approximation of the ideal events, at the expense of increased friction and wear, and the mechanism tended to be complicated. The usual compromise solution has been to provide "lap" by lengthening rubbing surfaces of the valve in such a way as to overlap the port on the admission side, with the effect that the exhaust side remains open for a longer period after cut-off on the admission side has occurred. This expedient has since been generally considered satisfactory for most purposes and makes possible the use of the simpler Stephenson, Joy and Walschaerts motions. Corliss, and later, poppet valve gears had separate admission and exhaust valves driven by trip mechanisms or cams profiled so as to give ideal events; most of these gears never succeeded outside of the stationary marketplace due to various other issues including leakage and more delicate mechanisms.
Before the exhaust phase is quite complete, the exhaust side of the valve closes, shutting a portion of the exhaust steam inside the cylinder. This determines the compression phase where a cushion of steam is formed against which the piston does work whilst its velocity is rapidly decreasing; it moreover obviates the pressure and temperature shock, which would otherwise be caused by the sudden admission of the high-pressure steam at the beginning of the following cycle.
The above effects are further enhanced by providing "lead": as was later discovered with the internal combustion engine, it has been found advantageous since the late 1830s to advance the admission phase, giving the valve "lead" so that admission occurs a little before the end of the exhaust stroke in order to fill the "clearance volume" comprising the ports and the cylinder ends (not part of the piston-swept volume) before the steam begins to exert effort on the piston.
Uniflow (or unaflow) engine.
Uniflow engines attempt to remedy the difficulties arising from the usual counterflow cycle where, during each stroke, the port and the cylinder walls will be cooled by the passing exhaust steam, whilst the hotter incoming admission steam will waste some of its energy in restoring working temperature. The aim of the uniflow is to remedy this defect and improve efficiency by providing an additional port uncovered by the piston at the end of each stroke making the steam flow only in one direction. By this means, the simple-expansion uniflow engine gives efficiency equivalent to that of classic compound systems with the added advantage of superior part-load performance, and comparable efficiency to turbines for smaller engines below one thousand horsepower. However, the thermal expansion gradient uniflow engines produce along the cylinder wall gives practical difficulties.
Turbine engines.
A steam turbine consists of one or more "rotors" (rotating discs) mounted on a drive shaft, alternating with a series of "stators" (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the USA with 60 Hertz power, 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore a reversing stage or gearbox is usually required where power is required in the opposite direction.
Steam turbines provide direct rotational force and therefore do not require a linkage mechanism to convert reciprocating to rotary motion. Thus, they produce smoother rotational forces on the output shaft. This contributes to a lower maintenance requirement and less wear on the machinery they power than a comparable reciprocating engine.
The main use for steam turbines is in electricity generation (in the 1990s about 90% of the world's electric production was by use of steam turbines) however the recent widespread application of large gas turbine units and typical combined cycle power plants has resulted in reduction of this percentage to the 80% regime for steam turbines. In electricity production, the high speed of turbine rotation matches well with the speed of modern electric generators, which are typically direct connected to their driving turbines. In marine service, (pioneered on the "Turbinia"), steam turbines with reduction gearing (although the Turbinia has direct turbines to propellers with no reduction gearbox) dominated large ship propulsion throughout the late 20th century, being more efficient (and requiring far less maintenance) than reciprocating steam engines. In recent decades, reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications.
Virtually all nuclear power plants generate electricity by heating water to provide steam that drives a turbine connected to an electrical generator. Nuclear-powered ships and submarines either use a steam turbine directly for main propulsion, with generators providing auxiliary power, or else employ turbo-electric transmission, where the steam drives a turbo generator set with propulsion provided by electric motors. A limited number of steam turbine railroad locomotives were manufactured. Some non-condensing direct-drive locomotives did meet with some success for long haul freight operations in Sweden and for express passenger work in Britain, but were not repeated. Elsewhere, notably in the U.S.A., more advanced designs with electric transmission were built experimentally, but not reproduced. It was found that steam turbines were not ideally suited to the railroad environment and these locomotives failed to oust the classic reciprocating steam unit in the way that modern diesel and electric traction has done.
Oscillating cylinder steam engines.
An oscillating cylinder steam engine is a variant of the simple expansion steam engine which does not require valves to direct steam into and out of the cylinder. Instead of valves, the entire cylinder rocks, or oscillates, such that one or more holes in the cylinder line up with holes in a fixed port face or in the pivot mounting (trunnion). These engines are mainly used in toys and models, because of their simplicity, but have also been used in full size working engines, mainly on ships where their compactness is valued.
Rotary steam engines.
It is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed, from the time of James Watt to the present day, but relatively few were actually built and even fewer went into quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very inefficient. Lack of expansive working, or any means of control of the cutoff is also a serious problem with many such designs.
By the 1840s, it was clear that the concept had inherent problems and rotary engines were treated with some derision in the technical press. However, the arrival of electricity on the scene, and the obvious advantages of driving a dynamo directly from a high-speed engine, led to something of a revival in interest in the 1880s and 1890s, and a few designs had some limited success.
Of the few designs that were manufactured in quantity, those of the Hult Brothers Rotary Steam Engine Company of Stockholm, Sweden, and the spherical engine of Beauchamp Tower are notable. Tower's engines were used by the Great Eastern Railway to drive lighting dynamos on their locomotives, and by the Admiralty for driving dynamos on board the ships of the Royal Navy. They were eventually replaced in these niche applications by steam turbines.
Rocket type.
The aeolipile represents the use of steam by the rocket-reaction principle, although not for direct propulsion.
In more modern times there has been limited use of steam for rocketry – particularly for rocket cars. Steam rocketry works by filling a pressure vessel with hot water at high pressure and opening a valve leading to a suitable nozzle. The drop in pressure immediately boils some of the water and the steam leaves through a nozzle, creating a propulsive force.
Safety.
Steam engines possess boilers and other components that are pressure vessels that contain a great deal of potential energy. Steam escapes and boiler explosions (typically BLEVEs) can and have in the past caused great loss of life. While variations in standards may exist in different countries, stringent legal, testing, training, care with manufacture, operation and certification is applied to ensure safety. See: Pressure vessel
Failure modes may include:
Steam engines frequently possess two independent mechanisms for ensuring that the pressure in the boiler does not go too high; one may be adjusted by the user, the second is typically designed as an ultimate fail-safe. Such safety valves traditionally used a simple lever to restrain a plug valve in the top of a boiler. One end of the lever carried a weight or spring that restrained the valve against steam pressure. Early valves could be adjusted by engine drivers, leading to many accidents when a driver fastened the valve down to allow greater steam pressure and more power from the engine. The more recent type of safety valve uses an adjustable spring-loaded valve, which is locked such that operators may not tamper with its adjustment unless a seal illegally is broken. This arrangement is considerably safer. 
Lead fusible plugs may be present in the crown of the boiler's firebox. If the water level drops, such that the temperature of the firebox crown increases significantly, the lead melts and the steam escapes, warning the operators, who may then manually suppress the fire. Except in the smallest of boilers the steam escape has little effect on dampening the fire. The plugs are also too small in area to lower steam pressure significantly, depressurizing the boiler. If they were any larger, the volume of escaping steam would itself endanger the crew.
Steam cycle.
The Rankine cycle is the fundamental thermodynamic underpinning of the steam engine. The cycle is an arrangement of components as is typically used for simple power production, and utilizes the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system. The heat is supplied externally to a closed loop with some of the heat added being converted to work and the waste heat being removed in a condenser. The Rankine cycle is used in virtually all steam power production applications. In the 1990s, Rankine steam cycles generated about 90% of all electric power used throughout the world, including virtually all solar, biomass, coal and nuclear power plants. It is named after William John Macquorn Rankine, a Scottish polymath.
The Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition (in the boiler) and rejection (in the condenser) are isobaric (constant pressure) processes in the Rankine cycle and isothermal (constant temperature) processes in the theoretical Carnot cycle. In this cycle a pump is used to pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the energy needed to compress the working fluid in gaseous form in a compressor (as in the Carnot cycle). The cycle of a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in the cylinder or in the steam inlet passages.
The working fluid in a Rankine cycle can operate as a closed loop system, where the working fluid is recycled continuously, or may be an "open loop" system, where the exhaust steam is directly released to the atmosphere, and a separate source of water feeding the boiler is supplied. Normally water is the fluid of choice due to its favourable properties, such as non-toxic and unreactive chemistry, abundance, low cost, and its thermodynamic properties. Mercury is the working fluid in the mercury vapor turbine. Low boiling hydrocarbons can be used in a binary cycle.
The steam engine contributed much to the development of thermodynamic theory; however, the only applications of scientific theory that influenced the steam engine were the original concepts of harnessing the power of steam and atmospheric pressure and knowledge of properties of heat and steam. The experimental measurements made by Watt on a model steam engine led to the development of the separate condenser. Watt independently discovered latent heat, which was confirmed by the original discoverer Joseph Black, who also advised Watt on experimental procedures. Watt was also aware of the change in the boiling point of water with pressure. Otherwise, the improvements to the engine itself were more mechanical in nature. The thermodynamic concepts of the Rankine cycle did give engineers the understanding needed to calculate efficiency which aided the development of modern high-pressure and -temperature boilers and the steam turbine.
Efficiency.
The efficiency of an engine can be calculated by dividing the energy output of mechanical work that the engine produces by the energy input to the engine by the burning fuel.
The historical measure of a steam engine's energy efficiency was its "duty". The concept of duty was first introduced by Watt in order to illustrate how much more efficient his engines were over the earlier Newcomen designs. Duty is the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal. The best examples of Newcomen designs had a duty of about 7 million, but most were closer to 5 million. Watt's original low-pressure designs were able to deliver duty as high as 25 million, but averaged about 17. This was a three-fold improvement over the average Newcomen design. Early Watt engines equipped with high-pressure steam improved this to 65 million.
No heat engine can be more efficient than the Carnot cycle, in which heat is moved from a high temperature reservoir to one at a low temperature, and the efficiency depends on the temperature difference. For the greatest efficiency, steam engines should be operated at the highest steam temperature possible (superheated steam), and release the waste heat at the lowest temperature possible.
The efficiency of a Rankine cycle is usually limited by the working fluid. Without the pressure reaching supercritical levels for the working fluid, the temperature range the cycle can operate over is quite small; in steam turbines, turbine entry temperatures are typically 565 °C (the creep limit of stainless steel) and condenser temperatures are around 30 °C. This gives a theoretical Carnot efficiency of about 63% compared with an actual efficiency of 42% for a modern coal-fired power station. This low turbine entry temperature (compared with a gas turbine) is why the Rankine cycle is often used as a bottoming cycle in combined-cycle gas turbine power stations.
One of the principal advantages the Rankine cycle holds over others is that during the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point. By condensing the fluid, the work required by the pump consumes only 1% to 3% of the turbine power and contributes to a much higher efficiency for a real cycle. The benefit of this is lost somewhat due to the lower heat addition temperature. Gas turbines, for instance, have turbine entry temperatures approaching 1500 °C. Nonetheless, the efficiencies of actual large steam cycles and large modern gas turbines are fairly well matched.
In practice, a steam engine exhausting the steam to atmosphere will typically have an efficiency (including the boiler) in the range of 1-10%, but with the addition of a condenser and multiple expansion, and high steam pressure/temperature, it may be greatly improved, historically into the regime of 10-20%, and very rarely slightly higher.
A modern large electrical power station (producing several hundred megawatts of electrical output) with steam reheat, economizer etc. will achieve efficiency in the mid 40% range, with the most efficient units approaching 50% thermal efficiency.
It is also possible to capture the waste heat using cogeneration in which the waste heat is used for heating a lower boiling point working fluid or as a heat source for district heating via saturated low-pressure steam.

</doc>
<doc id="27694" url="http://en.wikipedia.org/wiki?curid=27694" title="Satan">
Satan

Satan (Hebrew: שָּׂטָן "satan", meaning "adversary"; Arabic: شيطان "shaitan", meaning "astray" or "distant", sometimes "devil") is a figure appearing in the texts of the Abrahamic religions who brings evil and temptation, and is known as the deceiver who leads humanity astray. Some religious groups teach that he originated as an angel who fell out of favor with God, seducing humanity into the ways of sin, and who has power in the fallen world. In the Hebrew Bible and the New Testament, Satan is primarily an accuser and adversary, a decidedly malevolent entity, also called the devil, who possesses demonic qualities.
In Theistic Satanism, Satan is considered a positive force and deity who is either worshipped or revered. In LaVeyan Satanism, Satan is regarded as holding virtuous characteristics.
Judaism.
Hebrew Bible.
The original Hebrew term "satan" is a noun from a verb meaning primarily "to obstruct, oppose", as it is found in Numbers 22:22, 1 Samuel 29:4, Psalms 109:6. "Ha-Satan" is traditionally translated as "the accuser" or "the adversary". The definite article "ha-" (English: "the") is used to show that this is a title bestowed on a being, versus the name of a being. Thus, this being would be referred to as "the satan".
Thirteen occurrences.
"Ha-Satan" with the definite article occurs 13 times in the Masoretic Text, in two books of the Hebrew Bible: Job ch.1–2 (10x) and Zechariah 3:1–2 (3x).
"Satan" without the definite article is used in 10 instances, of which two are translated "diabolos" in the Septuagint and "Satan" in the King James Version:
The other eight instances of "satan" without the definite article are traditionally translated (in Greek, Latin and English) as "an adversary", etc., and taken to be humans or obedient angels:
Book of Job.
At the beginning of the book, Job is a good person "who revered God and turned away from evil" (Job 1:1), and has therefore been rewarded by God. When the angels present themselves to God, Satan comes as well. God informs Satan about Job's blameless, morally upright character. Between Job 1:9–10 and 2:4–5, Satan points out that God has given Job everything that a man could want, so of course Job would be loyal to God; Satan suggests that Job's faith would collapse if all he has been given (even his health) were to be taken away from him. God therefore gives Satan permission to test Job. In the end, Job remains faithful and righteous, and there is the implication that Satan is shamed in his defeat.
Second Temple period.
Septuagint.
In the Septuagint, the Hebrew "ha-Satan" in Job and Zechariah is translated by the Greek word "diabolos" (slanderer), the same word in the Greek New Testament from which the English word devil is derived. Where "satan" is used of human enemies in the Hebrew Bible, such as Hadad the Edomite and Rezon the Syrian, the word is left untranslated but transliterated in the Greek as "satan", a neologism in Greek. In Zechariah 3, this changes the vision of the conflict over Joshua the High Priest in the Septuagint into a conflict between "Jesus and the devil", identical with the Greek text of Matthew.
Dead Sea scrolls and Pseudepigrapha.
In Enochic Judaism, the concept of Satan being an opponent of God and a chief evil figure in among demons seems to have taken root in Jewish pseudepigrapha during the Second Temple period, particularly in the "apocalypses". 
The Book of Enoch contains references to Satariel, thought also to be Sataniel and Satan'el (etymology dating back to Babylonian origins). The similar spellings mirror that of his angelic brethren Michael, Raphael, Uriel, and Gabriel, previous to the fall from Heaven.
The Second Book of Enoch, also called the "Slavonic Book of Enoch", contains references to a Watcher (Grigori) called Satanael. It is a pseudepigraphic text of an uncertain date and unknown authorship. The text describes Satanael as being the prince of the Grigori who was cast out of heaven and an evil spirit who knew the difference between what was "righteous" and "sinful". A similar story is found in the book of 1 Enoch; however, in that book, the leader of the Grigori is called Semjâzâ.
In the Book of Wisdom, the devil is represented as the being who brought death into the world.
In the Book of Jubilees, Mastema induces God to test Abraham through the sacrifice of Isaac. He is identical to Satan in both name and nature.
Rabbinical Judaism.
In Judaism, Satan is a term used since its earliest biblical contexts to refer to a "human opponent". Occasionally, the term has been used to suggest "evil influence" opposing human beings, as in the Jewish exegesis of the Yetzer hara ("evil inclination" Genesis 6:5). Micaiah's "lying spirit" in 1 Kings 22:22 is sometimes related. Thus, Satan is personified as a character in three different places of the Tenakh, serving as an accuser (Zechariah 3:1–2), a seducer (1 Chronicles 21:1), or as a heavenly persecutor who is "among the sons of God" (Job 2:1). In any case, Satan is always subordinate to the power of God, having a role in the divine plan. Satan is rarely mentioned in Tannaitic literature, but is found in Babylonian aggadah.
In medieval Judaism, the Rabbis rejected these Enochic literary works into the Biblical canon, making every attempt to root them out. Traditionalists and philosophers in medieval Judaism adhered to rational theology, rejecting any belief in rebel or fallen angels, and viewing evil as abstract. The Yetzer hara ("evil inclination" Genesis 6:5) is a more common motif for evil in rabbinical texts. Rabbinical scholarship on the Book of Job generally follows the Talmud and Maimonides as identifying the "Adversary" in the prologue of Job as a metaphor.
In Hasidic Judaism, the Kabbalah presents Satan as an agent of God whose function is to tempt one into sin, then turn around and accuse the sinner on high. The Chasidic Jews of the 18th century associated ha-Satan with "Baal Davar".
Dualism and Zoroastrianism.
Some scholars see contact with religious dualism in Babylon, and early Zoroastrianism in particular, as being influenced by Second Temple period Judaism, and consequently early Christianity. Subsequent development of Satan as a "deceiver" has parallels with the evil spirit in Zoroastrianism, known as the Lie, who directs forces of darkness.
Christianity.
Satan is traditionally identified as the serpent who tempted Eve to eat the forbidden fruit, as he was in Judaism. Thus Satan has often been depicted as a serpent. Christian agreement with this can be found in the works of Justin Martyr, in Chapters 45 and 79 of "Dialogue with Trypho", where Justin identifies Satan and the serpent. Other early church fathers to mention this identification include Theophilus and Tertullian.
From the fourth century, Lucifer is sometimes used in Christian theology to refer to Satan, as a result of identifying the fallen "son of the dawn" of with the "accuser" of other passages in the Old Testament.
For most Christians, Satan is believed to be an angel who rebelled against God. His goal is to lead people away from the love of God; i.e., to lead them to evil.
In the New Testament he is called "the ruler of the demons" (), "the ruler of the world", and "the god of this world" (). The Book of Revelation describes how Satan was cast out of Heaven, having "great anger" and waging war against "those who obey God's commandments". Ultimately, Satan will be thrown into the lake of fire.
The early Christian church encountered opposition from pagans such as Celsus, who claimed that "it is blasphemy...to say that the greatest God...has an adversary who constrains his capacity to do good" and said that Christians "impiously divide the kingdom of God, creating a rebellion in it, as if there were opposing factions within the divine, including one that is hostile to God".
Terminology.
In Christianity, there are many synonyms for Satan. The most common English synonym for "Satan" is "Devil", which descends from Middle English "devel," from Old English "dēofol," that in turn represents an early Germanic borrowing of Latin "diabolus" (also the source of "diabolical"). This in turn was borrowed from Greek "diabolos" "slanderer", from "diaballein" "to slander": "dia-" "across, through" + "ballein" "to hurl". In the New Testament, "Satan" occurs more than 30 times in passages alongside "Diabolos" (Greek for "the devil"), referring to the same person or thing as Satan.
Beelzebub, meaning "Lord of Flies", is the contemptuous name given in the Hebrew Bible and New Testament to a Philistine god whose original name has been reconstructed as most probably "Ba'al Zabul", meaning "Baal the Prince". This pun was later used to refer to Satan as well.
The Book of Revelation twice refers to "the dragon, that ancient serpent, who is called the devil and Satan" (12:9, 20:2). The Book of Revelation also refers to "the deceiver", from which is derived the common epithet "the great deceiver".
Islam.
"Shaitan" (شيطان) is the equivalent of Satan in Islam. While "Shaitan" (شيطان, from the root šṭn شط⁬ن) is an adjective (meaning "astray" or "distant", sometimes translated as "devil") that can be applied to both man ("al-ins", الإنس) and Jinn, Iblis (]) is the personal name of the Devil who is mentioned in the Qur'anic account of Genesis. According to the Qur'an, Iblis (the Arabic name used) disobeyed an order from Allah to bow to Adam, and as a result Iblis was forced out of heaven. However, he was given respite from further punishment until the day of judgment.
When Allah commanded all of the angels to bow down before Adam (the first Human), Iblis, full of hubris and jealousy, refused to obey God's command (he could do so because he had free will), seeing Adam as being inferior in creation due to his being created from clay as compared to him (created of fire).
It is We Who created you and gave you shape; then We bade the angels prostrate to Adam, and they prostrate; not so Iblis (Lucifer); He refused to be of those who prostrate.
(Allah) said: "What prevented thee from prostrating when I commanded thee?" He said: "I am better than he: Thou didst create me from fire, and him from clay."—Qur'an 7:11–12
It was after this that the title of "Shaitan" was given, which can be roughly translated as "Enemy", "Rebel", "Evil", or "Devil". Shaitan then claims that, if the punishment for his act of disobedience is to be delayed until the Day of Judgment, then he will divert many of Adam's own descendants from the straight path during his period of respite. God accepts the claims of Iblis and guarantees recompense to Iblis and his followers in the form of Hellfire. In order to test mankind and jinn alike, Allah allowed Iblis to roam the earth to attempt to convert others away from his path. He was sent to earth along with Adam and Eve, after eventually luring them into eating the fruit from the forbidden tree.
Yazidism.
An alternative name for the main deity in the tentatively Indo-European pantheon of the Yazidi, Malek Taus, is Shaitan. However, rather than being Satanic, Yazidism is better understood as a remnant of a pre-Islamic Middle Eastern Indo-European religion, and/or a ghulat Sufi movement founded by Shaykh Adi. The connection with Satan, originally made by Muslim outsiders, attracted the interest of 19th century European travelers and esoteric writers.
Bahá'í Faith.
In the Bahá'í Faith, "Satan" is not regarded as an independent evil power as he is in some faiths, but signifies the "lower nature" of humans. `Abdu'l-Bahá explains: "This lower nature in man is symbolized as Satan — the evil ego within us, not an evil personality outside." All other evil spirits described in various faith traditions—such as fallen angels, demons, and jinns—are also metaphors for the base character traits a human being may acquire and manifest when he turns away from God.
Satanism.
Within Satanism, two major trends exists, theistic Satanism and atheistic Satanism, both having different views regarding the essence of Satan.
Theistic Satanism.
Theistic Satanism, commonly referred to as 'devil-worship', holds that Satan is an actual deity or force to revere or worship that individuals may contact and supplicate to, and represents loosely affiliated or independent groups and cabals which hold the belief that Satan is a real entity rather than an archetype.
Among non-Satanists, much modern Satanic folklore does not originate with the beliefs or practices of theistic or atheistic Satanists, but a mixture of medieval Christian folk beliefs, political or sociological conspiracy theories, and contemporary urban legends. An example is the Satanic ritual abuse scare of the 1980s—beginning with the memoir "Michelle Remembers"—which depicted Satanism as a vast conspiracy of elites with a predilection for child abuse and human sacrifice. This genre frequently describes Satan as physically incarnating in order to receive worship.
Atheistic Satanism.
Atheistic Satanism, most commonly referred to as LaVeyan Satanism, holds that Satan does not exist as a literal anthropomorphic entity, but rather a symbol of pride, carnality, liberty, enlightenment, undefiled wisdom, and of a cosmos which Satanists perceive to be permeated and motivated by a force that has been given many names by humans over the course of time. To adherents, he also serves as a conceptual framework and an external metaphorical projection of [the Satanists] highest personal potential.
In his essay, "Satanism: The Feared Religion", the current High Priest of the Church of Satan, Peter H. Gilmore, further expounds that "...Satan is a symbol of Man living as his prideful, carnal nature dictates. The reality behind Satan is simply the dark evolutionary force of entropy that permeates all of nature and provides the drive for survival and propagation inherent in all living things. Satan is not a conscious entity to be worshiped, rather a reservoir of power inside each human to be tapped at will."
References.
</dl>

</doc>
<doc id="27695" url="http://en.wikipedia.org/wiki?curid=27695" title="Structured programming">
Structured programming

Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of subroutines, block structures and for and while loops—in contrast to using simple tests and jumps such as the "goto" statement which could lead to "spaghetti code" which is difficult both to follow and to maintain.
It emerged in the 1960s—particularly from a famous letter, "Go To Statement Considered Harmful".—
and was bolstered theoretically by the structured program theorem, and practically by the emergence of languages such as ALGOL with suitably rich control structures.
Elements.
Control structures.
Following the structured program theorem, all programs are seen as composed of three control structures:
Subroutines.
Subroutines; callable units such as procedures, functions, methods, or subprograms are used to allow a sequence to be referred to by a single statement.
Blocks.
Blocks are used to enable groups of statements to be treated as if they were one statement. "Block-structured" languages have a syntax for enclosing structures in some formal way, such as an if-statement bracketed by codice_6 as in ALGOL 68, or a code section bracketed by codice_7, as in PL/I, whitespace indentation as in Python - or the curly braces codice_8 of C and many later languages.
Structured programming languages.
It is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I and Ada – but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult.
"Structured programming" (sometimes known as modular programming) is a subset of procedural programming that enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.
History.
Theoretical foundation.
The structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore a processor is always executing a "structured program" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself. The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.
Debate.
P. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:
Donald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed (and still disagrees) with abolishing the GOTO statement. In his 1974 paper, "Structured Programming with Goto Statements", he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs.
Structured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for the "New York Times" research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.
As late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with a letter, "'GOTO considered harmful' considered harmful." Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.
Outcome.
By the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.
Common deviations.
While goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.
Early exit.
The most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a codice_9 statement. At the level of loops, this is a codice_10 statement (terminate the loop) or codice_11 statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or test, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have "labeled breaks", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.
Multiple exits can arise for a variety of reasons, most often either that the subroutine has no more work to do (if returning a value, it has completed the calculation), or has encountered "exceptional" circumstances that prevent it from continuing, hence needing exception handling.
The most common problem in early exit is that cleanup or final statements are not executed – for example, allocated memory is not unallocated, or open files are not closed, causing memory leaks or resource leaks. These must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action which should be performed at the end of a subroutine (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as standard Pascal don't have this problem.
Most modern languages provide language-level support to prevent such leaks; see detailed discussion at resource management. Most commonly this is done via unwind protection, which ensures that certain code is guaranteed to be run when execution exits a block; this is a structured alternative to having a cleanup block and a codice_12. This is most often known as codice_13 and considered a part of exception handling. Various techniques exist to encapsulate resource management. An alternative approach, found primarily in C++, is Resource Acquisition Is Initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.
Kent Beck, Martin Fowler and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that "one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don’t". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors). Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single-exit point is an obsolete requirement.
In his 2004 textbook, David Watt writes that "single-entry multi-exit control flows are often desirable". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are a bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as "escape sequencers", defined as "sequencer that terminates execution of a textually enclosing command or procedure", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce "spaghetti code". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.
In contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like codice_10 and codice_11 "are just the old codice_12 in sheep's clothing" and strongly advised against their use.
Exception handling.
Based on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and propose that all inter-procedural exceptions should be forbidden. In C++ syntax, this is done by declaring all function signatures as codice_17 Bonang proposes that all single-exit conforming C++ should be written along the lines of:
MSMVP Peter Ritchie also notes that, in principle, even a single codice_18 right before the codice_9 in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions which wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages which support exceptions of engaging in cargo cult thinking.
David Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic 
overflows or input/output failures like file not found) is a kind of error that "is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where "the application code tends to get cluttered by tests of status flags" and that "the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) aren't as suitable as a dedicated exception sequencer with the semantics discussed above.
The textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like codice_2 loops because the transfer of control "is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred." Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like codice_4, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if codice_22 throws an exception in codice_23, then the usual exit point after check() is not reached. Citing multiple prior studies by others (1999-2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they "create hidden control-flow paths that are difficult for programmers to reason about".:8:27
The necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like codice_24, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from codice_10 to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.
Multiple entry.
More rarely, subprograms allow multiple "entry." This is most commonly only "re"-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.
It is very rare for subprograms to allow entry to an arbitrary position in the subprogram, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is very similar to a goto.
State machines.
Some programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers (including Knuth) implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.
However, it is possible to structure these systems by making each state-change a separate subprogram and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.
References.
</dl>

</doc>
<doc id="27696" url="http://en.wikipedia.org/wiki?curid=27696" title="Semiconductor device fabrication">
Semiconductor device fabrication

Semiconductor device fabrication is the process used to create the integrated circuits that are present in everyday electrical and electronic devices. It is a multiple-step sequence of photo lithographic and chemical processing steps during which electronic circuits are gradually created on a wafer made of pure semiconducting material. Silicon is almost always used, but various compound semiconductors are used for specialized applications.
The entire manufacturing process, from start to packaged chips ready for shipment, takes six to eight weeks and is performed in highly specialized facilities referred to as fabs.
History.
When feature widths were far greater than about 10 micrometres, purity was not the issue that it is today in device manufacturing. As devices became more integrated, cleanrooms became even cleaner. Today, the fabs are pressurized with filtered air to remove even the smallest particles, which could come to rest on the wafers and contribute to defects. The workers in a semiconductor fabrication facility are required to wear cleanroom suits to protect the devices from human contamination.
Semiconductor device manufacturing has spread from Texas and California in the 1960s to the rest of the world, including Europe, the Middle East, and Asia. It is a global business today. The leading semiconductor manufacturers typically have facilities all over the world. Intel, the world's largest manufacturer, has facilities in Europe and Asia as well as the U.S. Other top manufacturers include Taiwan Semiconductor Manufacturing Company (Taiwan), United Microelectronics Corporation (Taiwan),
STMicroelectronics (Europe), Analog Devices (US), Integrated Device Technology (US), Atmel (US/Europe), Freescale Semiconductor (US), Samsung (Korea), Texas Instruments (US), IBM (US), GlobalFoundries (Germany, Singapore, US), Toshiba (Japan), NEC Electronics (Japan), Infineon (Europe, US, Asia), Renesas (Japan), Fujitsu (Japan/US), NXP Semiconductors (Europe and US), Micron Technology (US), Hynix (Korea), and SMIC (China).
Wafers.
A typical wafer is made out of extremely pure silicon that is grown into mono-crystalline cylindrical ingots (boules) up to 300 mm (slightly less than 12 inches) in diameter using the Czochralski process. These ingots are then sliced into wafers about 0.75 mm thick and polished to obtain a very regular and flat surface.
Processing.
In semiconductor device fabrication, the various processing steps fall into four general categories: deposition, removal, patterning, and modification of electrical properties.
Modern chips have up to eleven metal levels produced in over 300 sequenced processing steps.
Front-end-of-line (FEOL) processing.
FEOL processing refers to the formation of the transistors directly in the silicon. The raw wafer is engineered by the growth of an ultrapure, virtually defect-free silicon layer through epitaxy. In the most advanced logic devices, "prior" to the silicon epitaxy step, tricks are performed to improve the performance of the transistors to be built. One method involves introducing a "straining step" wherein a silicon variant such as silicon-germanium (SiGe) is deposited. Once the epitaxial silicon is deposited, the crystal lattice becomes stretched somewhat, resulting in improved electronic mobility. Another method, called "silicon on insulator" technology involves the insertion of an insulating layer between the raw silicon wafer and the thin layer of subsequent silicon epitaxy. This method results in the creation of transistors with reduced parasitic effects.
Gate oxide and implants.
Front-end surface engineering is followed by growth of the gate dielectric (traditionally silicon dioxide), patterning of the gate, patterning of the source and drain regions, and subsequent implantation or diffusion of dopants to obtain the desired complementary electrical properties. In dynamic random access memory (DRAM) devices, storage capacitors are also fabricated at this time, typically stacked above the access transistor (the now defunct DRAM manufacturer Qimonda implemented these capacitors with trenches etched deep into the silicon surface).
Back-end-of-line (BEOL) processing.
Metal layers.
Once the various semiconductor devices have been created, they must be interconnected to form the desired electrical circuits. This occurs in a series of wafer processing steps collectively referred to as BEOL (not to be confused with "back end" of chip fabrication, which refers to the packaging and testing stages). BEOL processing involves creating metal interconnecting wires that are isolated by dielectric layers. The insulating material has traditionally been a form of SiO2 or a silicate glass, but recently new low dielectric constant materials are being used (such as silicon oxycarbide), typically providing dielectric constants around 2.7 (compared to 3.9 for SiO2), although materials with constants as low as 2.2 are being offered to chipmakers.
Interconnect.
Historically, the metal wires have been composed of aluminium. In this approach to wiring (often called "subtractive aluminium"), blanket films of aluminium are deposited first, patterned, and then etched, leaving isolated wires. Dielectric material is then deposited over the exposed wires. The various metal layers are interconnected by etching holes (called ""vias")" in the insulating material and then depositing tungsten in them with a CVD technique; this approach is still used in the fabrication of many memory chips such as dynamic random access memory (DRAM), because the number of interconnect levels is small (currently no more than four).
More recently, as the number of interconnect levels for logic has substantially increased due to the large number of transistors that are now interconnected in a modern microprocessor, the timing delay in the wiring has become so significant as to prompt a change in wiring material (from aluminium to copper layer) and a change in dielectric material (from silicon dioxides to newer low-K insulators). This performance enhancement also comes at a reduced cost via damascene processing, which eliminates processing steps. As the number of interconnect levels increases, planarization of the previous layers is required to ensure a flat surface prior to subsequent lithography. Without it, the levels would become increasingly crooked, extending outside the depth of focus of available lithography, and thus interfering with the ability to pattern. CMP (chemical-mechanical planarization) is the primary processing method to achieve such planarization, although dry "etch back" is still sometimes employed when the number of interconnect levels is no more than three.
Wafer test.
The highly serialized nature of wafer processing has increased the demand for metrology in between the various processing steps. For example, thin film metrology based on ellipsometry or reflectometry, is used to tightly control the thickness of gate oxide, as well as the thickness, refractive index and extinction coefficient of photoresist and other coatings. Wafer test metrology equipment is used to verify that the wafers haven't been damaged by previous processing steps up until testing; if too many dies on one wafer have failed, the entire wafer is scrapped to avoid the costs of further processing. Virtual metrology has been used to predict waver properties based on statistical methods without performing the physical measurement itself.
Device test.
Once the front-end process has been completed, the semiconductor devices are subjected to a variety of electrical tests to determine if they function properly. The proportion of devices on the wafer found to perform properly is referred to as the yield. Manufacturers are typically secretive about their yields, but it can be as low as 30%.
The fab tests the chips on the wafer with an electronic tester that presses tiny probes against the chip. The machine marks each bad chip with a drop of dye. Currently, electronic dye marking is possible if wafer test data is logged into a central computer database and chips are "binned" (i.e. sorted into virtual bins) according to predetermined test limits. The resulting binning data can be graphed, or logged, on a wafer map to trace manufacturing defects and mark bad chips. This map can also be used during wafer assembly and packaging.
Chips are also tested again after packaging, as the bond wires may be missing, or analog performance may be altered by the package. This is referred to as the "final test".
Usually, the fab charges for testing time, with prices in the order of cents per second. Testing times vary from a few milliseconds to a couple of seconds, and the test software is optimized for reduced testing time. Multiple chip (multi-site) testing is also possible, because many testers have the resources to perform most or all of the tests in parallel.
Chips are often designed with "testability features" such as scan chains or a "built-in self-test" to speed testing, and reduce testing costs. In certain designs that use specialized analog fab processes, wafers are also laser-trimmed during the testing, in order to achieve tightly-distributed resistance values as specified by the design.
Good designs try to test and statistically manage "corners" (extremes of silicon behavior caused by a high operating temperature combined with the extremes of fab processing steps). Most designs cope with at least 64 corners.
Die preparation.
Once tested, a wafer is typically reduced in thickness before the wafer is scored and then broken into individual dies, a process known as wafer dicing. Only the good, unmarked chips are packaged.
Packaging.
Plastic or ceramic packaging involves mounting the die, connecting the die pads to the pins on the package, and sealing the die. Tiny wires are used to connect the pads to the pins. In the old days, wires were attached by hand, but now specialized machines perform the task. Traditionally, these wires have been composed of gold, leading to a lead frame (pronounced "leed frame") of solder-plated copper; lead is poisonous, so lead-free "lead frames" are now mandated by RoHS.
Chip scale package (CSP) is another packaging technology. A plastic dual in-line package, like most packages, is many times larger than the actual die hidden inside, whereas CSP chips are nearly the size of the die; a CSP can be constructed for each die "before" the wafer is diced.
The packaged chips are retested to ensure that they were not damaged during packaging and that the die-to-pin interconnect operation was performed correctly. A laser then etches the chip's name and numbers on the package.
List of steps.
This is a list of processing techniques that are employed numerous times throughout the construction of a modern electronic device; this list does not necessarily imply a specific order.
Hazardous materials.
Many toxic materials are used in the fabrication process. These include:
It is vital that workers not be directly exposed to these dangerous substances. The high degree of automation common in the IC fabrication industry helps to reduce the risks of exposure. Most fabrication facilities employ exhaust management systems, such as wet scrubbers, combustors, heated absorber cartridges, etc., to control the risk to workers and to the environment.

</doc>
<doc id="27698" url="http://en.wikipedia.org/wiki?curid=27698" title="Sanskrit">
Sanskrit

Sanskrit (; संस्कृतम् "saṃskṛtam" ], or संस्कृत "saṃskṛta", originally संस्कृता वाक् "saṃskṛtā vāk", "refined speech") is the primary liturgical language of Hinduism, a philosophical language in Hinduism, Buddhism, and Jainism, and a literary language that was in use as a "lingua franca" in the Indian cultural zone. It is a standardised dialect of the Old Indo-Aryan language, originating as Vedic Sanskrit and tracing its linguistic ancestry back to Proto-Indo-Iranian and Proto-Indo-European. Today it is listed as one of the 22 scheduled languages of India and is an official language of the state of Uttarakhand. Sanskrit holds a prominent position in Indo-European studies.
The corpus of Sanskrit literature encompasses a rich tradition of poetry and drama as well as scientific, technical, philosophical and religious texts. Sanskrit continues to be widely used as a ceremonial language in Hindu religious rituals and Buddhist practice in the form of hymns and chants. Spoken Sanskrit has been revived in some villages with traditional institutions, and there are attempts at enhance its popularisation.
Name.
The Sanskrit verbal adjective "sáṃskṛta-" may be translated as "put together, constructed, well or completely formed; refined, adorned, highly elaborated". It is derived from the root word "saṃ-skar-" "to put together, compose, arrange, prepare" (cf. Norwegian 'sammen skjær', Afrikaans 'saamskaar').
As a term for "refined or elaborated speech" the adjective appears only in Epic and Classical Sanskrit, in the Manusmriti and in the Mahabharata. The language referred to as "saṃskṛta" "the cultured language" has by definition always been a "sacred" and "sophisticated" language, used for religious and learned discourse in ancient India, in contrast to the language spoken by the people, "prākṛta-" "natural, artless, normal, ordinary".
Variants.
The pre-Classical form of Sanskrit is known as Vedic Sanskrit, with the language of the Rigveda being the oldest and most archaic stage preserved, dating back to as early as the early second millennium BCE. This qualifies Rigvedic Sanskrit as one of the oldest attestations of any Indo-Iranian language, and one of the earliest members of the Indo-European languages, which includes English and most European languages.
"Classical Sanskrit" is the standard register as laid out in the grammar of Pāṇini, around the fourth century BCE. Its position in the cultures of Greater India is akin to that of Latin and Greek in Europe and it has significantly influenced most modern languages of the Indian subcontinent, particularly in India, Bangladesh, Pakistan, Sri Lanka and Nepal.
Vedic Sanskrit.
Sanskrit, as defined by Pāṇini, evolved out of the earlier Vedic form. The present form of Vedic Sanskrit can be traced back to as early as the second millennium BCE (for Rig-vedic). Scholars often distinguish Vedic Sanskrit and Classical or "Pāṇinian" Sanskrit as separate dialects. Though they are quite similar, they differ in a number of essential points of phonology, vocabulary, grammar and syntax. Vedic Sanskrit is the language of the Vedas, a large collection of hymns, incantations (Samhitas) and theological and religio-philosophical discussions in the Brahmanas and Upanishads. Modern linguists consider the metrical hymns of the Rigveda Samhita to be the earliest, composed by many authors over several centuries of oral tradition. The end of the Vedic period is marked by the composition of the Upanishads, which form the concluding part of the traditional Vedic corpus; however, the early Sutras are Vedic, too, both in language and content.
Classical Sanskrit.
For nearly 2,000 years, a cultural order existed that exerted influence across South Asia, Inner Asia, Southeast Asia, and to a certain extent East Asia. A significant form of post-Vedic Sanskrit is found in the Sanskrit of the Hindu Epics—the Ramayana and Mahabharata. The deviations from Pāṇini in the epics are generally considered to be on account of interference from Prakrits, or innovations, and not because they are pre-Paninian. Traditional Sanskrit scholars call such deviations "ārṣa" (आर्ष), meaning 'of the ṛṣis', the traditional title for the ancient authors. In some contexts, there are also more "prakritisms" (borrowings from common speech) than in Classical Sanskrit proper. Buddhist Hybrid Sanskrit is a literary language heavily influenced by Middle Indic, based on early Buddhist Prakrit texts which subsequently assimilated to the Classical Sanskrit standard in varying degrees.
There were four principal dialects of classical Sanskrit: "paścimottarī" (Northwestern, also called Northern or Western), "madhyadeśī" (lit., middle country), "pūrvi" (Eastern) and "dakṣiṇī" (Southern, arose in the Classical period). The predecessors of the first three dialects are attested in Vedic "Brāhmaṇas", of which the first one was regarded as the purest ("Kauṣītaki Brāhmaṇa, 7.6).
Contemporary usage.
As a spoken language.
In the 2001 census of India, 14,135 people reported Sanskrit as their native language. Since the 1990s, movements to spread spoken Sanskrit have been increasing. Organisations like "Samskrita Bharati" conduct Speak Sanskrit workshops to popularise the language.
Indian newspapers have published reports about several villages, where, as a result of recent revival attempts, large parts of the population, including children, are learning Sanskrit and are even using it to some extent in everyday communication:
In official use.
In India, Sanskrit is among the 14 original languages of the Eighth Schedule to the Constitution. The state of Uttarakhand in India has ruled Sanskrit as its second official language. In October 2012 social activist Hemant Goswami filed a writ petition in the Punjab and Haryana High Court for declaring Sanskrit as a 'minority' language.
Contemporary literature and patronage.
More than 3000 Sanskrit works have been composed since India's independence in 1947. Much of this work has been judged of high quality, in comparison to both classical Sanskrit literature and modern literature in other Indian languages.
The Sahitya Akademi has given an award for the best creative work in Sanskrit every year since 1967. In 2009, Satyavrat Shastri became the first Sanskrit author to win the Jnanpith Award, India's highest literary award.
In music.
Sanskrit is used extensively in the Carnatic and Hindustani branches of classical music. Kirtanas, bhajans, stotras, and shlokas of Sanskrit are popular throughout India. The samaveda uses musical notations in several of its recessions.
In Mainland China, musicians such as Sa Dingding have written pop songs in Sanskrit.
In mass media.
Over 90 weeklies, fortnightlies and quarterlies are published in Sanskrit. Sudharma, a daily newspaper in Sanskrit, has been published out of Mysore, India, since the year 1970, while Sanskrit Vartman Patram and Vishwasya Vrittantam started in Gujarat during the last five years.
Since 1974, there has been a short daily news broadcast on state-run All India Radio. These broadcasts are also made available on the internet on AIR's website. Sanskrit news is broadcast on TV and on the internet through the DD National channel at 6:55 AM IST.
As a liturgical language.
Sanskrit is the liturgical language of various Hindu, Buddhist, and Jainist traditions. It is used during worship in Hindu temples throughout the world. In Newar Buddhism, it is used in all monasteries, while Mahayana and Tibetan Buddhist religious texts and sutras are in Sanskrit as well as vernacular languages. Jain texts, including the Tattvartha sutra, Ratnakaranda śrāvakācāra and Agamas, are written in Sanskrit.
It is also popular amongst the many practitioners of yoga in the West, who find the language helpful for understanding texts such as the Yoga Sutras.
Symbolic usage.
In Nepal, India and Indonesia, Sanskrit phrases are widely used as mottoes for various national, educational and social organisations:
Many of India's and Nepal's scientific and administrative terms are named in Sanskrit. The Indian guided missile program that was commenced in 1983 by DRDO has named the five missiles (ballistic and others) that it developed Prithvi, Agni, Akash, Nag and Trishul. India's first modern fighter aircraft is named HAL Tejas.
Historical usage.
Origin and development.
Sanskrit is a member of the Indo-Iranian subfamily of the Indo-European family of languages. Its closest ancient relatives are the Iranian languages Avestan and Old Persian.
In order to explain the common features shared by Sanskrit and other Indo-European languages, many scholars have proposed migration hypotheses asserting that the original speakers of what became Sanskrit arrived in what is now India and Pakistan from the north-west some time during the early second millennium BCE. Evidence for such a theory includes the close relationship between the Indo-Iranian tongues and the Baltic and Slavic languages, vocabulary exchange with the non-Indo-European Uralic languages, and the nature of the attested Indo-European words for flora and fauna.
The earliest attested Sanskrit texts are Brahmanical texts of the Rigveda, from the mid-to-late second millennium BCE. No written records from such an early period survive, if they ever existed. However, scholars are confident that the oral transmission of the texts is reliable: they were ceremonial literature whose correct pronunciation was considered crucial to its religious efficacy.
From the Rigveda until the time of Pāṇini (fourth century BCE) the development of the early Vedic language can be observed in other Vedic texts: the Samaveda, Yajurveda, Atharvaveda, Brahmanas, and Upanishads. During this time, the prestige of the language, its use for sacred purposes, and the importance attached to its correct enunciation all served as powerful conservative forces resisting the normal processes of linguistic change. However, there is a clear, five-level linguistic development of Vedic from the Rigveda to the language of the Upanishads and the earliest Sutras (such as Baudhayana).
Standardisation by Panini.
The oldest surviving Sanskrit grammar is Pāṇini's "Aṣṭādhyāyī" ("Eight-Chapter Grammar"). It is essentially a prescriptive grammar, i.e., an authority that defines Sanskrit, although it contains descriptive parts, mostly to account for some Vedic forms that had become rare in Pāṇini's time. Classical Sanskrit became fixed with the grammar of Pāṇini (roughly 500 BCE), and remains in use as a learned language through the present day.
Coexistence with vernacular languages.
The term "Sanskrit" was not thought of as a specific language set apart from other languages, but rather as a particularly refined or perfected manner of speaking. Knowledge of Sanskrit was a marker of social class and educational attainment in ancient India, and the language was taught mainly to members of the higher castes through the close analysis of Sanskrit grammarians such as Pāṇini and Patanjali, who exhorted proper Sanskrit at all times, especially during ritual. Sanskrit, as the learned language of Ancient India, thus existed alongside the vernacular Prakrits, also called Middle Indic dialects. However, linguistic change led to an eventual loss of mutual intelligibility.
Many Sanskrit dramas also indicate that the language coexisted with Prakrits, spoken by multilingual speakers with a more extensive education. Sanskrit speakers were almost always multilingual. In the medieval era, Sanskrit continued to be spoken and written, particularly by learned Brahmins for scholarly communication. This was a thin layer of Indian society, but covered a wide geography. Centres like Varanasi, Paithan, Pune, and Kanchipuram had a strong presence of teaching and debating institutions, and high classical Sanskrit was maintained until British times.
Decline.
There are a number of sociolinguistic studies of spoken Sanskrit which strongly suggest that oral use of modern Sanskrit is limited, having ceased development sometime in the past.
Sheldon Pollock argues that "most observers would agree that, in some crucial way, Sanskrit is dead".:393 Pollock has further argued that, while Sanskrit continued to be used in literary cultures in India, it was never adapted to express the changing forms of subjectivity and sociality as embodied and conceptualised in the modern age.:416 Instead, it was reduced to "reinscription and restatements" of ideas already explored, and any creativity was restricted to hymns and verses.:398 A notable exception are the military references of Nīlakaṇṭha Caturdhara's 17th-century commentary on the Mahābhārata.
Pollock's characterisation has been contested by other authors like Hanneder and Hatcher, who point out that modern works continue to be produced in Sanskrit. On a more public level the statement that Sanskrit is a dead language is misleading, for Sanskrit is quite obviously not as dead as other dead languages and the fact that it is spoken, written and read will probably convince most people that it cannot be a dead language in the most common usage of the term. Pollock's notion of the “death of Sanskrit” remains in this unclear realm between academia and public opinion when he says that “most observers would agree that, in some crucial way, Sanskrit is dead.”—Hanneder
Hanneder has also argued that modern works in Sanskrit are either ignored or their "modernity" contested.
When the British imposed a Western-style education system in India in the nineteenth century, knowledge of Sanskrit and ancient literature continued to flourish as the study of Sanskrit changed from a more traditional style into a form of analytical and comparative scholarship mirroring that of Europe.
Public education and popularisation.
Adult and continuing education.
Attempts at reviving the Sanskrit language have been undertaken in the Republic of India since its foundation in 1947 (it was included in the 14 original languages of the Eighth Schedule to the Constitution).
Samskrita Bharati is an organisation working for Sanskrit revival. The "All-India Sanskrit Festival" (since 2002) holds composition contests. The 1991 Indian census reported 49,736 fluent speakers of Sanskrit. Sanskrit learning programmes also feature on the lists of most AIR broadcasting centres. The Mattur village in central Karnataka claims to have native speakers of Sanskrit among its population. Inhabitants of all castes learn Sanskrit starting in childhood and converse in the language. Even the local Muslims converse in Sanskrit. Historically, the village was given by king Krishnadevaraya of the Vijayanagara Empire to Vedic scholars and their families, while people in his kingdom spoke Kannada and Telugu. Another effort concentrates on preserving and passing along the oral tradition of the Vedas. Shri Vedabharathi is one such organisation based out of Hyderabad that has been digitising the Vedas by recording recitations of Vedic Pandits.
School curricula.
The "CBSE" (Central Board of Secondary Education) of India, along with several other state education boards, has made Sanskrit an alternative option to the state's own official language as a second or third language choice in the schools it governs. In such schools, learning Sanskrit is an option for grades 5 to 8 (Classes V to VIII). This is true of most schools affiliated with the ICSE board, especially in those states where the official language is Hindi. Sanskrit is also taught in traditional gurukulas throughout India.
In the West.
St James Junior School in London, England, offers Sanskrit as part of the curriculum. In the United States, since September 2009, high school students have been able to receive credits as Independent Study or toward Foreign Language requirements by studying Sanskrit, as part of the "SAFL: Samskritam as a Foreign Language" program coordinated by Samskrita Bharati. In Australia, the Sydney private boys' high school Sydney Grammar School offers Sanskrit from years 7 through to 12, including for the Higher School Certificate.
Universities.
A list of Sanskrit universities is given below in chronological order:
Many universities throughout the world train and employ Sanskrit scholars, either within a separate Sanskrit department or as part of a broader focus area, such as South Asian studies or Linguistics. For example, Delhi university has about 400 Sanskrit students, about half of which are in post-graduate programmes.
European scholarship.
European scholarship in Sanskrit, begun by Heinrich Roth (1620–1668) and Johann Ernst Hanxleden (1681–1731), is considered responsible for the discovery of an Indo-European language family by Sir William Jones (1746–1794). This research played an important role in the development of Western philology, or historical linguistics.
Sir William Jones was one of the most influential philologists of his time. He told The Asiatic Society in Calcutta on 2 February 1786:
The Sanskrit language, whatever be its antiquity, is of a wonderful structure; more perfect than the Greek, more copious than the Latin, and more exquisitely refined than either, yet bearing to both of them a stronger affinity, both in the roots of verbs and in the forms of grammar, than could have been produced by accident; so strong, indeed, that no philologer could examine them all three, without believing them to have sprung from some common source, which, perhaps, no longer exists.
British attitudes.
Orientalist scholars of the 18th century like Sir William Jones marked a wave of enthusiasm for Indian culture and for Sanskrit. According to Thomas Trautmann, after this period of "Indomania", a certain hostility to Sanskrit and to Indian culture in general began to assert itself in early 19th century Britain, manifested by a neglect of Sanskrit in British academia. This was the beginning of a general push in favor of the idea that India should be culturally, religiously and linguistically assimilated to Britain as far as possible. Trautmann considers two separate and logically opposite sources for the growing hostility: one was "British Indophobia", which he calls essentially a developmentalist, progressivist, liberal, and non-racial-essentialist critique of Hindu civilisation as an aid for the improvement of India along European lines; the other was race science, a theory of the English "common-sense view" that Indians constituted a "separate, inferior and unimprovable race".
Phonology.
Classical Sanskrit distinguishes about 36 phonemes; the presence of allophony leads the writing systems to generally distinguish 48 phones, or sounds. The sounds are traditionally listed in the order vowels ("Ac"), diphthongs ("Hal"), anusvara and visarga, plosives (Sparśa), nasals, and finally the liquids and fricatives, written in IAST as follows:
Writing system.
Sanskrit originated in an oral society, and the oral tradition was maintained through the development of early classical Sanskrit literature. Writing was not introduced to India until after Sanskrit had evolved into the Prakrits; when it was written, the choice of writing system was influenced by the regional scripts of the scribes. Therefore, Sanskrit has no native script of its own. As such, virtually all the major writing systems of South Asia have been used for the production of Sanskrit manuscripts.
The earliest known inscriptions in Sanskrit date to the first century BCE. They are in the Brāhmī script, which was originally used for Prakrit, not Sanskrit. It has been described as a paradox that the first evidence of written Sanskrit occurs centuries later than that of the Prakrit languages which are its linguistic descendants. In northern India, there are Brāhmī inscriptions dating from the third century BCE onwards, the oldest appearing on the famous Prakrit pillar inscriptions of king Ashoka. The earliest South Indian inscriptions in Tamil Brahmi, written in early Tamil, belong to the same period. When Sanskrit was written down, it was first used for texts of an administrative, literary or scientific nature. The sacred texts were preserved orally, and were set down in writing "reluctantly" (according to one commentator), and at a comparatively late date.
Brahmi evolved into a multiplicity of scripts of the Brahmic family, many of which were used to write Sanskrit. Roughly contemporary with the Brahmi, the Kharosthi script was used in the north-west of the subcontinent. Sometime between the fourth and eighth centuries CE, the Gupta script, derived from Brahmi, became prevalent. Around the eighth century the Sharada script evolved out of the Gupta script. The latter was displaced in its turn by Devanagari in the 11th or 12th century, with intermediary stages such as the Siddham script. In Eastern India, the Bengali script and, later, the Oriya script, were used. In the south, where Dravidian languages predominate, scripts used for Sanskrit include Kannada, Telugu, Tamil, Malayalam and Grantha.
Romanisation.
Since the late 18th century, Sanskrit has been transliterated using the Latin alphabet. The system most commonly used today is the IAST (International Alphabet of Sanskrit Transliteration), which has been the academic standard since 1888. ASCII-based transliteration schemes have also evolved due to difficulties representing Sanskrit characters in computer systems. These include Harvard-Kyoto and ITRANS, a transliteration scheme that is used widely on the Internet, especially in Usenet and in email, for considerations of speed of entry as well as rendering issues. With the wide availability of Unicode-aware web browsers, IAST has become common online. It is also possible to type using an alphanumeric keyboard and transliterate to Devanagari using software like Mac OS X's international support.
European scholars in the 19th century generally preferred Devanagari for the transcription and reproduction of whole texts and lengthy excerpts. However, references to individual words and names in texts composed in European Languages were usually represented with Roman transliteration. From the 20th century onwards, due to production costs, textual editions edited by Western scholars have mostly been in Romanised transliteration.
Grammar.
Sanskrit grammatical tradition ("vyākaraṇa", one of the six Vedanga disciplines) began in late Vedic India and culminated in the Aṣṭādhyāyī of Pāṇini, which consists of 3990 sutras (ca. the fifth century BCE). About a century after Pāṇini (around 400 BCE) Kātyāyana composed Vārtikas on Pāṇinian sũtras. Patañjali, who lived three centuries after Pāṇini, wrote the Mahābhāṣya, the "Great Commentary" on the Aṣṭādhyāyī and Vārtikas. Because of these three ancient Sanskrit grammarians this grammar is called "Trimuni Vyākarana". To understand the meaning of sutras, Jayaditya and Vāmana wrote the commentary named Kāsikā in 600 CE. Pāṇinian grammar is based on 14 Shiva sutras (aphorisms), where the whole Mātrika (alphabet) is abbreviated. This abbreviation is called Pratyāhara.
Sanskrit verbs are categorized into ten classes, which can be conjugated to form the present, imperfect, imperative, optative, perfect, aorist, future, and conditional tenses. Before Classical Sanskrit, older forms also included a subjunctive mood. Each conjugational ending conveys person, number, and voice.
Nouns are highly inflected, including three grammatical genders, three numbers, and eight cases. Nominal compounds are common, and can include over 10 word stems.
Word order is free, though there is a strong tendency toward subject–object–verb, the original system of Vedic prose.
Influence on other languages.
Indic languages.
Sanskrit has greatly influenced the languages of India that grew from its vocabulary and grammatical base; for instance, Hindi is a "Sanskritised register" of the Khariboli dialect. All modern Indo-Aryan languages, as well as Munda and Dravidian languages, have borrowed many words either directly from Sanskrit ("tatsama" words), or indirectly via middle Indo-Aryan languages ("tadbhava" words). Words originating in Sanskrit are estimated at roughly fifty percent of the vocabulary of modern Indo-Aryan languages, as well as the literary forms of Malayalam and Kannada. Literary texts in Telugu are lexically Sanskrit or Sanskritised to an enormous extent, perhaps seventy percent or more.
Interaction with other languages.
Sanskrit has also influenced Sino-Tibetan languages through the spread of Buddhist texts in translation. Buddhism was spread to China by Mahayana missionaries sent by Emperor Ashoka, mostly through translations of Buddhist Hybrid Sanskrit. Many terms were transliterated directly and added to the Chinese vocabulary. Chinese words like 剎那 "chànà" (Devanagari: क्षण "kṣaṇa" 'instantaneous period') were borrowed from Sanskrit. Many Sanskrit texts survive only in Tibetan collections of commentaries to the Buddhist teachings Tanjur.
In Southeast Asia, languages such as Thai and Lao contain many loan words from Sanskrit, as do Khmer, and even Vietnamese to a lesser extent, through Sinified hybrid Sanskrit. For example, in Thai, the Rāvana, the emperor of Sri Lanka, is called 'Thosakanth', a derivation of his Sanskrit name 'Dashakanth' ("of ten necks").
Many Sanskrit loanwords are also found in Austronesian languages, such as Javanese, particularly the older form in which nearly half the vocabulary is borrowed. Other Austronesian languages, such as traditional Malay and modern Indonesian, also derive much of their vocabulary from Sanskrit, albeit to a lesser extent, with a larger proportion derived from Arabic. Similarly, Philippine languages such as Tagalog have some Sanskrit loanwords, although more are derived from Spanish. A Sanskrit loanword encountered in many Southeast Asian languages is the word "bhāṣā", or spoken language, which is used to refer to the names of many languages.
In popular culture.
"Satyagraha", an opera by Philip Glass, uses texts from the "Bhagavad Gita", sung in Sanskrit. The closing credits of "The Matrix Revolutions" has a prayer from the "Brihadaranyaka Upanishad". The song "Cyber-raga" from Madonna's album "Music" includes Sanskrit chants, and "Shanti/Ashtangi" from her 1998 album "Ray of Light", which won a Grammy, is the Ashtanga Vinyasa Yoga chant. The lyrics include the mantra Om shanti. Composer John Williams featured choirs singing in Sanskrit for "Indiana Jones and the Temple of Doom" and in "". The theme song of "Battlestar Galactica 2004" is the Gayatri Mantra, taken from the Rigveda. The lyrics of "The Child In Us" by Enigma also contains Sanskrit verses..
Computational linguistics.
Analysis of Sanskrit is similar to Semantic network theory and it may be suitable for Knowledge representation as well as an artificial language for computers.

</doc>
<doc id="27699" url="http://en.wikipedia.org/wiki?curid=27699" title="Sign language">
Sign language

A sign language (also signed language or simply signing) is a language which uses manual communication and body language to convey meaning, as opposed to acoustically conveyed sound patterns. This can involve simultaneously combining hand shapes, orientation and movement of the hands, arms or body, and facial expressions to fluidly express a speaker's thoughts. They share many similarities with spoken languages (sometimes called "oral languages", which depend primarily on sound), which is why linguists consider both to be natural languages, but there are also some significant differences between signed and spoken languages.
Wherever communities of deaf people exist, sign languages have been developed. Signing is not only used by the deaf, it is also used by people who can hear, but cannot physically speak. While they use space for grammar in a way that spoken languages do not, sign languages show the same linguistic properties and use the same language faculty as do spoken languages. Hundreds of sign languages are in use around the world and are at the cores of local deaf cultures. Some sign languages have obtained some form of legal recognition, while others have no status at all.
A common misconception is that all sign languages are the same worldwide or that sign language is international. Aside from the pidgin International Sign, each country generally has its own, native sign language, and some have more than one, though sign languages may share similarities to each other, whether in the same country or another one.
It is not clear how many sign languages there are. The 2013 edition of Ethnologue lists 137 sign languages.
History.
Groups of deaf people have used sign languages throughout history. One of the earliest written records of a sign language is from the fifth century BC, in Plato's "Cratylus", where Socrates says: "If we hadn't a voice or a tongue, and wanted to express things to one another, wouldn't we try to make signs by moving our hands, head, and the rest of our body, just as dumb people do at present?"
Until the 19th century, most of what we know about historical sign languages is limited to the manual alphabets (fingerspelling systems) that were invented to facilitate transfer of words from a spoken to a signed language, rather than documentation of the rest of the language.
In 1620, Juan Pablo Bonet published "Reducción de las letras y arte para enseñar a hablar a los mudos" (‘Reduction of letters and art for teaching mute people to speak’) in Madrid. It is considered the first modern treatise of sign language phonetics, setting out a method of oral education for deaf people and a manual alphabet.
In Britain, manual alphabets were also in use for a number of purposes, such as secret communication, public speaking, or communication by deaf people. In 1648, John Bulwer described "Master Babington", a deaf man proficient in the use of a manual alphabet, "contryved on the joynts of his fingers", whose wife could converse with him easily, even in the dark through the use of tactile signing.
In 1680, George Dalgarno published "Didascalocophus, or, The deaf and dumb mans tutor", in which he presented his own method of deaf education, including an "arthrological" alphabet, where letters are indicated by pointing to different joints of the fingers and palm of the left hand. Arthrological systems had been in use by hearing people for some time; some have speculated that they can be traced to early Ogham manual alphabets.
The vowels of this alphabet have survived in the contemporary alphabets used in British Sign Language, Auslan and New Zealand Sign Language. The earliest known printed pictures of consonants of the modern two-handed alphabet appeared in 1698 with "Digiti Lingua", a pamphlet by an anonymous author who was himself unable to speak. He suggested that the manual alphabet could also be used by mutes, for silence and secrecy, or purely for entertainment. Nine of its letters can be traced to earlier alphabets, and 17 letters of the modern two-handed alphabet can be found among the two sets of 26 handshapes depicted.
Charles de La Fin published a book in 1692 describing an alphabetic system where pointing to a body part represented the first letter of the part (e.g. Brow=B), and vowels were located on the fingertips as with the other British systems. He described codes for both English and Latin.
By 1720, the British manual alphabet had found more or less its present form. Descendants of this alphabet have been used by deaf communities (or at least in classrooms) in former British colonies India, Australia, New Zealand, Uganda and South Africa, as well as the republics and provinces of the former Yugoslavia, Grand Cayman Island in the Caribbean, Indonesia, Norway, Germany and the USA.
Frenchman Charles-Michel de l'Épée published his manual alphabet in the 18th century, which has survived basically unchanged in France and North America until the present time. In 1755, Abbé de l'Épée founded the first school for deaf children in Paris; Laurent Clerc was arguably its most famous graduate. Clerc went to the United States with Thomas Hopkins Gallaudet to found the American School for the Deaf in Hartford, Connecticut, in 1817. Gallaudet's son, Edward Miner Gallaudet founded a school for the deaf in 1857 in Washington, D.C., which in 1864 became the National Deaf-Mute College. Now called Gallaudet University, it is still the only liberal arts university for deaf people in the world.
Sign languages generally do not have any linguistic relation to the spoken languages of the lands in which they arise. The correlation between sign and spoken languages is complex and varies depending on the country more than the spoken language. For example, the US, Canada, UK, Australia and New Zealand all have English as their dominant language, but American Sign Language (ASL), used in the US and most parts of Canada, is derived from French Sign Language whereas the other three countries sign dialects of British, Australian and New Zealand Sign Language. Similarly, the sign languages of Spain and Mexico are very different, despite Spanish being the national language in each country, and the sign language used in Bolivia is based on ASL rather than any sign language that is used in a Spanish-speaking country. Variations also arise within a 'national' sign language which don't necessarily correspond to dialect differences in the national spoken language; rather, they can usually be correlated to the geographic location of residential schools for the deaf.
International Sign, formerly known as Gestuno, is used mainly at international Deaf events such as the Deaflympics and meetings of the World Federation of the Deaf. Recent studies claim that while International Sign is a kind of a pidgin, they conclude that it is more complex than a typical pidgin and indeed is more like a full sign language.
Linguistics.
In linguistic terms, sign languages are as rich and complex as any spoken language, despite the common misconception that they are not "real languages". Professional linguists have studied many sign languages and found that they exhibit the fundamental properties that exist in all languages.
Sign languages are not mime—in other words, signs are conventional, often arbitrary and do not necessarily have a visual relationship to their referent, much as most spoken language is not onomatopoeic. While iconicity is more systematic and widespread in sign languages than in spoken ones, the difference is not categorical. The visual modality allows the human preference for close connections between form and meaning, present but suppressed in spoken languages, to be more fully expressed. This does not mean that sign languages are a visual rendition of a spoken language. They have complex grammars of their own, and can be used to discuss any topic, from the simple and concrete to the lofty and abstract.
Sign languages, like spoken languages, organize elementary, meaningless units (phonemes; once called cheremes in the case of sign languages) into meaningful semantic units. Like in spoken languages, these meaningless units are represented as (combinations of) features, although often also crude distinctions are made in terms of Handshape (or Handform), Orientation, Location (or Place of Articulation), Movement, and Non-manual expression.
More generally, both sign and spoken languages share the following common features that linguists have found in all natural human languages.
1) Mode of communication 2) Semanticity 3) Pragmatic function 4) Interchangeability 5) Cultural Transmission 6) Arbitrariness 7) Discreteness 8) Displacement 9) Productivity
These nine features serve to define the notion "language".
Common linguistic features of many sign languages are the occurrence of classifiers, a high degree of inflection, and a topic-comment syntax. More than spoken languages, sign languages can convey meaning by simultaneous means, e.g. by the use of space, two manual articulators, and the signer's face and body.
Though there is still much discussion on the topic of iconicity in sign languages, classifiers are generally perceived to be highly iconic, as these complex constructions "function as predicates that may express any or all of the following: motion, position, stative-descriptive, or handling information". It needs to be noted that the term classifier is not used by everyone working on these constructions. Across the field of sign language linguistics the same constructions are also referred with other terms.
Today, linguists study sign languages as true languages, part of the field of linguistics. However, the category "Sign languages" was not added to the "Linguistic Bibliography / Bibliographie Linguistique" until the 1988 volume, when it appeared with 39 entries.
Relationships with spoken languages.
A common misconception is that sign languages are somehow dependent on spoken languages: that they are spoken language expressed in signs, or that they were invented by hearing people. Hearing teachers in deaf schools, such as Charles-Michel de l'Épée or Thomas Hopkins Gallaudet, are often incorrectly referred to as “inventors” of sign language. Instead, sign languages, like all natural languages, are developed by the people who use them, in this case, Deaf people, who may have little or no knowledge of any spoken language. 
As a sign language develops, it sometimes borrows elements from spoken languages, just as all languages borrow from other languages that they are in contact with. Sign languages vary in how and how much they borrow from spoken languages. In many sign languages, a manual alphabet (fingerspelling) may be used in signed communication to borrow a word from a spoken language, by spelling out the letters. This is most commonly-used for proper names of people and places; it is also used in some languages for concepts for which no sign is available at that moment, particularly if the people involved are to some extent bilingual in the spoken language. Fingerspelling can sometimes be a source of new signs, such as initialized signs, in which the handshape represents the first letter of a spoken word with the same meaning.
On the whole, though, sign languages are independent of spoken languages and follow their own paths of development. For example, British Sign Language and American Sign Language (ASL) are quite different and mutually unintelligible, even though the hearing people of Britain and America share the same spoken language. The grammars of sign languages do not usually resemble that of spoken languages used in the same geographical area; in fact, in terms of syntax, ASL shares more with spoken Japanese than it does with English.
Similarly, countries which use a single spoken language throughout may have two or more sign languages, or an area that contains more than one spoken language might use only one sign language. South Africa, which has 11 official spoken languages and a similar number of other widely used spoken languages, is a good example of this. It has only one sign language with two variants due to its history of having two major educational institutions for the deaf which have served different geographic areas of the country.
Spatial grammar and simultaneity.
Sign languages exploit the unique features of the visual medium (sight), but may also exploit tactile features (tactile sign languages). Spoken language is by and large linear; only one sound can be made or received at a time. Sign language, on the other hand, is visual and, hence, can use simultaneous expression, although this is limited articulatorily and linguistically. Visual perception allows processing of simultaneous information.
One way in which many sign languages take advantage of the spatial nature of the language is through the use of classifiers. Classifiers allow a signer to spatially show a referent's type, size, shape, movement, or extent.
The large focus on the possibility of simultaneity in sign languages in contrast to spoken languages is sometimes exaggerated, though. The use of two manual articulators is subject to motor constraints, resulting in a large extent of symmetry or signing with one articulator only.
Non-manual signs.
Sign languages convey much of their prosody through non-manual signs. Postures or movements of the body, head, eyebrows, eyes, cheeks, and mouth are used in various combinations to show several categories of information, including lexical distinction, grammatical structure, adjectival or adverbial content, and discourse functions.
In ASL (American Sign Language), some signs have required facial components that distinguish them from other signs. An example of this sort of lexical distinction is the sign translated 'not yet', which requires that the tongue touch the lower lip and that the head rotate from side to side, in addition to the manual part of the sign. Without these features it would be interpreted as 'late'.
Grammatical structure that is shown through non-manual signs includes questions, negation, relative clauses, boundaries between sentences, and the argument structure of some verbs. ASL and BSL use similar non-manual marking for yes/no questions, for example. They are shown through raised eyebrows and a forward head tilt.
Some adjectival and adverbial information is conveyed through non-manual signs, but what these signs are varies from language to language. For instance, in ASL a slightly open mouth with the tongue relaxed and visible in the corner of the mouth means 'carelessly,' but a similar sign in BSL means 'boring' or 'unpleasant.'
Discourse functions such as turn taking are largely regulated through head movement and eye gaze. Since the addressee in a signed conversation must be watching the signer, a signer can avoid letting the other person have a turn by not looking at them, or can indicate that the other person may have a turn by making eye contact.
Iconicity.
The first studies on iconicity in ASL were published in the late 1970s, and early 1980s. Many early sign language linguists rejected the notion that iconicity was an important aspect of the language. Though they recognized that certain aspects of the language seemed iconic, they considered this to be merely extralinguistic, a property which did not influence the language. Frishberg (1975) wrote a very influential paper addressing the relationship between arbitrariness and iconicity in ASL. She concluded that though originally present in many signs, iconicity is degraded over time through the application of grammatical processes. In other words, over time, the natural processes of regularization in the language obscures any iconically motivated features of the sign.
Some researchers have suggested that the properties of ASL give it a clear advantage in terms of learning and memory. Psychologist Roger Brown was one of the first to document this benefit. In his study, Brown found that when children were taught signs that had high levels of iconic mapping they were significantly more likely to recall the signs in a later memory task than when they were taught signs that had little or no iconic properties.
The pioneers of sign language linguistics were yoked with the task of trying to prove that ASL was a real language and not merely a collection of gestures or “English on the hands.” One of the prevailing beliefs at this time was that ‘real languages’ must consist of an arbitrary relationship between form and meaning. Thus, if ASL consisted of signs that had iconic form-meaning relationship, it could not be considered a real language. As a result, iconicity as a whole was largely neglected in research of sign languages.
The cognitive linguistics perspective rejects a more traditional definition of iconicity as a relationship between linguistic form and a concrete, real-world referent. Rather it is a set of selected correspondences between the form and meaning of a sign. In this view, iconicity is grounded in a language user’s mental representation (“construal” in Cognitive Grammar). It is defined as a fully grammatical and central aspect of a sign language rather than periphery phenomena.
The cognitive linguistics perspective allows for some signs to be fully iconic or partially iconic given the number of correspondences between the possible parameters of form and meaning. In this way, the Israeli Sign Language (ISL) sign for ASK has parts of its form that are iconic (“movement away from the mouth” means “something coming from the mouth”), and parts that are arbitrary (the handshape, and the orientation).
Many signs have metaphoric mappings as well as iconic or metonymic ones. For these signs there are three way correspondences between a form, a concrete source and an abstract target meaning. The ASL sign LEARN has this three way correspondence. The abstract target meaning is “learning.” The concrete source is putting objects into the head from books. The form is a grasping hand moving from an open palm to the forehead. The iconic correspondence is between form and concrete source. The metaphorical correspondence is between concrete source and abstract target meaning. Because the concrete source is connected to two correspondences linguistics refer to metaphorical signs as “double mapped.”
Classification.
Although sign languages have emerged naturally in deaf communities alongside or among spoken languages, they are unrelated to spoken languages and have different grammatical structures at their core.
Sign languages may be classified by how they arise.
In non-signing communities, home sign is not a full language, but closer to a pidgin. Home sign is amorphous and generally idiosyncratic to a particular family, where a deaf child does not have contact with other deaf children and is not educated in sign. Such systems are not generally passed on from one generation to the next. Where they are passed on, creolization would be expected to occur, resulting in a full language. However, home sign may also be closer to full language in communities where the hearing population has a gestural mode of language; examples include various Australian Aboriginal sign languages and gestural systems across West Africa, such as Mofu-Gudur in Cameroon.
A village sign language is a local indigenous language that typically arises over several generations in a relatively insular community with a high incidence of deafness, and is used both by the deaf and by a significant portion of the hearing community, who have deaf family and friends. The most famous of these is probably Martha's Vineyard Sign Language of the US, but there are also numerous village languages scattered throughout Africa, Asia, and America.
Deaf-community sign languages, on the other hand, arise where deaf people come together to form their own communities. These include school sign, such as Nicaraguan Sign Language, which develop in the student bodies of deaf schools which do not use sign as a language of instruction, as well as community languages such as Bamako Sign Language, which arise where generally uneducated deaf people congregate in urban centers for employment. At first, Deaf-community sign languages are not generally known by the hearing population, in many cases not even by close family members. However, they may grow, in some cases becoming a language of instruction and receiving official recognition, as in the case of ASL.
Both contrast with speech-taboo languages such as the various Aboriginal Australian sign languages, which are developed by the hearing community and only used secondarily by the deaf. It is doubtful whether most of these are languages in their own right, rather than manual codes of spoken languages, though a few such as Yolngu Sign Language are independent of any particular oral language. Hearing people may also develop sign to communicate with speakers of other languages, as in Plains Indian Sign Language; this was a contact signing system or pidgin that was evidently not used by deaf people in the Plains nations, though it presumably influenced home sign.
Language contact and creolization is common in the development of sign languages, making clear family classifications difficult – it is often unclear whether lexical similarity is due to borrowing or a common parent language, or whether there was one or several parent languages, such as several village languages merging into a Deaf-community language. Contact occurs between sign languages, between sign and spoken languages (contact sign, a kind of pidgin), and between sign languages and gestural systems used by the broader community. One author has speculated that Adamorobe Sign Language, a village sign language of Ghana, may be related to the "gestural trade jargon used in the markets throughout West Africa", in vocabulary and areal features including prosody and phonetics.
The only comprehensive classification along these lines going beyond a simple listing of languages dates back to 1991. The classification is based on the 69 sign languages from the 1988 edition of Ethnologue that were known at the time of the 1989 conference on sign languages in Montreal and 11 more languages the author added after the conference.
In his classification, the author distinguishes between primary and auxiliary sign languages as well as between single languages and names that are thought to refer to more than one language. The prototype-A class of languages includes all those sign languages that seemingly cannot be derived from any other language. Prototype-R languages are languages that are remotely modelled on a prototype-A language (in many cases thought to have been French Sign Language) by a process Kroeber (1940) called "stimulus diffusion". The families of BSL, DGS, JSL, LSF (and possibly LSG) were the products of creolization and relexification of prototype languages. Creolization is seen as enriching overt morphology in sign languages, as compared to reducing overt morphology in spoken languages.
Typology.
Linguistic typology (going back on Edward Sapir) is based on word structure and distinguishes morphological classes such as agglutinating/concatenating, inflectional, polysynthetic, incorporating, and isolating ones.
Sign languages vary in word-order typology as there are different word orders in different languages. For example, Austrian Sign Language, Japanese Sign Language and so-called Indo-Pakistani Sign Language are Subject-Object-Verb while ASL is Subject-Verb-Object. Influence from the surrounding spoken languages is not improbable.
Sign languages tend to be incorporating classifier languages, where a classifier handshape representing the object is incorporated into those transitive verbs which allow such modification. For a similar group of intransitive verbs (especially motion verbs), it is the subject which is incorporated. Only in a very few sign languages (for instance Japanese Sign Language) are agents ever incorporated. in this way, since subjects of intransitives are treated similarly to objects of transitives, incorporation in sign languages can be said to follow an ergative pattern.
Brentari classifies sign languages as a whole group determined by the medium of communication (visual instead of auditory) as one group with the features monosyllabic and polymorphemic. That means, that via one syllable (i.e. one word, one sign) several morphemes can be expressed, like subject and object of a verb determine the direction of the verb's movement (inflection).
Another aspect of typology that has been studied in sign languages is their systems for cardinal numbers. Typologically significant differences have been found between sign languages.
Acquisition.
Children who are exposed to a sign language from birth will acquire it, just as hearing children acquire their native spoken language.
The acquisition of non-manual features follows an interesting pattern: When a word that always has a particular non-manual feature associated with it (such as a wh- question word) is learned, the non-manual aspects are attached to the word but don’t have the flexibility associated with adult use. At a certain point the non-manual features are dropped and the word is produced with no facial expression. After a few months the non-manuals reappear, this time being used the way adult signers would use them.
Written forms.
Sign languages do not have a traditional or formal written form. Many deaf people do not see a need to write their own language.
Several ways to represent sign languages in written form have been developed.
So far, there is no formal acceptance of any of these writing systems for any sign language, or even any consensus on the matter. None are widely used.
Sign perception.
For a native signer, sign perception influences how the mind makes sense of their visual language experience. For example, a handshape may vary based on the other signs made before or after it, but these variations are arranged in perceptual categories during its development. The mind detects handshape contrasts but groups similar handshapes together in one category. Different handshapes are stored in other categories. The mind ignores some of the similarities between different perceptual categories, at the same time preserving the visual information within each perceptual category of handshape variation.
In society.
Deaf communities and Deaf culture.
Deaf communities are very widespread in the world, and the cultures within them are very rich. Sometimes they do not even intersect with the culture of the hearing population because of the communication difficulties caused by the impediments for hard-of-hearing people to perceive aurally conveyed information.
Legal recognition.
Some sign languages have obtained some form of legal recognition, while others have no status at all. Sarah Batterbury has argued that sign languages should be recognized and supported not merely as an accommodation for the disabled, but as the communication medium of language communities.
Telecommunications.
One of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when AT&T's videophone (trademarked as the "Picturephone") was introduced to the public at the 1964 New York World's Fair – two deaf users were able to freely communicate with each other between the fair and another city. However, video communication did not become widely available until sufficient bandwidth for the high volume of video data became available in the early 2000s.
The Internet now allows deaf people to talk via a video link, either with a special-purpose videophone designed for use with sign language or with "off-the-shelf" video services designed for use with broadband and an ordinary computer webcam. The special videophones that are designed for sign language communication may provide better quality than 'off-the-shelf' services and may use data compression methods specifically designed to maximize the intelligibility of sign languages. Some advanced equipment enables a person to remotely control the other person's video camera, in order to zoom in and out or to point the camera better to understand the signing.
Interpretation.
In order to facilitate communication between deaf and hearing people, sign language interpreters are often used. Such activities involve considerable effort on the part of the interpreter, since sign languages are distinct natural languages with their own syntax, different from any spoken language.
The interpretation flow is normally between a sign language and a spoken language that are customarily used in the same country, such as French Sign Language (LSF) to spoken French in France, Spanish Sign Language (LSE) to spoken Spanish in Spain, British Sign Language (BSL) to spoken English in the U.K., and American Sign Language (ASL) to spoken English in the USA and most of anglophone Canada (since BSL and ASL are distinct sign languages both used in English-speaking countries), etc. Sign language interpreters who can translate between signed and spoken languages that are not normally paired (such as between LSE and English), are also available, albeit less frequently.
Remote interpreting.
Interpreters may be physically present with both parties to the conversation, but since the technological advancements in the early 2000s, provision of interpreters in remote locations has become available. In Video Remote Interpreting (VRI), the two clients (a sign-language user and a hearing person who wish to communicate with each other) are in one location, and the interpreter is in another. The interpreter communicates with the sign-language user via a video telecommunications link, and with the hearing person by an audio link. VRI can be used for situations in which no on-site interpreters are available.
However, VRI cannot be used for situations in which all parties are speaking via telephone alone. In Video Relay Service (VRS), the sign-language user, the interpreter, and the hearing person are in three separate locations, thus allowing the two clients to talk to each other on the phone through the interpreter.
Interpretation on television.
Sign language is sometimes provided for television programmes. The signer usually appears in the bottom corner of the screen, with the programme being broadcast full size or slightly shrunk away from that corner.
Paddy Ladd initiated deaf programming on British television in the 1980s and is credited with getting sign language on television and enabling deaf children to be educated in sign. 
In traditional analogue broadcasting, many programmes are repeated, often in the early hours of the morning, with the signer present rather than have them appear at the main broadcast time. This is due to the distraction they cause to those not wishing to see the signer. On the BBC, many programmes that broadcast late at night or early in the morning are signed. Some emerging television technologies allow the viewer to turn the signer on and off in a similar manner to subtitles and closed captioning.
Legal requirements covering sign language on television vary from country to country. In the United Kingdom, the Broadcasting Act 1996 addressed the requirements for blind and deaf viewers, but has since been replaced by the Communications Act 2003.
Use of sign languages in hearing communities.
On occasion, where the prevalence of deaf people is high enough, a deaf sign language has been taken up by an entire local community. Famous examples of this include Martha's Vineyard Sign Language in the USA, Kata Kolok in a village in Bali, Adamorobe Sign Language in Ghana and Yucatec Maya sign language in Mexico. In such communities deaf people are not socially disadvantaged.
Many Australian Aboriginal sign languages arose in a context of extensive speech taboos, such as during mourning and initiation rites. They are or were especially highly developed among the Warlpiri, Warumungu, Dieri, Kaytetye, Arrernte, and Warlmanpa, and are based on their respective spoken languages.
A pidgin sign language arose among tribes of American Indians in the Great Plains region of North America (see Plains Indian Sign Language). It was used to communicate among tribes with different spoken languages. There are especially users today among the Crow, Cheyenne, and Arapaho. Unlike other sign languages developed by hearing people, it shares the spatial grammar of deaf sign languages.
In the 1500s, a Spanish expeditionary, Cabeza de Vaca, observed natives in the western part of modern day Florida using sign language, and in the mid-16th century Coronado mentioned that communication with the Tonkawa using signs was possible without a translator. Whether or not these gesture systems reached the stage at which they could properly be called languages is still up for debate. There are estimates indicating that as many as 2% of Native Americans are seriously or completely deaf, a rate more than twice the national average.
Signs may also be used for manual communication in noisy or secret situations, such as hunting.
"Baby sign language" with hearing children.
It has become popular for hearing parents to teach signs (from ASL or some other sign language) to young hearing children. Since the muscles in babies' hands grow and develop quicker than their mouths, signs can be a beneficial option for better communication. Babies can usually produce signs before they can speak. This decreases the confusion between parents when trying to figure out what their child wants. When the child begins to speak, signing is usually abandoned, so the child does not progress to acquiring the grammar of the sign language.
This is in contrast to hearing children who grow up with Deaf parents, who generally acquire the full sign language natively, the same as Deaf children of Deaf parents.
Home sign.
Sign systems are sometimes developed within a single family. For instance, when hearing parents with no sign language skills have a deaf child, an informal system of signs will naturally develop, unless repressed by the parents. The term for these mini-languages is home sign (sometimes homesign or kitchen sign).
Home sign arises due to the absence of any other way to communicate. Within the span of a single lifetime and without the support or feedback of a community, the child naturally invents signals to facilitate the meeting of his or her communication needs. Although this kind of system is grossly inadequate for the intellectual development of a child and it comes nowhere near meeting the standards linguists use to describe a complete language, it is a common occurrence. No type of Home Sign is recognized as an official language.
Gestural theory of human language origins.
The gestural theory states that vocal human language developed from a gestural sign language. An important question for gestural theory is what caused the shift to vocalization.
Primate use.
There have been several notable examples of scientists teaching non-human primates basic signs in order to communicate with humans, but the degree to which these basic signs relate to human sign language and the ability of the animals in question to actually communicate is a matter of substantial controversy and dispute. Notable examples include:
External links.
"Note: the articles for specific sign languages (e.g. ASL or BSL) may contain further external links, e.g. for learning those languages."

</doc>
<doc id="27701" url="http://en.wikipedia.org/wiki?curid=27701" title="String (computer science)">
String (computer science)

In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally understood as a data type and is often implemented as an array of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. A string may also denote more general arrays or other sequence (or list) data types and structures.
Depending on programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold variable number of elements.
When a string appears literally in source code, it is known as a string literal or an anonymous string.
In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.
Formal theory.
Let Σ be a non-empty finite set of symbols (alternatively called characters), called the "alphabet". No assumption is made about the nature of the symbols. A string (or word) over Σ is any finite sequence of symbols from Σ. For example, if Σ = {0, 1}, then "01011" is a string over Σ.
The "length" of a string "s" is the number of symbols in "s" (the length of the sequence) and can be any non-negative integer; it is often denoted as |"s"|. The "empty string" is the unique string over Σ of length 0, and is denoted "ε" or "λ".
The set of all strings over Σ of length "n" is denoted Σ"n". For example, if Σ = {0, 1}, then Σ2 = {00, 01, 10, 11}. Note that Σ0 = {ε} for any alphabet Σ.
The set of all strings over Σ of any length is the Kleene closure of Σ and is denoted Σ*. In terms of Σ"n",
For example, if Σ = {0, 1}, then Σ* = {ε, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, ...}. Although the set Σ* itself is countably infinite, each element of Σ* is a string of finite length.
A set of strings over Σ (i.e. any subset of Σ*) is called a "formal language" over Σ. For example, if Σ = {0, 1}, the set of strings with an even number of zeros, {ε, 1, 00, 11, 001, 010, 100, 111, 0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111, ...}, is a formal language over Σ.
Concatenation and substrings.
"Concatenation" is an important binary operation on Σ*. For any two strings "s" and "t" in Σ*, their concatenation is defined as the sequence of symbols in "s" followed by the sequence of characters in "t", and is denoted "st". For example, if Σ = {a, b, ..., z}, "s" = bear, and "t" = hug, then "st" = bearhug and "ts" = hugbear.
String concatenation is an associative, but non-commutative operation. The empty string ε serves as the identity element; for any string "s", ε"s" = "s"ε = "s". Therefore, the set Σ* and the concatenation operation form a monoid, the free monoid generated by Σ. In addition, the length function defines a monoid homomorphism from Σ* to the non-negative integers (that is, a function formula_2, such that formula_3).
A string "s" is said to be a "substring" or "factor" of "t" if there exist (possibly empty) strings "u" and "v" such that "t" = "usv". The relation "is a substring of" defines a partial order on Σ*, the least element of which is the empty string.
Prefixes and suffixes.
A string "s" is said to be a prefix of "t" if there exists a string "u" such that "t" = "su". If "u" is nonempty, "s" is said to be a "proper" prefix of "t". Symmetrically, a string "s" is said to be a suffix of "t" if there exists a string "u" such that "t" = "us". If "u" is nonempty, "s" is said to be a "proper" suffix of "t". Suffixes and prefixes are substrings of "t". Both the relations "is a prefix of" and "is a suffix of" are prefix orders.
Rotations.
A string "s" = "uv" is said to be a rotation of "t" if "t" = "vu". For example, if Σ = {0, 1} the string 0011001 is a rotation of 0100110, where u = 00110 and v = 01.
Reversal.
The reverse of a string is a string with the same symbols but in reverse order. For example, if "s" = abc (where a, b, and c are symbols of the alphabet), then the reverse of "s" is cba. A string that is the reverse of itself (e.g., "s" = madam) is called a palindrome, which also includes the empty string and all strings of length 1.
Lexicographical ordering.
It is often useful to define an ordering on a set of strings. If the alphabet Σ has a total order (cf. alphabetical order) one can define a total order on Σ* called lexicographical order. For example, if Σ = {0, 1} and 0 < 1, then the lexicographical order on Σ* includes the relationships ε < 0 < 00 < 000 < ... < 0001 < 001 < 01 < 010 < 011 < 0110 < 01111 < 1 < 10 < 100 < 101 < 111 < 1111 < 11111 ... The lexicographical order is total if the alphabetical order is, but isn't well-founded for any nontrivial alphabet, even if the alphabetical order is.
See Shortlex for an alternative string ordering that preserves well-foundedness.
String operations.
A number of additional operations on strings commonly occur in the formal theory. These are given in the article on string operations.
Topology.
Strings admit the following interpretation as nodes on a graph:
The natural topology on the set of fixed-length strings or variable length strings is the discrete topology, but the natural topology on the set of infinite strings is the limit topology, viewing the set of infinite strings as the inverse limit of the sets of finite strings. This is the construction used for the "p"-adic numbers and some constructions of the Cantor set, and yields the same topology.
Isomorphisms between string representations of topologies can be found by normalizing according to the lexicographically minimal string rotation.
String datatypes.
A string datatype is a datatype modeled on the idea of a formal string. Strings are such an important and useful datatype that they are implemented in nearly every programming language. In some languages they are available as primitive types and in others as composite types. The syntax of most high-level programming languages allows for a string, usually quoted in some way, to represent an instance of a string datatype; such a meta-string is called a "literal" or "string literal".
String length.
Although formal strings can have an arbitrary (but finite) length, the length of strings in real languages is often constrained to an artificial maximum. In general, there are two types of string datatypes: "fixed-length strings", which have a fixed maximum length to be determined at compile time and which use the same amount of memory whether this maximum is needed or not, and "variable-length strings", whose length is not arbitrarily fixed and which can use varying amounts of memory depending on the actual requirements at run time. Most strings in modern programming languages are variable-length strings. Of course, even variable-length strings are limited in length – theoretically by the number of bits available to a pointer, practically by the current size of memory. The string length can be stored as a separate integer (which may put an artificial limit on the length) or implicitly through a termination character, usually a character value with all bits zero. See also "Null-terminated" below.
Character encoding.
String datatypes have historically allocated one byte per character, and, although the exact character set varied by region, character encodings were similar enough that programmers could often get away with ignoring this, since characters a program treated specially (such as period and space and comma) were in the same place in all the encodings a program would encounter. These character sets were typically based on ASCII or EBCDIC.
Logographic languages such as Chinese, Japanese, and Korean (known collectively as CJK) need far more than 256 characters (the limit of a one 8-bit byte per-character encoding) for reasonable representation. The normal solutions involved keeping single-byte representations for ASCII and using two-byte representations for CJK ideographs. Use of these with existing code led to problems with matching and cutting of strings, the severity of which depended on how the character encoding was designed. Some encodings such as the EUC family guarantee that a byte value in the ASCII range will represent only that ASCII character, making the encoding safe for systems that use those characters as field separators. Other encodings such as ISO-2022 and Shift-JIS do not make such guarantees, making matching on byte codes unsafe. These encodings also were not "self-synchronizing", so that locating character boundaries required backing up to the start of a string, and pasting two strings together could result in corruption of the second string (these problems were much less with EUC as any ASCII character did synchronize the encoding).
Unicode has simplified the picture somewhat. Most programming languages now have a datatype for Unicode strings. Unicode's preferred byte stream format UTF-8 is designed not to have the problems described above for older multibyte encodings. UTF-8, UTF-16 and UTF-32 require the programmer to know that the fixed-size code units are different than the "characters", the main difficulty currently is incorrectly designed API's that attempt to hide this difference (UTF-32 does make "code points" fixed-sized, but these are not "characters" due to composing codes).
Implementations.
Some languages like C++ implement strings as templates that can be used with any datatype, but this is the exception, not the rule.
Some languages, such as C++ and Ruby, normally allow the contents of a string to be changed after it has been created; these are termed "mutable" strings. In other languages, such as Java and Python, the value is fixed and a new string must be created if any alteration is to be made; these are termed "immutable" strings.
Strings are typically implemented as arrays of bytes, characters, or code units, in order to allow fast access to individual units or substrings—including characters when they have a fixed length. A few languages such as Haskell implement them as linked lists instead.
Some languages, such as Prolog and Erlang, avoid implementing a dedicated string datatype at all, instead adopting the convention of representing strings as lists of character codes.
Representations.
Representations of strings depend heavily on the choice of character repertoire and the method of character encoding. Older string implementations were designed to work with repertoire and encoding defined by ASCII, or more recent extensions like the ISO 8859 series. Modern implementations often use the extensive repertoire defined by Unicode along with a variety of complex encodings such as UTF-8 and UTF-16.
The term "bytestring" usually indicates a general-purpose string of bytes, rather than strings of only (readable) characters, strings of bits, or such. Byte strings often imply that bytes can take any value and any data can be stored as-is, meaning that there should be no value interpreted as a termination value.
Most string implementations are very similar to variable-length arrays with the entries storing the character codes of corresponding characters. The principal difference is that, with certain encodings, a single logical character may take up more than one entry in the array. This happens for example with UTF-8, where single codes (UCS code points) can take anywhere from one to four bytes, and single characters can take an arbitrary number of codes. In these cases, the logical length of the string (number of characters) differs from the logical length of the array (number of bytes in use). UTF-32 avoids the first part of the problem.
Null-terminated.
The length of a string can be stored implicitly by using a special terminating character; often this is the null character (NUL), which has all bits zero, a convention used and perpetuated by the popular C programming language. Hence, this representation is commonly referred to as a C string. This representation of an "n"-character string takes "n" + 1 space (1 for the terminator), and is thus an implicit data structure.
In terminated strings, the terminating code is not an allowable character in any string. Strings with "length" field do not have this limitation and can also store arbitrary binary data. In C two things are needed to handle binary data, a character pointer and the length of the data.
An example of a "null-terminated string" stored in a 10-byte buffer, along with its ASCII (or more modern UTF-8) representation as 8-bit hexadecimal numbers is:
The length of the string in the above example, "codice_1", is 5 characters, but it occupies 6 bytes. Characters after the terminator do not form part of the representation; they may be either part of another string or just garbage. (Strings of this form are sometimes called "ASCIZ strings", after the original assembly language directive used to declare them.)
Length-prefixed.
The length of a string can also be stored explicitly, for example by prefixing the string with the length as a byte value (a convention used in many Pascal dialects): as a consequence, some people call it a Pascal string or P-string. Storing the string length as byte limits the maximum string length to 255. To avoid such limitations, improved implementations of P-strings use 16-, 32-, or 64-bit words to store the string length. When the "length" field covers the address space, strings are limited only by the available memory. Encoding the length "n" takes log("n") space (see fixed-length code), so length-prefixed strings are a succinct data structure, encoding a string of length "n" in log("n") + "n" space. However, if the length is bounded, then the length can be encoded in constant space, typically a machine word, and thus is an implicit data structure, taking "n" + "k" space, where "k" is the number of characters in a word (8 for 8-bit ASCII on a 64-bit machine, 1 for 32-bit UTF-32/UCS-4 on a 32-bit machine, etc.).
Here is the equivalent Pascal string stored in a 10-byte buffer, along with its ASCII / UTF-8 representation:
Strings as records.
Many languages, including object-oriented ones, implement strings as records in a structure like:
Although this implementation is hidden, and accessed through member functions. The "text" will be a dynamically allocated memory area, that might be expanded if needed. See also string (C++).
Linked-list.
Both character termination and length codes limit strings: For example, C character arrays that contain null (NUL) characters cannot be handled directly by C string library functions: Strings using a length code are limited to the maximum value of the length code.
Both of these limitations can be overcome by clever programming, of course, but such workarounds are by definition not standard.
Rough equivalents of the C termination method have historically appeared in both hardware and software. For example, "data processing" machines like the IBM 1401 used a special word mark bit to delimit strings at the left, where the operation would start at the right. This meant that, while the IBM 1401 had a seven-bit word in "reality", almost no-one ever thought to use this as a feature, and override the assignment of the seventh bit to (for example) handle ASCII codes.
It is possible to create data structures and functions that manipulate them that do not have the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from run length encoding (replacing repeated characters by the character value and a length) and Hamming encoding.
While these representations are common, others are possible. Using ropes makes certain string operations, such as insertions, deletions, and concatenations more efficient.
Security concerns.
The differing memory layout and storage requirements of strings can affect the security of the program accessing the string data. String representations requiring a terminating character are commonly susceptible to buffer overflow problems if the terminating character is not present, caused by a coding error or an attacker deliberately altering the data. String representations adopting a separate length field are also susceptible if the length can be manipulated. In such cases, program code accessing the string data requires bounds checking to ensure that it does not inadvertently access or change data outside of the string memory limits.
String data is frequently obtained from user-input to a program. As such, it is the responsibility of the program to validate the string to ensure that it represents the expected format. Performing limited or no validation of user-input can cause a program to be vulnerable to code injection attacks. 
Text file strings.
In computer readable text files, for example programming language source files or configuration files, strings can be represented. The NUL byte is normally not used as terminator since that does not correspond to the ASCII text standard, and the length is usually not stored, since the file should be human editable without bugs.
Two common representations are:
Non-text strings.
While character strings are very common uses of strings, a string in computer science may refer generically to any sequence of homogeneously typed data. A string of bits or bytes, for example, may be used to represent non-textual binary data retrieved from a communications medium. This data may or may not be represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language's string implementation is not 8-bit clean, data corruption may ensue.
String processing algorithms.
There are many algorithms for processing strings, each with various trade-offs. Some categories of algorithms include:
Advanced string algorithms often employ complex mechanisms and data structures, among them suffix trees and finite state machines.
The name stringology was coined in 1984 by computer scientist Zvi Galil for the issue of algorithms and data structures used for string processing.
Character string-oriented languages and utilities.
Character strings are such a useful datatype that several languages have been designed in order to make string processing applications easy to write. Examples include the following languages:
Many Unix utilities perform simple string manipulations and can be used to easily program some powerful string processing algorithms. Files and finite streams may be viewed as strings.
Some APIs like Multimedia Control Interface, embedded SQL or printf use strings to hold commands that will be interpreted.
Recent scripting programming languages, including Perl, Python, Ruby, and Tcl employ regular expressions to facilitate text operations. Perl is particularly noted for its regular expression use, and many other languages and applications implement Perl compatible regular expressions.
Some languages such as Perl and Ruby support string interpolation, which permits arbitrary expressions to be evaluated and included in string literals.
Character string functions.
String functions are used to manipulate a string or change or edit the contents of a string. They also are used to query information about a string. They are usually used within the context of a computer programming language.
The most basic example of a string function is the string length function -- the function that returns the length of a string (not counting any terminator characters or any of the string's internal structural information) and does not modify the string. This function is often named codice_2 or codice_3. For example, codice_4 would return 11.
String buffers.
In some programming languages, a string buffer is an alternative to a string. It has the ability to be altered through adding or appending, whereas a String is normally fixed or immutable.
In Java.
Theory.
Java's standard way to handle text is to use its codice_5 class. Any given codice_6 in Java is an immutable object, which means its state cannot be changed. A codice_6 has an array of characters. Whenever a codice_6 must be manipulated, any changes require the creation of a new codice_6 (which, in turn, involves the creation of a new array of characters, and copying of the original array). This happens even if the original codice_6's value or intermediate codice_6s used for the manipulation are not kept. 
Java provides an alternate class for string manipulation, called a codice_5. A codice_13, like a codice_6, has an array to hold characters. It, however, is mutable (its state can be altered). Its array of characters is not necessarily completely filled (as oppose to a String, whose array is always the exact required length for its contents). Thus, it has the capability to add, remove, or change its state without creating a new object (and without the creation of a new array, and array copying). The exception to this is when its array is no longer of suitable length to hold its content. In this case, it is required to create a new array, and copy contents.
For these reasons, Java would handle an expression like
like this:
Implications.
Generally, a codice_13 is more efficient than a String in string handling. However, this is not necessarily the case, since a StringBuffer will be required to recreate its character array when it runs out of space. Theoretically, this is possible to happen the same number of times as a new String would be required, although this is unlikely (and the programmer can provide length hints to prevent this). Either way, the effect is not noticeable in modern desktop computers.
As well, the shortcomings of arrays are inherent in a codice_13. In order to insert or remove characters at arbitrary positions, whole sections of arrays must be moved. 
The method by which a codice_13 is attractive in an environment with low processing power takes this ability by using too much memory, which is likely also at a premium in this environment. This point, however, is trivial, considering the space required for creating many instances of Strings in order to process them. As well, the StringBuffer can be optimized to "waste" as little memory as possible.
The class, introduced in J2SE 5.0, differs from codice_13 in that it is unsynchronized. When only a single thread at a time will access the object, using a codice_19 processes more efficiently than using a codice_13.
codice_13 and codice_19 are included in the package.
In .NET.
Microsoft's .NET Framework has a codice_19 class in its Base Class Library.

</doc>
<doc id="27706" url="http://en.wikipedia.org/wiki?curid=27706" title="Satanism">
Satanism

Satanism is a broad term referring to a group of social movements with diverse ideological and philosophical beliefs. Satanism includes symbolic association with, or admiration for, Satan, whom Satanists see as an inspiring and liberating figure. It was estimated that there were 50,000 Satanists in 1990. There may now be as many as one hundred thousand Satanists in the world.
Although the public practice of Satanism began with the founding of The Church of Satan in 1966, historical precedents exist: a group called the Ophite Cultus Satanas was founded in Ohio by Herbert Arthur Sloane in 1948.
Satanist groups that appeared after the 1960s are widely diverse, but two major trends are theistic Satanism and atheistic Satanism. Theistic Satanists venerate Satan as a supernatural deity, viewing him not as omnipotent but rather as a patriarch. In contrast, atheistic Satanists regard Satan as merely a symbol of certain human traits.
There are signs that Satanistic beliefs have become more socially tolerated. Satanism is now allowed in the Royal Navy of the British Armed Forces, despite opposition from Christians, and in 2005, the Supreme Court of the United States debated over protecting the religious rights of prison inmates after a lawsuit challenging the issue was filed to them.
Contemporary Satanism is mainly an American phenomenon, the ideas spreading with the effects of globalization and the Internet. The Internet promotes awareness of other Satanists, and is also the main battleground for the definitions of Satanism today. Satanism started to reach Eastern Europe in the 1990s, in time with the fall of the Soviet Union, and most noticeably in Poland and Lithuania, predominantly Roman Catholic countries.
Historical background.
Particularly after the European Enlightenment, some works, such as "Paradise Lost", were taken up by Romantics like Byron and described as presenting the biblical Satan as an allegory representing a crisis of faith, individualism, free will, wisdom and enlightenment. Those works actually featuring Satan as a heroic character are fewer in number but do exist. George Bernard Shaw and Mark Twain (cf. "Letters from the Earth") included such characterizations in their works long before religious Satanists took up the pen. From then on, Satan and Satanism started to gain a new meaning outside of Christianity.
Theistic Satanism.
Theistic Satanism (also known as traditional Satanism, Spiritual Satanism or Devil worship) is a form of Satanism with the primary belief that Satan is an actual deity or force to revere or worship. Other characteristics of theistic Satanism may include a belief in magic, which is manipulated through ritual, although that is not a defining criterion, and theistic Satanists may focus solely on devotion. Unlike LaVeyan Satanism, theistic Satanism believes that Satan is a real being rather than a symbol of individualism.
Luciferianism.
Luciferianism can be understood best as a belief system or intellectual creed that venerates the essential and inherent characteristics that are affixed and commonly given to Lucifer. Luciferianism is often identified as an auxiliary creed or movement of Satanism, due to the common identification of Lucifer with Satan. Some Luciferians accept this identification and/or consider Lucifer as the "light bearer" and illuminated aspect of Satan, giving them the name of Satanists and the right to bear the title. Others reject it, giving the argument that Lucifer is a more positive and easy-going ideal than Satan. They are inspired by the ancient myths of Egypt, Rome and Greece, Gnosticism and traditional Western occultism.
Palladists.
Palladists are an alleged theistic Satanist society or member of that society. The name Palladian comes from Pallas and refers to the Greco-Roman goddess of wisdom and learning.
Our Lady of Endor Coven.
Our Lady of Endor Coven, also known as Ophite Cultus Satanas (originally spelled "Sathanas"), was a satanic cult founded in 1948 by Herbert Arthur Sloane in Toledo, Ohio. The group was heavily influenced by gnosticism (especially that found in the contemporary book by Hans Jonas, "The Gnostic Religion"), and worshiped Satanas, their name for Satan ("Cultus Satanas" is a Latin version of Cult of Satan). Satanas (or Satan) was defined in gnostic terms as the Serpent in the Garden of Eden who revealed the knowledge of the true God to Eve. That it called itself "Ophite" is a reference to the ancient gnostic sect of the Ophites, who were said to worship the serpent.
Atheistic Satanism.
LaVeyan Satanism, as codified in The Satanic Bible and overseen by the Church of Satan, was founded in 1966 by Anton Szandor LaVey. It is an atheistic and materialistic religion that champions individualism, epicureanism, secularism, and egoism, and propagates a worldview of naturalism, Social Darwinism, and Lex Talionis. Adherents describe it as a non-spiritual religion of the flesh, or "...the world's first carnal religion".
Contrary to popular belief, it does not involve "devil worship" or worship of any deities. The Church of Satan asserts that "In Satanism each individual is his or her own god—there is no room for any other god and that includes Satan, Lucifer, Cthulhu or whatever other name one might select or take from history or fiction.". Adherents instead see the character of Satan as a symbol of pride, carnality, liberty, enlightenment, and undefiled wisdom, and serves as a conceptual framework and an external metaphorical projection of the Satanist's highest personal potential. Satan (Hebrew: שָּׂטָן "satan", meaning "adversary") is seen as a symbol of defiance to the conservatism of mainstream philosophical and religious currents, mainly the Abrahamic religions, that see this character as their antithesis.
The prefix "LaVeyan" was never used by Anton LaVey or by the Church of Satan, nor does the term appear in any of its literature. The church has stated its contention that they are the first formally organized religion to adopt the term "Satanism" and asserts that Satanism and the 'worship of Satan' are not congruent. The term "Theistic Satanism" has been described as "oxymoronic" by the church and its High Priest. The Church of Satan rejects the legitimacy of any other organizations who claim to be Satanists, dubbing them reverse-Christians, pseudo-Satanists or Devil worshipers. Today, the Church of Satan promotes itself as the only authentic representation of Satanism, and it routinely publishes materials underscoring this contention.
The fundamentals of the religion's creed are synthesized in "The Nine Satanic Statements", "The Nine Satanic Sins", and "The Eleven Satanic Rules of the Earth".
Accusations of Satanism.
Historically, some people or groups have been specifically described as worshiping Satan or the Devil, or of being devoted to the work of Satan. The widespread preponderance of these groups in European cultures is in part connected with the importance and meaning of Satan within Christianity.
Islam.
The Yazidis, a minority religion of the Middle East who worship Melek Taus, are often referred to as Satan worshippers by some Muslims. Due to this, they have been targeted for conversion and extermination by the Islamic State of Iraq and the Levant.
Popular music.
Black metal has often been connected with Satanism, in part for the lyrical content of several bands and their frequent use of imagery often tied to left hand path beliefs (such as the inverted pentagram). More often than not musicians associating themselves with black metal say they do not believe in legitimate Satanic ideology and often profess to being atheists, agnostics, or religious skeptics. In some instances, followers of right hand path religions use Satanic references for entertainment purposes and shock value. Most of black metal's "first wave" bands only used Satanism for shock value; one of the few exceptions is Mercyful Fate singer King Diamond, who follows LaVeyan Satanism and whom Michael Moynihan calls "one of the only performers of the '80s Satanic Metal who was more than just a poseur using a devilish image for shock value". One early precursor to Satanic metal was the 1969 rock album Witchcraft Destroys Minds & Reaps Souls, which contained numerous references to Satanism that reappeared in later Satanic rock music.
Glen Benton, vocalist and bassist of the band Deicide, once openly claimed to be a practitioner of theistic Satanism, and has spoken publicly to profess staunch anti-Christian sentiment. The controversial Dissection frontman Jon Nödtveidt openly spoke about his "chaos-gnostic" satanic beliefs, being a member of the Misanthropic Luciferian Order, and called his band "the sonic propaganda unit of the MLO". Norwegian black metal artists such as Euronymous from Mayhem and Infernus from Gorgoroth have also identified themselves as Satanists and actively promoted their beliefs. Numerous church burnings that covered parts of Norway in the early 1990s were also attributed to youths involved in the black metal movement, which included people promoting theistic Satanic beliefs and strong anti-LaVeyan attitudes. However, the legitimacy of such actions as Satanic endeavors, rather than simply rebellious actions done for publicity, is something that has been doubted by even some of those who contribute to the genre.
Organizations.
The Church of Satan.
On Walpurgisnacht, April 30, 1966, Anton LaVey founded the "The Satanic Church" (which he would later rename the "Church of Satan"). The Church of Satan is an organization dedicated to the acceptance of the carnal self, as articulated in "The Satanic Bible", written in 1969 by Anton Szandor LaVey.
First Satanic Church.
After LaVey's death in 1997 the Church of Satan was taken over by a new administration and its headquarters was moved to New York. LaVey's daughter, the High Priestess Karla LaVey, felt this to be a disservice to her father's legacy. The First Satanic Church was re-founded on October 31, 1999 by Karla LaVey to carry on the legacy of her father. She continues to run it out of San Francisco, California.
Temple of Set.
The Temple of Set is an initiatory occult society claiming to be the world's leading left-hand path religious organization. It was established in 1975 by Michael A. Aquino and certain members of the priesthood of the Church of Satan, who left because of administrative and philosophical disagreements. ToS deliberately self-differentiates from CoS in several ways, most significantly in theology and sociology. The philosophy of the Temple of Set may be summed up as "enlightened individualism" — enhancement and improvement of oneself by personal education, experiment and initiation. This process is necessarily different and distinctive for each individual. The members do not agree on whether Set is "real" or not, and they're not expected to.
Setianism, in theory, is similar to theistic Satanism. The principle deity of Setianism is the ancient Egyptian god Set, or Seth, the god of adversary. Set supposedly is the Dark Lord behind the Hebrew entity Satan. Set, as the first principle of consciousness, is emulated by Setians, who symbolize the concept of individual, subjective intelligence distinct from the natural order as the "Black Flame". (Some people who are not members of the Temple of Set find spiritual inspiration in the Egyptian god Set, and may share some beliefs with the organization. The belief system in general is referred to as Setianism.)
Members of the Temple of Set are mostly male, between the ages of twenty and fifty.
Order of Nine Angles.
The authors Per Faxneld and Jesper Petersen write that the Order of Nine Angles (ONA, O9A) "represent a dangerous and extreme form of Satanism". The ONA first attracted public attention during the 1980s and 1990s after being mentioned in books detailing fascist Satanism. They were initially formed in the United Kingdom and are presently organized around clandestine cells (which it calls "traditional nexions") and around what it calls "sinister tribes".
The Satanic Temple.
The Satanic Temple uses the literary Satan as a mythological foundation for a non-supernatural religion, in order to construct a cultural narrative that can usefully contextualize life experiences and promote pragmatic skepticism, rational reciprocity, personal autonomy, and curiosity.
As it lacks the creed of elitism and Social Darwinism that define the Church of Satan in favor of other characteristics of the literary Satan, it contrasts itself by actively participating in public affairs and providing outreach to the wider public. This has manifested in several public political actions and efforts at lobbying, with a focus on the separation of church and state and using satire against religious organizations that it believes interfere with freedom and the pursuit of happiness.
The only requirements to be a member are to support the tenets and beliefs of the organization, and to name yourself a member.
The group has held "worship" services that include dance music, porn rooms, phallic imagery, S&M behaviors and nudity. It also considers gay marriage a religious sacrament, and therefore argues that bans on the practice violate Satanists' freedom of religion. Because the group regards "inviolability of the body" as a key doctrine, it also views all restrictions on abortion, including mandatory waiting periods, as an infringement on the rights of Satanists to practice their religion. 

</doc>
<doc id="27707" url="http://en.wikipedia.org/wiki?curid=27707" title="Socialist law">
Socialist law

Socialist law or Soviet law denotes a general type of legal system which has been used in communist and formerly communist states. It is based on the civil law system, with major modifications and additions from Marxist-Leninist ideology. There is controversy as to whether socialist law ever constituted a separate legal system or not. If so, prior to the end of the Cold War, "socialist law" would be ranked among the major legal systems of the world.
While civil law systems have traditionally put great pains in defining the notion of private property, how it may be acquired, transferred, or lost, socialist law systems provide for most property to be owned by the state or by agricultural co-operatives, and having special courts and laws for state enterprises.
Many scholars argue that socialist law was not a separate legal classification. Although the command economy approach of the communist states meant that most types of property could not be owned, the Soviet Union always had a civil code, courts that interpreted this civil code, and a civil law approach to legal reasoning (thus, both legal process and legal reasoning were largely analogous to the French or German civil code system). Legal systems in all socialist states preserved formal criteria of the Romano-Germanic civil law; for this reason, law theorists in post-socialist states usually consider the Socialist law as a particular case of the Romano-Germanic civil law. Cases of development of common law into Socialist law are unknown because of incompatibility of basic principles of these two systems (common law presumes influential rule-making role of courts while courts in socialist states play a dependent role).
Soviet legal theory.
Soviet law displayed many special characteristics that derived from the socialist nature of the Soviet state and reflected Marxist-Leninist ideology. Vladimir Lenin accepted the Marxist conception of the law and the state as instruments of coercion in the hands of the bourgeoisie and postulated the creation of popular, informal tribunals to administer revolutionary justice. One of the main theoreticians of Soviet socialist legality in this early phase was Pēteris Stučka.
Alongside this utopian trend was one more critical of the concept of "proletarian justice", represented by Evgeny Pashukanis. A dictatorial trend developed that advocated the use of law and legal institutions to suppress all opposition to the regime. This trend reached its zenith under Joseph Stalin with the ascendancy of Andrey Vyshinsky, when the administration of justice was carried out mainly by the security police in special tribunals.
During the de-Stalinization of the Nikita Khrushchev era, a new trend developed, based on socialist legality, that stressed the need to protect the procedural and statutory rights of citizens, while still calling for obedience to the state. New legal codes, introduced in 1960, were part of the effort to establish legal norms in administering laws. Although socialist legality remained in force after 1960, the dictatorial and utopian trends continued to influence the legal process. Persecution of political and religious dissenters continued, but at the same time there was a tendency to decriminalize lesser offenses by handing them over to people's courts and administrative agencies and dealing with them by education rather than by incarceration.
By late 1986, the Mikhail Gorbachev era was stressing anew the importance of individual rights in relation to the state and criticizing those who violated procedural law in implementing Soviet justice. This signaled a resurgence of socialist legality as the dominant trend. It should be noted, however, that socialist legality itself still lacked features associated with Western jurisprudence.
Characteristic traits.
Socialist law is similar to common law or civil law but with a greatly increased public law sector and decreased private law sector.
A specific institution characteristic to Socialist law was the so-called burlaw court (or, verbally, "court of comrades", Russian товарищеский суд) which decided on minor offences.
Chinese Socialist law.
Among the remaining communist governments, some (most notably the People's Republic of China) have added extensive modifications to their legal systems. In general, this is a result of their market-oriented economic changes. However, some communist influence can still be seen. For example, in Chinese real estate law there is no unified concept of real property; the state owns all land but often not the structures that sit on that land. A rather complex "ad hoc" system of use rights to land property has developed, and these use rights are the things being officially traded (rather than the property itself). In some cases (for example in the case of urban residential property), the system results in something that resembles real property transactions in other legal systems.
In other cases, the Chinese system results in something quite different. For example, it is a common misconception that reforms under Deng Xiaoping resulted in the privatization of agricultural land and a creation of a land tenure system similar to those found in Western countries. In actuality, the village committee owns the land and contracts the right to use this land to individual farmers who may use the land to make money from agriculture. Hence the rights that are normally unified in Western economies are split up between the individual farmer and the village committee.
This has a number of consequences. One of them is that, because the farmer does not have an absolute right to transfer the land, he cannot borrow against his use rights. On the other hand, there is some insurance against risk in the system, in that the farmer can return his land to the village committee if he wants to stop farming and start some other sort of business. Then, if this business does not work, he can get a new contract with the village committee and return to farming. The fact that the land is redistributable by the village committee also ensures that no one is left landless; this creates a form of social welfare.
There have been a number of proposals to reform this system and they have tended to be in the direction of fully privatizing rural land for the alleged purpose of increasing efficiency. These proposals have usually not received any significant support, largely because of the popularity of the current system among the farmers themselves. There is little risk that the village committee will attempt to impose a bad contract on the farmers, since this would reduce the amount of money the village committee receives. At the same time, the farmer has some flexibility to decide to leave farming for other ventures and to return at a later time.

</doc>
<doc id="27709" url="http://en.wikipedia.org/wiki?curid=27709" title="Semiconductor">
Semiconductor

A semiconductor material has an electrical conductivity value falling between that of a conductor, such as copper, and an insulator, such as glass. Semiconductors are the foundation of modern electronics. Semiconducting materials exist in two types - elemental materials and compound materials. The modern understanding of the properties of a semiconductor relies on quantum physics to explain the movement of electrons and holes in a crystal lattice. The unique arrangement of the crystal lattice makes silicon and germanium the most commonly used elements in the preparation of semiconducting materials. An increased knowledge of semiconductor materials and fabrication processes has made possible continuing increases in the complexity and speed of microprocessors and memory devices. Some of the information on this page may be outdated within a year, due to the fact that new discoveries are made in the field frequently.
The electrical conductivity of a semiconductor material increases with increasing temperature, which is behaviour opposite to that of a metal. Semiconductor devices can display a range of useful properties such as passing current more easily in one direction than the other, showing variable resistance, and sensitivity to light or heat. Because the electrical properties of a semiconductor material can be modified by controlled addition of impurities, or by the application of electrical fields or light, devices made from semiconductors can be used for amplification, switching, and energy conversion.
Current conduction in a semiconductor occurs through the movement of free electrons and "holes", collectively known as charge carriers. Adding impurity atoms to a semiconducting material, known as "doping", greatly increases the number of charge carriers within it. When a doped semiconductor contains mostly free holes it is called "p-type", and when it contains mostly free electrons it is known as "n-type". The semiconductor materials used in electronic devices are doped under precise conditions to control the concentration and regions of p- and n-type dopants. A single semiconductor crystal can have many p- and n-type regions; the p–n junctions between these regions are responsible for the useful electronic behaviour.
Some of the properties of semiconductor materials were observed throughout the mid 19th and first decades of the 20th century. Development of quantum physics in turn allowed the development of the transistor in 1947. Although some pure elements and many compounds display semiconductor properties, silicon, germanium, and compounds of gallium are the most widely used in electronic devices. Elements near the so-called "metalloid staircase", where the metalloids are located on the periodic table, are usually used as semiconductors.
The nickname of the southern area of Northern California is Silicon Valley because of all the influential tech companies that have their headquarters there. An integral part of today’s technology is built upon semiconductors, which are made primarily of silicon. Some major companies include Marvell Technology Group, National Semiconductor, and Advanced Micro Devices.
Materials.
A large number of elements and compounds have semiconducting properties, including: 
Most common semiconducting materials are crystalline solids, but amorphous and liquid semiconductors are also known. These include hydrogenated amorphous silicon and mixtures of arsenic, selenium and tellurium in a variety of proportions. These compounds share with better known semiconductors the properties of intermediate conductivity and a rapid variation of conductivity with temperature, as well as occasional negative resistance. Such disordered materials lack the rigid crystalline structure of conventional semiconductors such as silicon. They are generally used in thin film structures, which do not require material of higher electronic quality, being relatively insensitive to impurities and radiation damage.
Preparation of semiconductor materials.
Almost all of today’s technology involves the use of semiconductors, with the most important aspect being the integrated circuit (IC). Some examples of devices that contain integrated circuits includes laptops, scanners, cell-phones, etc. Semiconductors for IC’s are mass-produced. To create an ideal semiconducting material, chemical purity is a must. Any small imperfection can have a drastic effect on how the semiconducting material behaves due to the scale that which the materials are used.
A high degree of crystalline perfection is also required, since faults in crystal structure (such as dislocations, twins, and stacking faults) interfere with the semiconducting properties of the material. Crystalline faults are a major cause of defective semiconductor devices. The larger the crystal, the more difficult it is to achieve the necessary perfection. Current mass production processes use crystal ingots between 100 and 300 mm (4 and 12 in) in diameter which are grown as cylinders and sliced into wafers.
There is a combination of processes that is used to prepare semiconducting materials for IC’s. One process is called thermal oxidation, which forms silicon dioxide on the surface of the silicon. This is used as a gate insulator and field oxide. Other processes are called photomasks and photolithography. This process is what creates the patterns on the circuity in the integrated circuit. Ultraviolet light is used along with a photoresist layer to create a chemical change that generates the patterns for the circuit.
Etching is the next process that is required. The part of the silicon that was not covered by the photoresist layer from the previous step can now be etched. The main process typically used today is called plasma etching. Plasma etching usually involves an etch gas pumped in a low-pressure chamber to create plasma. A common etch gas is chlorofluorocarbon, or more commonly known Freon. A high radio-frequency voltage between the cathode and anode is what creates the plasma in the chamber. The silicon wafer is located on the cathode, which causes it to be hit by the positively charged ions that are released from the plasma. The end result is silicon that is etched anisotropically.
The last process is called diffusion. This is the process that gives the semiconducting material its desired semiconducting properties. It is also known as doping. The process introduces an impure atom to the system, which creates the p-n junction. In order to get the impure atoms embedded in the silicon wafer, the wafer is first put in a 1100 degree Celsius chamber. The atoms are injected in and eventually diffuse with the silicon. After the process is completed and the silicon has reached room temperature, the doping process is done and the semiconducting material is ready to be used in an integrated circuit.
Physics of semiconductors.
Energy bands and electrical conduction.
Semiconductors are defined by their unique electric conductive behavior, somewhere between that of a metal and an insulator.
The differences between these materials can be understood in terms of the quantum states for electrons, each of which may contain zero or one electron (by the Pauli exclusion principle). These states are associated with the electronic band structure of the material.
Electrical conductivity arises due to the presence of electrons in states that are delocalized (extending through the material), however in order to transport electrons a state must be "partially filled", containing an electron only part of the time. If the state is always occupied with an electron, then it is inert, blocking the passage of other electrons via that state.
The energies of these quantum states are critical, since a state is partially filled only if its energy is near the Fermi level (see Fermi–Dirac statistics).
High conductivity in a material comes from it having many partially filled states and much state delocalization.
Metals are good electrical conductors and have many partially filled states with energies near their Fermi level.
Insulators, by contrast, have few partially filled states, their Fermi levels sit within band gaps with few energy states to occupy.
Importantly, an insulator can be made to conduct by increasing its temperature: heating provides energy to promote some electrons across the band gap, inducing partially filled states in both the band of states beneath the band gap (valence band) and the band of states above the band gap (conduction band).
An (intrinsic) semiconductor has a band gap that is smaller than that of an insulator and at room temperature significant numbers of electrons can be excited to cross the band gap.
A pure semiconductor, however, is not very useful, as it is neither a very good insulator nor a very good conductor.
However, one important feature of semiconductors (and some insulators, known as "semi-insulators") is that their conductivity can be increased and controlled by doping with impurities and gating with electric fields. Doping and gating move either the conduction or valence band much closer to the Fermi level, and greatly increase the number of partially filled states.
Some wider-band gap semiconductor materials are sometimes referred to as semi-insulators. When undoped, these have electrical conductivity nearer to that of electrical insulators, however they can be doped (making them as useful as semiconductors). Semi-insulators find niche applications in micro-electronics, such as substrates for HEMT. An example of a common semi-insulator is gallium arsenide. Some materials, such as titanium dioxide, can even be used as insulating materials for some applications, while being treated as wide-gap semiconductors for other applications.
Charge carriers (electrons and holes).
The partial filling of the states at the bottom of the conduction band can be understood as adding electrons to that band.
The electrons do not stay indefinitely (due to the natural thermal recombination) but they can move around for some time.
The actual concentration of electrons is typically very dilute, and so (unlike in metals) it is possible to think of the electrons in the conduction band of a semiconductor as a sort of classical ideal gas, where the electrons fly around freely without being subject to the Pauli exclusion principle. In most semiconductors the conduction bands have a parabolic dispersion relation, and so these electrons respond to forces (electric field, magnetic field, etc.) much like they would in a vacuum, though with a different effective mass.
Because the electrons behave like an ideal gas, one may also think about conduction in very simplistic terms such as the Drude model, and introduce concepts such as electron mobility.
For partial filling at the top of the valence band, it is helpful to introduce the concept of an electron hole.
Although the electrons in the valence band are always moving around, a completely full valence band is inert, not conducting any current.
If an electron is taken out of the valence band, then the trajectory that the electron would normally have taken is now missing its charge.
For the purposes of electric current, this combination of the full valence band, minus the electron, can be converted into a picture of a completely empty band containing a positively charged particle that moves in the same way as the electron.
Combined with the "negative" effective mass of the electrons at the top of the valence band, we arrive at a picture of a positively charged particle that responds to electric and magnetic fields just as a normal positively charged particle would do in vacuum, again with some positive effective mass.
This particle is called a hole, and the collection of holes in the valence can again be understood in simple classical terms (as with the electrons in the conduction band).
Carrier generation and recombination.
When ionizing radiation strikes a semiconductor, it may excite an electron out of its energy level and consequently leave a hole. This process is known as "electron–hole pair generation". Electron-hole pairs are constantly generated from thermal energy as well, in the absence of any external energy source.
Electron-hole pairs are also apt to recombine. Conservation of energy demands that these recombination events, in which an electron loses an amount of energy larger than the band gap, be accompanied by the emission of thermal energy (in the form of phonons) or radiation (in the form of photons).
In some states, the generation and recombination of electron–hole pairs are in equipoise. The number of electron-hole pairs in the steady state at a given temperature is determined by quantum statistical mechanics. The precise quantum mechanical mechanisms of generation and recombination are governed by conservation of energy and conservation of momentum.
As the probability that electrons and holes meet together is proportional to the product of their amounts, the product is in steady state nearly constant at a given temperature, providing that there is no significant electric field (which might "flush" carriers of both types, or move them from neighbour regions containing more of them to meet together) or externally driven pair generation. The product is a function of the temperature, as the probability of getting enough thermal energy to produce a pair increases with temperature, being approximately exp(−"E""G"/"kT"), where "k" is Boltzmann's constant, "T" is absolute temperature and "E""G" is band gap.
The probability of meeting is increased by carrier traps—impurities or dislocations which can trap an electron or hole and hold it until a pair is completed. Such carrier traps are sometimes purposely added to reduce the time needed to reach the steady state.
Doping.
The conductivity of semiconductors may easily be modified by introducing impurities into their crystal lattice. The process of adding controlled impurities to a semiconductor is known as "doping". The amount of impurity, or dopant, added to an "intrinsic" (pure) semiconductor varies its level of conductivity. Doped semiconductors are referred to as "extrinsic". By adding impurity to the pure semiconductors, the electrical conductivity may be varied by factors of thousands or millions.
A 1 cm3 specimen of a metal or semiconductor has of the order of 1022 atoms. In a metal, every atom donates at least one free electron for conduction, thus 1 cm3 of metal contains on the order of 1022 free electrons, whereas a 1 cm3 sample of pure germanium at 20 °C contains about atoms, but only free electrons and holes. The addition of 0.001% of arsenic (an impurity) donates an extra 1017 free electrons in the same volume and the electrical conductivity is increased by a factor of 10,000.
The materials chosen as suitable dopants depend on the atomic properties of both the dopant and the material to be doped. In general, dopants that produce the desired controlled changes are classified as either electron acceptors or donors. Semiconductors doped with "donor" impurities are called "n-type", while those doped with "acceptor" impurities are known as "p-type". The n and p type designations indicate which charge carrier acts as the material's majority carrier. The opposite carrier is called the minority carrier, which exists due to thermal excitation at a much lower concentration compared to the majority carrier.
For example, the pure semiconductor silicon has four valence electrons which bond each silicon atom to its neighbors. In silicon, the most common dopants are "group III" and "group V" elements. Group III elements all contain three valence electrons, causing them to function as acceptors when used to dope silicon. When an acceptor atom replaces a silicon atom in the crystal, a vacant state ( an electron "hole") is created, which can move around the lattice and functions as a charge carrier. Group V elements have five valence electrons, which allows them to act as a donor; substitution of these atoms for silicon creates an extra free electron. Therefore, a silicon crystal doped with boron creates a p-type semiconductor whereas one doped with phosphorus results in an n-type material.
During manufacture, dopants can be diffused into the semiconductor body by contact with gaseous compounds of the desired element, or ion implantation can be used to accurately position the doped regions.
Early history of semiconductors.
The history of the understanding of semiconductors begins with experiments on the electrical properties of materials. The properties of negative temperature coefficient of resistance, rectification, and light-sensitivity were observed starting in the early 19th century.
In 1833, Michael Faraday reported that the resistance of specimens of silver sulfide decreases when they are heated. This is contrary to the behavior of metallic substances such as copper. In 1839, A. E. Becquerel reported observation of a voltage between a solid and a liquid electrolyte when struck by light, the photovoltaic effect. In 1873 Willoughby Smith observed that selenium resistors exhibit decreasing resistance when light falls on them. In 1874 Karl Ferdinand Braun observed conduction and rectification in metallic sulphides, and Arthur Schuster found that a copper oxide layer on wires has rectification properties that ceases when the wires are cleaned. Adams and Day observed the photovoltaic effect in selenium in 1876.
A unified explanation of these phenomena required a theory of solid-state physics which developed greatly in the first half of the 20th Century. In 1878 Edwin Herbert Hall demonstrated the deflection of flowing charge carriers by an applied magnetic field, the Hall effect. The discovery of the electron by J.J. Thomson in 1897 prompted theories of electron-based conduction in solids. Karl Baedeker, by observing a Hall effect with the reverse sign to that in metals, theorized that copper iodide had positive charge carriers. Johan Koenigsberger classified solid materials as metals, insulators and "variable conductors" in 1914, although his student Josef Weiss introduced term "Halbleiter" (semiconductor) in modern meaning in PhD thesis already in 1910. Felix Bloch published a theory of the movement of electrons through atomic lattices in 1928. In 1930, B. Gudden stated that conductivity in semiconductors was due to minor concentrations of impurities. By 1931, the band theory of conduction had been established by Alan Herries Wilson and the concept of band gaps had been developed. Walter H. Schottky and Nevill Francis Mott developed models of the potential barrier and of the characteristics of a metal-semiconductor junction. By 1938, Boris Davydov had developed a theory of the copper-oxide rectifer, identifying the effect of the p–n junction and the importance of minority carriers and surface states.
Agreement between theoretical predictions (based on developing quantum mechanics) and experimental results was sometimes poor. This was later explained by John Bardeen as due to the extreme "structure sensitive" behavior of semiconductors, whose properties change dramatically based on tiny amounts of impurities. Commercially pure materials of the 1920s containing varying proportions of trace contaminants produced differing experimental results. This spurred the development of improved material refining techniques, culminating in modern semiconductor refineries producing materials with parts-per-trillion purity.
Devices using semiconductors were at first constructed based on empirical knowledge, before semiconductor theory provided a guide to construction of more capable and reliable devices.
Alexander Graham Bell used the light-sensitive property of selenium to transmit sound over a beam of light in 1880. A working solar cell, of low efficiency, was constructed by Charles Fritts in 1883 using a metal plate coated with selenium and a thin layer of gold; the device became commercially useful in photographic light meters in the 1930s. Point-contact microwave detector rectifiers made of lead sulfide were used by Jagadish Chandra Bose in 1904; the cat's-whisker detector using natural galena or other materials became a common device in the development of radio. However, it was somewhat unpredictable in operation and required manual adjustment for best performance. In 1906 H.J. Round observed light emission when electric current passed through silicon carbide crystals, the principle behind the light emitting diode. Oleg Losev observed similar light emission in 1922 but at the time the effect had no practical use. Power rectifiers, using copper oxide and selenium, were developed in the 1920s and became commercially important as an alternative to vacuum tube rectifiers.
In the years preceding World War II, infra-red detection and communications devices prompted research into lead-sulfide and lead-selenide materials. These devices were used for detecting ships and aircraft, for infrared rangefinders, and for voice communication systems. The point-contact crystal detector became vital for microwave radio systems, since available vacuum tube devices could not serve as detectors above about 4000 MHz; advanced radar systems relied on the fast response of crystal detectors. Considerable research and development of silicon materials occurred during the war to develop detectors of consistent quality.
Detector and power rectifiers could not amplify a signal. Many efforts were made to develop a solid-state amplifier, but these were unsuccessful because of limited theoretical understanding of semiconductor materials. In 1922 Oleg Losev developed two-terminal, negative resistance amplifiers for radio; however, he perished in the Siege of Leningrad. In 1926 Julius Edgar Lilienfeld patented a device resembling a modern field-effect transistor, but it was not practical. R. Hilsch and R. W. Pohl in 1938 demonstrated a solid-state amplifier using a structure resembling the control grid of a vacuum tube; although the device displayed power gain, it had a cut-off frequency of one cycle per second, too low for any practical applications, but an effective application of the available theory. At Bell Labs, William Shockley and A. Holden started investigating solid-state amplifiers in 1938. The first p–n junction in silicon was observed by Russell Ohl about 1941, when a specimen was found to be light-sensitive, with a sharp boundary between p-type impurity at one end and n-type at the other. A slice cut from the specimen at the p–n boundary developed a voltage when exposed to light.
In France, during the war, Herbert Mataré had observed amplification between adjacent point contacts on a germanium base. After the war, Mataré's group announced their "Transistron" amplifier only shortly after Bell Labs announced the "transistor".

</doc>
<doc id="27711" url="http://en.wikipedia.org/wiki?curid=27711" title="Starch">
Starch

"For the Urhobo cuisine dish known as starch see usi (food)"
Starch or amylum is a carbohydrate consisting of a large number of glucose units joined by glycosidic bonds. This polysaccharide is produced by most green plants as an energy store. It is the most common carbohydrate in human diets and is contained in large amounts in such staple foods as potatoes, wheat, maize (corn), rice, and cassava.
Pure starch is a white, tasteless and odorless powder that is insoluble in cold water or alcohol. It consists of two types of molecules: the linear and helical amylose and the branched amylopectin.
Depending on the plant, starch generally contains 20 to 25% amylose and 75 to 80% amylopectin by weight. Glycogen, the glucose store of animals, is a more branched version of amylopectin.
Starch is processed to produce many of the sugars in processed foods. Dissolving starch in warm water gives wheatpaste, which can be used as a thickening, stiffening or gluing agent. The biggest industrial non-food use of starch is as adhesive in the papermaking process. Starch can be applied to parts of some garments before ironing, to stiffen them.
Etymology.
The word "starch" is from a Germanic root with the meanings "strong, stiff, strengthen, stiffen". Modern German "Stärke" (starch) is related.
"Amylum" for starch is from the Greek αμυλον, "amylon" which means "not ground at a mill". The root amyl is used in biochemistry for several compounds related to starch.
History.
Starch grains from the rhizomes of "Typha" (cattails, bullrushes) as flour have been identified from grinding stones in Europe dating back to 30,000 years ago. Starch grains from sorghum were found on grind stones in caves in Ngalue, Mozambique dating up to 100,000 years ago.
Pure extracted wheat starch paste was used in Ancient Egypt possibly to glue papyrus. The extraction of starch is first described in the "Natural History" of Pliny the Elder around AD 77-79. Romans used it also in cosmetic creams, to powder the hair and to thicken sauces. Persians and Indians used it to make dishes similar to gothumai wheat halva. Rice starch as surface treatment of paper has been used in paper production in China, from 700 AD onwards.
In addition to starchy plants consumed directly, 66 million tonnes of starch were being produced per year world-wide by 2008. In the EU this was around 8.5 million tonnes, with around
40% being used for industrial applications and 60% for food uses, most of the latter as glucose syrups.
Energy store of plants.
Most green plants use starch as their energy store. An exception is the family Asteraceae (asters, daisies and sunflowers), where starch is replaced by the fructan inulin.
In photosynthesis, plants use light energy to produce glucose from carbon dioxide. The glucose is stored mainly in the form of starch granules, in plastids such as chloroplasts and especially amyloplasts. Toward the end of the growing season, starch accumulates in twigs of trees near the buds. Fruit, seeds, rhizomes, and tubers store starch to prepare for the next growing season.
Glucose is soluble in water, hydrophilic, binds with water and then takes up much space and is osmotically active; glucose in the form of starch, on the other hand, is not soluble, therefore osmotically inactive and can be stored much more compactly.
Glucose molecules are bound in starch by the easily hydrolyzed alpha bonds. The same type of bond is found in the animal reserve polysaccharide glycogen. This is in contrast to many structural polysaccharides such as chitin, cellulose and peptidoglycan, which are bound by beta bonds and are much more resistant to hydrolysis.
Biosynthesis.
Plants produce starch by first converting glucose 1-phosphate to ADP-glucose using the enzyme glucose-1-phosphate adenylyltransferase. This step requires energy in the form of ATP. The enzyme starch synthase then adds the ADP-glucose via a 1,4-alpha glycosidic bond to a growing chain of glucose residues, liberating ADP and creating amylose. Starch branching enzyme introduces 1,6-alpha glycosidic bonds between these chains, creating the branched amylopectin. The starch debranching enzyme isoamylase removes some of these branches. Several isoforms of these enzymes exist, leading to a highly complex synthesis process.
Glycogen and amylopectin have the same structure, but the former has about one branch point per ten 1,4-alpha bonds, compared to about one branch point per thirty 1,4-alpha bonds in amylopectin. Amylopectin is synthesized from ADP-glucose while mammals and fungi synthesize glycogen from UDP-glucose; for most cases, bacteria synthesize glycogen from (analogous to starch).
In addition to starch synthesis in plants, starch can be synthesized from non-food starch mediated by an enzyme cocktail. In this cell-free biosystem, beta-1,4-glycosidic bond-linked cellulose is partially hydrolyzed to cellobioase. Cellobiose phosphorylase cleaves to glucose 1-phosphate and glucose; the other enzyme—potato alpha-glucan phosphorylase can add glucose unit from glucose 1-phosphorylase to the non-ruducing ends of starch. In it, phosphate is internally recycled. The other product—glucose—can be assimilated by a yeast. This cell-free bioprocessing does not need any costly chemical and energy input, can be conducted in aqueous solution, and does not have sugar losses. As a result, cellulosic starch could be used to feed the world because cellulose resource is about 50 times of starch resource.
Degradation.
Starch is synthesized in plant leaves during the day, in order to serve as an energy source at night. Starch is stored as granulates. These insoluble highly branched chains have to be phosphorylated in order to be accessible for degrading enzymes. The enzyme glucan, water dikinase (GWD) phosphorylates at the C-6 position of a glucose molecule, close to the chains 1,6-alpha branching bonds. A second enzyme, phosphoglucan, water dikinase (PWD) phosphorylates the glucose molecule at the C-3 position. A loss of these enzymes, for example a loss of the GWD, leads to a starch excess (sex) phenotype. Because starch cannot be phosphorylated, it accumulates in the plastid.
After the phosphorylation, the first degrading enzyme, beta-amylase (BAM) is able to attack the glucose chain at its non-reducing end. Maltose is released as the main product of starch degradation. If the glucose chain consists of three or less molecules, BAM cannot release maltose. A second enzyme, disproportionating enzyme-1 (DPE1), combines two maltotriose molecules. From this chain, a glucose molecule is released. Now, BAM can release another maltose molecule from the remaining chain. This cycle repeats until starch is degraded completely. If BAM comes close to the phosphorylated branching point of the glucose chain, it can no longer release maltose. In order for the phosphorylated chain to be degraded, the enzyme isoamylase (ISA) is required.
The products of starch degradation are to the major part maltose and to a less extensive part glucose. These molecules are now exported from the plastid to the cytosol. Maltose is exported via the maltose transporter. If this transporter is mutated (MEX1-mutant), maltose accumulates in the plastid. Glucose is exported via the plastidic glucose translocator (pGlcT). Now, these two sugars act as a precursor for sucrose synthesis. Sucrose can the be used in the oxidative pentose phosphate pathway in the mitochondria, in order to generate ATP at night.
Properties.
Structure.
While amylose was traditionally thought to be completely unbranched, it is now known that some of its molecules contain a few branch points. Although in absolute mass only about one quarter of the starch granules in plants consist of amylose, there are about 150 times more amylose molecules than amylopectin molecules. Amylose is a much smaller molecule than amylopectin.
Starch molecules arrange themselves in the plant in semi-crystalline granules. Each plant species has a unique starch granular size: rice starch is relatively small (about 2μm) while potato starches have larger granules (up to 100μm).
Starch becomes soluble in water when heated. The granules swell and burst, the semi-crystalline structure is lost and the smaller amylose molecules start leaching out of the granule, forming a network that holds water and increasing the mixture's viscosity. This process is called starch gelatinization. During cooking, the starch becomes a paste and increases further in viscosity. During cooling or prolonged storage of the paste, the semi-crystalline structure partially recovers and the starch paste thickens, expelling water. This is mainly caused by retrogradation of the amylose. This process is responsible for the hardening of bread or staling, and for the water layer on top of a starch gel (syneresis).
Some cultivated plant varieties have pure amylopectin starch without amylose, known as "waxy starches". The most used is waxy maize, others are glutinous rice and waxy potato starch. Waxy starches have less retrogradation, resulting in a more stable paste. High amylose starch, amylomaize, is cultivated for the use of its gel strength and for use as a resistant starch (a starch that resists digestion) in food products.
Synthetic amylose made from cellulose has a well-controlled degree of polymerization. Therefore, it can be used as a potential drug deliver carrier.
Hydrolysis.
The enzymes that break down or hydrolyze starch into the constituent sugars are known as amylases.
Alpha-amylases are found in plants and in animals. Human saliva is rich in amylase, and the pancreas also secretes the enzyme. Individuals from populations with a high-starch diet tend to have more amylase genes than those with low-starch diets;
Beta-amylase cuts starch into maltose units. This process is important in the digestion of starch and is also used in brewing, where amylase from the skin of seed grains is responsible for converting starch to maltose (Malting, Mashing).
Dextrinization.
If starch is subjected to dry heat, it breaks down to form dextrins, also called "pyrodextrins" in this context. This break down process is known as dextrinization. (Pyro)dextrins are mainly yellow to brown in color and dextrinization is partially responsible for the browning of toasted bread.
Chemical tests.
Iodine solution is used to test for starch; a dark blue color indicates the presence of starch. The details of this reaction are not yet fully known, but it is thought that the iodine (I3− and I5− ions) fit inside the coils of amylose, the charge transfers between the iodine and the starch, and the energy level spacings in the resulting complex correspond to the absorption spectrum in the visible light region. The strength of the resulting blue color depends on the amount of amylose present. Waxy starches with little or no amylose present will color red.
Starch indicator solution consisting of water, starch and iodine is often used in redox titrations: in the presence of an oxidizing agent the solution turns blue, in the presence of reducing agent the blue color disappears because triiodide (I3−) ions break up into three iodide ions, disassembling the starch-iodine complex.
A 0.3% w/w solution is the standard concentration for a starch indicator. It is made by adding 3 grams of soluble starch to 1 liter of heated water; the solution is cooled before use (starch-iodine complex becomes unstable at temperatures above 35 °C).
Each species of plant has a unique type of starch granules in granular size, shape and crystallization pattern. Under the microscope, starch grains stained with iodine illuminated from behind with polarized light show a distinctive Maltese cross effect (also known as extinction cross and birefringence).
Food.
Starch is the most common carbohydrate in the human diet and is contained in many staple foods. The major sources of starch intake worldwide are the cereals (rice, wheat, and maize) and the root vegetables (potatoes and cassava). Many other starchy foods are grown, some only in specific climates, including acorns, arrowroot, arracacha, bananas, barley, breadfruit, buckwheat, canna, colacasia, katakuri, kudzu, malanga, millet, oats, oca, polynesian arrowroot, sago, sorghum, sweet potatoes, rye, taro, chestnuts, water chestnuts and yams, and many kinds of beans, such as favas, lentils, mung beans, peas, and chickpeas.
Widely used prepared foods containing starch are bread, pancakes, cereals, noodles, pasta, porridge and tortilla.
Digestive enzymes have problems digesting crystalline structures. Raw starch will digest poorly in the duodenum and small intestine, while bacterial degradation will take place mainly in the colon. When starch is cooked, the digestibility is increased. Hence, before humans started using fire, eating grains was not a very useful way to get energy.
Starch gelatinization during cake baking can be impaired by sugar competing for water, preventing gelatinization and improving texture.
Starch industry.
The starch industry extracts and refines starches from seeds, roots and tubers, by wet grinding, washing, sieving and drying. Today, the main commercial refined starches are cornstarch, tapioca, wheat, rice and potato starch. To a lesser extent, sources include rice, sweet potato, sago and mung bean. Historically, Florida arrowroot was also commercialized. To this day, starch is extracted from more than 50 types of plants.
Untreated starch requires heat to thicken or gelatinize. When a starch is pre-cooked, it can then be used to thicken instantly in cold water. This is referred to as a pregelatinized starch.
Starch sugars.
Starch can be hydrolyzed into simpler carbohydrates by acids, various enzymes, or a combination of the two. The resulting fragments are known as dextrins. The extent of conversion is typically quantified by dextrose equivalent (DE), which is roughly the fraction of the glycosidic bonds in starch that have been broken.
These starch sugars are by far the most common starch based food ingredient and are used as sweetener in many drinks and foods. They include:
Modified starches.
A modified starch is a starch that has been chemically modified to allow the starch to function properly under conditions frequently encountered during processing or storage, such as high heat, high shear, low pH, freeze/thaw and cooling.
The modified food starches are E coded according to the International Numbering System for Food Additives (INS):
INS 1400, 1401, 1402, 1403 and 1405 are in the EU food ingredients without an E-number. Typical modified starches for technical applications are cationic starches, hydroxyethyl starch and carboxymethylated starches.
Use as food additive.
As an additive for food processing, food starches are typically used as thickeners and stabilizers in foods such as puddings, custards, soups, sauces, gravies, pie fillings, and salad dressings, and to make noodles and pastas.
Gummed sweets such as jelly beans and wine gums are not manufactured using a mold in the conventional sense. A tray is filled with native starch and leveled. A positive mold is then pressed into the starch leaving an impression of 1,000 or so jelly beans. The jelly mix is then poured into the impressions and put into a stove to set. This method greatly reduces the number of molds that must be manufactured.
Use in pharmaceutical industry.
In the pharmaceutical industry, starch is also used as an excipient, as tablet disintegrant or as binder.
Resistant starch.
Resistant starch is starch that escapes digestion in the small intestine of healthy individuals.
High amylose starch from corn has a higher gelatinization temperature than other types of starch and retains its resistant starch content through baking, mild extrusion and other food processing techniques. It is used as an insoluble dietary fiber in processed foods such as bread, pasta, cookies, crackers, pretzels and other low moisture foods. It is also utilized as a dietary supplement for its health benefits. Published studies have shown that Type 2 resistant corn helps to improve insulin sensitivity, increases satiety and improves markers of colonic function.
It has been suggested that resistant starch contributes to the health benefits of intact whole grains.
Industrial applications.
Papermaking.
Papermaking is the largest non-food application for starches globally, consuming millions of metric tons annually. In a typical sheet of copy paper for instance, the starch content may be as high as 8%. Both chemically modified and unmodified starches are used in papermaking. In the wet part of the papermaking process, generally called the "wet-end", the starches used are cationic and have a positive charge bound to the starch polymer. These starch derivatives associate with the anionic or negatively charged paper fibers / cellulose and inorganic fillers. Cationic starches together with other retention and internal sizing agents help to give the necessary strength properties to the paper web formed in the papermaking process (wet strength), and to provide strength to the final paper sheet (dry strength).
In the dry end of the papermaking process, the paper web is rewetted with a starch based solution. The process is called surface sizing. Starches used have been chemically, or enzymatically depolymerized at the paper mill or by the starch industry (oxidized starch). The size - starch solutions are applied to the paper web by means of various mechanical presses (size presses). Together with surface sizing agents the surface starches impart additional strength to the paper web and additionally provide water hold out or "size" for superior printing properties. Starch is also used in paper coatings as one of the binders for the coating formulations which include a mixture of pigments, binders and thickeners. Coated paper has improved smoothness, hardness, whiteness and gloss and thus improves printing characteristics.
Corrugated board adhesives.
Corrugated board adhesives are the next largest application of non-food starches globally. Starch glues are mostly based on unmodified native starches, plus some additive such as borax and caustic soda. Part of the starch is gelatinized to carry the slurry of uncooked starches and prevent sedimentation. This opaque glue is called a SteinHall adhesive. The glue is applied on tips of the fluting. The fluted paper is pressed to paper called liner. This is then dried under high heat, which causes the rest of the uncooked starch in glue to swell/gelatinize. This gelatinizing makes the glue a fast and strong adhesive for corrugated board production.
Clothing starch.
Clothing or laundry starch is a liquid that is prepared by mixing a vegetable starch in water (earlier preparations also had to be boiled), and is used in the laundering of clothes. Starch was widely used in Europe in the 16th and 17th centuries to stiffen the wide collars and ruffs of fine linen which surrounded the necks of the well-to-do. During the 19th century and early 20th century, it was stylish to stiffen the collars and sleeves of men's shirts and the ruffles of girls' petticoats by applying starch to them as the clean clothes were being ironed. Aside from the smooth, crisp edges it gave to clothing, it served practical purposes as well. Dirt and sweat from a person's neck and wrists would stick to the starch rather than to the fibers of the clothing, and would easily wash away along with the starch. After each laundering, the starch would be reapplied. Today, the product is sold in aerosol cans for home use.
Other.
Another large non-food starch application is in the construction industry, where starch is used in the gypsum wall board manufacturing process. Chemically modified or unmodified starches are added to the stucco containing primarily gypsum. Top and bottom heavyweight sheets of paper are applied to the formulation, and the process is allowed to heat and cure to form the eventual rigid wall board. The starches act as a glue for the cured gypsum rock with the paper covering, and also provide rigidity to the board.
Starch is used in the manufacture of various adhesives or glues for book-binding, wallpaper adhesives, paper sack production, tube winding, gummed paper, envelope adhesives, school glues and bottle labeling. Starch derivatives, such as yellow dextrins, can be modified by addition of some chemicals to form a hard glue for paper work; some of those forms use borax or soda ash, which are mixed with the starch solution at 50 – to create a very good adhesive. Sodium silicate can be added to reinforce these formula.

</doc>
<doc id="27712" url="http://en.wikipedia.org/wiki?curid=27712" title="Sugar">
Sugar

Sugar is the generalized name for sweet, short-chain, soluble carbohydrates, many of which are used in food. They are carbohydrates, composed of carbon, hydrogen, and oxygen. There are various types of sugar derived from different sources. Simple sugars are called monosaccharides and include glucose (also known as dextrose), fructose and galactose. The table or granulated sugar most customarily used as food is sucrose, a disaccharide. (In the body, sucrose hydrolyses into fructose and glucose.) Other disaccharides include maltose and lactose. Longer chains of sugars are called oligosaccharides. Chemically-different substances may also have a sweet taste, but are not classified as sugars. Some are used as lower-calorie food substitutes for sugar described as artificial sweeteners.
Sugars are found in the tissues of most plants, but are present in sufficient concentrations for efficient extraction only in sugarcane and sugar beet. Sugarcane refers to any of several species of giant grass in the genus "Saccharum" that have been cultivated in tropical climates in South Asia and Southeast Asia since ancient times. A great expansion in its production took place in the 18th century with the establishment of sugar plantations in the West Indies and Americas. This was the first time that sugar became available to the common people, who had previously had to rely on honey to sweeten foods. Sugar beet, a cultivated variety of "Beta vulgaris", is grown as a root crop in cooler climates and became a major source of sugar in the 19th century when methods for extracting the sugar became available. Sugar production and trade have changed the course of human history in many ways, influencing the formation of colonies, the perpetuation of slavery, the transition to indentured labour, the migration of peoples, wars between sugar-trade–controlling nations in the 19th century, and the ethnic composition and political structure of the new world.
The world produced about 168 million tonnes of sugar in 2011. The average person consumes about 24 kg of sugar each year (33.1 kg in industrialised countries), equivalent to over 260 food calories per person, per day.
Since the latter part of the twentieth century, it has been questioned whether a diet high in sugars, especially refined sugars, is good for human health. Sugar has been linked to obesity, and suspected of, or fully implicated as a cause in the occurrence of diabetes, cardiovascular disease, dementia, macular degeneration, and tooth decay. Numerous studies have been undertaken to try to clarify the position, but with varying results, mainly because of the difficulty of finding populations for use as controls that do not consume or are largely free of any sugar consumption.
Etymology.
The etymology reflects the spread of the commodity. The English word "sugar" originates from the Arabic word سكر "sukkar", which came from the Persian شکر "shekar",
itself derived from Sanskrit "शर्करा" "śarkarā", which originated from Tamil "சக்கரை" "Sakkarai". It most probably came to England by way of Italian merchants. The contemporary Italian word is "zucchero", whereas the Spanish and Portuguese words, "azúcar" and "açúcar" respectively, have kept a trace of the Arabic definite article. The Old French word is "zuchre" – contemporary French "sucre". The earliest Greek word attested is σάκχαρις ("sákkʰaris"). A satisfactory "pedigree" explaining the spread of the word has yet to be done. The English word "jaggery", a coarse brown sugar made from date palm sap or sugar cane juice, has a similar etymological origin; Portuguese "xagara" or "jagara", derived from Malayalam "chakkarā" from the Sanskrit "śarkarā".
History.
Ancient times and Middle Ages.
Sugar has been produced in the Indian subcontinent since ancient times. It was not plentiful or cheap in early times and honey was more often used for sweetening in most parts of the world. Originally, people chewed raw sugarcane to extract its sweetness. Sugarcane was a native of tropical South Asia and Southeast Asia. Different species seem to have originated from different locations with "Saccharum barberi" originating in India and "S. edule" and "S. officinarum" coming from New Guinea. One of the earliest historical references to sugarcane is in Chinese manuscripts dating back to 8th century BC that state that the use of sugarcane originated in India.
Sugar remained relatively unimportant until the Indians discovered methods of turning sugarcane juice into granulated crystals that were easier to store and to transport. Crystallized sugar was discovered by the time of the Imperial Guptas, around the 5th century AD. In the local Indian language, these crystals were called "khanda" (Devanagari:खण्ड,Khaṇḍa), which is the source of the word "candy".
Indian sailors, who carried clarified butter and sugar as supplies, introduced knowledge of sugar on the various trade routes they travelled. Buddhist monks, as they travelled around, brought sugar crystallization methods to China. During the reign of Harsha (r. 606–647) in North India, Indian envoys in Tang China taught methods of cultivating sugarcane after Emperor Taizong of Tang (r. 626–649) made known his interest in sugar. China then established its first sugarcane plantations in the seventh century. Chinese documents confirm at least two missions to India, initiated in 647 AD, to obtain technology for sugar refining. In South Asia, the Middle East and China, sugar became a staple of cooking and desserts.
The triumphant progress of Alexander the Great was halted on the banks of the Indus River by the refusal of his troops to go further east. They saw people in the Indian subcontinent growing sugarcane and making "granulated, salt-like sweet powder", locally called "Sharkara" (Devanagari:शर्करा,Śarkarā), pronounced as saccharum (ζάκχαρι). On their return journey, the Macedonian soldiers carried the "honey-bearing reeds" home with them. Sugarcane remained a little-known crop in Europe for over a millennium, sugar a rare commodity, and traders in sugar wealthy.
Crusaders brought sugar home with them to Europe after their campaigns in the Holy Land, where they encountered caravans carrying "sweet salt". Early in the 12th century, Venice acquired some villages near Tyre and set up estates to produce sugar for export to Europe, where it supplemented honey, which had previously been the only available sweetener.
Crusade chronicler William of Tyre, writing in the late 12th century, described sugar as "very necessary for the use and health of mankind". In the 15th century, Venice was the chief sugar refining and distribution centre in Europe.
Modern history.
In August 1492, Christopher Columbus stopped at La Gomera in the Canary Islands, for wine and water, intending to stay only four days. He became romantically involved with the governor of the island, Beatriz de Bobadilla y Ossorio, and stayed a month. When he finally sailed, she gave him cuttings of sugarcane, which became the first to reach the New World.
The Portuguese took sugar to Brazil. By 1540, there were 800 cane sugar mills in Santa Catarina Island and there were another 2,000 on the north coast of Brazil, Demarara, and Surinam. The first sugar harvest happened in Hispaniola in 1501; and, many sugar mills had been constructed in Cuba and Jamaica by the 1520s.
Sugar was a luxury in Europe prior to the 18th century, when it became more widely available. It then became popular and by the 19th century, sugar came to be considered a necessity. This evolution of taste and demand for sugar as an essential food ingredient unleashed major economic and social changes. It drove, in part, colonization of tropical islands and nations where labor-intensive sugarcane plantations and sugar manufacturing could thrive. The demand for cheap labor to perform the hard work involved in its cultivation and processing increased the demand for the slave trade from Africa (in particular West Africa). After slavery was abolished, there was high demand for indentured laborers from South Asia (in particular India). Millions of slave and indentured laborers were brought into the Caribbean and the Americas, Indian Ocean colonies, southeast Asia, Pacific Islands, and East Africa and Natal. The modern ethnic mix of many nations that have been settled in the last two centuries has been influenced by the demand for sugar.
Sugar also led to some industrialization of former colonies. For example, Lieutenant J. Paterson, of the Bengal establishment, persuaded the British Government that sugar cane could be cultivated in British India with many advantages and at less expense than in the West Indies. As a result, sugar factories were established in Bihar in eastern India.
During the Napoleonic Wars, sugar beet production increased in continental Europe because of the difficulty of importing sugar when shipping was subject to blockade. By 1880, the sugar beet was the main source of sugar in Europe. It was cultivated in Lincolnshire and other parts of England, although the United Kingdom continued to import the main part of its sugar from its colonies.
Until the late nineteenth century, sugar was purchased in loaves, which had to be cut using implements called Sugar nips. In later years, granulated sugar was more usually sold in bags.
Sugar cubes were produced in the nineteenth century. The first inventor of a process to make sugar in cube form was Moravian Jakub Kryštof Rad, director of a sugar company in Dačice. He began sugar cube production after being granted a five-year patent for the invention on January 23, 1843. Henry Tate of Tate & Lyle was another early manufacturer of sugar cubes at his refineries in Liverpool and London. Tate purchased a patent for sugar cube manufacture from German Eugen Langen, who in 1872 had invented a different method of processing of sugar cubes.
Chemistry.
Scientifically, "sugar" loosely refers to a number of carbohydrates, such as monosaccharides, disaccharides, or oligosaccharides. Monosaccharides are also called "simple sugars," the most important being glucose. Almost all sugars have the formula CnH2nOn (n is between 3 and 7). Glucose has the molecular formula C6H12O6. The names of typical sugars end with "ose", as in "glucose", "dextrose", and "fructose". Sometimes such words may also refer to any types of carbohydrates soluble in water. The acyclic mono- and disaccharides contain either aldehyde groups or ketone groups. These carbon-oxygen double bonds (C=O) are the reactive centers. All saccharides with more than one ring in their structure result from two or more monosaccharides joined by glycosidic bonds with the resultant loss of a molecule of water (H2O) per bond.
Monosaccharides in a closed-chain form can form glycosidic bonds with other monosaccharides, creating disaccharides (such as sucrose) and polysaccharides (such as starch). Enzymes must hydrolyze or otherwise break these glycosidic bonds before such compounds become metabolized. After digestion and absorption the principal monosaccharides present in the blood and internal tissues include glucose, fructose, and galactose. Many pentoses and hexoses can form ring structures. In these closed-chain forms, the aldehyde or ketone group remains non-free, so many of the reactions typical of these groups cannot occur. Glucose in solution exists mostly in the ring form at equilibrium, with less than 0.1% of the molecules in the open-chain form.
Natural polymers of sugars.
Biopolymers of sugars are common in nature. Through photosynthesis, plants produce glyceraldehyde-3-phosphate (G3P), a phosphated 3-carbon sugar that is used by the cell to make monosaccharides such as glucose (C6H12O6) or (as in cane and beet) sucrose (C12H22O11). Monosaccharides may be further converted into structural polysaccharides such as cellulose and pectinfor cell wall construction or into energy reserves in the form of storage polysaccharides such as starch or inulin. Starch, consisting of two different polymers of glucose, is a readily degradable form of chemical energy stored by cells, and can be converted to other types of energy. Another polymer of glucose is cellulose, which is a linear chain composed of several hundred or thousand glucose units. It is used by plants as a structural component in their cell walls. Humans can digest cellulose only to a very limited extent, though ruminants can do so with the help of symbiotic bacteria in their gut. DNA and RNA are built up of the monosaccharides deoxyribose and ribose, respectively. Deoxyribose has the formula C5H10O4 and ribose the formula C5H10O5.
Flammability.
Sugars are organic substances that burn easily upon exposure to an open flame. Because of this, the handling of sugars presents a risk for dust explosion. The 2008 Georgia sugar refinery explosion, which resulted in 14 deaths, 40 injured, and more than half of the facility's destruction, was caused by the ignition of sugar dust.
Types of sugar.
Monosaccharides.
Glucose, fructose and galactose are all simple sugars, monosaccharides, with the general formula C6H12O6. They have five hydroxyl groups (−OH) and a carbonyl group (C=O) and are cyclic when dissolved in water. They each exist as several isomers with dextro- and laevo-rotatory forms that cause polarized light to diverge to the right or the left.
Glucose, dextrose or grape sugar occurs naturally in fruits and plant juices and is the primary product of photosynthesis. Most ingested carbohydrates are converted into glucose during digestion and it is the form of sugar that is transported around the bodies of animals in the bloodstream. It can be manufactured from starch by the addition of enzymes or in the presence of acids. Glucose syrup is a liquid form of glucose that is widely used in the manufacture of foodstuffs. It can be manufactured from starch by enzymatic hydrolysis.
Fructose or fruit sugar occurs naturally in fruits, some root vegetables, cane sugar and honey and is the sweetest of the sugars. It is one of the components of sucrose or table sugar. It is used as a high-fructose syrup, which is manufactured from hydrolyzed corn starch that has been processed to yield corn syrup, with enzymes then added to convert part of the glucose into fructose.
In general, galactose does not occur in the free state but is a constituent with glucose of the disaccharide lactose or milk sugar. It is less sweet than glucose. It is a component of the antigens found on the surface of red blood cells that determine blood groups.
Disaccharides.
Sucrose, maltose, and lactose are all compound sugars, disaccharides, with the general formula C12H22O11. They are formed by the combination of two monosaccharide molecules with the exclusion of a molecule of water.
Sucrose is found in the stems of sugar cane and roots of sugar beet. It also occurs naturally alongside fructose and glucose in other plants, in particular fruits and some roots such as carrots. The different proportions of sugars found in these foods determines the range of sweetness experienced when eating them. A molecule of sucrose is formed by the combination of a molecule of glucose with a molecule of fructose. After being eaten, sucrose is split into its constituent parts during digestion by a number of enzymes known as sucrases.
Maltose is formed during the germination of certain grains, the most notable being barley, which is converted into malt, the source of the sugar's name. A molecule of maltose is formed by the combination of two molecules of glucose. It is less sweet than glucose, fructose or sucrose. It is formed in the body during the digestion of starch by the enzyme amylase and is itself broken down during digestion by the enzyme maltase.
Lactose is the naturally occurring sugar found in milk. A molecule of lactose is formed by the combination of a molecule of galactose with a molecule of glucose. It is broken down when consumed into its constituent parts by the enzyme lactase during digestion. Children have this enzyme but some adults no longer form it and they are unable to digest lactose.
Sources.
The sugar contents of common fruits and vegetables are presented in Table 1.
All data with a unit of g (gram) are based on 100 g of a food item.
The fructose/glucose ratio is calculated by dividing the sum of free fructose plus half sucrose by the sum of free glucose plus half sucrose.
Production.
Sugar beet.
Sugar beet ("Beta vulgaris") is a biennial plant in the Family Amaranthaceae, the tuberous root of which contains a high proportion of sucrose. It is cultivated in temperate regions with adequate rainfall and requires a fertile soil. The crop is harvested mechanically in the autumn and the crown of leaves and excess soil removed. The roots do not deteriorate rapidly and may be left in a clamp in the field for some weeks before being transported to the processing plant. Here the crop is washed and sliced and the sugar extracted by diffusion. Milk of lime is added to the raw juice and carbonatated in a number of stages in order to purify it. Water is evaporated by boiling the syrup under a vacuum. The syrup is then cooled and seeded with sugar crystals. The white sugar that crystallizes out can be separated in a centrifuge and dried. It requires no further refining.
Sugarcane.
Sugarcane ("Saccharum spp.") is a perennial grass in the family Poaceae. It is cultivated in tropical and sub-tropical regions for the sucrose that is found in its stems. It requires a frost-free climate with sufficient rainfall during the growing season to make full use of the plant's great growth potential. The crop is harvested mechanically or by hand, chopped into lengths and conveyed rapidly to the processing plant. Here, it is either milled and the juice extracted with water or extracted by diffusion. The juice is then clarified with lime and heated to kill enzymes. The resulting thin syrup is concentrated in a series of evaporators, after which further water is removed by evaporation in vacuum containers. The resulting supersaturated solution is seeded with sugar crystals and the sugar crystallizes out and is separated from the fluid and dried. Molasses is a by-product of the process and the fiber from the stems, known as bagasse, is burned to provide energy for the sugar extraction process. The crystals of raw sugar have a sticky brown coating and either can be used as they are or can be bleached by sulfur dioxide or can be treated in a carbonatation process to produce a whiter product.
Refining.
Refined sugar is made from raw sugar that has undergone a refining process to remove the molasses. Raw sugar is a sucrose which is synthesized from sugar cane or sugar beet and cannot immediately be consumed before going through the refining process to produce refined sugar or white sugar.
The sugar may be transported in bulk to the country where it will be used and the refining process often takes place there. The first stage is known as affination and involves immersing the sugar crystals in a concentrated syrup that softens and removes the sticky brown coating without dissolving them. The crystals are then separated from the liquor and dissolved in water. The resulting syrup is treated either by a carbonatation or by a phosphatation process. Both involve the precipitation of a fine solid in the syrup and when this is filtered out, many of the impurities are removed at the same time. Removal of colour is achieved by using either a granular activated carbon or an ion-exchange resin. The sugar syrup is concentrated by boiling and then cooled and seeded with sugar crystals, causing the sugar to crystallize out. The liquor is spun off in a centrifuge and the white crystals are dried in hot air and ready to be packaged or used. The surplus liquor is made into refiners' molasses.
The International Commission for Uniform Methods of Sugar Analysis sets standards for the measurement of the purity of refined sugar, known as ICUMSA numbers; lower numbers indicate a higher level of purity in the refined sugar.
Refined sugar is widely used for industrial needs for higher quality. Refined sugar is purer (ICUMSA below 300) than raw sugar (ICUMSA over 1,500). The level of purity associated with the colors of sugar, expressed by standard number ICUMSA (International Commission for Uniform Methods of sugar Analysis), the smaller ICUMSA numbers indicate that higher purity of sugar.
Producing countries.
The five largest producers of sugar in 2011 were Brazil, India, the European Union, China and Thailand. In the same year, the largest exporter of sugar was Brazil, distantly followed by Thailand, Australia and India. The largest importers were the European Union, United States and Indonesia. At present, Brazil has the highest per capita consumption of sugar, followed by Australia, Thailand, and the European Union.
Forms and uses.
Granulated sugars are used at the table to sprinkle on foods and to sweeten hot drinks and in home baking to add sweetness and texture to cooked products. They are also used as a preservative to prevent micro-organisms from growing and perishable food from spoiling as in jams, marmalades, and candied fruits.
Milled sugars (known as powdered sugar and confectioner's sugar) are ground to a fine powder. They are used as icing sugar, for dusting foods and in baking and confectionery.
Screened sugars are crystalline products separated according to the size of the grains. They are used for decorative table sugars, for blending in dry mixes and in baking and confectionery.
Brown sugars are granulated sugars with the grains coated in molasses to produce a light, dark, or demerara sugar. They are used in baked goods, confectionery, and toffees.
Sugar cubes (sometimes called sugar lumps) are white or brown granulated sugars lightly steamed and pressed together in block shape. They are used to sweeten drinks.
Liquid sugars are strong syrups consisting of 67% granulated sugar dissolved in water. They are used in the food processing of a wide range of products including beverages, ice cream, jams, and hard candy.
Invert sugars and syrups are blended to manufacturers specifications and are used in breads, cakes, and beverages for adjusting sweetness, aiding moisture retention and avoiding crystallization of sugars.
Syrups and treacles are dissolved invert sugars heated to develop the characteristic flavors. Treacles have added molasses. They are used in a range of baked goods and confectionery including toffees and licorice.
Low-calorie sugars and sweeteners are often made of maltodextrin with added sweeteners. Maltodextrin is an easily digestible synthetic polysaccharide consisting of short chains of glucose molecules and is made by the partial hydrolysis of starch. The added sweeteners are often aspartame, saccharin, stevia, or sucralose.
Polyols are sugar alcohols and are used in chewing gums where a sweet flavor is required that lasts for a prolonged time in the mouth.
In winemaking, fruit sugars are converted into alcohol by a fermentation process. If the must formed by pressing the fruit has a low sugar content, additional sugar may be added to raise the alcohol content of the wine in a process called chaptalization. In the production of sweet wines, fermentation may be halted before it has run its full course, leaving behind some residual sugar that gives the wine its sweet taste.
Molasses is commonly used to make rum, and sugar byproducts are used to make ethanol for fuel.
Consumption.
In most parts of the world, sugar is an important part of the human diet, making food more palatable and providing food energy. After cereals and vegetable oils, sugar derived from sugar cane and beet provided more kilocalories per capita per day on average than other food groups. According to the FAO, an average of 24 kg of sugar, equivalent to over 260 food calories per day, was consumed annually per person of all ages in the world in 1999. Even with rising human populations, sugar consumption is expected to increase to 25.1 kg per person per year by 2015.
Data collected in multiple nationwide surveys between 1999 and 2008 show that the intake of added sugars has declined by 24 percent with declines occurring in all age, ethnic and income groups.
The per capita consumption of refined sugar in the United States has varied between 27 and in the last 40 years. In 2008, American per capita total consumption of sugar and sweeteners, exclusive of artificial sweeteners, equalled 61.9 kg per year. This consisted of 29.65 kg pounds of refined sugar and 31 kg pounds of corn-derived sweeteners per person.
Health effects.
Some studies involving the health impact of sugars are effectively inconclusive. The WHO and FAO meta studies have shown directly contrasting impacts of sugar in refined and unrefined forms and since most studies do not use a population that do not consume any "free sugars" at all, the baseline is effectively flawed. Hence, there are articles such as "Consumer Reports on Health" that stated in 2008, "Some of the supposed dietary dangers of sugar have been overblown. Many studies have debunked the idea that it causes hyperactivity, for example".
Blood glucose levels.
It used to be believed that sugar raised blood glucose levels more quickly than did starch because of its simpler chemical structure. However, it turned out that white bread or French fries have the same effect on blood sugar as pure glucose, while fructose, although a simple carbohydrate, has a minimal effect on blood sugar. As a result, as far as blood sugar is concerned, carbohydrates are classified according to their glycemic index, a system for measuring how quickly a food that is eaten raises blood sugar levels, and glycemic load, which takes into account both the glycemic index and the amount of carbohydrate in the food. This has led to carbohydrate counting, a method used by diabetics for planning their meals.
Obesity and diabetes.
Controlled trials have now shown unequivocally that consumption of sugar-sweetened beverages increases body weight and body fat, and that replacement of sugar by artificial sweeteners reduces weight.
Studies on the link between sugars and diabetes are inconclusive, with some suggesting that eating excessive amounts of sugar does not increase the risk of diabetes, although the extra calories from consuming large amounts of sugar can lead to obesity, which may itself increase the risk of developing this metabolic disease. 
Other studies show correlation between refined sugar (free sugar) consumption and the onset of diabetes, and negative correlation with the consumption of fiber. These included a 2010 meta-analysis of eleven studies involving 310,819 participants and 15,043 cases of type 2 diabetes. 
This found that "SSBs (sugar-sweetened beverages) may increase the risk of metabolic syndrome and type 2 diabetes not only through obesity but also by increasing dietary glycemic load, leading to insulin resistance, β-cell dysfunction, and inflammation". As an overview to consumption related to chronic disease and obesity, the World Health Organization's independent meta-studies specifically distinguish free sugars ("all monosaccharides and disaccharides added to foods by the manufacturer, cook or consumer, plus sugars naturally present in honey, syrups and fruit juices") from sugars occurring naturally in food. The reports prior to 2000 set the limits for free sugars at a maximum of 10% of carbohydrate intake, measured by energy, rather than mass, and since 2002 have aimed for a level across the entire population of less than 10%. The consultation committee recognized that this goal is "controversial. However, the Consultation considered that the studies showing no effect of free sugars on excess weight have limitations".
Cardiovascular disease.
Studies in animals have suggested that chronic consumption of refined sugars can contribute to metabolic and cardiovascular dysfunction. Some experts have suggested that refined fructose is more damaging than refined glucose in terms of cardiovascular risk. Cardiac performance has been shown to be impaired by switching from a carbohydrate diet including fiber to a high-carbohydrate diet.
Switching from saturated fatty acids to carbohydrates with high glycemic index values shows a statistically-significant increase in the risk of myocardial infarction. Other studies have shown that the risk of developing coronary heart disease is decreased by adopting a diet high in polyunsaturated fatty acids but low in sugar, whereas a low-fat, high-carbohydrate diet brings no reduction. This suggests that consuming a diet with a high glycemic load typical of the "junk food" diet is strongly associated with an increased risk of developing coronary heart disease.
The consumption of added sugars has been positively associated with multiple measures known to increase cardiovascular disease risk amongst adolescents as well as adults.
Studies are suggesting that the impact of refined carbohydrates or high glycemic load carbohydrates are more significant than the impact of saturated fatty acids on cardiovascular disease.
A high dietary intake of sugar (in this case, sucrose or disaccharide) can substantially increase the risk of heart and vascular diseases. According to a Swedish study of 4301 people undertaken by Lund University and Malmö University College, sugar was associated with higher levels of bad blood lipids, causing a high level of small and medium low-density lipoprotein (LDL) and reduced high-density lipoprotein (HDL). In contrast, the amount of fat eaten did not affect the level of blood fats. Incidentally quantities of alcohol and protein were linked to an increase in the good HDL blood fat.
Alzheimer's disease.
Claims have been made of a sugar–Alzheimer's disease connection, but debate continues over whether cognitive decline is attributable to dietary fructose or to overall energy intake.
Tooth decay.
In regard to contributions to tooth decay, the role of free sugars is also recommended to be below an absolute maximum of 10% of energy intake, with a minimum of zero. There is "convincing evidence from human intervention studies, epidemiological studies, animal studies and experimental studies, for an association between the amount and frequency of free sugars intake and dental caries" while other sugars (complex carbohydrate) consumption is normally associated with a lower rate of dental caries. Lower rates of tooth decay have been seen in individuals with hereditary fructose intolerance.
Also, studies have shown that the consumption of sugar and starch have different impacts on oral health with the ingestion of starchy foods and fresh fruit being associated with low levels of dental caries.
Addiction.
Sugar addiction is the term for the relationship between sugar and the various aspects of food addiction including "bingeing, withdrawal, craving and cross-sensitization". Some scientists assert that consumption of sweets or sugar could have a heroin addiction-like effect.
Hyperactivity.
There is a common notion that sugar leads to hyperactivity, in particular in children, but studies and meta-studies tend to disprove this. Some articles and studies do refer to the increasing evidence supporting the links between refined sugar and hyperactivity. The WHO FAO meta-study suggests that such inconclusive results are to be expected when some studies do not effectively segregate or control for free sugars as opposed to sugars still in their natural form (entirely unrefined) while others do. One study followed thirty-five 5-to-7-year-old boys who were reported by their mothers to be behaviorally "sugar-sensitive". They were randomly assigned to experimental and control groups. In the experimental group, mothers were told that their children were fed sugar, and, in the control group, mothers were told that their children received a placebo. In fact, all children received the placebo, but mothers in the sugar expectancy condition rated their children as significantly more hyperactive. This result suggests that the real effect of sugar is that it increases worrying among parents with preconceived notions.
Measurements.
Different culinary sugars have different densities due to differences in particle size and inclusion of moisture.
Domino Sugar gives the following weight to volume conversions (in United States customary units):
Another source gives different values for the bulk densities:

</doc>
<doc id="27715" url="http://en.wikipedia.org/wiki?curid=27715" title="Saint Louis">
Saint Louis

Saint Louis, Saint-Louis or St. Louis may refer to:

</doc>
<doc id="27717" url="http://en.wikipedia.org/wiki?curid=27717" title="Salma Hayek">
Salma Hayek

Salma Hayek Pinault (born September 2, 1966) is a Mexican and American film actress, director, and producer. She began her career in Mexico starring in the telenovela "Teresa" and went on to star in the film "El Callejón de los Milagros" ("Miracle Alley") for which she was nominated for an Ariel Award.
In 1991 Hayek moved to Hollywood and came to prominence with roles in Hollywood movies such as "Desperado" (1995), "Dogma" (1999), and "Wild Wild West" (1999).
Her breakthrough role was in the 2002 film "Frida" as Mexican painter Frida Kahlo for which she was nominated in the category of Best Actress for an Academy Award, BAFTA Award, Screen Actors Guild Award, and Golden Globe Award. This movie received widespread attention and was a critical and commercial success. She won a Daytime Emmy Award for Outstanding Directing in a Children/Youth/Family Special in 2004 for "The Maldonado Miracle" and received an Emmy Award nomination for Outstanding Guest Actress in a Comedy Series in 2007 after guest-starring in the ABC television comedy-drama "Ugly Betty." She also guest-starred on the NBC comedy series "30 Rock" from 2009 to 2013.
Hayek's recent films include "Grown Ups" (2010), "Grown Ups 2" (2013), and "Puss in Boots" (2011).
Early life.
Hayek was born Salma Hayek Jiménez in Coatzacoalcos, Veracruz, Mexico. Her younger brother, Sami (born 1972), is a furniture designer. She is the daughter of Diana Jiménez Medina, an opera singer and talent scout. Her father, Sami Hayek Dominguez who once ran for mayor of Coatzacoalcos, was either an oil company executive or the owner of an industrial-equipment firm.
Her paternal grandfather was Lebanese. She also has Spanish ancestry. Her first given name, Salma, is Arabic for "peace" or "calm". Raised in a wealthy, devout Roman Catholic family, she was sent to the Academy of the Sacred Heart in Grand Coteau, Louisiana USA, at the age of twelve. As a teen, she was diagnosed with dyslexia. She attended university in Mexico City, where she studied International Relations at the Universidad Iberoamericana.
Career.
Mexico.
At the age of 23, Hayek landed the title role in "Teresa" (1989), a successful Mexican telenovela that made her a star in Mexico. In 1994, Hayek starred in the film "El Callejón de los Milagros" ("Miracle Alley"), which has won more awards than any other movie in the history of Mexican cinema. For her performance, Hayek was nominated for an Ariel Award.
Early Hollywood acting work.
Hayek moved to Los Angeles, California, in 1991 to study acting under Stella Adler. She had limited fluency in English, and dyslexia. Robert Rodriguez, and his producer and then-wife, Elizabeth Avellan, soon gave Hayek a starring role opposite Antonio Banderas in 1995's "Desperado". She followed her role in "Desperado" with a brief role as a vampire queen in "From Dusk till Dawn", in which she performed a table-top snake dance.
Hayek had a starring role opposite Matthew Perry in the 1997 romantic comedy "Fools Rush In". In 1999 she co-starred in Will Smith's big-budget "Wild Wild West", and played a supporting role in Kevin Smith's "Dogma". In 2000 Hayek had an uncredited acting part opposite Benicio del Toro in "Traffic". In 2003, she reprised her role from "Desperado" by appearing in "Once Upon a Time in Mexico", the final film of the "Mariachi Trilogy".
Director, producer and actress.
Around 2000, Hayek founded film production company Ventanarosa, through which she produces film and television projects. Her first feature as a producer was 1999's "El Coronel No Tiene Quien Le Escriba", Mexico's official selection for submission for Best Foreign Film at the Oscars.
"Frida", co-produced by Hayek, was released in 2002. Starring Hayek as Frida Kahlo, and Alfred Molina as her unfaithful husband, Diego Rivera, the film was directed by Julie Taymor and featured an entourage of stars in supporting and minor roles (Valeria Golino, Ashley Judd, Edward Norton, Geoffrey Rush) and cameos (Antonio Banderas). She earned a Best Actress Academy Award nomination for her performance.
"In the Time of the Butterflies" is a 2001 feature film based on the Julia Álvarez book of the same name, covering the lives of the Mirabal sisters. In the movie, Salma Hayek plays one of the sisters, Minerva, and Edward James Olmos plays the Dominican dictator Rafael Leónidas Trujillo whom the sisters opposed.
In 2003, Hayek produced and directed "The Maldonado Miracle", a Showtime movie based on the book of the same name, winning her a Daytime Emmy Award for Outstanding Directing in a Children/Youth/Family Special. In December 2005, she directed a music video for Prince, titled "Te Amo Corazon" ("I love you, sweetheart") that featured Mía Maestro.
Hayek was an executive producer of "Ugly Betty", a television series that aired around the world from 2006 to 2010. Hayek adapted the series for American television with Ben Silverman, who acquired the rights and scripts from the Colombian telenovela "Yo Soy Betty La Fea" in 2001. Originally intended as a half hour sitcom for NBC in 2004, the project would later be picked up by ABC for the 2006–2007 season with Silvio Horta also producing. Hayek guest-starred on "Ugly Betty" as Sofia Reyes, a magazine editor. She also had a cameo playing an actress in the telenovela within the show. The show won a Golden Globe Award for Best Comedy Series in 2007. Hayek's performance as Sofia resulted in a nomination for Outstanding Guest Actress in a Comedy Series at the 59th Primetime Emmy Awards.
In April 2007, Hayek finalized negotiations with MGM to become the CEO of her own Latin themed film production company, Ventanarosa. The following month, she signed a two-year deal with ABC for Ventanarosa to develop projects for the network.
Hayek played the wife of Adam Sandler's character in the buddy comedy "Grown Ups", which also co-starred Chris Rock and Kevin James. At his insistence, Hayek co-starred with Antonio Banderas in the "Shrek" spin-off film "Puss in Boots" as the voice of the character Kitty Softpaws, who serves as Puss's female counterpart and love interest. In 2012, Hayek directed Jada Pinkett Smith in the music video "Nada se compara." She reprised her role in "Grown Ups 2", which was released in July 2013.
Singing credits.
Hayek has been credited as a song performer in three movies. The first was "Desperado" for the song "Quedate Aquí". In "Frida" she performed the Mexican folk song "La Bruja" with the band Los Vega. She also recorded "Siente mi amor", which played during the end credits of "Once Upon a Time in Mexico". Hayek along with actor Joe Anderson sing together a cover of The Beatles song "Happiness Is A Warm Gun" on the "Across the Universe" motion picture soundtrack.
Promotional work.
Hayek has been a spokesperson for Avon cosmetics since February 2004. She formerly was a spokesperson for Revlon in 1998. In 2001, she modeled for Chopard and was featured in 2006 Campari adverts, photographed by Mario Testino. On April 3, 2009, she helped introduce La Doña, a watch by Cartier inspired by fellow Mexican actress María Félix.
Hayek has worked with the Procter & Gamble Company and UNICEF to promote the funding (through disposable diaper sales) of vaccines against maternal and neonatal tetanus. She is a global spokesperson for the Pampers/UNICEF "partnership" 1 Pack = 1 Vaccine to help raise awareness of the program. This "partnership" involves Procter & Gamble donating the cost of one tetanus vaccination (approximately 24 cents) for every pack of Pampers sold. (Pampers diapers cost approximately 25 cents each, or about US $1,000 per child per year in the US market).
In 2011, Hayek launched her own line of cosmetics, skincare, and haircare products called Nuance by Salma Hayek, to be sold at CVS stores in North America. She was inspired to create a cosmetic line from her grandmother, who used to make her own facial care products.
Hayek was also featured in a series of Spanish language commercials for Lincoln cars. 
In art.
In spring 2006, the Blue Star Contemporary Art Center in San Antonio, Texas displayed 16 portrait paintings by muralist George Yepes and filmmaker Robert Rodriguez of Hayek as Aztec goddess Itzpapalotl.
Personal life.
Hayek is a naturalized United States citizen. She studied at Ramtha's School of Enlightenment and is a practitioner of yoga. Hayek, who was raised Catholic, said she is not very devout anymore and does not believe in the institution [Church], but still believes in Jesus Christ and God.
On March 9, 2007, Hayek confirmed her engagement to French billionaire and Kering CEO, François-Henri Pinault, as well as her pregnancy. She gave birth to daughter, Valentina Paloma Pinault, in 2007 at Cedars-Sinai Medical Center in Los Angeles, California. On July 18, 2008, Hayek and Pinault announced the end of their engagement. They later reconciled and were married on Valentine's Day 2009 in Paris. On April 25, 2009, they were married a second time in Venice.
In July 2011 Hayek's husband was named in a paternity case. According to reports, Pinault is the father of supermodel Linda Evangelista's four-year-old son, Augustin James. He denied all allegations, although he later reached a settlement with Evangelista.
Activism.
Hayek's charitable work includes increasing awareness on violence against women and discrimination against immigrants. On July 19, 2005, Hayek testified before the U.S. Senate Committee on the Judiciary supporting reauthorizing the Violence Against Women Act. In February 2006, she donated $25,000 to a Coatzacoalcos, Mexico, shelter for battered women and another $50,000 to Monterrey based anti-domestic violence groups. Hayek is a board member of V-Day, the charity founded by playwright Eve Ensler.
Hayek also advocates breastfeeding. During a UNICEF fact-finding trip to Sierra Leone, she breastfed a hungry week-old baby whose mother could not produce milk. She said she did it to reduce the stigma associated with breastfeeding and to encourage infant nutrition.
In 2010 Hayek's humanitarian work earned her a nomination for the VH1 Do Something Awards. In 2013 Hayek launched with Beyoncé Knowles and Frida Giannini a Gucci campaign, "Chime For Change", that aims to spread female empowerment.
For International Women's Day 2014 Hayek is one of the artist signatories of Amnesty International's letter to UK Prime Minister David Cameron campaigning for women's rights in Afghanistan.
Honors and awards.
In July 2007, "The Hollywood Reporter" ranked Hayek fourth in their inaugural Latino Power 50, a list of the most powerful members of the Hollywood Latino community. That same month, a poll found Hayek to be the "sexiest celebrity" out of a field of 3,000 celebrities (male and female); according to the poll, "65 percent of the U.S. population would use the term 'sexy' to describe her". In 2008, she was awarded the Women in Film Lucy Award in recognition of her excellence and innovation in her creative works that have enhanced the perception of women through the medium of television In December of that year, "Entertainment Weekly" ranked Hayek number 17 in their list of the "25 Smartest People in TV."

</doc>
<doc id="27718" url="http://en.wikipedia.org/wiki?curid=27718" title="Super Bowl">
Super Bowl

The Super Bowl is the annual championship game of the National Football League (NFL), the highest level of professional football in the United States, culminating a season that begins in the late summer of the previous calendar year. The Super Bowl normally uses Roman numerals to identify each game, rather than the year in which it is held. For example, Super Bowl I was played on January 15, 1967, following the 1966 regular season. The lone exception will be the next game, Super Bowl 50, which will be played on February 7, 2016, following the 2015 season.
The game was created as part of a merger agreement between the NFL and its then-rival league, the American Football League (AFL). It was agreed that the two leagues' champion teams would play in the AFL–NFL World Championship Game until the merger was to officially begin in 1970. After the merger, each league was redesignated as a "conference", and the game has since been played between the conference champions to determine the NFL's league champion. Currently, the National Football Conference (NFC) leads the league with 26 wins to 23 wins for the American Football Conference (AFC). The Pittsburgh Steelers hold the record for Super Bowl victories with six.
The day on which the Super Bowl is played, now considered by some an unofficial American national holiday, is called "Super Bowl Sunday". It is the second-largest day for U.S. food consumption, after Thanksgiving Day. In addition, the Super Bowl has frequently been the most-watched American television broadcast of the year; the four most-watched broadcasts in U.S. television history are Super Bowls. In 2015, Super Bowl XLIX became the most-watched American television program in history with an average audience of 114.4 million viewers, the fifth time in six years the game had set a record, starting with the 2010 Super Bowl, which itself had taken over the number-one spot held for 27 years by the final episode of "M*A*S*H". The Super Bowl is also among the most-watched sporting events in the world, almost all audiences being North American, and is second to soccer's UEFA Champions League final as the most watched "annual" sporting event worldwide.
The NFL restricts the use of its "Super Bowl" trademark; it is frequently called the Big Game or other generic terms by non-sponsoring corporations. Because of the high viewership, commercial airtime during the Super Bowl broadcast is the most expensive of the year, leading to companies regularly developing their most expensive advertisements for this broadcast. As a result, watching and discussing the broadcast's commercials has become a significant aspect of the event. In addition, popular singers and musicians including Michael Jackson, Madonna, Prince, The Rolling Stones, The Who, and Whitney Houston have performed during the event's pre-game and halftime ceremonies.
Origin.
For four decades after its 1920 inception, the NFL successfully fended off several rival leagues. However, in 1960, it encountered its most serious competitor when the American Football League (AFL) was formed. The AFL vied heavily with the NFL for both players and fans, but by the middle of the decade the strain of competition led to serious merger talks between the two leagues. Prior to the 1966 season, the NFL and AFL reached a merger agreement that was to take effect for the 1970 season. As part of the merger, the champions of the two leagues agreed to meet in a world championship game for professional American football until the merger was effected.
Lamar Hunt, owner of the AFL's Kansas City Chiefs, first used the term "Super Bowl" to refer to this game in the merger meetings. Hunt would later say the name was likely in his head because his children had been playing with a Super Ball toy (a vintage example of the ball is on display at the Pro Football Hall of Fame in Canton, Ohio). In a July 25, 1966, letter to NFL commissioner Pete Rozelle, Hunt wrote, "I have kiddingly called it the 'Super Bowl,' which obviously can be improved upon." Although the leagues' owners decided on the name "AFL-NFL Championship Game," the media immediately picked up on Hunt's "Super Bowl" name, which would become official beginning with the third annual game.
The "Super Bowl" name was derived from the bowl game, a post-season college football game. The original "bowl game" was the Rose Bowl Game in Pasadena, California, which was first played in 1902 as the "Tournament East-West football game" as part of the Pasadena Tournament of Roses and moved to the new Rose Bowl Stadium in 1923. The stadium got its name from the fact that the game played there was part of the Tournament of Roses and that it was shaped like a bowl, much like the Yale Bowl in New Haven, Connecticut; the Tournament of Roses football game itself eventually came to be known as the Rose Bowl Game. Exploiting the Rose Bowl Game's popularity, post-season college football contests were created for Miami (the Orange Bowl), New Orleans (the Sugar Bowl), and El Paso, TX (the Sun Bowl) in 1935, and for Dallas (the Cotton Bowl) in 1937. Thus, by the time the first Super Bowl was played, the term "bowl" for any big-time American football game was well established.
After the NFL's Green Bay Packers won the first two Super Bowls, some team owners feared for the future of the merger. At the time, many doubted the competitiveness of AFL teams compared with their NFL counterparts, though that perception changed when the AFL's New York Jets defeated the NFL's Baltimore Colts in Super Bowl III in Miami. One year later, the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV in New Orleans, which was the final AFL-NFL World Championship Game played before the merger. Beginning with the 1970 season, the NFL realigned into two conferences; the former AFL teams plus three NFL teams (the Colts, Pittsburgh Steelers, and Cleveland Browns) would constitute the American Football Conference (AFC), while the remaining NFL clubs would form the National Football Conference (NFC). The champions of the two conferences would play each other in the Super Bowl.
The winning team receives the Vince Lombardi Trophy, named after the coach of the Green Bay Packers, who won the first two Super Bowl games and three of the five preceding NFL championships in 1961, 1962, and 1965. Following Lombardi's death in September 1970, the trophy was named the Vince Lombardi Trophy, and was the first awarded as such to the Baltimore Colts following their win in Super Bowl V in Miami.
Date.
The game is played annually on a Sunday as the final game of the NFL Playoffs. Originally, the game took place in early to mid-January, following a fourteen-game regular season and two rounds of playoffs. Over the years, the date of the Super Bowl has progressed from the second Sunday in January, to the third, and then the fourth Sunday in January; the game is currently played on the first Sunday in February, given the current seventeen-week (sixteen games and one bye week) regular season and three rounds of playoffs. Also, February is television's "sweeps" month, thus affording the television network carrying the game an immense opportunity to pad its viewership when negotiating for advertising revenue. The progression of the dates of the Super Bowl was caused by several factors: the expansion of the NFL's regular season in 1978 from fourteen games to sixteen; the expansion of the pre-Super Bowl playoff field from four teams (two AFL and two NFL) in the 1966–67 season, to six in 1967–68 (two AFL and four NFL), to eight (four AFL and four NFL) in the 1969–70 season which was continued after the merger, then to ten in 1978–79, and finally twelve in 1990–91, necessitating additional rounds of playoffs; the addition of the regular season bye-week in the 1990s; and the decision to start the regular season the week following Labor Day.
To date, 36 games have been played in January, and 13 in February. The earliest game played was Super Bowl XI on January 9, 1977. The latest played was Super Bowl XLIV on February 7, 2010. The most frequent date for the game has been January 26, with four games played. Between January 9 and February 7, the only dates not to feature the game have been January 10, 19 and 23. Super Bowl XLVIII was the first Super Bowl played on February 2, a date commonly celebrated as Groundhog Day.
Game history.
The Pittsburgh Steelers have won six Super Bowls, the most of any team; the Dallas Cowboys and San Francisco 49ers have five victories each, while the Green Bay Packers, New York Giants and New England Patriots have four Super Bowl championships. Thirteen other NFL franchises have won at least one Super Bowl. Nine teams have appeared in Super Bowl games without a win. The Minnesota Vikings were the first team to have appeared a record four times without a win. The Buffalo Bills played in a record four Super Bowls in a row, and lost every one. Four teams (the Cleveland Browns, Detroit Lions, Jacksonville Jaguars, and Houston Texans) have never appeared in a Super Bowl. The Browns and Lions both won NFL Championships prior to the Super Bowl's creation, while the Jaguars (1995) and Texans (2002) are both recent NFL expansion teams. The Minnesota Vikings won the last NFL Championship before the merger, but lost to the AFL champion Kansas City Chiefs in Super Bowl IV.
1960s: Early history.
The Green Bay Packers won the first two Super Bowls, defeating the Kansas City Chiefs and Oakland Raiders following the 1966 and 1967 seasons, respectively. The Packers were led by quarterback Bart Starr, who was named the Most Valuable Player (MVP) for both games. These two championships, coupled with the Packers' NFL championships in 1961, 1962, and 1965, amount to the most successful stretch in NFL History; five championships in seven years.
In Super Bowl III, the AFL's New York Jets defeated the eighteen-point favorite Baltimore Colts of the NFL, 16–7. The Jets were led by quarterback Joe Namath (who had famously guaranteed a Jets win prior to the game) and former Colts head coach Weeb Ewbank, and their victory proved that the AFL was the NFL's competitive equal. This was reinforced the following year, when the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV.
1970s: Dominant franchises.
After the AFL–NFL merger was completed in 1970, three franchises – the Dallas Cowboys, Miami Dolphins, and Pittsburgh Steelers – would go on to dominate the 1970s, winning a combined eight Super Bowls in the decade.
The Baltimore Colts, now a member of the AFC, would start the decade by defeating the Cowboys in Super Bowl V, a game which is notable as being the only Super Bowl to date in which a player from the losing team won the Super Bowl MVP (Cowboys' linebacker Chuck Howley). This Super Bowl, as well as all Super Bowls since, have served as the NFL's league championship game.
The Cowboys, coming back from a loss the previous season, won Super Bowl VI over the Dolphins. However, this would be the Dolphins' final loss in over a year, as the next year, the Dolphins would go 14–0 in the regular season and eventually win all of their playoff games, capped off with a 14-7 victory in Super Bowl VII, becoming the first and only team to finish an entire perfect regular and post season. The Dolphins would repeat as league champions by winning Super Bowl VIII a year later.
In the late 1970s, the Steelers became the first NFL dynasty of the post-merger era by winning four Super Bowls (IX, X, XIII, and XIV) in six years. They were led by head coach Chuck Noll, the play of offensive stars Terry Bradshaw, Franco Harris, Lynn Swann, John Stallworth, and Mike Webster, and their dominant "Steel Curtain" defense, led by "Mean" Joe Greene, L.C. Greenwood, Ernie Holmes, Mel Blount, Jack Ham, and Jack Lambert. The coaches and administrators also were part of the dynasty's greatness as evidenced by the team's "final pieces" being part of the famous 1974 draft. The selections in that class have been considered the best by any pro franchise ever, as Pittsburgh selected four future Hall of Famers, the most for any team in any sport in a single draft. The Steelers were the first team to win three and then four Super Bowls and appeared in six AFC Championship Games during the decade, making the playoffs in eight straight seasons. Nine players and three coaches and administrators on the team have been inducted into the Pro Football Hall of Fame. Pittsburgh still remains the only team to win back-to-back Super Bowls twice and four Super Bowls in a six-year period.
The Steelers' dynasty was interrupted only by the Cowboys winning their second Super Bowl of the decade and the Oakland Raiders' Super Bowl XI win.
1980s and 1990s: The NFC's winning streak.
In the 1980s and 1990s, the tables turned for the AFC, as the NFC dominated the Super Bowls of the new decade and most of those of the 1990s. The NFC won 16 of the 20 Super Bowls during these two decades, including 13 straight from Super Bowl XIX to Super Bowl XXXI.
The most successful team of the 1980s was the San Francisco 49ers, which featured the West Coast offense of Hall of Fame head coach Bill Walsh. This offense was led by three-time Super Bowl MVP and Hall of Fame quarterback Joe Montana, Super Bowl MVP and Hall of Fame wide receiver Jerry Rice, and tight end Brent Jones. Under their leadership, the 49ers won four Super Bowls in the decade (XVI, XIX, XXIII, and XXIV) and made nine playoff appearances between 1981 and 1990, including eight division championships, becoming the second dynasty of the post-merger NFL.
The 1980s also produced the 1985 Chicago Bears, who posted an 18–1 record under head coach Mike Ditka; colorful quarterback Jim McMahon; and Hall of Fame running back Walter Payton. Their team won Super Bowl XX in dominating fashion. The Washington Redskins and New York Giants were also top teams of this period; the Redskins won Super Bowls XVII, XXII, and XXVI. The Giants claimed Super Bowls XXI and XXV. As in the 1970s, the Oakland Raiders were the only team to interrupt the Super Bowl dominance of other teams; they won Super Bowls XV and XVIII (the latter as the Los Angeles Raiders).
Following several seasons with poor records in the 1980s, the Dallas Cowboys rose back to prominence in the 1990s. During this decade, the Cowboys made post-season appearances every year except for the seasons of 1990 and 1997. From 1992 to 1996, the Cowboys won their division championship each year. In this same period, the Buffalo Bills had made their mark reaching the Super Bowl for 4 consecutive years, only to lose in all of them. After Super Bowl championships by division rivals New York (1990) and Washington (1991), the Cowboys won three of the next four Super Bowls (XXVII, XXVIII, and XXX) led by quarterback Troy Aikman, running back Emmitt Smith, and wide receiver Michael Irvin. All three of these players went to the Hall of Fame. The Cowboys' streak was interrupted by the 49ers, who won their league-leading fifth title overall with Super Bowl XXIX in dominating fashion under Super Bowl MVP and Hall of Fame quarterback Steve Young, Hall of Fame wide receiver Jerry Rice, and Hall of Fame cornerback Deion Sanders; however, the Cowboys' victory in Super Bowl XXX the next year also gave them five titles overall and they did so with Deion Sanders after he won the Super Bowl the previous year with the San Francisco 49ers. The NFC's winning streak was continued by the Green Bay Packers who, under quarterback Brett Favre, won Super Bowl XXXI, their first championship since Super Bowl II in the late 1960s.
1997–2009: AFC resurgence.
Super Bowl XXXII saw quarterback John Elway and running back Terrell Davis lead the Denver Broncos to an upset victory over the defending champion Packers, snapping the NFC's 13 year winning streak. The following year, the Broncos defeated the Atlanta Falcons in Super Bowl XXXIII, Elway's fifth Super Bowl appearance, his second NFL championship, and his final NFL game. The back-to-back victories heralded a change in momentum in which AFC teams would win 10 out of 13 Super Bowls. In the years between 2001 and 2011, three teams – the Patriots, Steelers, and Colts – accounted for ten of the AFC Super Bowl appearances, with those same teams often meeting each other earlier in the playoffs. In contrast, the NFC saw a different representative in the Super Bowl every season from 2001 through 2010.
The year following the Denver Broncos' second victory, however, a surprising St. Louis Rams led by undrafted quarterback Kurt Warner would close out the 1990s in a wild battle against the Tennessee Titans in Super Bowl XXXIV. The tense game came down to the final play in which Tennessee had the opportunity to tie the game and send it to overtime. The Titans nearly pulled it off, but the tackle of receiver Kevin Dyson by linebacker Mike Jones kept the ball out of the end zone by a matter of inches. In 2007, ESPN would rank "The Tackle" as the 2nd greatest moment in Super Bowl history.
Super Bowl XXXV was played by the AFC's Baltimore Ravens and the NFC's New York Giants. The Ravens defeated the Giants by the score of 34–7. The game was played on January 28, 2001, at Raymond James Stadium in Tampa, Florida.
The New England Patriots became the dominant team throughout the early 2000s, winning the championship three out of four years early in the decade. They would become only the second team in the history of the NFL to do so (after the 1990s Dallas Cowboys). In Super Bowl XXXVI, first-year starting quarterback Tom Brady led his team to a 20–17 upset victory over the St. Louis Rams. Brady would go on to win the MVP award for this game. The Patriots also won Super Bowls XXXVIII and XXXIX defeating the Carolina Panthers and the Philadelphia Eagles respectively. This four-year stretch of Patriot dominance was only interrupted by the Tampa Bay Buccaneers' 48-21 Super Bowl XXXVII victory over the Oakland Raiders.
The Pittsburgh Steelers and Indianapolis Colts continued the era of AFC dominance by winning Super Bowls XL and XLI in 2005-06 and 2006–07, respectively defeating the Seattle Seahawks and Chicago Bears.
In the 2007 season, the Patriots became the second team in NFL history to have a perfect regular season record, after the 1972 Miami Dolphins, and the first to finish 16–0. They easily marched through the AFC playoffs and were heavy favorites in Super Bowl XLII. However, they lost that game to Eli Manning and the New York Giants 17–14, leaving the Patriots' 2007 record at 18-1.
The following season, the Steelers logged their record sixth Super Bowl title (XLIII) in a 27-23, final-minute victory against the Arizona Cardinals.
2010–present: The NFC re-emerges.
The 2010s have seen a return to dominance by NFC teams. Between 2010 and 2015, four of the six Super Bowl winners hailed from the NFC.
The Giants won another title after the 2011 season, again defeating the Patriots in Super Bowl XLVI. Prior to that Super Bowl victory, the New Orleans Saints won their first (XLIV) by defeating the Indianapolis Colts in February 2010, and the Green Bay Packers won their fourth Super Bowl (XLV) and record thirteenth NFL championship overall by defeating the Pittsburgh Steelers in February 2011.
The Baltimore Ravens interrupted the NFC's streak by winning Super Bowl XLVII in a 34-31 nail-biter over the San Francisco 49ers.
Super Bowl XLVIII, played at New Jersey's MetLife Stadium in February 2014, was the first Super Bowl held outdoors in a cold weather environment. The Seattle Seahawks won their first NFL title with a 43-8 defeat of the Denver Broncos, in a highly touted matchup that pitted Seattle's top-ranked defense against a Peyton-Manning-led Denver offense that had broken the NFL's single-season scoring record.
In Super Bowl XLIX, the New England Patriots, the AFC champions, beat the NFC and defending Super Bowl champions, the Seattle Seahawks.
The Super Bowls of the 2000s and early 2010s are notable for the performances (and the pedigrees) of several of the participating quarterbacks. During that era, Tom Brady (six Super Bowl appearances, four wins), Ben Roethlisberger (three appearances, two wins), Peyton Manning (three appearances, one win), Eli Manning (two appearances, two wins), Kurt Warner (three appearances, one win), Drew Brees (one appearance, one win), Aaron Rodgers (one appearance, one win), Joe Flacco (one appearance, one win), and Russell Wilson (two appearances, one win) have all added Super Bowl championships to their lists of individual accomplishments.
Television coverage and ratings.
The Super Bowl is one of the most watched annual sporting events in the world. The only other annual events that gather more viewers are the UEFA Champions League final, and "El Clásico" in Spain. For many years, the Super Bowl has possessed a large US and global television viewership, and it is often the most watched United States originating television program of the year. The game tends to have high Nielsen television ratings, which is usually around a 40 rating and 60 share. This means that on average, more than 100 million people from the United States alone are tuned into the Super Bowl at any given moment.
In press releases preceding each year's event, the NFL typically claims that that year's Super Bowl will have a potential worldwide audience of around one billion people in over 200 countries. This figure refers to the number of people "able" to watch the game, not the number of people "actually" watching. However the statements have been frequently misinterpreted in various media as referring to the latter figure, leading to a common misperception about the game's actual global audience. The New York-based media research firm Initiative measured the global audience for the 2005 Super Bowl at 93 million people, with 98 percent of that figure being viewers in North America, which meant roughly 2 million people outside North America watched the Super Bowl that year.
2015's Super Bowl XLIX holds the record for total number of U.S. viewers, with a final number of 114.4 million, making the game the most-viewed television broadcast of any kind in American history. The halftime show was the most watched ever with 118.5 million viewers tuning in, and an all-time high of 168 million viewers in the United States had watched several portions of the Super Bowl 2015 broadcast. The game set a record for total viewers for the fifth time in six years.
The highest-rated game according to Nielsen was Super Bowl XVI in 1982, which was watched in 49.1 percent of households (73 share), or 40,020,000 households at the time. Ratings for that game, a San Francisco victory over Cincinnati, may have been aided by a large blizzard that had affected much of the northeastern United States on game day, leaving residents to stay at home more than usual. Super Bowl XVI still ranks fourth on Nielsen's list of top-rated programs of all time, and three other Super Bowls, XII, XVII, and XX, made the top ten.
Famous commercial campaigns include the Budweiser "Bud Bowl" campaign and the 1999 and 2000 dot-com ads. Prices have increased every year, with advertisers paying as much as $3.5 million for a thirty-second spot during Super Bowl XLVI in 2012. A segment of the audience tunes into the Super Bowl solely to view commercials. In 2010, Nielsen reported that 51 percent of Super Bowl viewers tune in for the commercials. The Super Bowl halftime show has spawned another set of alternative entertainment such as the Lingerie Bowl, the Beer Bottle Bowl, and others.
The Super Bowl is scheduled for East Coast viewers, and has begun between 6:19 and 6:40 PM EST since 1991.
Super Bowl on TV.
<br>
^ *: Not currently broadcasting NFL games.
^ **: The extended current TV contracts with the networks expire after the 2022 season (or Super Bowl LVII in early 2023) and the Super Bowl is rotated annually between CBS, Fox and NBC in that order.
^ ***: The first Super Bowl was simultaneously broadcast by CBS and NBC, with each network using the same video feed, but providing its own commentary.
Super Bowls I–VI were blacked out in the television markets of the host cities, due to league restrictions then in place.
Lead-out programming.
The Super Bowl provides an extremely strong lead-in to programming following it on the same channel, the effects of which can last for several hours. For instance, in discussing the ratings of a local TV station, Buffalo television critic Alan Pergament noted on the coattails from Super Bowl XLVII, which aired on CBS: "A paid program that ran on Channel 4 (WIVB-TV) at 2:30 in the morning had a 1.3 rating. That’s higher than some CW prime time shows get on WNLO-TV, Channel 4’s sister station."
Because of this strong coattail effect, the network that airs the Super Bowl typically takes advantage of the large audience to air an episode of a hit series, or to premiere the pilot of a promising new one in the lead-out slot, which immediately follows the Super Bowl and post-game coverage.
Entertainment.
 Initially, it was sort of a novelty and so it didn't quite feel right. But it was just like, this is the year. ... Bands of our generation, you can sort of be seen on a stage like this or, like, not seen. There's not a lot of middle places. It is a tremendous venue.
 — Bruce Springsteen on why he turned down several invitations to perform at the Super Bowl before finally agreeing to appear in Super Bowl XLIII.
Early Super Bowls featured a halftime show consisting of marching bands from local colleges or high schools; but as the popularity of the game increased, a trend where popular singers and musicians performed during its pre-game ceremonies and the halftime show, or simply sang the national anthem of the United States, emerged. Unlike regular season or playoff games, thirty minutes are allocated for the Super Bowl halftime. The first halftime show to have featured only one star performer was Super Bowl XXVII in 1993, at which Michael Jackson performed. The NFL specifically went after him to increase viewership and to continue expanding the Super Bowl's realm. Sports bloggers have ranked Jackson's appearance as the No. 1 Super Bowl halftime show since its inception. Another notable performance came during Super Bowl XXXVI in 2002, when U2 performed; during their third song, "Where the Streets Have No Name", the band played under a large projection screen which scrolled through names of the victims of the September 11 attacks.
The halftime show of Super Bowl XXXVIII in 2004 generated controversy when Justin Timberlake removed a piece of Janet Jackson's top, exposing her right breast with a star-shaped pastie around the nipple. Timberlake and Jackson have maintained that the incident was accidental, calling it a "wardrobe malfunction". The game was airing live on CBS, and MTV had produced the halftime show. Immediately after the moment, the footage jump-cut to a wide-angle shot and went to a commercial break; however, video captures of the moment in detail circulated quickly on the internet. The NFL, embarrassed by the incident, permanently banned MTV from conducting future halftime shows. This also led to the FCC tightening controls on indecency and fining CBS and CBS-owned stations a total of $550,000 for the incident. The fine was later reversed in July 2008. CBS and MTV eventually split into two separate companies in part because of the fiasco, with CBS going under the control of CBS Corporation and MTV falling under the banner of Viacom (although both corporations remain under the ownership of National Amusements). For six years following the incident, all of the performers in Super Bowl halftime shows were artists associated with the classic rock genre of the 1970s and 1980s (including three acts from the British Invasion of the 1960s), with only one act playing the entire halftime show. Paul McCartney (formerly of The Beatles) played Super Bowl XXXIX in 2005, The Rolling Stones played Super Bowl XL in 2006, and The Who played Super Bowl XLIV in 2010. The halftime show returned to a modern act in 2011 with The Black Eyed Peas. But during the halftime show of Super Bowl XLVI in 2012, M.I.A. gave the middle finger during a performance of "Give Me All Your Luvin'" with Madonna, which was caught by TV cameras. An attempt to censor the gesture by blurring the entire screen came late.
Excluding Super Bowl XXXIX, the famous "I'm going to Disney World!" advertising campaign took place at every Super Bowl since Super Bowl XXI, when quarterback Phil Simms from the New York Giants became the first player to say the tagline.
Venue.
As of Super Bowl XLVIII, 27 of 49 Super Bowls have been played in three cities: New Orleans (ten times), the Greater Miami area (ten times), and the Greater Los Angeles area (seven times). Stadiums that do not host an NFL franchise are not, by rule, prohibited from hosting the Super Bowl, and non-NFL stadiums have hosted the game nine times, with the Rose Bowl accounting for five of these. To date, however, no market or region without an NFL franchise has ever hosted a Super Bowl; all five Rose Bowl Super Bowls were hosted before the Los Angeles Rams and Los Angeles Raiders left for St. Louis and Oakland respectively in 1995.
No team has ever played the Super Bowl in its home stadium. The closest have been the San Francisco 49ers who played Super Bowl XIX in Stanford Stadium, rather than Candlestick Park, and the Los Angeles Rams who played Super Bowl XIV in the Rose Bowl, rather than the Los Angeles Memorial Coliseum. In both cases, the stadium in which the Super Bowl was held was perceived to be a better stadium for a large, high-profile event than the stadiums the Rams and 49ers were playing in at the time; this situation has not arisen since 1993, in part because the league has traditionally awarded the Super Bowl in modern times to the newest stadiums. Besides those two, the only other Super Bowl venue that was not the home stadium to an NFL team at the time was Rice Stadium in Houston: the Houston Oilers had played there previously, but moved to the Astrodome several years prior to Super Bowl VIII. The Orange Bowl was the only AFL stadium to host a Super Bowl and the only stadium to host consecutive Super Bowls, hosting Super Bowls II and III.
Traditionally, the NFL does not award Super Bowls to stadiums that are located in climates with an expected average daily temperature less than 50 °F (10 °C) on game day unless the field can be completely covered by a fixed or retractable roof. Five Super Bowls have been played in northern cities: two in the Detroit area—Super Bowl XVI at Pontiac Silverdome in Pontiac, Michigan and Super Bowl XL at Ford Field in Detroit, one in Minneapolis—Super Bowl XXVI, one in Indianapolis at Lucas Oil Stadium for Super Bowl XLVI, and one in the New York area—Super Bowl XLVIII at MetLife Stadium. Only MetLife Stadium did not have a roof (be it fixed or retractable) but it was still picked as the host stadium for Super Bowl XLVIII in an apparent waiver of the warm-climate rule. A sixth Super Bowl is planned in a northern city as Minneapolis has been picked to host Super Bowl LII in 2018 in the under-construction New Minnesota Stadium.
There have been a few instances where the league has yanked the Super Bowl from cities. Super Bowl XXVII in 1993 was originally awarded to Sun Devil Stadium in Tempe, Arizona, but after Arizona voted to not recognize Martin Luther King, Jr. Day in 1990, the NFL moved the game to the Rose Bowl in Pasadena, California in protest. After Arizona opted to create the holiday by ballot in 1992, Super Bowl XXX in 1996 was awarded to Tempe. Super Bowl XXXIII was awarded first to Candlestick Park in San Francisco, but when plans to renovate the stadium fell through the game was moved to Pro Player Stadium in greater Miami. Super Bowl XXXVII was awarded to a new stadium not yet built in San Francisco, when that stadium failed to be built, the game was moved to San Diego. Super Bowl XLIV, slated for February 7, 2010, was withdrawn from New York City's proposed West Side Stadium, because the city, state, and proposed tenants New York Jets could not agree on funding. Super Bowl XLIV was then eventually awarded to Sun Life Stadium in Miami Gardens, Florida. And Super Bowl XLIX in 2015 was originally given to Arrowhead Stadium in Kansas City, Missouri, but after two sales taxes failed to pass at the ballot box, and opposition by local business leaders and politicians increased, Kansas City eventually withdrew its request to host the game. Super Bowl XLIX was then eventually awarded to University of Phoenix Stadium in Glendale, Arizona.
In 2011, Texas Attorney General Greg Abbott said, "It's commonly known as the single largest human trafficking incident in the United States." According to Forbes, 10,000 prostitutes were brought to Miami in 2010 for the Super Bowl.
Selection process.
The location of the Super Bowl is chosen by the NFL well in advance, usually three to five years before the game. Cities place bids to host a Super Bowl and are evaluated in terms of stadium renovation and their ability to host. In 2014, a document listing the specific requirements of Super Bowl hosts was leaked, giving a clear list of what was required for a Super Bowl host. Much of the cost of the Super Bowl is to be assumed by the host community, although some costs are enumerated within the requirements to be assumed by the NFL. Some of the host requirements include:
The NFL owners meet to make a selection on the site, usually three years prior to the event. In 2007, NFL commissioner Roger Goodell suggested that a Super Bowl might be played in London, England, perhaps at Wembley Stadium. The game has never been played in a region that lacks an NFL franchise; seven Super Bowls have been played in Los Angeles, but none since the Los Angeles Raiders and Los Angeles Rams relocated to Oakland and St. Louis respectively in 1995. New Orleans, the site of the 2013 Super Bowl, invested more than $1 billion in infrastructure improvements in the years leading up to the game.
Home team designation.
The designated "home team" alternates between the NFC team in odd-numbered games and the AFC team in even-numbered games. This alternation was initiated with the first Super Bowl, when the Green Bay Packers were the designated home team. Regardless of being the home or away team of record, each team has their team wordmark painted in one of the end zones along with their conference designation. Designated away teams have won 29 of 49 Super Bowls to date (59.2%).
Since Super Bowl XIII in January 1979, the home team is given the choice of wearing their colored or white jerseys. Formerly, the designated home team was specified to wear their colored jerseys, which resulted in Dallas donning their less familiar dark blue jerseys for Super Bowl V. While most of the home teams in the Super Bowl have chosen to wear their colored jerseys, there have been four exceptions; the Cowboys during Super Bowl XIII and XXVII, the Washington Redskins during Super Bowl XVII, and the Pittsburgh Steelers during Super Bowl XL. The Cowboys, since 1965, and Redskins, since the arrival of coach Joe Gibbs in 1981, have traditionally worn white jerseys at home. Meanwhile, the Steelers, who have always worn their black jerseys at home since the AFL-NFL merger in 1970, opted for the white jerseys after winning three consecutive playoff games on the road, wearing white. The Steelers' decision was compared with the New England Patriots in Super Bowl XX; the Patriots had worn white jerseys at home during the 1985 season, but after winning road playoff games against the New York Jets and Miami Dolphins wearing red jerseys, New England opted to switch to red for the Super Bowl as the designated home team. White-shirted teams have won 31 of 49 Super Bowls to date (63.2%).
Host cities/regions.
Fifteen different regions have hosted Super Bowls.
Host stadiums.
A total of twenty-four different stadiums have hosted, or are scheduled to host, Super Bowls. Years listed in the table below are the years the game was actually played "(will be played)" rather than what NFL season it is considered to have been.
^ Stadium is now demolished.<br>
† The original Stanford Stadium, which hosted Super Bowl XIX, was demolished and replaced with a new stadium in 2006.<br>
The game has never been played in a region that lacked an NFL franchise, though cities without NFL teams are not categorically ineligible to host the event. London, England has occasionally been mentioned as a host city for a Super Bowl in the near future. Wembley Stadium has hosted several NFL games as part of the NFL International Series and is specifically designed for large, individual events. NFL Commissioner Roger Goodell has openly discussed the possibility on different occasions. Time zone complications are a significant obstacle to a Super Bowl in London; a typical 6:30 p.m. Eastern Time start would result in the game beginning at 11:30 p.m. local time in London, an unusually late hour to be holding spectator sports (the NFL has never in its history started a game later than 9:15 p.m. local time).
Super Bowl trademark.
The NFL is vigilant on stopping what it says is unauthorized commercial use of its trademarked terms "NFL", "Super Bowl", and "Super Sunday". As a result, many events and promotions tied to the game, but not sanctioned by the NFL, are asked to refer to it with colloquialisms such as "The Big Game", or other generic descriptions. A radio spot for Planters nuts parodied this, by saying "it would be "super"...to have a "bowl"...of Planters nuts while watching the big game!" and comedian Stephen Colbert began referring to the game in 2014 as the "Superb Owl". The NFL claims that the use of the phrase "Super Bowl" implies an NFL affiliation, and on this basis the league asserts broad rights to restrict how the game may be shown publicly; for example, the league says Super Bowl showings are prohibited in churches or at other events that "promote a message", while venues that do not regularly show sporting events cannot show the Super Bowl on any television screen larger than 55 inches. Some critics say the NFL is exaggerating its ownership rights by stating that "any use is prohibited", as this contradicts the broad doctrine of fair use in the United States. Legislation was proposed by Utah Senator Orrin Hatch in 2008 "to provide an exemption from exclusive rights in copyright for certain nonprofit organizations to display live football games", and "for other purposes".
In 2006, the NFL made an attempt to trademark "The Big Game" as well; however, it withdrew the application in 2007 due to growing commercial and public-relations opposition to the move, mostly from Stanford University and the University of California, Berkeley and their fans, as the Stanford Cardinal football and California Golden Bears football teams compete in the "Big Game", which has been played since 1892 (28 years before the formation of the NFL and 75 years before Super Bowl I). Additionally, the Mega Millions lottery game was known as The Big Game from 1996 to 2002.
Use of the phrase "world champions".
Like the other major professional leagues in the United States, the winner of the Super Bowl is usually declared "world champions", a title often mocked by non-Americans. Others feel the title is fitting, since it is the only professional league of its kind.
The practice by the U.S. major leagues of using the "World Champion" moniker originates from the World Series, and it was later used during the first three Super Bowls when they were referred to as AFL-NFL World Championship Games. The phrase is still engraved on the Super Bowl rings.

</doc>
<doc id="27725" url="http://en.wikipedia.org/wiki?curid=27725" title="Surface area">
Surface area

The surface area of a solid object is a measure of the total area that the surface of an object occupies. The mathematical definition of surface area in the presence of curved surfaces is considerably more involved than the definition of arc length of one-dimensional curves, or of the surface area for polyhedra (i.e., objects with flat polygonal faces), for which the surface area is the sum of the areas of its faces. Smooth surfaces, such as a sphere, are assigned surface area using their representation as parametric surfaces. This definition of surface area is based on methods of infinitesimal calculus and involves partial derivatives and double integration. 
A general definition of surface area was sought by Henri Lebesgue and Hermann Minkowski at the turn of the twentieth century. Their work led to the development of geometric measure theory, which studies various notions of surface area for irregular objects of any dimension. An important example is the Minkowski content of a surface.
Definition.
While the areas of many simple surfaces have been known since antiquity, a rigorous mathematical "definition" of area requires a great deal of care. 
This should provide a function
which assigns a positive real number to a certain class of surfaces that satisfies several natural requirements. The most fundamental property of the surface area is its additivity: "the area of the whole is the sum of the areas of the parts". More rigorously, if a surface "S" is a union of finitely many pieces "S"1, …, "S""r" which do not overlap except at their boundaries, then 
Surface areas of flat polygonal shapes must agree with their geometrically defined area. Since surface area is a geometric notion, areas of congruent surfaces must be the same and the area must depend only on the shape of the surface, but not on its position and orientation in space. This means that surface area is invariant under the group of Euclidean motions. These properties uniquely characterize surface area for a wide class of geometric surfaces called "piecewise smooth". Such surfaces consist of finitely many pieces that can be represented in the parametric form
with a continuously differentiable function formula_4 The area of an individual piece is defined by the formula 
Thus the area of "S""D" is obtained by integrating the length of the normal vector formula_6 to the surface over the appropriate region "D" in the parametric "uv" plane. The area of the whole surface is then obtained by adding together the areas of the pieces, using additivity of surface area. The main formula can be specialized to different classes of surfaces, giving, in particular, formulas for areas of graphs "z" = "f"("x","y") and surfaces of revolution.
One of the subtleties of surface area, as compared to arc length of curves, is that surface area cannot be defined simply as the limit of areas of polyhedral shapes approximating a given smooth surface. It was demonstrated by Hermann Schwarz that already for the cylinder, different choices of approximating flat surfaces can lead to different limiting values of the area (Known as Schwarz's paradox.)
Various approaches to a general definition of surface area were developed in the late nineteenth and the early twentieth century by Henri Lebesgue and Hermann Minkowski. While for piecewise smooth surfaces there is a unique natural notion of surface area, if a surface is very irregular, or rough, then it may not be possible to assign an area to it at all. A typical example is given by a surface with spikes spread throughout in a dense fashion. Many surfaces of this type occur in the study of fractals. Extensions of the notion of area which partially fulfill its function and may be defined even for very badly irregular surfaces are studied in geometric measure theory. A specific example of such an extension is the Minkowski content of the surface.
Common formulas.
Ratio of surface areas of a sphere and cylinder of the same radius and height.
The below given formulas can be used to show that the surface area of a sphere and cylinder of the same radius and height are in the ratio 2 : 3, as follows.
Let the radius be "r" and the height be "h" (which is 2"r" for the sphere).
formula_7
The discovery of this ratio is credited to Archimedes.
In chemistry.
Surface area is important in chemical kinetics. Increasing the surface area of a substance generally increases the rate of a chemical reaction. For example, iron in a fine powder will combust, while in solid blocks it is stable enough to use in structures. For different applications a minimal or maximal surface area may be desired.
In biology.
The surface area of an organism is important in several considerations, such as regulation of body temperature and digestion. Animals use their teeth to grind food down into smaller particles, increasing the surface area available for digestion. The epithelial tissue lining the digestive tract contains microvilli, greatly increasing the area available for absorption. Elephants have large ears, allowing them to regulate their own body temperature. In other instances, animals will need to minimize surface area; for example, people will fold their arms over their chest when cold to minimize heat loss. 
The surface area to volume ratio (SA:V) of a cell imposes upper limits on size, as the volume increases much faster than does the surface area, thus limiting the rate at which substances diffuse from the interior across the cell membrane to interstitial spaces or to other cells. Indeed, representing a cell as an idealized sphere of radius "r", the volume and surface area are, respectively, "V" = 4/3 π "r"3; "SA" = 4 π "r"2. The resulting surface area to volume ratio is therefore 3/"r". Thus, if a cell has a radius of 1 μm, the SA:V ratio is 3; whereas if the radius of the cell is instead 10 μm, then the SA:V ratio becomes 0.3. With a cell radius of 100, SA:V ratio is 0.03. Thus, the surface area falls off steeply with increasing volume.

</doc>
<doc id="27727" url="http://en.wikipedia.org/wiki?curid=27727" title="Solid state">
Solid state

Solid state may refer to:
In science:
In electronics:
In music:

</doc>
<doc id="27730" url="http://en.wikipedia.org/wiki?curid=27730" title="Serbo-Croatian">
Serbo-Croatian

Serbo-Croatian , also called Serbo-Croat , Serbo-Croat-Bosnian (SCB), Bosnian-Croatian-Serbian (BCS), or Bosnian-Croatian-Montenegrin-Serbian (BCMS), is a South Slavic language and the primary language of Bosnia and Herzegovina, Croatia, Montenegro, and Serbia. It is a pluricentric language with four mutually intelligible standard varieties.
South Slavic dialects historically formed a continuum. The turbulent history of the area, particularly due to expansion of the Ottoman Empire, resulted in a patchwork of dialectal and religious differences. Due to population migrations, the Shtokavian dialect became the most widespread in the western Balkans, intruding westwards into the area previously occupied by Chakavian and Kajkavian dialects (which further blend into the Slovene language in the northwest). Bosniaks, Croats and Serbs differ in religion and were historically often part of different cultural circles, although a large part of the nations have lived side by side under foreign overlords. During that period, the language was referred to under a variety of names, such as "Slavic", "Illyrian", or according to region, "Bosnian", "Serbian" and "Croatian", the latter often in combination with "Slavonian" or "Dalmatian".
Serbo-Croatian was standardized in the mid-19th century Vienna Literary Agreement of Croatian and Serbian writers and philologists, decades before a Yugoslav state was established. From the very beginning, there were slightly different literary Serbian and Croatian standards, although both were based on the same Shtokavian subdialect, Eastern Herzegovinian. In the 20th century, Serbo-Croatian served as the official language of the Kingdom of Yugoslavia (when it was called "Serbo-Croato-Slovenian"), and later as one of the official languages of the Socialist Federal Republic of Yugoslavia. The dissolution of Yugoslavia affected language attitudes, so that social conceptions of the language separated on ethnic and political lines. Since the breakup of Yugoslavia, Bosnian has likewise been established as an official standard in Bosnia and Herzegovina, and there is an ongoing movement to codify a separate Montenegrin standard. Serbo-Croatian thus generally goes by the ethnic names Serbian, Croatian, Bosnian, and sometimes Montenegrin.
Like other South Slavic languages, Serbo-Croatian has a simple phonology, with the common five-vowel system and twenty-five consonants. Its grammar evolved from Common Slavic, with complex inflection, preserving seven grammatical cases in nouns, pronouns, and adjectives. Verbs exhibit imperfective or perfective aspect, with a moderately complex tense system. Serbo-Croatian is a pro-drop language with flexible word order, subject–verb–object being the default. It can be written in Serbian Cyrillic or Gaj's Latin alphabet, whose thirty letters mutually map one-to-one, and the orthography is highly phonemic in all standards.
Name.
Throughout the history of the South Slavs, the vernacular, literary, and written languages (e.g. Chakavian, Kajkavian, Shtokavian) of the various regions and ethnicities developed and diverged independently. Prior to the 19th century, they were collectively called "Illyric", "Slavic", "Slavonian", "Bosnian", "Dalmatian", "Serbian" or "Croatian". As such, the term "Serbo-Croatian" was first used by Jacob Grimm in 1824, popularized by the Vienna philologist Jernej Kopitar in the following decades, and accepted by Croatian Zagreb grammarians in 1854 and 1859. At that time, Serb and Croat lands were still part of the Ottoman and Austrian Empires. Officially, the language was called variously "Serbo-Croat, Croato-Serbian, Serbian and Croatian, Croatian and Serbian, Serbian or Croatian, Croatian or Serbian." Unofficially, Serbs and Croats typically called the language "Serbian" or "Croatian", respectively, without implying a distinction between the two, and again recently in newly independent Bosnia and Herzegovina, "Bosnian", "Croatian", and "Serbian" were considered to be three names of a single official language. Croatian linguist Dalibor Brozović advocated the term "Serbo-Croatian" as late as 1988, claiming that in an analogy with Indo-European, Serbo-Croatian does not only name the two components of the same language, but simply charts the limits of the region in which it is spoken and includes everything between the limits (‘Bosnian’ and ‘Montenegrin’). Today, use of the term "Serbo-Croatian" is controversial due to the prejudice that nation and language must match. It is still used for lack of a succinct alternative, though alternative names have been used, such as "Bosnian/Croatian/Serbian" (BCS), which is often seen in political contexts such as the Hague War Crimes tribunal.
History.
Early development.
The beginning of written Serbo-Croatian can be traced to the 9th century, when Old Church Slavonic was adopted as the language of the liturgy. This language was gradually adapted to non-liturgical purposes and became known as the Croatian version of Old Slavonic. The two variants of the language, liturgical and non-liturgical, continued to be a part of the Glagolitic service as late as the middle of the 19th century. The earliest known Croatian Church Slavonic Glagolitic manuscripts are the "Glagolita Clozianus" and the "Vienna Folia" from the 11th century.
From the 10th century and on Serbo-Croatian medieval texts were written in five scripts: Latin, Glagolitic, Early Cyrillic, Bosnian Cyrillic ("bosančica/bosanica"), and Arebica, the last principally by Bosniak nobility. Serbo-Croatian competed with the more established literary languages of Latin and Old Slavonic in the west and Persian and Arabic in the east. Old Slavonic developed into the Serbo-Croatian variant of Church Slavonic between the 12th and 16th centuries.
Among the earliest attestations of Serbo-Croatian are the Humac tablet, dating from the 10th or 11th century, written in Bosnian Cyrillic and Glagolitic; the Plomin tablet, dating from the same era, written in Glagolitic; the Valun tablet, dated to the 11th century, written in Glagolitic and Latin; and the Inscription of Župa Dubrovačka, a Glagolitic tablet dated to the 11th century.
The Baška tablet from the late 11th century was written in Glagolitic. It is a large stone tablet found in the small Church of St. Lucy, Jurandvor on the Croatian island of Krk that contains text written mostly in Chakavian in the Croatian script. It is also important in the history of the nation as it mentions Zvonimir, the king of Croatia at the time.
The Charter of Ban Kulin of 1189, written by Ban Kulin of Bosnia, was an early Shtokavian text, written in Bosnian Cyrillic.
The luxurious and ornate representative texts of Serbo-Croatian Church Slavonic belong to the later era, when they coexisted with the Serbo-Croatian vernacular literature. The most notable are the "Missal of Duke Novak" from the Lika region in northwestern Croatia (1368), "Evangel from Reims" (1395, named after the town of its final destination), Hrvoje's Missal from Bosnia and Split in Dalmatia (1404), and the first printed book in Serbo-Croatian, the Glagolitic Missale Romanum Glagolitice (1483).
During the 13th century Serbo-Croatian vernacular texts began to appear, the most important among them being the "Istrian land survey" of 1275 and the "Vinodol Codex" of 1288, both written in the Chakavian dialect.
The Shtokavian dialect literature, based almost exclusively on Chakavian original texts of religious provenance (missals, breviaries, prayer books) appeared almost a century later. The most important purely Shtokavian vernacular text is the Vatican Croatian Prayer Book (c. 1400).
Both the language used in legal texts and that used in Glagolitic literature gradually came under the influence of the vernacular, which considerably affected its phonological, morphological, and lexical systems. From the 14th and the 15th centuries, both secular and religious songs at church festivals were composed in the vernacular.
Writers of early Serbo-Croatian religious poetry ("začinjavci") gradually introduced the vernacular into their works. These "začinjavci" were the forerunners of the rich literary production of the 16th-century literature, which, depending on the area, was Chakavian-, Kajkavian-, or Shtokavian-based. The language of religious poems, translations, miracle and morality plays contributed to the popular character of medieval Serbo-Croatian literature.
One of the earliest dictionaries, also in the Slavic languages as a whole, was the "Bosnian–Turkish Dictionary" of 1631 authored by Muhamed Hevaji Uskufi and was written in the Arebica script.
Modern standardization.
In the mid-19th century, Serbian (led by self-taught writer and folklorist Vuk Stefanović Karadžić) and most Croatian writers and linguists (represented by the Illyrian movement and led by Ljudevit Gaj and Đuro Daničić), proposed the use of the most widespread dialect, Shtokavian, as the base for their common standard language. Karadžić standardised the Serbian Cyrillic alphabet, and Gaj and Daničić standardized the Croatian Latin alphabet, on the basis of vernacular speech phonemes and the principle of phonological spelling. In 1850 Serbian and Croatian writers and linguists signed the Vienna Literary Agreement, declaring their intention to create a unified standard. Thus a complex bi-variant language appeared, which the Serbs officially called "Serbo-Croatian" or "Serbian or Croatian" and the Croats "Croato-Serbian", or "Croatian or Serbian". Yet, in practice, the variants of the conceived common literary language served as different literary variants, chiefly differing in lexical inventory and stylistic devices. The common phrase describing this situation was that Serbo-Croatian or "Croatian or Serbian" was a single language. During the Austro-Hungarian occupation of Bosnia and Herzegovina, the language of all three nations was called "Bosnian" until the death of administrator von Kállay in 1907, at which point the name was changed to "Serbo-Croatian".
With unification of the first the Kingdom of the Serbs, Croats, and Slovenes – the approach of Karadžić and the Illyrians became dominant. The official language was called "Serbo-Croato-Slovenian" ("srpsko-hrvatsko-slovenački") in the 1921 constitution. In 1929, the constitution was suspended, and the country was renamed the Kingdom of Yugoslavia, while the official language of Serbo-Croato-Slovene was reinstated in the 1931 constitution.
In June 1941, the Nazi puppet Independent State of Croatia began to rid the language of "Eastern" (Serbian) words, and shut down Serbian schools.
On January 15, 1944, the Anti-Fascist Council of the People's Liberation of Yugoslavia (AVNOJ) declared Croatian, Serbian, Slovene, and Macedonian to be equal in the entire territory of Yugoslavia. In 1945 the decision to recognize Croatian and Serbian as separate languages was reversed in favor of a single Serbo-Croatian or Croato-Serbian language. In the Communist-dominated second Yugoslavia, ethnic issues eased to an extent, but the matter of language remained blurred and unresolved.
In 1954, major Serbian and Croatian writers, linguists and literary critics, backed by Matica srpska and Matica hrvatska signed the Novi Sad Agreement, which in its first conclusion stated: "Serbs, Croats and Montenegrins share a single language with two equal variants that have developed around Zagreb (western) and Belgrade (eastern)". The agreement insisted on the equal status of Cyrillic and Latin scripts, and of Ekavian and Ijekavian pronunciations. It also specified that "Serbo-Croatian" should be the name of the language in official contexts, while in unofficial use the traditional "Serbian" and "Croatian" were to be retained. Matica hrvatska and Matica srpska were to work together on a dictionary, and a committee of Serbian and Croatian linguists was asked to prepare a "pravopis". During the sixties both books were published simultaneously in Ijekavian Latin in Zagreb and Ekavian Cyrillic in Novi Sad. Yet Croatian linguists claim that it was an act of unitarianism. The evidence supporting this claim is patchy: Croatian linguist Stjepan Babić complained that the television transmission from Belgrade always used the Latin alphabet— which was true, but was not proof of unequal rights, but of frequency of use and prestige. Babić further complained that the Novi Sad Dictionary (1967) listed side by side words from both the Croatian and Serbian variants wherever they differed, which one can view as proof of careful respect for both variants, and not of unitarism. Moreover, Croatian linguists criticized those parts of the Dictionary for being unitaristic that were written by Croatian linguists. And finally, Croatian linguists ignored the fact that the material for the "Pravopisni rječnik" came from the Croatian Philological Society. Regardless of these facts, Croatian intellectuals brought the Declaration on the Status and Name of the Croatian Literary Language in 1967. On occasion of the publication’s 45th anniversary, the Croatian weekly journal Forum published the Declaration again in 2012, accompanied by a critical analysis.
West European scientists judge the Yugoslav language policy as an exemplary one: although three-quarters of the population spoke one language, no single language was official on a federal level. Official languages were declared only at the level of constituent republics and provinces, and very generously: Vojvodina had five (among them Slovak and Romanian, spoken by 0.5 per cent of the population), and Kosovo four (Albanian, Turkish, Romany and Serbo-Croatian). Newspapers, radio and television studios used sixteen languages, fourteen were used as languages of tuition in schools, and nine at universities. Only the Yugoslav Army used Serbo-Croatian as the sole language of command, with all other languages represented in the army’s other activities—however, this is not different from other armies of multilingual states, or in other specific institutions, such as international air traffic control where English is used worldwide. All variants of Serbo-Croatian were used in state administration and republican and federal institutions. Both Serbian and Croatian variants were represented in respectively different grammar books, dictionaries, school textbooks and in books known as pravopis (which detail spelling rules). Serbo-Croatian was a kind of soft standardisation. However, legal equality could not dampen the prestige Serbo-Croatian had: since it was the language of three quarters of the population, it functioned as an unofficial lingua franca. And within Serbo-Croatian, the Serbian variant, with twice as many speakers as the Croatian, enjoyed greater prestige, reinforced by the fact that Slovene and Macedonian speakers preferred it to the Croatian variant because their languages are also Ekavian. This is a common situation in other pluricentric languages, e.g. the variants of German differ according to their prestige, the variants of Portuguese too. Moreover, all languages differ in terms of prestige: "the fact is that languages (in terms of prestige, learnability etc.) are not equal, and the law cannot make them equal".
Demographics.
The total number of persons who declared their native language as either 'Bosnian', 'Croatian', 'Serbian', 'Montenegrin', or 'Serbo-Croatian' in countries of the region is about 16 million.
Serbian is spoken by about 9.5 million mostly in Serbia (6.7m), Bosnia and Herzegovina (1.4m), and Montenegro (0.4m). Serbian minorities are found in the Republic of Macedonia and in Romania. In Serbia, there are about 760,000 second-language speakers of Serbian, including Hungarians in Vojvodina and the 400,000 estimated Roma. Familiarity of Kosovo Albanians with Serbian in Kosovo varies depending on age and education, and exact numbers are not available.
Croatian is spoken by roughly 4.8 million including some 575,000 in Bosnia and Herzegovina. A small Croatian minority lives in Italy known as Molise Croats have somewhat preserved traces of the Croatian language. In Croatia, 170,000 mostly Italians and Hungarians use it as a second language.
Bosnian is spoken by 2.2 million people, chiefly Bosniaks, including about 220,000 in Serbia and Montenegro.
Notion of Montenegrin as a separate standard from Serbian is relatively recent. In the 2003 census, around 150,000 Montenegrins, of the country's 620,000, declared Montenegrin as their native language. That figure is likely to increase since, due to the country's independence and strong institutional backing of Montenegrin language.
Serbo-Croatian is also a second language of many Slovenians and Macedonians, especially those born during the time of Yugoslavia. According to the 2002 Census, Serbo-Croatian and its variants have the largest number of speakers of the minority languages in Slovenia.
Outside the Balkans, there are over 2 million native speakers of the language(s), especially in countries which are frequent targets of immigration, such as Australia, Austria, Brazil, Canada, Chile, Germany, Hungary, Italy, Sweden and the United States.
Grammar.
Serbo-Croatian is a highly inflected language. Traditional grammars list seven cases for nouns and adjectives: nominative, genitive, dative, accusative, vocative, locative, and instrumental, reflecting the original seven cases of Proto-Slavic, and indeed older forms of Serbo-Croatian itself. However, in modern Shtokavian the locative has almost merged into dative (the only difference is based on accent in some cases), and the other cases can be shown declining; namely:
Like most Slavic languages, there are mostly three genders for nouns: masculine, feminine, and neuter, a distinction which is still present even in the plural (unlike Russian and, in part, the Čakavian dialect). They also have two numbers: singular and plural. However, some consider there to be three numbers (paucal or "dual," too), since (still preserved in closely related Slovene) after two ("dva", "dvije"/"dve"), three ("tri") and four ("četiri"), and all numbers ending in them (e.g. twenty-two, ninety-three, one hundred four) the genitive singular is used, and after all other numbers five ("pet") and up, the genitive plural is used. (The number one ["jedan"] is treated as an adjective.) Adjectives are placed in front of the noun they modify and must agree in both case and number with it.
There are seven tenses for verbs: past, present, future, exact future, aorist, imperfect, and plusquamperfect; and three moods: indicative, imperative, and conditional. However, the latter three tenses are typically used only in Shtokavian writing, and the time sequence of the exact future is more commonly formed through an alternative construction.
In addition, like most Slavic languages, the Shtokavian verb also has one of two aspects: perfective or imperfective. Most verbs come in pairs, with the perfective verb being created out of the imperfective by adding a prefix or making a stem change. The imperfective aspect typically indicates that the action is unfinished, in progress, or repetitive; while the perfective aspect typically denotes that the action was completed, instantaneous, or of limited duration. Some Štokavian tenses (namely, aorist and imperfect) favor a particular aspect (but they are rarer or absent in Čakavian and Kajkavian). Actually, aspects "compensate" for the relative lack of tenses, because aspect of the verb determines whether the act is completed or in progress in the referred time.
Phonology.
Vowels.
The Serbo-Croatian vowel system is simple, with only five vowels in Shtokavian. All vowels are monophthongs. The oral vowels are as follows:
The vowels can be short or long, but the phonetic quality doesn't change depending on the length. In a word, vowels can be long in the stressed syllable and the syllables following it, never in the ones preceding it.
Consonants.
The consonant system is more complicated, and its characteristic features are series of affricate and palatal consonants. As in English, voice is phonemic, but aspiration is not.
In consonant clusters all consonants are either voiced or voiceless. All the consonants are voiced (if the last consonant is normally voiced) or voiceless (if the last consonant is normally voiceless). This rule does not apply to approximants – a consonant cluster may contain voiced approximants and voiceless consonants; as well as to foreign words ("Washington" would be transcribed as "VašinGton"), personal names and when consonants are not inside of one syllable.
/r/ can be syllabic, playing the role of the syllable nucleus in certain words (occasionally, it can even have a long accent). For example, the tongue-twister "navrh brda vrba mrda" involves four words with syllabic /r/. A similar feature exists in Czech, Slovak, and Macedonian. Very rarely other sonorants can be syllabic, like /l/ (in "bicikl"), /ʎ/ (surname "Štarklj"), /n/ (unit "njutn"), as well as /m/ and /ɲ/ in slang.
Pitch accent.
Apart from Slovene, Serbo-Croatian is the only Slavic language with a pitch accent (simple tone) system. This feature is present in some other Indo-European languages, such as Swedish, Norwegian, and Ancient Greek. Neo-Shtokavian Serbo-Croatian, which is used as the basis for standard Bosnian, Croatian, Montenegrin, and Serbian, has four "accents", which involve either a rising or falling tone on either long or short vowels, with optional post-tonic lengths:
The tone stressed vowels can be approximated in English with "set" vs. "setting?" said in isolation for a short tonic "e," or "leave" vs. "leaving?" for a long tonic "i," due to the prosody of final stressed syllables in English.
General accent rules in the standard language:
There are no other rules for accent placement, thus the accent of every word must be learned individually; furthermore, in inflection, accent shifts are common, both in type and position (the so-called "mobile paradigms"). The second rule is not strictly obeyed, especially in borrowed words.
Comparative and historical linguistics offers some clues for memorising the accent position: If one compares many standard Serbo-Croatian words to e.g. cognate Russian words, the accent in the Serbo-Croatian word will be one syllable before the one in the Russian word, with the rising tone. Historically, the rising tone appeared when the place of the accent shifted to the preceding syllable (the so-called "Neoštokavian retraction"), but the quality of this new accent was different – its melody still "gravitated" towards the original syllable. Most Štokavian dialects (Neoštokavian) dialects underwent this shift, but Čakavian, Kajkavian and the Old Štokavian dialects did not.
Accent diacritics are not used in the ordinary orthography, but only in the linguistic or language-learning literature (e.g. dictionaries, orthography and grammar books). However, there are very few minimal pairs where an error in accent can lead to misunderstanding.
Orthography.
Serbo-Croatian orthography is almost entirely phonetic. Thus, most words should be spelled as they are pronounced. In practice, the writing system does not take into account allophones which occur as a result of interaction between words:
Also, there are some exceptions, mostly applied to foreign words and compounds, that favor morphological/etymological over phonetic spelling:
One systemic exception is that the consonant clusters ds and dš do not change into ts and tš (although "d" tends to be unvoiced in normal speech in such clusters):
Only a few words are intentionally "misspelled", mostly in order to resolve ambiguity:
Writing systems.
Through history, this language has been written in a number of writing systems:
The oldest texts since the 11th century are in Glagolitic, and the oldest preserved text written completely in the Latin alphabet is "Red i zakon sestara reda Svetog Dominika", from 1345. Arabic alphabet formerly was used by Bosniaks; Greek writing recently is out of use there, and Arabic and Glagolitic persisted so far partly in religious liturgies.
Today, it is written in both the Latin and Cyrillic scripts. Serbian and Bosnian variants use both alphabets, while Croatian uses the Latin only.
The Serbian Cyrillic alphabet was revised by Vuk Stefanović Karadžić in the 19th century.
The Croatian Latin alphabet ("Gajica") followed suit shortly afterwards, when Ljudevit Gaj defined it as standard Latin with five extra letters that had diacritics, apparently borrowing much from Czech, but also from Polish, and inventing the unique digraphs "lj", "nj" and "dž". These digraphs are represented as "ļ, ń and ǵ" respectively in the "Rječnik hrvatskog ili srpskog jezika", published by the former Yugoslav Academy of Sciences and Arts in Zagreb. The latter digraphs, however, are unused in the literary standard of the language. All in all, this makes Serbo-Croatian the only Slavic language to officially use both the Latin and Cyrillic scripts, albeit the Latin version is more commonly used.
In both cases, spelling is phonetic and spellings in the two alphabets map to each other one-to-one:
Latin to Cyrillic
Cyrillic to Latin
The digraphs "Lj", "Nj" and "Dž" represent distinct phonemes and are considered to be single letters. In crosswords, they are put into a single square, and in sorting, lj follows l and nj follows n, except in a few words where the individual letters are pronounced separately. For instance, "nadživ(j)eti" "to outlive" is composed of the prefix "nad-" "out, over" and the verb "živ(j)eti" "to live". The Cyrillic alphabet avoids such ambiguity by providing a single letter for each phoneme.
"Đ" used to be commonly written as "Dj" on typewriters, but that practice led to too many ambiguities. It is also used on car license plates. Today "Dj" is often used again in place of "Đ" on the Internet as a replacement due to the lack of installed Serbo-Croat keyboard layouts.
Dialects.
South Slavic historically formed a dialect continuum, i.e. each dialect has some similarities with the neighboring one, and differences grow with distance. However, migrations from the 16th to 18th centuries resulting from the spread of Ottoman Empire on the Balkans have caused large-scale population displacement that broke the dialect continuum into many geographical pockets. Migrations in the 20th century, primarily caused by urbanization and wars, also contributed to the reduction of dialectal differences.
The primary dialects are named after the most common question word for "what": Shtokavian uses the pronoun "što" or "šta", Chakavian uses "ča" or "ca", Kajkavian ("kajkavski"), "kaj" or "kej". In native terminology they are referred to as "nar(j)ečje", which would be equivalent of "group of dialects", whereas their many subdialects are referred to as "dijalekti ""dialects" or "govori ""speeches".
The pluricentric Serbo-Croatian standard language and all four contemporary standard variants are based on the Eastern Herzegovinian subdialect of Neo-Shtokavian. Other dialects are not taught in schools or used by the state media. The Torlakian dialect is often added to the list, though sources usually note that it is a transitional dialect between Shtokavian and the Bulgaro-Macedonian dialects.
The Serbo-Croatian dialects differ not only in the question word they are named after, but also heavily in phonology, accentuation and intonation, case endings and tense system (morphology) and basic vocabulary. In the past, Chakavian and Kajkavian dialects were spoken on a much larger territory, but have subsequently been replaced by Štokavian during the period of migrations caused by Ottoman Turkish conquest of the Balkans in the 15th and the 16th centuries. These migrations caused the koinéisation of the Shtokavian dialects, that used to form the West Shtokavian (more closer and transitional towards the neighbouring Chakavian and Kajkavian dialects) and East Shtokavian (transitional towards the Torlakian and the whole Bulgaro-Macedonian area) dialect bundles, and their subsequent spread at the expense of Chakavian and Kajkavian. As a result, Štokavian now covers an area larger than all the other dialects combined, and continues to make its progress in the enclaves where non-literary dialects are still being spoken.
The differences among the dialects can be illustrated on the example of Schleicher's fable. Diacritic signs are used to show the difference in accents and prosody, which are often quite significant, but which are not reflected in the usual orthography.
Division by "jat" reflex.
A basic distinction among the dialects is in the reflex of the long Common Slavic vowel "jat", usually transcribed as *ě. Depending on the reflex, the dialects are divided into Ikavian, Ekavian, and Ijekavian, with the reflects of "jat" being /i/, /e/, and /ije/ or /je/ respectively. The long and short "jat" is reflected as long or short */i/ and /e/ in Ikavian and Ekavian, but Ijekavian dialects introduce a "ije"/"je" alternation to retain a distinction.
Standard Croatian and Bosnian are based on Ijekavian, whereas Serbian uses both Ekavian and Ijekavian forms (Ijekavian for Bosnian Serbs, Ekavian for most of Serbia). Influence of standard language through state media and education has caused non-standard varieties to lose ground to the literary forms.
The jat-reflex rules are not without exception. For example, when short "jat" is preceded by "r", in most Ijekavian dialects developed into /re/ or, occasionally, /ri/. The prefix "prě-" ("trans-, over-") when long became "pre-" in eastern Ijekavian dialects but to "prije-" in western dialects; in Ikavian pronunciation, it also evolved into "pre-" or "prije-" due to potential ambiguity with "pri-" ("approach, come close to"). For verbs that had "-ěti " in their infinitive, the past participle ending "-ěl" evolved into "-io" in Ijekavian Neoštokavian.
The following are some examples:
Present sociolinguistic situation.
Comparison with other pluricentric languages.
Enisa Kafadar argues that there is only one Serbo-Croatian language with several varieties. This has made possible to include all four varieties into a new grammar book. Daniel Bunčić concludes that it is a pluricentric language, with four standard variants spoken in Serbia, Croatia, Montenegro and Bosnia and Herzegovina. The mutual intelligibility between their speakers "exceeds that between the standard variants of English, French, German, or Spanish". Sean McLennan argues that the differences between the variants of Serbo-Croatian are less significant than those between the variants of English. Heinz-Dieter Pohl maintains that the differences between the variants of Serbo-Croatian are less significant than those between the variants of German. Bernhard Gröschel asserts that the differences between the variants of Serbo-Croatian are less significant than those between the Dutch and the Flemish variants of Dutch. Gröschel argues that even linguistic differences between Whites and Blacks in the USA major cities exceed those between the standard variants of Serbo-Croatian. Daniel Blum maintains that the distinctions between the variants of Serbo-Croatian are less significant than those between the Hindi and the Urdu variants of Hindustani.
Among pluricentric languages, Serbo-Croatian was the only one with a pluricentric standardisation within one state. The dissolution of Yugoslavia has made Serbo-Croatian even more typical pluricentric language, since the variants of other pluricentric languages are also spoken in different states.
Contemporary names.
The current Serbian constitution of 2006 refers to the official language as "Serbian", while the Montenegrin constitution of 2007 proclaimed "Montenegrin" as the primary official language, but also grants other languages the right of official use.
The International Organization for Standardization (ISO) has specified different Universal Decimal Classification (UDC) numbers for Croatian "(UDC 862," abbreviation hr) and Serbian "(UDC 861", abbreviation sr), while the cover term "Serbo-Croatian" is used to refer to the combination of original signs ("UDC 861/862," abbreviation sh). Furthermore, the "ISO 639" standard designates the Bosnian language with the abbreviations bos and bs.
The International Criminal Tribunal for the former Yugoslavia considers what it calls "BCS" (Bosnian-Croatian-Serbian) to be the main language of all Bosnian, Croatian, and Serbian defendants. The indictments, documents, and verdicts of the ICTY are not written with any regard for consistently following the grammatical prescriptions of any of the three standards – be they Serbian, Croatian, or Bosnian.
For utilitarian purposes, the Serbo-Croatian language is often called "Naš jezik" ("Our language") by native speakers. This politically correct term is frequently used to describe the Serbo-Croatian language by those who wish to avoid nationalistic and linguistic discussions.
Views of linguists in the former Yugoslavia.
Serbian linguists.
The majority of mainstream Serbian linguists consider Serbian and Croatian to be one language, that is called Serbo-Croatian ("srpskohrvatski") or Croato-Serbian ("hrvatskosrpski"). A minority of Serbian linguists are of the opinion that Serbo-Croatian did exist, but has, in the meantime, dissolved.
Croatian linguists.
The opinion of the majority of Croatian linguists is that there has never been a Serbo-Croatian language, but two different standard languages that overlapped sometime in the course of history. However, Croatian linguist Snježana Kordić has been leading an academic discussion on that issue in the Croatian journal "Književna republika" from 2001 to 2010. In the discussion, she shows that linguistic criteria such as mutual intelligibility, huge overlap in linguistic system, and the same dialectic basis of standard language provide evidence that Croatian, Serbian, Bosnian and Montenegrin are four national variants of the pluricentric Serbo-Croatian language. Igor Mandić states: "During the last ten years, it has been the longest, the most serious and most acrid discussion (...) in 21st-century Croatian culture". Inspired by that discussion, a monograph on language and nationalism has been published.
The views of the majority of Croatian linguists that there is no Serbo-Croatian language, but several different standard languages, have been sharply criticized by German linguist Bernhard Gröschel in his monograph "Serbo-Croatian Between Linguistics and Politics".
A more detailed overview, incorporating arguments from the Croatian philology and contemporary linguistics, would be as follows:
The linguistic debate in this region is more about politics than about linguistics per se.
The topic of language for writers from Dalmatia and Dubrovnik prior to the 19th century made a distinction only between speakers of Italian or Slavic, since those were the two main groups that inhabited Dalmatian city-states at that time. Whether someone spoke Croatian or Serbian was not an important distinction then, as the two languages were not distinguished by most speakers. This has been used as an argument to state that Croatian literature Croatian per se, but also includes Serbian and other languages that are part of Serbo-Croatian, These facts undermine the Croatian language proponents' argument that modern-day Croatian is based on a language called Old Croatian.
However, most intellectuals and writers from Dalmatia who used the Štokavian dialect and practiced the Catholic faith saw themselves as part of a Croatian nation as far back as the mid-16th to 17th centuries, some 300 years before Serbo-Croatian ideology appeared. Their loyalty was first and foremost to Catholic Christendom, but when they professed an ethnic identity, they referred to themselves as "Slovin" and "Illyrian" (a sort of forerunner of Catholic baroque pan-Slavism) and Croat – these 30-odd writers over the span of c. 350 years always saw themselves as Croats first and never as part of a Serbian nation. It should also be noted that, in the pre-national era, Catholic religious orientation did not necessarily equate with Croat ethnic identity in Dalmatia. A Croatian follower of Vuk Karadžić, Ivan Broz, noted that for a Dalmatian to identify oneself as a Serb was seen as foreign as identifying oneself as Macedonian or Greek. Vatroslav Jagić pointed out in 1864:
On the other hand, the opinion of Jagić from 1864 is argued not to have firm grounds. When Jagić says "Croatian", he refers to a few cases referring to the Dubrovnik vernacular as "ilirski" (Illyrian). This was a common name for all Slavic vernaculars in Dalmatian cities among the Roman inhabitants. In the meantime, other written monuments are found that mention "srpski", "lingua serviana" (= Serbian), and some that mention Croatian. By far the most competent Serbian scientist on the Dubrovnik language issue, Milan Rešetar, who was born in Dubrovnik himself, wrote behalf of language characteristics: "The one who thinks that Croatian and Serbian are two separate languages must confess that Dubrovnik always (linguistically) used to be Serbian."
Finally, the former "medieval" texts from Dubrovnik and Montenegro dating before the 16th century were neither true Štokavian nor Serbian, but mostly specific a Jekavian-Čakavian that was nearer to actual Adriatic islanders in Croatia.
Political connotations.
Nationalists have conflicting views about the language(s). The nationalists among the Croats conflictingly claim either that they speak an entirely separate language from Serbs and Bosnians or that these two peoples have, due to the longer lexicographic tradition among Croats, somehow "borrowed" their standard languages from them. Bosniak nationalists claim that both Croats and Serbs have "appropriated" the Bosnian language, since Ljudevit Gaj and Vuk Karadžić preferred the Neoštokavian-Ijekavian dialect, widely spoken in Bosnia and Herzegovina, as the basis for language standardization, whereas the nationalists among the Serbs claim either that any divergence in the language is artificial, or claim that the Štokavian dialect is theirs and the Čakavian Croats'— in more extreme formulations Croats have "taken" or "stolen" their language from the Serbs. 
Proponents of unity among Southern Slavs claim that there is a single language with normal dialectal variations. The term "Serbo-Croatian" (or synonyms) is not officially used in any of the successor countries of former Yugoslavia.
In Serbia, the Serbian language is the official one, while both Serbian and Croatian are official in the province of Vojvodina. A large Bosniak minority is present in the southwest region of Sandžak, but the "official recognition" of Bosnian language is moot. Bosnian is an optional course in 1st and 2nd grade of the elementary school, while it is also in official use in the municipality of Novi Pazar. However, its nomenclature is controversial, as there is incentive that it is referred to as "Bosniak" ("bošnjački") rather than "Bosnian" ("bosanski") (see Bosnian language for details).
Croatian is the official language of Croatia, while Serbian is also official in municipalities with significant Serb population.
In Bosnia and Herzegovina, all three languages are recorded as official but in practice and media, mostly Bosnian and Serbian are applied. Confrontations have on occasion been absurd. The academic Muhamed Filipović, in an interview to Slovenian television, told of a local court in a Croatian district requesting a paid translator to translate from Bosnian to Croatian before the trial could proceed.

</doc>
<doc id="27737" url="http://en.wikipedia.org/wiki?curid=27737" title="Saint Kitts">
Saint Kitts

Saint Kitts, also known more formally as Saint Christopher Island ("Saint-Christophe" in French), is an island in the West Indies. The west side of the island borders the Caribbean Sea, and the eastern coast faces the Atlantic Ocean. Saint Kitts and the neighbouring island of Nevis constitute one country: the Federation of Saint Kitts and Nevis.
The island is one of the Leeward Islands in the Lesser Antilles. It is situated about 2100 km southeast of Miami, Florida. The land area of St. Kitts is about 168 km2, being approximately 29 km long and on average about 8 km across.
Saint Kitts has a population of around 45,000, the majority of whom are mainly of African descent. The primary language is English, with a literacy rate of approximately 98%. Residents call themselves Kittitians.
Brimstone Hill Fortress National Park, a UNESCO World Heritage Site, is the largest fortress ever built in the Eastern Caribbean. The island of Saint Kitts is home to the Warner Park Cricket Stadium, which was used to host 2007 Cricket World Cup matches. This made St. Kitts and Nevis the smallest nation to ever host a World Cup event. Saint Kitts is also home to several institutions of higher education, including Ross University School of Veterinary Medicine, Windsor University School of Medicine, and the University of Medicine and Health Sciences.
Geography.
The capital of the two-island nation, and also its largest port, is the town of Basseterre on Saint Kitts. There is a modern facility for handling large cruise ships there. A ring road goes around the perimeter of the island with smaller roads branching off of it; the interior of the island is too steep for habitation.
Saint Kitts is 10 km away from Sint Eustatius to the north and 3 km from Nevis to the south. St. Kitts has three distinct groups of volcanic peaks: the North West or Mount Misery Range; the Middle or Verchilds Range and the South East or Olivees Range. The highest peak is Mount Liamuiga, formerly Mount Misery, a dormant volcano 1,156 m high.
Parishes.
There are nine parishes on the island of St. Kitts:
Economy.
St. Kitts & Nevis uses the Eastern Caribbean dollar, which maintains a fixed exchange rate of 2.7-to-one with the United States dollar. The US dollar is almost as widely accepted as the Eastern Caribbean dollar.
For hundreds of years, St. Kitts operated as a sugar monoculture, but due to decreasing profitability, the government closed the industry in 2005. Tourism is a major and growing source of income to the island, although the number and density of resorts is less than on many other Caribbean islands. Transportation, non-sugar agriculture, manufacturing and construction are the other growing sectors of the economy.
St. Kitts is dependent on tourism to drive its economy. Tourism has been increasing since 1978. In 2009, there were 587,479 arrivals to Saint Kitts compared to 379,473 in 2007, which represents an increase of just under 40% growth in a two-year period. As tourism grows, the demand for vacation property increases in conjunction.
St. Kitts & Nevis also acquires foreign direct investment from their unique citizenship by investment program, outlined in their Citizenship Act of 1984. Interested parties can acquire citizenship if they pass the government's strict background checks and make an investment into an approved real estate development. Purchasers who pass government due diligence and make a minimum investment of US$400,000, into qualifying government approved real estate, are entitled to apply for citizenship of the Federation of St. Kitts and Nevis. Many projects are approved under the citizenship by investment program, and the main qualifying projects of interest can be found within the Henley Estates market overview .
In addition to this, in hopes of expanding tourism, the country hosts its annual St. Kitts Music Festival.
History.
During the last Ice Age, the sea level was 200 ft lower and St. Kitts and Nevis were one island along with Sint Eustatius (also known as Statia).
St. Kitts was originally settled by pre-agricultural, pre-ceramic "Archaic people", who migrated south down the archipelago from Florida. In a few hundred years they disappeared, to be replaced by the ceramic-using and agriculturalist Saladoid people around 100 BC, who migrated to St. Kitts north up the archipelago from the banks of the Orinoco River in Venezuela. Around 800 AD, they were replaced by the Igneri people, members of the Arawak group.
Around 1300, the Kalinago, or Carib people arrived on the islands. These war-like people quickly dispersed the Igneri, and forced them northwards to the Greater Antilles. They named Saint Kitts "Liamuiga" meaning "fertile island", and would likely have expanded further north if not for the arrival of Europeans.
A Spanish expedition under Christopher Columbus discovered and claimed the island for Spain in 1493. A short-lived French Huguenot settlement was established at Dieppe Bay in 1538.
The first English colony was established in 1623, followed by a French colony in 1625. The English and French briefly united to massacre the local Kalinago (preempting a Kalinago plan to massacre the Europeans), and then partitioned the island, with the English colonists in the middle and the French on either end. In 1629, a Spanish force sent to clear the islands of foreign settlement seized St. Kitts. The English settlement was rebuilt following the 1630 peace between England and Spain.
The island alternated repeatedly between English (then British) and French control during the 17th and 18th centuries, as one power took the whole island, only to have it switch hands due to treaties or military action. Parts of the island were heavily fortified, as exemplified by the UNESCO World Heritage Site at Brimstone Hill and the now-crumbling Fort Charles.
Since 1783, St. Kitts has been affiliated with the Kingdom of Great Britain, which became the United Kingdom.
Slavery.
The island originally produced tobacco; but it changed to sugar cane in 1640, due to stiff competition from the colony of Virginia. The labour-intensive cultivation of sugar cane was the reason for the large-scale importation of African slaves. The importation began almost immediately upon the arrival of Europeans to the region.
The purchasing of enslaved Africans was outlawed in the British Empire by an Act of Parliament in 1807. Slavery was abolished by an Act of Parliament which became law on 1 August 1834. This emancipation was followed by four years of apprenticeship, put in place to protect the planters from losing their labour force.
August the 1st is now celebrated as a public holiday and is called Emancipation Day. In 1883, St. Kitts, Nevis, and Anguilla were all linked under one presidency, located on St. Kitts, to the dismay of the Nevisians and Anguillans. Anguilla eventually separated out of this arrangement, in 1971, after an armed raid on St. Kitts.
Sugar production continued to dominate the local economy until 2005, when, after 365 years of having a mono-culture, the government closed the sugar industry. This was due to huge losses and European Union plans to greatly cut sugar prices.
Transportation.
Robert L. Bradshaw International Airport serves St. Kitts. British Airways flies in twice a week from London and daily connections from Miami and New York are available.
The Basseterre Ferry Terminal facilitates travel between St. Kitts and sister island Nevis.
The narrow-gauge (30 inches) St Kitts Scenic Railway circles the island and offers passenger service from its headquarters near the airport, although the service is geared more for tourists than as day-to-day transportation for residents. Built between 1912 and 1926 to haul sugar cane from farms to the sugar factory in Basseterre, since 2003 the railway has offered a 3.5 hour, 30-mile circle tour of the island on specially designed double-decker open-air coaches, with 12 miles of the trip being by bus.
Notable residents.
Saint Kitts is or was the residence of:

</doc>
<doc id="27739" url="http://en.wikipedia.org/wiki?curid=27739" title="Shogi">
Shogi

Shogi (将棋, shōgi) (, ] or ]), also known as Japanese chess or the Generals' Game, is a two-player strategy board game in the same family as Western (international) chess, chaturanga, makruk, shatranj and xiangqi, and is the most popular of a family of chess variants native to Japan. "Shōgi" means general's ("shō" 将) board game ("gi" 棋).
The earliest predecessor of the game, chaturanga, originated in India in the 6th century, and sometime in the 10th to 12th centuries xiangqi (Chinese chess) was brought to Japan where it spawned a number of variants. Shogi in its present form was played as early as the 16th century, while a direct ancestor without the "drop rule" was recorded from 1210 in a historical document "Nichūreki", which is an edited copy of "Shōchūreki" and "Kaichūreki" from the late Heian period (c. 1120).
According to "The Chess Variant Pages" :Perhaps the enduring popularity of shogi can be attributed to its "drop rule"; it was the first chess variant wherein captured pieces could be returned to the board to be used as one's own. David Pritchard credits the drop rule to the practice of 16th century mercenaries who switched loyalties when captured—no doubt as an alternative to execution.
Equipment.
Two players, "Sente" 先手 (Black; more literally, "person with the first move") and "Gote" 後手 (White; "person with the second move"), play on a board composed of rectangles in a grid of 9 "ranks" (rows) by 9 "files" (columns). The rectangles are undifferentiated by marking or color. The board is nearly always rectangular; square boards are uncommon. Pairs of dots mark the players' promotion zones.
Each player has a set of 20 wedge-shaped pieces of slightly different sizes. Except for the kings, opposing pieces are undifferentiated by marking or color. Pieces face "forward" (toward the opponent's side); this shows who controls the piece during play. The pieces from largest (most important) to smallest (least important) are:
Several of these names were chosen to correspond to their rough equivalents in international chess, and not as literal translations of the Japanese names.
Each piece has its name written on its surface in the form of two "kanji" (Chinese characters used in Japanese), usually in black ink. On the reverse side of each piece, other than the king and gold general, are one or two other characters, in amateur sets often in a different color (usually red); this side is turned face up during play to indicate that the piece has been promoted.
The suggestion that the Japanese characters have deterred Western players from learning shogi has led to "Westernized" or "international" pieces which use iconic symbols instead of characters. Most players soon learn to recognize the characters, however, partially because the traditional pieces are already iconic by size, with more powerful pieces being larger. As a result Westernized pieces have never become popular. Bilingual pieces with both Japanese characters and English captions have been developed.
Following is a table of the pieces with their Japanese representations and English equivalents. The abbreviations are used for game notation and often when referring to the pieces in speech in Japanese.
English speakers sometimes refer to promoted bishops as "horses" and promoted rooks as "dragons", after their Japanese names, and generally use the Japanese term "tokin" for promoted pawns. Silver generals and gold generals are commonly referred to simply as "silvers" and "golds".
The characters inscribed on the reverse sides of the pieces to indicate promotion may be in red ink, and are usually cursive. The characters on the backs of the pieces that promote to gold generals are cursive variants of 金 'gold', becoming more cursive (more abbreviated) as the value of the original piece decreases. These cursive forms have these equivalents in print: 全 for promoted silver, 今 for promoted knight, 仝 for promoted lance, and 个 for promoted pawn (tokin). Another typographic convention has abbreviated versions of the original values, with a reduced number of strokes: 圭 for a promoted knight (桂), 杏 for a promoted lance (香), and the 全 as above for a promoted silver, but と for "tokin".
Setup and gameplay.
Each player sets up his pieces facing forward (toward his opponent).
Traditionally, even the order of placing the pieces on the board is determined. There are two commonly used orders, "Ohashi" and "Ito". Placement sets pieces with multiples (generals, knights, lances) from left to right in all cases, and follows the order:
One player takes Black and moves first; then players alternate turns. (The terms "Black" and "White" are used to differentiate sides although there is no difference in the color of the pieces.) For each turn a player may either move a piece that is currently on the board (and potentially promote it, capture an opposing piece, or both) or else "drop" a piece that has been previously captured onto an empty square of the board. These options are explained below.
Professional games are timed as in international chess, but professionals are never expected to keep time in their games. Instead a timekeeper is assigned, typically an apprentice professional. Time limits are much longer than in international chess (9 hours a side plus extra time in the prestigious "Meijin" title match), and in addition "byōyomi" (literally "second counting") is employed. This means that when the ordinary time has run out, the player will from that point on have a certain amount of time to complete every move (a "byōyomi" period), typically upwards of one minute. The final ten seconds are counted down, and if the time expires the player to move loses the game immediately. Amateurs often play with electronic clocks that beep out the final ten seconds of a "byōyomi" period, with a prolonged beep for the last five.
An illegal move results in an immediate loss of the game in professional and tournament shogi, even if play continued and the move was discovered later.
Rules.
Movement.
Most shogi pieces can move only to an adjacent square. A few may move across the board, and one jumps over intervening pieces. Shogi pieces capture the same as they move.
Every piece blocks the movement of all other non-jumping pieces through the square it occupies. If a piece occupies a legal destination for an opposing piece, it may be "captured" by removing it from the board and replacing it with the opposing piece. The capturing piece may not continue beyond that square on that turn.
It is common to keep captured pieces on a wooden stand (or "komadai)" which is traditionally placed so that its bottom left corner aligns with the bottom right corner of the board from the perspective of each player. It is not permissible to hide pieces from full view. This is because captured pieces, which are said to be "pieces "in hand"" (持ち駒, mochi goma), have a crucial impact on the course of the game.
The knight "jumps", that is, it passes "over" any intervening piece, whether friend or foe, without an effect on either. It is the only piece to do this.
The lance, bishop, and rook are "ranging" pieces: They can move any number of squares along a straight line limited only by intervening pieces and the edge of the board. If an opposing piece intervenes, it may be captured by removing it from the board and replacing it with the moving piece. If a friendly piece intervenes, the moving piece must stop short of that square; if the friendly piece is adjacent, the moving piece may not move in that direction at all.
All pieces but the knight move either horizontally, vertically, or diagonally. These directions cannot be combined in a single move; one direction must be chosen.
Normally when moving a piece, a player snaps it to the board with the ends of the fingers of the same hand. This makes a sudden sound effect, bringing the piece to the attention of the opponent. This is also true for capturing and dropping pieces. On a traditional "shogi-ban", the pitch of the snap is deeper, delivering a subtler effect.
King.
A king moves one square in any direction, orthogonal or diagonal.
Rook.
A rook moves any number of squares in an orthogonal direction.
Bishop.
A bishop moves any number of squares in a diagonal direction.
Because they cannot move orthogonally, the players' unpromoted bishops can reach only half the squares of the board, unless one is captured and then dropped.
Gold general.
A gold general moves one square orthogonally, or one square diagonally forward, giving it six possible destinations. It cannot move diagonally backwards.
Silver general.
A silver general moves one square diagonally, or one square straight forward, giving it five possible destinations.
Because an unpromoted silver can retreat more easily than a promoted one, it is common to leave a silver unpromoted at the far side of the board (see Promotion).
Knight.
A knight "jumps" at an angle intermediate to orthogonal and diagonal, amounting to one square straight forward plus one square diagonally forward, in a single move. Thus the knight has two possible forward destinations. The knight cannot move to the sides or in a backwards direction.
The knight is the only piece that ignores intervening pieces on the way to its destination. It is not blocked from moving if the square in front of it is occupied, but neither can it capture a piece on that square.
It is often useful to leave a knight unpromoted at the far side of the board. A knight "must" promote, however, if it reaches either of the two furthest ranks (see Promotion).
Lance.
A lance moves any number of squares directly forward. It cannot move backwards or to the sides.
It is often useful to leave a lance unpromoted at the far side of the board . A lance "must" promote, however, if it arrives at the furthest rank (see Promotion).
Pawn.
A pawn moves one square straight forward. It cannot retreat. Unlike international chess pawns, shogi pawns capture the same as they move. 
A pawn "must" promote if it arrives at the furthest rank (see Promotion). In practice, however, a pawn is usually promoted whenever possible.
There are two restrictions on where a pawn may be dropped (see Drops).
Promotion.
A player's "promotion zone" consists of the furthest one-third of the board – the three ranks occupied by the opponent's pieces at setup. The zone is typically delineated on shogi boards by two inscribed dots. When a piece is moved, if part of the piece's path lies within the promotion zone (that is, if the piece moves into, out of, or wholly within the zone; but "not" if it is dropped into the zone – see Drops), then the player has the option to "promote" the piece at the end of the turn. Promotion is indicated by turning the piece over after it moves, revealing the character of the promoted piece.
If a pawn or lance is moved to the furthest rank, or a knight is moved to either of the two furthest ranks, that piece "must" promote (otherwise, it would have no legal move on subsequent turns). A silver general is never required to promote, and it is often advantageous to keep a silver general unpromoted. (It is easier, for example, to extract an unpromoted silver from behind enemy lines; whereas a promoted silver, with only one line of retreat, can be easily blocked.)
Promoting a piece changes the way it moves. The various pieces promote as follows:
When captured, a piece loses its promoted status. Otherwise promotion is permanent.
Promoted rook.
A promoted rook ("dragon king", "Ryūō") moves as a rook or as a king, but not as both on the same turn.
Promoted bishop.
A promoted bishop ("dragon horse", "Ryūma") moves as a bishop or as a king, but not as both on the same turn.
Promoted silver.
A promoted silver ("narigin") moves the same as a gold general.
Promoted knight.
A promoted knight ("narikei") moves the same as a gold general.
Promoted lance.
A promoted lance ("narikyō") moves the same as a gold general.
Promoted pawn.
A promoted pawn ("tokin") moves the same as a gold general.
Drops.
Captured pieces are retained "in hand", and can be brought back into play under the capturing player's control. On any turn, instead of moving a piece on the board, a player may select a piece in hand and place it—unpromoted side up and facing the opposing side—on any empty square. The piece is then one of that player's active pieces on the board and can be moved accordingly. This is called "dropping" the piece, or simply, a "drop". A drop counts as a complete move.
A drop cannot capture a piece, nor does dropping within the promotion zone result in immediate promotion. Capture and/or promotion may occur normally, however, on subsequent moves of the piece.
A pawn, knight, or lance may not be dropped on the furthest rank, since those pieces would have no legal moves on subsequent turns. For the same reason, a knight may not be dropped on the penultimate (player's 8th) rank.
There are two additional restrictions when dropping pawns:
It is common for players to swap bishops, which oppose each other across the board, early in the game. This leaves each player with a bishop in hand to be dropped later. The ability for drops in shogi give the game tactical richness and complexity. The fact that no piece ever goes entirely out of play, accounts for the rarity of draws.
Winning.
When a player's move threatens to capture the opposing king on the next turn, the move is said to "give check" to the king; the king is said to be "in check". If a player's king is in check, that player's responding move must remove the check if possible; if no such move exists, the checking move is also "checkmate" (tsumi 詰み) and immediately wins the game. The losing player should resign out of courtesy at this point, although in practice this rarely occurs, as players normally resign as soon as a loss is deemed inevitable.
To announce "check!" in Japanese, one says "ōte!" (王手). This is an influence of international chess and is not required, however, even as a courtesy.
In professional and serious amateur games, a player who makes an illegal move loses immediately.
There are two other possible, if uncommon, ways for a game to end: "repetition" (千日手 "sennichite") and "impasse" (持将棋 "jishōgi"):
As this impasse generally needs to be agreed on for the rule to be invoked, a player may refuse to do so, on the grounds that the player could gain further material or position before an outcome has to be decided. If that happens, one player may force "jishōgi" upon getting his king and all his pieces protected in the promotion zone.
In professional tournaments the rules typically require drawn games to be replayed with colors (sides) reversed, possibly with reduced time limits. This is rare compared to chess and xiangqi, occurring at a rate of 1–2% even in amateur games. The 1982 "Meijin" title match between Makoto Nakahara and Hifumi Kato was unusual in this regard, with "jishōgi" in the first game (only the fifth draw in the then 40-year history of the tournament), a game which lasted for 223 moves (not counting in pairs of moves), with 114 minutes spent pondering a single move, and "sennichite" in the sixth and eighth games. Thus this best-of-seven match lasted eight games and took over three months to finish; Black did not lose a single game and the eventual victor was Kato at 4–3.
Player rank and handicaps.
Amateur players are ranked from 15 "kyū" to 1 "kyū" and then from 1 "dan" and upwards; this is the same terminology as in many other arts in Japan. Professional players operate with their own scale, from professional 4 "dan" and upwards to 9 "dan" for elite players. Amateur and professional ranks are offset (with amateur 4 "dan" being equivalent to professional 6 "kyū").
Games between players of disparate strengths are often played with handicaps. In a handicap game, one or more of White's pieces are removed from the setup, and in exchange White plays first. Note that the missing pieces are not available for drops and play no further part in the game. The imbalance created by this method of handicapping is not as strong as it is in international chess because material advantage is not as powerful in shogi.
Common handicaps, in increasing order of severity, include the following:
Other handicaps are also occasionally used, especially when teaching the game to new players. The relationship between handicaps and differences in rank is not universally agreed upon, with several systems in use.
If a "jishōgi" occurs in a handicap game, the removed pieces are counted towards White's total.
Notation.
The method used in English-language texts to express shogi moves was established by George Hodges in 1976. It is derived from the algebraic notation used for chess, but differs in several respects. It is not used in Japanese-language texts, as it is no more concise than traditional notation with kanji and two ciphers which was originated in Edo period.
A typical move might be notated P-8f. The first letter represents the piece moved: P for Pawn. (There is also L/lance, N/knight, S/silver, G/gold, B/bishop, R/rook, and K/king.) Promoted pieces are indicated by a + preceding the letter: +P is a tokin (promoted pawn).
Following the abbreviation for the piece is a symbol for the type of move: - (hyphen) for a simple move, x for a capture, or * (asterisk) for a drop. Next is the square on which the piece lands. This is indicated by a numeral for the file and a lowercase letter for the rank, with 1a being the top right corner (Black's perspective) and 9i being the bottom left corner. This is based on Japanese convention, which, however, uses Japanese numerals instead of letters. (For example, square 2c is "2三" in Japanese.)
If a move entitles the player to promote, then a + is added to the end if the promotion was taken, or an = if it was declined. For example, Nx7c= indicates a knight capturing on 7c without promoting.
In cases where the piece is ambiguous, the starting square is added to the letter for the piece. For example, at setup Black has two golds which can move to square 5h (in front of the king). These are distinguished as G6i-5h (from the left) and G4i-5h (from the right).
Moves are numbered per player's move, unlike chess which counts each pair of moves as one move. For example, the start of a game might look like this:
In handicap games White plays first, so Black's move 1 is replaced by an ellipsis.
Strategy and tactics.
Shogi is similar to chess but has a much larger game tree complexity because of the use of drops. Like chess, however, the game can be divided into the opening, middle game and endgame, each requiring a different strategy. The opening consists of arranging one's defenses and positioning for attack, the mid game consists of attempting to break through the opposing defenses while maintaining one's own, and the end game starts when one side's defenses have been compromised.
History.
From "The Chess Variant Pages":The world's first chess variant chaturanga arose in India in approximately the seventh century AD. From there it migrated both westward and northward, mutating along the way. The western branch became shatranj in Arabia and Orthodox Chess in Europe. The northern branch became xiangqi in China and janggi in Korea. Sometime in the 10th to 12th centuries, 'chess' crossed the channel to Japan where it spawned a number of interesting variants. One of these was called 'Small Shogi'. Eventually, Small Shogi (though it went through many forms) won out over the larger variants and is now referred to simply as 'Shogi'. It is certain that Shogi in its present form was played in Japan as early as the 16th century. 
It is not clear when chess was brought to Japan. The earliest generally accepted mention of shogi is Shin Saru Gakuki (新猿楽記) (1058–64) by Fujiwara Akihira. The oldest archaeological evidence is a group of 16 shogi pieces excavated from the grounds of Kōfuku-ji in Nara Prefecture. As it was physically associated with a wooden tablet written on in the sixth year of Tenki (1058), the pieces are thought to date from that period. These simple pieces were cut from a writing plaque in the same five-sided shape as modern pieces, with the names of the pieces written on them.
The dictionary of common folk culture, Nichūreki (二中歴) (c. 1210–21), a collection based on the two works Shōchūreki (掌中歴) and Kaichūreki (懐中歴), describes two forms of shogi, large "(dai)" shogi and small "(shō)" shogi. These are now called Heian shogi (or Heian small shogi) and Heian dai shogi. Heian small shogi is the version on which modern shogi is based, but the "Nichūreki" states that one wins if one's opponent is reduced to a single king, indicating that drops had not yet been introduced. According to Kōji Shimizu, chief researcher at the Archaeological Institute of Kashihara, Nara Prefecture, the names of the Heian shogi pieces keep those of chaturanga (general, elephant, horse, chariot and soldier), and add to them the five treasures of Buddhism (jade, gold, silver, katsura tree, and incense).
Around the 13th century the game of dai shogi developed, created by increasing the number of pieces in Heian shogi, as was sho shogi, which added the rook, bishop, and drunken elephant from dai shogi to Heian shogi. Around the 15th century, the rules of dai shogi were simplified, creating the game of chu shogi in a form close to the modern game. It is thought that the rules of standard shogi were fixed in the 16th century, when the drunken elephant was removed from the set of pieces. There is no clear record of when drops were introduced, however.
In the Edo period, shogi variants were greatly expanded: tenjiku shogi, dai dai shogi, maka dai dai shogi, tai shogi, and taikyoku shogi were all invented. It is thought that these were played to only a very limited extent, however. Both standard shogi and Go were promoted by the Tokugawa shogunate. In 1612, the shogunate passed a law giving endowments to top shogi players (Meijin (名人)). During the reign of the eighth shogun, Tokugawa Yoshimune, castle shogi tournaments were held once a year on the 17th day of Kannazuki, corresponding to November 17, which is Shogi Day on the modern calendar.
The title of "meijin" became hereditary in the Ōhashi and Itō families until the fall of the shogunate, when it came to be passed by recommendation. Today the title is used for the winner of the Meijin-sen competition, the first modern title match. From around 1899, newspapers began to publish records of shogi matches, and high-ranking players formed alliances with the aim of having their games published. In 1909, the Shogi Association (将棋同盟社) was formed, and in 1924, the Tokyo Shogi Association (東京将棋同盟社) was formed. This was an early incarnation of the modern Japan Shogi Association (日本将棋連盟, nihon shōgi renmei), or JSA, and 1924 is considered by the JSA to be the date it was founded.
In 1935, "meijin" Kinjirō Sekine stepped down, and the rank of meijin came to be awarded to the winner of a Meijin title match (名人戦, meijin-sen). Yoshio Kimura (木村義雄) became the first Meijin under this system in 1937. This was the start of the (see titleholder system). After the war other tournaments were promoted to title matches, culminating with the Ryūō title match (竜王戦, ryūō-sen) in 1988 for the modern line-up of seven. About 200 professional shogi players compete. Each year, the title holder defends the title against a challenger chosen from knockout or round matches.
After the Second World War, SCAP (occupational government mainly led by US) tried to eliminate all "feudal" factors from Japanese society and shogi was included in the possible list of items to be banned along with Bushido (philosophy of samurai) and other things. The reason for banning shogi for SCAP was its exceptional character as a board game seen in the usage of captured pieces. SCAP insisted that this could lead to the idea of prisoner abuse. But Kozo Masuda, then one of the top professional shogi players, when summoned to the SCAP headquarters for an investigation, criticized such understanding of shogi and insisted that it is not shogi but western chess that potentially contains the idea of prisoner abuse because it just kills the pieces of the opponent while shogi is rather democratic for giving prisoners the chance to get back into the game. Masuda also said that chess contradicts the ideal of gender equality in western society because the king shields itself behind the queen and runs away. Masuda’s assertion is said to have eventually led to the exemption of shogi from the list of items to be banned.
The closest cousin of shogi in the chaturanga family is makruk of Thailand. Not only the similarity in distribution and movements of the pieces but also the names of shogi pieces suggest intimacy between shogi and makruk by its Buddhist symbolism (gold, silver, Cassia and Incense), which is not recognized in Chinese chess at all. In fact, Chinese chess and its East Asian variants are far remoter relatives than makruk. Though some early variants of chaturanga more similar to shogi and makruk are known to have been played in Tang Dynasty China, they are thought to have been extinguished in Song Dynasty China and in East Asia except in Japan probably owing to the popularity of Chinese chess.
Tournament play.
There are two organizations for shogi professional players in Japan: the JSA, and the Ladies Professional Shogi-players' Association of Japan (日本女子プロ将棋協会, nihon joshi puro shōgi kyōkai), or LPSA. The JSA is the primary organization for men and women's professional shogi while the LPSA is a group of women professionals who broke away from the JSA in 2007 to establish their own independent organization. Both organize tournaments for their members and have reached an agreement to cooperate with each other to promote shogi through events and other activities.
The JSA recognizes two categories of shogi professionals: Professional (棋士, "Kishi"), and Female Professional (女流棋士, "Joryūkishi"). Sometimes "kishi" are addressed as Seikishi (正棋士), a term from Go used to distinguish "kishi" from other classes of players. JSA professional ranks and female professional ranks are not equivalent and each has their own promotion criteria and ranking system. In 2006, the JSA officially granted women "professional status". This is not equivalent, however, to the more traditional way of "gaining professional status", i.e., being promoted from the "Shoreikai System" (奨励会): leagues of strong amateur players aspiring to become a professional. Rather, it is a separate system especially designed for female professionals. Qualified amateurs, regardless of gender, may apply for the "Shoreikai System" and all those who successfully "graduate" are granted "kishi" status; however, no woman has yet to accomplish this feat, so "kishi" is de facto only used to refer to male shogi professionals.
The JSA is the only body which can organize tournaments for "professionals", e.g., the seven major tournaments in the titleholder system and other professional tournaments. In 1996, Yoshiharu Habu became the only "kishi" to hold all existing seven major titles at the same time. For female professionals, both the JSA and LPSA organize tournaments, either jointly or separately. Tournaments for amateurs may be organized by the JSA and LPSA as well as local clubs, newspapers, private corporations, educational institutions or municipal governments for cities or prefectures under the guidance of the JSA or LPSA.
Since the 1990s, shogi has grown in popularity outside Japan, particularly in the People's Republic of China, and especially in Shanghai. The January 2006 edition of "Kindai Shogi" (近代将棋) stated that there were 120,000 shogi players in Shanghai. The spread of the game to countries where Chinese characters are not in common use, however, has been slower.
Etiquette.
Shogi players are expected to follow etiquette in addition to rules explicitly described. Commonly accepted etiquette include following:
Shogi piece set may contain two types of king pieces, 王 (king) and 玉 (jewel). In this case, the higher classed player, in either social or genuine shogi player rank, may take the king piece. For example, in titleholder system games, the current titleholder takes the king piece as the higher.
Computer shogi.
Shogi has the highest game complexity of all popular chess variants. Computers have steadily improved in playing shogi since the 1970s. In 2007, champion Yoshiharu Habu estimated the strength of the 2006 world computer shogi champion Bonanza at the level of two-dan shoreikai.
The JSA prohibits its professionals from playing computers in public without prior permission, with the reason of promoting shogi and monetizing the computer-human events.
On October 12, 2010, after some 35 years of development, a computer finally beat a professional player, when the top ranked female champion Ichiyo Shimizu was beaten by the Akara2010 system in a game lasting just over 6 hours.
On July 24, 2011, computer shogi programs Bonanza and Akara crushed the amateur team of Kosaku and Shinoda in two games. The allotted time for the amateurs was one hour and then three minutes per move. The allotted time for the computer was 25 minutes and then 10 seconds per move.
On April 20, 2013, GPS Shogi defeated 8-dan professional shogi player Hiroyuki Miura in a 102-move game which lasted over 8 hours.
The highest rated player on Shogi Club 24 is computer program Bonkras, rated 3335 on December 2, 2011.
Shogi video games.
Hundreds of video games were released exclusively in Japan for several consoles.
In popular culture.
Shogi has been a central plot point in manga and anime "Shion no Ō", manga and television series "81diver" and manga "March Comes in Like a Lion".
In the manga series "Naruto", shogi plays an essential part in Shikamaru Nara's character development. He often plays it with his teacher, Asuma Sarutobi, apparently always beating him. When Asuma is fatally injured in battle, he reminds Shikamaru that the shogi king must always be protected, and draws a parallel between the king in shogi and his yet-unborn daughter, Mirai, whom he wanted Shikamaru to guide. After Asuma's death, Shikamaru realizes that Asuma really meant that the entire next generation of ninja should be protected.
References.
Bibliography
</dl>
External links.
Rules
Online play

</doc>
<doc id="27743" url="http://en.wikipedia.org/wiki?curid=27743" title="Solar energy">
Solar energy

Solar energy is radiant light and heat from the sun harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, solar thermal energy, solar architecture and artificial photosynthesis.
It is an important source of renewable energy and its technologies are broadly characterized as either passive solar or active solar depending on the way they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power and solar water heating to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air.
In 2011, the International Energy Agency said that "the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared".
Energy from the Sun.
The Earth receives 174,000 terawatts (TW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most people around the world live in areas with insolation levels of 150 to 300 watt per square meter or 3.5 to 7.0 kWh/m2 per day.
Earth's land surface, oceans and atmosphere absorb solar radiation, and this raises their temperature. Warm air containing evaporated water from the oceans rises, causing atmospheric circulation or convection. When the air reaches a high altitude, where the temperature is low, water vapor condenses into clouds, which rain onto the Earth's surface, completing the water cycle. The latent heat of water condensation amplifies convection, producing atmospheric phenomena such as wind, cyclones and anti-cyclones. Sunlight absorbed by the oceans and land masses keeps the surface at an average temperature of 14 °C. By photosynthesis green plants convert solar energy into chemical energy, which produces food, wood and the biomass from which fossil fuels are derived.
The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 3,850,000 exajoules (EJ) per year. In 2002, this was more energy in one hour than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass. The technical potential available from biomass is from 100–300 EJ/year. The amount of solar energy reaching the surface of the planet is so vast that in one year it is about twice as much as will ever be obtained from all of the Earth's non-renewable resources of coal, oil, natural gas, and mined uranium combined,
Solar energy can be harnessed at different levels around the world, mostly depending on distance from the equator.
Solar energy refers primarily to the use of solar radiation for practical ends. However, all renewable energies, other than geothermal and tidal, derive their energy from the sun.
Solar technologies are broadly characterized as either passive or active depending on the way they capture, convert and distribute sunlight. Active solar techniques use photovoltaic panels, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternate resources and are generally considered demand side technologies. CSP-Stirling is known to have the highest efficiency of all solar technologies (around 30%, compared to solar PV's approximately 15%) and is predicted to be able to produce the cheapest energy among all renewable energy sources in high scale production and hot areas, semi-deserts, etc.
Thermal energy.
Solar thermal technologies can be used for water heating, space heating, space cooling and process heat generation.
Early commercial adaption.
In 1897, Frank Shuman, a U.S. inventor, engineer and solar energy pioneer built a small demonstration solar engine that worked by reflecting solar energy onto square boxes filled with ether, which has a lower boiling point than water, and were fitted internally with black pipes which in turn powered a steam engine. In 1908 Shuman formed the Sun Power Company with the intent of building larger solar power plants. He, along with his technical advisor A.S.E. Ackermann and British physicist Sir Charles Vernon Boys, developed an improved system using mirrors to reflect solar energy upon collector boxes, increasing heating capacity to the extent that water could now be used instead of ether. Shuman then constructed a full-scale steam engine powered by low-pressure water, enabling him to patent the entire solar engine system by 1912.
Shuman built the world’s first solar thermal power station in Maadi, Egypt, between 1912 and 1913. Shuman’s plant used parabolic troughs to power a 45 – engine that pumped more than 22000 litres of water per minute from the Nile River to adjacent cotton fields. Although the outbreak of World War I and the discovery of cheap oil in the 1930s discouraged the advancement of solar energy, Shuman’s vision and basic design were resurrected in the 1970s with a new wave of interest in solar thermal energy. In 1916 Shuman was quoted in the media advocating solar energy's utilization, saying:
We have proved the commercial profit of sun power in the tropics and have more particularly proved that after our stores of oil and coal are exhausted the human race can receive unlimited power from the rays of the sun.—Frank Shuman, New York Times, July 2, 1916
Water heating.
Solar hot water systems use sunlight to heat water. In low geographical latitudes (below 40 degrees) from 60 to 70% of the domestic hot water use with temperatures up to 60 °C can be provided by solar heating systems. The most common types of solar water heaters are evacuated tube collectors (44%) and glazed flat plate collectors (34%) generally used for domestic hot water; and unglazed plastic collectors (21%) used mainly to heat swimming pools.
As of 2007, the total installed capacity of solar hot water systems is approximately 154 thermal gigawatt (GWth). China is the world leader in their deployment with 70 GWth installed as of 2006 and a long-term goal of 210 GWth by 2020. Israel and Cyprus are the per capita leaders in the use of solar hot water systems with over 90% of homes using them. In the United States, Canada and Australia heating swimming pools is the dominant application of solar hot water with an installed capacity of 18 GWth as of 2005.
Heating, cooling and ventilation.
In the United States, heating, ventilation and air conditioning (HVAC) systems account for 30% (4.65 EJ) of the energy used in commercial buildings and nearly 50% (10.1 EJ) of the energy used in residential buildings. Solar heating, cooling and ventilation technologies can be used to offset a portion of this energy.
Thermal mass is any material that can be used to store heat—heat from the Sun in the case of solar energy. Common thermal mass materials include stone, cement and water. Historically they have been used in arid climates or warm temperate regions to keep buildings cool by absorbing solar energy during the day and radiating stored heat to the cooler atmosphere at night. However, they can be used in cold temperate areas to maintain warmth as well. The size and placement of thermal mass depend on several factors such as climate, daylighting and shading conditions. When properly incorporated, thermal mass maintains space temperatures in a comfortable range and reduces the need for auxiliary heating and cooling equipment.
A solar chimney (or thermal chimney, in this context) is a passive solar ventilation system composed of a vertical shaft connecting the interior and exterior of a building. As the chimney warms, the air inside is heated causing an updraft that pulls air through the building. Performance can be improved by using glazing and thermal mass materials in a way that mimics greenhouses.
Deciduous trees and plants have been promoted as a means of controlling solar heating and cooling. When planted on the southern side of a building in the northern hemisphere or the northern side in the southern hemisphere, their leaves provide shade during the summer, while the bare limbs allow light to pass during the winter. Since bare, leafless trees shade 1/3 to 1/2 of incident solar radiation, there is a balance between the benefits of summer shading and the corresponding loss of winter heating. In climates with significant heating loads, deciduous trees should not be planted on the Equator facing side of a building because they will interfere with winter solar availability. They can, however, be used on the east and west sides to provide a degree of summer shading without appreciably affecting winter solar gain.
Cooking.
Solar cookers use sunlight for cooking, drying and pasteurization. They can be grouped into three broad categories: box cookers, panel cookers and reflector cookers. The simplest solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. It can be used effectively with partially overcast skies and will typically reach temperatures of 90-150 C. Panel cookers use a reflective panel to direct sunlight onto an insulated container and reach temperatures comparable to box cookers. Reflector cookers use various concentrating geometries (dish, trough, Fresnel mirrors) to focus light on a cooking container. These cookers reach temperatures of 315 C and above but require direct light to function properly and must be repositioned to track the Sun.
Process heat.
Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste streams. Clothes lines, clotheshorses, and clothes racks dry clothes through evaporation by wind and sunlight without consuming electricity or gas. In some states of the United States legislation protects the "right to dry" clothes. Unglazed transpired collectors (UTC) are perforated sun-facing walls used for preheating ventilation air. UTCs can raise the incoming air temperature up to 22 C-change and deliver outlet temperatures of 45 -. The short payback period of transpired collectors (3 to 12 years) makes them a more cost-effective alternative than glazed collection systems. As of 2003, over 80 systems with a combined collector area of 35000 m2 had been installed worldwide, including an 860 m2 collector in Costa Rica used for drying coffee beans and a 1300 m2 collector in Coimbatore, India, used for drying marigolds.
Water treatment.
Solar distillation can be used to make saline or brackish water potable. The first recorded instance of this was by 16th-century Arab alchemists. A large-scale solar distillation project was first constructed in 1872 in the Chilean mining town of Las Salinas. The plant, which had solar collection area of 4700 m2, could produce up to 22700 L per day and operate for 40 years. Individual still designs include single-slope, double-slope (or greenhouse type), vertical, conical, inverted absorber, multi-wick, and multiple effect. These stills can operate in passive, active, or hybrid modes. Double-slope stills are the most economical for decentralized domestic purposes, while active multiple effect units are more suitable for large-scale applications.
Solar water disinfection (SODIS) involves exposing water-filled plastic polyethylene terephthalate (PET) bottles to sunlight for several hours. Exposure times vary depending on weather and climate from a minimum of six hours to two days during fully overcast conditions. It is recommended by the World Health Organization as a viable method for household water treatment and safe storage. Over two million people in developing countries use this method for their daily drinking water.
Solar energy may be used in a water stabilisation pond to treat waste water without chemicals or electricity. A further environmental advantage is that algae grow in such ponds and consume carbon dioxide in photosynthesis, although algae may produce toxic chemicals that make the water unusable.
Electricity production.
Solar power is the conversion of sunlight into electricity, either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP). CSP systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. PV converts light into electric current using the photoelectric effect.
Solar power is anticipated to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16 and 11 percent to the global overall consumption, respectively.
Commercial CSP plants were first developed in the 1980s. Since 1985 the eventually 354 MW SEGS CSP installation, in the Mojave Desert of California, is the largest solar power plant in the world. Other large CSP plants include the 150 MW Solnova Solar Power Station and the 100 MW Andasol solar power station, both in Spain. The 250 MW Agua Caliente Solar Project, in the United States, and the 221 MW Charanka Solar Park in India, are the world’s largest photovoltaic plants. Solar projects exceeding 1 GW are being developed, but most of the deployed photovoltaics are in small rooftop arrays of less than 5 kW, which are grid connected using net metering and/or a feed-in tariff.
Photovoltaics.
      Europe
      Asia-Pacific
      Americas
      China
      Middle East and Africa
Worldwide growth of PV capacity grouped by region in MW (2006–2014)
In the last two decades, photovoltaics (PV), also known as solar PV, has evolved from a pure niche market of small scale applications towards becoming a mainstream electricity source. A solar cell is a device that converts light directly into electricity using the photoelectric effect. The first solar cell was constructed by Charles Fritts in the 1880s. In 1931 a German engineer, Dr Bruno Lange, developed a photo cell using silver selenide in place of copper oxide. Although the prototype selenium cells converted less than 1% of incident light into electricity, both Ernst Werner von Siemens and James Clerk Maxwell recognized the importance of this discovery. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the crystalline silicon solar cell in 1954. These early solar cells cost 286 USD/watt and reached efficiencies of 4.5–6%. By 2012 available efficiencies exceed 20% and the maximum efficiency of research photovoltaics is over 40%.
Concentrated solar power.
Concentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the concentrating linear fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the Sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage.
Architecture and urban planning.
Sunlight has influenced building design since the beginning of architectural history. Advanced solar architecture and urban planning methods were first employed by the Greeks and Chinese, who oriented their buildings toward the south to provide light and warmth.
The common features of passive solar architecture are orientation relative to the Sun, compact proportion (a low surface area to volume ratio), selective shading (overhangs) and thermal mass. When these features are tailored to the local climate and environment they can produce well-lit spaces that stay in a comfortable temperature range. Socrates' Megaron House is a classic example of passive solar design. The most recent approaches to solar design use computer modeling tying together solar lighting, heating and ventilation systems in an integrated solar design package. Active solar equipment such as pumps, fans and switchable windows can complement passive design and improve system performance.
Urban heat islands (UHI) are metropolitan areas with higher temperatures than that of the surrounding environment. The higher temperatures are a result of increased absorption of the Solar light by urban materials such as asphalt and concrete, which have lower albedos and higher heat capacities than those in the natural environment. A straightforward method of counteracting the UHI effect is to paint buildings and roads white and plant trees. Using these methods, a hypothetical "cool communities" program in Los Angeles has projected that urban temperatures could be reduced by approximately 3 °C at an estimated cost of US$1 billion, giving estimated total annual benefits of US$530 million from reduced air-conditioning costs and healthcare savings.
Agriculture and horticulture.
Agriculture and horticulture seek to optimize the capture of solar energy in order to optimize the productivity of plants. Techniques such as timed planting cycles, tailored row orientation, staggered heights between rows and the mixing of plant varieties can improve crop yields. While sunlight is generally considered a plentiful resource, the exceptions highlight the importance of solar energy to agriculture. During the short growing seasons of the Little Ice Age, French and English farmers employed fruit walls to maximize the collection of solar energy. These walls acted as thermal masses and accelerated ripening by keeping plants warm. Early fruit walls were built perpendicular to the ground and facing south, but over time, sloping walls were developed to make better use of sunlight. In 1699, Nicolas Fatio de Duillier even suggested using a tracking mechanism which could pivot to follow the Sun. Applications of solar energy in agriculture aside from growing crops include pumping water, drying crops, brooding chicks and drying chicken manure. More recently the technology has been embraced by vinters, who use the energy generated by solar panels to power grape presses.
Greenhouses convert solar light to heat, enabling year-round production and the growth (in enclosed environments) of specialty crops and other plants not naturally suited to the local climate. Primitive greenhouses were first used during Roman times to produce cucumbers year-round for the Roman emperor Tiberius. The first modern greenhouses were built in Europe in the 16th century to keep exotic plants brought back from explorations abroad. Greenhouses remain an important part of horticulture today, and plastic transparent materials have also been used to similar effect in polytunnels and row covers.
Transport.
Development of a solar-powered car has been an engineering goal since the 1980s. The World Solar Challenge is a biannual solar-powered car race, where teams from universities and enterprises compete over 3021 km across central Australia from Darwin to Adelaide. In 1987, when it was founded, the winner's average speed was 67 km/h and by 2007 the winner's average speed had improved to 90.87 km/h.
The North American Solar Challenge and the planned South African Solar Challenge are comparable competitions that reflect an international interest in the engineering and development of solar powered vehicles.
Some vehicles use solar panels for auxiliary power, such as for air conditioning, to keep the interior cool, thus reducing fuel consumption.
In 1975, the first practical solar boat was constructed in England. By 1995, passenger boats incorporating PV panels began appearing and are now used extensively. In 1996, Kenichi Horie made the first solar powered crossing of the Pacific Ocean, and the "sun21" catamaran made the first solar powered crossing of the Atlantic Ocean in the winter of 2006–2007. There were plans to circumnavigate the globe in 2010.
In 1974, the unmanned AstroFlight Sunrise plane made the first solar flight. On 29 April 1979, the "Solar Riser" made the first flight in a solar-powered, fully controlled, man carrying flying machine, reaching an altitude of 40 ft. In 1980, the "Gossamer Penguin" made the first piloted flights powered solely by photovoltaics. This was quickly followed by the "Solar Challenger" which crossed the English Channel in July 1981. In 1990 Eric Scott Raymond in 21 hops flew from California to North Carolina using solar power. Developments then turned back to unmanned aerial vehicles (UAV) with the "Pathfinder" (1997) and subsequent designs, culminating in the "Helios" which set the altitude record for a non-rocket-propelled aircraft at 29524 m in 2001. The "Zephyr", developed by BAE Systems, is the latest in a line of record-breaking solar aircraft, making a 54-hour flight in 2007, and month-long flights were envisioned by 2010. As of 2015, Solar Impulse, an electric aircraft, is currently circumnavigating the globe. It is a single-seat plane powered by solar cells and capable of taking off under its own power. The designed allows the aircraft to remain airborne for 36 hours.
A solar balloon is a black balloon that is filled with ordinary air. As sunlight shines on the balloon, the air inside is heated and expands causing an upward buoyancy force, much like an artificially heated hot air balloon. Some solar balloons are large enough for human flight, but usage is generally limited to the toy market as the surface-area to payload-weight ratio is relatively high.
Fuel production.
Solar chemical processes use solar energy to drive chemical reactions. These processes offset energy that would otherwise come from a fossil fuel source and can also convert solar energy into storable and transportable fuels. Solar induced chemical reactions can be divided into thermochemical or photochemical. A variety of fuels can be produced by artificial photosynthesis. The multielectron catalytic chemistry involved in making carbon-based fuels (such as methanol) from reduction of carbon dioxide is challenging; a feasible alternative is hydrogen production from protons, though use of water as the source of electrons (as plants do) requires mastering the multielectron oxidation of two water molecules to molecular oxygen. Some have envisaged working solar fuel plants in coastal metropolitan areas by 2050 – the splitting of sea water providing hydrogen to be run through adjacent fuel-cell electric power plants and the pure water by-product going directly into the municipal water system. Another vision involves all human structures covering the earth's surface (i.e., roads, vehicles and buildings) doing photosynthesis more efficiently than plants.
Hydrogen production technologies been a significant area of solar chemical research since the 1970s. Aside from electrolysis driven by photovoltaic or photochemical cells, several thermochemical processes have also been explored. One such route uses concentrators to split water into oxygen and hydrogen at high temperatures (2300 -). Another approach uses the heat from solar concentrators to drive the steam reformation of natural gas thereby increasing the overall hydrogen yield compared to conventional reforming methods. Thermochemical cycles characterized by the decomposition and regeneration of reactants present another avenue for hydrogen production. The Solzinc process under development at the Weizmann Institute uses a 1 MW solar furnace to decompose zinc oxide (ZnO) at temperatures above 1200 C. This initial reaction produces pure zinc, which can subsequently be reacted with water to produce hydrogen.
Energy storage methods.
Thermal mass systems can store solar energy in the form of heat at domestically useful temperatures for daily or interseasonal durations. Thermal storage systems generally use readily available materials with high specific heat capacities such as water, earth and stone. Well-designed systems can lower peak demand, shift time-of-use to off-peak hours and reduce overall heating and cooling requirements.
Phase change materials such as paraffin wax and Glauber's salt are another thermal storage media. These materials are inexpensive, readily available, and can deliver domestically useful temperatures (approximately 64 C). The "Dover House" (in Dover, Massachusetts) was the first to use a Glauber's salt heating system, in 1948.
Solar energy can be stored at high temperatures using molten salts. Salts are an effective storage medium because they are low-cost, have a high specific heat capacity and can deliver heat at temperatures compatible with conventional power systems. The Solar Two used this method of energy storage, allowing it to store 1.44 TJ in its 68 m3 storage tank with an annual storage efficiency of about 99%.
Off-grid PV systems have traditionally used rechargeable batteries to store excess electricity. With grid-tied systems, excess electricity can be sent to the transmission grid, while standard grid electricity can be used to meet shortfalls. Net metering programs give household systems a credit for any electricity they deliver to the grid. This is handled by 'rolling back' the meter whenever the home produces more electricity than it consumes. If the net electricity use is below zero, the utility then rolls over the kilowatt hour credit to the next month. Other approaches involve the use of two meters, to measure electricity consumed vs. electricity produced. This is less common due to the increased installation cost of the second meter. Most standard meters accurately measure in both directions, making a second meter unnecessary.
Pumped-storage hydroelectricity stores energy in the form of water pumped when energy is available from a lower elevation reservoir to a higher elevation one. The energy is recovered when demand is high by releasing the water, with the pump becoming a hydroelectric power generator.
Development, deployment and economics.
Beginning with the surge in coal use which accompanied the Industrial Revolution, energy consumption has steadily transitioned from wood and biomass to fossil fuels. The early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. However, development of solar technologies stagnated in the early 20th century in the face of the increasing availability, economy, and utility of coal and petroleum.
The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world and brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the US (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer Institute for Solar Energy Systems ISE).
Commercial solar water heaters began appearing in the United States in the 1890s. These systems saw increasing use until the 1920s but were gradually replaced by cheaper and more reliable heating fuels. As with photovoltaics, solar water heating attracted renewed attention as a result of the oil crises in the 1970s but interest subsided in the 1980s due to falling petroleum prices. Development in the solar water heating sector progressed steadily throughout the 1990s and growth rates have averaged 20% per year since 1999. Although generally underestimated, solar water heating and cooling is by far the most widely deployed solar technology with an estimated capacity of 154 GW as of 2007.
The International Energy Agency has said that solar energy can make considerable contributions to solving some of the most urgent problems the world now faces: 
The development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared.
In 2011, a report by the International Energy Agency found that solar energy technologies such as photovoltaics, solar hot water and concentrated solar power could provide a third of the world’s energy by 2060 if politicians commit to limiting climate change. The energy from the sun could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters. "The strength of solar is the incredible variety and flexibility of applications, from small scale to big scale".
We have proved ... that after our stores of oil and coal are exhausted the human race can receive unlimited power from the rays of the sun.—Frank Shuman, New York Times, July 2, 1916
ISO standards.
The International Organization for Standardization has established a number of standards relating to solar energy equipment. For example, ISO 9050 relates to glass in building while ISO 10217 relates to the materials used in solar water heaters.
Earth's mass conversion.
Solar energy is converted into part of the mass of Earth by Photosynthetic pigments, so effectively the sun is sending matter that is stored on earth, with photosynthesizing organisms and energy as the intermediaries. In the case of Solar photovoltaics, they generally do not add to the mass of Earth because their energy is merely transmitted and subsequently radiated (as electricity or heat) which is not converted into chemical means to be stored on earth.
References.
</dl>

</doc>
<doc id="27745" url="http://en.wikipedia.org/wiki?curid=27745" title="Standard conditions for temperature and pressure">
Standard conditions for temperature and pressure

Standard conditions for temperature and pressure are standard sets of conditions for experimental measurements established to allow comparisons to be made between different sets of data. The most used standards are those of the International Union of Pure and Applied Chemistry (IUPAC) and the National Institute of Standards and Technology (NIST), although these are not universally accepted standards. Other organizations have established a variety of alternative definitions for their standard reference conditions.
In chemistry, IUPAC established standard temperature and pressure (informally abbreviated as STP) as a temperature of 273.15 K (0 °C, 32 °F) and an absolute pressure of exactly 100,000 Pa (1 bar, 14.5 psi, 0.9869 atm). An unofficial, but commonly used standard is standard ambient temperature and pressure (SATP) as a temperature of 298.15 K (25 °C, 77 °F). The STP and the SATP should not be confused with the standard state commonly used in thermodynamic evaluations of the Gibbs energy of a reaction.
NIST uses a temperature of 20 °C (293.15 K, 68 °F) and an absolute pressure of 101.325 kPa (14.696 psi, 1 atm). The International Standard Metric Conditions for natural gas and similar fluids are 288.15 K and 101.325 kPa.
In industry and commerce, standard conditions for temperature and pressure are often necessary to define the standard reference conditions to express the volumes of gases and liquids and related quantities such as the rate of volumetric flow (the volumes of gases vary significantly with temperature and pressure). However, many technical publications (books, journals, advertisements for equipment and machinery) simply state "standard conditions" without specifying them, often leading to confusion and errors. Good practice is to always incorporate the reference conditions of temperature and pressure.
Definitions.
Past use.
In the last five to six decades, professionals and scientists using the metric system of units defined the standard reference conditions of temperature and pressure for expressing gas volumes as being 15 C and 101.325 kPa. During those same years, the most commonly used standard reference conditions for people using the imperial or U.S. customary systems was 60 F and 14.696 psi (1 atm) because it was almost universally used by the oil and gas industries worldwide. The above definitions are no longer the most commonly used in either system of units.
Current use.
Many different definitions of standard reference conditions are currently being used by organizations all over the world. The table below lists a few of them, but there are more. Some of these organizations used other standards in the past. For example, IUPAC has, since 1982, defined standard reference conditions as being 0 °C and 100 kPa (1 bar), in contrast to its old standard of 0 °C and 101.325 kPa (1 atm).
Natural gas companies in Europe and South America have adopted 15 °C (59 °F) and 101.325 kPa (14.696 psi) as their standard gas volume reference conditions. Also, the International Organization for Standardization (ISO), the United States Environmental Protection Agency (EPA) and National Institute of Standards and Technology (NIST) each have more than one definition of standard reference conditions in their various standards and regulations.
In Russia, State Standard GOST 2939-63 sets the following standard conditions: 20 °C (293.15 K), 760 mmHg (101325 N/m2) and zero humidity.
Notes:
International Standard Atmosphere.
In aeronautics and fluid dynamics the "International Standard Atmosphere" (ISA) is a specification of pressure, temperature, density, and speed of sound at each altitude. The International Standard Atmosphere is representative of atmospheric conditions at mid latitudes. In the USA this information is specified the U.S. Standard Atmosphere which is identical to the "International Standard Atmosphere" at all altitudes up to 65,000 feet above sea level.
Standard laboratory conditions.
Due to the fact that many definitions of standard temperature and pressure differ in temperature significantly from standard laboratory temperatures (e.g., 0 °C vs. ~25 °C), reference is often made to "standard laboratory conditions" (a term deliberately chosen to be different from the term "standard conditions for temperature and pressure", despite its semantic near identity when interpreted literally). However, what is a "standard" laboratory temperature and pressure is inevitably culture-bound, given that different parts of the world differ in climate, altitude and the degree of use of heat/cooling in the workplace. For example, schools in New South Wales, Australia use 25 °C at 100 kPa for standard laboratory conditions.
ASTM International has published Standard ASTM E41- Terminology Relating to Conditioning and hundreds of special conditions for particular materials and test methods. Other standards organizations also have specialized standard test conditions.
Molar volume of a gas.
It is equally as important to indicate the applicable reference conditions of temperature and pressure when stating the molar volume of a gas as it is when expressing a gas volume or volumetric flow rate. Stating the molar volume of a gas without indicating the reference conditions of temperature and pressure has very little meaning and can cause confusion.
The molar volume of gases around STP can be calculated with an accuracy that is usually sufficient by using the ideal gas law. The molar volume of any ideal gas may be calculated at various standard reference conditions as shown below:
Technical literature can be confusing because many authors fail to explain whether they are using the ideal gas constant R, or the specific gas constant Rs. The relationship between the two constants is Rs = R / M, where M is the molecular weight of the gas.
The US Standard Atmosphere (USSA) uses 8.31432 m3·Pa/(mol·K) as the value of "R". However, the USSA,1976 does recognize that this value is not consistent with the values of the Avogadro constant and the Boltzmann constant.

</doc>
<doc id="27750" url="http://en.wikipedia.org/wiki?curid=27750" title="Script kiddie">
Script kiddie

In programming culture a script kiddie or skiddie (also known as "skid", "script bunny", "script kitty") is an unskilled individual who uses scripts or programs developed by others to attack computer systems and networks, and deface websites. It is generally assumed that script kiddies are juveniles who lack the ability to write sophisticated programs or exploits on their own, and that their objective is to try to impress their friends or gain credit in computer-enthusiast communities. The term is generally considered to be pejorative.
Characteristics.
In a Carnegie Mellon report prepared for the U.S. Department of Defense in 2005, script kiddies are defined as "The more immature but unfortunately often just as dangerous exploiter of security lapses on the Internet. The typical script kiddy uses existing and frequently well known and easy-to-find techniques and programs or scripts to search for and exploit weaknesses in other computers on the Internet—often randomly and with little regard or perhaps even understanding of the potentially harmful consequences.
Script kiddies have at their disposal a large number of effective, easily downloadable programs capable of breaching computers and networks. Such programs have included remote denial-of-service WinNuke, trojans Back Orifice, NetBus, Sub7, and ProRat, vulnerability scanner/injector kit Metasploit, and often software intended for legitimate security auditing. A survey of college students in 2010, supported by the UK's Association of Chief Police Officers, indicated a high level of interest in beginning hacking: "23% of 'uni' students have hacked into IT systems [...] 32% thought hacking was 'cool' [...] 28% considered it to be easy."
Script kiddies vandalize websites both for the thrill of it and to increase their reputation among their peers. Some more malicious script kiddies have used virus toolkits to create and propagate the Anna Kournikova and Love Bug viruses.
Script kiddies lack, or are only developing, programming skills sufficient to understand the effects and side effects of their actions. As a result, they leave significant traces which lead to their detection, or directly attack companies which have detection and countermeasures already in place, or in recent cases, leave automatic crash reporting turned on.

</doc>
<doc id="27751" url="http://en.wikipedia.org/wiki?curid=27751" title="Scalable Vector Graphics">
Scalable Vector Graphics

Scalable Vector Graphics (SVG) is an XML-based vector image format for two-dimensional graphics with support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999.
SVG images and their behaviors are defined in XML text files. This means that they can be searched, indexed, scripted, and compressed. As XML files, SVG images can be created and edited with any text editor, but are more often created with drawing software.
All major modern web browsers—including Mozilla Firefox, Internet Explorer, Google Chrome, Opera, and Safari—have at least some degree of SVG rendering support.
Overview.
SVG has been in development since 1999 by a group of companies within the W3C after the competing standards Precision Graphics Markup Language (PGML, developed from Adobe's PostScript) and Vector Markup Language (VML, developed from Microsoft's RTF) were submitted to W3C in 1998. SVG drew on experience from the designs of both those formats.
SVG allows three types of graphic objects: vector graphics, raster graphics, and text. Graphical objects, including PNG and JPEG raster images, can be grouped, styled, transformed, and composited into previously rendered objects. SVG does not directly support z-indices that separate drawing order from document order for overlapping objects, unlike some other vector markup languages like VML. Text can be in any XML namespace suitable to the application, which enhances search ability and accessibility of the SVG graphics. The feature set includes nested transformations, clipping paths, alpha masks, filter effects, template objects, and extensibility.
Since 2001, the SVG specification has been updated to version 1.1.
The SVG Mobile Recommendation introduced two simplified "profiles" of SVG 1.1, SVG Basic and SVG Tiny, meant for devices with reduced computational and display capabilities.
An enhanced version of SVG Tiny, called SVG Tiny 1.2, later became an autonomous Recommendation.
Work is currently in progress on SVG 2, which incorporates several new features in addition to those of SVG 1.1 and SVG Tiny 1.2.
Printing.
Though the SVG Specification primarily focuses on vector graphics markup language, its design includes the basic capabilities of a page description language like Adobe's PDF. It contains provisions for rich graphics, and is compatible with CSS for styling purposes. SVG has the information needed to place each glyph and image in a chosen location on a printed page. (By contrast, XHTML's primary purpose is to communicate content, not presentation, so XHTML specifies objects to be displayed but not where to place them.) A print-specialized subset of SVG (SVG Print, authored by Canon, HP, Adobe and Corel) is currently[ [update]] a W3C Working Draft.
Scripting and animation.
SVG drawings can be dynamic and interactive. Time-based modifications to the elements can be described in SMIL, or can be programmed in a scripting language (e.g. ECMAScript or JavaScript). The W3C explicitly recommends SMIL as the standard for animation in SVG. A rich set of event handlers such as "onmouseover" and "onclick" can be assigned to any SVG graphical object.
Compression.
SVG images, being XML, contain many repeated fragments of text, so they are well suited for lossless data compression algorithms. When an SVG image has been compressed with the industry standard gzip algorithm, it is referred to as an "SVGZ" image and uses the corresponding codice_1 filename extension. Conforming SVG 1.1 viewers will display compressed images. An SVGZ file is typically 20 to 50 percent of the original size. W3C provides SVGZ files to test for conformance.
Development history.
SVG was developed by the W3C SVG Working Group starting in 1998, after Macromedia and Microsoft introduced VML whereas Adobe Systems and Sun Microsystems submitted a competing format known as PGML. The working group was chaired by Chris Lilley of the W3C.
The MPEG-4 Part 20 standard - "Lightweight Application Scene Representation (LASeR) and Simple Aggregation Format (SAF)" is based on SVG Tiny. It was developed by MPEG (ISO/IEC JTC1/SC29/WG11) and published as ISO/IEC 14496-20:2006. SVG capabilities are enhanced in MPEG-4 Part 20 with key features for mobile services, such as dynamic updates, binary encoding, state-of-art font representation. SVG was also accommodated in MPEG-4 Part 11, in the Extensible MPEG-4 Textual (XMT) format - a textual representation of the MPEG-4 multimedia content using XML.
Mobile profiles.
Because of industry demand, two mobile profiles were introduced with SVG 1.1: "SVG Tiny" (SVGT) and "SVG Basic" (SVGB). These are subsets of the full SVG standard, mainly intended for user agents with limited capabilities. In particular, SVG Tiny was defined for highly restricted mobile devices such as cellphones; it doesn't support styling or scripting. SVG Basic was defined for higher-level mobile devices, such as PDAs.
In 2003, the 3GPP, an international telecommunications standards group, adopted SVG Tiny as the mandatory vector graphics media format for next-generation phones. SVGT is the required vector graphics format and support of SVGB is optional for Multimedia Messaging Service (MMS) and Packet-switched Streaming Service. It was later added as required format for vector graphics in 3GPP IP Multimedia Subsystem (IMS).
Neither mobile profile includes support for the full DOM, while only SVG Basic has optional support for scripting, but because they are fully compatible subsets of the full standard, most SVG graphics can still be rendered by devices which only support the mobile profiles.
SVGT 1.2 adds a microDOM (μDOM), styling and scripting.
Functionality.
The SVG 1.1 specification defines 14 functional areas or feature sets:
An SVG document can define components including shapes, gradients etc., and use them repeatedly. SVG images can also contain raster graphics, such as PNG and JPEG images, and further SVG images.
Example.
This code will produce a green square with a black outline:
SVG on the web.
The use of SVG on the web was limited by the lack of support in older versions of Internet Explorer (IE). Many web sites that serve SVG images, such as Wikipedia, also provide the images in a raster format, either automatically by HTTP content negotiation or by allowing the user directly to choose the file.
Google announced on 31 August 2010 that it had started to index SVG content on the web, whether it is in standalone files or embedded in HTML, and that users would begin to see such content listed among their search results.
It was announced on 8 December 2010 that Google Image Search would also begin indexing SVG files. On 28 January 2011, it was discovered that Google was allowing Image Search results to be restricted exclusively to SVG files. This feature was announced officially on 11 February 2011.
Native browser support.
Konqueror was the first browser to support SVG in release version 3.2 in February 2004. As of 2011, all major desktop browsers, and many minor ones, have some level of SVG support. Other browsers' implementations are not yet complete; see comparison of layout engines for further details.
Some earlier versions of Firefox (e.g. versions between 1.5 and 3.6), as well as a smattering of other now-outdated web browsers capable of displaying SVG graphics, needed them embedded in codice_16 or codice_17 elements to display them integrated as parts of an HTML webpage instead of using the standard way of integrating images with codice_18. However, SVG images may be included in XHTML pages using XML namespaces.
Tim Berners-Lee, the inventor of the World Wide Web, has been critical of (earlier versions of) Internet Explorer for its failure to support SVG.
There are several advantages to native and full support: plugins are not needed, SVG can be freely mixed with other content in a single document, and rendering and scripting become considerably more reliable.
Plug-in browser support.
Internet Explorer, up to and including IE8, was the only major browser not to provide native SVG support. IE8 and older require a plug-in to render SVG content. There are a number of plug-ins available to assist, including:
On 5 January 2010, a senior manager of the Internet Explorer team at Microsoft announced on his official blog that Microsoft had just requested to join the SVG Working Group of the W3C in order to "take part in ensuring future versions of the SVG spec will meet the needs of developers and end users," although no plans for SVG support in Internet Explorer were mentioned at that time. Internet Explorer 9 beta supported a basic SVG feature set based on the SVG 1.1 W3C recommendation. Functionality has been implemented for most of the SVG document structure, interactivity through scripting and styling inline and through CSS. The presentation elements, attributes and DOM interfaces that have been implemented include basic shapes, colors, filling, gradients, patterns, paths and text.
Mobile support.
SVG Tiny (SVGT) 1.1 and 1.2 are mobile profiles for SVG. SVGT 1.2 includes some features not found in SVG 1.1, including non-scaling strokes, which are supported by some SVG 1.1 implementations, such as Opera, Firefox and WebKit. As shared code bases between desktop and mobile browsers increased, the use of SVG 1.1 over SVGT 1.2 also increased.
Support for SVG may be limited to SVGT on older or more limited smart phones, or may be primarily limited by their respective operating system. Adobe Flash Lite has optionally supported SVG Tiny since version 1.1. At the SVG Open 2005 conference, Sun demonstrated a mobile implementation of SVG Tiny 1.1 for the Connected Limited Device Configuration (CLDC) platform.
Mobiles that use Opera Mobile, as well as the iPhone's built in browser, also include SVG support. However, even though it used the WebKit engine, the Android built-in browser did not support SVG prior to v3.0 (Honeycomb). Prior to v3.0, Firefox Mobile 4.0b2 (beta) for Android was the first browser running under Android to support SVG by default.
The level of SVG Tiny support available varies from mobile to mobile, depending on the SVG engine installed. Many newer mobile products support additional features beyond SVG Tiny 1.1, like gradient and opacity; this is sometimes referred as "SVGT 1.1+", though there is no such standard.
Rim's BlackBerry has built-in support for SVG Tiny 1.1 since version 5.0. Support continues for WebKit-based BlackBerry Torch browser in OS 6 and 7.
Nokia's S60 platform has built-in support for SVG. For example, icons are generally rendered using the platform's SVG engine. Nokia has also led the JSR 226: Scalable 2D Vector Graphics API expert group that defines Java ME API for SVG presentation and manipulation. This API has been implemented in S60 Platform 3rd Edition Feature Pack 1 and onward. Some Series 40 phones also support SVG (such as Nokia 6280).
Most Sony Ericsson phones beginning with K700 (by release date) support SVG Tiny 1.1. Phones beginning with K750 also support such features as opacity and gradients. Phones with Sony Ericsson Java Platform-8 have support for JSR 226.
Windows Phone has supported SVG since version 7.5
SVG is also supported on various mobile devices from Motorola, Samsung, LG, and Siemens mobile/BenQ-Siemens. eSVG, an SVG rendering library mainly written for embedded devices, is available on some mobile platforms.
OpenVG is an API designed for hardware-accelerated 2D vector graphics. Its primary platforms are handheld devices, mobile phones, gaming or media consoles, and consumer electronic devices including operating systems with Gallium3D based graphics drivers.
Online SVG converters.
This is an incomplete list of web applications that can convert SVG files to raster image formats (this process is known as rasterization), or raster images to SVG (this process is known as image tracing or vectorization) - without the need of installing a desktop software or browser plug-in.
Application support.
SVG images can be produced by the use of a vector graphics editor, such as Inkscape, Adobe Illustrator, Adobe Flash Professional or CorelDRAW, and rendered to common raster image formats such as PNG using the same software. Inkscape uses a (built-in) potrace to import raster image formats.
Software can be programmed to render SVG images by using a library such as librsvg used by GNOME since 2000, or Batik. SVG images can also be rendered to any desired popular image format by using the free software command-line utility ImageMagick (which also uses librsvg under the hood).
Other uses for SVG include embedding for use in word processing (e.g. with LibreOffice) and desktop publishing (e.g. Scribus), plotting graphs (e.g. gnuplot), and importing paths (e.g. for use in GIMP or Blender). The Uniform Type Identifier for SVG used by Apple is public.svg-image and conforms to public.image and public.xml.
External links.
 

</doc>
<doc id="27752" url="http://en.wikipedia.org/wiki?curid=27752" title="Spectroscopy">
Spectroscopy

Spectroscopy is the study of the interaction between matter and electromagnetic radiation. Historically, spectroscopy originated through the study of visible light dispersed according to its wavelength, by a prism. Later the concept was expanded greatly to comprise any interaction with radiative energy as a function of its wavelength or frequency. Spectroscopic data is often represented by a spectrum, a plot of the response of interest as a function of wavelength or frequency.
Introduction.
Spectroscopy and spectrography are terms used to refer to the measurement of radiation intensity as a function of wavelength and are often used to describe experimental spectroscopic methods. Spectral measurement devices are referred to as spectrometers, spectrophotometers, spectrographs or spectral analyzers.
Daily observations of color can be related to spectroscopy. Neon lighting is a direct application of atomic spectroscopy. Neon and other noble gases have characteristic emission frequencies (colors). Neon lamps use collision of electrons with the gas to excite these emissions. Inks, dyes and paints include chemical compounds selected for their spectral characteristics in order to generate specific colors and hues. A commonly encountered molecular spectrum is that of nitrogen dioxide. Gaseous nitrogen dioxide has a characteristic red absorption feature, and this gives air polluted with nitrogen dioxide a reddish brown color. Rayleigh scattering is a spectroscopic scattering phenomenon that accounts for the color of the sky.
Spectroscopic studies were central to the development of quantum mechanics and included Max Planck's explanation of blackbody radiation, Albert Einstein's explanation of the photoelectric effect and Niels Bohr's explanation of atomic structure and spectra. Spectroscopy is used in physical and analytical chemistry because atoms and molecules have unique spectra. As a result, these spectra can be used to detect, identify and quantify information about the atoms and molecules. Spectroscopy is also used in astronomy and remote sensing on earth. Most research telescopes have spectrographs. The measured spectra are used to determine the chemical composition and physical properties of astronomical objects (such as their temperature and velocity).
Theory.
One of the central concepts in spectroscopy is a resonance and its corresponding resonant frequency. Resonances were first characterized in mechanical systems such as pendulums. Mechanical systems that vibrate or oscillate will experience large amplitude oscillations when they are driven at their resonant frequency. A plot of amplitude vs. excitation frequency will have a peak centered at the resonance frequency. This plot is one type of spectrum, with the peak often referred to as a spectral line, and most spectral lines have a similar appearance.
In quantum mechanical systems, the analogous resonance is a coupling of two quantum mechanical stationary states of one system, such as an atom, via an oscillatory source of energy such as a photon. The coupling of the two states is strongest when the energy of the source matches the energy difference between the two states. The energy formula_1 of a photon is related to its frequency formula_2 by formula_3 where formula_4 is Planck's constant, and so a spectrum of the system response vs. photon frequency will peak at the resonant frequency or energy. Particles such as electrons and neutrons have a comparable relationship, the de Broglie relations, between their kinetic energy and their wavelength and frequency and therefore can also excite resonant interactions.
Spectra of atoms and molecules often consist of a series of spectral lines, each one representing a resonance between two different quantum states. The explanation of these series, and the spectral patterns associated with them, were one of the experimental enigmas that drove the development and acceptance of quantum mechanics. The hydrogen spectral series in particular was first successfully explained by the Rutherford-Bohr quantum model of the hydrogen atom. In some cases spectral lines are well separated and distinguishable, but spectral lines can also overlap and appear to be a single transition if the density of energy states is high enough.
Classification of methods.
Spectroscopy is a sufficiently broad field that many sub-disciplines exist, each with numerous implementations of specific spectroscopic techniques. The various implementations and techniques can be classified in several ways.
Type of radiative energy.
Types of spectroscopy are distinguished by the type of radiative energy involved in the interaction. In many applications, the spectrum is determined by measuring changes in the intensity or frequency of this energy. The types of radiative energy studied include:
Nature of the interaction.
Types of spectroscopy can also be distinguished by the nature of the interaction between the energy and the material. These interactions include:
Type of material.
Spectroscopic studies are designed so that the radiant energy interacts with specific types of matter.
Atoms.
Atomic spectroscopy was the first application of spectroscopy developed. Atomic absorption spectroscopy (AAS) and atomic emission spectroscopy (AES) involve visible and ultraviolet light. These absorptions and emissions, often referred to as atomic spectral lines, are due to electronic transitions of outer shell electrons as they rise and fall from one electron orbit to another. Atoms also have distinct x-ray spectra that are attributable to the excitation of inner shell electrons to excited states.
Atoms of different elements have distinct spectra and therefore atomic spectroscopy allows for the identification and quantitation of a sample's elemental composition. Robert Bunsen and Gustav Kirchhoff discovered new elements by observing their emission spectra. Atomic absorption lines are observed in the solar spectrum and referred to as Fraunhofer lines after their discoverer. A comprehensive explanation of the hydrogen spectrum was an early success of quantum mechanics and explained the Lamb shift observed in the hydrogen spectrum led to the development of quantum electrodynamics.
Modern implementations of atomic spectroscopy for studying visible and ultraviolet transitions include flame emission spectroscopy, inductively coupled plasma atomic emission spectroscopy, glow discharge spectroscopy, microwave induced plasma spectroscopy, and spark or arc emission spectroscopy. Techniques for studying x-ray spectra include X-ray spectroscopy and X-ray fluorescence (XRF).
Molecules.
The combination of atoms into molecules leads to the creation of unique types of energetic states and therefore unique spectra of the transitions between these states. Molecular spectra can be obtained due to electron spin states (electron paramagnetic resonance), molecular rotations, molecular vibration and electronic states. Rotations are collective motions of the atomic nuclei and typically lead to spectra in the microwave and millimeter-wave spectral regions; rotational spectroscopy and microwave spectroscopy are synonymous. Vibrations are relative motions of the atomic nuclei and are studied by both infrared and Raman spectroscopy. Electronic excitations are studied using visible and ultraviolet spectroscopy as well as fluorescence spectroscopy.
Studies in molecular spectroscopy led to the development of the first maser and contributed to the subsequent development of the laser.
Crystals and extended materials.
The combination of atoms or molecules into crystals or other extended forms leads to the creation of additional energetic states. These states are numerous and therefore have a high density of states. This high density often makes the spectra weaker and less distinct, i.e., broader. For instance, blackbody radiation is due to the thermal motions of atoms and molecules within a material. Acoustic and mechanical responses are due to collective motions as well.
Pure crystals, though, can have distinct spectral transitions and the crystal arrangement also has an effect on the observed molecular spectra. The regular lattice structure of crystals also scatters x-rays, electrons or neutrons allowing for crystallographic studies.
Nuclei.
Nuclei also have distinct energy states that are widely separated and lead to gamma ray spectra. Distinct nuclear spin states can have their energy separated by a magnetic field, and this allows for NMR spectroscopy.
Other types.
Other types of spectroscopy are distinguished by specific applications or implementations:
History.
"For main article see History of spectroscopy"
The history of spectroscopy began with Isaac Newton's optics experiments (1666–1672). Newton applied the word "spectrum" to describe the rainbow of colors that combine to form white light and that are revealed when the white light is passed through a prism. During the early 1800s, Joseph von Fraunhofer made experimental advances with dispersive spectrometers that enabled spectroscopy to become a more precise and quantitative scientific technique. Since then, spectroscopy has played and continues to play a significant role in chemistry, physics and astronomy.

</doc>
<doc id="27753" url="http://en.wikipedia.org/wiki?curid=27753" title="List of science fiction themes">
List of science fiction themes

The following is a list of recurring themes in science fiction.

</doc>
<doc id="27760" url="http://en.wikipedia.org/wiki?curid=27760" title="Statute of Anne">
Statute of Anne

The Statute of Anne (c.19), an act of the Parliament of Great Britain, was the first statute to provide for copyright regulated by the government and courts, rather than by private parties.
Prior to the statute's enactment in 1710, copying restrictions were authorized by the Licensing of the Press Act 1662. These restrictions were enforced by the Stationers' Company, a guild of printers given the exclusive power to print—and the responsibility to censor—literary works. The censorship administered under the Licensing Act led to public protest; as the act had to be renewed at two-year intervals, authors and others sought to prevent its reauthorisation. In 1694, Parliament refused to renew the Licensing Act, ending the Stationers' monopoly and press restrictions.
Over the next 10 years the Stationers repeatedly advocated bills to re-authorize the old licensing system, but Parliament declined to enact them. Faced with this failure, the Stationers decided to emphasise the benefits of licensing to authors rather than publishers, and the Stationers succeeded in getting Parliament to consider a new bill. This bill, which after substantial amendments was granted Royal Assent on 5 April 1710, became known as the Statute of Anne due to its passage during the reign of Queen Anne. The new law prescribed a copyright term of 14 years, with a provision for renewal for a similar term, during which only the author and the printers they chose to license their works to could publish the author's creations. Following this, the work's copyright would expire, with the material falling into the public domain. Despite a period of instability known as the Battle of the Booksellers when the initial copyright terms under the Statute began to expire, the Statute of Anne remained in force until the Copyright Act 1842 replaced it.
The statute is considered a "watershed event in Anglo-American copyright history ... transforming what had been the publishers' private law copyright into a public law grant". Under the statute, copyright was for the first time vested in authors rather than publishers; it also included provisions for the public interest, such as a legal deposit scheme. The Statute was an influence on copyright law in several other nations, including the United States, and even in the 21st century is "frequently invoked by modern judges and academics as embodying the utilitarian underpinnings of copyright law".
Background.
With the introduction of the printing press to England by William Caxton in 1476, printed works became both more common and more economically important. As early as 1483, Richard III recognised the value of literary works by specifically exempting them from the government's protectionist legislation. Over the next fifty years, the government moved further towards economic regulation, abolishing the provision with the Printers and Binders Act 1534, which also banned the import of foreign works and empowered the Lord Chancellor to set maximum pricing for English books. This was followed by increasing degrees of censorship. A further proclamation of 1538, aiming to stop the spread of Lutheran doctrine, saw Henry VIII note that "sondry contentious and sinyster opiniones, have by wrong teachynge and naughtye bokes increaced and growen within this his realme of England", and declare that all authors and printers must allow the Privy Council or their agents to read and censor books before publication.
Stationers' Company.
This censorship peaked on 4 May 1557, when Mary I issued a royal warrant formally incorporating the Stationers' Company. The old method of censorship had been limited by the Second Statute of Repeal, and with Mary's increasing unpopularity the existing system was unable to cope with the number of critical works being printed. Instead, the royal warrant devolved this power to the Company. This was done by decreeing that only the Company's publishers could print and distribute books. Their Wardens were given the power to enter any printing premises, destroy illegal works and imprison anyone found manufacturing them. In this way the government "harnessed the self interest of the publishers to the yoke of royal incentive", guaranteeing that the Company would follow the rules due to the economic monopoly it gave their members. With the abolition of the Star Chamber and Court of High Commission by the Long Parliament, the legal basis for this warrant was removed, but the Long Parliament chose to replace it with the Licensing Act 1662. This provided that the Company would retain their original powers, and imposed additional restrictions on printing; King's Messengers were permitted to enter any home or business in search of illegal presses. The legislation required renewal every two years, and was regularly reapproved.
This was not "copyright" as is normally understood; although there was a monopoly on the right to copy, this was available to publishers, not authors, and did not exist by default; it only applied to books which had been accepted and published by the Company. A member of the Company would register the book, and would then have a perpetual copyright over its printing, copying and publication, which could be leased, transferred to others or given to heirs upon the member's death. The only exception to this was that, if a book was out of print for more than 6 months and the publisher ignored a warning to make it available, the copyright would be released and other publishers would be permitted to copy it. Authors themselves were not particularly respected until the 18th century, and were not permitted to be members of the Company, playing no role in the development or use of its licenses despite the Company's sovereign authority to decide what was published. There is evidence that some authors were recognised by the Company itself to have the right to copy and the right to alter their works; these authors were uniformly the writers of uneconomical books who were underwriting their publication.
The Company's monopoly, censorship and failure to protect authors made the system highly unpopular; John Milton wrote "Areopagitica" as a result of his experiences with the Company, accusing Parliament of being deceived by "the fraud of some old patentees and monopolisers in the trade of bookselling". He was not the first writer to criticise the system, with John Locke writing a formal memorandum to the MP Edward Clarke in 1693 while the Licensing Act was being renewed, complaining that the existing system restricted the free exchange of ideas and education while providing an unfair monopoly for Company members. Academic Mark Rose attributes the efforts of Milton to promote the "bourgeois public sphere", along with the Glorious Revolution's alterations to the political system and the rise of public coffee houses, as the source of growing public unhappiness with the system. At the same time, this was a period in which clearly defined political parties were taking shape, and with the promise of regular elections, an environment where the public were of increasing importance to the political process. The result was a "developing public sphere [which] provided the context that enabled the collapse of traditional press controls".
Lapse of the Licensing Act.
The result of this environment was the lapse of the Licensing Act. In November 1694, a committee was appointed by the Commons to see what laws were "lately expired and expiring [and] fit to be revived and continued". The Committee reported in January 1695, and suggested the renewal of the Licensing Act; this was included in the "Continuation Bill", but rejected by the House of Commons on 11 February. When it reached the House of Lords, the Lords re-included the Licensing Act, and returned the bill to the Commons. In response, a second committee was appointed - this one to produce a report indicating why the Commons disagreed with the inclusion of the Licensing Act, and chaired by Edward Clarke. This committee soon reported to the Commons, and Clarke was ordered to carry a message to the Lords requesting a conference over the Act. On 18 April 1695, Clarke met with representatives of the Lords, and they agreed to allow the Continuation Bill to pass without the renewal of the Licensing Act. With this, "the Lords' decision heralded an end to a relationship that had developed throughout the sixteenth and seventeenth centuries between the State and the Company of Stationers", ending both nascent publishers' copyright and the existing system of censorship.
John Locke's close relationship with Clarke, along with the respect he commanded, is seen by academics as what led to this decision. Locke had spent the early 1690s campaigning against the statute, considering it "ridiculous" that the works of dead authors were held perpetually in copyright. In letters to Clarke he wrote of the absurdity of the existing system, complaining primarily about the unfairness of it to authors, and "[t]he parallels between Locke's commentary and those reasons presented by the Commons to the Lords for refusing to renew the 1662 Act are striking". He was assisted by a number of independent printers and booksellers, who opposed the monopolistic aspects of the Act, and introduced a petition in February 1693 that the Act prevented them from conducting their business. The "developing public sphere", along with the harm the existing system had caused to both major political parties, is also seen as a factor.
The failure to renew the Licensing Act led to confusion and both positive and negative outcomes; while the government no longer played a part in censoring publications, and the monopoly of the Company over printing was broken, there was uncertainty as to whether or not copyright was a binding legal concept without the legislation. Economic chaos also resulted; with the Company now unable to enforce any monopoly, provincial towns began establishing printing presses, producing cheaper books than the London booksellers. The absence of the censorship provisions also opened Britain up as a market for internationally printed books, which were similarly cheaper than those British printers could produce.
Attempts at replacement.
The rejection of the existing system was not done with universal approval, and there were ultimately twelve unsuccessful attempts to replace it. The first was introduced to the House of Commons on 11 February 1695. A committee, again led by Clarke, was to write a "Bill for the Better Regulating of Printing and the Printing Presses". This bill was essentially a copy of the Licensing Act, but with a narrower jurisdiction; only books covering religion, history, the affairs of the state or the law would require official authorisation. Four days after its introduction, the Stationers' held an emergency meeting to agree to petition the Commons - this was because the bill did not contain any reference to books as property, eliminating their monopoly on copying. Clarke also had issues with the provisions, and the debate went on until the end of the Parliamentary session, with the bill failing to pass.
With the end of the Parliamentary session came the first general election under the Triennial Act 1694, which required the Monarch to dissolve Parliament every 3 years, causing a general election. This led to the "golden age" of the English electorate, and allowed for the forming of two major political parties - the Whigs and Tories. At the same time, with the failure to renew the Licensing Act, a political press developed. While the Act had been in force only one official newspaper existed; the "London Gazette", published by the government. After its demise, a string of newspapers sprang into being, including the "Flying Post", the "Evening Post" and the "Daily Courant". Newspapers had a strong bias towards particular parties, with the "Courant" and the "Flying Post" supporting the Whigs and the "Evening Post" in favour of the Tories, leading to politicians from both parties realising the importance of an efficient propaganda machine in influencing the electorate. This added a new dimension to the Commons' decision to reject two new renewals of the Licensing Act in the new Parliamentary session.
Authors, as well as Stationers, then joined the demand for a new system of licensing. Jonathan Swift was a strong advocate for licensing, and Daniel Defoe wrote on 8 November 1705 that with the absence of licensing, "One Man Studies Seven Year, to bring a finish'd Peice into the World, and a Pyrate Printer, Reprints his Copy immediately, and Sells it for a quarter of the Price ... these things call for an Act of Parliament". Seeing this, the Company took the opportunity to experiment with a change to their approach and argument. Instead of lobbying because of the impact the absence of legislation was having on their trade, they lobbied on behalf of the authors, but seeking the same things. The first indication of this change in approach comes from the 1706 pamphlet by John How, a stationer, titled "Reasons humbly Offer'd for a Bill for the Encouragement of Learning and the Improvement of Printing". This argued for a return to licensing, not with reference to the printers, but because without something to protect authors and guarantee them an income, "Learned men will be wholly discouraged from Propagating the most useful Parts of Knowledge and Literature". Using these new tactics and the support of authors, the Company petitioned Parliament again in both 1707 and 1709 to introduce a bill providing for copyright.
Act.
Passage.
Although both bills failed, they led to media pressure that was exacerbated by both Defoe and How. Defoe's "A Review", published on 3 December 1709 and demanding "a Law in the present Parliament ... for the Encouragement of Learning, Arts, and Industry, by securing the Property of Books to the Authors or Editors of them", was followed by How's "Some Thoughts on the Present State of Printing and Bookselling", which hoped that Parliament "might think fit to secure Property in Books by a Law". This was followed by another review by Defoe on 6 December, in which he even went so far as to provide a draft text for the bill. On 12 December, the Stationers submitted yet another petition asking for legislation on the issue, and the House of Commons gave three MPs – Spencer Compton, Craven Peyton and Edward Wortley – permission to form a drafting committee. On 11 January 1710, Wortley introduced this bill, titling it "A Bill for the Encouragement of Learning and for Securing the Property of Copies of Books to the rightful Owners thereof".
The bill allowed for fines for anyone who imported or traded in unlicensed or foreign books, required every book that would be given copyright protection to be entered into the Stationers' Register, provided a legal deposit system centred around the King's Library, the University of Oxford and the University of Cambridge, but said nothing about limiting the term of copyright. It also specified that books were property; an emphasis on the idea that authors deserved copyright simply due to their efforts. The Stationers were enthusiastic, urging Parliament to pass the bill, and it received its second reading on 9 February. A Committee of the Whole met to amend it on 21 February, with further alterations made when it was passed back to the House of Commons on 25 February. Alterations during this period included minor changes, such as extending the legal deposit system to cover Sion College and the Faculty of Advocates, but also major ones, including the introduction of a limit on the length of time for which copyright would be granted.
Linguistic amendments were also included; the line in the preamble emphasising that authors possessed books as they would any other piece of property was dropped, and the bill moved from something designed "for Securing the Property of Copies of Books to the rightful Owners thereof" to a bill "for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or Purchasers of such Copies". Another amendment allowed anyone to own and trade in copies of books, undermining the Stationers. Other changes were made when the bill went to the House of Lords, and it was finally returned to the Commons on 5 April. The aims of the resulting statute are debated; Ronan Deazley suggests that the intent was to balance the rights of the author, publisher and public in such a way as to ensure the maximum dissemination of works, while other academics argue that the bill was intended to protect the Company's monopoly or, conversely, to weaken it. Oren Bracha, writing in the "Berkeley Technology Law Journal", says that when considering which of these options are correct, "the most probable answer [is] all of them". Whatever the motivations, the bill was passed on 5 April 1710, and is commonly known simply as the Statute of Anne due its passage during the reign of Queen Anne.
Text.
Consisting of 11 sections, the Statute of Anne is formally titled "An Act for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or Purchasers of Copies, during the Times therein mentioned". The preamble for the Statute indicates the purpose of the legislation - to bring order to the book trade - saying: Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing, Reprinting, and Publishing, or causing to be Printed, Reprinted, and Published Books, and other Writings, without the Consent of the Authors or Proprietors of such Books and Writings, to their very great Detriment, and too often to the Ruin of them and their Families: For Preventing therefore such Practices for the future, and for the Encouragement of Learned Men to Compose and Write useful Books; May it please Your Majesty, that it may be Enacted ...
The Statute then moved on to stating the nature of copyright. The right granted was the right to copy; to have sole control over the printing and reprinting of books, with no provision to benefit the owner of this right after the sale. This right, previously held by the Stationers' Company's members, would automatically be given to the author as soon as it was published, although they had the ability to license these rights to another person. The copyright could be gained through two stages; first, the registration of the book's publication with the Company, to prevent unintentional infringement, and second, the deposit of copies of the book at the Stationers' Company, the royal library and various universities. One restriction on copyright was a "cumbersome system" designed to prohibit unreasonably high prices for books, which limited how much authors could charge for copies. There was also a prohibition on importing foreign works, with exceptions made for Latin and Greek classics.
Once registration had been completed and the deposits were made, the author was granted an exclusive right to control the copying of the book. Penalties for infringing this right were severe, with all infringing copies to be destroyed and large fines to be paid to both the copyright holder and the government; there was only a three-month statute of limitations on bringing a case, however. This exclusive right's length was dependent on when the book had been published. If it was published after 10 April 1710, the length of copyright was 14 years; if published before that date, 21 years. An author who survived until the copyright expired would be granted an additional 14-year term, and when that ran out, the works would enter the public domain. Copyright under the Statute applied to Scotland and England, as well as Ireland when that country joined the union in 1800.
Aftermath.
Impact.
The passage of the Statute was initially much welcomed, ushering in "stability to an insecure book trade" while providing for a "pragmatic bargain" between the rights of the author, publisher and public intended to boost public learning and the availability of knowledge. The clause requiring book deposits, however, was not seen as a success. If the books were not deposited, the penalties would be severe, with a fine of £5. The number of deposits required, however, meant that it was a substantial burden; a print run might only be of 250 copies, and if they were particularly expensive to print, it could be cheaper to ignore the law. Some booksellers argued that the deposit provision only applied to registered books, and so deliberately avoided registration just to be able to minimise their liability. This was further undermined by the ruling in "Beckford v Hood", where the Court of King's Bench confirmed that, even without registration, copyright could be enforced against infringers.
Another failure, identified by Bracha, is not found in what the Statute covered, but in what it did not. The Statute did not provide any means for identifying authors, did not identify what constituted authored works, and covered only "books", even while discussing "property" as a whole. Moreover, the right provided was merely that of "making and selling ... exact reprints. To a large extent, the new regime was the old stationer's privilege, except it was universalised, capped in time, and formally conferred upon authors rather than publishers". The impact of the Statute on authors was also minimal. Previously, publishers would have bought the original manuscript from writers for a lump sum; with the passage of the Statute, they simply did the same thing, but with the manuscript's copyright as well. The remaining economic power of the Company also allowed them to pressure booksellers and distributors into continuing their past arrangements, meaning that even theoretically "public domain" works were, in practise, still treated as copyrighted.
Battle of the Booksellers.
When the copyrights granted to works published before the Statute began to expire in 1731, the Stationers' Company and their publishers again began to fight to preserve the status quo. Their first port of call was Parliament, where they lobbied for new legislation to extend the length of copyright, and when this failed, they turned to the courts. Their principal argument was that copyright had not been created by the Statute of Anne; it existed beforehand, in the common law, and was perpetual. As such, even though the Statute provided for a limited term, all works remained in copyright under the common law regardless of when statutory copyright expired. Starting in 1743, this began a thirty-year campaign known as the "Battle of the Booksellers". They first tried going to the Court of Chancery and applying for injunctions prohibiting other publishers from printing their works, and this was initially successful. A series of legal setbacks over the next few years, however, left the law ambiguous.
The first major action taken to clarify the situation was "Millar v Taylor". Andrew Millar, a British publisher, purchased the rights to James Thomson's "The Seasons" in 1729, and when the copyright term expired, a competing publisher named Robert Taylor began issuing his own reprints of the work. Millar sued, and went to the Court of King's Bench to obtain an injunction and advocate perpetual copyright at common law. The jury found that the facts submitted by Millar were accurate, and asked the judges to clarify whether common law copyright existed. The first arguments were delivered on 30 June 1767, with John Dunning representing Millar and Edward Thurlow representing Taylor. A second set of arguments were submitted for Millar by William Blackstone on 7 June, and judgment was given on 20 April 1769. The final decision, written by Lord Mansfield and endorsed by Aston and Willes JJ, confirmed that there existed copyright at common law that turned "upon Principles before and independent" of the Statute of Anne, something justified because it was right "that an Author should reap the pecuniary Profits of his own Ingenuity and Labour". In other words, regardless of the Statute, there existed a perpetual copyright under the common law. Yates J dissented, on the grounds that the focus on the author obscured the impact this decision would have on "the rest of mankind", which he felt would be to create a virtual monopoly, something that would have a detrimental impact on the public and should certainly not be considered "an encouragement of the propagation of learning".
Although this decision was a boon to the Stationers, it was short-lived. Following "Millar", the right to print "The Seasons" was sold to a coalition of publishers including Thomas Becket. Two Scottish printers, Alexander and John Donaldson, began publishing an unlicensed edition, and Becket successfully obtained an injunction to stop them. This decision was appealed in "Donaldson v Beckett", and eventually went to the House of Lords. After consulting with the judges of the King's Bench, Common Pleas and Exchequer of Pleas, the Lords concluded that there was no copyright at common law - certainly not perpetual copyright - and as such, that the term permitted by the Statute of Anne was the maximum length of legal protection for publishers and authors alike.
Expansion and repeal.
Until its repeal, most extensions to copyright law were based around provisions found in the Statute of Anne. The one successful bill from the lobbying in the 1730s, which came into force on 29 September 1739, extended the provision prohibiting the import of foreign books to also prohibit the import of books that, while originally published in Britain, were being reprinted in foreign nations and then shipped to England and Wales. This was intended to stop the influx of cheap books from Ireland, and also repealed the price restrictions in the Statute of Anne. Another alteration was over the legal deposit provisions of the Statute, which many booksellers found unfair. Despite an initial period of compliance, the principle of donating copies of books to certain libraries lapsed, partly due to the unwieldiness of the statute's provisions and partly because of a lack of cooperation by the publishers. In 1775 Lord North, who was Chancellor of the University of Oxford, succeeded in passing a bill that reiterated the legal deposit provisions and granted the universities perpetual copyright on their works.
Another range of extensions came in relation to what could be copyrighted. The Statute only referred to books, and being an Act of Parliament, it was necessary to pass further legislation to include various other types of intellectual property. The Engraving Copyright Act 1734 extended copyright to cover engravings, statutes in 1789 and 1792 involved cloth, sculptures were copyrighted in 1914 and the performance of plays and music were covered by copyright in 1833 and 1842 respectively. The length of copyright was also altered; the Copyright Act 1814 set a copyright term of either 28 years, or the natural life of the author if this was longer. Despite these expansions, some still felt copyright was not a strong enough regime. In 1837, Thomas Noon Talfourd introduced a bill into Parliament to expand the scope of copyright. A friend of many men of letters, Talfourd aimed to provide adequate rewards for authors and artists. He campaigned for copyright to exist for the life of the author, with an additional 60 years after that. He also proposed that existing statutes be codified under the bill, so that the case law that had arisen around the Statute of Anne was clarified.
Talfourd's proposals led to opposition, and he reintroduced modified versions of them year on year. Printers, publishers and booksellers were concerned about the cost implications for original works, and for reprinting works that had fallen out of copyright. Many within Parliament argued that the bill failed to take into account the public interest, including Lord Macaulay, who succeeded in defeating one of Talfourd's bills in 1841. The Copyright Act 1842 passed, but "fell far short of Talfourd's dream of a uniform, consistent, codified law of copyright". It extended copyright to life plus seven years, and, as part of the codification clauses, repealed the Statute of Anne.
Significance.
The Statute of Anne is traditionally seen as "a historic moment in the development of copyright", and the first statute in the world to provide for copyright. Craig Joyce and Lyman Ray Patterson, writing in the "Emory Law Journal", call this a "too simple understanding [that] ignores the statute's source", arguing that it is at best a derivative of the Licensing Act. Even considering this, however, the Statute of Anne was "the watershed event in Anglo-American copyright history ... transforming what had been the publishers' private law copyright into a public law grant". Patterson, writing separately, does note the differences between the Licensing Act and the Statute of Anne; the question of censorship was, by 1710, out of the question, and in that regard the Statute is distinct, not providing for censorship.
It also marked the first time that copyright had been vested primarily in the author, rather than the publisher, and also the first time that the injurious treatment of authors by publishers was recognised; regardless of what authors signed away, the second 14-year term of copyright would automatically return to them. Even in the 21st century, the Statute of Anne is "frequently invoked by modern judges and academics as embodying the utilitarian underpinnings of copyright law". In "IceTV v Nine Network", for example, the High Court of Australia noted that the title of the Statute "echoed explicitly the emphasis on the practical or utilitarian importance that certain seventeenth-century philosophers attached to knowledge and its encouragement in the scheme of human progress". Despite "widely recognised flaws", the Act became a model copyright statute, both within the United Kingdom and internationally. Christophe Geiger notes that it is "a difficult, almost impossible task" to analyse the relationship between the Statute of Anne and early French copyright law, both because it is difficult to make a direct connection, and because the ongoing debate over both has led to radically different interpretations of each nation's law.
Similarly, Belgium took no direct influence from the Statute or English copyright theory, but Joris Deene of the University of Ghent identifies an indirect influence "at two levels"; the criteria for what constitutes copyrightable material, which comes from the work of English theorists such as Locke and Edward Young, and the underlying justification of copyright law. In Belgium, this justification is both that copyright serves the public interest, and that copyright is a "private right" that serves the interests of individual authors. Both theories were taken into account in "Donaldson v Beckett", as well as in the drafting of the Statute of Anne, and Deene infers that they subsequently had an impact on the Belgian debates over their first copyright statute. In the United States, the Copyright Clause of the United States Constitution and the first Federal copyright statute, the Copyright Act of 1790, both draw on the Statute of Anne. The 1790 Act contains provisions for a 14-year term of copyright and sections that provide for authors who published their works before 1790, both of which mirror the protection offered by the Statute 80 years previously.

</doc>
<doc id="27761" url="http://en.wikipedia.org/wiki?curid=27761" title="School choice">
School choice

School choice is a term or label given to a wide array of programs offering students and their families alternatives to publicly provided schools, to which students are generally assigned by the location of their family residence. In the United States, the most common—both by number of programs and by number of participating students—school choice programs are scholarship tax credit programs, which allow individuals or corporations to receive tax credits toward their state taxes in exchange for donations made to non-profit organizations that grant private school scholarships. In other cases, a similar subsidy may be provided by the state through a school voucher program. Other school choice options include open enrollment laws (which allow students to attend public schools outside of the district in which the students live), charter schools, magnet schools, virtual schools, homeschooling, education savings accounts (ESAs), and individual tax credits or deductions for educational expenses.
Forms.
Scholarship Tax Credits.
States with scholarship tax credit programs grant individuals and/or businesses a credit, whether full or partial, toward their taxes for donations made to scholarship granting organizations (also called school tuition organizations). SGOs/STOs use the donations to create scholarships that are then given to help pay for the cost of tuition for students. These scholarships allow students to attend private schools or out-of-district public schools that would otherwise be prohibitively expensive for many families. These programs currently exist in fourteen states: Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia in the United States.
Vouchers.
In a traditional public education system, schools receive funding from the state on a per student basis. Under a voucher system, eligible "students" receive state funding ("vouchers") which can be spent at whatever eligible private schools the parents choose for their children. The two most common voucher designs are universal vouchers and means-tested vouchers. Means-tested vouchers are directed towards low-income families and constitute the bulk of voucher plans in the United States.
Charter schools.
Charter schools are independent public schools which are exempt from many of the state and local regulations which govern most public schools. These exemptions grant charter schools some autonomy and flexibility with decision-making, such as teacher union contracts, hiring, and curriculum. In return, charter schools are subject to stricter accountability on spending and academic performance. The majority of states (and the District of Columbia) have charter school laws, though they vary in how charter schools are approved. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy High School, opened in St. Paul, Minnesota in 1992.
22-26% of Dayton, Ohio children are in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%), and Arizona. Almost 1 in 4 public schools in Arizona are charter schools, comprising about 8% of total enrollment.
Charter schools can also come in the form of cyber charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like all charter schools, cyber charters are public schools, but they are free from some of the rules and regulations that conventional public schools must follow.
Magnet schools.
Magnet schools are public schools that often have a specialized function like science, technology, or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, some (but not all) magnet schools require a test to get in.
Home schooling.
"Home education" or "home schooling" is instruction in a child's home, or provided primarily by a parent, or under direct parental control. Informal home education has always taken place, and formal instruction in the home has at times also been very popular. As public education grew in popularity during the 1900s, however, the number of people educated at home using a planned curriculum dropped. In the last 20 years, in contrast, the number of children being formally educated at home has grown tremendously, in particular in the United States. The laws relevant to home education differ throughout the country. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the federal government, about 1.1 million children were home educated in 2003.
Education Savings Accounts.
Education Savings Accounts (ESAs) are somewhat similar to vouchers: a percentage of the funds that the state would otherwise spend to educate a student in a public school are instead given to the student's family to spend on private school tuition. However, ESAs give parents additional flexibility to customize their children's educations. For example, in addition to private school tuition, ESA funds may be used for private tutoring or online learning. Alternatively, ESA funds may be saved to pay for future higher education costs. Currently, there are ESA programs in two states: Arizona ("Empowerment Savings Accounts") and Florida (the "Personal Learning Scholarship Account Program").
Tax Credit/Deduction For Educational Expenses.
Certain states allow parents to claim a tax credit or deduction as a means to provide relief for certain educational expenses. These can include private school tuition, textbooks, school supplies and equipment, tutoring, and transportation. Currently, Alabama, Illinois, Indiana, Iowa, Louisiana, Minnesota, and Wisconsin have such programs.
Debate.
Support.
The goal of school choice programs is to give parents more control over their child's education and to allow parents to pursue the most appropriate learning environments for children. For example, school choice may enable parents to choose a school that provides religious instruction, stronger discipline, better foundational skills (including reading, writing, mathematics, and science), everyday skills (from handling money to farming), or other desirable foci.
Supporters of voucher models of school choice argue that choice creates competition between schools for students. Schools that fail to attract students can be closed. Advocates of school choice argue that this competition for students (and the dollars that come with them) create a catalyst for schools to create innovative programs, become more responsive to parental demands, and to increase student achievement. Caroline Hoxby suggests that this competition increases the productivity of a school. Hoxby describes a productive school as being one that produces high achievements in its student for each dollar that is spends. Others suggest that this competition gives parents more power to influence their child's school in the school marketplace. Parents and students become the consumers and schools must work to attract new students with new programs. Parents also have the ability to punish schools that they judge to be inferior by leaving the 'bad' school for a better, more highly ranked school. Parents look for schools that will advocate for the needs of their child and if the school does not meet the needs required for that child, parents have the choice to find a school that will be more suitable
Another argument in favor of school choice is based on cost-effectiveness. Studies undertaken by the Cato Institute and other libertarian and conservative thinktanks conclude that privately run education both costs less and produces superior outcomes compared to public education.
Others argue that since children from impoverished families almost exclusively attend D or F ranked public schools, school choice programs would give parents the power to opt their children out of poorly-performing schools assigned by zip code and seek better education elsewhere. Supporters say this would level the playing field by broadening opportunities for low-income students—particularly minorities—to attend high-quality schools that would otherwise be accessible only to higher-income families.
The Organisation Internationale pour le Droit à l'Education et la Liberté d'Enseignement (OIDEL), an international non-profit organization for the development of freedom of education, maintains that the right to education is a fundamental human right which cannot exist without the presence of State benefits and the protection of individual liberties. According to the organization, freedom of education notably implies the freedom for parents to choose a school for their children without discrimination on the basis of finances. To advance freedom of education, OIDEL promotes a greater parity between public and private schooling systems.
Opposition.
Some school choice measures are criticized by public school entities, organizations opposed to church-state entanglement, and self-identified liberal advocacy groups. Known plaintiffs who have filed suit to challenge the constitutionality of state sponsored school choice laws are as follows: School Boards Associations, Public School Districts, Federations for Teachers, Associations of School Business Officials, Education Associations/Associations of Educators (unions for public school teachers), the American Civil Liberties Union, Freedom From Religion Foundation, and People for the American Way.
Public school entities are chiefly concerned that these school choice measures are taking funding away from public schools and therefore depleting their already strained resources. Alternatively, public school entities may argue that non-traditional forms of education lack sufficient regulation and oversight. Other opponents of certain school choice policies (particularly vouchers) have cited the Establishment Clause and individual state Blaine amendments, which forbid, to one degree or another, the use of direct government aid to religiously affiliated entities. This is of particular concern in the voucher debate because voucher dollars are often spent at parochial schools.
International overview and major institutional options.
France.
The French government subsidizes most private primary and secondary schools, including those affiliated with religious denominations, under contracts stipulating that education must follow the same curriculum as public schools and that schools cannot discriminate on grounds of religion or force pupils to attend religion classes.
This system of "école libre" (Free Schooling) is mostly used not for religious reasons, but for practical reasons (private schools may offer more services, such as after-class tutoring) as well as the desire of parents living in disenfranchised areas to send their children away from the local schools, where they perceive that the youth are too prone to delinquency or have too many difficulties keeping up with schooling requirements that the educational content is bound to suffer. The threatened repealing of that status in the 1980s triggered mass street demonstrations in favor of the status. 
Sweden.
Sweden reformed its school system in 1992. Its system of school choice is one of the freest in the world, allowing students to use public funds for the publicly or privately run school of their choice, including religious and for-profit schools. Fifteen years after the reform, private school enrolment had increased from 1% to 10% of the student population.
Canada.
Ontario is the only large province in Canada with limited school choice funding, Catholic, Secular and one Protestant school receive funding and are open to all students. In 2003, following an international human rights ruling, the provincial Conservative government gradually introduced a tax credit over 5 years, (when it would have been fully implemented it would have been worth up to 50% of tuition to a maximum of $3,500 at any independent school in Ontario) in order to meet the human rights norms and expand funded choice to all interested parents. However, the tax credit was retroactively canceled by the subsequent Liberal government when it had been only been in place for two years to the $1,000 point. Currently there are over 900 independent schools in Ontario. The only school choice program available to non-rich parents who wish to send their children to an independent school is a privately funded program called , a program of The Fraser Institute.
Chile.
In Chile, there is an extensive voucher system in which the state pays private and municipal schools directly, based on average attendance (90% of the country students utilize such a system). The result has been a steady increase in the number and recruitment of private schools that show consistently better results in standardized testing than municipal schools. The reduction of students in municipal schools has gone from 78% of all students in 1981, to 57% in 1990, and to less than 50% in 2005.
Regarding vouchers in Chile, researchers have found that when controls for the student's background (parental income and education) are introduced, the difference in performance between public and private subsectors is not significant. There is also greater variation within each subsector than between the two systems.
United States.
A variety of forms of school choice exist in the United States.
Scholarship Tax Credits.
Scholarship tax credit programs currently exist in Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia.
Arizona has a well-known and fast-growing tax credit program. In the Arizona Individual Private School Tuition Tax Credit Program, in accordance with A.R.S. §43-1089 and §1089.03, individuals can claim up to $1,053 and couples filing joint returns can claim up to $2106 (for 2014, amounts are indexed annually). Nearly 24,000 children received scholarships in the 2011-2012 school year. Since the program has started in 1998, over 77,500 taxpayers have participated in the program, providing over $500 million in scholarship money for children at private schools across the state.
The Arizona program was challenged in court in "ACSTO v Winn" by a group of state taxpayers on the grounds that the tax credit violated the First Amendment because the tuition grants could go to students who attend private schools with religious affiliations. The suit was initially brought against the state until the Arizona Christian School Tuition Organization (ACSTO), one of the largest School Tuition Organizations in the state, voluntarily stepped in to represent the defense with the help of the Alliance Defending Freedom (formerly Alliance Defense Fund). Typically, taxpayers are not allowed to bring suit against the government regarding how taxes are spent because injury would be purely speculative. In addition, insomuch as a donation to a School Tuition Organization is still a charitable act, just like any donation to a charity, there would be no standing unless all charitable deduction programs nationwide were brought under scrutiny. The Court ruled 5-4 to let the tax credit program stand. In April 2011, a Fairleigh Dickinson University PublicMind poll found that a majority of American voters (60%) felt that the tax credits support school choice for parents whereas 26% felt as it the tax credits support religion.
In Iowa, the Educational Opportunities Act was signed into law in 2006, creating a pool of tax credits for eligible donors to student tuition organizations (STOs). At first, these tax caps were $5 million but in 2007, Governor Chet Culver increased the total amount to $7.5 million. The Iowa Alliance for Choice in Education (Iowa ACE) oversees the STOs and advocates for school choice in Iowa.
Greater Opportunities for Access to Learning (GOAL) is the Georgia program which offers a state income tax credit to donors of scholarships to private schools. Representative David Casas was responsible for passing the Georgia version of the school choice legislation.
Vouchers.
Vouchers currently exist in Wisconsin, Ohio, Florida, and, most recently, the District of Columbia and Georgia.
The largest and oldest Voucher program is in Milwaukee. Started in 1990, and expanded in 1995, it currently allows no more than 15% of the district's public school enrollment to use vouchers. As of 2005 over 14,000 students use vouchers and they are nearing the 15% cap.
School vouchers are legally controversial in some states. In 2014 a lawsuit sought to challenge the legality of the Florida voucher program.
In the U.S., the legal and moral precedents for vouchers may have been set by the G.I. bill, which includes a voucher program for university-level education of veterans. The G.I. bill permits veterans to take their educational benefits at religious schools, an extremely divisive issue when applied to primary and secondary schools.
In "Zelman v. Simmons-Harris", 536 U.S. 639 (2002), the Supreme Court of the United States held that school vouchers could be used to pay for education in sectarian schools without violating the Establishment Clause of the First Amendment. As a result, states are basically free to enact voucher programs that provide funding for any school of the parent's choosing.
The Supreme Court has not decided, however, whether states can provide vouchers for secular schools only, excluding sectarian schools. Proponents of funding for parochial schools argue that such an exclusion would violate the free exercise clause. However, in "Locke v. Davey", 540 U.S. 712 (2004), the Court held that states could exclude majors in "devotional theology" from an otherwise generally available college scholarship. The Court has not indicated, however, whether this holding extends to the public school context, and it may well be limited to the context of individuals training to enter the ministry.
Charter schools.
The majority of states (and the District of Columbia) have charter school laws. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy, opened in St. Paul, Minnesota in 1992.
Dayton, Ohio has between 22–26% of all children in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%) and the State of Arizona. Almost 1 in 4 public schools in Arizona are charter schools, comprising about 8% of total enrollment.
Charter schools can also come in the form of Cyber Charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like charter schools, they are public schools, but free of many of the rules and regulations that public schools must follow.
Magnet schools.
Magnet schools are public schools that often have a specialized function like science, technology or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, the students must test into the school.
Home schooling.
The laws relevant to homeschooling differ between US states. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the Federal Government, about 1.1 million children were Home Educated in 2003.
College.
The United States has school choice at the university level. College students can get subsidized tuition by attending "any" public college or university within their state of residence. Furthermore, the U.S. federal government provides tuition assistance for both public and private colleges via the G.I. Bill and federally guaranteed student loans.

</doc>
<doc id="27762" url="http://en.wikipedia.org/wiki?curid=27762" title="Star Frontiers">
Star Frontiers

Star Frontiers is a science fiction role-playing game produced by TSR beginning in 1982. The game offered a space-opera action-adventure setting.
Setting.
"Star Frontiers" takes place near the center of a spiral galaxy (the setting does not specify whether the galaxy is our own Milky Way). 
A previously undiscovered quirk of the laws of physics allows starships to jump to "The Void", a hyperspatial realm that greatly shortens the travel times between inhabited worlds, once they reach 1% of the speed of light (3,000 km/s).
The basic game setting was an area known as "The Frontier Sector" where four sentient races (Dralasite, Humans, Vrusk, and Yazirian) had met and formed the United Planetary Federation (UPF). The original homeworlds of the Dralasites, Humans, and Vrusk were never detailed in the setting and it is possible that they no longer existed. A large number of the star systems shown on the map of the Frontier sector in the basic rulebook were unexplored and undetailed, allowing the Gamemaster (called the "referee" in the game) to put whatever they wished there.
Players could take on any number of possible roles in the setting but the default was to act as hired agents of the Pan Galactic corporation in exploring the Frontier and fighting the aggressive incursions of the alien and mysterious worm-like race known as the Sathar. Most published modules for the game followed these themes.
Sapient races.
These races were altered heavily and reused in TSR's "Spelljammer", and were later loosely republished for "d20 Future" by Wizards of the Coast.
Game mechanics.
The game was a percentile-based system and used only 10-sided dice (d10). Characters had attributes rated from 1-100 (usually in the 25-75 range) which could be rolled against for raw-attribute actions such as lifting items or getting out of the way of falling rocks. There were eight attributes that were paired together (and shared the same rating to begin with)—Strength/Stamina, Dexterity/Reaction Speed, Intuition/Logic, and Personality/Leadership.
Characters also each had a Primary Skill Area (PSA—Military, Technological, or Biosocial) which allowed them to buy skills that fell into their PSA at a discount. Skills were rated from 1–6 and usually consisted of a set of subskills that gave a chance for accomplishing a particular action as a base percentage plus a 10% bonus for each skill level the character had in the skill. Weapon skills were based on the character's relevant attribute (Dexterity or Strength) but other skills had a base chance of success independent of the character's attributes. Many of the technological skills were penalized by the complexity of the robot, security system, or computer the character was attempting to manipulate (also rated from 1 to 6).
Characters were usually quite durable in combat—it would take several hits from normal weapons to kill an average character. Medical technology was also advanced—characters could recover quickly from wounds with appropriate medical attention and a dead character could be "frozen" and revived later.
Vehicle and robot rules were included in the "Alpha Dawn" basic set. A beneficial feature of the game was its seamless integration of personal, vehicle and aerial combat simulation. The "Knight Hawks" rules expansion set included detailed rules for starships.
The basic set also included a short "bestiary" of creatures native to the world of Volturnus (the setting for the introductory module included with the basic boxed set), along with rules for creating new creatures.
Character advancement consisted of spending experience points on improving skills and attributes.
Products.
The basic boxed set was renamed "Alpha Dawn" after the expansions began publication. It included two ten-sided dice, a large set of cardboard counters, and a folding map with a futuristic city on one side and various wilderness areas on the other for use with the included adventure, SF-0: "Crash on Volturnus".
A second boxed set called "Knight Hawks" followed shortly. It provided rules for using starships in the setting and also a set of wargame rules for fighting space battles between the UPF and Sathar. Included were counters for starships, two-ten sided dice, a large folding map with open space on one side and on the other a space station and starship (for use with the included adventure), and the adventure SFKH-0: "Warriors of White Light". This set was designed by Douglas Niles (who also designed the D&D wargame "Battlesystem", released two years later).
Adventures printed separately for the game included two more adventures set on Volturnus (SF-1: "Volturnus, Planet of Mystery" and SF-2: "Starspawn of Volturnus" continuing the adventure included in the basic set), SF-3: "Sundown on Starmist", SF-4: "Mission to Alcazzar", SF-5: "Bugs in the System" and SF-6: "Dark Side of the Moon". The last two modules (SF-5 and SF-6) were written by authors from TSR's UK division, and are distinctly different from the others in the series in tone and production style.
Adventures using the "Knight Hawks" rules included SFKH-1: "Dramune Run" and a trilogy set "Beyond the Frontier" in which the players learn more about the Sathar and foil their latest plot (SFKH-2: "Mutiny on the Eleanor Moraes", SFKH-3: "Face of the Enemy", and SFKH-4: "The War Machine").
Two modules also re-created the plot and setting of the movies ' and '.
A late addition to the line was "Zebulon's Guide to Frontier Space" which introduced several additional races and radical changes to the game's mechanics. Of the three planned volumes of the Guide, only the first was ever published (in 1985), leaving the game in an uncomfortable, half-overhauled state. Gamers were given little to no practical advice on how to convert their existing characters to the new rules, and TSR never published any further products using the "Zebulon's" concepts.
Current products.
Wizards of the Coast published many of the races originally found in "Star Frontiers" in their "d20 Future" supplement for d20 Modern.
Current versions of the original game are fanworks. In addition to the "Star frontiers" Redux and "Knight Hawks" Vector Rules, a high-quality fan E-zine named "Star Frontiersman" is being published on a regular basis. A multiplayer flight simulator version of Knight Hawks Vector is being developed for Orbiter Space Simulator program, and a virtual tabletop version is also in the works. Play-by-forum post and OpenRPG games are currently being played with new ones starting all the time in the Starfrontiers.org forum.
A version of the setting called "Star Law", which uses the d20 system rules was published as an alternate campaign setting in the d20 Future book. It uses the species names of Vrusk, Dralasite, Sathar, and Yazirian, but is not actually the "Star Frontiers" setting.

</doc>
<doc id="27763" url="http://en.wikipedia.org/wiki?curid=27763" title="Structuralism">
Structuralism

In sociology, anthropology and linguistics, structuralism is the theory that elements of human culture must be understood in terms of their relationship to a larger, overarching system or structure. It works to uncover the structures that underlie all the things that humans do, think, perceive, and feel. Alternatively, as summarized by philosopher Simon Blackburn, structuralism is "the belief that phenomena of human life are not intelligible except through their interrelations. These relations constitute a structure, and behind local variations in the surface phenomena there are constant laws of abstract culture".
Structuralism in Europe developed in the early 1900s, in the structural linguistics of Ferdinand de Saussure and the subsequent Prague, Moscow and Copenhagen schools of linguistics. In the late 1950s and early '60s, when structural linguistics was facing serious challenges from the likes of Noam Chomsky and thus fading in importance, an array of scholars in the humanities borrowed Saussure's concepts for use in their respective fields of study. French anthropologist Claude Lévi-Strauss was arguably the first such scholar, sparking a widespread interest in Structuralism.
The structuralist mode of reasoning has been applied in a diverse range of fields, including anthropology, sociology, psychology, literary criticism, economics and architecture. The most prominent thinkers associated with structuralism include Lévi-Strauss, linguist Roman Jakobson, and psychoanalyst Jacques Lacan. As an intellectual movement, structuralism was initially presumed to be the heir apparent to existentialism. However, by the late 1960s, many of structuralism's basic tenets came under attack from a new wave of predominantly French intellectuals such as the philosopher and historian Michel Foucault, the philosopher and social commentator Jacques Derrida, the Marxist philosopher Louis Althusser, and the literary critic Roland Barthes. Though elements of their work necessarily relate to structuralism and are informed by it, these theorists have generally been referred to as post-structuralists.
In the 1970s, structuralism was criticised for its rigidity and ahistoricism. Despite this, many of structuralism's proponents, such as Jacques Lacan, continue to assert an influence on continental philosophy and many of the fundamental assumptions of some of structuralism's post-structuralist critics are a continuation of structuralism.
Overview.
The term "structuralism" is a belated term that describes a particular philosophical/literary movement or moment. The term appeared in the works of French anthropologist Claude Lévi-Strauss and gave rise, in France, to the "structuralist movement." Influencing the thinking of writers such as Louis Althusser, the psychoanalyst Jacques Lacan, as well as the structural Marxism of Nicos Poulantzas, most of whom disavowed themselves as being a part of this movement.
The origins of structuralism connect with the work of Ferdinand de Saussure on linguistics, along with the linguistics of the Prague and Moscow schools. In brief, de Saussure's structural linguistics propounded three related concepts.
Proponents of structuralism would argue that a specific domain of culture may be understood by means of a structure—modelled on language—that is distinct both from the organizations of reality and those of ideas or the imagination—the "third order". In Lacan's psychoanalytic theory, for example, the structural order of "the Symbolic" is distinguished both from "the Real" and "the Imaginary"; similarly, in Althusser's Marxist theory, the structural order of the capitalist mode of production is distinct both from the actual, real agents involved in its relations and from the ideological forms in which those relations are understood.
Blending Freud and de Saussure, the French (post)structuralist Jacques Lacan applied structuralism to psychoanalysis and, in a different way, Jean Piaget applied structuralism to the study of psychology. But Jean Piaget, who would better define himself as constructivist, considers structuralism as "a method and not a doctrine" because for him "there exists no structure without a construction, abstract or genetic".
Although the French theorist Louis Althusser is often associated with a brand of structural social analysis which helped give rise to "structural Marxism", such association was contested by Althusser himself in the Italian foreword to the second edition of "Reading Capital". In this foreword Althusser states the following: 
"Despite the precautions we took to distinguish ourselves from the 'structuralist' ideology ..., despite the decisive intervention of categories foreign to 'structuralism' ..., the terminology we employed was too close in many respects to the 'structuralist' terminology not to give rise to an ambiguity. With a very few exceptions ... our interpretation of Marx has generally been recognized and judged, in homage to the current fashion, as 'structuralist'... We believe that despite the terminological ambiguity, the profound tendency of our texts was not attached to the 'structuralist' ideology."
In a later development, feminist theorist Alison Assiter enumerated four ideas that she says are common to the various forms of structuralism. First, that a structure determines the position of each element of a whole. Second, that every system has a structure. Third, structural laws deal with co-existence rather than change. Fourth, structures are the "real things" that lie beneath the surface or the appearance of meaning.
Structuralism in linguistics.
In Ferdinand de Saussure's "Course in General Linguistics" (written by Saussure's colleagues after his death and based on student notes), the analysis focuses not on the "use" of language (called ""parole", or speech), but rather on the underlying system of language (called "langue""). This approach examines how the elements of language relate to each other in the present, synchronically rather than diachronically. Saussure argued that linguistic signs were composed of two parts:
This was quite different from previous approaches that focused on the relationship between words and the things in the world that they designate. Other key notions in structural linguistics include paradigm, syntagm, and value (though these notions were not fully developed in Saussure's thought). A structural "idealism" is a class of linguistic units (lexemes, morphemes or even constructions) that are possible in a certain position in a given linguistic environment (such as a given sentence), which is called the "syntagm". The different functional role of each of these members of the paradigm is called "value" ("valeur" in French).
Saussure's "Course" influenced many linguists between World War I and World War II. In the United States, for instance, Leonard Bloomfield developed his own version of structural linguistics, as did Louis Hjelmslev in Denmark and Alf Sommerfelt in Norway. In France Antoine Meillet and Émile Benveniste continued Saussure's project. Most importantly, however, members of the Prague school of linguistics such as Roman Jakobson and Nikolai Trubetzkoy conducted research that would be greatly influential. However, by the 1950s Saussure's linguistic concepts were under heavy criticism and were soon largely abandoned by practicing linguists: 
"Saussure's views are not held, so far as I know, by modern linguists, only by literary critics and the occasional philosopher. [Strict adherence to Saussure] has elicited wrong film and literary theory on a grand scale. One can find dozens of books of literary theory bogged down in signifiers and signifieds, but only a handful that refer to Chomsky."
The clearest and most important example of Prague school structuralism lies in phonemics. Rather than simply compiling a list of which sounds occur in a language, the Prague school sought to examine how they were related. They determined that the inventory of sounds in a language could be analyzed in terms of a series of contrasts. Thus in English the sounds /p/ and /b/ represent distinct phonemes because there are cases (minimal pairs) where the contrast between the two is the only difference between two distinct words (e.g. 'pat' and 'bat'). Analyzing sounds in terms of contrastive features also opens up comparative scope—it makes clear, for instance, that the difficulty Japanese speakers have differentiating /r/ and /l/ in English is because these sounds are not contrastive in Japanese. Phonology would become the paradigmatic basis for structuralism in a number of different fields.
Structuralism in anthropology.
According to structural theory in anthropology and social anthropology, meaning is produced and reproduced within a culture through various practices, phenomena and activities that serve as systems of signification. A structuralist approach may study activities as diverse as food-preparation and serving rituals, religious rites, games, literary and non-literary texts, and other forms of entertainment to discover the deep structures by which meaning is produced and reproduced within the culture. For example, Lévi-Strauss analyzed in the 1950s cultural phenomena including mythology, kinship (the alliance theory and the incest taboo), and food preparation. In addition to these studies, he produced more linguistically focused writings in which he applied Saussure's distinction between "langue" and "parole" in his search for the fundamental structures of the human mind, arguing that the structures that form the "deep grammar" of society originate in the mind and operate in people unconsciously. Lévi-Strauss took inspiration from mathematics.
Another concept used in structural anthropology came from the Prague school of linguistics, where Roman Jakobson and others analyzed sounds based on the presence or absence of certain features (such as voiceless vs. voiced). Lévi-Strauss included this in his conceptualization of the universal structures of the mind, which he held to operate based on pairs of binary oppositions such as hot-cold, male-female, culture-nature, cooked-raw, or marriageable vs. tabooed women.
A third influence came from Marcel Mauss (1872–1950), who had written on gift-exchange systems. Based on Mauss, for instance, Lévi-Strauss argued that kinship systems are based on the exchange of women between groups (a position known as 'alliance theory') as opposed to the 'descent'-based theory described by Edward Evans-Pritchard and Meyer Fortes. While replacing Marcel Mauss at his "Ecole Pratique des Hautes Etudes" chair, Lévi-Strauss' writing became widely popular in the 1960s and 1970s and gave rise to the term "structuralism" itself.
In Britain, authors such as Rodney Needham and Edmund Leach were highly influenced by structuralism. Authors such as Maurice Godelier and Emmanuel Terray combined Marxism with structural anthropology in France. In the United States, authors such as Marshall Sahlins and James Boon built on structuralism to provide their own analysis of human society. Structural anthropology fell out of favour in the early 1980s for a number of reasons. D'Andrade suggests that this was because it made unverifiable assumptions about the universal structures of the human mind. Authors such as Eric Wolf argued that political economy and colonialism should be at the forefront of anthropology. More generally, criticisms of structuralism by Pierre Bourdieu led to a concern with how cultural and social structures were changed by human agency and practice, a trend which Sherry Ortner has referred to as 'practice theory'.
Some anthropological theorists, however, while finding considerable fault with Lévi-Strauss's version of structuralism, did not turn away from a fundamental structural basis for human culture. The Biogenetic Structuralism group for instance argued that some kind of structural foundation for culture must exist because all humans inherit the same system of brain structures. They proposed a kind of Neuroanthropology which would lay the foundations for a more complete scientific account of cultural similarity and variation by requiring an integration of cultural anthropology and neuroscience—a program that theorists such as Victor Turner also embraced.
Structuralism in literary theory and criticism.
In literary theory, structuralist criticism relates literary texts to a larger structure, which may be a particular genre, a range of intertextual connections, a model of a universal narrative structure, or a system of recurrent patterns or motifs. Structuralism argues that there must be a structure in every text, which explains why it is easier for experienced readers than for non-experienced readers to interpret a text. Hence, everything that is written seems to be governed by specific rules, or a "grammar of literature", that one learns in educational institutions and that are to be unmasked.
A potential problem of structuralist interpretation is that it can be highly reductive, as scholar Catherine Belsey puts it: "the structuralist danger of collapsing all difference." An example of such a reading might be if a student concludes the authors of "West Side Story" did not write anything "really" new, because their work has the same structure as Shakespeare's "Romeo and Juliet". In both texts a girl and a boy fall in love (a "formula" with a symbolic operator between them would be "Boy + Girl") despite the fact that they belong to two groups that hate each other ("Boy's Group - Girl's Group" or "Opposing forces") and conflict is resolved by their death. Structuralist readings focus on how the structures of the single text resolve inherent narrative tensions. If a structuralist reading focuses on multiple texts, there must be some way in which those texts unify themselves into a coherent system. The versatility of structuralism is such that a literary critic could make the same claim about a story of two "friendly" families ("Boy's Family + Girl's Family") that arrange a marriage between their children despite the fact that the children hate each other ("Boy - Girl") and then the children commit suicide to escape the arranged marriage; the justification is that the second story's structure is an 'inversion' of the first story's structure: the relationship between the values of love and the two pairs of parties involved have been reversed.
Structuralistic literary criticism argues that the "literary banter of a text" can lie only in new structure, rather than in the specifics of character development and voice in which that structure is expressed. Literary structuralism often follows the lead of Vladimir Propp, Algirdas Julien Greimas, and Claude Lévi-Strauss in seeking out basic deep elements in stories, myths, and more recently, anecdotes, which are combined in various ways to produce the many versions of the ur-story or ur-myth.
There is considerable similarity between structural literary theory and Northrop Frye's archetypal criticism, which is also indebted to the anthropological study of myths. Some critics have also tried to apply the theory to individual works, but the effort to find unique structures in individual literary works runs counter to the structuralist program and has an affinity with New Criticism.
History and background.
Throughout the 1940s and 1950s, existentialism, such as that propounded by Jean-Paul Sartre, was the dominant European intellectual movement. Structuralism rose to prominence in France in the wake of existentialism, particularly in the 1960s. The initial popularity of structuralism in France led to its spread across the globe.
Structuralism rejected the concept of human freedom and choice and focused instead on the way that human experience and thus, behavior, is determined by various structures. The most important initial work on this score was Claude Lévi-Strauss's 1949 volume "The Elementary Structures of Kinship". Lévi-Strauss had known Jakobson during their time together at the New School in New York during WWII and was influenced by both Jakobson's structuralism as well as the American anthropological tradition. In "Elementary Structures" he examined kinship systems from a structural point of view and demonstrated how apparently different social organizations were in fact different permutations of a few basic kinship structures. In the late 1950s he published "Structural Anthropology", a collection of essays outlining his program for structuralism.
By the early 1960s structuralism as a movement was coming into its own and some believed that it offered a single unified approach to human life that would embrace all disciplines. Roland Barthes and Jacques Derrida focused on how structuralism could be applied to literature.
The so-called "Gang of Four" of structuralism was Lévi-Strauss, Lacan, Barthes, and Foucault.
Interpretations and general criticisms.
Structuralism is less popular today than other approaches, such as post-structuralism and deconstruction. Structuralism has often been criticized for being ahistorical and for favoring deterministic structural forces over the ability of people to act. As the political turbulence of the 1960s and 1970s (and particularly the student uprisings of May 1968) began affecting academia, issues of power and political struggle moved to the center of people's attention.
In the 1980s, deconstruction—and its emphasis on the fundamental ambiguity of language rather than its crystalline logical structure—became popular. By the end of the century structuralism was seen as an historically important school of thought, but the movements that it spawned, rather than structuralism itself, commanded attention.
Several social thinkers and academics have strongly criticized structuralism or even dismissed it "in toto". The French hermeneutic philosopher Paul Ricœur (1969) criticized Lévi-Strauss for constantly overstepping the limits of validity of the structuralist approach, ending up in what Ricoeur described as "a Kantianism without a transcendental subject". Anthropologist Adam Kuper (1973) argued that "'Structuralism' came to have something of the momentum of a millennial movement and some of its adherents felt that they formed a secret society of the seeing in a world of the blind. Conversion was not just a matter of accepting a new paradigm. It was, almost, a question of salvation." Philip Noel Pettit (1975) called for an abandoning of "the positivist dream which Lévi-Strauss dreamed for semiology" arguing that semiology is not to be placed among the natural sciences. Cornelius Castoriadis (1975) criticized structuralism as failing to explain symbolic mediation in the social world; he viewed structuralism as a variation on the "logicist" theme, and he argued that, contrary to what structuralists advocate, language—and symbolic systems in general—cannot be reduced to logical organizations on the basis of the binary logic of oppositions. Critical theorist Jürgen Habermas (1985) accused structuralists, such as Foucault, of being positivists; he remarked that while Foucault is not an ordinary positivist, he nevertheless paradoxically uses the tools of science to criticize science (see "Performative contradiction" and "Foucault–Habermas debate"). Sociologist Anthony Giddens (1993) is another notable critic; while Giddens draws on a range of structuralist themes in his theorizing, he dismisses the structuralist view that the reproduction of social systems is merely "a mechanical outcome".

</doc>
<doc id="27764" url="http://en.wikipedia.org/wiki?curid=27764" title="Systems engineering">
Systems engineering

Systems" "engineering is an interdisciplinary field of engineering that focuses on how to design and manage complex engineering systems over their life cycles. Issues such as requirements engineering, reliability, logistics, "coordination" of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system development, design, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as control engineering, industrial engineering, software engineering, organizational studies, and project management. Systems engineering ensures that all likely aspects of a project or system are considered, and integrated into a whole.
The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high quality outputs with minimum cost and time. The systems engineering process must begin by discovering the real problems that need to be resolved, and identify the most probable or highest impact failures that can occur - systems engineering involves finding elegant solutions to these problems.
History.
The term "systems engineering" can be traced back to Bell Telephone Laboratories in the 1940s. The need to identify and manipulate the properties of a system as a whole, which in complex engineering projects may greatly differ from the sum of the parts' properties, motivated various industries to apply the discipline.
When it was no longer possible to rely on design evolution to improve upon a system and the existing tools were not sufficient to meet growing demands, new methods began to be developed that addressed the complexity directly. The continuing evolution of systems engineering comprises the development and identification of new methods and modeling techniques. These methods aid in better comprehension of engineering systems as they grow more complex. Popular tools that are often used in the systems engineering context were developed during these times, including USL, UML, QFD, and IDEF0.
In 1990, a professional society for systems engineering, the "National Council on Systems Engineering" (NCOSE), was founded by representatives from a number of U.S. corporations and organizations. NCOSE was created to address the need for improvements in systems engineering practices and education. As a result of growing involvement from systems engineers outside of the U.S., the name of the organization was changed to the International Council on Systems Engineering (INCOSE) in 1995. Schools in several countries offer graduate programs in systems engineering, and continuing education options are also available for practicing engineers.
Concept.
Systems engineering signifies only an approach and, more recently, a discipline in engineering. The aim of education in systems engineering is to simply formalize the approach and in doing so, identify new methods and research opportunities similar to the way it occurs in other fields of engineering. As an approach, systems engineering is holistic and interdisciplinary in flavour.
Origins and traditional scope.
The traditional scope of engineering embraces the design, development, production and operation of physical systems, and systems engineering, as originally conceived, falls within this scope. "Systems engineering", in this sense of the term, refers to the distinctive set of concepts, methodologies, organizational structures (and so on) that have been developed to meet the challenges of engineering functional physical systems of unprecedented complexity. The Apollo program is a leading example of a systems engineering project.
Evolution to broader scope.
The use of the term "systems engineer" has evolved over time to embrace a wider, more holistic concept of "systems" and of engineering processes. This evolution of the definition has been a subject of ongoing controversy, and the term continues to apply to both the narrower and broader scope.
Traditional systems engineering was seen as a branch of engineering in the classical sense, that is, as applied only to physical system, such as space craft and aircraft. More recently, systems engineering has evolved to a take on a broader meaning especially when humans were seen as an essential component of a system. Checkland, for example, captures the broader meaning of systems engineering by stating that 'engineering' "can be read in its general sense; you can engineer a meeting or a political agreement." 
Consistent with the broader scope of systems engineering, the Systems Engineering Body of Knowledge (SEBoK) has defined three types of systems engineering: (1) Product Systems Engineering (PSE) is the traditional systems engineering focused on the design of physical systems consisting of hardware and software. (2) Enterprise Systems Engineering (ESE) pertains to the view of enterprises, that is, organizations or combinations of organizations, as systems. (3) Service Systems Engineering (SSE) has to do with the engineering of service systems. Checkland defines a service system as a system which is conceived as serving another system. Most civil infrastructure systems are service systems.
Holistic view.
Systems engineering focuses on analyzing and eliciting customer needs and required functionality early in the development cycle, documenting requirements, then proceeding with design synthesis and system validation while considering the complete problem, the system lifecycle. This includes fully understanding all of the stakeholders involved. Oliver "et al." claim that the systems engineering process can be decomposed into
Within Oliver's model, the goal of the Management Process is to organize the technical effort in the lifecycle, while the Technical Process includes "assessing available information", "defining effectiveness measures", to "create a behavior model", "create a structure model", "perform trade-off analysis", and "create sequential build & test plan".
Depending on their application, although there are several models that are used in the industry, all of them aim to identify the relation between the various stages mentioned above and incorporate feedback. Examples of such models include the Waterfall model and the VEE model.
Interdisciplinary field.
System development often requires contribution from diverse technical disciplines. By providing a systems (holistic) view of the development effort, systems engineering helps mold all the technical contributors into a unified team effort, forming a structured development process that proceeds from concept to production to operation and, in some cases, to termination and disposal.
This perspective is often replicated in educational programs, in that systems engineering courses are taught by faculty from other engineering departments, which helps create an interdisciplinary environment.
Managing complexity.
The need for systems engineering arose with the increase in complexity of systems and projects, in turn exponentially increasing the possibility of component friction, and therefore the unreliability of the design. When speaking in this context, complexity incorporates not only engineering systems, but also the logical human organization of data. At the same time, a system can become more complex due to an increase in size as well as with an increase in the amount of data, variables, or the number of fields that are involved in the design. The International Space Station is an example of such a system.
The development of smarter control algorithms, microprocessor design, and analysis of environmental systems also come within the purview of systems engineering. Systems engineering encourages the use of tools and methods to better comprehend and manage complexity in systems. Some examples of these tools can be seen here:
Taking an interdisciplinary approach to engineering systems is inherently complex since the behavior of and interaction among system components is not always immediately well defined or understood. Defining and characterizing such systems and subsystems and the interactions among them is one of the goals of systems engineering. In doing so, the gap that exists between informal requirements from users, operators, marketing organizations, and technical specifications is successfully bridged.
Scope.
One way to understand the motivation behind systems engineering is to see it as a method, or practice, to identify and improve common rules that exist within a wide variety of systems. Keeping this in mind, the principles of systems engineering – holism, emergent behavior, boundary, et al. – can be applied to any system, complex or otherwise, provided systems thinking is employed at all levels. Besides defense and aerospace, many information and technology based companies, software development firms, and industries in the field of electronics & communications require systems engineers as part of their team.
An analysis by the INCOSE Systems Engineering center of excellence (SECOE) indicates that optimal effort spent on systems engineering is about 15-20% of the total project effort. At the same time, studies have shown that systems engineering essentially leads to reduction in costs among other benefits. However, no quantitative survey at a larger scale encompassing a wide variety of industries has been conducted until recently. Such studies are underway to determine the effectiveness and quantify the benefits of systems engineering.
Systems engineering encourages the use of modeling and simulation to validate assumptions or theories on systems and the interactions within them.
Use of methods that allow early detection of possible failures, in safety engineering, are integrated into the design process. At the same time, decisions made at the beginning of a project whose consequences are not clearly understood can have enormous implications later in the life of a system, and it is the task of the modern systems engineer to explore these issues and make critical decisions. No method guarantees today's decisions will still be valid when a system goes into service years or decades after first conceived. However, there are techniques that support the process of systems engineering. Examples include soft systems methodology, Jay Wright Forrester's System dynamics method, and the Unified Modeling Language (UML)—all currently being explored, evaluated, and developed to support the engineering decision process.
Education.
Education in systems engineering is often seen as an extension to the regular engineering courses, reflecting the industry attitude that engineering students need a foundational background in one of the traditional engineering disciplines (e.g., aerospace engineering, automotive engineering, electrical engineering, mechanical engineering, industrial engineering)—plus practical, real-world experience to be effective as systems engineers. Undergraduate university programs in systems engineering are rare. Typically, systems engineering is offered at the graduate level in combination with interdisciplinary study.
INCOSE maintains a continuously updated Directory of Systems Engineering Academic Programs worldwide. As of 2009, there are about 80 institutions in United States that offer 165 undergraduate and graduate programs in systems engineering. Education in systems engineering can be taken as "Systems-centric" or "Domain-centric".
Both of these patterns strive to educate the systems engineer who is able to oversee interdisciplinary projects with the depth required of a core-engineer.
Systems engineering topics.
Systems engineering tools are strategies, procedures, and techniques that aid in performing systems engineering on a project or product. The purpose of these tools vary from database management, graphical browsing, simulation, and reasoning, to document production, neutral import/export and more.
System.
There are many definitions of what a system is in the field of systems engineering. Below are a few authoritative definitions:
The systems engineering process.
Depending on their application, tools are used for various stages of the systems engineering process:
Using models.
Models play important and diverse roles in systems engineering. A model can be defined in several
ways, including:
Together, these definitions are broad enough to encompass physical engineering models used in the verification of a system design, as well as schematic models like a functional flow block diagram and mathematical (i.e., quantitative) models used in the trade study process. This section focuses on the last.
The main reason for using mathematical models and diagrams in trade studies is to provide estimates of system effectiveness, performance or technical attributes, and cost from a set of known or estimable quantities. Typically, a collection of separate models is needed to provide all of these outcome variables. The heart of any mathematical model is a set of meaningful quantitative relationships among its inputs and outputs. These relationships can be as simple as adding up constituent quantities to obtain a total, or as complex as a set of differential equations describing the trajectory of a spacecraft in a gravitational field. Ideally, the relationships express causality, not just correlation. Furthermore, key to successful systems engineering activities are also the methods with which these models are efficiently and effectively managed and used to simulate the systems. However, diverse domains often present recurring problems of modeling and simulation for systems engineering, and new advancements are aiming to crossfertilize methods among distinct scientific and engineering communities, under the title of 'Modeling & Simulation-based Systems Engineering'.
Modeling formalisms and graphical representations.
Initially, when the primary purpose of a systems engineer is to comprehend a complex problem, graphic representations of a system are used to communicate a system's functional and data requirements. Common graphical representations include:
A graphical representation relates the various subsystems or parts of a system through functions, data, or interfaces. Any or each of the above methods are used in an industry based on its requirements. For instance, the N2 chart may be used where interfaces between systems is important. Part of the design phase is to create structural and behavioral models of the system.
Once the requirements are understood, it is now the responsibility of a systems engineer to refine them, and to determine, along with other engineers, the best technology for a job. At this point starting with a trade study, systems engineering encourages the use of weighted choices to determine the best option. A decision matrix, or Pugh method, is one way (QFD is another) to make this choice while considering all criteria that are important. The trade study in turn informs the design, which again affects graphic representations of the system (without changing the requirements). In an SE process, this stage represents the iterative step that is carried out until a feasible solution is found. A decision matrix is often populated using techniques such as statistical analysis, reliability analysis, system dynamics (feedback control), and optimization methods.
Other tools.
Systems Modeling Language (SysML), a modeling language used for systems engineering applications, supports the specification, analysis, design, verification and validation of a broad range of complex systems.
Lifecycle Modeling Language (LML), is an open-standard modeling language designed for systems engineering that supports the full lifecycle: conceptual, utilization, support and retirement stages.
Related fields and sub-fields.
Many related fields may be considered tightly coupled to systems engineering. These areas have contributed to the development of systems engineering as a distinct entity.

</doc>
<doc id="27765" url="http://en.wikipedia.org/wiki?curid=27765" title="September 4">
September 4

September 4 is the day of the year in the Gregorian calendar.

</doc>
<doc id="27766" url="http://en.wikipedia.org/wiki?curid=27766" title="Sam &amp; Max">
Sam &amp; Max

Sam & Max is a media franchise focusing on the fictional characters of Sam and Max, the Freelance Police. The characters, who occupy a universe that parodies American popular culture, were created by Steve Purcell in his youth, and later debuted in a 1987 comic book series. The characters have since been the subject of a graphic adventure video game developed by LucasArts, a produced for Fox in cooperation with Nelvana Limited, and a series of episodic adventure games developed by Telltale Games. In addition, a variety of machinima and a webcomic have been produced for the series.
The characters are a pair of anthropomorphic, vigilante private investigators based in a dilapidated office block in New York City. Sam is a calculative six-foot dog wearing a suit and a fedora, while Max is a short and aggressive "hyperkinetic rabbity thing". Both enjoy solving problems and cases as maniacally as possible, often with complete disregard for the law. Driving a seemingly indestructible black-and-white 1960 DeSoto Adventurer, the pair travel to many contemporary and historical locations to fight crime, including the Moon, Ancient Egypt, the White House and the Philippines, as well as several fictitious locations.
The series has been very successful despite its relatively limited amount of media, and has gathered a significant fan base. However, the franchise did not gain more widespread recognition until after the 1993 release of LucasArts' "Sam & Max Hit the Road", which cultivated interest in Purcell's original comics. "Sam & Max Hit the Road" is regarded as an exceptional adventure game and an iconic classic of computer gaming in the 1990s. Subsequent video games and the television series have also fared well with both critics and fans; critics consider the episodic video games to be the first successful application of the episodic distribution model.
Overview.
Creation.
The idea of "Sam & Max" originated with Steve Purcell's younger brother, Dave, who invented the concept of a comic about a detective team consisting of a dog and a rabbit in his youth. Dave would often leave the comics around the house, so Steve, in a case of sibling rivalry, often finished the incomplete stories in parodies of their original form, deliberately making the characters mix up each other's names, over-explain things, shoot at each other and mock the way in which they had been drawn, as "kind of a parody of the way a kid talks when he's writing comics". Over time, this developed from Steve merely mocking his brother's work to him creating his own full stories with the characters. Ultimately, in the late 1970s, Dave Purcell gave Steve the rights to the characters, signing them over in a contract on Steve's birthday and allowing him to develop the characters in his own way. In 1980, Purcell began to produce "Sam & Max" comic strips for the weekly newsletter of the California College of Arts and Crafts. Whilst the visual appearance of the characters had not yet been fully developed, the stories were similar in style to those that would follow when Purcell was offered by "Fish Police" author Steven Moncuse the chance to publish his work properly in 1987. 
Many aspects of the "Sam & Max" comics were influenced by Purcell's own experiences. Rats and roaches are common throughout the franchise, the former inspired by Purcell's pet rat. In another example, Sam and Max are occasionally shown playing a game called "fizzball", in which the object of the exercise is to hit a can of beer in mid-air with a solid axe handle. Purcell had previously invented the game with his friends, including fellow comic book writers Art Adams and Mike Mignola.
Characters.
Sam is a laid-back but enthusiastic, brown-coated anthropomorphic "canine shamus". He wears either a gray or blue suit with a matching fedora, to make people more cooperative when conversing with a six-foot talking dog. A warped sense of justice makes Sam the more passionate of the pair for their police work, only held back from taking his job seriously by Max. Nevertheless, he enjoys the mannerisms and dress that come with their line of work. Sam possesses near encyclopedic amounts of knowledge, particularly on obscure topics, and is prone to long-winded sentences filled with elaborate terminology. Although he is always keen to display this information—regardless of its accuracy—Sam can be capable of total ignorance towards more practical matters; for instance, despite his regard for his DeSoto Adventurer, he is severely negligent with the car's maintenance. Sam still retains various doglike qualities: he is excitable, enthusiastic but also susceptible to emotions of embarrassment and guilt. Nevertheless, Sam is "not above sticking his head out the car window and letting his tongue flap in the breeze". Sam rarely loses his temper, and is able to react to panic-inducing situations with extreme calm. When he does get angry, Sam tends to react in a violent, uncharacteristically savage manner, in which case it is usually Max that calms him down and prevents him from acting upon his anger. Sam usually is armed with an oversized .44 revolver.
Max is an anthropomorphic "hyperkinetic, three-foot rabbity thing" with white fur, but prefers being called a lagomorph. Max retains few characteristics consistent with a rabbit, with permanently rigid ears set in an excited posture and a huge jaw normally stuck in a crazed grin. Unhinged, uninhibited and near psychotic, Max enjoys violence and tends to prefer the aggressive way of solving problems, seeing the world as little more than a vessel for his "pinball-like stream of consciousness". This creates a seeming disregard for self-preservation; Max will revel in dangerous situations with little impression that he understands the risks he faces. As a result, Max is usually enthusiastic to engage in any activity, including being used by Sam as a cable cutter or an impromptu bludgeon. Despite this, Max possesses a sharp mind and an observational nature, and enjoys interpreting new experiences in as unpredictable manner as possible. However, Max has a distaste for long stories and occasionally loses focus during lengthy scenes of plot exposition; by his own admission, Max possesses a particularly short attention span. Despite his seemingly heartless personality, he believes strongly in protecting Sam. However, Max can still act violently towards his friend, stating that when he dies he will take Sam with him. Moreover, Max is extremely possessive of Sam and their status as partners and best friends. Max traditionally carries a Luger pistol, but as he wears no clothes, other characters often make comments as to where Max keeps it on his person. Purcell considers Max to be representative of pure id, the uncoordinated instinctual trends of the human psyche.
Media.
Comic books.
Sam and Max debuted in the 1987 comic book series "Sam & Max: Freelance Police", published by Fishwrap Productions, also the publisher of "Fish Police". The first comic, "Monkeys Violating The Heavenly Temple", was Steve Purcell's first full story. The comic came about after Purcell agreed to create a full "Sam & Max" story for publication alongside Steve Moncuse's "Fish Police" series. "Monkeys Violating The Heavenly Temple" established many of the key features in the series; the main story of the comic saw the Freelance Police journey to the Philippines to stop a volcano god cult. "Night of the Gilded Heron-Shark" and "Night of the Cringing Wildebeest" accompanied the main story, focusing on a stand-off with a group of gangsters in Sam and Max's office and an investigation into a carnival refreshment booth respectively.
Over the subsequent years, several other comics were published, often by different publishers, including Comico Comics and Epic Comics. "Fair Wind to Java" was originally published in 1988 as a Munden's Bar story in the pages of First Comics' "Grimjack", featuring the Freelance Police fighting pyramid-building aliens in Ancient Egypt, and was followed in 1989 by "On the Road", a three chapter story showing what Sam and Max do on vacation. In 1990, a Christmas themed story, "The Damned Don't Dance" was released. 1992 saw the release of a further two comics; "Bad Day On The Moon" took the Freelance Police to deal with a roach infestation bothering giant rats on the Moon, and was later adapted as a story for the animated TV series, whilst "Beast From The Cereal Aisle" focused on the duo conducting an exorcism at the local supermarket. Two more comics were produced in 1997, "The Kids Take Over" and "Belly Of The Beast". The former has Sam and Max wake up from cryogenic sleep to discover that the entire world is now ruled by children while the latter sees the Freelance Police confronting a vampire abducting children at Halloween.
Purcell joined LucasArts in 1988 as an artist and game designer, where he was approached about contributing to LucasArts' new quarterly newsletter, "The Adventurer", a publication designed to inform customers about upcoming LucasArts games and company news. From its debut issue in 1990 to 1996, Purcell created twelve comic strips for the newsletter. The strips portrayed a variety of stories, from similar plots as in the comic books to parodies of LucasArts games such as "Monkey Island" and "Full Throttle" and the Lucasfilm franchises "Star Wars" and "Indiana Jones".
In 1995, all of the comics and "The Adventurer" strips published to that date were released in a compilation, "Sam & Max: Surfin' the Highway". Published by Marlowe & Company, the 154 page book was updated and republished in 1996. This original version of "Surfin' the Highway" went out of print in 1997, becoming a high priced collectors item sold through services such as eBay. In 2007, a 197 page twenty-year anniversary edition, containing all printed comics and strips as well as a variety of other artwork, was co-designed by Steve Purcell and Jake Rodkin and published by Telltale Games. This second publication received an Eisner Award nomination for "Best Graphic Album – Reprint" in 2009.
In December 2005, Purcell started a "Sam & Max" webcomic, hosted on the website of Telltale Games. Entitled "The Big Sleep", the webcomic began with Sam and Max bursting out of their graves at Kilpeck Church in England, symbolizing the Freelance Police's return after nearly a decade. In the twelve page story, Max has to save Sam after earwigs start a colony in Sam's brain. The webcomic concluded in April 2007, and was later awarded the Eisner Award for "Best Digital Comic" of 2007.
Video games.
Following LucasArts' employment of Purcell in 1988, the characters of Sam and Max appeared in internal testing material for new SCUMM engine programmers; Purcell created animated versions of the characters and an office backdrop for the programmers to practice on. In 1992, LucasArts offered Purcell the chance to create a video game out of the characters, out of a wish to use new characters after the success of its two other main adventure titles, "Monkey Island" and "Maniac Mansion", and after a positive reaction from fans to the "Sam & Max" comic strips featured in LucasArts' "The Adventurer" newsletter. Consequently, development on a graphic adventure game, "Sam & Max Hit the Road", began shortly after. Based on the SCUMM engine and designed by Sean Clark, Michael Stemmle, Steve Purcell and his future wife Collette Michaud, the game was partially based on the 1989 comic "On The Road", and featured the Freelance Police travelling across America in search of an escaped bigfoot. Sam was voiced in the game by comedian Bill Farmer, while actor Nick Jameson voiced Max. "Sam & Max Hit the Road" was originally released for DOS in November 1993. Soon after "Sam & Max Hit the Road", another "Sam & Max" game using SCUMM entered planning under Purcell and Dave Grossman, but was abandoned. In a later interview Grossman described this sequel's highlight as "a giant spaceship shaped like Max's head".
In September 2001 development began on a new project, "Sam & Max Plunge Through Space". The game was to be an Xbox exclusive title, developed by Infinite Machine, a small company consisting of a number of former LucasArts employees. The story of the game was developed by Purcell and fellow designer Chuck Jordan and involved the Freelance Police travelling the galaxy to find a stolen Statue of Liberty. However, Infinite Machine went bankrupt within a year, and the project was abandoned.
At the 2002 Electronic Entertainment Expo convention, nearly a decade after the release of "Sam & Max Hit the Road", LucasArts announced the production of a PC sequel, entitled "Sam & Max: Freelance Police". "Freelance Police", like "Hit the Road", was to be a point-and-click graphic adventure game, utilising a new 3D game engine. Development of "Freelance Police" was led by Michael Stemmle. Steve Purcell contributed to the project by writing the story and producing concept art. Farmer and Jameson were also set to reprise their voice acting roles. In March 2004, however, quite far into the game's development, "Sam & Max: Freelance Police" was abruptly cancelled by LucasArts, citing "current market place realities and underlying economic considerations" in a short press release. The fan reaction to the cancellation was strong; a petition of 32,000 signatures stating the disappointment of fans was later presented to LucasArts.
After LucasArts' license with Steve Purcell expired in 2005, the "Sam & Max" franchise moved to Telltale Games, a company of former LucasArts employees who had worked on a number of LucasArts adventure games, including on the development of "Freelance Police". Under Telltale Games, a new episodic series of "Sam & Max" video games was announced. Like both "Sam & Max Hit the Road" and "Freelance Police", "Sam & Max Save the World" was in a point-and-click graphic adventure game format. The game utilized a new 3D game engine, different from the one used in "Freelance Police". The first season ran for six episodes, each with a self-contained storyline but with an overall story arc involving hypnotism running through the series. The first episode was released on GameTap in October 2006, with episodes following regularly until April 2007. Sam is voiced by David Nowlin, while Max is voiced by William Kasten in all episodes except the first one, where Andrew Chaikin voices the character. In addition, Telltale Games produced fifteen machinima shorts to accompany the main episodes. These shorts were released in groups of three in between the release of each episode, showing the activities of the Freelance Police in between each story.
A second season of episodic video games developed by Telltale Games was announced in July 2007. "Sam & Max Beyond Time and Space" followed the same overall format as "Save the World", with each episode having a contained storyline with an overarching storyline involving laundering of the souls of the dead. As with "Save the World", episodes were originally published on GameTap before being made available for general release. The season consisted of five episodes and ran from November 2007 to April 2008. Nowlin and Kasten both returned to reprise their voice roles. In addition to the main games, a twenty-minute machinima video was produced, taking the form of a "Sam & Max" Christmas special. 
A third and final game entitled "Sam & Max: The Devil's Playhouse" was confirmed in May 2008 for release in 2009; the title was later pushed back to 2010, with concept art emerging after Telltale's completion of "Tales of Monkey Island". The season again ran for five episodes, released monthly from April to August 2010. "The Devil's Playhouse" followed a structure similar to "'Tales of Monkey Island", with each episode forming a part of an on-going narrative, involving psychic powers and forces that would use them for world domination. A two-minute Flash cartoon also accompanied the game, dealing with the origin story of General Skun-ka'pe, one of the game's antagonists. Max also appears in Telltale's 2010 casual game "Poker Night at the Inventory" alongside Tycho Brahe from "Penny Arcade", the Heavy from "Team Fortress 2" and Strong Bad from "Homestar Runner". Sam and Max (now voiced by Dave Boat) also appear in the game's sequel alongside Claptrap from "Borderlands", Brock Samson from "The Venture Bros.", Ash Williams from "Evil Dead" and GLaDOS from "Portal".
Television series.
"Sam & Max" were adapted into a cartoon series for Fox in 1997. Produced by Canadian studio Nelvana, the series ran for 24 episodes. Each episode was approximately ten minutes, and were often aired in pairs. Broadcast on Fox Kids in the United States, YTV in Canada, and Channel 4 in the United Kingdom, the first episode was aired on October 4, 1997; the series concluded on April 25, 1998. As opposed to the more adult humor in the rest of the series, "The Adventures of Sam & Max: Freelance Police" was aimed more at children, even though some humor in it was often directed at adults. As such, the violence inherent in the franchise is toned down, including removing Sam and Max's guns, and the characters do not use the moderate profanity that they use in their other appearances. As in most "Sam & Max" stories, the series revolves around the Freelance Police accepting missions from their mysterious superior, the commissioner, and embarking on cases to a large variety of implausible locations. Sam is voiced by Harvey Atkin, while Max is voiced by Robert Tinkler. The series performed well and was considered a success, and in 1998 received the Gemini Award for "Best Animated Program or Series". Despite the series' success, a second series was never commissioned. In June 2007, it was reported that Shout! Factory were preparing a DVD release of the series. In October 2007, as part of their marketing for "Sam & Max Save the World", GameTap hosted the series on their website. The DVD release of the series was later published in March 2008.
Music.
The "Sam & Max" franchise features a variety of soundtracks that accompany its video game products. This music is mostly grounded in film noir jazz, incorporating various other styles at certain points, such as Dixieland, waltz and mariachi, usually to support the cartoon nature of the series. The first "Sam & Max" game, "Sam & Max Hit the Road", was one of the first games to feature a fully scored music soundtrack, written by LucasArts' composers Clint Bajakian, Michael Land and Peter McConnell. The music was incorporated into the game using Land and McConnell's iMUSE engine, which allowed for audio to be synchronized with the visuals. Although the full soundtrack was never released, audio renders of four of the game's MIDI tracks were included on the CD version of the game.
For "Sam & Max Save the World", "Beyond Time and Space", and "The Devil's Playhouse", Telltale Games contracted composer Jared Emerson-Johnson, a musician whose previous work included composition and sound editing for LucasArts, to write the scores. The soundtracks for the first two games were released in two disc sets after the release of the games themselves; the "Season One Soundtrack" was published in July 2007, whilst the "Season Two Soundtrack" was released in September 2008. Emerson-Johnson's scores use live performances as opposed to synthesized music often used elsewhere in the video games industry. Critics reacted positively to Emerson-Johnson's scores, IGN described Emerson-Johnson's work as a "breath of fresh air", while 1UP.com praised his work as "top-caliber" and Music4Games stated that the "whimsical nature of [the classical jazz approach] is well suited to the "Sam & Max" universe, which approaches American popular culture with a level of irreverence". Purcell later commented that Emerson-Johnson had seamlessly blended a "huge palette of genres and styles", whilst in September 2008, Brendan Q. Ferguson, one of the lead designers on "Save the World" and "Beyond Time and Space", stated that he believed that it was Emerson-Johnson's scores that created the vital atmosphere in the games, noting that prior to the implementation of the soundtracks, playing the games was an "unrelenting horror".
Cultural impact and reception.
The "Sam & Max" franchise has been highly successful critically, and is considered an iconic and influential aspect of the video game industry in the 1990s and the adventure game genre. In 2007, Steve Purcell wrote that he was somewhat surprised at the success of his creation, noting that the series had gained a large fan gathering despite the small size of the franchise. As the series contains only a small amount of comics, video games and a short TV series, Purcell commented that there was "certainly not enough material to build that relentless traction of an endlessly renewed sitcom or a syndicated comic that has existed since the Korean Conflict". The comics were well received by critics, many praising the humor and style of the stories and characters. However, later commentators have noted that the comic book series did not gain much popularity or recognition until after the release of "Sam & Max Hit the Road" in 1993; the later episodic video games are seen to have revived interest in the comics again, resulting in the creation of the webcomic "The Big Sleep" and publication of an anniversary edition of "Surfin' The Highway".
Upon its release in 1993, "Sam & Max Hit the Road" was met with near universal acclaim. Critics praised the title for its humor, voice acting, graphics, music and gameplay. It has since come to be regarded as a classic graphic adventure game, one of the most critically successful projects by LucasArts to date. "Sam & Max Hit the Road" is regularly featured in lists of top games, and was nominated for the 1994 Annie Award for "Best Animated CD-ROM", although the award instead went to LucasArts' "". The abrupt cancellation of the sequel to "Sam & Max Hit the Road" in 2004 garnered substantial criticism of LucasArts. In addition to a petition of 32,000 signatures objecting to the termination of development on "Sam & Max: Freelance Police", both Steve Purcell and the media were critical of LucasArts' decision. Purcell stated that he failed to understand quite why the game was cancelled, as he believed the development of the game was proceeding without hindrance, while the media put forward the view that LucasArts was moving to consolidate its position with low business risk "Star Wars" video games instead of pursuing the adventure games that had brought them success in earlier years. The cancellation of "Freelance Police" is often cited as the culmination in a perceived decline in the overall adventure game genre, and LucasArts later dismissed many of the designers involved with developing their adventure games, effectively ending their adventure game era.
Although "Sam & Max Save the World" did not receive the critical acclaim that "Sam & Max Hit the Road" acquired, it still received a favorable response from critics across its release in 2006 and 2007. Critics praised the game's humor, graphics and gameplay, although concerns were voiced over the low difficulty of the puzzles and the effectiveness of the story. "Save the World" is considered by journalists in the video game industry to be the first successful application of episodic gaming, as Telltale Games had managed to release a steady stream content with only small time gaps. Previous attempts by Valve Software with the "Half-Life" series, Ritual Entertainment with "SiN Episodes" and Telltale Games themselves with "" were for a variety of reasons not considered successful implementations of the distribution model. "Beyond Time and Space" was considered similar to "Save the World" and reviewers equally praised and faulted the game on this, although overall "Beyond Time and Space" received a good reception from critics.
The success of the franchise has spawned a selection of merchandise, including posters and prints, items of clothing and sketchbooks of Purcell's work during various stages of the series' development. Collectable statues of the characters have also been created. However, despite references in Purcell's sketchbooks and demand from both fans and journalists alike, plush toys of the characters have not been produced.

</doc>
