<doc id="31660" url="http://en.wikipedia.org/wiki?curid=31660" title="Eighth Amendment to the United States Constitution">
Eighth Amendment to the United States Constitution

 
The Eighth Amendment (Amendment VIII) to the United States Constitution is the part of the United States Bill of Rights (ratified December 15, 1791) prohibiting the federal government from imposing excessive bail, excessive fines, or cruel and unusual punishments, including torture. The U.S. Supreme Court has ruled that this amendment's Cruel and Unusual Punishment Clause also applies to the states. The phrases in this amendment originated in the English Bill of Rights of 1689.
Text.
Excessive bail shall not be required, nor excessive fines imposed, nor cruel and unusual punishments inflicted.
Background.
The Eighth Amendment was adopted, as part of the Bill of Rights, in 1791. It is almost identical to a provision in the English Bill of Rights of 1689, in which Parliament declared, "as their ancestors in like cases have usually done...that excessive bail ought not to be required, nor excessive fines imposed, nor cruel and unusual punishments inflicted."
The provision was largely inspired by the case in England of Titus Oates who, after the ascension of King James II in 1685, was tried for multiple acts of perjury which had caused many executions of people whom Oates had wrongly accused. Oates was sentenced to imprisonment including an annual ordeal of being taken out for two days pillory plus one day of whipping while tied to a moving cart. The Oates case eventually became a topic of the U.S. Supreme Court’s Eighth Amendment jurisprudence. The punishment of Oates involved ordinary penalties collectively imposed in a barbaric, excessive and bizarre manner. The reason why the judges in Oates' perjury case were not allowed to impose the death penalty (unlike in the cases of those whom Oates had falsely accused) may be because such a punishment would have deterred even honest witnesses from testifying in later cases.
England’s declaration against "cruel and unusual punishments" was approved by Parliament in February 1689, and was read to King William III and his wife Queen Mary II on the following day. Members of Parliament then explained in August 1689 that “the Commons had a particular regard…when that Declaration was first made” to punishments like the one that had been inflicted by the King's Bench against Titus Oates. Parliament then enacted the English Bill of Rights into law in December 1689. Members of parliament characterized the punishment in the Oates case as not just "barbarous" and "inhuman" but also "extravagant" and “exorbitant".
In England, the "cruel and unusual punishments" clause was a limitation on the discretion of judges, and required judges to adhere to precedent. According to the great treatise of the 1760s by William Blackstone entitled "Commentaries on the Laws of England":[H]owever unlimited the power of the court may seem, it is far from being wholly arbitrary; but its discretion is regulated by law. For the bill of rights has particularly declared, that excessive fines ought not to be imposed, nor cruel and unusual punishments inflicted: (which had a retrospect to some unprecedented proceedings in the court of king's bench, in the reign of king James the second)...
Virginia adopted this provision of the English Bill of Rights in the Virginia Declaration of Rights of 1776, and the Virginia convention that ratified the U.S. Constitution recommended in 1788 that this language also be included in the Constitution.
Virginians such as George Mason and Patrick Henry wanted to ensure that this restriction would also be applied as a limitation on Congress. Mason warned that, otherwise, Congress may “inflict unusual and severe punishments.” Henry emphasized that Congress should not be allowed to depart from precedent:What has distinguished our ancestors?--That they would not admit of tortures, or cruel and barbarous punishment. But Congress may introduce the practice of the civil law, in preference to that of the common law. They may introduce the practice of France, Spain, and Germany...
 Ultimately, Henry and Mason prevailed, and the Eighth Amendment was adopted. James Madison changed "ought" to "shall", when he proposed the amendment to Congress in 1789.
Excessive bail.
In England, sheriffs originally determined whether to grant bail to criminal suspects. Since they tended to abuse their power, Parliament passed a statute in 1275 whereby bailable and non-bailable offenses were defined. The King's judges often subverted the provisions of the law. It was held that an individual may be held without bail upon the Sovereign's command. Eventually, the Petition of Right of 1628 argued that the King did not have such authority. Later, technicalities in the law were exploited to keep the accused imprisoned without bail even where the offenses were bailable; such loopholes were for the most part closed by the Habeas Corpus Act 1679. Thereafter, judges were compelled to set bail, but they often required impracticable amounts. Finally, the English Bill of Rights (1689) held that "excessive bail ought not to be required." Nevertheless, the Bill did not determine the distinction between bailable and non-bailable offenses. Thus, the Eighth Amendment has been interpreted to mean that bail may be denied if the charges are sufficiently serious. The Supreme Court has also permitted "preventive" detention without bail. In "United States v. Salerno", 481 U.S. (1987), the Supreme Court held that the only limitation imposed by the bail clause is that "the government's proposed conditions of release or detention not be 'excessive' in light of the perceived evil." In "Stack v. Boyle", 342 U.S. (1951), the Supreme Court declared that a bail amount is "excessive" under the Eighth Amendment if it were "a figure higher than is reasonably calculated" to ensure the defendant's appearance at trial.
Excessive fines.
Waters-Pierce Oil Co. v. Texas.
In "Waters-Pierce Oil Co. v. Texas", 212 U.S. (1909), the Supreme Court held that excessive fines are those which are "so grossly excessive as to amount to a deprivation of property without due process of law." The Court wrote in its syllabus:
The fixing of punishment for crime and penalties for unlawful acts is within the police power of the state, and this Court cannot interfere with state legislation in fixing fines, or judicial action in imposing them, unless so grossly excessive as to amount to deprivation of property without due process of law. Where a state antitrust law fixed penalties at $5,000 a day, and, after verdict of guilty for over 300 days, a defendant corporation was fined over $1,600,000, this Court will not hold that the fine is so excessive as to amount to deprivation of property without due process of law where it appears that the business was extensive and profitable during the period of violation, and that the corporation has over $40,000,000 of assets and has declared dividends amounting to several hundred percent
The Court further stated in its opinion:
[I]t is contended that the fines imposed are so excessive as to constitute a taking of the defendant's property without due process of law. It is not contended in this connection that the prohibition of the Eighth Amendment to the federal Constitution against excessive fines operates to control the legislation of the states. The fixing of punishment for crime or penalties for unlawful acts against its laws is within the police power of the state. We can only interfere with such legislation and judicial action of the states enforcing it if the fines imposed are so grossly excessive as to amount to a deprivation of property without due process of law.
In "Waters-Pierce Oil Co. v. Texas" the Supreme Court set up a standard for judging whether or not a fine is "excessive." The standard set up is that a fine must not be "so grossly excessive as to amount to deprivation of property without due process of law." In other words, the government must not be able to confiscate such a large amount of property without following an established set of rules created by the legislature.
Browning-Ferris v. Kelco.
In "Browning-Ferris Industries v. Kelco Disposal", 492 U.S. (1989), the Supreme Court ruled that the Excessive Fines Clause does not apply "when the government neither has prosecuted the action nor has any right to receive a share of the damages awarded." While punitive damages in civil cases are not covered by the Excessive Fines Clause, such damages were held to be covered by the Due Process Clause of the Fourteenth Amendment, notably in "State Farm v. Campbell", 538 U.S. 408 (2003).
United States v. Bajakajian.
In "United States v. Bajakajian", 524 U.S. (1998), the Supreme Court ruled that it was unconstitutional to confiscate $357,144 from Hosep Bajakajian who failed to report possession of over $10,000 while leaving the United States. In what was the first case in which the Supreme Court ruled a fine to violate the Excessive Fines Clause, the Court held that it was "grossly disproportional" to take all of the money which Mr. Bajakajian attempted to take out of the United States, in violation of a federal law that required him to report an amount in excess of $10,000. In describing what constituted "gross disproportionality," the Court could not find any guidance from the history of the Excessive Fines Clause and so relied on Cruel and Unusual Punishment Clause case law:We must therefore rely on other considerations in deriving a constitutional excessiveness standard, and there are two that we find particularly relevant. The first, which we have emphasized in our cases interpreting the Cruel and Unusual Punishments Clause, is that judgments about the appropriate punishment for an offense belong in the first instance to the legislature. See, e.g., Solem v. Helm, 463 U.S. 277, 290 (1983) (“Reviewing courts … should grant substantial deference to the broad authority that legislatures necessarily possess in determining the types and limits of punishments for crimes”); see also Gore v. United States, 357 U.S. 386, 393 (1958) (“Whatever views may be entertained regarding severity of punishment, … these are peculiarly questions of legislative policy”). The second is that any judicial determination regarding the gravity of a particular criminal offense will be inherently imprecise. Both of these principles counsel against requiring strict proportionality between the amount of a punitive forfeiture and the gravity of a criminal offense, and we therefore adopt the standard of gross disproportionality articulated in our Cruel and Unusual Punishments Clause precedents. See, e.g., Solem v. Helm, supra, at 288; Rummel v. Estelle, 445 U.S. 263, 271 (1980).
Thus the Court declared that, within the context of judicial deference to the legislature's power to set punishments, a fine would not offend the Eighth Amendment unless it were "grossly disproportional to the gravity of a defendant's offense." 
Cruel and unusual punishments.
According to the Supreme Court, the Eighth Amendment forbids some punishments entirely, and forbids some other punishments that are excessive when compared to the crime, or compared to the competence of the perpetrator.
In "Louisiana ex rel. Francis v. Resweber", 329 U.S. (1947), the Supreme Court assumed "arguendo" that the Cruel and Unusual Punishments Clause applied to the states through the Due Process Clause of the Fourteenth Amendment. In "Robinson v. California", 370 U.S. (1962), the Court ruled that it did apply to the states through the Fourteenth Amendment. "Robinson" was the first case in which the Supreme Court applied the Eighth Amendment against the state governments through the Fourteenth Amendment. Before "Robinson", the Eighth Amendment had only been applied against the federal government.
Justice Potter Stewart's opinion for the "Robinson" Court held that "infliction of cruel and unusual punishment [is] in violation of the Eighth and Fourteenth Amendments." The framers of the Fourteenth Amendment, such as John Bingham, had discussed this subject:[M]any instances of State injustice and oppression have already occurred in the State legislation of this Union, of flagrant violations of the guarantied privileges of citizens of the United States, for which the national Government furnished and could furnish by law no remedy whatever. Contrary to the express letter of your Constitution, "cruel and unusual punishments" have been inflicted under State laws within this Union upon citizens, not only for crimes committed, but for sacred duty done, for which and against which the Government of the United States had provided no remedy and could provide none.
In "Furman v. Georgia", 408 U.S. (1972), Justice Brennan wrote, "There are, then, four principles by which we may determine whether a particular punishment is 'cruel and unusual'."
Justice Brennan also wrote that he expected no state would pass a law obviously violating any one of these principles, so court decisions regarding the Eighth Amendment would involve a "cumulative" analysis of the implication of each of the four principles. In this way, the United States Supreme Court "set the standard that a punishment would be cruel and unusual [,if] it was too severe for the crime, [if] it was arbitrary, if it offended society's sense of justice, or if it was not more effective than a less severe penalty."
Punishments forbidden regardless of the crime.
In "Wilkerson v. Utah", 99 U.S. (1878), the Supreme Court commented that drawing and quartering, public dissection, burning alive, or disembowelment constituted cruel and unusual punishment. In "Thompson v. Oklahoma", 487 U.S. (1988), the Supreme Court ruled that the death penalty constituted cruel and unusual punishment if the defendant is under age 16 when the crime was committed. In "Atkins v. Virginia", 536 U.S. (2002), the Court declared that executing people who are mentally handicapped constituted cruel and unusual punishment. Likewise, in "Roper v. Simmons", 543 U.S. (2005), the Court barred the executing of people who were under age 18 when the crime was committed.
Punishments forbidden for certain crimes.
The case of "Weems v. United States", 217 U.S. (1910), marked the first time that the Supreme Court exercised judicial review to overturn a criminal sentence as cruel and unusual. The Court overturned a punishment called cadena temporal, which mandated "hard and painful labor," shackling for the duration of incarceration, and permanent civil disabilities. This case is often viewed as establishing a principle of proportionality under the Eighth Amendment. However, others have written that "it is hard to view "Weems" as announcing a constitutional requirement of proportionality."
In "Trop v. Dulles", 356 U.S. (1958), the Supreme Court held that punishing a natural-born citizen for a crime by revoking his citizenship is unconstitutional, being "more primitive than torture" because it involved the "total destruction of the individual's status in organized society."
In "Robinson v. California", 370 U.S. (1962), the Court decided that a California law authorizing a 90-day jail sentence for "be[ing] addicted to the use of narcotics" violated the Eighth Amendment, as narcotics addiction "is apparently an illness," and California was attempting to punish people based on the state of this illness, rather than for any specific act. The Court wrote:"To be sure, imprisonment for ninety days is not, in the abstract, a punishment which is either cruel or unusual. But the question cannot be considered in the abstract. Even one day in prison would be a cruel and unusual punishment for the 'crime' of having a common cold."
 However, in "Powell v. Texas", 392 U.S. (1968), the Court upheld a statute barring public intoxication by distinguishing "Robinson" on the basis that "Powell" dealt with a person who was drunk "in public", not merely for being addicted to alcohol.
Traditionally, the length of a prison sentence was not subject to scrutiny under the Eighth Amendment, regardless of the crime for which the sentence was imposed. It was not until the case of "Solem v. Helm", 463 U.S. (1983), that the Supreme Court held that incarceration, standing alone, could constitute cruel and unusual punishment if it were "disproportionate" in duration to the offense. The Court outlined three factors that were to be considered in determining if the sentence is excessive: "(i) the gravity of the offense and the harshness of the penalty; (ii) the sentences imposed on other criminals in the same jurisdiction; and (iii) the sentences imposed for commission of the same crime in other jurisdictions." The Court held that in the circumstances of the case before it and the factors to be considered, a sentence of life imprisonment without parole for cashing a $100 check on a closed account was cruel and unusual.
However, in "Harmelin v. Michigan", 501 U.S. (1991), a fractured Court retreated from the "Solem" test and held that for non-capital sentences, the Eighth Amendment only constrains the length of prison terms by a "gross disproportionality principle." Under this principle, the Court sustained a mandatory sentence of life without parole imposed for possession of 672 grams (1.5 pounds) or more of cocaine. The Court acknowledged that a punishment could be cruel but not unusual, and therefore not prohibited by the Constitution. Additionally, in "Harmelin", Justice Scalia, joined by Chief Justice Rehnquist, said "the Eighth Amendment contains no proportionality guarantee," and that "what was 'cruel and unusual' under the Eighth Amendment was to be determined without reference to the particular offense." Scalia wrote "If 'cruel and unusual punishments' included disproportionate punishments, the separate prohibition of disproportionate fines (which are certainly punishments) would have been entirely superfluous." Moreover, "There is little doubt that those who framed, proposed, and ratified the Bill of Rights were aware of such provisions [outlawing disproportional punishments], yet chose not to replicate them."
In "Graham v. Florida", 560 U.S. ___ (2010), the Supreme Court declared that a life sentence without any chance of parole, for a crime other than murder, is cruel and unusual punishment for a minor. Two years later, in "Miller v. Alabama", 567 U.S. ___ (2012), the Court went further, holding that mandatory life sentences without parole cannot be imposed on minors, even for homicide.
Death penalty for rape.
In "Coker v. Georgia", 433 U.S. (1977), the Court declared that the death penalty was unconstitutionally excessive for rape of a woman and, by implication, for any crime where a death does not occur. The majority in "Coker" stated that "death is indeed a disproportionate penalty for the crime of raping an adult woman." The dissent countered that the majority "takes too little account of the profound suffering the crime imposes upon the victims and their loved ones." The dissent also characterized the majority as "myopic" for only considering legal history of "the past five years".
In "Kennedy v. Louisiana", (2008), the Court extended the reasoning of "Coker" by ruling that the death penalty was excessive for child rape "where the victim’s life was not taken." The Supreme Court failed to note a federal law, which applies to military court-martial proceedings, providing for the death penalty in cases of child rape. On October 1, 2008, the Court declined to reconsider its opinion in this case, but did amend the majority and dissenting opinions in order to acknowledge that federal law. Justice Scalia (joined by Chief Justice Roberts) wrote in dissent that "the proposed Eighth Amendment would have been laughed to scorn if it had read 'no criminal penalty shall be imposed which the Supreme Court deems unacceptable.'"
Special procedures for death penalty cases.
The first significant general challenge to capital punishment that reached the Supreme Court was the case of "Furman v. Georgia", 408 U.S. (1972). In a 5–4 decision, the Supreme Court overturned the death sentences of Furman for murder, as well as two other defendants for rape. Of the five justices voting to overturn the death penalty, two found capital punishment to be unconstitutionally cruel and unusual, while three found that the statutes at issue were implemented in a random and capricious fashion, discriminating against blacks and the poor. "Furman v. Georgia" did not hold – even though it is sometimes claimed that it did – that capital punishment is "per se" unconstitutional.
States with capital punishment rewrote their laws to address the Supreme Court's decision, and the Court then revisited the issue in a murder case: "Gregg v. Georgia", 428 U.S. (1976). In "Gregg", the Court found, in a 7–2 ruling, that Georgia's new death penalty laws passed Eighth Amendment scrutiny: the statutes provided a bifurcated trial in which guilt and sentence were determined separately; and, the statutes provided for "specific jury findings" followed by state supreme court review comparing each death sentence "with the sentences imposed on similarly situated defendants to ensure that the sentence of death in a particular case is not disproportionate." Because of the "Gregg" decision, executions resumed in 1977.
Some states have passed laws imposing mandatory death penalties in certain cases. The Supreme Court found these laws to be unconstitutional under the Eighth Amendment, in the murder case of "Woodson v. North Carolina", 428 U.S. (1976), because these laws remove discretion from the trial judge to make an individualized determination in each case. Other statutes specifying factors for courts to use in making their decisions have been upheld. Some have not: in "Godfrey v. Georgia", 446 U.S. (1980), the Supreme Court overturned a sentence based upon a finding that a murder was "outrageously or wantonly vile, horrible, and inhuman," as it deemed that any murder may be reasonably characterized in this manner. Similarly, in "Maynard v. Cartwright", 486 U.S. (1988), the Court found that an "especially heinous, atrocious or cruel" standard in a homicide case was too vague. However, the meaning of this language depends on how lower courts interpret it. In "Walton v. Arizona", 497 U.S. (1990), the Court found that the phrase "especially heinous, cruel, or depraved" was not vague in a murder case, because the state supreme court had expounded on its meaning.
The Court has generally held that death penalty cases require extra procedural protections. As the Court said in "Herrera v. Collins", 506 U.S. (1993), which involved the murder of a police officer, "the Eighth Amendment requires increased reliability of the process..."
Punishments specifically allowed.
In "Wilkerson v. Utah", 99 U.S. (1878), the Court stated that death by firing squad was not cruel and unusual punishment under the Eighth Amendment.
In "Rummel v. Estelle", 445 U.S. (1980), the Court upheld a life sentence with the possibility of parole imposed per Texas's three strikes law for fraud crimes totaling $230. A few months after pleading guilty Rummel was released.
In "Harmelin v. Michigan", 501 U.S. (1991), the Court upheld a life sentence without the possibility of parole for possession of 672 grams (1.5 pounds) of cocaine.
In "Lockyer v. Andrade", 538 U.S. (2003), the Court upheld a 50 years to life sentence with the possibility of parole imposed under California's three strikes law when the defendant was convicted of shoplifting videotapes worth a total of about $150.
Evolving standards of decency.
In "Trop v. Dulles", 356 U.S. (1958), Chief Justice Earl Warren said: "The [Eighth] Amendment must draw its meaning from the evolving standards of decency that mark the progress of a maturing society." Subsequently, the Court has looked to societal developments, as well as looking to its own independent judgment, in determining what are those "evolving standards of decency". 
Originalists like Justice Antonin Scalia argue that societies may rot instead of maturing, and may decrease in virtue or wisdom instead of increasing. Thus, they say, the framers wanted the amendment to be understood as it was written and ratified, instead of morphing as times change, and in any event legislators are more competent than judges to take the pulse of the public as to changing standards of decency.
The "evolving standards" test is not without its scholarly critics as well. For example, Professor John Stinneford asserts that the "evolving standards" test misinterprets the Eighth Amendment:The Framers of the Bill of Rights understood the word “unusual” to mean “contrary to long usage.” Recognition of the word’s original meaning will precisely invert the “evolving standards of decency” test, and ask the Court to compare challenged punishments with the longstanding principles and precedents of the common law, rather than shifting and nebulous notions of “societal consensus” and contemporary “standards of decency.”
On the other hand, Dennis Baker defends the evolving standards of decency test as a way to advance the moral purpose of the Eighth Amendment and the Framer’s intent to ban all forms of unjust punishment.
Proportionality.
The Court has applied evolving standards not only to say what punishments are inherently cruel, but also to say what punishments that are not inherently cruel are nevertheless "grossly disproportionate" to the offense in question. An example can be seen in "Jackson v. Bishop" (8th Cir., 1968), an Eighth Circuit decision outlawing corporal punishment in the Arkansas prison system: "The scope of the Amendment is not static...[D]isproportion, both among punishments and between punishment and crime, is a factor to be considered..."
Professor Stinneford asserts that the Eighth Amendment forbids punishments that are very disproportionate to the offense, even if the punishment by itself is not intrinsically barbaric, but Stinneford argues that "proportionality is to be measured primarily in terms of prior practice" according to the word "unusual" in the amendment, instead of being measured according to shifting and nebulous evolving standards. Similarly, Professor John Bessler points to "An Essay on Crimes and Punishments," written by Cesare Beccaria in the 1760s, which advocated proportionate punishments; many of the Founding Fathers, including Thomas Jefferson and James Madison, read Beccaria's treatise and were influenced by it. 
Thus, Stinneford and Bessler disagree with the view of Justice Scalia, joined by Chief Justice Rehnquist, in "Harmelin v. Michigan" where they denied that the Punishments Clause contains any proportionality principle. With Scalia and Rehnquist, Richard Epstein argues that the amendment does not refer broadly to the imposition of penalties, but rather refers more narrowly to the penalties themselves; Epstein says that judges who favor the broad view tend to omit the letter "s" at the end of the word "punishments".

</doc>
<doc id="31661" url="http://en.wikipedia.org/wiki?curid=31661" title="Ninth Amendment to the United States Constitution">
Ninth Amendment to the United States Constitution

The Ninth Amendment (Amendment IX) to the United States Constitution, which is part of the Bill of Rights, addresses rights, retained by the people, that are not specifically enumerated in the Constitution.
Text.
The enumeration in the Constitution, of certain rights, shall not be construed to deny or disparage others retained by the people. 
Background before adoption.
When the U.S. Constitution was put to the states for ratification after being signed on September 17, 1787, the Anti-Federalists argued that a Bill of Rights should be added. One of the arguments the Federalists gave against the addition of a Bill of Rights, during the debates about ratification of the Constitution, was that a listing of rights could problematically enlarge the powers specified in of the new Constitution by implication. For example, in Federalist , Alexander Hamilton asked, "Why declare that things shall not be done which there is no power to do?" Likewise, James Madison explained to Thomas Jefferson, "I conceive that in a certain degree ... the rights in question are reserved by the manner in which the federal powers are granted" by Article One, Section 8 of the Constitution.
The Anti-Federalists persisted in favor of a Bill of Rights during the ratification debates, but also were against ratification, and consequently several of the state ratification conventions gave their assent with accompanying resolutions proposing amendments to be added. In 1788, the Virginia Ratifying Convention attempted to solve the problem that Hamilton and the Federalists had identified by proposing a constitutional amendment specifying:
That those clauses which declare that Congress shall not exercise certain powers be not interpreted in any manner whatsoever to extend the powers of Congress. But that they may be construed either as making exceptions to the specified powers where this shall be the case, or otherwise as inserted merely for greater caution.
This proposal ultimately led to the Ninth Amendment.
In 1789, while introducing to the House of Representatives nineteen draft Amendments, James Madison addressed what would become the Ninth Amendment as follows:
It has been objected also against a Bill of Rights, that, by enumerating particular exceptions to the grant of power, it would disparage those rights which were not placed in that enumeration; and it might follow by implication, that those rights which were not singled out, were intended to be assigned into the hands of the General Government, and were consequently insecure. This is one of the most plausible arguments I have ever heard against the admission of a bill of rights into this system; but, I conceive, that it may be guarded against. I have attempted it, as gentlemen may see by turning to the last clause of the fourth resolution.
Like Alexander Hamilton, Madison was concerned that enumerating various rights could "enlarge the powers delegated by the constitution." To attempt to solve this problem, Madison submitted this draft to Congress:
The exceptions here or elsewhere in the constitution, made in favor of particular rights, shall not be so construed as to diminish the just importance of other rights retained by the people; or as to enlarge the powers delegated by the constitution; but either as actual limitations of such powers, or as inserted merely for greater caution.
This was an intermediate form of the Ninth Amendment that borrowed language from the Virginia proposal, while foreshadowing the final version.
The final text of the Ninth Amendment, like Madison's draft, speaks of other rights than those enumerated in the Constitution. The character of those other rights was indicated by Madison in his speech introducing the Bill of Rights (emphasis added):
It has been said, by way of objection to a bill of rights...that in the Federal Government they are unnecessary, because the powers are enumerated, and it follows, that all that are not granted by the constitution are retained; that the constitution is a bill of powers, the great residuum being the rights of the people; and, therefore, a bill of rights cannot be so necessary as if the residuum was thrown into the hands of the Government. I admit that these arguments are not entirely without foundation, but they are not as conclusive to the extent it has been proposed. It is true the powers of the general government are circumscribed; they are directed to particular objects; but even if government keeps within those limits, it has certain discretionary powers with respect to the means, which may admit of abuse.
The First through Eighth Amendments address the means by which the federal government exercises its enumerated powers, while the Ninth Amendment addresses a "great residuum" of rights that have not been "thrown into the hands of the government," as Madison put it. The Ninth Amendment became part of the Constitution on December 15, 1791 upon ratification by three-fourths of the states.
Judicial interpretation.
The Ninth Amendment has generally been regarded by the courts as negating any expansion of governmental power on account of the enumeration of rights in the Constitution, but the Amendment has not been regarded as further limiting governmental power. The U.S. Supreme Court explained this, in "U.S. Public Workers v. Mitchell" 330 U.S. (1947): "If granted power is found, necessarily the objection of invasion of those rights, reserved by the Ninth and Tenth Amendments, must fail."
The Supreme Court held in "Barron v. Baltimore" (1833) that the Amendment was enforceable by the federal courts only against the federal government, and not against the states. Thus, the Ninth Amendment originally applied only to the federal government, which is a government of enumerated powers.
Some jurists have asserted that the Ninth Amendment is relevant to interpretation of the Fourteenth Amendment. Justice Arthur Goldberg (joined by Chief Justice Earl Warren and Justice William Brennan) expressed this view in a concurring opinion in the case of "Griswold v. Connecticut" (1965):
The Framers did not intend that the first eight amendments be construed to exhaust the basic and fundamental rights... I do not mean to imply that the ... Ninth Amendment constitutes an independent source of rights protected from infringement by either the States or the Federal Government...While the Ninth Amendment - and indeed the entire Bill of Rights - originally concerned restrictions upon federal power, the subsequently enacted Fourteenth Amendment prohibits the States as well from abridging fundamental personal liberties. And, the Ninth Amendment, in indicating that not all such liberties are specifically mentioned in the first eight amendments, is surely relevant in showing the existence of other fundamental personal rights, now protected from state, as well as federal, infringement. In sum, the Ninth Amendment simply lends strong support to the view that the "liberty" protected by the Fifth and Fourteenth Amendments from infringement by the Federal Government or the States is not restricted to rights specifically mentioned in the first eight amendments. Cf. United Public Workers v. Mitchell, 330 U.S. 75, 94-95.
Since "Griswold", some judges have tried to use the Ninth Amendment to justify judicially enforcing rights that are not enumerated. For example, the District Court that heard the case of "Roe v. Wade" ruled in favor of a "Ninth Amendment right to choose to have an abortion," although it stressed that the right was "not unqualified or unfettered." However, Justice William O. Douglas rejected that view; Douglas wrote that, "The Ninth Amendment obviously does not create federally enforceable rights." See "Doe v. Bolton" (1973). Douglas joined the majority opinion of the U.S. Supreme Court in "Roe", which stated that a federally enforceable right to privacy, "whether it be founded in the Fourteenth Amendment's concept of personal liberty and restrictions upon state action, as we feel it is, or, as the District Court determined, in the Ninth Amendment's reservation of rights to the people, is broad enough to encompass a woman's decision whether or not to terminate her pregnancy."
The Sixth Circuit Court of Appeals stated in "Gibson v. Matthews", 926 F.2d 532, 537 (6th Cir. 1991) that the Ninth Amendment was intended to vitiate the maxim of "expressio unius est exclusio alterius" according to which the express mention of one thing excludes all others:
[T]he ninth amendment does not confer substantive rights in addition to those conferred by other portions of our governing law. The ninth amendment was added to the Bill of Rights to ensure that the maxim expressio unius est exclusio alterius would not be used at a later time to deny fundamental rights merely because they were not specifically enumerated in the Constitution.
Justice Antonin Scalia has expressed the view, in the dissenting opinion of "Troxel v. Granville" 530 U.S. 57 (2000), that:
The Declaration of Independence...is not a legal prescription conferring powers upon the courts; and the Constitution’s refusal to 'deny or disparage' other rights is far removed from affirming any one of them, and even farther removed from authorizing judges to identify what they might be, and to enforce the judges’ list against laws duly enacted by the people.
Scholarly interpretation.
Professor Laurence Tribe shares the view that this amendment does not confer substantive rights: "It is a common error, but an error nonetheless, to talk of 'ninth amendment rights.' The ninth amendment is not a source of rights as such; it is simply a rule about how to read the Constitution."
In 2000, Harvard historian Bernard Bailyn gave a speech at the White House on the subject of the Ninth Amendment. He said that the Ninth Amendment refers to "a universe of rights, possessed by the people — latent rights, still to be evoked and enacted into law...a reservoir of other, unenumerated rights that the people retain, which in time may be enacted into law." Similarly, journalist Brian Doherty has argued that the Ninth Amendment "specifically roots the Constitution in a natural rights tradition that says we are born with more rights than any constitution could ever list or specify."
Robert Bork, often considered an originalist, stated during his Supreme Court confirmation hearing that a judge should not apply a constitutional provision like this one if he does not know what it means; the example Bork then gave was a clause covered by an inkblot. Upon further study, Bork later ascribed a meaning to the Ninth Amendment in his book "The Tempting of America". In that book, Bork subscribed to the interpretation of constitutional historian Russell Caplan, who asserted that this Amendment was meant to ensure that the federal Bill of Rights would not affect provisions in state law that restrain state governments.
A libertarian originalist, Randy Barnett, has argued that the Ninth Amendment requires what he calls a presumption of liberty. Barnett also argues that the Ninth Amendment prevents the government from invalidating a ruling by either a jury or lower court through strict interpretation of the Bill of Rights.
Still others, such as Thomas B. McAffee, have argued that the Ninth Amendment protects the unenumerated "residuum" of rights which the federal government was never empowered to violate.
According to lawyer and diplomat Frederic Jesup Stimson, the framers of the Constitution and the Ninth Amendment intended that no rights that they already held would be lost through omission. Law professor Charles Lund Black took a similar position, though Stimson and Black respectively acknowledged that their views differed from the modern view, and differed from the prevalent view in academic writing.
Gun rights activists in recent decades have sometimes argued for a fundamental natural right to keep and bear arms in the United States that both predates the U.S. Constitution and is covered by the Constitution's Ninth Amendment; according to this viewpoint, the Second Amendment only enumerates a pre-existing right to keep and bear arms.
Recapitulation.
The Ninth Amendment explicitly bars denial of unenumerated rights if the denial is based on the "enumeration of certain rights" in the Constitution, but this amendment does not explicitly bar denial of unenumerated rights if the denial is based on the "enumeration of certain powers" in the Constitution. It is to that enumeration of powers that the courts have pointed, in order to determine the extent of the unenumerated rights mentioned in the Ninth Amendment.

</doc>
<doc id="31662" url="http://en.wikipedia.org/wiki?curid=31662" title="Tenth Amendment to the United States Constitution">
Tenth Amendment to the United States Constitution

The Tenth Amendment (Amendment X) to the United States Constitution, which is part of the Bill of Rights, was ratified on December 15, 1791. It expresses the principle of federalism, which undergirds the entire plan of the original Constitution, by stating that the federal government possesses only those powers delegated to it by the Constitution. All remaining powers are reserved for the states or the people. In drafting this amendment, its framers had two purposes in mind: first, as a necessary rule of construction; and second, as a reaffirmation of the nature of the federal system.
Text.
The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.
Drafting and adoption.
The Tenth Amendment is similar to an earlier provision of the Articles of Confederation: "Each state retains its sovereignty, freedom, and independence, and every power, jurisdiction, and right, which is not by this Confederation expressly delegated to the United States, in Congress assembled." After the Constitution was ratified, South Carolina Representative Thomas Tudor Tucker and Massachusetts Representative Elbridge Gerry separately proposed similar amendments limiting the federal government to powers "expressly" delegated, which would have denied implied powers. James Madison opposed the amendments, stating that "it was impossible to confine a Government to the exercise of express powers; there must necessarily be admitted powers by implication, unless the Constitution descended to recount every minutia." The word "expressly" ultimately did not appear in the Tenth Amendment as ratified, and therefore the Tenth Amendment did not reject the powers implied by the Necessary and Proper Clause.
When he introduced the Tenth Amendment in Congress, James Madison explained that many states were eager to ratify this amendment, despite critics who deemed the amendment superfluous or unnecessary:
I find, from looking into the amendments proposed by the State conventions, that several are particularly anxious that it should be declared in the Constitution, that the powers not therein delegated should be reserved to the several States. Perhaps words which may define this more precisely than the whole of the instrument now does, may be considered as superfluous. I admit they may be deemed unnecessary: but there can be no harm in making such a declaration, if gentlemen will allow that the fact is as stated. I am sure I understand it so, and do therefore propose it.
The states decided to ratify the Tenth Amendment, and thus declined to signal that there are unenumerated powers in addition to unenumerated rights. The amendment rendered unambiguous what had previously been at most a mere suggestion or implication.
The phrase "..., or to the people." was appended in handwriting by the clerk of the Senate as the Bill of Rights circulated between the two Houses of Congress.
Judicial interpretation.
The Tenth Amendment, which makes explicit the idea that the federal government is limited to only the powers granted in the Constitution, has been declared to be a truism by the Supreme Court. In "United States v. Sprague" (1931) the Supreme Court asserted that the amendment "added nothing to the [Constitution] as originally ratified."
States and local governments have occasionally attempted to assert exemption from various federal regulations, especially in the areas of labor and environmental controls, using the Tenth Amendment as a basis for their claim. An often-repeated quote, from "United States v. Darby Lumber", 312 U.S. 100, 124 (1941), reads as follows:
The amendment states but a truism that all is retained which has not been surrendered. There is nothing in the history of its adoption to suggest that it was more than declaratory of the relationship between the national and state governments as it had been established by the Constitution before the amendment or that its purpose was other than to allay fears that the new national government might seek to exercise powers not granted, and that the states might not be able to exercise fully their reserved powers.
Forced participation or commandeering.
The Supreme Court rarely declares laws unconstitutional for violating the Tenth Amendment. In the modern era, the Court has only done so where the federal government compels the states to enforce federal statutes. In 1992, in "New York v. United States", 505 U.S. 144 (1992), for only the second time in 55 years, the Supreme Court invalidated a portion of a federal law for violating the Tenth Amendment. The case challenged a portion of the Low-Level Radioactive Waste Policy Amendments Act of 1985. The act provided three incentives for states to comply with statutory obligations to provide for the disposal of low-level radioactive waste. The first two incentives were monetary. The third, which was challenged in the case, obliged states to take title to any waste within their borders that was not disposed of prior to January 1, 1996, and made each state liable for all damages directly related to the waste. The Court, in a 6–3 decision, ruled that the imposition of that obligation on the states violated the Tenth Amendment. Justice Sandra Day O'Connor wrote that the federal government can encourage the states to adopt certain regulations through the spending power (e.g. by attaching conditions to the receipt of federal funds, see "South Dakota v. Dole", 483 U.S. 203 (1987), or through the commerce power (by directly pre-empting state law). However, Congress cannot directly compel states to enforce federal regulations.
In 1998, the Court again ruled that the Brady Handgun Violence Prevention Act violated the Tenth Amendment ("Printz v. United States", 521 U.S. 898 (1997)). The act required state and local law enforcement officials to conduct background checks on persons attempting to purchase handguns. Justice Antonin Scalia, writing for the majority, applied "New York v. United States" to show that the law violated the Tenth Amendment. Since the act "forced participation of the State's executive in the actual administration of a federal program", it was unconstitutional.
Commerce clause.
In modern times, the Commerce Clause has become one of the most frequently-used sources of Congress's power, and thus its interpretation is very important in determining the allowable scope of federal government.
In the 20th century, complex economic challenges arising from the Great Depression triggered a reevaluation in both Congress and the Supreme Court of the use of Commerce Clause powers to maintain a strong national economy.
In "Wickard v. Filburn" (1942), in the context of World War II, the Court ruled that federal regulations of wheat production could constitutionally be applied to wheat grown for "home consumption" on a farm – that is, wheat grown to be fed to farm animals or otherwise consumed on the farm. The rationale was that a farmer's growing "his own wheat" can have a substantial cumulative effect on interstate commerce, because if all farmers were to exceed their production quotas, a significant amount of wheat would either not be sold on the market or would be bought from other producers. Hence, in the aggregate, if farmers were allowed to consume their own wheat, it would affect the interstate market in wheat.
In "Garcia v. San Antonio Metropolitan Transit Authority" (1985), the Court changed the analytic framework to be applied in Tenth Amendment cases. Prior to the Garcia decision, the determination of whether there was state immunity from federal regulation turned on whether the state activity was "traditional" for or "integral" to the state government. The Court noted that this analysis was "unsound in principle and unworkable in practice", and rejected it without providing a replacement. The Court's holding declined to set any formula to provide guidance in future cases. Instead, it simply held "...we need go no further than to state that we perceive nothing in the overtime and minimum-wage requirements of the FLSA ... that is destructive of state sovereignty or violative of any constitutional provision." It left to future courts how best to determine when a particular federal regulation may be "destructive of state sovereignty or violative of any constitutional provision."
In "United States v. Lopez" 514 U.S. (1995), a federal law mandating a "gun-free zone" on and around public school campuses was struck down because, the Supreme Court ruled, there was no clause in the Constitution authorizing it. This was the first modern Supreme Court opinion to limit the government's power under the Commerce Clause. The opinion did not mention the Tenth Amendment, and the Court's 1985 "Garcia" opinion remains the controlling authority on that subject.
Most recently, the Commerce Clause was cited in the 2005 decision Gonzales v. Raich. In this case, a California woman sued the Drug Enforcement Administration after her medical cannabis crop was seized and destroyed by federal agents. Medical cannabis was explicitly made legal under California state law by Proposition 215; however, cannabis is prohibited at the federal level by the Controlled Substances Act. Even though the woman grew cannabis strictly for her own consumption and never sold any, the Supreme Court stated that growing one's own cannabis "affects" the interstate market of cannabis. The theory was that the cannabis "could" enter the stream of interstate commerce, even if it clearly wasn't grown for that purpose and that was unlikely ever to happen (the same reasoning as in the "Wickard v. Filburn" decision). It therefore ruled that this practice may be regulated by the federal government under the authority of the Commerce Clause.
Federal funding.
The federal system limits the ability of the federal government to use state governments as an instrument of the national government, as held in "Printz v. United States", 521 U.S. 898 (1997). However, where Congress or the Executive has the power to implement programs, or otherwise regulate, there are, arguably, certain incentives in the national government encouraging States to become the instruments of such national policy, rather than to implement the program directly. One incentive is that state implementation of national programs places implementation in the hands of local officials who are closer to local circumstances. Another incentive is that implementation of federal programs at the state level would in principle limit the growth of the national bureaucracy.
For this reason, Congress often seeks to exercise its powers by offering or encouraging States to implement national programs consistent with national minimum standards; a system known as cooperative federalism. One example of the exercise of this device was to condition allocation of federal funding where certain state laws do not conform to federal guidelines. For example, federal educational funds may not be accepted without implementation of special education programs in compliance with IDEA. Similarly, the nationwide state 55 mph (90 km/h) speed limit, .08 legal blood alcohol limit, and the nationwide state 21-year drinking age were imposed through this method; the states would lose highway funding if they refused to pass such laws (though the national speed limit has since been repealed). See e.g. "South Dakota v. Dole", 483 U.S. 203 (1987).
State legislative actions in protest of federal actions.
Several states have introduced various resolutions and legislation in protest to federal actions. Despite this, the Supreme Court has explicitly rejected the idea that the states can nullify federal law. In "Cooper v. Aaron" (1958), the Supreme Court of the United States held that federal law prevails over state law due to the operation of the Supremacy Clause, and that federal law "can neither be nullified openly and directly by state legislators or state executive or judicial officers nor nullified indirectly by them through evasive schemes . . ." Thus, state laws purporting to nullify federal statutes or to exempt states and their citizens from federal statutes have only symbolic impact.
State sovereignty resolutions (10th Amendment resolutions).
These resolutions attempt to reassert state sovereignty over any area not listed among the "enumerated powers" (i.e., any law based on an "expansive reading" of the Commerce Clause, the Necessary and Proper Clause, or the Supremacy Clause would, according to this resolution, be invalid).
State sovereignty bills (10th Amendment Bills).
A "State Sovereignty Bill" is one step beyond a State Sovereignty Resolution. The bill would mandate action against what the state legislature perceives as unconstitutional federal legislation.
Firearms freedom legislation and federal gun laws nullification.
Starting in 2012, in response to a threat of law made through executive orders by President Obama, more than a dozen states around the US began proposing legislation that would "...declare that any firearms made and retained in-state are beyond the authority of Congress under its constitutional power to regulate commerce among the states". The legislation would require that the firearm be prominently marked as being "Made in {name of state}" and further prohibit federal regulation solely on the basis that "basic materials" and "generic and insignificant parts" of the firearm may have their origins from outside the state.
Cannabis laws.
s of 2014[ [update]], 24 states (Alaska, Arizona, California, Colorado, Hawaii, Massachusetts, Maine, Michigan, Minnesota, Montana, Nevada, New Jersey, New Mexico, New York, Oregon, Rhode Island, Vermont, and Washington) and the District of Columbia have passed legislation which permit the use of medical cannabis. California Proposition 19 would have gone one step further, and legalized cannabis use by persons over age 21 for any purpose whatsoever; however, the state constitutional amendment did not pass.
In 2012, both Washington and Colorado legalized the recreational use of cannabis. Both states previously legalized the use of medical cannabis. The ballot measure allows for anyone over 21 to possess up to an ounce from licensed vendors. Colorado also allows citizens to grow up to 6 plants.
The Obama administration announced in October 2009 that it advised federal prosecutors not to target legally-operating medicinal cannabis users, or their suppliers, in states that have passed such laws. However, in the same year, the DEA conducted a record number of medical cannabis raids.
The DEA has continued to raid federally prohibited medical facilities in several states, and the DOJ has gone so far as to prevent such cases from being well publicized.
The IRS has also attempted to prevent the sale of medical cannabis in California by refusing to treat normally-deductible business expenses as such for dispensaries, notably for the Harborside Health Center in Oakland.
REAL ID Act.
s of 2010[ [update]], 25 states (beginning with Maine in 2007) have passed legislation and/or resolutions which opposed the REAL ID Act. Though the legislation is still on the books, its implementation has been delayed on several occasions and is currently not being enforced.
National health care nullification.
s of 2010[ [update]], legislators in 30 states had introduced legislation which would declare certain provisions of any proposed national health care bill to be null and void within the state; the legislation passed in Arizona, Idaho, Utah, and Virginia. Such provisions include mandatory participation in such a system as well as preserving the right of a patient to pay a health care professional for treatment (and for the professional to accept it) outside of a single-payer system. Arizona's legislation passed as a proposed constitutional amendment, approved by voters in 2010. On February 1, 2010, the Virginia Senate took a stand against a key provision of a proposed federal health care overhaul, passing legislation declaring that Virginia residents cannot be forced to buy health insurance. On March 17, 2010, Idaho Governor C.L. "Butch" Otter signed a bill requiring the Attorney General to sue the federal government if Idaho residents are required to buy health insurance.
Bring the Guard home.
s of 2010[ [update]], legislators in seven states had introduced legislation which would permit the governor of the state to recall any National Guard troops from overseas deployments (such as in Iraq and Afghanistan); the bills failed in Maryland and New Mexico.
Legal tender.
s of 2010[ [update]], legislators in seven states have introduced legislation which would seek to nullify federal legal tender laws in the state by authorizing payment in gold and silver or a paper note backed 100% by gold or silver; the legislation failed in Colorado and Montana.
Cap-and-trade nullification.
s of 2010[ [update]], legislators in four states have introduced legislation which would nullify any proposed federal emissions regulation under the "cap-and-trade" model; none have advanced beyond the introductory stage.
State sovereignty and federal tax funds acts.
s of 2010[ [update]], legislators in three states have introduced legislation which would require businesses (and in some cases, individuals) to remit their federal tax payments to the state treasurer (or equivalent body) for deposit into an escrow fund. If the state legislature determined that a portion of the federal budget was not constitutional, or if the federal government imposed penalties or sanctions upon the state for creating the fund, then the money would be withheld. None have advanced beyond the introductory stage.
Sheriffs first legislation.
s of 2010[ [update]], legislators in three states had introduced legislation which would make it a crime for any federal agent to make an arrest, search or seizure within the state without getting the advance, written permission of the sheriff of the county in which the event would take place. The bills would provide for the following exceptions:
None have advanced beyond the introductory stage.
Federal land legislation.
s of 2010[ [update]], legislators in Utah have introduced legislation to allow the use of eminent domain on federal land. Rep. Christopher Herrod has introduced the bill in a state where the federal government controls over 60% of the land. The effort has the full support of Republican Attorney General Mark Shurtleff, who would have to defend the law. The proposal includes setting aside $3 million for legal defense.
Nullification of federal intrastate commerce regulation.
s of 2010[ [update]], legislators in four states had introduced legislation which would nullify federal regulation of trade and activities which are solely within the boundaries of a state and which do not cross state lines; that is activities that are by definition NOT COMMERCE, under the Constitution. The Virginia legislation has passed one house.
Sanctuary city.
Another form of protest against enforcement of immigration laws, several United States cities have declared themselves sanctuary cities, whereby they have ordered the local police department to specifically not work with United States Customs and Border Protection officials to arrest persons illegally residing within the boundaries of the city, and not to inquire as to a person's immigration status, even if the person was arrested.
Intrastate Coal and Use Act.
In protest of the Environmental Protection Agency allegedly overstepping its authority by interfering with intrastate commerce, the West Virginia Intrastate Coal and Use Act (H.B. 2554) was being introduced into the West Virginia House of Delegates by Delegate Gary Howell. The bill states that coal sold and used within the borders of West Virginia is not subject to EPA authority because no interstate commerce exists and the state retains the rights to control its own intrastate commerce under the 10th Amendment. The American Legislative Exchange Council (ALEC) recommends the Intrastate Coal and Use Act for model legislation in other states.

</doc>
<doc id="31663" url="http://en.wikipedia.org/wiki?curid=31663" title="Eleventh Amendment to the United States Constitution">
Eleventh Amendment to the United States Constitution

The Eleventh Amendment (Amendment XI) to the United States Constitution, which was passed by the Congress on March 4, 1794, and ratified by the states on February 7, 1795, deals with each state's sovereign immunity and was adopted in order to overrule the U.S. Supreme Court's decision in "Chisholm v. Georgia", 2 U.S. (1793).
Text.
The Judicial power of the United States shall not be construed to extend to any suit in law or equity, commenced or prosecuted against one of the United States by Citizens of another State, or by Citizens or Subjects of any Foreign State.
Background.
The Eleventh Amendment was the first Constitutional amendment adopted after the Bill of Rights. The amendment was adopted following the Supreme Court's ruling in "Chisholm v. Georgia", 2 U.S. (1793). In "Chisholm", the Court ruled that federal courts had the authority to hear cases in law and equity brought by private citizens against states and that states did not enjoy sovereign immunity from suits made by citizens of other states in federal court. Thus, the amendment clarified Article III, Section 2 of the Constitution, which gave diversity jurisdiction to the judiciary to hear cases "between a state and citizens of another state."
Proposal and ratification.
The Eleventh Amendment was proposed by the 3rd Congress on March 4, 1794, when it was approved by the House of Representatives by vote of 81 – 9, having been previously passed by the Senate, 23 – 2, on January 14, 1794. The amendment was ratified by the state legislatures of the following states:<br> 
Since there were 15 States in the Union at the time, the ratification by twelve States added the Eleventh Amendment to the Constitution. It was subsequently ratified by:
On January 8, 1798, approximately three years after the Eleventh Amendment's adoption, President John Adams stated in a message to Congress that the Eleventh Amendment had been ratified by the necessary number of States and that it was now a part of the Constitution of the United States. New Jersey and Pennsylvania did not take action on the amendment; neither did Tennessee, which had become a State on June 16, 1796.
Impact.
In "Hollingsworth v. Virginia", 3 U.S. (1798), the Supreme Court held that every pending action brought under "Chisholm" had to be dismissed because of the amendment's adoption.
The amendment's text does not mention suits brought against a state by its own citizens. However, in "Hans v. Louisiana," 134 U.S. (1890), the Supreme Court ruled that the amendment reflects a broader principle of sovereign immunity. As Justice Anthony Kennedy, writing for a five Justice majority, stated in "Alden v. Maine", 527 U.S. (1999):[S]overeign immunity derives not from the Eleventh Amendment but from the structure of the original Constitution itself...Nor can we conclude that the specific Article I powers delegated to Congress necessarily include, by virtue of the Necessary and Proper Clause or otherwise, the incidental authority to subject the States to private suits as a means of achieving objectives otherwise within the scope of the enumerated powers.
Writing for a four-Justice dissent in "Alden", Justice David Souter said the states surrendered their sovereign immunity when they ratified the Constitution. The dissenting justices read the amendment's text as reflecting a narrow form of sovereign immunity that limited only the diversity jurisdiction of the federal courts. They concluded that the states are not insulated from suits by individuals by either the Eleventh Amendment in particular or the Constitution in general.
Although the Eleventh Amendment grants immunity to states from suit for money damages or equitable relief without their consent, in "Ex parte Young", 209 U.S. (1908), the Supreme Court ruled that federal courts may enjoin state officials from violating federal law. In "Fitzpatrick v. Bitzer", 427 U.S. (1976), the Supreme Court ruled that Congress may abrogate state immunity from suit under Section 5 of the Fourteenth Amendment. In "Central Virginia Community College v. Katz", 546 U.S. (2006), the Court ruled the Congress could do the same regarding bankruptcy cases by way of of the Constitution. In "Lapides v. Board of Regents of Univ. System of Ga.", 535 U.S. (2002), the Supreme Court ruled that when a state invokes a federal court's removal jurisdiction, it waives the Eleventh Amendment in the removed case.
The United States Court of Appeals for the First Circuit has ruled that Puerto Rico enjoys Eleventh Amendment immunity.

</doc>
<doc id="31664" url="http://en.wikipedia.org/wiki?curid=31664" title="Twelfth Amendment to the United States Constitution">
Twelfth Amendment to the United States Constitution

The Twelfth Amendment (Amendment XII) to the United States Constitution provides the procedure for electing the President and Vice President. It replaced , which provided the original procedure by which the Electoral College functioned. Problems with the original procedure arose in the elections of 1796 and 1800. The Twelfth Amendment refined the process whereby a President and a Vice President are elected by the Electoral College. The amendment was proposed by the Congress on December 9, 1803, and was ratified by the requisite three–fourths of state legislatures on June 15, 1804.
Text.
The Electors shall meet in their respective states, and vote by ballot for President and Vice-President, one of whom, at least, shall not be an inhabitant of the same state with themselves; they shall name in their ballots the person voted for as President, and in distinct ballots the person voted for as Vice-President, and they shall make distinct lists of all persons voted for as President, and all persons voted for as Vice-President and of the number of votes for each, which lists they shall sign and certify, and transmit sealed to the seat of the government of the United States, directed to the President of the Senate.
The President of the Senate shall, in the presence of the Senate and House of Representatives, open all the certificates and the votes shall then be counted.
The person having the greatest Number of votes for President, shall be the President, if such number be a majority of the whole number of Electors appointed; and if no person have such majority, then from the persons having the highest numbers not exceeding three on the list of those voted for as President, the House of Representatives shall choose immediately, by ballot, the President. But in choosing the President, the votes shall be taken by states, the representation from each state having one vote; a quorum for this purpose shall consist of a member or members from two-thirds of the states, and a majority of all the states shall be necessary to a choice. "And if the House of Representatives shall not choose a President whenever the right of choice shall devolve upon them, before the fourth day of March next following, then the Vice-President shall act as President, as in the case of the death or other constitutional disability of the President".
The person having the greatest number of votes as Vice-President, shall be the Vice-President, if such number be a majority of the whole number of Electors appointed, and if no person have a majority, then from the two highest numbers on the list, the Senate shall choose the Vice-President; a quorum for the purpose shall consist of two-thirds of the whole number of Senators, and a majority of the whole number shall be necessary to a choice. But no person constitutionally ineligible to the office of President shall be eligible to that of Vice-President of the United States.
Background.
Under the original procedure for the Electoral College, as provided in , each elector could vote for two persons. The two people chosen by the elector could not both inhabit the same state as that elector. This prohibition was designed to keep electors from voting for two "favorite sons" of their respective states. The person receiving the greatest number of votes, provided that number equaled a majority of the electors, was elected President.
If there was more than one individual who received the same number of votes, and such number equaled a majority of the electors, the House of Representatives would choose one of them to be President. If no individual had a majority, then the House of Representatives would choose from the five individuals with the greatest number of electoral votes. In either case, a majority of state delegations in the House was necessary for a candidate to be chosen to be President.
Selecting the Vice President was a simpler process. Whichever candidate received the greatest number of votes, except for the one elected President, became Vice President. The Vice President, unlike the President, did not require the votes of a majority of electors. In the event of a tie for second place among multiple candidates, the Senate would choose one of them to be Vice President, with each Senator casting one vote. It was not specified in the Constitution whether the sitting Vice President could cast a tie-breaking vote for Vice President under the original formula.
Problems developed almost immediately. In the 1796 election, John Adams, the Federalist Party presidential candidate, received a majority of the electoral votes. However, the Federalist electors scattered their second votes, resulting in the Democratic-Republican Party presidential candidate, Thomas Jefferson, receiving the second highest number of electoral votes and thus being elected Vice President. On January 6, 1797, Representative William L. Smith of South Carolina responded to this result by presenting a resolution on the floor of the House of Representatives for an amendment to the Constitution requiring each elector to cast one vote for President and another for Vice President. However, no action was taken on his proposal, setting the stage for the deadlocked election of 1800.
The 1800 election exposed a defect in the original formula in that if each member of the Electoral College followed party tickets, there would be a tie between the two candidates from the most popular ticket. The emergence of partisan political activity caused the failure of the original constitutional plan.
Additionally, it was becoming increasingly apparent that a situation in which the Vice President had been a defeated electoral opponent of the President would impede the ability of the two to effectively work together. By having the Presidential and Vice Presidential elected on a party ticket, this possibility would be eliminated, or at least minimized.
Proposal and ratification.
The Twelfth Amendment was proposed by the 8th Congress on December 9, 1803, when it was approved by the House of Representatives by vote of 83–42, having been previously passed by the Senate, 22–10, on December 2. The amendment was officially submitted to the states on December 12, 1803, and was ratified by the legislatures of the following states:<br> 
Having been ratified by the requisite three–fourths of the several states, there being 17 states in the Union at the time, the ratification of the Twelfth Amendment was completed. It was subsequently ratified by:
The amendment was rejected by:
On September 25, 1804, in a circular letter to the governors of the states, Secretary of State Madison declared the amendment ratified by three-fourths of the states.
Electoral College under the Twelfth Amendment.
While the Twelfth Amendment did not change the composition of the Electoral College or the duties of the electors, it did change the process whereby a President and a Vice President are elected. The new electoral process was first used for the 1804 election. Each presidential election since has been conducted under the terms of the Twelfth Amendment.
The Twelfth Amendment stipulates that each elector must cast distinct votes for President and Vice President, instead of two votes for President. Additionally, electors may not vote for presidential and vice-presidential candidates who both reside in the elector's state—at least one of them must be an inhabitant of another state. 
The Twelfth Amendment explicitly states that the Vice President must meet the same constitutional requirements as provided for the President. A majority of electoral votes is still required for a person to be elected President or Vice President by the Electoral College.
If no candidate for President has a majority of the total votes, the House of Representatives, voting by states and with the same quorum requirements as under the original procedure, chooses the President. The Twelfth Amendment requires the House to choose from the three highest receivers of electoral votes, compared to five under the original procedure.
If no candidate for Vice President has a majority of the total votes, the Senate, with each Senator having one vote, chooses the Vice President. The Twelfth Amendment requires the Senate to choose between the candidates with the "two highest numbers" of electoral votes. If multiple individuals are tied for second place, the Senate may consider all of them, in addition to the individual with the greatest number of votes. The Twelfth Amendment introduced a quorum requirement of two-thirds of the whole number of Senators for the conduct of balloting. Furthermore, the Twelfth Amendment provides that the votes of "a majority of the whole number" of Senators are required to arrive at a choice.
In order to prevent deadlocks from keeping the nation leaderless, the Twelfth Amendment provided that if the House could not choose a President before March 4 (then the first day of a Presidential term), the individual elected Vice President would act as President, "as in the case of the death or other constitutional disability of the President." The Twelfth Amendment did not state for how long the Vice President would act as President or if the House could still choose a President after March 4. Section 3 of the Twentieth Amendment, adopted in 1933, modified that provision of the Twelfth Amendment by changing the date upon which a new presidential term commences to January 20, clarifying that the Vice President-elect would only "act as President" if the House has not chosen a President by January 20, and permitting the Congress to statutorily provide "who shall then act as President" if there is no President-elect or Vice President-elect by January 20. It also clarifies that if there is no President-elect on January 20, whoever acts as President does so until a person "qualified" to occupy the Presidency is elected to be President.
There is a point of contention regarding the interpretation of the Twelfth Amendment as it relates to the Twenty-second Amendment, ratified in 1951, which prohibits a person from being "elected" President more than twice.
While it is clear that under the Twelfth Amendment the original constitutional qualifications of age, citizenship, and residency apply to both the President and Vice President, it is unclear whether a two-term president could later serve as Vice President. Some argue that the Twenty-second Amendment and Twelfth Amendment both bar any two-term president from later serving as Vice President as well as from succeeding to the presidency from any point in the United States presidential line of succession. Others contend that the Twelfth Amendment concerns qualification for "service", while the Twenty-second Amendment concerns qualifications for "election", and thus a former two-term president is still eligible to "serve" as vice president. The practical applicability of this distinction has not been tested, as no former president has ever sought the vice presidency, and thus the courts have never been required to make a judgment regarding the matter.
Elections since 1804.
Starting with the election of 1804, each Presidential election has been conducted under the Twelfth Amendment.
Only once since then has the House of Representatives chosen the President: In 1824, Andrew Jackson received 99 electoral votes, John Quincy Adams (son of John Adams) 84, William H. Crawford 41 and Henry Clay 37. All of the candidates were members of the Democratic-Republican Party (though there were significant political differences among them), and each had fallen short of the 131 votes necessary to win. Because the House could only consider the top three candidates, Clay could not become President. Crawford's poor health following a stroke made his election by the House unlikely. Andrew Jackson expected the House to vote for him, as he had won a plurality of the popular and electoral votes. Instead, the House elected Adams on the first ballot with thirteen states, followed by Jackson with seven and Crawford with four. Clay had endorsed Adams for the Presidency; the endorsement carried additional weight because Clay was the Speaker of the House. When Adams later appointed Clay his Secretary of State, many—particularly Jackson and his supporters—accused the pair of making a "Corrupt Bargain". In the less contested election for vice president, John C. Calhoun received 182 votes and was elected outright.
In 1836, the Whig Party nominated different candidates in different regions in the hopes of splintering the electoral vote and denying Martin Van Buren, the Democratic candidate, a majority in the Electoral College, thereby throwing the election into the Whig-controlled House. However, this strategy failed with Van Buren winning majorities of both the popular and electoral vote. In that same election no candidate for Vice President secured a majority in the electoral college as Democratic Vice Presidential nominee Richard Mentor Johnson did not receive the electoral votes of Democratic electors from Virginia, because of his relationship with a former slave. As a result Johnson received 147 electoral votes, one vote short of a majority; to be followed by Francis Granger with 77, John Tyler with 47 and William Smith with 23. This caused the Senate to choose whether Johnson or Granger would be the new Vice President. Johnson won with 33 votes, with Granger receiving 16.
Since 1836, no major U.S. party has nominated multiple regional presidential or vice presidential candidates in an election. However, since the Civil War there have been two serious attempts by Southern-based parties to run regional candidates in hopes of denying either of the two major candidates an electoral college majority. Both attempts (in 1948 and 1968) failed, but not by much—in both cases a shift in the result of two close states would have forced the respective elections into the House.
In modern elections, a running mate is often selected in order to appeal to a different set of voters. A Habitation Clause issue arose during the 2000 presidential election contested by George W. Bush (running-mate Dick Cheney) and Al Gore (running-mate Joe Lieberman), because it was alleged that Bush and Cheney were both inhabitants of Texas and that the Texas electors therefore violated the Twelfth Amendment in casting their ballots for both. Bush's residency was unquestioned, as he was Governor of Texas at the time. Cheney and his wife had moved to Dallas five years earlier when he assumed the role of chief executive at Halliburton. Cheney grew up in Wyoming and had represented it in Congress. A few months before the election, he switched his voter registration and driver's license to Wyoming and put his home in Dallas up for sale. Three Texas voters challenged the election in a federal court in Dallas and then appealed the decision to the United States Court of Appeals for the Fifth Circuit, where it was dismissed.

</doc>
<doc id="31665" url="http://en.wikipedia.org/wiki?curid=31665" title="Thirteenth Amendment to the United States Constitution">
Thirteenth Amendment to the United States Constitution

The Thirteenth Amendment to the United States Constitution abolished slavery and involuntary servitude, except as punishment for a crime. In Congress, it was passed by the Senate on April 8, 1864, and by the House on January 31, 1865. The amendment was ratified by the required number of states on December 6, 1865. On December 18, 1865, Secretary of State William H. Seward proclaimed its adoption. It was the first of the three Reconstruction Amendments adopted following the American Civil War.
Slavery had been tacitly protected in the original Constitution through clauses such as the Three-Fifths Compromise, by which three-fifths of the slave population was counted for representation in the United States House of Representatives. Though many slaves had been declared free by President Abraham Lincoln's 1863 Emancipation Proclamation, their post-war status was uncertain. On April 8, 1864, the Senate passed an amendment to abolish slavery. After one unsuccessful vote and extensive legislative maneuvering by the Lincoln administration, the House followed suit on January 31, 1865. The measure was swiftly ratified by nearly all Northern states, along with a sufficient number of border and "reconstructed" Southern states, to cause it to be adopted before the end of the year.
Though the amendment formally abolished slavery throughout the United States, factors such as Black Codes, white supremacist violence, and selective enforcement of statutes continued to subject some black Americans to involuntary labor, particularly in the South. In contrast to the other Reconstruction Amendments, the Thirteenth Amendment was rarely cited in later case law, but has been used to strike down peonage and some race-based discrimination as "badges and incidents of slavery". While the Fourteenth and Fifteenth Amendments apply only to state actors, the Thirteenth applies also to private citizens. The amendment also enables Congress to pass laws against sex trafficking and other modern forms of slavery.
Text.
Section 1. Neither slavery nor involuntary servitude, except as a punishment for crime whereof the party shall have been duly convicted, shall exist within the United States, or any place subject to their jurisdiction.
Section 2. Congress shall have power to enforce this article by appropriate legislation.
Slavery in the United States.
The institution of slavery existed in all of the original thirteen British North American colonies. Prior to the Thirteenth Amendment, the United States Constitution (adopted in 1789) did not expressly use the words "slave" or "slavery" but included several provisions about unfree persons. The Three-Fifths Clause (in Article I, Section 2) allocated Congressional representation based "on the whole Number of free Persons" and "three fifths of all other Persons". This clause was a compromise between Southerners who wished slaves to be counted as 'persons' for congressional representation and northerners rejecting these out of concern of too much power for the South, because representation in the new Congress would be based on population in contrast to the one-vote-for-one-state principle in the earlier Continental Congress. Under the Fugitive Slave Clause (Article IV, Section 2), "No person held to Service or Labour in one State" would be freed by escaping to another. allowed Congress to pass legislation outlawing the "Importation of Persons", but not until 1808. However, for purposes of the Fifth Amendment—which states that, "No person shall... be deprived of life, liberty, or property, without due process of law"—slaves were understood as property. Although abolitionists used the Fifth Amendment to argue against slavery, it became part of the legal basis for treating slaves as property with "Dred Scott v. Sandford" (1857).
Stimulated by the philosophy of the Declaration of Independence between 1777 and 1804, every Northern state provided for the immediate or gradual abolition of slavery. Most of the slaves involved were household servants. No Southern state did so, and the slave population of the South continued to grow, peaking at almost 4 million people in 1861. An abolitionist movement headed by such figures as William Lloyd Garrison grew in strength in the North, calling for the end of slavery nationwide and exacerbating tensions between North and South. The American Colonization Society, an alliance between abolitionists who felt the races should be kept separated and slaveholders who feared the presence of freed blacks would encourage slave rebellions, called for the emigration and colonization of both free blacks and slaves to Africa. Its views were endorsed by politicians such as Henry Clay, who feared that the main abolitionist movement would provoke a civil war. Proposals to eliminate slavery by constitutional amendment were introduced by Representative Arthur Livermore in 1818 and by John Quincy Adams in 1839, but failed to gain significant traction.
As the country continued to expand, the issue of slavery in its new territories became the dominant national issue. The Southern position was that slaves were property and therefore could be moved to the territories like all other forms of property. The 1820 Missouri Compromise provided for the admission of Missouri as a slave state and Maine as a free state, preserving the Senate's equality between the regions. In 1846, the Wilmot Proviso was introduced to a war appropriations bill to ban slavery in all territories acquired in the Mexican–American War; the Proviso repeatedly passed the House, but not the Senate. The Compromise of 1850 temporarily defused the issue by admitting California as a free state, instituting a stronger Fugitive Slave Act, banning the slave trade in Washington, D.C., and allowing New Mexico and Utah self-determination on the slavery issue.
Despite the compromise, tensions between North and South continued to rise over the subsequent decade, inflamed by, amongst other things, the publication of the 1852 anti-slavery novel "Uncle Tom's Cabin"; fighting between pro-slave and abolitionist forces in Kansas, beginning in 1854; the 1857 "Dred Scott" decision, which struck down provisions of the Compromise of 1850; abolitionist John Brown's 1859 attempt to start a slave revolt at Harpers Ferry and the 1860 election of slavery critic Abraham Lincoln to the presidency. The Southern states seceded from the Union in the months following Lincoln's election, forming the Confederate States of America, and beginning the American Civil War.
Proposal and ratification.
Crafting the amendment.
Acting under presidential war powers, Lincoln issued the Emancipation Proclamation on January 1, 1863, which proclaimed the freedom of slaves in the ten states that were still in rebellion. However, it did not affect the status of slaves in the border states that had remained loyal to the Union. That December, Lincoln again used his war powers and issued a "Proclamation for Amnesty and Reconstruction", which offered Southern states a chance to peacefully rejoin the Union if they abolished slavery and collected loyalty oaths from 10% of their voting population. Southern states did not readily accept the deal, and the status of slavery remained uncertain.
In the final years of the Civil War, Union lawmakers debated various proposals for Reconstruction. Some of these called for a constitutional amendment to abolish slavery nationally and permanently. On December 14, 1863, a bill proposing such an amendment was introduced by Representative James Mitchell Ashley. Representative James F. Wilson soon followed with a similar proposal. On January 11, 1864, Senator John B. Henderson of Missouri submitted a joint resolution for a constitutional amendment abolishing slavery. The Senate Judiciary Committee, chaired by Lyman Trumbull, became involved in merging different proposals for an amendment.
Radical Republicans led by Senator Charles Sumner and Representative Thaddeus Stevens sought a more expansive version of the amendment. On February 8, 1864, Sumner submitted a constitutional amendment stating:
All persons are equal before the law, so that no person can hold another as a slave; and the Congress shall have power to make all laws necessary and proper to carry this declaration into effect everywhere in the United States.
Sumner tried to promote his own more expansive wording by circumventing the Trumbull-controlled Judiciary Committee, but failed. On February 10, the Senate Judiciary Committee presented the Senate with an amendment proposal based on drafts of Ashley, Wilson and Henderson.
The Committee's version used text from the Northwest Ordinance of 1787, which stipulates, "There shall be neither slavery nor involuntary servitude in the said territory, otherwise than in the punishment of crimes whereof the party shall have been duly convicted." Though using Henderson's proposed amendment as the basis for its new draft, the Judiciary Committee removed language that would have allowed a constitutional amendment to be adopted with only a majority vote in each House of Congress and ratification by two-thirds of the states (instead of two-thirds and three-fourths, respectively).
Passage by Congress.
The Senate passed the amendment on April 8, 1864, by a vote of 38 to 6; two Democrats, Reverdy Johnson of Maryland and James Nesmith of Oregon voted "aye." However, just over two months later on June 15, the House failed to do so, with 93 in favor and 65 against, thirteen votes short of the two-thirds vote needed for passage; the vote split largely along party lines, with Republicans supporting and Democrats opposing. In the 1864 presidential race, former Free Soil Party candidate John C. Frémont threatened a third-party run opposing Lincoln, this time on a platform endorsing an anti-slavery amendment. The Republican Party platform had, as yet, failed to include a similar plank, though Lincoln endorsed the amendment in a letter accepting his nomination. Fremont withdrew from the race on September 22, 1864 and endorsed Lincoln.
With no Southern states represented, few members of Congress pushed moral and religious arguments in favor of slavery. Democrats who opposed the amendment generally made arguments based on federalism and states' rights. Some argued that the proposed change so violated the spirit of the Constitution that it would not be a valid "amendment" but would instead constitute "revolution". Some opponents warned that the amendment would lead to full citizenship for blacks.
Republicans portrayed slavery as uncivilized and argued for abolition as a necessary step in national progress. Amendment supporters also argued that the slave system had negative effects on white people. These included the lower wages resulting from competition with forced labor, as well as repression of abolitionist whites in the South. Advocates said ending slavery would restore the First Amendment and other constitutional rights violated by censorship and intimidation in slave states.
White Northern Republicans, and some Democrats, became excited about an abolition amendment, holding meetings and issuing resolutions. Many blacks, particularly in the South, focused more on landownership and education as the key to liberation. As slavery began to seem politically untenable, an array of Northern Democrats successively announced their support for the amendment, including Representative James Brooks, Senator Reverdy Johnson, and Tammany Hall, a powerful New York political machine.
President Lincoln had had concerns that the Emancipation Proclamation of 1863 might be reversed or found invalid after the war. He saw constitutional amendment as a more permanent solution. He had remained outwardly neutral on the amendment because he considered it politically too dangerous. Nonetheless, Lincoln's 1864 party platform resolved to abolish slavery by constitutional amendment. After winning the election of 1864, Lincoln made the passage of the Thirteenth Amendment his top legislative priority, beginning his efforts while the "lame duck" session was still in office. Popular support for the amendment mounted. and Lincoln urged Congress on in his December 6 State of the Union speech: "there is only a question of "time" as to when the proposed amendment will go to the States for their action. And as it is to so go, at all events, may we not agree that the sooner the better?"
Lincoln instructed Secretary of State William H. Seward, Representative John B. Alley and others to procure votes by any means necessary, and they promised government posts and campaign contributions to outgoing Democrats willing to switch sides. Seward had a large fund for direct bribes. Ashley, who reintroduced the measure into the House, also lobbied several Democrats to vote in favor of the measure. Representative Thaddeus Stevens commented later that "the greatest measure of the nineteenth century was passed by corruption, aided and abetted by the purest man in America"; however, Lincoln's precise role in making deals for votes remains unknown.
Republicans in Congress claimed a mandate for abolition, having gained in the elections for Senate and House. The 1864 Democratic vice-presidential nominee, Representative George H. Pendleton, led opposition to the measure. Republicans toned down their language of radical equality in order to broaden the amendment's coalition of supporters. In order to reassure critics worried that the amendment would tear apart the social fabric, some Republicans explicitly promised that the amendment would leave patriarchy intact.
In mid-January, 1865, Speaker of the House Schuyler Colfax estimated the amendment to be five votes short of passage. Ashley postponed the vote. At this point, Lincoln intensified his push for the amendment, making direct emotional appeals to particular members of Congress. On January 31, 1865, the House called another vote on the amendment, with neither side being certain of the outcome. Every Republican supported the measure, as well as 16 Democrats, almost all of them lame ducks. The amendment finally passed by a vote of 119 to 56, narrowly reaching the required two-thirds majority. The House exploded into celebration, with some members openly weeping. Black onlookers, who had only been allowed to attend Congressional sessions since the previous year, cheered from the galleries.
While under the Constitution, the President plays no formal role in the amendment process, the joint resolution was sent to Lincoln for his signature. Under the usual signatures of the Speaker of the House and the President of the Senate, President Lincoln wrote the word "Approved" and added his signature to the joint resolution on February 1, 1865. On February 7, Congress passed a resolution affirming that the Presidential signature was unnecessary. The Thirteenth Amendment is the only ratified amendment signed by a President, although James Buchanan had signed the Corwin Amendment that the 36th Congress had adopted and sent to the states in March 1861.
Ratification by the states.
When the Thirteenth Amendment was submitted to the states on February 1, 1865, it was quickly taken up by several legislatures. By the end of the month it had been ratified by eighteen states. Among them were the ex-confederate states of Virginia and Louisiana, where ratifications were submitted by Reconstruction governments. These, along with subsequent ratifications from Arkansas and Tennessee raised the issues of how many seceded states had legally valid legislatures; and if there were fewer legislatures than states, if Article V required ratification by three-fourths of the states or three-fourths of the legally valid state legislatures. President Lincoln in his last speech, on April 11, 1865, called the question about whether the Southern states were in or out of the Union a "pernicious abstraction." Obviously, he declared, they were not "in their proper practical relation with the Union"; whence everyone's object should be to restore that relation. Lincoln was assassinated three days later.
With Congress out of session, the new President, Andrew Johnson, began a period known as "Presidential Reconstruction", in which he personally oversaw the creation of new state governments throughout the South. He oversaw the convening of state political conventions populated by delegates whom he deemed to be loyal. Three leading issues came before the convention: secession itself, the abolition of slavery, and the Confederate war debt. Alabama, Florida, Georgia, Mississippi, North Carolina, and South Carolina held conventions in 1865, while Texas' convention did not organize until March 1866. Johnson hoped to prevent deliberation over whether to re-admit the Southern states by accomplishing full ratification before Congress reconvened in December. He believed he could silence those who wished to deny the Southern states their place in the Union by pointing to how essential their assent had been to the successful ratification of the Thirteenth Amendment.
Direct negotiations between state governments and the Johnson administration ensued. As the summer wore on, administration officials began including assurances of the measure's limited scope with their demands for ratification. Johnson himself suggested directly to the governors of Mississippi and North Carolina that they could proactively control the allocation of rights to freedmen. Though Johnson obviously expected the freed people to enjoy at least some civil rights, including, as he specified, the right to testify in court, he wanted state lawmakers to know that the power to confer such rights would remain with the states. When South Carolina provisional governor Benjamin Franklin Perry objected to the scope of the amendment's enforcement clause, Secretary of State Seward responded by telegraph that in fact the second clause "is really restraining in its effect, instead of enlarging the powers of Congress". White politicians throughout the South were concerned that Congress might cite the amendment's enforcement powers as a way to authorize black suffrage.
When South Carolina ratified the amendment in November 1865, it issued its own interpretive declaration that "any attempt by Congress toward legislating upon the political status of former slaves, or their civil relations, would be contrary to the Constitution of the United States". Alabama and Louisiana also declared that their ratification did not imply federal power to legislate on the status of former slaves. During the first week of December, North Carolina and Georgia gave the amendment the final votes needed for it to become part of the Constitution.
The Thirteenth Amendment became part of the Constitution on December 6, 1865, based on the following ratifications:
Having been ratified by the legislatures of three-fourths of the several states—27 of the 36 states (including those that had been "in rebellion"), Secretary of State Seward, on December 18, 1865, certified that the Thirteenth Amendment had become valid, to all intents and purposes, as a part of the Constitution. Included on the enrolled list of ratifying states were the three ex-confederate states that had given their assent, but with strings attached. Seward accepted their affirmative votes, brushed aside their "interpretive declarations" without comment, challenge or any acknowledgment at all.
The Thirteenth Amendment was subsequently ratified by:
The Thirteenth Amendment became part of the Constitution 61 years after the Twelfth Amendment. This is the longest interval between constitutional amendments.
Effects.
The impact of the abolition of slavery was felt quickly. When the Thirteenth Amendment became operational, the scope of Lincoln's 1863 Emancipation Proclamation was widened to include the entire nation. Although the majority of Kentucky's slaves had been emancipated, 65,000–100,000 people remained to be legally freed when the Amendment went into effect on December 18. In Delaware, where a large number of slaves had escaped during the war, nine hundred people became legally free.
In addition to abolishing slavery and prohibiting involuntary servitude, except as a punishment for crime, the Thirteenth Amendment also nullified the Fugitive Slave Clause and the Three-Fifths Compromise. The population of a state originally included (for congressional apportionment purposes) all "free persons", three-fifths of "other persons" (i.e., slaves) and excluded untaxed Native Americans. The Three-Fifths Compromise was a provision in the Constitution that required three-fifths of the population of slaves be counted for purposes of apportionment of seats in the House of Representatives and taxes among the states. This compromise had the effect of increasing the political power of slave-holding states by increasing their share of seats in the House of Representatives, and consequently their share in the Electoral College (where a state's influence over the election of the President is tied to the size of its congressional delegation).
Even as the Thirteenth Amendment was working its way through the ratification process, Republicans in Congress grew increasingly concerned about the potential for there to be a large increase the congressional representation of the Democratic-dominated Southern states. Because the full population of freed slaves would be counted rather than three-fifths, the Southern states would dramatically increase their power in the population-based House of Representatives. Republicans hoped to offset this advantage by attracting and protecting votes of the newly enfranchised black population.
Political and economic change in the South.
Southern culture remained deeply racist, and those blacks who remained faced a dangerous situation. J. J. Gries reported to the Joint Committee on Reconstruction: "There is a kind of innate feeling, a lingering hope among many in the South that slavery will be regalvanized in some shape or other. They tried by their laws to make a worse slavery than there was before, for the freedman has not the protection which the master from interest gave him before." W. E. B. Du Bois wrote in 1935:
Slavery was not abolished even after the Thirteenth Amendment. There were four million freedmen and most of them on the same plantation, doing the same work that they did before emancipation, except as their work had been interrupted and changed by the upheaval of war. Moreover, they were getting about the same wages and apparently were going to be subject to slave codes modified only in name. There were among them thousands of fugitives in the camps of the soldiers or on the streets of the cities, homeless, sick, and impoverished. They had been freed practically with no land nor money, and, save in exceptional cases, without legal status, and without protection.
Official emancipation did not substantially alter the economic situation of most blacks who remained in the south.
As the amendment still permitted labor as punishment for convicted criminals, Southern states responded with what historian Douglas A. Blackmon called "an array of interlocking laws essentially intended to criminalize black life". These laws, passed or updated after emancipation, were known as Black Codes. Mississippi was the first state to pass such codes, with an 1865 law titled "An Act to confer Civil Rights on Freedmen". The Mississippi law required black workers to contract with white farmers by January 1 of each year or face punishment for vagrancy. Blacks could be sentenced to forced labor for crimes including petty theft, using obscene language, or selling cotton after sunset. States passed new, strict vagrancy laws that were selectively enforced against blacks without white protectors. The labor of these convicts was then sold to farms, factories, lumber camps, quarries, and mines.
After its ratification of the Thirteenth Amendment in November 1865, the South Carolina legislature immediately began to legislate Black Codes. The Black Codes created a separate set of laws, punishments, and acceptable behaviors for anyone with more than one black great-grandparent. Under these Codes, Blacks could only work as farmers or servants and had few Constitutional rights. Restrictions on black land ownership threatened to make economic subservience permanent.
Some states mandated indefinitely long periods of child "apprenticeship". Some laws did not target Blacks specifically, but instead affected farm workers, most of whom were Black. At the same time, many states passed laws to actively prevent Blacks from acquiring property.
Southern business owners sought to reproduce the profitable arrangement of slavery with a system called peonage, in which (disproportionately black) workers were entrapped by loans and compelled to work indefinitely because of their debt. Peonage continued well through Reconstruction and ensnared a large proportion of black workers in the South. These workers remained destitute and persecuted, forced to work dangerous jobs and further confined legally by the racist Jim Crow laws that governed the South. Peonage differed from chattel slavery because it was not strictly hereditary and did not allow the sale of people in exactly the same fashion. However, a person's debt—and by extension a person—could still be sold, and the system resembled antebellum slavery in many ways.
Congressional and executive enforcement.
As its first enforcement legislation, Congress passed the Civil Rights Act of 1866, which guaranteed black Americans citizenship and equal protection of the law, though not the right to vote. The Amendment was also used as authorization for several Freedmen's Bureau bills. President Andrew Johnson vetoed these bills, but a Congressional supermajority overrode his veto to pass the Civil Rights Act and the Second Freedmen's Bureau Bill.
Proponents of the Act including Trumbull and Wilson argued that Section 2 of the Thirteenth Amendment (enforcement power) authorized the federal government to legislate civil rights for the States. Others disagreed, maintaining that inequality conditions were distinct from slavery. Seeking more substantial justification, and fearing that future opponents would again seek to overturn the legislation, Congress and the states added additional protections to the Constitution: the Fourteenth Amendment (1868), which defined citizenship and mandated equal protection under the law, and the Fifteenth Amendment (1870), which banned racial voting restrictions.
The Freedmen's Bureau enforced the Amendment locally, providing a degree of support for people subject to the Black Codes. (Reciprocally, the Thirteenth Amendment established the Bureau's legal basis to operate in Kentucky.) The Civil Rights Act circumvented racism in local jurisdictions by allowing blacks access to the federal courts. The Enforcement Acts of 1870–1871 and the Civil Rights Act of 1875, in combating the violence and intimidation of white supremacy, were also part of the effort to end slave conditions for Southern blacks. However, the effect of these laws waned as political will diminished and the federal government lost authority in the South, particularly after the Compromise of 1877 ended Reconstruction in exchange for a Republican presidency.
Peonage law.
With the Peonage Act of 1867, Congress abolished "the holding of any person to service or labor under the system known as peonage", specifically banning "the voluntary or involuntary service or labor of any persons as peons, in liquidation of any debt or obligation, or otherwise."
In 1939, the Department of Justice created the Civil Rights Section, which focused primarily on First Amendment and labor rights. The increasing scrutiny of totalitarianism in the lead-up to World War II brought increased attention to issues of slavery and involuntary servitude, abroad and at home. The U.S. sought to counter foreign propaganda and increase its credibility on the race issue by combatting the Southern peonage system. Under the leadership of Attorney General Francis Biddle, the Civil Rights Section invoked the constitutional amendments and legislation of the Reconstruction Era as the basis for its actions.
In 1947, the DOJ successfully prosecuted Elizabeth Ingalls for keeping domestic servant Dora L. Jones in conditions of slavery. The court found that Jones "was a person wholly subject to the will of defendant; that she was one who had no freedom of action and whose person and services were wholly under the control of defendant and who was in a state of enforced compulsory service to the defendant." The Thirteenth Amendment enjoyed a swell of attention during this period, but from "Brown v. Board" (1954) until "Jones v. Alfred H. Mayer Co." (1968) it was again eclipsed by the Fourteenth Amendment.
Human trafficking.
Victims of human trafficking and other conditions of forced labor are commonly coerced by threat of legal actions to their detriment. Victims of forced labor and trafficking are protected by Title 18 of the U.S. Code.
Conspiracy to injure, oppress, threaten, or intimidate any person's rights or privileges secured by the Constitution or the laws of the United States
It is a crime for any person acting under color of law (federal, state or local officials who enforce statutes, ordinances, regulations, or customs) to willfully deprive or cause to be deprived the rights, privileges, or immunities of any person secured or protected by the Constitution and laws of the U.S. This includes willfully subjecting or causing to be subjected any person to different punishments, pains, or penalties, than those prescribed for punishment of citizens on account of such person being an alien or by reason of his/her color or race.
Judicial interpretation.
In contrast to the other "Reconstruction Amendments", the Thirteenth Amendment was rarely cited in later case law. As historian Amy Dru Stanley summarizes, "beyond a handful of landmark rulings striking down debt peonage, ﬂagrant involuntary servitude, and some instances of race-based violence and discrimination, the Thirteenth Amendment has never been a potent source of rights claims".
Black slaves and their descendants.
"U. S. v. Rhodes" (1866), one of the first Thirteenth Amendment cases, tested the Constitutionality of provisions in the Civil Rights Act of 1866 that granted blacks redress in the federal courts. Kentucky law prohibited blacks from testifying against whites—an arrangement which compromised the ability of Nancy Talbot ("a citizen of the United States of the African race") to reach justice against a white person accused of robbing her. After Talbot attempted to try the case in federal court; the Kentucky Supreme Court ruled this federal option unconstitutional. Noah Swayne (a Supreme Court justice sitting on the Kentucky Circuit Court) overturned the Kentucky decision, holding that without the material enforcement provided by the Civil Rights Act, slavery would not truly be abolished. With "In Re Turner" (1867), Chief Justice Salmon P. Chase ordered freedom for Elizabeth Turner, a former slave in Maryland who became indentured to her former master.
In "", (1872) the Supreme Court heard another Civil Rights Act case relating to federal courts in Kentucky. John Bylew and George Kennard were white men visiting the cabin of a black family, the Fosters. Bylew apparently became angry with sixteen-year-old Richard Foster and hit him twice in the head with an ax. Bylew and Kennard killed Richard's parents, Sallie and Jack Foster, and his blind grandmother, Lucy Armstrong. They severely wounded the Fosters' two young daughters. Kentucky courts would not allow the Foster children to testify against Blyew and Kennard. But federal courts, authorized by the Civil Rights Act, found Blyew and Kennard guilty of murder. When the Supreme Court took the case, they ruled (5–2) that the Foster children did not have standing in federal courts because only living people could take advantage of the Act. In doing so, the Courts effectively ruled that Thirteenth Amendment did not permit a federal remedy in murder cases. Swayne and Joseph P. Bradley dissented, maintaining that in order to have meaningful effects, the Thirteenth Amendment would have to address systemic racial oppression.
Though based on a technicality, the "Blyew" case set a precedent in state and federal courts that led to the erosion of Congress's Thirteenth Amendment powers. The Supreme Court continued along this path in the "Slaughter-House Cases" (1873), which upheld a state-sanctioned monopoly of white butchers. In "United States v. Cruikshank" (1876), the Court ignored Thirteenth Amendment dicta from a circuit court decision to exonerate perpetrators of the Colfax massacre and invalidate the Enforcement Act of 1870.
The Thirteenth Amendment was not solely a ban on chattel slavery, but also covers a much broader array of labor arrangements and social deprivations. As the U.S. Supreme Court explicated in the Slaughter-House Cases (1873) with respect to the Fourteenth and Fifteenth Amendment and the Thirteenth Amendment in special:
In the "Civil Rights Cases" (1883), the Supreme Court reviewed five consolidated cases dealing with the Civil Rights Act of 1875, which outlawed racial discrimination at "inns, public conveyances on land or water, theaters, and other places of public amusement". The Court ruled that the Thirteenth Amendment did not ban most forms of racial discrimination by non-government actors. In the majority decision, Bradley wrote (again in non-binding dicta) that the Thirteenth Amendment empowered Congress to attack "badges and incidents of slavery". However, he distinguished between "fundamental rights" of citizenship, protected by the Thirteenth Amendment, and the "social rights of men and races in the community". The majority opinion held that "it would be running the slavery argument into the ground to make it apply to every act of discrimination which a person may see fit to make as to guests he will entertain, or as to the people he will take into his coach or cab or car; or admit to his concert or theatre, or deal with in other matters of intercourse or business." In his solitary dissent, John Marshall Harlan (a Kentucky lawyer who changed his mind about civil rights law after witnessing organized racist violence) argued that "such discrimination practiced by corporations and individuals in the exercise of their public or quasi-public functions is a badge of servitude, the imposition of which congress may prevent under its power."
The Court in the "Civil Rights Cases" also held that appropriate legislation under the amendment could go beyond nullifying state laws establishing or upholding slavery, because the amendment "has a reflex character also, establishing and decreeing universal civil and political freedom throughout the United States" and thus Congress was empowered "to pass all laws necessary and proper for abolishing all badges and incidents of slavery in the United States." The Court stated about the scope the amendment:
Attorneys in "Plessy v. Ferguson" (1896) argued that racial segregation involved "observances of a servile character coincident with the incidents of slavery", in violation of the Thirteenth Amendment. In their brief to the Supreme Court, Plessy's lawyers wrote that "distinction of race and caste" was inherently unconstitutional. The Supreme Court rejected this reasoning and upheld state laws enforcing segregation under the "separate but equal" doctrine. In the (7–1) majority decision, the Court found that "a statute which implies merely a legal distinction between the white and colored races—a distinction which is founded on the color of the two races and which must always exist so long as white men are distinguished from the other race by color—has no tendency to destroy the legal equality of the two races, or reestablish a state of involuntary servitude." Harlan dissented, writing: "The thin disguise of 'equal' accommodations for passengers in railroad coaches will not mislead any one, nor, atone for the wrong this day done."
In "Hodges v. United States" (1906), the Court struck down a federal statute providing for the punishment of two or more people who "conspire to injure, oppress, threaten or intimidate any citizen in the free exercise or enjoyment of any right or privilege secured to him by the Constitution or laws of the United States". A group of white men in Arkansas conspired to violently prevent eight black workers from performing their jobs at a lumber mill; the group was convicted by a federal grand jury. The Supreme Court ruled that the federal statute, which outlawed conspiracies to deprive citizens of their liberty, was not authorized by the Thirteenth Amendment. It held that "no mere personal assault or trespass or appropriation operates to reduce the individual to a condition of slavery". Harlan dissented, maintaining his opinion that the Thirteenth Amendment should protect freedom beyond "physical restraint". "Corrigan v. Buckley" (1922) reaffirmed the interpretation from "Hodges", finding that the amendment does not apply to restrictive covenants.
Enforcement of federal civil rights law in the South created numerous peonage cases, which slowly traveled up through the judiciary. The Supreme Court ruled in "Clyatt v. United States" (1905) that peonage was involuntary servitude. It held that although employers sometimes described their workers' entry into contract as voluntary, the servitude of peonage was always (by definition) involuntary.
In "Bailey v. Alabama" the U.S. Supreme Court again reaffirmed its holding that Thirteenth Amendment was not solely a ban on chattel slavery, but also covers a much broader array of labor arrangements and social deprivations In addition to the aforesaid the Court also ruled on Congress enforcement power under the Thirteenth Amendment. The Court said:
"Jones" and beyond.
Legal histories cite "Jones v. Alfred H. Mayer Co." (1968) as a turning point of Thirteen Amendment jurisprudence. The Supreme Court confirmed in "Jones" that Congress may act "rationally" to prevent private actors from imposing "badges and incidents of servitude". The Joneses were a black couple in St. Louis County, Missouri who sued a real estate company for refusing to sell them a house. The Court held:
Congress has the power under the Thirteenth Amendment rationally to determine what are the badges and the incidents of slavery, and the authority to translate that determination into effective legislation. [...] this Court recognized long ago that, whatever else they may have encompassed, the badges and incidents of slavery -- its "burdens and disabilities" -- included restraints upon "those fundamental rights which are the essence of civil freedom, namely, the same right . . . to inherit, purchase, lease, sell and convey property, as is enjoyed by white citizens." Civil Rights Cases, 109 U. S. 3, 109 U. S. 22.
Just as the Black Codes, enacted after the Civil War to restrict the free exercise of those rights, were substitutes for the slave system, so the exclusion of Negroes from white communities became a substitute for the Black Codes. And when racial discrimination herds men into ghettos and makes their ability to buy property turn on the color of their skin, then it too is a relic of slavery.
Negro citizens, North and South, who saw in the Thirteenth Amendment a promise of freedom—freedom to "go and come at pleasure" and to "buy and sell when they please"—would be left with "a mere paper guarantee" if Congress were powerless to assure that a dollar in the hands of a Negro will purchase the same thing as a dollar in the hands of a white man. At the very least, the freedom that Congress is empowered to secure under the Thirteenth Amendment includes the freedom to buy whatever a white man can buy, the right to live wherever a white man can live. If Congress cannot say that being a free man means at least this much, then the Thirteenth Amendment made a promise the Nation cannot keep.
The Court in "Jones" reopened the issue of linking racism in contemporary society to the history of slavery in the United States.
The "Jones" precedent has been used to justify Congressional action to protect migrant workers and target sex trafficking. The direct enforcement power found in the Thirteenth Amendment contrasts with that of the Fourteenth, which allows only responses to institutional discrimination of state actors.
Other cases of involuntary servitude.
The Supreme Court has taken an especially narrow view of involuntary servitude claims made by people not descended from black (African) slaves. In "Robertson v. Baldwin" (1897), a sailor challenged federal rules mandating the capture and return of deserters. The Court ruled that "the amendment was not intended to introduce any novel doctrine with respect to certain descriptions of service which have always been treated as exceptional." In this case, as in numerous "badges and incidents" cases, Justice Harlan authored a dissent favoring broader Thirteenth Amendment protections.
In "Selective Draft Law Cases", the Supreme Court ruled that the military draft was not "involuntary servitude". In "United States v. Kozminski", the Supreme Court ruled that the Thirteenth Amendment did not prohibit compulsion of servitude through psychological coercion. "Kozminski" defined involuntary servitude for purposes of criminal prosecution as "a condition of servitude in which the victim is forced to work for the defendant by the use or threat of physical restraint or physical injury or by the use or threat of coercion through law or the legal process. This definition encompasses cases in which the defendant holds the victim in servitude by placing him or her in fear of such physical restraint or injury or legal coercion."
The U.S. Courts of Appeals, in "Immediato v. Rye Neck School District", "Herndon v. Chapel Hill", and "Steirer v. Bethlehem School District", have ruled that the use of community service as a high school graduation requirement did not violate the Thirteenth Amendment.
Prior proposed Thirteenth Amendments.
During the six decades following the 1804 ratification of the Twelfth Amendment two proposals to amend the Constitution were adopted by Congress and sent to the states for ratification. Neither has been ratified by the number of states necessary to become part of the Constitution. Commonly known as the Titles of Nobility Amendment and the Corwin Amendment, both are referred to as "Article Thirteen", as was the successful Thirteenth Amendment, in the joint resolution passed by Congress.

</doc>
<doc id="31666" url="http://en.wikipedia.org/wiki?curid=31666" title="Fourteenth Amendment to the United States Constitution">
Fourteenth Amendment to the United States Constitution

The Fourteenth Amendment (Amendment XIV) to the United States Constitution was adopted on July 9, 1868, as one of the Reconstruction Amendments. The amendment addresses citizenship rights and equal protection of the laws, and was proposed in response to issues related to former slaves following the American Civil War. The amendment was bitterly contested, particularly by Southern states, which were forced to ratify it in order for them to regain representation in Congress. The Fourteenth Amendment, particularly its first section, is one of the most litigated parts of the Constitution, forming the basis for landmark decisions such as "Roe v. Wade" (1973), regarding abortion, and "Bush v. Gore" (2000), regarding the 2000 presidential election. The amendment limits the actions of all state and local officials, including those acting on behalf of such an official.
The second, third, and fourth sections of the amendment are seldom litigated. However, Section 2's reference to "rebellion and other crimes" has been invoked as a constitutional ground for felony disenfranchisement. The fifth section gives Congress enforcement power. The amendment's first section includes several clauses: the Citizenship Clause, Privileges or Immunities Clause, Due Process Clause, and Equal Protection Clause. The Citizenship Clause provides a broad definition of citizenship, overruling the Supreme Court's decision in "Dred Scott v. Sandford" (1857), which had held that Americans descended from African slaves could not be citizens of the United States. The Privileges or Immunities Clause has been interpreted in such a way that it does very little.
The Due Process Clause prohibits state and local government officials from depriving persons of life, liberty, or property without legislative authorization. This clause has also been used by the federal judiciary to make most of the Bill of Rights applicable to the states, as well as to recognize substantive and procedural requirements that state laws must satisfy.
The Equal Protection Clause requires each state to provide equal protection under the law to all people within its jurisdiction. This clause was the basis for "Brown v. Board of Education" (1954), the Supreme Court decision that precipitated the dismantling of racial segregation, and for many other decisions rejecting irrational or unnecessary discrimination against people belonging to various groups.
Text.
Section 1. All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside. No State shall make or enforce any law which shall abridge the privileges or immunities of citizens of the United States; nor shall any State deprive any person of life, liberty, or property, without due process of law; nor deny to any person within its jurisdiction the equal protection of the laws.
Section 2. Representatives shall be apportioned among the several States according to their respective numbers, counting the whole number of persons in each State, excluding Indians not taxed. But when the right to vote at any election for the choice of electors for President and Vice President of the United States, Representatives in Congress, the Executive and Judicial officers of a State, or the members of the Legislature thereof, is denied to any of the male inhabitants of such State, being twenty-one years of age, and citizens of the United States, or in any way abridged, except for participation in rebellion, or other crime, the basis of representation therein shall be reduced in the proportion which the number of such male citizens shall bear to the whole number of male citizens twenty-one years of age in such State.
Section 3. No person shall be a Senator or Representative in Congress, or elector of President and Vice President, or hold any office, civil or military, under the United States, or under any State, who, having previously taken an oath, as a member of Congress, or as an officer of the United States, or as a member of any State legislature, or as an executive or judicial officer of any State, to support the Constitution of the United States, shall have engaged in insurrection or rebellion against the same, or given aid or comfort to the enemies thereof. But Congress may, by a vote of two-thirds of each House, remove such disability.
Section 4. The validity of the public debt of the United States, authorized by law, including debts incurred for payment of pensions and bounties for services in suppressing insurrection or rebellion, shall not be questioned. But neither the United States nor any State shall assume or pay any debt or obligation incurred in aid of insurrection or rebellion against the United States, or any claim for the loss or emancipation of any slave; but all such debts, obligations and claims shall be held illegal and void.
Section 5. The Congress shall have power to enforce, by appropriate legislation, the provisions of this article."
Adoption.
Proposal by Congress.
In the final years of the American Civil War and the Reconstruction Era that followed, Congress repeatedly debated the rights of black former slaves freed by the 1863 Emancipation Proclamation and the 1865 Thirteenth Amendment, the latter of which had formally abolished slavery. Following the passage of the Thirteenth Amendment by Congress, however, Republicans grew concerned over the increase it would create in the congressional representation of the Democratic-dominated Southern states. Because the full population of freed slaves would now be counted for determining congressional representation, rather than the three-fifths previously mandated by the Three-Fifths Compromise, the Southern states would dramatically increase their power in the population-based House of Representatives, regardless of whether the former slaves were allowed to vote. Republicans began looking for a way to offset this advantage, either by protecting and attracting votes of former slaves, or at least by discouraging their disenfranchisement.
In 1865, Congress passed what would become the Civil Rights Act of 1866, guaranteeing citizenship without regard to race, color, or previous condition of slavery or involuntary servitude. The bill also guaranteed equal benefits and access to the law, a direct assault on the Black Codes passed by many post-war states. The Black Codes attempted to return ex-slaves to something like their former condition by, among other things, restricting their movement, forcing them to enter into year-long labor contracts, prohibiting them from owning firearms, and by preventing them from suing or testifying in court.
Although strongly urged by moderates in Congress to sign the bill, President Johnson vetoed it on March 27, 1866. In his veto message, he objected to the measure because it conferred citizenship on the freedmen at a time when 11 out of 36 states were unrepresented in the Congress, and that it discriminated in favor of African-Americans and against whites. Three weeks later, Johnson's veto was overridden and the measure became law. This was the first time in American history that Congress was able to muster the votes necessary to override a presidential veto. Despite this victory, even some Republicans who had supported the goals of the Civil Rights Act began to doubt that Congress really possessed constitutional power to turn those goals into laws. The experience also encouraged both radical and moderate Republicans to seek Constitutional guarantees for black rights, rather than relying on temporary political majorities.
Over 70 proposals for an amendment were drafted. In late 1865, the Joint Committee on Reconstruction proposed an amendment stating that any citizens barred from voting on the basis of race by a state would not be counted for purposes of representation of that state. This amendment passed the House, but was blocked in the Senate by a coalition of Radical Republicans led by Charles Sumner, who believed the proposal a "compromise with wrong", and Democrats opposed to black rights. Consideration then turned to a proposed amendment by Representative John A. Bingham of Ohio, which would enable Congress to safeguard "equal protection of life, liberty, and property" of all citizens; this proposal failed to pass the House. In April 1866, the Joint Committee forwarded a third proposal to Congress, a carefully negotiated compromise that combined elements of the first and second proposals as well as addressing the issues of Confederate debt and voting by ex-Confederates. The House of Representatives passed "House Resolution 127, 39th Congress" several weeks later and sent to the Senate for action. The resolution was debated and several amendments to it were proposed. Amendments to Sections 2, 3 and 4 were adopted on June 8, 1866 and the modified resolution passed by a 33 to 11 vote. The House agreed to the Senate amendments on June 13 by a 138-36 vote. A concurrent resolution requesting the President to transmit the proposal to the executives of the several states was passed by both houses of Congress on June 18.
The Radical Republicans were satisfied that they had secured civil rights for blacks, but were disappointed that the amendment would not also secure political rights for blacks, in particular the right to vote. For example, Thaddeus Stevens, a leader of the disappointed Radical Republicans, said: "I find that we shall be obliged to be content with patching up the worst portions of the ancient edifice, and leaving it, in many of its parts, to be swept through by the tempests, the frosts, and the storms of despotism." Abolitionist Wendell Phillips called it a "fatal and total surrender". This point would later be addressed by the Fifteenth Amendment.
Ratification by the states.
Ratification of the amendment was bitterly contested. State legislatures in every formerly Confederate state, with the exception of Tennessee, refused to ratify it. This refusal led to the passage of the Reconstruction Acts. Ignoring the existing state governments, military government was imposed until new civil governments were established and the Fourteenth Amendment was ratified. It also prompted Congress to pass a law on March 2, 1867, requiring that a former Confederate state must ratify the Fourteenth Amendment before "said State shall be declared entitled to representation in Congress".
The first twenty-eight states to ratify the Fourteenth Amendment were:
On July 20, 1868, Secretary of State William H. Seward certified that the amendment had become part of the Constitution on July 9, 1868, if withdrawals of ratification by New Jersey and Ohio were ineffective. The following day, Congress adopted and transmitted to the Department of State a concurrent resolution declaring the Fourteenth Amendment to be a part of the Constitution and directing the Secretary of State to promulgate it as such. Both New Jersey and Ohio were named in the congressional resolution as having ratified the amendment. Their inclusion as ratifying states by Congress goes to the merits of rescinding a ratification after it has been affirmed, and of approving a ratification after it has been rejected. It would appear that Congress has determined both have no impact on the ratification process – see "Coleman v. Miller", 307 U.S. (1939). Accordingly, Seward issued an unconditional certificate of ratification, dated July 28, 1868, declaring that the Fourteenth Amendment had been duly ratified by the requisite three-fourths of the states. During the preceding week, two additional states had ratified the amendment, which left no doubt that the amendment had indeed become operational.
The Fourteenth Amendment was subsequently ratified:
Citizenship and civil rights.
Background.
Section 1 of the amendment formally defines United States citizenship and also protects various civil rights from being abridged or denied by any state or state actor. Abridgment or denial of those civil rights by private persons is not addressed by this amendment; the Supreme Court held in the "Civil Rights Cases" (1883) that the amendment was limited to "state action" and, therefore, did not authorize the Congress to outlaw racial discrimination by private individuals or organizations (though Congress can sometimes reach such discrimination via other parts of the Constitution). U.S. Supreme Court Justice Joseph P. Bradley commented in the Civil Rights Cases that "individual invasion of individual rights is not the subject-matter of the [14th] Amendment. It has a deeper and broader scope. It nullifies and makes void all state legislation, and state action of every kind, which impairs the privileges and immunities of citizens of the United States, or which injures them in life, liberty or property without due process of law, or which denies to any of them the equal protection of the laws."
The Radical Republicans who advanced the Thirteenth Amendment hoped to ensure broad civil and human rights for the newly freed people—but its scope was disputed before it even went into effect. The framers of the Fourteenth Amendment wanted these principles enshrined in the Constitution to protect the new Civil Rights Act from being declared unconstitutional by the Supreme Court and also to prevent a future Congress from altering it by a mere majority vote. This section was also in response to violence against black people within the Southern states. The Joint Committee on Reconstruction found that only a Constitutional amendment could protect black people's rights and welfare within those states.
This first section of the amendment has been the most frequently litigated part of the amendment, and this amendment in turn has been the most frequently litigated part of the Constitution.
Citizenship Clause.
The Citizenship Clause overruled the Supreme Court's "Dred Scott" decision that black people were not citizens and could not become citizens, nor enjoy the benefits of citizenship. Some members of Congress voted for the Fourteenth Amendment in order to eliminate doubts about the constitutionality of the Civil Rights Act of 1866, or to ensure that no subsequent Congress could later repeal or alter the main provisions of that Act. The Civil Rights Act of 1866 had granted citizenship to all persons born in the United States if they were not subject to a foreign power, and this clause of the Fourteenth Amendment constitutionalized this rule.
There are varying interpretations of the original intent of Congress and of the ratifying states, based on statements made during the congressional debate over the amendment, as well as the customs and understandings prevalent at that time. Some of the major issues that have arisen about this clause are the extent to which it included Native Americans, its coverage of non-citizens legally present in the United States when they have a child, whether the clause allows revocation of citizenship, and whether the clause applies to illegal immigrants.
Native Americans.
During the original congressional debate over the amendment Senator Jacob M. Howard of Michigan—the author of the Citizenship Clause—described the clause as having the same content, despite different wording, as the earlier Civil Rights Act of 1866, namely, that it excludes Native Americans who maintain their tribal ties and "persons born in the United States who are foreigners, aliens, who belong to the families of ambassadors or foreign ministers." According to historian Glenn W. LaFantasie of Western Kentucky University, "A good number of his fellow senators supported his view of the citizenship clause." Others also agreed that the children of ambassadors and foreign ministers were to be excluded.
Senator James Rood Doolittle of Wisconsin asserted that all Native Americans were subject to United States jurisdiction, so that the phrase "Indians not taxed" would be preferable, but Senate Judiciary Committee Chairman Lyman Trumbull and Howard disputed this, arguing that the federal government did not have full jurisdiction over Native American tribes, which govern themselves and make treaties with the United States. In "Elk v. Wilkins" (1884), the clause's meaning was tested regarding whether birth in the United States automatically extended national citizenship. The Supreme Court held that Native Americans who voluntarily quit their tribes did not automatically gain national citizenship. The issue was resolved with the passage of the Indian Citizenship Act of 1924, which granted full U.S. citizenship to indigenous peoples.
Children born to citizens of other countries.
The Fourteenth Amendment provides that children born in the United States become American citizens regardless of the citizenship of their parents. At the time of the amendment's passage, three Senators, including Trumbull, the author of the Civil Rights Act, as well as President Andrew Johnson, asserted that both the Civil Rights Act and the Fourteenth Amendment would confer citizenship on such children at birth; however, Senator Edgar Cowan of Pennsylvania had a definitively contrary opinion. These congressional remarks applied to non-citizens lawfully present in the United States, as the problem of unauthorized immigration did not exist in 1866, and some scholars dispute whether the Citizenship Clause applies to unauthorized immigrants, although the law of the land continues to be based on the standard interpretation. Congress during the 21st century has occasionally discussed revising the clause to reduce the practice of "birth tourism", in which a pregnant foreign national gives birth in the United States for purposes of the child's citizenship.
The clause's meaning with regard to a child of legal immigrants was tested in "United States v. Wong Kim Ark" (1898). The Supreme Court held that under the Fourteenth Amendment, a man born within the United States to Chinese citizens who have a permanent domicile and residence in the United States and are carrying on business in the United States—and whose parents were not employed in a diplomatic or other official capacity by a foreign power—was a citizen of the United States. Subsequent decisions have applied the principle to the children of foreign nationals of non-Chinese descent.
Loss of citizenship.
Loss of national citizenship is possible only under the following circumstances:
For much of the country's history, voluntary acquisition or exercise of a foreign citizenship was considered sufficient cause for revocation of national citizenship. This concept was enshrined in a series of treaties between the United States and other countries (the Bancroft Treaties). However, the Supreme Court repudiated this concept in "Afroyim v. Rusk" (1967), as well as "Vance v. Terrazas" (1980), holding that the Citizenship Clause of the Fourteenth Amendment barred the Congress from revoking citizenship. However, Congress can revoke citizenship that it had previously granted to a person not born in the United States.
Privileges or Immunities Clause.
The Privileges or Immunities Clause, which protects the privileges and immunities of national citizenship from interference by the states, was patterned after the Privileges and Immunities Clause of Article IV, which protects the privileges and immunities of state citizenship from interference by other states. In the "Slaughter-House Cases" (1873), the Supreme Court concluded that the Constitution recognized two separate types of citizenship—"national citizenship" and "state citizenship"—and the Court held that the Privileges or Immunities Clause prohibits states from interfering only with privileges and immunities possessed by virtue of national citizenship. The Court concluded that the privileges and immunities of national citizenship included only those rights that "owe their existence to the Federal government, its National character, its Constitution, or its laws." The Court recognized few such rights, including access to seaports and navigable waterways, the right to run for federal office, the protection of the federal government while on the high seas or in the jurisdiction of a foreign country, the right to travel to the seat of government, the right to peaceably assemble and petition the government, the privilege of the writ of habeas corpus, and the right to participate in the government's administration. This decision has not been overruled and has been specifically reaffirmed several times. Largely as a result of the narrowness of the "Slaughter-House" opinion, this clause subsequently lay dormant for well over a century.
In "Saenz v. Roe" (1999), the Court ruled that a component of the "right to travel" is protected by the Privileges or Immunities Clause:Despite fundamentally differing views concerning the coverage of the Privileges or Immunities Clause of the Fourteenth Amendment, most notably expressed in the majority and dissenting opinions in the "Slaughter-House Cases" (1873), it has always been common ground that this Clause protects the third component of the right to travel. Writing for the majority in the "Slaughter-House Cases", Justice Miller explained that one of the privileges "conferred by this Clause" "is that a citizen of the United States can, of his own volition, become a citizen of any State of the Union by a bona fide residence therein, with the same rights as other citizens of that State." (emphasis added)
Justice Miller actually wrote in the "Slaughter-House Cases" that the right to become a citizen of a state (by residing in that state) "is conferred by "the very article" under consideration" (emphasis added), rather than by the "clause" under consideration.
In "McDonald v. Chicago "(2010), Justice Clarence Thomas, while concurring with the majority in incorporating the Second Amendment against the states, declared that he reached this conclusion through the Privileges or Immunities Clause instead of the Due Process Clause. Randy Barnett has referred to Justice Thomas's concurring opinion as a "complete restoration" of the Privileges or Immunities Clause.
Due Process Clause.
The Due Process Clause of the Fourteenth Amendment applies only against the states, but it is otherwise textually identical to the Due Process Clause of the Fifth Amendment, which applies against the federal government; both clauses have been interpreted to encompass identical doctrines of procedural due process and substantive due process. Procedural due process is the guarantee of a fair legal process when the government tries to interfere with a person's protected interests in life, liberty, or property, and substantive due process is the guarantee that the fundamental rights of citizens will not be encroached on by government. The Due Process Clause of the Fourteenth Amendment also incorporates most of the provisions in the Bill of Rights, which were originally applied against only the federal government, and applies them against the states.
Substantive due process.
Beginning with "Allgeyer v. Louisiana" (1897), the Court interpreted the Due Process Clause as providing substantive protection to private contracts, thus prohibiting a variety of social and economic regulation; this principle was referred to as "freedom of contract". Thus, the Court struck down a law decreeing maximum hours for workers in a bakery in "Lochner v. New York" (1905) and struck down a minimum wage law in "Adkins v. Children's Hospital" (1923). In "Meyer v. Nebraska" (1923), the Court stated that the "liberty" protected by the Due Process Clause"[w]ithout doubt...denotes not merely freedom from bodily restraint but also the right of the individual to contract, to engage in any of the common occupations of life, to acquire useful knowledge, to marry, establish a home and bring up children, to worship God according to the dictates of his own conscience, and generally to enjoy those privileges long recognized at common law as essential to the orderly pursuit of happiness by free men."
However, the Court did uphold some economic regulation, such as state Prohibition laws ("Mugler v. Kansas", 1887), laws declaring maximum hours for mine workers ("Holden v. Hardy", 1898), laws declaring maximum hours for female workers ("Muller v. Oregon", 1908), and President Woodrow Wilson's intervention in a railroad strike ("Wilson v. New", 1917), as well as federal laws regulating narcotics ("United States v. Doremus", 1919). The Court repudiated, but did not explicitly overrule, the "freedom of contract" line of cases in "West Coast Hotel v. Parrish" (1937).
Although the "freedom of contract" has fallen into disfavor, by the 1960s, the Court had extended its interpretation of substantive due process to include other rights and freedoms that are not enumerated in the Constitution but that, according to the Court, extend or derive from existing rights. For example, the Due Process Clause is also the foundation of a constitutional right to privacy. The Court first ruled that privacy was protected by the Constitution in "Griswold v. Connecticut" (1965), which overturned a Connecticut law criminalizing birth control. While Justice William O. Douglas wrote for the majority that the right to privacy was found in the "penumbras" of various provisions in the Bill of Rights, Justices Arthur Goldberg and John Marshall Harlan II wrote in concurring opinions that the "liberty" protected by the Due Process Clause included individual privacy.
The right to privacy was the basis for "Roe v. Wade" (1973), in which the Court invalidated a Texas law forbidding abortion except to save the mother's life. Like Goldberg's and Harlan's concurring opinions in "Griswold", the majority opinion authored by Justice Harry A. Blackmun located the right to privacy in the Due Process Clause's protection of liberty. The decision disallowed many state and federal abortion restrictions, and it became one of the most controversial in the Court's history. In "Planned Parenthood v. Casey" (1992), the Court decided that "the essential holding of "Roe v. Wade" should be retained and once again reaffirmed." In "Lawrence v. Texas" (2003), the Court found that a Texas law against same-sex sexual intercourse violated the right to privacy.
Procedural due process.
When the government seeks to burden a person's protected liberty interest or property interest, the Supreme Court has held that procedural due process requires that, at a minimum, the government provide the person notice, an opportunity to be heard at an oral hearing, and a decision by a neutral decision maker. For example, such process is due when a government agency seeks to terminate civil service employees, expel a student from public school, or cut off a welfare recipient's benefits.
The Court has also ruled that the Due Process Clause requires judges to recuse themselves in cases where the judge has a conflict of interest. For example, in "Caperton v. A.T. Massey Coal Co." (2009), the Court ruled that a justice of the Supreme Court of Appeals of West Virginia had to recuse himself from a case involving a major contributor to his campaign for election to that court.
Incorporation.
While many state constitutions are modeled after the United States Constitution and federal laws, those state constitutions did not necessarily include provisions comparable to the Bill of Rights. In "Barron v. Baltimore" (1833), the Supreme Court unanimously ruled that the Bill of Rights restrained only the federal government, not the states. However, the Supreme Court has subsequently held that most provisions of the Bill of Rights apply to the states through the Due Process Clause of the Fourteenth Amendment under a doctrine called "incorporation."
Whether incorporation was intended by the amendment's framers, such as John Bingham, has been debated by legal historians. According to legal scholar Akhil Reed Amar, the framers and early supporters of the Fourteenth Amendment believed that it would ensure that the states would be required to recognize the same individual rights as the federal government; all of these rights were likely understood as falling within the "privileges or immunities" safeguarded by the amendment.
By the latter half of the 20th century, nearly all of the rights in the Bill of Rights had been applied to the states. The Supreme Court has held that the amendment's Due Process Clause incorporates all of the substantive protections of the First, Second, Fourth, Fifth (except for its Grand Jury Clause) and Sixth Amendments and the Cruel and Unusual Punishment Clause of the Eighth Amendment. While the Third Amendment has not been applied to the states by the Supreme Court, the Second Circuit ruled that it did apply to the states within that circuit's jurisdiction in "Engblom v. Carey". The Seventh Amendment right to jury trial in civil cases has been held not to be applicable to the states, but the amendment's Re-Examination Clause applies not only to federal courts, but also to "a case tried before a jury in a state court and brought to the Supreme Court on appeal."
Equal Protection Clause.
The Equal Protection Clause was created largely in response to the lack of equal protection provided by law in states with Black Codes. Under Black Codes, blacks could not sue, give evidence, or be witnesses. They also were punished more harshly than whites. The Supreme Court in "Strauder v. West Virginia" stated that the Equal Protection Clause was
designed to assure to the colored race the enjoyment of all the civil rights that under the law are enjoyed by white persons, and to give to that race the protection of the general government, in that enjoyment, whenever it should be denied by the States.
The Clause mandates that individuals in similar situations be treated equally by the law. Although the text of the Fourteenth Amendment applies the Equal Protection Clause only against the states, the Supreme Court, since "Bolling v. Sharpe" (1954), has applied the Clause against the federal government through the Due Process Clause of the Fifth Amendment under a doctrine called "reverse incorporation."
In "Yick Wo v. Hopkins" (1886), the Supreme Court has clarified that the meaning of "person" and "within its jurisdiction" in the Equal Protection Clause would not be limited to discrimination against African Americans, but would extend to other races, colors, and nationalities such as (in this case) legal aliens in the United States who are Chinese citizens: 
These provisions are universal in their application to all persons within the territorial jurisdiction, without regard to any differences of race, of color, or of nationality, and the equal protection of the laws is a pledge of the protection of equal laws.
Persons "within its jurisdiction" are entitled to equal protection from a state. Largely because the Privileges and Immunities Clause of Article IV has from the beginning guaranteed the privileges and immunities of citizens in the several states, the Supreme Court has rarely construed the phrase "within its jurisdiction" in relation to natural persons. In "Plyler v. Doe", 457 U.S. 202, 210–16 (1982) where the Court hold that aliens illegally present in a state are "within its jurisdiction" and may thus raise equal protection claims the Court explicated the meaning of the phrase "within its jurisdiction" as follows: "[U]se of the phrase "within its jurisdiction" confirms the understanding that the Fourteenth Amendment's protection extends to anyone, citizen or stranger, who is subject to the laws of a State, and reaches into every corner of a State's territory." The Court reached this understanding among other things from Senator Howard, a member of the Joint Committee of Fifteen, and the floor manager of the amendment in the Senate. Senator Howard was explicit about the broad objectives of the Fourteenth Amendment and the intention to make its provisions applicable to all who "may happen to be" within the jurisdiction of a state:
The last two clauses of the first section of the amendment disable a State from depriving not merely a citizen of the United States, but "any person, whoever he may be", of life, liberty, or property without due process of law, or from denying to him the equal protection of the laws of the State. This abolishes all class legislation in the States and does away with the injustice of subjecting one caste of persons to a code not applicable to another. ... It will, if adopted by the States, forever disable every one of them from passing laws trenching upon those fundamental rights and privileges which pertain to citizens of the United States, "and to all person who may happen to be within their jurisdiction." [emphasis added by the U.S. Supreme Court]
The relationship between the Fifth and Fourteenth Amendments was addressed by Justice Field in "Wong Wing v. United States". He observed with respect to the phrase "within its jurisdiction": "The term 'person,' used in the Fifth Amendment, is broad enough to include any and every human being within the jurisdiction of the republic. A resident, alien born, is entitled to the same protection under the laws that a citizen is entitled to. He owes obedience to the laws of the country in which he is domiciled, and, as a consequence, he is entitled to the equal protection of those laws. ... The contention that persons within the territorial jurisdiction of this republic might be beyond the protection of the law was heard with pain on the argument at the bar—in face of the great constitutional amendment which declares that no State shall deny to any person within its jurisdiction the equal protection of the laws."
Whether foreign corporations are also "within the jurisdiction" of a state was also decided by the Supreme Court. The Supreme Court held that a foreign corporation which sued in a state court in which it was not licensed to do business to recover possession of property wrongfully taken from it in another state was "within the jurisdiction" and could not be subjected to unequal burdens in the maintenance of the suit. When a state has admitted a foreign corporation to do business within its borders, that corporation is entitled to equal protection of the laws but not necessarily to identical treatment with domestic corporations.
In "Santa Clara County v. Southern Pacific Railroad" (1886), the court reporter included a statement by Chief Justice Morrison Waite in the decision's headnote: 
The court does not wish to hear argument on the question whether the provision in the Fourteenth Amendment to the Constitution, which forbids a State to deny to any person within its jurisdiction the equal protection of the laws, applies to these corporations. We are all of the opinion that it does.
This dictum, which established that corporations enjoyed personhood under the Equal Protection Clause, was repeatedly reaffirmed by later courts. It remained the predominant view throughout the twentieth century, though it was challenged in dissents by justices such as Hugo Black and William O. Douglas.
In the decades following the adoption of the Fourteenth Amendment, the Supreme Court overturned laws barring blacks from juries ("Strauder v. West Virginia", 1880) or discriminating against Chinese Americans in the regulation of laundry businesses ("Yick Wo v. Hopkins", 1886), as violations of the Equal Protection Clause. However, in "Plessy v. Ferguson" (1896), the Supreme Court held that the states could impose segregation so long as they provided similar facilities—the formation of the "separate but equal" doctrine.
The Court went even further in restricting the Equal Protection Clause in "Berea College v. Kentucky" (1908), holding that the states could force private actors to discriminate by prohibiting colleges from having both black and white students. By the early 20th century, the Equal Protection Clause had been eclipsed to the point that Justice Oliver Wendell Holmes, Jr. dismissed it as "the usual last resort of constitutional arguments."
The Court held to the "separate but equal" doctrine for more than fifty years, despite numerous cases in which the Court itself had found that the segregated facilities provided by the states were almost never equal, until "Brown v. Board of Education" (1954) reached the Court. In "Brown" the Court ruled that even if segregated black and white schools were of equal quality in facilities and teachers, segregation by itself was harmful to black students and so was unconstitutional. "Brown" met with a campaign of resistance from white Southerners, and for decades the federal courts attempted to enforce "Brown"‍ '​s mandate against repeated attempts at circumvention. This resulted in the controversial desegregation busing decrees handed down by federal courts in various parts of the nation. In "Parents Involved in Community Schools v. Seattle School District No. 1" (2007), the Court ruled that race could not be the determinative factor in determining to which public schools parents may transfer their children.
In "Plyler v. Doe" (1982) the Supreme Court struck down a Texas statute denying free public education to illegal immigrants as a violation of the Equal Protection Clause of the Fourteenth Amendment because discrimination on the basis of illegal immigration status did not further a substantial state interest. The Court reasoned that illegal aliens and their children, though not citizens of the United States or Texas, are people "in any ordinary sense of the term" and, therefore, are afforded Fourteenth Amendment protections.
In "Hernandez v. Texas" (1954), the Court held that the Fourteenth Amendment protects those beyond the racial classes of white or "Negro" and extends to other racial and ethnic groups, such as Mexican Americans in this case. In the half-century following "Brown", the Court extended the reach of the Equal Protection Clause to other historically disadvantaged groups, such as women and illegitimate children, although it has applied a somewhat less stringent standard than it has applied to governmental discrimination on the basis of race ("United States v. Virginia", 1996; "Levy v. Louisiana", 1968).
The Supreme Court ruled in "Regents of the University of California v. Bakke" (1978) that affirmative action in the form of racial quotas in public university admissions was a violation of Title VI of the Civil Rights Act of 1964; however, race could be used as one of several factors without violating of the Equal Protection Clause or Title VI. In "Gratz v. Bollinger" (2003) and "Grutter v. Bollinger" (2003), the Court considered two race-conscious admissions systems at the University of Michigan. The university claimed that its goal in its admissions systems was to achieve racial diversity. In "Gratz", the Court struck down a points-based undergraduate admissions system that added points for minority status, finding that its rigidity violated the Equal Protection Clause; in "Grutter", the Court upheld a race-conscious admissions process for the university's law school that used race as one of many factors to determine admission. In "Fisher v. University of Texas" (2013), the Court ruled that before race can be used in a public university's admission policy, there must be no workable race-neutral alternative. In "Schuette v. Coalition to Defend Affirmative Action" (2014), the Court upheld the constitutionality of a state constitutional prohibition on the state or local use of affirmative action.
"Reed v. Reed" (1971), which struck down an Idaho probate law favoring men, was the first decision in which the Court ruled that arbitrary gender discrimination violated the Equal Protection Clause. In "Craig v. Boren" (1976), the Court ruled that statutory or administrative sex classifications had to be subjected to an intermediate standard of judicial review. "Reed" and "Craig" later served as precedents to strike down a number of state laws discriminating by gender.
Since "Wesberry v. Sanders" (1964) and "Reynolds v. Sims" (1964), the Supreme Court has interpreted the Equal Protection Clause as requiring the states to apportion their congressional districts and state legislative seats according to "one man, one vote". The Court has also struck down redistricting plans in which race was a key consideration. In "Shaw v. Reno" (1993), the Court prohibited a North Carolina plan aimed at creating majority-black districts to balance historic underrepresentation in the state's congressional delegations.
The Equal Protection Clause served as the basis for the decision in "Bush v. Gore" (2000), in which the Court ruled that no constitutionally valid recount of Florida's votes in the 2000 presidential election could be held within the needed deadline; the decision effectively secured Bush's victory in the disputed election. In "League of United Latin American Citizens v. Perry" (2006), the Court ruled that House Majority Leader Tom DeLay's Texas redistricting plan intentionally diluted the votes of Latinos and thus violated the Equal Protection Clause.
State actor doctrine.
Individual liberties guaranteed by the United States Constitution protect, with exception of the Thirteenth Amendment's ban on slavery, not against actions by private persons or entities, but only against actions by government officials. Regarding the Fourteenth Amendment, the Supreme Court ruled in "Shelley v. Kraemer", 334 U.S. (1948): "[T]he action inhibited by the first section of the Fourteenth Amendment is only such action as may fairly be said to be that of the States. That Amendment erects no shield against merely private conduct, however discriminatory or wrongful." The court added in "Civil Rights Cases", 109 U.S. (1883): "It is State action of a particular character that is prohibited. Individual invasion of individual rights is not the subject matter of the amendment. It has a deeper and broader scope. It nullifies and makes void all State legislation, and State action of every kind, which impairs the privileges and immunities of citizens of the United States, or which injures them in life, liberty, or property without due process of law, or which denies to any of them the equal protection of the laws."
Vindication of federal constitutional rights are limited to those situations where there is "state action" meaning action of government officials who are exercising their governmental power. In "Ex parte Virginia", 100 U.S. (1880), the Supreme Court found that the prohibitions of the Fourteenth Amendment "have reference to actions of the political body denominated by a State, by whatever instruments or in whatever modes that action may be taken. A State acts by its legislative, its executive, or its judicial authorities. It can act in no other way. The constitutional provision, therefore, must mean that no agency of the State, or of the officers or agents by whom its powers are exerted, shall deny to any person within its jurisdiction the equal protection of the laws. Whoever, by virtue of public position under a State government, deprives another of property, life, or liberty, without due process of law, or denies or takes away the equal protection of the laws, violates the constitutional inhibition; and as he acts in the name and for the State, and is clothed with the State's power, his act is that of the State."
There are however instances where people are the victims of civil-rights violations that occur in circumstances involving both government officials and private actors. In the 1960s, the United States Supreme Court adopted an expansive view of state action opening the door to wide-ranging civil-rights litigation against private actors when they act as state actors (i.e., acts done or otherwise "sanctioned in some way" by the state). The Court found that the state action doctrine is equally applicable to denials of privileges or immunities, due process, and equal protection of the laws.
The critical factor in determining the existence of state action is not governmental involvement with private persons or private corporations, but "the inquiry must be whether there is a sufficiently close nexus between the State and the challenged action of the regulated entity so that the action of the latter may be fairly treated as that of the State itself." "Only by sifting facts and weighing circumstances can the nonobvious involvement of the State in private conduct be attributed its true significance."
The Supreme Court asserted that plaintiffs must establish not only that a private party "acted under color of the challenged statute, but also that its actions are properly attributable to the State. [...]" "And the actions are to be attributable to the State apparently only if the State compelled the actions and not if the State merely established the process through statute or regulation under which the private party acted."
The rules developed by the Supreme Court for business regulation are that (1) the "mere fact that a business is subject to state regulation does not by itself convert its action into that of the State for purposes of the Fourteenth Amendment," and (2) "a State normally can be held responsible for a private decision only when it has exercised coercive power or has provided such significant encouragement, either overt or covert, that the choice must be deemed to be that of the State."
Apportionment of representation in House of Representatives.
Section 2 altered the way each state's representation in the House of Representatives is determined. It counts all residents for apportionment, overriding of the Constitution, which counted only three-fifths of each state's slave population.
Section 2 also reduces a state's apportionment if it wrongfully denies any adult male's right to vote. However, this part of Section 2 was not enforced and so Southern states continued to use pretexts to prevent many blacks from voting until the passage of the Voting Rights Act of 1965. Abolitionist leaders criticized the amendment's failure to specifically prohibit the states from denying people the right to vote on the basis of race. Section 2 protects the right to vote only of adult males, not adult females, making it the only provision of the Constitution to explicitly discriminate on the basis of sex. Section 2 was condemned by women's suffragists, such as Elizabeth Cady Stanton and Susan B. Anthony, who had long seen their cause as linked to that of black rights. The separation of black civil rights from women's civil rights split the two movements for decades.
Some have argued that Section 2 was implicitly repealed by the Fifteenth Amendment, but the Supreme Court acknowledged the provisions of Section 2 in some later decisions. For example, in "Richardson v. Ramirez" (1974), the Court cited Section 2 as justifying the states disenfranchising felons. In "Hunter v. Underwood" (1985), a case involving disenfranchising black misdemeanants, the Supreme Court concluded that the Tenth Amendment cannot save legislation prohibited by the subsequently enacted Fourteenth Amendment. More specifically the Court concluded that laws passed with a discriminatory purpose are not excepted from the operation of the Equal Protection Clause by the "other crime" provision of Section 2. The Court held that Section 2 "was not designed to permit the purposeful racial discrimination [...] which otherwise violates [Section] 1 of the Fourteenth Amendment."
Participants in rebellion.
Section 3 prohibits the election or appointment to any federal or state office of any person who had held any of certain offices and then engaged in insurrection, rebellion or treason. However, a two-thirds vote by each House of the Congress can override this limitation. In 1898, the Congress enacted a general removal of Section 3's limitation. In 1975, the citizenship of Confederate general Robert E. Lee was restored by a joint congressional resolution, retroactive to June 13, 1865. In 1978, pursuant to Section 3, the Congress posthumously removed the service ban from Confederate president Jefferson Davis.
Section 3 was used to prevent Socialist Party of America member Victor L. Berger, convicted of violating the Espionage Act for his anti-militarist views, from taking his seat in the House of Representatives in 1919 and 1920.
Validity of public debt.
Section 4 confirmed the legitimacy of all public debt appropriated by the Congress. It also confirmed that neither the United States nor any state would pay for the loss of slaves or debts that had been incurred by the Confederacy. For example, during the Civil War several British and French banks had lent large sums of money to the Confederacy to support its war against the Union. In "Perry v. United States" (1935), the Supreme Court ruled that under Section 4 voiding a United States bond "went beyond the congressional power."
The debt-ceiling crises of 2011 and 2013 raised the question of what is the President's authority under Section 4. Some, such as legal scholar Garrett Epps, fiscal expert Bruce Bartlett and Treasury Secretary Timothy Geithner, have argued that a debt ceiling may be unconstitutional and therefore void as long as it interferes with the duty of the government to pay interest on outstanding bonds and to make payments owed to pensioners (that is, Social Security and Railroad Retirement Act recipients). Legal analyst Jeffrey Rosen has argued that Section 4 gives the President unilateral authority to raise or ignore the national debt ceiling, and that if challenged the Supreme Court would likely rule in favor of expanded executive power or dismiss the case altogether for lack of standing. Erwin Chemerinsky, professor and dean at University of California, Irvine School of Law, has argued that not even in a "dire financial emergency" could the President raise the debt ceiling as "there is no reasonable way to interpret the Constitution that [allows him to do so]". Jack Balkin, Knight Professor of Constitutional Law at Yale University, opined that like Congress the President is bound by the Fourteenth Amendment, for otherwise he could violate any part of the amendment at will. Because the President must obey the Section 4 requirement not to put the validity of the public debt into question, Balkin argued that President Obama is obliged "to prioritize incoming revenues to pay the public debt: interest on government bonds and any other 'vested' obligations. What falls into the latter category is not entirely clear, but a large number of other government obligations—and certainly payments for future services—would not count and would have to be sacrificed. This might include, for example, Social Security payments."
Power of enforcement.
Section 5, also known as the Enforcement Clause of the Fourteenth Amendment, enables Congress to pass laws enforcing the amendment's other provisions. In the "Civil Rights Cases" (1883), the Supreme Court interpreted Section 5 narrowly, stating that "the legislation which Congress is authorized to adopt in this behalf is not general legislation upon the rights of the citizen, but corrective legislation". In other words, the amendment authorizes Congress to pass laws only to combat violations of the rights protected in other sections.
In "Katzenbach v. Morgan" (1966), the Court upheld Section 4(e) of the Voting Rights Act of 1965, which prohibits certain forms of literacy requirements as a condition to vote, as a valid exercise of Congressional power under Section 5 to enforce the Equal Protection Clause. The Court ruled that Section 5 enabled Congress to act both remedially and prophylactically to protect the rights guaranteed by the amendment. However, in "City of Boerne v. Flores" (1997), the Court narrowed Congress's enforcement power, holding that Congress may not enact legislation under Section 5 that substantively defines or interprets Fourteenth Amendment rights. The Court ruled that legislation is valid under Section 5 only if there is a "congruence and proportionality" between the injury to a person's Fourteenth Amendment right and the means Congress adopted to prevent or remedy that injury.

</doc>
<doc id="31667" url="http://en.wikipedia.org/wiki?curid=31667" title="Fifteenth Amendment to the United States Constitution">
Fifteenth Amendment to the United States Constitution

The Fifteenth Amendment (Amendment XV) to the United States Constitution prohibits the federal and state governments from denying a citizen the right to vote based on that citizen's "race, color, or previous condition of servitude." It was ratified on February 3, 1870, as the third and last of the Reconstruction Amendments.
In the final years of the American Civil War and the Reconstruction Era that followed, Congress repeatedly debated the rights of the millions of black former slaves. By 1869, amendments had been passed to abolish slavery and provide citizenship and equal protection under the laws, but the election of Ulysses S. Grant to the presidency in 1868 convinced a majority of Republicans that protecting the franchise of black voters was important for the party's future. After rejecting more sweeping versions of a suffrage amendment, Congress proposed a compromise amendment banning franchise restrictions on the basis of race, color, or previous servitude on February 26, 1869. The amendment survived a difficult ratification fight and was adopted on March 30, 1870.
United States Supreme Court decisions in the late nineteenth century interpreted the amendment narrowly. From 1890 to 1910, most black voters in the South were effectively disenfranchised by new state constitutions and state laws incorporating such obstacles as poll taxes and discriminatory literacy tests, from which white voters were exempted by grandfather clauses. A system of whites-only primaries and violent intimidation by white groups also suppressed black participation.
In the twentieth century, the Court began to interpret the amendment more broadly, striking down grandfather clauses in "Guinn v. United States" (1915) and dismantling the white primary system in the "Texas primary cases" (1927–1953). Along with later measures such as the Twenty-fourth Amendment, which forbade poll taxes in federal elections, and "Harper v. Virginia State Board of Elections" (1966), which forbade poll taxes in state elections, these decisions significantly increased black participation in the American political system. To enforce the amendment, Congress enacted the Voting Rights Act of 1965, which provided federal oversight of elections in discriminatory jurisdictions, banned literacy tests and similar discriminatory devices, and created legal remedies for people affected by voting discrimination.
Background.
In the final years of the American Civil War and the Reconstruction Era that followed, Congress repeatedly debated the rights of black former slaves freed by the 1863 Emancipation Proclamation and the 1865 Thirteenth Amendment, the latter of which had formally abolished slavery. Following the passage of the Thirteenth Amendment by Congress, however, Republicans grew concerned over the increase it would create in the congressional representation of the Democratic-dominated Southern states. Because the full population of freed slaves would be now counted rather than the three-fifths mandated by the previous Three-Fifths Compromise, the Southern states would dramatically increase their power in the population-based House of Representatives. Republicans hoped to offset this advantage by attracting and protecting votes of the newly enfranchised black population.
In 1865, Congress passed what would become the Civil Rights Act of 1866, guaranteeing citizenship without regard to race, color, or previous condition of slavery or involuntary servitude. The bill also guaranteed equal benefits and access to the law, a direct assault on the Black Codes passed by many post-war Southern states. The Black Codes attempted to return ex-slaves to something like their former condition by, among other things, restricting their movement, forcing them to enter into year-long labor contracts, prohibiting them from owning firearms, and by preventing them from suing or testifying in court. Although strongly urged by moderates in Congress to sign the bill, President Johnson vetoed it on March 27, 1866. In his veto message, he objected to the measure because it conferred citizenship on the freedmen at a time when 11 out of 36 states were unrepresented in the Congress, and that it discriminated in favor of African Americans and against whites. Three weeks later, Johnson's veto was overridden and the measure became law. This was the first time in American history that Congress was able to muster the votes necessary to override a presidential veto. Despite this victory, even some Republicans who had supported the goals of the Civil Rights Act began to doubt that Congress possessed the constitutional power to turn those goals into laws. The experience encouraged both radical and moderate Republicans to seek Constitutional guarantees for black rights, rather than relying on temporary political majorities.
On June 18, 1866, Congress adopted the Fourteenth Amendment, which guaranteed citizenship and equal protection under the laws regardless of race, and sent it to the states for ratification. After a bitter struggle that included attempted rescissions of ratification by two states, the Fourteenth Amendment was adopted on July 28, 1868.
The second section of the Fourteenth Amendment punished, by reduced representation in the House of Representatives, any states that disenfranchised any male citizens over 21 years of age. By failing to adopt a harsher penalty, this signaled to the states that they still possessed the right to deny ballot access to blacks. Northern states were generally as adverse to granting voting rights to blacks as Southern states. In the year of its ratification, only eight Northern states allowed blacks to vote. In the South, blacks were able to vote in many areas, but only through the intervention of the occupying Union Army.
Proposal and ratification.
Proposal.
Anticipating an increase in Democratic membership in the following Congress, Republicans used the lame-duck session of the 40th United States Congress to pass an amendment protecting black suffrage. Representative John Bingham, the primary author of the Fourteenth Amendment, pushed for a wide-ranging ban on suffrage limitations, but a broader proposal banning voter restriction on the basis of "race, color, nativity, property, education, or religious beliefs" was rejected. A proposal to specifically ban literacy tests was also rejected. Some Representatives from the North, where nativism was a major force, wished to preserve restrictions denying the franchise to foreign-born citizens, as did Representatives from the West, where ethnic Chinese were banned from voting. Both Southern and Northern Republicans also wanted to continue to deny the vote temporarily to Southerners disfranchised for support of the Confederacy, and they were concerned that a sweeping endorsement of suffrage would enfranchise this group.
A House and Senate conference committee proposed the amendment's final text, which banned voter restriction only on the basis of "race, color, or previous condition of servitude". To attract the broadest possible base of support, the amendment made no mention of poll taxes or other measures to block voting, and did not guarantee the right of blacks to hold office. This compromise proposal was approved by the House on February 25, 1869, and the Senate the following day.
The vote in the House was 144 to 44, with 35 not voting. The House vote was almost entirely along party lines, with no Democrats supporting the bill and only 3 Republicans voting against it. The House of Representatives passed the amendment with 143 Republican & 1 Conservative Republican votes of "Yea", 39 Democrat, 3 Republican, 1 Independent Republican & 1 Conservative votes of "Nay" and with 26 Republican, 8 Democrat & 1 Independent Republican not voting. The final vote in the Senate was 39 to 13, with 14 not voting. The Senate passed the amendment with a vote of 39 Republican votes of "Yea", 8 Democrat & 5 Republican votes of "Nay" and with 13 Republican & 1 Democrat not voting. Some Radicals, such as Massachusetts Senator Charles Sumner, abstained from voting because the amendment did not prohibit literacy tests and poll taxes. Following congressional approval the proposed amendment was then sent by Secretary of State William Henry Seward to the states for ratification or rejection.
Ratification.
Though many of the original proposals for the amendment had been moderated by negotiations in committee, the final draft nonetheless faced significant hurdles in being ratified by three-fourths of the states. Historian William Gillette wrote of the process, "it was hard going and the outcome was uncertain until the very end."
One source of opposition to the proposed amendment was the women's suffrage movement, which before and during the Civil War had made common cause with the abolitionist movement. However, with the passage of the Fourteenth Amendment, which had explicitly protected only male citizens in its second section, activists found the civil rights of women divorced from those of blacks. Matters came to a head with the proposal of the Fifteenth Amendment, which barred race discrimination but not gender discrimination in voter laws. After an acrimonious debate, the American Equal Rights Association, the nation's leading suffragist group, split into two rival organizations: the National Woman Suffrage Association of Susan B. Anthony and Elizabeth Cady Stanton, who opposed the amendment, and the American Woman Suffrage Association of Lucy Stone and Henry Browne Blackwell, who supported it. The two groups remained divided until the 1890s.
Nevada was the first state to ratify the amendment, on March 1, 1869. The New England states and most Midwest states also ratified the amendment soon after its proposal. Southern states still controlled by Radical reconstruction governments, such as North Carolina, also swiftly ratified. Newly elected U.S. President Ulysses S. Grant (a Republican) strongly endorsed the amendment, calling it "a measure of grander importance than any other one act of the kind from the foundation of our free government to the present day." He privately asked Nebraska's governor to call a special legislative session to speed the process, securing the state's ratification. In April and December 1869, Congress passed Reconstruction bills mandating that Virginia, Mississippi, Texas and Georgia ratify the amendment as a precondition to regaining congressional representation; all four states did so. The struggle for ratification was particularly close in Indiana and Ohio, which voted to ratify in May 1869 and January 1870, respectively. New York, which had ratified on April 14, 1869, tried to revoke its ratification on January 5, 1870. However, in February 1870, Georgia, Iowa, Nebraska, and Texas ratified the amendment, bringing the total ratifying states to twenty-nine—one more than the required twenty-eight ratifications from the thirty-seven states, and forestalling any court challenge to New York's resolution to withdraw its consent.
The first twenty-eight states to ratify the Fifteenth Amendment were: 
Secretary of State Hamilton Fish certified the amendment on March 30, 1870. New York, despite rescinding its ratification, was included as a ratifying state, as were: 
The remaining seven states all subsequently ratified the amendment:
The amendment's adoption was met with widespread celebrations in black communities and abolitionist societies; many of the latter disbanded, feeling that black rights had been secured and their work was complete. President Grant said of the amendment that it "completes the greatest civil change and constitutes the most important event that has occurred since the nation came to life". Many Republicans felt that with the amendment's passage, black Americans no longer needed federal protection; congressman and future president James A. Garfield stated that the amendment's passage "confers upon the African race the care of its own destiny. It places their fortunes in their own hands." Congressman John R. Lynch later wrote that ratification of those two amendments made Reconstruction a success.
Application.
Reconstruction.
The first known black voter after the amendment's adoption was Thomas Mundy Peterson, who cast his ballot on March 31, 1870 in the Perth Amboy, New Jersey mayoral election.
In "United States v. Reese" (1876), the first U.S. Supreme Court decision interpreting the Fifteenth Amendment, the Court interpreted the amendment narrowly, upholding ostensibly race-neutral limitations on suffrage including poll taxes, literacy tests, and a grandfather clause that exempted citizens from other voting requirements if their grandfathers had been registered voters, a condition only whites could generally meet. The Court also stated that the amendment does not confer the right of suffrage, but it invests citizens of the United States with the right of exemption from discrimination in the exercise of the elective franchise on account of their race, color, or previous condition of servitude, and empowers Congress to enforce that right by "appropriate legislation." The Court wrote:
The Fifteenth Amendment does not confer the right of suffrage upon any one. It prevents the States, or the United States, however, from giving preference, in this particular, to one citizen of the United States over another on account of race, color, or previous condition of servitude. Before its adoption, this could be done. It was as much within the power of a State to exclude citizens of the United States from voting on account of race, &c., as it was on account of age, property, or education. Now it is not. If citizens of one race having certain qualifications are permitted by law to vote, those of another having the same qualifications must be. Previous to this amendment, there was no constitutional guaranty against this discrimination: now there is. It follows that the amendment has invested the citizens of the United States with a new constitutional right which is within the protecting power of Congress. That right is exemption from discrimination in the exercise of the elective franchise on account of race, color, or previous condition of servitude. This, under the express provisions of the second section of the amendment, Congress may enforce by "appropriate legislation."
White supremacists such as the Ku Klux Klan (KKK) used paramilitary violence to prevent black people from voting. A number of blacks were killed at the Colfax massacre of 1873 while attempting to defend their right to vote. The Enforcement Acts were passed by Congress in 1870–1871 to authorize federal prosecution of the KKK and others who violated the amendment. However, as Reconstruction neared its end and federal troops withdrew, prosecutions under the Enforcement Acts dropped significantly. In "United States v. Cruikshank" (1876), the Supreme Court ruled that the federal government did not have the authority to prosecute the perpetrators of the Colfax massacre because they were not state actors.
 
Congress removed a provision against conspiracy from the acts in 1894, weakening them further. In 1877, Republican Rutherford B. Hayes was elected president after a highly contested election, receiving support from three Southern states in exchange for a pledge to allow white Democratic governments to rule without federal interference. As president, he refused to enforce federal civil rights protections, allowing states to begin to implement racially discriminatory Jim Crow laws. A Federal Elections Bill was successfully filibustered in the Senate..
Post-reconstruction.
From 1890 to 1910, poll taxes and literacy tests were instituted across the South, effectively disenfranchising the great majority of blacks. White-only primary elections also served to reduce the influence of blacks in the political system. Along with increasing legal obstacles, blacks were excluded from the political system by threats of violent reprisals by whites in the form of lynch mobs and terrorist attacks by the Ku Klux Klan.
In the 20th century, the Court began to read the Fifteenth Amendment more broadly. In "Guinn v. United States" (1915), a unanimous Court struck down an Oklahoma grandfather clause that effectively exempted white voters from a literacy test, finding it to be discriminatory. The Court ruled in the related case "Myers v. Anderson" (1915), that the officials who enforced such a clause were liable for civil damages.
The Court addressed the white primary system in a series of decisions later known as the "Texas primary cases". In "Nixon v. Herndon" (1927), Nixon sued for damages under federal civil rights laws after being denied a ballot in a Democratic party primary election on the basis of race. The Court found in his favor on the basis of the Fourteenth Amendment, which guarantees equal protection under the law, while not discussing his Fifteenth Amendment claim. After Texas amended its statute to allow the political party's state executive committee to set voting qualifications, Nixon sued again; in "Nixon v. Condon" (1932), the Court again found in his favor on the basis of the Fourteenth Amendment.
Following "Nixon", the Democratic Party's state convention instituted a rule that only whites could vote in its primary elections; the Court unanimously upheld this rule as constitutional in "Grovey v. Townsend" (1935), distinguishing the discrimination by a private organization from that of the state in the previous primary cases. However, in "United States v. Classic" (1941), the Court ruled that primary elections were an essential part of the electoral process, undermining the reasoning in "Grovey". Based on "Classic", the Court in "Smith v. Allwright" (1944), overruled "Grovey", ruling that denying non-white voters a ballot in primary elections was a violation of the Fifteenth Amendment. In the last of the Texas primary cases, "Terry v. Adams" (1953), the Court ruled that black plaintiffs were entitled to damages from a group that organized whites-only pre-primary elections with the assistance of Democratic party officials.
The Court also used the amendment to strike down a gerrymander in "Gomillion v. Lightfoot" (1960). The decision found that the redrawing of city limits by Tuskegee, Alabama officials to exclude the mostly black area around the Tuskegee Institute discriminated on the basis of race. The Court later relied on this decision in "Rice v. Cayetano" (2000), which struck down ancestry-based voting in elections for the Office of Hawaiian Affairs; the ruling held that the elections violated the Fifteenth Amendment by using "ancestry as a racial definition and for a racial purpose".
After judicial enforcement of the Fifteenth Amendment ended grandfather clauses, white primaries, and other discriminatory tactics, Southern black voter registration gradually increased, rising from five percent in 1940 to twenty-eight percent in 1960. Although the Fifteenth Amendment was never interpreted to prohibit poll taxes, in 1962 the Twenty-fourth Amendment was adopted banning poll taxes in federal elections, and in 1966 the Supreme Court ruled in "Harper v. Virginia State Board of Elections" (1966) that state poll taxes violate the Fourteenth Amendment's Equal Protection Clause.
Under its authority pursuant to Section 2 of the Fifteenth Amendment, Congress passed the Voting Rights Act of 1965 to achieve further racial equality in voting. Sections 4 and 5 of the Voting Rights Act required states and local governments with histories of racial discrimination in voting to submit all changes to their voting laws or practices to the federal government for approval before they could take effect, a process called "preclearance." By 1976, sixty-three percent of Southern blacks were registered to vote, only five percent less than Southern whites.
The Supreme Court originally upheld these provisions as constitutional in "South Carolina v. Katzenbach" (1966). However, in "Shelby County v. Holder" (2013), the Supreme Court ruled that Section 4(b) of the Voting Rights Act, which established the coverage formula that determined which jurisdictions were subject to preclearance, was no longer constitutional and exceeded Congress's enforcement authority under Section 2 of the Fifteenth Amendment. The Court declared that the Fifteenth Amendment "commands that the right to vote shall not be denied or abridged on account of race or color, and it gives Congress the power to enforce that command. The Amendment is not designed to punish for the past; its purpose is to ensure a better future." According to the Court, "Regardless of how to look at the record no one can fairly say that it shows anything approaching the 'pervasive,' 'flagrant,' 'widespread,' and 'rampant' discrimination that faced Congress in 1965, and that clearly distinguished the covered jurisdictions from the rest of the nation." While the preclearance provision itself was not struck down, it will continue to be inoperable unless Congress passes a new coverage formula.

</doc>
<doc id="31668" url="http://en.wikipedia.org/wiki?curid=31668" title="Sixteenth Amendment to the United States Constitution">
Sixteenth Amendment to the United States Constitution

The Sixteenth Amendment (Amendment XVI) to the United States Constitution allows the Congress to levy an income tax without apportioning it among the states or basing it on the United States Census. This amendment exempted income taxes from the constitutional requirements regarding direct taxes, after income taxes on rents, dividends, and interest were ruled to be direct taxes in the court case of "Pollock v. Farmers' Loan & Trust Co." (1895). The amendment was adopted on February 3, 1913.
Text.
The Congress shall have power to lay and collect taxes on incomes, from whatever source derived, without apportionment among the several States, and without regard to any census or enumeration.
Other Constitutional provisions regarding taxes.
Article I, Section 2, Clause 3:
Representatives and direct taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers...
Article I, Section 8, Clause 1:
The Congress shall have Power to lay and collect Taxes, Duties, Imposts and Excises, to pay the Debts and provide for the common Defence and general Welfare of the United States; but all Duties, Imposts and Excises shall be uniform throughout the United States.
Article I, Section 9, Clause 4:
No Capitation, or other direct, Tax shall be laid, unless in proportion to the Census or Enumeration herein before directed to be taken.
This clause basically refers to a tax on property, such as a tax based on the value of land, as well as a capitation.
Income taxes before the "Pollock" case.
Until 1913, customs duties (tariffs) and excise taxes were the primary sources of federal revenue. During the War of 1812, Secretary of the Treasury A.J. Dallas made the first public proposal for an income tax, but it was never implemented. The Congress did introduce an income tax to fund the Civil War through the Revenue Act of 1861. It levied a flat tax of three percent on annual income above $800. This act was replaced the following year with the Revenue Act of 1862, which levied a graduated tax of three to five percent on income above $600 and specified a termination of income taxation in 1866. The Civil War income taxes, which expired in 1872, proved to be both highly lucrative and drawing mostly from the more industrialized states, with New York, Pennsylvania, and Massachusetts generating about 60 percent of the total revenue that was collected. During the two decades following the expiration of the Civil War income tax, the Greenback movement, the Labor Reform Party, the Populist Party, the Democratic Party and many others called for a graduated income tax.
The Socialist Labor Party advocated a graduated income tax in 1887. The Populist Party "demand[ed] a graduated income tax" in its 1892 platform. The Democratic Party, led by William Jennings Bryan, advocated the income tax law passed in 1894, and proposed an income tax in its 1908 platform.
Before "Pollock v. Farmers' Loan & Trust Co.", all income taxes had been considered to be indirect taxes imposed without respect to geography, unlike direct taxes, that have to be apportioned among the states according to population.
The "Pollock" case.
In 1894, an amendment was attached to the Wilson–Gorman Tariff Act that attempted to impose a federal tax of two percent on incomes over $4,000 (equal to $<br>{Inflation} - Amount must not have "" prefix: 4.  ,000 in 2014). The federal income tax was strongly favored in the South, and it was moderately supported in the eastern North Central states, but it was strongly opposed in the Far West and the Northeastern States (with the exception of New Jersey). The tax was derided as "un-Democratic, inquisitorial, and wrong in principle."
In "Pollock v. Farmers' Loan & Trust Co.", the U.S. Supreme Court declared certain taxes on incomes — such as those on property under the 1894 Act — to be unconstitutionally unapportioned direct taxes. The Court reasoned that a tax on "income from property" should be treated as a tax on "property by reason of its ownership" and so should be required to be apportioned. The reasoning was that taxes on the rents from land, the dividends from stocks, and so forth, burdened the property generating the income in the same way that a tax on "property by reason of its ownership" burdened that property.
After "Pollock", while income taxes on wages (as indirect taxes) were still not required to be apportioned by population, taxes on interest, dividends, and rental income were required to be apportioned by population. The "Pollock" ruling made the "source of the income" (e.g., property versus labor, etc.) relevant in determining whether the tax imposed on that income was deemed to be "direct" (and thus required to be apportioned among the states according to population) or, alternatively, "indirect" (and thus required only to be imposed with geographical uniformity).
Dissenting in "Pollock", Justice John Marshall Harlan stated:When, therefore, this court adjudges, as it does now adjudge, that Congress cannot impose a duty or tax upon personal property, or upon income arising either from rents of real estate or from personal property, including invested personal property, bonds, stocks, and investments of all kinds, except by apportioning the sum to be so raised among the States according to population, it practically decides that, without an amendment of the Constitution — two-thirds of both Houses of Congress and three-fourths of the States concurring — such property and incomes can never be made to contribute to the support of the national government.
Members of Congress responded to "Pollock" by expressing widespread concern that many of the wealthiest Americans had consolidated too much economic power.
Adoption.
On June 16, 1909, President William Howard Taft, in an address to the Sixty-first Congress, proposed a two percent federal income tax on corporations by way of an excise tax and a constitutional amendment to allow the previously enacted income tax.Upon the privilege of doing business as an artificial entity and of freedom from a general partnership liability enjoyed by those who own the stock.
An income tax amendment to the Constitution was first proposed by Senator Norris Brown of Nebraska. He submitted two proposals, Senate Resolutions Nos. 25 and 39. The amendment proposal finally accepted was Senate Joint Resolution No. 40, introduced by Senator Nelson W. Aldrich of Rhode Island, the Senate majority leader and Finance Committee Chairman.
On July 12, 1909, the resolution proposing the Sixteenth Amendment was passed by the Congress and was submitted to the state legislatures. Support for the income tax was strongest in the western and southern states and opposition was strongest in the northeastern states. Supporters of the income tax believed that it would be a much better method of gathering revenue than tariffs, which were the primary source of revenue at the time. From well before 1894, Democrats, Progressives, Populists and other left-oriented parties argued that tariffs disproportionately affected the poor, interfered with prices, were unpredictable, and were an intrinsically limited source of revenue. The South and the West tended to support income taxes because their residents were generally less prosperous, more agricultural and more sensitive to fluctuations in commodity prices. A sharp rise in the cost of living between 1897 and 1913 greatly increased support for the idea of income taxes, including in the urban Northeast. A growing number of Republicans also began supporting the idea, notably Theodore Roosevelt and the "Insurgent" Republicans (who would go on to form the Progressive Party). These Republicans were driven mainly by a fear of the increasingly large and sophisticated military forces of Japan, Britain and the European powers, their own imperial ambitions and the perceived need to defend American merchant ships. Moreover, these progressive Republicans were, as the name suggests, convinced that central governments could play a positive role in national economies. A bigger government and a bigger military, of course, required a correspondingly larger and steadier source of revenue to support it.
Opposition to the Sixteenth Amendment was led by establishment Republicans because of their close ties to wealthy industrialists, although not even they were uniformly opposed to the general idea of a permanent income tax. In 1910, New York Governor Charles Evans Hughes, shortly before becoming a Supreme Court Justice, spoke out against the income tax amendment. While he supported the idea of a federal income tax, Hughes believed the words "from whatever source derived" in the proposed amendment implied that the federal government would have the power to tax state and municipal bonds. He believed this would excessively centralize governmental power and "would make it impossible for the state to keep any property".
Between 1909 and 1913, several conditions favored passage of the Sixteenth Amendment. Inflation was high and many blamed federal tariffs for the rising prices. The Republican Party was divided and weakened by the loss of Roosevelt and the Insurgents who joined the Progressive party, a problem that blunted opposition even in the Northeast. The Democrats won both houses and the Presidency in 1912 and the country was generally in a left-leaning mood, with the Socialist Party winning a seat in the House in 1910 and polling six percent of the popular presidential vote in 1912.
Three advocates for a federal income tax ran in the presidential election of 1912. On February 25, 1913, Secretary of State Philander Knox proclaimed that the amendment had been ratified by three-fourths of the states and so had become part of the Constitution. The Revenue Act of 1913 was enacted shortly thereafter.
According to the United States Government Printing Office, the following states ratified the amendment:
Ratification (by the requisite 36 states) was completed on February 3, 1913 with the ratification by Delaware. The amendment was subsequently ratified by the following states, bringing the total number of ratifying states to forty-two of the forty-eight then existing:
The legislatures of the following states rejected the amendment without ever subsequently ratifying it:
The legislatures of the following states never considered the proposed amendment:
"Pollock" overruled.
The Sixteenth Amendment removed the precedent set by the "Pollock" decision.
Professor Sheldon D. Pollack at the University of Delaware has written:
From William D. Andrews, Professor of Law, Harvard Law School:
From Professor Boris Bittker, who was a tax law professor at Yale Law School:
Professor Erik Jensen at Case Western Reserve University Law School has written:
Professor Calvin H. Johnson, a tax professor at the University of Texas School of Law, has written:
From Gale Ann Norton:
From Alan O. Dixler:
Congress may impose taxes on income from any source without having to apportion the total dollar amount of tax collected from each state according to each state's population in relation to the total national population.
In "Wikoff v. Commissioner", the United States Tax Court said:[I]t is immaterial, with respect to Federal income taxes, whether the tax is a direct or an indirect tax. Mr. Wikoff [the taxpayer] relied on the Supreme Court's decision in Pollock v. Farmers' Loan & Trust Co. [ . . . ] but the effect of that decision has been nullified by the enactment of the 16th Amendment.
In "Abrams v. Commissioner", the Tax Court said:Since the ratification of the Sixteenth Amendment, it is immaterial with respect to income taxes, whether the tax is a direct or indirect tax. The whole purpose of the Sixteenth Amendment was to relieve all income taxes when imposed from [the requirement of] apportionment and from [the requirement of] a consideration of the source whence the income was derived.
Case law.
The federal courts' interpretations of the Sixteenth Amendment have changed considerably over time and there have been many disputes about the applicability of the amendment.
The "Brushaber" case.
In "Brushaber v. Union Pacific Railroad", 240 U.S. (1916), the Supreme Court ruled that (1) the Sixteenth Amendment removes the "Pollock" requirement that certain income taxes (such as taxes on income "derived from real property" that were the subject of the "Pollock" decision), be apportioned among the states according to population; (2) the federal income tax statute does not violate the Fifth Amendment's prohibition against the government taking property without due process of law; (3) the federal income tax statute does not violate the Article I, Section 8, Clause 1 requirement that excises, also known as indirect taxes, be imposed with geographical uniformity.
The "Kerbaugh-Empire Co". case.
In "Bowers v. Kerbaugh-Empire Co.", 271 U.S. (1926), the Supreme Court, through Justice Pierce Butler, stated:It was not the purpose or the effect of that amendment to bring any new subject within the taxing power. Congress already had the power to tax all incomes. But taxes on incomes from some sources had been held to be "direct taxes" within the meaning of the constitutional requirement as to apportionment. [citations omitted] The Amendment relieved from that requirement and obliterated the distinction in that respect between taxes on income that are direct taxes and those that are not, and so put on the same basis all incomes "from whatever source derived". [citations omitted] "Income" has been taken to mean the same thing as used in the Corporation Excise Tax of 1909 (36 Stat. 112), in the Sixteenth Amendment, and in the various revenue acts subsequently passed. [citations omitted] After full consideration, this court declared that income may be defined as gain derived from capital, from labor, or from both combined, including profit gained through sale or conversion of capital.
The "Glenshaw Glass" case.
In "Commissioner v. Glenshaw Glass Co.", 348 U.S. (1955), the Supreme Court laid out what has become the modern understanding of what constitutes "gross income" to which the Sixteenth Amendment applies, declaring that income taxes could be levied on "accessions to wealth, clearly realized, and over which the taxpayers have complete dominion." Under this definition, "any" increase in wealth — whether through wages, benefits, bonuses, sale of stock or other property at a profit, bets won, lucky finds, awards of punitive damages in a lawsuit, qui tam actions — are all within the definition of income, unless the Congress makes a specific exemption, as it has for items such as life insurance proceeds received by reason of the death of the insured party, gifts, bequests, devises and inheritances, and certain scholarships.
Income taxation of wages, etc..
Federal courts have ruled that the Sixteenth Amendment allows a direct tax on "wages, salaries, commissions, etc. without apportionment."
The "Penn Mutual" case.
Although the Sixteenth Amendment is often cited as the "source" of the congressional power to tax incomes, at least one court has reiterated the point made in "Brushaber" and other cases that the Sixteenth Amendment itself did not grant the Congress the power to tax incomes, a power the Congress had since 1789, but only removed the possible requirement that any income tax be apportioned among the states according to their respective populations. In "Penn Mutual Indemnity", the United States Tax Court stated:
In dealing with the scope of the taxing power the question has sometimes been framed in terms of whether something can be taxed as income under the Sixteenth Amendment. This is an inaccurate formulation... and has led to much loose thinking on the subject. The source of the taxing power is not the Sixteenth Amendment; it is Article I, Section 8, of the Constitution.
The United States Court of Appeals for the Third Circuit agreed with the Tax Court, stating:
It did not take a constitutional amendment to entitle the United States to impose an income tax. Pollock v. Farmers' Loan & Trust Co., 157 U. S. 429, 158 U. S. 601 (1895), only held that a tax on the income derived from real or personal property was so close to a tax on that property that it could not be imposed without apportionment. The Sixteenth Amendment removed that barrier. Indeed, the requirement for apportionment is pretty strictly limited to taxes on real and personal property and capitation taxes.
It is not necessary to uphold the validity of the tax imposed by the United States that the tax itself bear an accurate label. Indeed, the tax upon the distillation of spirits, imposed very early by federal authority, now reads and has read in terms of a tax upon the spirits themselves, yet the validity of this imposition has been upheld for a very great many years.
It could well be argued that the tax involved here [an income tax] is an "excise tax" based upon the receipt of money by the taxpayer. It certainly is not a tax on property and it certainly is not a capitation tax; therefore, it need not be apportioned. We do not think it profitable, however, to make the label as precise as that required under the Food and Drug Act. Congress has the power to impose taxes generally, and if the particular imposition does not run afoul of any constitutional restrictions then the tax is lawful, call it what you will.
The "Murphy" case.
On December 22, 2006, a three-judge panel of the United States Court of Appeals for the District of Columbia Circuit vacated its unanimous decision (of August 2006) in "Murphy v. Internal Revenue Service and United States". In an unrelated matter, the court had also granted the government's motion to dismiss Murphy's suit against the "Internal Revenue Service." Under federal sovereign immunity, a taxpayer may sue the federal government, but not a government agency, officer, or employee (with some exceptions). The Court ruled:
Insofar as the Congress has waived sovereign immunity with respect to suits for tax refunds under  , that provision specifically contemplates only actions against the "United States". Therefore, we hold the IRS, unlike the United States, may not be sued "eo nomine" in this case.
An exception to federal sovereign immunity is in the United States Tax Court, in which a taxpayer may sue the Commissioner of Internal Revenue. The original three-judge panel then agreed to rehear the case itself. In its original decision, the Court had ruled that 26 U.S.C.  was unconstitutional under the Sixteenth Amendment to the extent that the statute purported to tax, as income, a recovery for a nonphysical personal injury for mental distress and loss of reputation not received in lieu of taxable income such as lost wages or earnings.
Because the August 2006 opinion was vacated, the Court of Appeals did not hear the case "en banc".
On July 3, 2007, the Court (through the original three-judge panel) ruled (1) that the taxpayer's compensation was received on account of a nonphysical injury or sickness; (2) that gross income under section 61 of the Internal Revenue Code does include compensatory damages for nonphysical injuries, even if the award is not an "accession to wealth," (3) that the income tax imposed on an award for nonphysical injuries is an indirect tax, regardless of whether the recovery is restoration of "human capital," and therefore the tax does not violate the constitutional requirement of Article I, Section 9, Clause 4, that capitations or other direct taxes must be laid among the states only in proportion to the population; (4) that the income tax imposed on an award for nonphysical injuries does not violate the constitutional requirement of Article I, Section 8, Clause 1, that all duties, imposts and excises be uniform throughout the United States; (5) that under the doctrine of sovereign immunity, the Internal Revenue Service may not be sued in its own name.
The Court stated that "[a]lthough the 'Congress cannot make a thing income which is not so in fact,' [ . . . ] it can "label" a thing income and tax it, so long as it acts within its constitutional authority, which includes not only the Sixteenth Amendment but also Article I, Sections 8 and 9." The court ruled that Ms. Murphy was not entitled to the tax refund she claimed, and that the personal injury award she received was "within the reach of the Congressional power to tax under Article I, Section 8 of the Constitution" – even if the award was "not income within the meaning of the Sixteenth Amendment". See also the "Penn Mutual" case cited above.
On April 21, 2008, the U.S. Supreme Court declined to review the decision by the Court of Appeals.

</doc>
<doc id="31669" url="http://en.wikipedia.org/wiki?curid=31669" title="Seventeenth Amendment to the United States Constitution">
Seventeenth Amendment to the United States Constitution

The Seventeenth Amendment (Amendment XVII) to the United States Constitution established the election of United States Senators by the people of the states. The amendment supersedes , Clauses 1 and 2 of the Constitution, under which senators were elected by state legislatures. It also alters the procedure for filling vacancies in the Senate, allowing for state legislatures to permit their governors to make temporary appointments until a special election can be held. Under the original provisions of the Constitution, senators were elected by state legislatures; this was intended to prevent the federal government from indirectly absconding with the powers and funds of the states. However, over time various issues with these provisions, such as the risk of corruption and the potential for electoral deadlocks or a lack of representation should a seat become vacant, led to a campaign for reform.
Reformers introduced constitutional amendments in 1828, 1829, and 1855, with the issues finally reaching a head during the 1890s and 1900s. Progressives, such as William Jennings Bryan, called for reform to the way senators were chosen. Elihu Root and George Frisbie Hoar were prominent figures in the campaign to maintain the state legislative selection of senators. By 1910, 31 state legislatures had passed motions calling for reform. By 1912, 239 political parties at both the state and national level had pledged some form of direct election, and 33 states had introduced the use of direct primaries. With a campaign for a state-led constitutional amendment gaining strength, and a fear that this could result in a "runaway convention", the proposal to mandate direct elections for the Senate was finally introduced in the Congress. It was passed by the Congress and, on May 13, 1912, was submitted to the states for ratification. By April 8, 1913, three-fourths of the states had ratified the proposed amendment, making it the Seventeenth Amendment. Secretary of State William Jennings Bryan formally declared the amendment's adoption on May 31, 1913.
Critics of the Seventeenth Amendment claim that by altering the way senators are elected, the states lost any representation they had in the federal government and that, in addition to violating the unamendable state suffrage clause of Article V, this led to the gradual "slide into ignominy" of state legislatures, as well as an overextension of federal power and the rise of special interest groups to fill the power vacuum previously occupied by state legislatures. Additionally, concerns have been raised about the power of governors to appoint temporary replacements to fill vacant senate seats, both in terms of how this provision should be interpreted and whether it should be permitted at all. Accordingly, noted public figures have expressed a desire to reform the Seventeenth Amendment, while a few politicians have called for outright repeal of the amendment.
Text.
The Senate of the United States shall be composed of two Senators from each State, elected by the people thereof, for six years; and each Senator shall have one vote. The electors in each State shall have the qualifications requisite for electors of the most numerous branch of the State legislatures.
When vacancies happen in the representation of any State in the Senate, the executive authority of such State shall issue writs of election to fill such vacancies: Provided, That the legislature of any State may empower the executive thereof to make temporary appointments until the people fill the vacancies by election as the legislature may direct.
This amendment shall not be so construed as to affect that the election or term of any Senator chosen before it becomes valid as part of the Constitution.
Background.
Original composition.
Originally, under , Clauses 1 and 2 of the Constitution, each state legislature elected its state's senators for a six-year term. Each state, regardless of size, is entitled to two senators as part of the Connecticut Compromise between the small and large states. This contrasted with the House of Representatives, a body elected by popular vote, and was described as an uncontroversial decision to make; James Wilson was the sole advocate of popularly electing the Senate and his proposal was defeated 10–1. There were many advantages to the original method of electing senators. Prior to the Constitution, a federal body was one where states effectively formed nothing more than permanent treaties, with citizens retaining their loyalty to their original state. However, under the Constitution the states were subordinated to a central government; the election of senators by the states reassured Antifederalists that there would be some protection against the swallowing up of states and their powers by an ever-expanding federal government, providing a check on the power of the federal government.
Additionally, the longer terms and avoidance of popular election turned the Senate into a body to "temper" the populism of the House. While the Representatives existed in a two-year direct election cycle, the senators could afford to "take a more detached view of issues coming before Congress". State legislatures also retained the theoretical right to "instruct" their senators to vote for or against proposals, giving them both direct and indirect representation in the federal government. The Senate also provided formal bicameralism, with the members of the Senate and House responsible to completely distinct constituencies; this helped defeat the problem of the federal government being subject to "special interests". Members of the Constitutional Convention also saw it as an equivalent to the British House of Lords, containing the "better men" of society; it was hoped that they would provide more coolness and stability than the House of Representatives due to the senators' status.
Issues.
According to Judge Jay Bybee of the United States Court of Appeals for the Ninth Circuit, those in favor of popular elections for senators felt that there were primarily two problems caused by the original provisions: legislative corruption and electoral deadlocks. In terms of corruption, the general feeling was that senatorial elections were "bought and sold", changing hands for favors and sums of money rather than because of the competence of the candidate. Between 1857 and 1900, the Senate investigated three elections over corruption. In 1900, for example, William A. Clark had his election voided after the Senate concluded that he had bought votes in the Montana legislature. However, Bybee and Todd Zywicki believe this concern was largely unfounded; there was a "dearth of hard information" on the subject, and in over a century of elections, only 10 were contested with allegations of impropriety.
Electoral deadlocks were another issue. Because state legislatures were charged with deciding who to appoint as senators, the system relied on them being able to agree. Some states could not, and thus delayed sending representatives to Congress; in a few cases, the system broke down to the point where states completely lacked representation. Deadlocks started to become an issue in the 1850s, with a dead-locked Indiana legislature allowing a Senate seat to sit vacant for two years. Between 1891 and 1905, 46 elections were deadlocked, in 20 different states; in one extreme example, a Senate seat for Delaware went unfilled from 1899 until 1903. The business of holding elections also caused great disruption in the state legislatures, with a full third of the Oregon House of Representatives choosing not to swear the oath of office in 1897 due to a dispute over an open Senate seat. The result was that the legislature was unable to pass legislation that year.
Zywicki again argues that this was not a serious issue. Deadlocks were a problem, but they were the exception rather than the norm; many legislatures did not deadlock over elections at all. Most of those that did in the 19th century were the newly admitted western states, which suffered from "inexperienced legislatures and weak party discipline...as western legislatures gained experience, deadlocks became less frequent." While Utah suffered from deadlocks in 1897 and 1899, they became "a good teaching experience," and Utah never again failed to elect senators. Another concern was that when deadlocks occurred, state legislatures found themselves unable to conduct their normal business; James Christian Ure, writing in the "South Texas Law Review", notes that this did not in fact occur. In a deadlock situation, state legislatures would deal with the matter by holding "one vote at the beginning of the day—then the legislators would continue with their normal affairs".
There was also a feeling that state legislative elections themselves had become dominated by the business of picking senators. Senator John H. Mitchell noted that the Senate became the "vital issue" in all legislative campaigns, with the policy stances and qualifications of state legislative candidates ignored by voters who were more interested in the indirect Senate election. To remedy this, some state legislatures created "advisory elections" that served as de facto general elections, allowing legislative campaigns to focus on local issues.
Calls for reform.
Calls for a constitutional amendment regarding Senate elections started in the early 19th century, with Henry R. Storrs in 1826 proposing an amendment to provide for popular election. Similar amendments were introduced in 1829 and 1855, with the "most prominent" proponent being Andrew Johnson, who raised the issue in 1868 and considered the idea's merits "so palpable" that no additional explanation was necessary. The 1860s also saw the first major Congressional disputes over the issue, with the House and Senate voting to veto the appointment of John P. Stockton to the Senate due to his approval by a plurality rather than a majority. In reaction, the Congress passed a bill in July 1866 that required state legislatures to elect senators by an absolute majority.
By the 1890s, support for the introduction of direct election for the Senate had substantially increased, and reformers worked on two fronts. On the first front, the Populist Party incorporated the direct election of senators into its Omaha Platform, adopted in 1892. In 1908, Oregon passed the first law that based the selection of U.S. senators on a popular vote. Oregon was soon followed by Nebraska. Proponents for popular election noted that ten states already had non-binding primaries for Senate candidates, in which the candidates would be voted on by the public, effectively serving as advisory referenda instructing state legislatures how to vote; reformers campaigned for more states to introduce a similar method.
William Randolph Hearst opened a nationwide popular readership for direct election of U.S. Senators in a 1906 series of articles using flamboyant language attacking “The Treason of the Senate” in his “Cosmopolitan Magazine”. David Graham Philips, one of those yellow journalists Teddy Roosevelt called “muckrakers”, described Nelson Aldrich of Rhode Island as the principal “traitor” among the “scurvy lot” in control of the Senate by theft, perjury and bribes corrupting the state legislatures to gain election to the Senate. A few state legislatures began to petition the Congress for direct election of senators. By 1893, the House had the two-thirds vote for just such an amendment. However, when the joint resolution reached the Senate, it failed from neglect, as it did again in 1900, 1904 and 1908; each time the House approved the appropriate resolution, and each time it died in the Senate.
On the second national legislative front, reformers worked towards a constitutional amendment, which was strongly supported in the House of Representatives but initially opposed by the Senate—Bybee notes that the state legislatures, who would lose power if the reforms went through, were supportive of the campaign. By 1910, 31 state legislatures had passed resolutions calling for a constitutional amendment allowing direct election, and in the same year ten Republican senators who were opposed to reform were forced out of their seats, acting as a "wake-up call to the Senate".
Reformers included William Jennings Bryan, while opponents counted respected figures such as Elihu Root and George Frisbie Hoar amongst their number; Root cared so strongly about the issue that after the passage of the Seventeenth Amendment, he refused to stand for re‑election to the Senate. Bryan and the reformers argued for popular election through highlighting perceived flaws with the existing system, specifically corruption and electoral deadlocks, and through arousing populist sentiment. Most important was the populist argument; that there was a need to "Awaken, in the senators...a more acute sense of responsibility to the people", which it was felt they lacked; election through state legislatures was seen as an anachronism that was out of step with the wishes of the American people, and one that had led to the Senate becoming "a sort of aristocratic body – too far removed from the people, beyond their reach, and with no special interest in their welfare".
Hoar replied that the people were both a less permanent and a less trusted body than state legislatures, and that moving the responsibility for the election of senators to them would see it passing into the hands of a body that "[lasted] but a day" before changing. Other counterarguments were that renowned senators could not have been elected directly, and that since a large number of senators had experience in the House, which was already directly elected, a constitutional amendment would be pointless. It was also seen as a threat to the rights and independence of the states, who were "sovereign, entitled...to have a separate branch of Congress...to which they could send their ambassadors". This was countered by the argument that a change in the mode in which senators were elected would not change their responsibilities.
The Senate freshman class of 1910 brought new hope to the reformers. Fourteen of the thirty newly elected senators had been elected through party primaries, which amounted to popular choice in their states. More than half of the states had some form of primary selection for the Senate. The Senate finally joined the House to submit the Seventeenth Amendment to the states for ratification, nearly ninety years after it first was presented to the Senate in 1826.
By 1912, 239 political parties at both the state and national level had pledged some form of direct election, and 33 states had introduced the use of direct primaries. Twenty-seven states had called for a constitutional convention on the subject, with 31 states needed to reach the threshold; Arizona and New Mexico each achieved statehood that year (bringing the total number of states to 48), and were expected to support the motion, while Alabama and Wyoming, already states, had passed resolutions in favor of a convention without formally calling for one.
Proposal and ratification.
Proposed by the Congress.
In 1911, the House of Representatives passed House Joint Resolution 39 proposing a constitutional amendment for direct election of senators. However, it included a “race rider” meant to bar federal intervention in cases of racial discrimination among voters. When the resolution came before the Senate, a substitute resolution, one without the rider, as proposed by Joseph L. Bristow of Kansas. It was adopted by a vote of 64 to 24, with 4 not voting. Nearly a year later, the House accepted the change. The conference report that would become the Seventeenth Amendment was approved by the Senate 42 to 36 on April 12, 1912, and by the House 238 to 39, with 110 not voting on May 13, 1912.
Ratification by the states.
Having been passed by Congress, the amendment was sent to the states for ratification and was ratified by:
The Utah legislature rejected the amendment on February 26, 1913. No action on the amendment has been completed by: Florida, Georgia, Kentucky, Mississippi, South Carolina, or Virginia.
Effect.
The Seventeenth Amendment altered the process for electing United States Senators and changed the way vacancies would be filled. Under the original constitutional provision, state legislatures filled vacancies when a Senator left office before the end of the term; the Seventeenth Amendment provides that state legislatures can grant governors the right to make temporary appointments, which last until a special election is provided to fill the seat. The power to call such an election can also be granted to the governor. It also had an immediate and dramatic impact on the political composition of the U.S. Senate.
Before the Supreme Court required one-man one-vote in a 1964 case, rural counties and cities could be given equal weight in the state legislatures, enabling one rural vote to equal 200 city votes. The malapportioned state legislatures would have given the Republicans control of the Senate in the 1916 Senate elections. With direct election, each vote represented equally, the Democrats retained control of the Senate.
The reputation of corrupt and arbitrary state legislatures continued to decline as the Senate joined the House of Representatives implementing popular reforms. Judge Bybee has argued that the amendment led to complete "ignominy" for state legislatures without the props of a state-based check on Congress. Progressive measures were enacted to enable the federal government to supersede the discredited states repeatedly over decades. However, Schleiches argues that the separation of state legislatures and the Senate has had a beneficial effect on the states, as it has allowed state legislative campaigns to focus on local rather than national issues.
New Deal legislation is another example of expanding federal regulation overruling the state legislatures promoting their local state interests in coal, oil, corn and cotton. Ure agrees, saying that not only is each Senator now free to ignore his state's interests, Senators "have incentive to use their advice-and-consent powers to install Supreme Court justices who are inclined to increase federal power at the expense of state sovereignty". Over the first half of the 20th century, with a popularly elected Senate confirming nominations, both Republican and Democratic, the Supreme Court began to apply the Bill of Rights to the states, overturning state laws whenever they harmed individual state citizens.
First direct elections to the Senate.
Oklahoma, admitted to statehood in 1907, chose a Senator by legislative election three times: twice in 1907, when admitted, and once in 1908. In 1912, Oklahoma reelected Robert Owen by advisory popular vote.
New Mexico, admitted to statehood in 1912, chose only its first two Senators legislatively. Arizona, admitted to statehood in 1912, chose its first two Senators by advisory popular vote. Alaska, and Hawaii, admitted to statehood in 1959, have never chosen a U.S. Senator legislatively.
The first direct elections to the Senate following the Seventeenth Amendment being adopted were:
Interpretation and advocacy for reform.
In "Trinsey v. Pennsylvania" (1991), the United States Court of Appeals for the Third Circuit was faced with a situation where, following the death of Senator H. John Heinz III of Pennsylvania, Governor Robert P. Casey had provided for a replacement and for a special election that did not include a primary. A voter and prospective candidate, John S. Trinsey, Jr., argued that the lack of a primary violated the Seventeenth Amendment and his right to vote under the Fourteenth Amendment. These arguments were rejected by the Third Circuit, which ruled that the Seventeenth Amendment does not require primaries.
Another subject of analysis is whether statutes restricting the authority of governors to appoint temporary replacements are constitutional. Vikram Amar, writing in the "Hastings Constitutional Law Quarterly", claims that Wyoming's requirement that its governor fill a senatorial vacancy by nominating a person of the same party as the person who vacated that Senate seat violates the Seventeenth Amendment. This is based on the text of the Seventeenth Amendment, which states that "the legislature of any state may empower the executive thereof to make temporary appointments". The amendment only empowers the legislature to delegate the authority to the governor and, once that authority has been delegated, does not permit the legislature to intervene. The authority is to decide whether or not the governor shall have the power to appoint temporary senators, not in what fashion he should do so. Sanford Levinson, in his rebuttal to Amar, argues that rather than engaging in a textual interpretation, those examining the meaning of constitutional provisions should interpret them in the fashion that provides the most benefit, and that legislatures being able to restrict gubernatorial appointment authority provides a substantial benefit to the states.
Due to the controversy over the impact of the Seventeenth Amendment, there has been advocacy for both reform and repeal of the Seventeenth Amendment. With the commencement of the Obama Administration in 2009, four sitting Democratic senators left the Senate for executive branch positions: Barack Obama (President), Joe Biden (Vice President), Hillary Rodham Clinton (Secretary of State), and Ken Salazar (Secretary of the Interior). Controversies developed about the successor appointments made by Illinois Governor Rod Blagojevich and New York Governor David Paterson. This created interest in abolishing Senate appointment by the governor. Accordingly, Senator Russ Feingold of Wisconsin and Representative David Dreier of California proposed an amendment to remove this power; Senators John McCain and Dick Durbin became co-sponsors, as did Representative John Conyers. The Tea Party movement has been arguing for repealing the Seventeenth Amendment entirely, claiming that it would protect states' rights and reduce the power of the federal government.

</doc>
<doc id="31670" url="http://en.wikipedia.org/wiki?curid=31670" title="Nineteenth Amendment to the United States Constitution">
Nineteenth Amendment to the United States Constitution

The Nineteenth Amendment (Amendment XIX) to the United States Constitution prohibits any United States citizen from being denied the right to vote on the basis of sex. It was ratified on August 18, 1920. The Constitution allows the states to determine the qualifications of voters, subject to limitations imposed by later amendments. Until the 1910s, most states disenfranchised women. The amendment was the culmination of the women's suffrage movement in the United States, which fought at both state and national levels to achieve the vote. It effectively overruled "Minor v. Happersett", in which a unanimous Supreme Court ruled that the Fourteenth Amendment did not give women the right to vote.
The Nineteenth Amendment was first introduced in Congress in 1878 by Senator Aaron A. Sargent. Forty-one years later, in 1919, Congress approved the amendment and submitted it to the states for ratification. It was ratified by the requisite number of states a year later, with Tennessee's ratification being the final vote needed to add the amendment to the Constitution. In "Leser v. Garnett" (1922), the Supreme Court rejected claims that the amendment was unconstitutionally adopted.
Text.
The right of citizens of the United States to vote shall not be denied or abridged by the United States or by any State on account of sex.
Congress shall have power to enforce this article by appropriate legislation.
Background.
The United States Constitution, adopted in 1789, left the boundaries of suffrage undefined. The only directly elected body created by the original Constitution was the House of Representatives, for which voter qualifications were explicitly delegated to the individual states. At that time, all states denied voting rights to women (with the exception of New Jersey, which initially carried women's suffrage but revoked it in 1807).
While scattered movements and organizations dedicated to women's rights existed previously, the 1848 Seneca Falls Convention in New York is traditionally held as the start of the American women's rights movement. Suffrage was not a focus of the convention, however, and its advancement was minimal in the decades preceding the Civil War. While suffrage bills were introduced into most state legislatures during this period, they were generally disregarded and few came to a vote.
The women's suffrage movement took hold after the Civil War, during the Reconstruction era (1865–1877). During this period, women's rights leaders advocated for inclusion of universal suffrage as a civil right in the Reconstruction amendments (the Thirteenth, Fourteenth, and Fifteenth Amendments). Despite their efforts, these amendments did nothing to promote women's suffrage. Section 2 of the Fourteenth Amendment explicitly discriminated between men and women by penalizing states who deprived adult male citizens of the vote, but not for denying the vote to adult female citizens.
Continued settlement of the western frontier, along with the establishment of territorial constitutions, allowed the issue to be raised continually at the state level. Through the activism of suffrage organizations and independent political parties, women's suffrage was established in the newly formed constitutions of Wyoming Territory (1869), Utah (1870), and Washington Territory (1883). Existing state legislatures began to consider suffrage bills, and several even held voter referenda, but they were unsuccessful. Efforts at the national level persisted through a strategy of congressional testimony, petitioning, and lobbying.
Two rival organizations, the National Woman Suffrage Association (NWSA) and the American Woman Suffrage Association (AWSA), were formed in 1869. The NWSA, led by suffrage leaders Elizabeth Cady Stanton and Susan B. Anthony, attempted several unsuccessful court challenges in the mid-1870s. Their legal case, known as the "New Departure" strategy, was that the Fourteenth Amendment (granting universal citizenship) and Fifteenth Amendment (granting the vote irrespective of race) together served to guarantee voting rights to women. Three Supreme Court decisions from 1873 to 1875 rejected this argument, so these groups shifted to advocating for a new constitutional amendment.
Proposal and ratification.
The Nineteenth Amendment is identical to the Fifteenth Amendment, except that the Nineteenth prohibits the denial of suffrage because of sex and the Fifteenth because of "race, color, or previous condition of servitude".
Colloquially known as the "Anthony Amendment", it was first introduced in the Senate by Republican Senator Aaron A. Sargent of California. Sargent, who had met and befriended Anthony on a train ride in 1872, was a dedicated women's suffrage advocate. He had frequently attempted to insert women's suffrage provisions into unrelated bills, but did not formally introduce a constitutional amendment until January 1878. Stanton and other women testified before the Senate in support of the amendment. The proposal sat in a committee until it was considered by the full Senate and rejected in a 16 to 34 vote in 1887.
A three-decade period known as "the doldrums" followed, during which the amendment was not considered by Congress and the women's suffrage movement achieved few victories. During this period, the suffragists pressed for the right to vote in the laws of individual states and territories while retaining the goal of federal recognition. A flurry of activity began in 1910 and 1911 with surprise successes in Washington and California. Over the next few years, most western states passed legislation or voter referenda enacting full or partial suffrage for women. These successes were linked to the 1912 election, which saw the rise of the Progressive and Socialist parties, as well as the election of Democratic President Woodrow Wilson. Not until 1914 was the constitutional amendment again considered by the Senate, where it was again rejected.
Carrie Chapman Catt was instrumental in the final push to gain ratification of the Nineteenth Amendment. In 1900, she succeeded Susan B. Anthony as the president of the National American Woman Suffrage Association (NAWSA). Starting in 1915, Catt revitalized NAWSA and led a successful campaign in New York to achieve state-level suffrage in 1917. When the U.S. entered World War I, Catt made the controversial decision to support the war effort, despite the widespread pacifist sentiment of many of her colleagues and supporters. NAWSA women’s work to aid the war effort turned them into highly visible symbols of nationalism.
The republican work of NAWSA stood in contrast to the more radical and aggressive tactics of the National Woman's Party (NWP) led by Alice Paul and Lucy Burns. In 1917, the NWP staged controversial demonstrations in Washington, D.C. to draw attention away from the war and back to women’s suffrage. Catt was successful in turning NAWSA into a patriotic organization, entirely separate from the NWP, and was rewarded when President Wilson spoke out in favor of women’s suffrage in his 1918 State of the Union address before Congress.
Another proposal was brought before the House on January 10, 1918. During the previous evening, President Wilson made a strong and widely published appeal to the House to pass the amendment. It was passed by the required two-thirds of the House, with only one vote to spare. The vote was then carried into the Senate. Wilson again made an appeal, but on September 30, 1918, the proposal fell two votes short of passage. On February 10, 1919, it was again voted upon and failed by only one vote.
There was considerable desire among politicians of both parties to have the proposal made part of the Constitution before the 1920 general elections, so the President called a special session of the Congress so the proposal would be brought before the House again. On May 21, 1919, it passed the House, 42 votes more than necessary being obtained. On June 4, 1919, it was brought before the Senate and, after a long discussion, it was passed with 56 ayes and 25 nays. Within a few days, Illinois, Wisconsin, and Michigan ratified the amendment, their legislatures being in session. Other states followed suit at a regular pace, until the amendment had been ratified by 35 of the necessary 36 state legislatures. Much of the opposition to the amendment came from Southern Democrats, a trend which remained consistent with Tennessee as the last state to pass the amendment, during a special session right before the ratification period was to expire. On August 18, 1920, Tennessee narrowly approved the Nineteenth Amendment, with 50 of 99 members of the Tennessee House of Representatives voting yes. This provided the final ratification necessary to add the amendment to the Constitution.
Ratification timeline.
The Congress proposed the Nineteenth Amendment on June 4, 1919, and the following states ratified the amendment.
Ratification was completed on August 18, 1920, and the following states subsequently ratified the amendment:
Alaska and Hawaii were not states when the Nineteenth Amendment was ratified.
"Leser v. Garnett".
The amendment's validity was unanimously upheld in "Leser v. Garnett", 258 U.S. (1922).
Oscar Leser sued to stop two women registered to vote in Baltimore, Maryland, because he believed that the Maryland Constitution limited the suffrage to men and the Maryland legislature had refused to vote to ratify the Nineteenth Amendment. Two months before, the federal government had proclaimed the amendment incorporated into the Constitution on August 26, 1920.
First, Leser said the amendment "destroyed State autonomy" because it increased Maryland's electorate without the state's consent. The Court answered that the Nineteenth Amendment was worded like the Fifteenth Amendment, which had expanded state electorates without regard to race for over 50 years by that time despite being rejected by six states, including Maryland.
Second, Leser claimed that the state constitutions in some ratifying states did not allow their legislatures to ratify. The Court replied that state ratification was a federal function which came from Article V of the Constitution and so is not subject to limitations by a state constitution.
Third, those bringing suit asserted the Nineteenth Amendment was not adopted, because Tennessee and West Virginia violated their own rules of procedure. The Court ruled that the point was moot, because since then Connecticut and Vermont had ratified the amendment and so there was a sufficient number of ratifications for the Nineteenth Amendment to be considered adopted even without Tennessee and West Virginia. Also, the Court ruled that Tennessee and West Virginia's certifying of their ratifications was binding and had been duly authenticated by the Secretary of State.
Thus, the two women were permitted to be registered to vote in Baltimore.
Effects.
Following the Nineteenth Amendment's adoption, many legislators feared that a powerful women's bloc would emerge in American politics. This led to the passage of such laws as the Sheppard–Towner Act of 1921, which expanded maternity care during the 1920s. However, a women's bloc did not emerge in American politics until the 1950s.
See also.
</dl>
References.
Bibliography.
</dl>

</doc>
<doc id="31671" url="http://en.wikipedia.org/wiki?curid=31671" title="Eighteenth Amendment to the United States Constitution">
Eighteenth Amendment to the United States Constitution

The Eighteenth Amendment (Amendment XVIII) of the United States Constitution effectively established the prohibition of alcoholic beverages in the United States by declaring illegal the production, transport and sale of alcohol (though not the consumption or private possession). The separate Volstead Act set down methods of enforcing the Eighteenth Amendment, and defined which "intoxicating liquors" were prohibited, and which were excluded from prohibition (e.g., for medical and religious purposes). The Amendment was the first to set a time delay before it would take effect following ratification, and the first to set a time limit for its ratification by the states. Its ratification was certified on January 16, 1919, with the amendment taking effect on January 17, 1920.
The police, courts and prisons were overwhelmed with new cases; organized crime increased in power, and corruption extended among law enforcement officials. The amendment was repealed in 1933 by ratification of the Twenty-first Amendment, the only instance in United States history that a constitutional amendment was repealed in its entirety.
Text.
Section 1. After one year from the ratification of this article the manufacture, sale, or transportation of intoxicating liquors within, the importation thereof into, or the exportation thereof from the United States and all the territory subject to the jurisdiction thereof for beverage purposes is hereby prohibited.
Section 2. The Congress and the several States shall have concurrent power to enforce this article by appropriate legislation.
Section 3. This article shall be inoperative unless it shall have been ratified as an amendment to the Constitution by the legislatures of the several States, as provided in the Constitution, within seven years from the date of the submission hereof to the States by the Congress.
Background.
The Eighteenth Amendment was the result of decades of effort by the temperance movement in the United States and at the time was generally considered a progressive amendment.
Many state legislatures had already enacted statewide prohibition prior to the ratification of the Eighteenth Amendment, but didn't ban consumption of alcohol in most households.
Proposal and ratification.
On August 1, 1917, the Senate passed a resolution containing the language of the amendment to be presented to the states for ratification. The vote was 65 to 20, with the Democrats voting 36 in favor and 12 in opposition; and the Republicans voting 29 in favor and 8 in opposition. The House of Representatives passed a revised resolution on December 17, 1917.
In the House, the vote was 282 to 128, with the Democrats voting 141 in favor and 64 in opposition; and the Republicans voting 137 in favor and 62 in opposition. Four Independents in the House voted in favor and two Independents cast votes against the amendment. It was officially proposed by the Congress to the states when the Senate passed the resolution, by a vote of 47 to 8, the next day, December 18.
The amendment and its enabling legislation did not ban the consumption of alcohol, but made it difficult to obtain alcoholic beverages legally, as it prohibited the sale and distribution of them in U.S. territory.
The proposed amendment was the first to contain a provision setting a deadline for its ratification. That clause of the amendment was challenged, with the case reaching the US Supreme Court. It upheld the constitutionality of such a deadline in "Dillon v. Gloss" (1921).
The ratification of the Amendment was completed on January 16, 1919, when Nebraska became the 36th of the 48 states then in the Union to ratify it. On January 29, acting Secretary of State Frank L. Polk certified the ratification.
The following states ratified the amendment:
The following states rejected the amendment:
To define the language used in the Amendment, Congress enacted enabling legislation called the National Prohibition Act, better known as the Volstead Act, on October 28, 1919. President Woodrow Wilson vetoed that bill, but the House of Representatives immediately voted to override the veto and the Senate voted similarly the next day. The Volstead Act set the starting date for nationwide prohibition for January 17, 1920, which was the earliest date allowed by the 18th Amendment.
Impact.
After the 18th Amendment's adoption, there was a significant reduction in alcohol consumption among the general public and particularly among low-income groups. Likewise, there was a general reduction in overall crime, mainly in the types of crimes associated with the effects of alcohol consumption, though there were significant increases in crimes involved in the production and distribution of illegal alcohol to supply the segment of the population that refused to quit alcohol. Those who continued to use alcohol tended to turn to organized criminal syndicates, who were able to take advantage of weak enforcement, understaffed police forces, and corrupt public officials to establish powerful smuggling networks. Anti-prohibition groups arose shortly after the amendments, and backed by powerful business interests and media members were able in over a decade of activism, advocacy, legal disobedience, and legal action to build enough opposition to have prohibition repealed.
The 21st Amendment repealed the 18th Amendment on December 5, 1933. The amendment remains the only constitutional amendment to be repealed in its entirety; leaving only the power to regulate transportation solely to the federal government.

</doc>
<doc id="31672" url="http://en.wikipedia.org/wiki?curid=31672" title="Twentieth Amendment to the United States Constitution">
Twentieth Amendment to the United States Constitution

The Twentieth Amendment (Amendment XX) to the United States Constitution moved the beginning and ending of the terms of the President and Vice President from March 4 to January 20, and of members of Congress from March 4 to January 3. It also has provisions that determine what is to be done when there is no President-elect. The Twentieth Amendment was ratified on January 23, 1933.
Text.
Section 1. The terms of the President and Vice President shall end at noon on the 20th day of January, and the terms of Senators and Representatives at noon on the 3d day of January, of the years in which such terms would have ended if this article had not been ratified; and the terms of their successors shall then begin.
Section 2. The Congress shall assemble at least once in every year, and such meeting shall begin at noon on the 3d day of January, unless they shall by law appoint a different day.
Section 3. If, at the time fixed for the beginning of the term of the President, the President elect shall have died, the Vice President elect shall become President. If a President shall not have been chosen before the time fixed for the beginning of his term, or if the President elect shall have failed to qualify, then the Vice President elect shall act as President until a President shall have qualified; and the Congress may by law provide for the case wherein neither a President elect nor a Vice President elect shall have qualified, declaring who shall then act as President, or the manner in which one who is to act shall be selected, and such person shall act accordingly until a President or Vice President shall have qualified.
Section 4. The Congress may by law provide for the case of the death of any of the persons from whom the House of Representatives may choose a President whenever the right of choice shall have devolved upon them, and for the case of the death of any of the persons from whom the Senate may choose a Vice President whenever the right of choice shall have devolved upon them.
Section 5. Sections 1 and 2 shall take effect on the 15th day of October following the ratification of this article.
Section 6. This article shall be inoperative unless it shall have been ratified as an amendment to the Constitution by the legislatures of three-fourths of the several States within seven years from the date of its submission.
Historical background.
Before the Twentieth Amendment, the schedules which determined the terms of office of elected officials, and when sessions of Congress began and ended, were set by a sometimes awkward intersection of law, historical precedent, and constitutional mandate. The Constitution did not set any dates for congressional or presidential elections, nor for the commencement of terms of office for elected federal officials; the only specific date given was in , which stated that Congress was required to convene at least once each year, on the first Monday in December, though Congress could by law set another date and the President could summon special sessions.
In September of 1788, after the necessary nine states had ratified the Constitution, the Congress of the Confederation set March 4, 1789, as the date "for commencing proceedings" of the newly reorganized government. Despite the fact that the new Congress and presidential administration did not begin operation until April, March 4 was deemed to be the beginning of the newly elected officials' terms of office, and thus of the terms of their successors. By the time of the second presidential election in 1792, Congress had passed a law requiring presidential electors to be chosen during November or early December, which was narrowed to a single day in early November by 1845; Congressional elections were generally held on the same day.
The upshot of these scheduling decisions was that there was a long four month lame duck period between the election of the president and his inauguration. For Congress, the situation was perhaps even more awkward. Because Article I, Section 4, Clause 2 mandated a Congressional meeting every December, after the election but before the congressional terms of office had expired, a lame duck session was required by the Constitution in even-numbered years; the next session wasn't required until the next December, meaning that new members of Congress might not begin their work until more than a year after they were elected. Special sessions sometimes met earlier in the year, but this never became a regular practice, despite the Constitution allowing for it. In practice, Congress usually met in a long session beginning in Decembers of odd-numbered years, and in a short lame duck session in December of even-numbered years.
The long lame duck period might have been a practical necessity at the end of the 18th century, when any newly elected official might require several months to put his affairs in order and then undertake an arduous journey from his home to the national capital, but it eventually had the effect of impeding the functioning of government in the modern age. From the early 19th century onward, it also meant that a lame Congress and Presidential administration would fail to adequately respond to a significant national crisis in a timely manner. Each institution could do this on the theory that at best, a lame duck Congress or administration had neither the time nor the mandate to tackle problems, whereas the incoming administration or Congress would have both the time, and a fresh electoral mandate, to examine and address the problems that the nation faced. These problems very likely would have been at the center of the debate of the just completed election cycle.
This dilemma was seen most notably in 1861 and 1933, after the elections of Abraham Lincoln and Franklin D. Roosevelt, respectively, plus the newly elected Senators and Representatives. Under the Constitution at the time, these presidents had to wait four months before they and the incoming Congresses could deal with the secession of Southern states and the Great Depression respectively.
Effect of the amendment.
Section 1 of the Twentieth Amendment moved the beginning of the Presidential and Vice Presidential terms to January 20, reducing the time those officials may serve as lame ducks by about six weeks. In Section 2, the Congressional term was set to begin even earlier, to January 3, with the annually mandated session set to begin that day. This removed the requirement for a lame duck session, but lame duck sessions in December of election years are still common.
The amendment was ratified on January 23, 1933. Pursuant to Section 5 of the amendment, the changes made by Sections 1 and 2 took effect on October 15, 1933. This delay resulted in the first inauguration of President Roosevelt and Vice President John Garner taking place on March 4, 1933. Section 5 also resulted in the 73rd Congress not being required to meet until December 4, 1933. Therefore, pursuant to a presidential proclamation, a special 100-day session of Congress was convened from March 9, 1933, to June 16, 1933.
On February 15, 1933, 23 days after this amendment was ratified, President-elect Franklin D. Roosevelt was the target of an unsuccessful assassination attempt by Giuseppe Zangara. If the attempt had been successful then, pursuant to Section 3 of the amendment, Vice President-elect John N. Garner would have become President on March 4, 1933.
Section 3 also allows Congress to determine who should act as President if a new President has not been chosen by the end of the previous Presidential term. The Constitution was originally silent on this point, an omission that nearly caused a constitutional crisis when Congress seemed be unable to break its deadlock and resolve the election of 1800.
Section 4 of the amendment permits Congress to statutorily provide for what should occur if either the House of Representatives must elect the President and one of the candidates from whom it may choose dies, or the Senate must elect the Vice President and one of the candidates from whom it may choose dies. Congress has never enacted such a statute.
The first Congressional terms to begin under Section 1 were those of the 74th Congress, on January 3, 1935. The first Presidential and Vice Presidential terms to begin under Section 1 were the second terms of President Roosevelt and Vice President Garner, on January 20, 1937.
Because of this amendment, if the Electoral College fails to resolve who will be the President or Vice President, the incoming Congress, as opposed to the outgoing one, would choose who would occupy the unresolved office or offices.
Proposal and ratification.
The Congress proposed the Twentieth Amendment on March 2, 1932 and the following states ratified it:
Missouri was the 36th state to ratify, satisfying the requirement that three-fourths of the then 48 states approve the amendment. The amendment was subsequently ratified by the following states:

</doc>
<doc id="31713" url="http://en.wikipedia.org/wiki?curid=31713" title="Universal Postal Union">
Universal Postal Union

The Universal Postal Union (UPU, French: Union postale universelle) is a specialized agency of the United Nations that coordinates postal policies among member nations, in addition to the worldwide postal system. The UPU contains four bodies consisting of the Congress, the Council of Administration (CA), the Postal Operations Council (POC) and the International Bureau (IB). It also oversees the Telematics and EMS cooperatives. Each member agrees to the same terms for conducting international postal duties. The UPU's headquarters are located in Bern, Switzerland.
French is the official language of the UPU. English was added as a working language in 1994. The majority of the UPU's documents and publications – including its flagship magazine, "Union Postale" – are available in the United Nations' official languages.
Overview.
Prior to the establishment of the UPU, each country had to prepare a separate postal treaty with other nations it wished to carry international mail to or from. In some cases, senders would have to calculate postage for each leg of a journey, and potentially find mail forwarders in a third country if there was no direct delivery. To simplify the complexity of this system, the United States called for an International Postal Congress in 1863. This led Heinrich von Stephan, Royal Prussian and later German Minister for Posts, to found the Universal Postal Union. It is currently the third oldest international organization after the Rhine Commission and the ITU. The UPU was created in 1874, initially under the name "General Postal Union", as a result of the Treaty of Bern signed on October 9, 1874. Four years later, the name was changed to "Universal Postal Union."
The UPU established that:
One of the most important results of the UPU Treaty was that it ceased to be necessary, as it often had been previously, to affix the stamps of any country through which one's letter or package would pass in transit. The UPU provides that stamps of member nations are accepted for the entire international route. Toward the end of the nineteenth century, the UPU issued rules concerning stamp design, intended to ensure maximum efficiency in handling international mail. One rule specified that stamp values be given in numerals (denominations spelled out in letters not being universally comprehensible), another, that member nations all use the same colors on their stamps issued for post cards (green), normal letters (red) and international mail (blue), a system that remained in use for several decades.
After the foundation of the United Nations, the UPU became a specialized agency of the UN in 1948. In 1969, the UPU introduced a new system of payment where fees were payable between countries according to the difference in the total weight of mail between them. These fees were called terminal dues. Ultimately, this new system was fairer when traffic was heavier in one direction than the other. As a matter of example, in 2012, terminal dues for transit from China to the USA was 0.635 SDR/kg, or about 1 USD/kg.
As this affected the cost of the delivery of periodicals, the UPU devised a new "threshold" system, which it later implemented in 1991. The system sets separate letter and periodical rates for countries which receive at least 150 tonnes of mail annually. For countries with less mail, the original flat rate is still maintained. The United States has negotiated a separate terminal dues formula with thirteen European countries that includes a rate per piece plus a rate per kilogram; it has a similar arrangement with Canada. The UPU also operates the system of International Reply Coupons and addresses concerns with ETOEs.
Standards.
Standards are important prerequisites for effective postal operations and for interconnecting the global network. The UPU's "Standards Board" develops and maintains a growing number of international standards to improve the exchange of postal-related information between postal operators. It also promotes the compatibility of UPU and international postal initiatives. The organization works closely with postal handling organizations, customers, suppliers and other partners, including various international organizations. The Standards Board ensures that coherent regulations are developed in areas such as Electronic Data Interchange (EDI), mail encoding, postal forms and meters. UPU standards are drafted in accordance with the rules given in Part V of the "General information on UPU Standards" and are published by the UPU International Bureau in accordance with Part VII of that publication.
Member countries.
All United Nations member states are allowed to become members of the UPU. A non-member state of the United Nations may also become a member if two-thirds of the UPU member countries approve its request. The UPU currently has 192 members (190 states and two joint memberships of dependent territories groups).
Member states of the UPU are the Vatican City and the 193 UN members except Andorra, Marshall Islands, the Federated States of Micronesia, and Palau. The newest member is South Sudan, which joined on 4 October 2011.
The overseas constituent countries of the Netherlands (Aruba, Curaçao and Sint Maarten) are represented as a single UPU member as are the entire British overseas territories. These members were originally listed separately as "Colonies, Protectorates, etc." in the Universal Postal Convention and they were grandfathered in when membership was restricted to sovereign states.
Andorra, Marshall Islands, Micronesia and Palau have their mail delivered through another UPU member (France and Spain for Andorra, and the United States for the Compact of Free Association states).
Palestine was granted special observer status to the UPU in 1999, and in 2008 Israel agreed for its mail to be routed through Jordan though this had not been implemented as of November 2012.
The Republic of China joined the UPU on March 1, 1914. After the People's Republic of China was founded, the Republic of China continued to represent China in the UPU, until the organization decided on April 13, 1972 to recognize the People's Republic of China as the only legitimate Chinese representative. Because of this, International Reply Coupons are not available for Taiwan. Mail addressed to Taiwan must be delivered through Japan, the United States, or at one point Hong Kong.
The other states with limited recognition, such as Somaliland and the Turkish Republic of Northern Cyprus (TRNC), also route their mail through third countries because the UPU will not allow direct international deliveries. For example, the TRNC's mail goes via Turkey and Somaliland's mail via Ethiopia.
Congresses.
The "Universal Postal Congress" is the most important body of the UPU. The main purpose of the quadrennial Congress is to examine proposals to amend the Acts of the UPU, including the UPU Constitution, General Regulations, Convention and Postal Payment Services Agreement. The Congress also serves as forum for participating member countries to discuss a broad range of issues impacting international postal services, such market trends, regulation and other strategic issues. The first UPU Congress was held in Bern, Switzerland in 1874. Delegates from 22 countries participated. UPU Congresses are held every four years and delegates often receive special philatelic albums produced by member countries covering the period since the previous Congress.
Philatelic activities.
The Universal Postal Union, in conjunction with the World Association for the Development of Philately, developed the WADP Numbering System (WNS). It was launched on January 1, 2002. The website displays entries for some 160 countries and emitting postal entities, with over 25,000 registered stamps since 2002. Many of them have images, which generally remain copyrighted by the issuing country, but the UPU and WADP permit them to be downloaded.
Electronic telecommunication.
In some countries, telegraph and later telephones came under the same government department as the postal system. Similarly there was an International Telegraph Bureau, based in Bern, akin to the UPU. The International Telecommunication Union currently facilitates international electronic communication.
In order to integrate postal services and the Internet, the UPU sponsors .post. Developing their own standards, the UPU expects to unveil a whole new range of international digital postal services, including "e-post". They have appointed a body, the ".post group (DPG)" to oversee the development of that platform.

</doc>
<doc id="31715" url="http://en.wikipedia.org/wiki?curid=31715" title="Uralic languages">
Uralic languages

The Uralic languages (sometimes called "Uralian" "languages") constitute a language family of some 38 languages spoken by approximately 25 million people. The Uralic languages with the most native speakers are Hungarian, Finnish, and Estonian, which are official languages of Hungary, Finland, and Estonia, respectively, and of the European Union. Other Uralic languages with significant numbers of speakers are Erzya, Moksha, Mari, Udmurt, and Komi, which are officially recognized languages in various regions of Russia.
The name "Uralic" derives from the fact that areas where the languages are spoken spread on both sides of the Ural Mountains. Also, the original homeland (Urheimat) is commonly hypothesized to lie in the vicinity of the Urals.
Finno-Ugric is sometimes used as a synonym for Uralic, though Finno-Ugric is widely understood to exclude the Samoyedic languages.
History.
Homeland.
In recent times, linguists often place the Urheimat (original homeland) of the Proto-Uralic language in the vicinity of the Volga River, west of the Urals, close to the Urheimat of the Indo-European languages, or to the east and southeast of the Urals. Gyula László places its origin in the forest zone between the Oka River and central Poland. E.N. Setälä and M. Zsirai place it between the Volga and Kama Rivers. According to E. Itkonen, the ancestral area extended to the Baltic Sea. P. Hajdu has suggested a homeland in western and northwestern Siberia.
Early attestations.
The first plausible mention of a Uralic people is in Tacitus's "Germania" (c. 98 AD), mentioning the "Fenni" (usually interpreted as referring to the Sami) and two other possibly Uralic tribes living in the farthest reaches of Scandinavia. There are many possible earlier mentions, including the Irycae (perhaps related to Yugra) described by Herodotus living in what is now European Russia, and the Budini, described by Herodotus as notably red-haired (a characteristic feature of the Udmurts) and living in northeast Ukraine and/or adjacent parts of Russia. In the late 15th century, European scholars noted the resemblance of the names "Hungaria" and "Yugria", the names of settlements east of the Ural. They assumed a connection but did not seek linguistic evidence.
Uralic studies.
The affinity of Hungarian and Finnish was first proposed in the late 17th century. Two of the more important contributors were the German scholar Martin Vogel, who established several grammatical and lexical parallels between Finnish and Hungarian, and the Swedish scholar Georg Stiernhielm, who commented on the similarities of Sami, Estonian and Finnish, and also on a few similar words between Finnish and Hungarian. These two authors were thus the first to outline what was to become the classification of the Finno-Ugric, and later Uralic family. This proposal received some of its initial impetus from the fact that these languages, unlike most of the other languages spoken in Europe, are not part of what is now known as the Indo-European family.
In 1717, Swedish professor Olof Rudbeck proposed about 100 etymologies connecting Finnish and Hungarian, of which about 40 are still considered valid. In the same year, the German scholar Johann Georg von Eckhart, in an essay published in Leibniz's "Collectanea Etymologica", proposed for the first time a relation to the Samoyedic languages.
Philip Johan von Strahlenberg in 1730 published his book "Das Nord-und Ostliche Theil von Europa und Asia", surveying the geography, peoples and languages of Russia. All the main groups of the Uralic languages were already identified here. Nonetheless, these relationships were not widely accepted. Hungarian intellectuals especially were not interested in the theory and preferred to assume connections with Turkic tribes, an attitude characterized by Ruhlen (1987) as due to "the wild unfettered Romanticism of the epoch". Still, in spite of this hostile climate, the Hungarian Jesuit János Sajnovics travelled with Maximilian Hell to survey the alleged relationship between Hungarian and Sami. Sajnovics published his results in 1770, arguing for a relationship based on several grammatical features. In 1799, the Hungarian Sámuel Gyarmathi published the most complete work on Finno-Ugric to that date.
At the beginning of the 19th century, research on this family was thus more advanced#redirect than Indo-European research. But the rise of Indo-European comparative linguistics absorbed so much attention and enthusiasm that Uralic linguistics was all but eclipsed in Europe; in Hungary, the only European country that would have had a vested interest in the family (Finland and Estonia being under Russian rule), the political climate was too hostile for the development of Uralic comparative linguistics.
Progress resumed after a number of decades with the first major field research expeditions on the Uralic languages spoken in the more eastern parts of Russia. These were made in the 1840s by Matthias Castrén (1813–1852) and Antal Reguly (1819–1858), who focused especially on the Samoyedic and the Ob-Ugric languages, respectively. Reguly's materials were worked on by the Hungarian linguist Pál Hunfalvy (1810–1891) and German Josef Budenz (1836–1892), who both supported the Uralic affinity of Hungarian. Budenz was the first scholar to bring this result to popular consciousness in Hungary, and to attempt a reconstruction of the Proto-Finno-Ugric grammar and lexicon. Another late-19th-century Hungarian contribution is that of Ignác Halász (1855–1901), who published extensive comparative material of Finno-Ugric and Samoyedic in the 1890s, and whose work is at the base of today's wide acceptance of the inclusion of Samoyedic as a part of Uralic. Meanwhile in the autonomous Grand Duchy of Finland, a chair for Finnish language and linguistics at the University of Helsinki was created in 1850, first held by Castrén.
In 1883 the Finno-Ugrian Society was founded in Helsinki on the proposal of Otto Donner, and during the late 19th and early 20th century (until the separation of Finland from Russia following the Russian revolution), a large number of stipendiates were sent by the Society to survey the less known Uralic languages. Major researchers of this period included Heikki Paasonen (studying especially the Mordvinic languages), Yrjö Wichmann (Permic), Artturi Kannisto (Mansi), Kustaa Fredrik Karjalainen (Khanty), Toivo Lehtisalo (Nenets), and Kai Donner (Kamass). The vast amounts of data collected on these expeditions would provide edition work for later generations of Finnish Uralicists for more than a century.
Classification.
The Uralic family comprises nine undisputed groups with no consensus classification between them. (Some of the proposals are listed in the next section.) An agnostic approach treats them as separate branches.
Obsolete names are displayed in italics.
There is also historical evidence of a number of extinct languages of uncertain affiliation:
Traces of Finno-Ugric substrata, especially in toponymy, in the northern part of European Russia have been proposed as evidence for even more extinct Uralic languages.
Traditional classification.
All Uralic languages are thought to have descended, through independent processes of language change, from Proto-Uralic. The internal structure of the Uralic family has been debated since the family was first proposed. Doubts about the validity of most of the proposed higher-order branchings (grouping the nine undisputed families) are becoming more common.
A traditional classification of the Uralic languages has existed since the late 19th century, tracing back to Donner (1879). It has enjoyed frequent adaptation in whole or in part in encyclopedias, handbooks, and overviews of the Uralic family. Donner's model is as follows:
At Donner's time, the Samoyedic languages were still poorly known, and he was not able to address their position. As they became better known in the early 20th century, they were found to be quite divergent, and they were assumed to have separated already early on. The terminology adopted for this was "Uralic" for the entire family, "Finno-Ugric" for the non-Samoyedic languages (though "Finno-Ugric" has, to this day, remained in use also as a synonym for the whole family). Finno-Ugric and Samoyedic are listed in ISO 639-5 as primary branches of Uralic.
Nodes of the traditional family tree recognized in some overview sources:
Little explicit evidence has however been presented in favor of Donner's model since his original proposal, and numerous alternate schemes have been proposed. Especially in Finland there has been a growing tendency to reject the Finno-Ugric intermediate protolanguage. A recent competing proposal instead unites Ugric and Samoyedic in an "East Uralic" group for which shared innovations can be noted.
The Finno-Permic grouping still holds some support, though the arrangement of its subgroups is a matter of some dispute. Mordvinic is commonly seen as particularly closely related to or part of Finno-Samic. The term "Volgaic" (or "Volga-Finnic") was used to denote a branch previously believed to include Mari, Mordvinic and a number of the extinct languages, but it is now obsolete and considered a geographic classification rather than a linguistic one.
Within Ugric, uniting Mansi with Hungarian rather than Khanty has been a competing hypothesis to Ob-Ugric.
Lexical isoglosses.
Lexicostatistics has been used in defense of the traditional family tree. A recent re-evaluation of the evidence however fails to find support for Finno-Ugric and Ugric, suggesting four lexically distinct branches (Finno-Permic, Hungarian, Ob-Ugric and Samoyedic).
One alternate proposal for a family tree, with emphasis on the development of numerals, is as follows:
Phonological isoglosses.
Another, more divergent from the standard, focusing on consonant isoglosses (which does not consider the position of the Samoyedic languages) is presented by Viitso (1997), and refined in Viitso (2000):
The grouping of the four bottom-level branches remains to some degree open to interpretation, with competing models of Finno-Saamic vs. Eastern Finno-Ugric (Mari, Mordvinic, Permic-Ugric; *k > ɣ between vowels, degemination of stops) and Finno-Volgaic (Finno-Saamic, Mari, Mordvinic; *δ́ > δ between vowels) vs. Permic-Ugric. Viitso finds no evidence for a Finno-Permic grouping.
Extending this approach to cover the Samoyedic languages suggests affinity with Ugric, resulting in the aforementioned East Uralic grouping, as it also shares the same sibilant developments. A further non-trivial Ugric-Samoyedic isogloss is the reduction *k, *x, *w > ɣ when before *i, and after a vowel (cf. *k > ɣ above), or adjacent to *t, *s, *š, or *ś.
Finno-Ugric consonant developments after Viitso (2000); Samoyedic changes after Sammallahti (1988)
The inverse relationship between consonant gradation and medial lenition of stops (the pattern also continuing within the three families where gradation "is" found) is noted by Helimski (1995): an original allophonic gradation system between voiceless and voiced stops would have been easily disrupted by a spreading of voicing to previously unvoiced stops as well.
Typology.
Structural characteristics generally said to be typical of Uralic languages include:
Lexicography.
Basic vocabulary of about 200 words, including body parts (e.g. eye, heart, head, foot, mouth), family members (e.g. father, mother-in-law), animals (e.g. viper, partridge, fish), nature objects (e.g. tree, stone, nest, water), basic verbs (e.g. live, fall, run, make, see, suck, go, die, swim, know), basic pronouns (e.g. who, what, we, you, I), numerals (e.g. two, five); derivatives increase the number of common words.
The Estonian philologist Mall Hellam even proposed cognate sentences that she asserted to be mutually intelligible among the three most widely spoken Uralic languages, Finnish, Estonian, and Hungarian:
However, linguist Geoffrey Pullum reports that neither Finns nor Hungarians could understand the other language's version of the sentence.
Selected cognates.
The following is a very brief selection of cognates in basic vocabulary across the Uralic family, which may serve to give an idea of the sound changes involved. This is not a list of translations: cognates have a common origin, but their meaning may be shifted and loanwords may have replaced them.
Orthographical notes: The hacek denotes postalveolar articulation (⟨ž⟩ [ʒ], ⟨š⟩ [ʃ], ⟨č⟩ [t͡ʃ]), while the acute denotes a secondary palatal articulation (⟨ś⟩ [sʲ ~ ɕ], ⟨ć⟩ [tsʲ ~ tɕ], ⟨l⟩ [lʲ]). The Finnish and Inari Saami letter ⟨y⟩ and the letter ⟨ü⟩ in other languages represent the high rounded vowel [y], while ⟨y⟩ in transcriptions of Permic is a central unrounded vowel [ɨ]. The letters ⟨ä⟩ and ⟨ö⟩ are the front vowels [æ] and [ø].
It is immediately apparent from the list that Finnish is the most conservative of the Uralic languages, with nearly half the words on the list below identical to their Proto-Uralic reconstructions and most of the remainder only having minor changes, such as the conflation of *ś into /s/, or widespread changes such as the loss of *x and alteration of *ï. Finnish has even preserved old Indo-European borrowings relatively unchanged as well. (An example is "porsas" ("pig"), loaned from Proto-Indo-European "*porḱos" or pre-Proto-Indo-Iranian "*porśos", unchanged since loaning save for loss of palatalization, *ś > s.)
Possible relations with other families.
Many relationships between Uralic and other language families have been suggested, but none of these are generally accepted by linguists at the present time.
Ural–Altaic.
Theories proposing a close relationship with the Altaic languages were formerly popular, based on similarities in vocabulary as well as in grammatical and phonological features, in particular the similarities in the Uralic and Altaic pronouns and the presence of agglutination in both sets of languages, as well as vowel harmony in some. For example, the word for "language" is similar in Estonian ("keel") and Mongolian ("хэл" ("hel")). These theories are now generally rejected and most such similarities are attributed to coincidence or language contact, and a few to possible relationship at a deeper genetic level.
Indo-Uralic.
The Indo-Uralic (or Uralo-Indo-European) hypothesis suggests that Uralic and Indo-European are related at a fairly close level or, in its stronger form, that they are more closely related than either is to any other language family. It is viewed as certain by a few linguists (see main article) and as possible by a larger number.
Uralic–Yukaghir.
The Uralic–Yukaghir hypothesis identifies Uralic and Yukaghir as independent members of a single language family. It is currently widely accepted that the similarities between Uralic and Yukaghir languages are due to ancient contacts. Regardless, the hypothesis is accepted by a few linguists and viewed as attractive by a somewhat larger number.
Eskimo–Uralic.
The Eskimo–Uralic hypothesis associates Uralic with the Eskimo–Aleut languages. This is an old thesis whose antecedents go back to the 18th century. An important restatement of it is Bergsland 1959.
Uralo-Siberian.
Uralo-Siberian is an expanded form of the Eskimo–Uralic hypothesis. It associates Uralic with Yukaghir, Chukotko-Kamchatkan, and Eskimo–Aleut. It was propounded by Michael Fortescue in 1998.
Nostratic.
Nostratic associates Uralic, Indo-European, Altaic, and various other language families of Asia. The Nostratic hypothesis was first propounded by Holger Pedersen in 1903 and subsequently revived by Vladislav Illich-Svitych and Aharon Dolgopolsky in the 1960s.
Eurasiatic.
Eurasiatic resembles Nostratic in including Uralic, Indo-European, and Altaic, but differs from it in excluding the South Caucasian languages, Dravidian, and Afroasiatic and including Chukotko-Kamchatkan, Nivkh, Ainu, and Eskimo–Aleut. It was propounded by Joseph Greenberg in 2000–2002. Similar ideas had earlier been expressed by (1933) and by Björn Collinder (1965:30–34).
Uralo-Dravidian.
The hypothesis that the Dravidian languages display similarities with the Uralic language group, suggesting a prolonged period of contact in the past, is popular amongst Dravidian linguists and has been supported by a number of scholars, including Robert Caldwell, Thomas Burrow, Kamil Zvelebil, and Mikhail Andronov. This hypothesis has, however, been rejected by some specialists in Uralic languages, and has in recent times also been criticised by other Dravidian linguists, such as Bhadriraju Krishnamurti.
All of these hypotheses are minority views at the present time in Uralic studies.
Other comparisons.
Various unorthodox comparisons have been advanced such as Finno-Basque and Hungaro-Sumerian. These are considered spurious by specialists.

</doc>
<doc id="31716" url="http://en.wikipedia.org/wiki?curid=31716" title="Utah">
Utah

Utah ( or ; Arapaho: "Wo'tééneihí" ) is a state in the western United States. It became the 45th state admitted to the Union on January 4, 1896. Utah is the 13th-largest, the 33rd-most populous, and the 10th-least-densely populated of the 50 United States. Utah has a population of about 2.9 million, approximately 80% of whom live along the Wasatch Front, centering on Salt Lake City. Utah is bordered by Colorado to the east, Wyoming to the northeast, Idaho to the north, Arizona to the south, and Nevada to the west. It also touches a corner of New Mexico in the southeast.
Approximately 62% of Utahns are reported to be members of The Church of Jesus Christ of Latter-day Saints or LDS (Mormons), which greatly influences Utah culture and daily life. The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital, Salt Lake City. Utah is the most religiously homogeneous state in the United States, the only state with a Mormon majority, and the only state with a majority population belonging to a single church.
The state is a center of transportation, education, information technology and research, government services, mining, and a major tourist destination for outdoor recreation. In 2013, the U.S. Census Bureau estimated that Utah had the second fastest-growing population of any state. St. George was the fastest–growing metropolitan area in the United States from 2000 to 2005. A 2012 Gallup national survey found Utah overall to be the "best state to live in" based on 13 forward-looking measurements including various economic, lifestyle, and health-related outlook metrics.
Etymology.
The name "Utah" is derived from the name of the Ute tribe. It means "people of the mountains" in the Ute language.
History.
Pre-Columbian.
Thousands of years before the arrival of European explorers, the Anasazi/Ancestral Pueblo and the Fremont tribes lived in what is now known as Utah. These Native American tribes are subgroups of the Ute-Aztec Native American ethnicity, and were sedentary. The Anasazi built their homes through excavations in mountains, and the Fremont built houses of straw before disappearing from the region around the 15th century.
Another group of Native Americans, the Navajo, settled in the region around the 18th century. In the mid-18th century, other Uto-Aztecan tribes, including the Goshute, the Paiute, the Shoshone, and the Ute people, also settled in the region. These five groups were present when the first European explorers arrived.
Spanish exploration (1540).
The southern Utah region was explored by the Spanish in 1540, led by Francisco Vásquez de Coronado, while looking for the legendary Cíbola. A group led by two Catholic priests—sometimes called the Dominguez-Escalante Expedition—left Santa Fe in 1776, hoping to find a route to the coast of California. The expedition traveled as far north as Utah Lake and encountered the native residents. The Spanish made further explorations in the region, but were not interested in colonizing the area because of its desert nature. In 1821, the year Mexico achieved its independence from Spain, the region of Utah became part of Mexico, as part of Alta California.
Trappers and fur traders explored some areas of Utah in the early 19th century. The city of Provo, Utah was named for one of those men, Étienne Provost, who visited the area in 1825. The city of Ogden, Utah was named after Peter Skene Ogden, a Canadian explorer who traded furs in the Weber Valley.
In late 1824, Jim Bridger became the first white person to sight the Great Salt Lake. Due to the high salinity of its waters, Bridger thought he had found the Pacific Ocean; he subsequently found that this body of water was nothing but a giant salt lake. After the discovery of the lake, hundreds of traders and trappers established trading posts in the region. In the 1830s, thousands of people traveling from the East toward the U.S. West began to make stops in the region of the Great Salt Lake, then known as Lake Youta.
LDS settlement (1847).
Following the death of Joseph Smith in 1844, Brigham Young as president of the Quorum of the Twelve became the effective leader of the Latter Day Saints in Nauvoo, Illinois. To address the growing conflicts between his people and their neighbors, Young agreed with Illinois Governor Thomas Ford in October 1845 that the Mormons would leave by the following year.
Brigham Young and the first band of Mormon pioneers came to the Salt Lake Valley on July 24, 1847. Over the next 22 years, more than 70,000 pioneers crossed the plains and settled in Utah.
For the first few years, Brigham Young and the thousands of early settlers of Salt Lake City struggled to survive. The barren desert land was deemed by the Mormons as desirable as a place where they could practice their religion without harassment.
Utah was the source of many pioneer settlements located elsewhere in the West. Salt Lake City was the hub of a "far-flung commonwealth" of Mormon settlements. Fed by a continuing supply of church converts coming from the East and around the world, Church leaders often assigned groups of church members to establish settlements throughout the West. Beginning with settlements along Utah's Wasatch front (Salt Lake City, Bountiful and Weber Valley, and Provo and Utah Valley), irrigation enabled the establishment of fairly large pioneer populations in an area that Jim Bridger had advised Young would be inhospitable for the cultivation of crops because of frost. Throughout the remainder of the 19th century, Mormon pioneers called by Brigham Young would leave Salt Lake City and establish hundreds of other settlements in Utah, Idaho, Nevada, Arizona, Wyoming, California, Canada, and Mexico – including in Las Vegas, Nevada; Franklin, Idaho (the first white settlement in Idaho); San Bernardino, California; Star Valley, Wyoming; and Carson Valley, Nevada.
Prominent settlements in Utah included St. George, Logan, and Manti (where settlers completed the first three temples in Utah, each started after but finished many years before the larger and better known temple built in Salt Lake City was completed in 1893), as well as Parowan, Cedar City, Bluff, Moab, Vernal, Fillmore (which served as the territorial capital between 1850 and 1856), Nephi, Levan, Spanish Fork, Springville, Provo Bench (now Orem), Pleasant Grove, American Fork, Lehi, Sandy, Murray, Jordan, Centerville, Farmington, Huntsville, Kaysville, Grantsville, Tooele, Roy, Brigham City, and many other smaller towns and settlements. Young had an expansionist's view of the territory that he and the Mormon pioneers were settling, calling it Deseret – which according to the Book of Mormon was an ancient word for "honeybee" – hence the beehive which can still be found on the Utah flag, and the state's motto, "Industry."
Utah was Mexican territory when the first pioneers arrived in 1847. Early in the Mexican-American War in late 1846, the United States had captured New Mexico and California, and the whole Southwest became U.S. territory upon the signing of the Treaty of Guadalupe Hidalgo, February 2, 1848. The treaty was ratified by the United States Senate on March 11. Learning that California and New Mexico were applying for statehood, the settlers of the area (originally having planned to petition for territorial status) applied for statehood with an ambitious plan for a State of Deseret.
Utah Territory (1850–1896).
The Utah Territory was much smaller than the proposed state of Deseret, but it still contained all of the present states of Nevada and Utah as well as pieces of modern Wyoming and Colorado. It was created with the Compromise of 1850, and Fillmore, named after President Millard Fillmore, was designated the capital. The territory was given the name Utah after the Ute tribe of Native Americans. Salt Lake City replaced Fillmore as the territorial capital in 1856.
Disputes between the Mormon inhabitants and the U.S. government intensified due to prejudice against The Church of Jesus Christ of Latter-day Saints in the Northeast and the practice of plural marriage, or polygamy, among its members. The Mormons were still pushing for the establishment of a State of Deseret with the new borders of the Utah Territory. Most, if not all of the members of the U.S. government opposed the polygamous practices of the Mormons.
Members of the LDS Church were viewed as un-American and rebellious when news of their polygamous practices spread. In 1857, particularly heinous accusations of abdication of government and general immorality were stated by former associate justice William W. Drummond, among others. The detailed reports of life in Utah caused the administration of James Buchanan to send a secret military "expedition" to Utah. When the supposed rebellion should be quelled, Alfred Cumming would take the place of Brigham Young as territorial governor. The resulting conflict is known as the Utah War, nicknamed "Buchanan's Blunder" by the Mormon leaders.
In September 1857, about 120 American settlers of the Baker–Fancher wagon train, en route to California from Arkansas, were murdered by Utah Territorial Militia and some Paiute Native Americans in the Mountain Meadows massacre.
Before troops led by Albert Sidney Johnston entered the territory, Brigham Young ordered all residents of Salt Lake City to evacuate southward to Utah Valley and sent out a force, known as the Nauvoo Legion, to delay the government's advance. Although wagons and supplies were burned, eventually the troops arrived in 1858, and Young surrendered official control to Cumming, although most subsequent commentators claim that Young retained true power in the territory. A steady stream of governors appointed by the president quit the position, often citing the traditions of their supposed territorial government. By agreement with Young, Johnston established Camp Floyd, 40 mi away from Salt Lake City, to the southwest.
Salt Lake City was the last link of the First Transcontinental Telegraph, completed in October 1861. Brigham Young was among the first to send a message, along with Abraham Lincoln and other officials.
Because of the American Civil War, federal troops were pulled out of Utah Territory in 1861. This was a boon to the local economy as the army sold everything in camp for pennies on the dollar before marching back east to join the war. The territory was then left in LDS hands until Patrick E. Connor arrived with a regiment of California volunteers in 1862. Connor established Fort Douglas just 3 mi east of Salt Lake City and encouraged his people to discover mineral deposits to bring more non-Mormons into the territory. Minerals were discovered in Tooele County and miners began to flock to the territory.
Beginning in 1865, Utah's Black Hawk War developed into the deadliest conflict in the territory's history. Chief Antonga Black Hawk died in 1870, but fights continued to break out until additional federal troops were sent in to suppress the Ghost Dance of 1872. The war is unique among Indian Wars because it was a three-way conflict, with mounted Timpanogos Utes led by Antonga Black Hawk fighting federal and LDS authorities.
On May 10, 1869, the First Transcontinental Railroad was completed at Promontory Summit, north of the Great Salt Lake. The railroad brought increasing numbers of people into the territory and several influential businesspeople made fortunes there.
During the 1870s and 1880s laws were passed to punish polygamists due, in part, to the stories coming forth regarding Utah. Notably, Ann Eliza Young—tenth wife to divorce Brigham Young, women's advocate, national lecturer and author of "Wife No. 19 or My Life of Bondage" and Mr. and Mrs. Fanny Stenhouse, authors of "The Rocky Mountain Saints" (T. B. H. Stenhouse, 1873) and "Tell It All: My Life in Mormonism" (Fanny Stenhouse, 1875) . Both of these women, Ann Eliza and Fanny, testify to the happiness of the very early Church members before polygamy began to be practiced. They independently published their books in 1875. These books and the lectures of Ann Eliza Young have been credited with the United States Congress passage of anti-polygamy laws by newspapers throughout the United States as recorded in "The Ann Eliza Young Vindicator", a pamphlet which detailed Ms Young's travels and warm reception throughout her lecture tour.
T. B. H. Stenhouse, former Utah Mormon polygamist, Mormon missionary for thirteen years and a Salt Lake City newspaper owner, finally left Utah and wrote "The Rocky Mountain Saints". His book gives a witnessed account of his life in Utah, both the good and the bad. He finally left Utah and Mormonism after financial ruin occurred when Brigham Young sent Stenhouse to relocate to Ogden, Utah, according to Stenhouse, to take over his thriving pro-Mormon "Salt Lake Telegraph" newspaper. In addition to these testimonies, "The Confessions of John D. Lee", written by John D. Lee—alleged "Scape goat" for the Mountain Meadow Massacre—also came out in 1877. The corroborative testimonies coming out of Utah from Mormons and former Mormons had an impact on Congress and the people of the United States.
In the 1890 Manifesto, the LDS Church banned polygamy. When Utah applied for statehood again, it was accepted. One of the conditions for granting Utah statehood was that a ban on polygamy be written into the state constitution. This was a condition required of other western states that were admitted into the Union later. Statehood was officially granted on January 4, 1896.
20th century.
Beginning in the early 20th century, with the establishment of such national parks as Bryce Canyon National Park and Zion National Park, Utah became known for its natural beauty. Southern Utah became a popular filming spot for arid, rugged scenes, and such natural landmarks as Delicate Arch and "the Mittens" of Monument Valley are instantly recognizable to most national residents. During the 1950s, '60s, and '70s, with the construction of the Interstate highway system, accessibility to the southern scenic areas was made easier.
Beginning in 1939, with the establishment of Alta Ski Area, Utah's skiing has become world-renowned. The dry, powdery snow of the Wasatch Range is considered some of the best skiing in the world (thus the license plate, "the Greatest Snow on Earth"). Salt Lake City won the bid for the 2002 Winter Olympic Games in 1995, and this has served as a great boost to the economy. The ski resorts have increased in popularity, and many of the Olympic venues scattered across the Wasatch Front continue to be used for sporting events. This also spurred the development of the light-rail system in the Salt Lake Valley, known as TRAX, and the re-construction of the freeway system around the city.
In 1957, Utah created the Utah State Parks Commission with just four parks. Today, Utah State Parks manages 43 parks and several undeveloped areas totaling over 95000 acre of land and more than 1000000 acre of water. Utah's state parks are scattered throughout Utah; from Bear Lake State Park at the Utah/Idaho border to Edge of the Cedars State Park Museum deep in the Four Corners region, and everywhere in between. Utah State Parks is also home to the state's off highway vehicle office, state boating office and the trails program.
During the late 20th century, the state grew quickly. In the 1970s growth was phenomenal in the suburbs of the Wasatch Front. Sandy was one of the fastest-growing cities in the country at that time. Today, many areas of Utah are seeing phenomenal growth. Northern Davis, southern and western Salt Lake, Summit, eastern Tooele, Utah, Wasatch, and Washington counties are all growing very quickly. Transportation and urbanization are major issues in politics as development consumes agricultural land and wilderness areas.
Geography.
Utah is known for its natural diversity and is home to features ranging from arid deserts with sand dunes to thriving pine forests in mountain valleys. It is a rugged and geographically diverse state that is located at the convergence of three distinct geological regions: the Rocky Mountains, the Great Basin, and the Colorado Plateau.
Utah is one of the Four Corners states, and is bordered by Idaho in the north, Wyoming in the north and east; by Colorado in the east; at a single point by New Mexico to the southeast; by Arizona in the south; and by Nevada in the west. It covers an area of 84899 sqmi. The state is one of only three U.S. states (with Colorado and Wyoming) that have only lines of latitude and longitude for boundaries.
One of Utah's defining characteristics is the variety of its terrain. Running down the middle of the northern third of the state is the Wasatch Range, which rises to heights of almost 12000 ft above sea level. Utah is home to world-renowned ski resorts, made popular by the light, fluffy snow, and winter storms which regularly dump 1 to 3 feet of overnight snow accumulation. In the northeastern section of the state, running east to west, are the Uinta Mountains, which rise to heights of over 13,000 ft. The highest point in the state, Kings Peak, at 13,528 ft, lies within the Uinta Mountains.
At the western base of the Wasatch Range is the Wasatch Front, a series of valleys and basins that are home to the most populous parts of the state. It stretches approximately from Brigham City at the north end to Nephi at the south end. Approximately 75 percent of the population of the state live in this corridor, and population growth is rapid.
Western Utah is mostly arid desert with a basin and range topography. Small mountain ranges and rugged terrain punctuate the landscape. The Bonneville Salt Flats are an exception, being comparatively flat as a result of once forming the bed of ancient Lake Bonneville. Great Salt Lake, Utah Lake, Sevier Lake, and Rush Lake are all remnants of this ancient freshwater lake, which once covered most of the eastern Great Basin. West of the Great Salt Lake, stretching to the Nevada border, lies the arid Great Salt Lake Desert. One exception to this aridity is Snake Valley, which is (relatively) lush due to large springs and wetlands fed from groundwater derived from snow melt in the Snake Range, Deep Creek Range, and other tall mountains to the west of Snake Valley. Great Basin National Park is just over the Nevada state line in the southern Snake Range. One of western Utah's most impressive, but least visited attractions is Notch Peak, the tallest limestone cliff in North America, located west of Delta.
Much of the scenic southern and southeastern landscape (specifically the Colorado Plateau region) is sandstone, specifically Kayenta sandstone and Navajo sandstone. The Colorado River and its tributaries wind their way through the sandstone, creating some of the world's most striking and wild terrain (the area around the confluence of the Colorado and Green Rivers was the last to be mapped in the lower 48 United States). Wind and rain have also sculpted the soft sandstone over millions of years. Canyons, gullies, arches, pinnacles, buttes, bluffs, and mesas are the common sight throughout south-central and southeast Utah.
This terrain is the central feature of protected state and federal parks such as Arches, Bryce Canyon, Canyonlands, Capitol Reef, and Zion national parks, Cedar Breaks, Grand Staircase-Escalante, Hovenweep, and Natural Bridges national monuments, Glen Canyon National Recreation Area (site of the popular tourist destination, Lake Powell), Dead Horse Point and Goblin Valley state parks, and Monument Valley. The Navajo Nation also extends into southeastern Utah. Southeastern Utah is also punctuated by the remote, but lofty La Sal, Abajo, and Henry mountain ranges.
Eastern (northern quarter) Utah is a high-elevation area covered mostly by plateaus and basins, particularly the Tavaputs Plateau and San Rafael Swell, which remain mostly inaccessible, and the Uinta Basin, where the majority of eastern Utah's population lives. Economies are dominated by mining, oil shale, oil, and natural gas-drilling, ranching, and recreation. Much of eastern Utah is part of the Uintah and Ouray Indian Reservation. The most popular destination within northeastern Utah is Dinosaur National Monument near Vernal.
Southwestern Utah is the lowest and hottest spot in Utah. It is known as Utah's Dixie because early settlers were able to grow some cotton there. Beaverdam Wash in far southwestern Utah is the lowest point in the state, at 2,000 ft. The northernmost portion of the Mojave Desert is also located in this area. Dixie is quickly becoming a popular recreational and retirement destination, and the population is growing rapidly. Although the Wasatch Mountains end at Mount Nebo near Nephi, a complex series of mountain ranges extends south from the southern end of the range down the spine of Utah. Just north of Dixie and east of Cedar City is the state's highest ski resort, Brian Head.
Like most of the western and southwestern states, the federal government owns much of the land in Utah. Over 70 percent of the land is either BLM land, Utah State Trustland, or U.S. National Forest, U.S. National Park, U.S. National Monument, National Recreation Area or U.S. Wilderness Area. Utah is the only state where every county contains some national forest.
Climate.
Utah features a dry, semi-arid to desert climate, although its many mountains feature a large variety of climates, with the highest points in the Uinta Mountains being above the timberline. The dry weather is a result of the state's location in the rain shadow of the Sierra Nevada in California. The eastern half of the state lies in the rain shadow of the Wasatch Mountains. The primary source of precipitation for the state is the Pacific Ocean, with the state usually lying in the path of large Pacific storms from October to May. In summer, the state, especially southern and eastern Utah, lies in the path of monsoon moisture from the Gulf of California.
Most of the lowland areas receive less than 12 in of precipitation annually, although the I-15 corridor, including the densely populated Wasatch Front, receives approximately 15 in. The Great Salt Lake Desert is the driest area of the state, with less than 5 in. Snowfall is common in all but the far southern valleys. Although St. George only receives about 3 in per year, Salt Lake City sees about 60 in, enhanced by the lake-effect snow from the Great Salt Lake, which increases snowfall totals to the south, southeast, and east of the lake.
Some areas of the Wasatch Range in the path of the lake-effect receive up to 500 in per year. The consistently deep powder snow led Utah's ski industry to adopt the slogan "the Greatest Snow on Earth" in the 1980s. In the winter, temperature inversions are a common phenomenon across Utah's low basins and valleys, leading to thick haze and fog that can sometimes last for weeks at a time, especially in the Uintah Basin. Although at other times of year its air quality is good, winter inversions give Salt Lake City some of the worst wintertime pollution in the country.
Utah's temperatures are extreme, with cold temperatures in winter due to its elevation, and very hot summers statewide (with the exception of mountain areas and high mountain valleys). Utah is usually protected from major blasts of cold air by mountains lying north and east of the state, although major Arctic blasts can occasionally reach the state. Average January high temperatures range from around 30 F in some northern valleys to almost 55 F in St. George.
Temperatures dropping below 0 F should be expected on occasion in most areas of the state most years, although some areas see it often (for example, the town of Randolph averages about 50 days per year with temperatures dropping that low). In July, average highs range from about 85 to. However, the low humidity and high elevation typically leads to large temperature variations, leading to cool nights most summer days. The record high temperature in Utah was 118 F, recorded south of St. George on July 4, 2007, and the record low was -69 F, recorded at Peter Sinks in the Bear River Mountains of northern Utah on February 1, 1985. However, the record low for an inhabited location is -49 F at Woodruff on December 12, 1932.
Utah, like most of the western United States, has few days of thunderstorms. On average there are fewer than 40 days of thunderstorm activity during the year, although these storms can be briefly intense when they do occur. They are most likely to occur during monsoon season from about mid-July through mid-September, especially in southern and eastern Utah. Dry lightning strikes and the general dry weather often spark wildfires in summer, while intense thunderstorms can lead to flash flooding, especially in the rugged terrain of southern Utah. Although spring is the wettest season in northern Utah, late summer is the wettest period for much of the south and east of the state. Tornadoes are uncommon in Utah, with an average of two striking the state yearly, rarely higher than EF1 intensity.
One exception of note, however, was the unprecedented F2 Salt Lake City Tornado that moved directly across downtown Salt Lake City on August 11, 1999, killing 1 person, injuring 60 others, and causing approximately $170 million in damage. The only other reported tornado fatality in Utah's history was a 7-year-old girl who was killed while camping in Summit County on July 6, 1884. The last tornado of above (E)F0 intensity occurred on September 8, 2002, when an F2 tornado hit Manti. On August 11, 1993, an F3 tornado hit the Uinta Mountains north of Duchesne at an elevation of 10500 ft, causing some damage to a Boy Scouts campsite. This is the strongest tornado ever recorded in Utah.
Demographics.
The United States Census Bureau estimates that the population of Utah was 2,942,902 on July 1, 2014, a 6.48% increase since the 2010 United States Census. The center of population of Utah is located in Utah County in the city of Lehi. Much of the population lives in cities and towns along the Wasatch Front, a metropolitan region that runs north-south with the Wasatch Mountains rising on the eastern side. Growth outside the Wasatch Front is also increasing. The St. George metropolitan area is currently the second-fastest growing in the country after the Las Vegas metropolitan area, while the Heber micropolitan area is also the second-fastest growing in the country (behind Palm Coast, Florida).
Utah contains 5 metropolitan areas (Logan, Ogden-Clearfield, Salt Lake City, Provo-Orem, and St. George), and 6 micropolitan areas (Brigham City, Heber, Vernal, Price, Richfield, and Cedar City).
Health and fertility.
Utah ranks 47th in teenage pregnancy, lowest in percentage of births out of wedlock, lowest in number of abortions per capita, and lowest in percentage of teen pregnancies terminated in abortion. However, statistics relating to pregnancies and abortions may also be artificially low from teenagers going out of state for abortions because of parental notification requirements. Utah has the lowest child poverty rate in the country, despite its young demographics. According to the Gallup State of Well-Being Report, Utah has the fourth highest well-being in the United States as of 2013. A widely circulated national prescription drug study from 2002 observed that antidepressant drugs were "prescribed in Utah more often than in any other state, at a rate nearly twice the national average"; however, more recent studies by the CDC have shown rates of depression in Utah to be no higher than the national average.
Ancestry and race.
At the 2010 Census, 81.4% of the population was non-Hispanic White, down from 91.2% in 1990, 1% non-Hispanic Black or African American, 1% non-Hispanic American Indian and Alaska Native, 2% non-Hispanic Asian, 0.9% non-Hispanic Native Hawaiian and Other Pacific Islander, 0.1% from some other race (non-Hispanic) and 1.8% of two or more races (non-Hispanic). 13.0% of Utah's population was of Hispanic, Latino, or Spanish origin (of any race).
The largest ancestry groups in the state are:
Most Utahns are of Northern European descent. In 2011 one-third of Utah's workforce was reported to be bilingual, developed through a program of acquisition of second languages beginning in elementary school, and related to Mormonism's missionary goals for its young people.
In 2011, 28.6% of Utah's population younger than the age of one were minorities, meaning that they had at least one parent who was not non-Hispanic white.
Religion.
A majority of the state's residents are members of The Church of Jesus Christ of Latter-day Saints (LDS Church). As of 2012, 62.2% of Utahns are counted as members of The Church of Jesus Christ of Latter-day Saints, although only 41.6% of them are active members. Mormons now make up about 34%–41% of Salt Lake City, while rural and suburban areas tend to be prominently Mormon. The religious body with the largest number of congregations is The Church of Jesus Christ of Latter-day Saints (with 4,815 congregations).
Though the LDS Church officially maintains a policy of neutrality in regards to political parties, the church's doctrine has a strong regional influence on politics. Another doctrine effect can be seen in Utah's high birth rate (25 percent higher than the national average; the highest for a state in the U.S.). The Mormons in Utah tend to have conservative views when it comes to most political issues and the majority of voter-age Utahns are unaffiliated voters (60%) who vote overwhelmingly Republican. Mitt Romney received 72.8% of the Utahn votes in 2012, while John McCain polled 62.5% in the United States presidential election, 2008 and 70.9% for George W. Bush in 2004. In 2010 the Association of Religion Data Archives (ARDA) reported that the three largest denominational groups in Utah are The Church of Jesus Christ of Latter-day Saints with 1,910,504 adherents; the Catholic Church with 160,125 adherents, and the Southern Baptist Convention with 12,593 adherents. There is a growing Jewish presence in the state including Chabad and Rohr Jewish Learning Institute.
According to a report produced by the Pew Forum on Religion & Public Life the self-identified religious affiliations of Utahns over the age of 18 as of 2008 are:
Margin of error +/− 6%
According to results from the 2010 United States Census, Mormons (members of The Church of Jesus Christ of Latter-day Saints) represented 62.1% of Utah's total population. The Utah county with the lowest percentage of Mormons was Grand County, at 26.5%, while the county with the highest percentage was Morgan County, at 86.1%. In addition, the result for the most populated county, Salt Lake County, was 51.4%.
According to a Gallup poll, Utah had the 2nd-highest number of people reporting as "Very Religious" in 2011, at 57% (trailing only Mississippi). However, it also had a higher rate of people reporting as "Nonreligious" (28%) than any of the other "most religious" states, and the smallest percentage of people reporting as "Moderately Religious" (15%) of any state.
Age and gender.
Utah has the highest total birth rate and accordingly, the youngest population of any U.S. state. In 2010, the state's population was 50.2% male and 49.8% female.
Economy.
According to the Bureau of Economic Analysis, the gross state product of Utah in 2012 was $130.5 billion, or 0.87% of the total United States GDP of $14.991 trillion for the same year. The per capita personal income was $45,700 in 2012. Major industries of Utah include: mining, cattle ranching, salt production, and government services.
According to the 2007 State New Economy Index, Utah is ranked the top state in the nation for Economic Dynamism, determined by "the degree to which state economies are knowledge-based, globalized, entrepreneurial, information technology-driven and innovation-based". In 2014, Utah was ranked number one in Forbes' list of "Best States For Business". A November 2010 article in "Newsweek" highlighted Utah and particularly the Salt Lake City area's economic outlook, calling it "the new economic Zion", and examined how the area has been able to bring in high-paying jobs and attract high-tech corporations to the area during a recession. s of September 2014[ [update]], the state's unemployment rate was 3.5%. In terms of "small business friendliness", in 2014 Utah emerged as number one, based on a study drawing upon data from over 12,000 small business owners.
In eastern Utah petroleum production is a major industry. Near Salt Lake City, petroleum refining is done by a number of oil companies. In central Utah, coal production accounts for much of the mining activity.
A 2009 study published in the "Journal of Economic Perspectives" found that Utah residents were the largest consumers of paid internet pornography per capita in the United States. However Edelman, the study's author, came to this conclusion after looking at subscriptions for just one top-10 seller of online adult entertainment, comparing ZIP codes associated with all credit card subscriptions between 2006 and 2008. The author is also quick to admit that the difference in usage between states is relatively small. Another report found that Utah was not significantly higher than other states in regards to pornographic Google search terms; and in regards to the most common search term, was ranked last.
According to Internal Revenue Service tax returns, Utahns rank first among all U.S. states in the proportion of income given to charity by the wealthy. This is due to the standard 10% of all earnings that Mormons give to the LDS Church. According to the Corporation for National and Community Service, Utah had an average of 884,000 volunteers between 2008 and 2010, each of whom contributed 89.2 hours per volunteer. This figure equates to $3.8 billion of service contributed, ranking Utah number one for volunteerism in the nation.
Taxation.
Utah collects personal income tax; since 2008 the tax has been a flat 5 percent for all taxpayers. The state sales tax has a base rate of 6.45 percent, with cities and counties levying additional local sales taxes that vary among the municipalities. Property taxes are assessed and collected locally. Utah does not charge intangible property taxes and does not impose an inheritance tax.
Tourism.
Tourism is a major industry in Utah. With five national parks (Arches, Bryce Canyon, Canyonlands, Capitol Reef, and Zion), Utah has the third most national parks of any state after Alaska and California. In addition, Utah features seven national monuments (Cedar Breaks, Dinosaur, Grand Staircase-Escalante, Hovenweep, Natural Bridges, Rainbow Bridge, and Timpanogos Cave), two national recreation areas (Flaming Gorge and Glen Canyon), seven national forests (Ashley, Caribou-Targhee, Dixie, Fishlake, Manti-La Sal, Sawtooth, and Uinta-Wasatch-Cache), and numerous state parks and monuments.
The Moab area, in the southeastern part of the state, is known for its challenging mountain biking trails, including Slickrock. Moab also hosts the famous Moab Jeep Safari semiannually.
Utah has seen an increase in tourism since the 2002 Winter Olympics. Park City is home to the United States Ski Team. Utah's ski resorts are primarily located in northern Utah near Salt Lake City, Park City, Ogden, and Provo. Between 2007 and 2011 Deer Valley in Park City, has been ranked the top ski resort in North America in a survey organized by "Ski Magazine".
In addition to having prime snow conditions and world-class amenities, Northern Utah's ski resorts are well liked among tourists for their convenience and proximity to a large city and international airport, as well as the close proximity to other ski resorts, allowing skiers the ability to ski at multiple locations in one day. The 2009 Ski Magazine reader survey concluded that six out of the top ten resorts deemed most "accessible" and six out of the top ten with the best snow conditions were located in Utah. In Southern Utah, Brian Head Ski Resort is located in the mountains near Cedar City. Former Olympic venues including Utah Olympic Park and Utah Olympic Oval are still in operation for training and competition and allows the public to participate in numerous activities including ski jumping, bobsleigh, and speed skating.
Utah features many cultural attractions such as Temple Square, the Sundance Film Festival, the Red Rock Film Festival, the DOCUTAH Film Festival, and the Utah Shakespearean Festival. Temple Square is ranked as the 16th most visited tourist attraction in the United States by "Forbes" magazine, with over five million annual visitors.
Other attractions include Monument Valley, the Great Salt Lake, the Bonneville Salt Flats, and Lake Powell.
Mining.
Beginning in the late 19th century with the state's mining boom (including the Bingham Canyon Mine, among the world's largest open pit mines), companies attracted large numbers of immigrants with job opportunities. Since the days of the Utah Territory mining has played a major role in Utah's economy. Historical mining towns include Mercur in Tooele County, Silver Reef in Washington County, Eureka in Juab County, Park City in Summit County and numerous coal mining camps throughout Carbon County such as Castle Gate, Spring Canyon, and Hiawatha.
These settlements were characteristic of the boom and bust cycle that dominated mining towns of the American West. During the early part of the Cold War era, uranium was mined in eastern Utah. Today mining activity still plays a major role in the state's economy. Minerals mined in Utah include copper, gold, silver, molybdenum, zinc, lead, and beryllium. Fossil fuels including coal, petroleum, and natural gas continue to play a large role in Utah's economy, especially in the eastern part of the state in counties such as Carbon, Emery, Grand, and Uintah.
Incidents.
In 2007, nine people were killed at the Crandall Canyon Mine collapse.
On March 22, 2013, one miner died and another was injured after they became trapped in a cave-in at a part of the Castle Valley Mining Complex, about 10 miles west of the small mining town of Huntington in Emery County.
Energy.
Source:
Potential to use renewable energy sources.
Utah has the potential to generate 31.6 TWh/year from 13.1 GW of wind power, and 10,290 TWh/year from solar power using 4,048 GW of photovoltaic (PV), including 5.6 GW of rooftop photovoltaic, and 1,638 GW of concentrated solar power.
Transportation.
I-15 and I-80 are the main interstate highways in the state, where they intersect and briefly merge near downtown Salt Lake City. I-15 traverses the state north-to-south, entering from Arizona near St. George, paralleling the Wasatch Front, and crossing into Idaho near Portage. I-80 spans northern Utah east-to-west, entering from Nevada at Wendover, crossing the Wasatch Mountains east of Salt Lake City, and entering Wyoming near Evanston. I-84 West enters from Idaho near Snowville (from Boise) and merges with I-15 from Tremonton to Ogden, then heads southeast through the Wasatch Mountains before terminating at I-80 near Echo Junction.
I-70 splits from I-15 at Cove Fort in central Utah and heads east through mountains and rugged desert terrain, providing quick access to the many national parks and national monuments of southern Utah, and has been noted for its beauty. The 103-mile (163 km) stretch from Salina to Green River is the longest stretch of interstate in the country without services and, when completed in 1970, was the longest stretch of entirely new highway constructed in the U.S. since the Alaska Highway was completed in 1943.
TRAX, a light rail system in the Salt Lake Valley, consists of three lines. The Blue Line (formerly Salt Lake/Sandy Line) begins in the suburb of Draper and ends in Downtown Salt Lake City. The Red Line (Mid-Jordan/University Line) begins in the Daybreak Community of South Jordan, a southwestern valley suburb, and ends at the University of Utah. The Green Line begins in West Valley City passes through downtown Salt Lake City and ends at the Salt Lake City International Airport.
The Utah Transit Authority (UTA), which operates TRAX, also operates a bus system that stretches across the Wasatch Front, west into Grantsville, and east into Park City. In addition, UTA provides winter service to the ski resorts east of Salt Lake City, Ogden, and Provo. Several bus companies also provide access to the ski resorts in winter, and local bus companies also serve the cities of Cedar City, Logan, Park City, and St. George. A commuter rail line known as "FrontRunner", also operated by UTA, runs between Pleasant View and Provo via Salt Lake City. Amtrak's "California Zephyr", with one train in each direction daily, runs east-west through Utah with stops in Green River, Helper, Provo, and Salt Lake City.
The cities of Logan, Hyrum, Smithfield, Richmond, in addition to nearby Preston, Idaho, are served by a local sales-tax-funded zero-fare bus system called the Cache Valley Transit District (CVTD). It is the only system-wide free public transit system in the state.
Salt Lake City International Airport is the only international airport in the state and serves as one of the hubs for Delta Air Lines. The airport has consistently ranked first in on-time departures and had the fewest cancellations among U.S. airports. The airport has non-stop service to over 100 destinations throughout the United States, Canada, and Mexico, as well as to Paris and Tokyo. Canyonlands Field (near Moab), Cedar City Regional Airport, Ogden-Hinckley Airport, Provo Municipal Airport, St. George Municipal Airport, and Vernal Regional Airport all provide limited commercial air service. An entirely new regional airport at St. George opened on January 12, 2011, replacing the old airport that existed on top of a plateau and had no room for expansion. SkyWest Airlines is also headquartered in St. George and maintains a hub at Salt Lake City. The airlines and service from Provo have seem to remain in a state of change since commercial service began in 2011.
Law and government.
Utah government, like most U.S. states, is divided into three branches: executive, legislative, and judicial. The current governor of Utah is Gary Herbert, who was sworn in on August 11, 2009. The governor is elected for a four-year term. The Utah State Legislature consists of a Senate and a House of Representatives. State senators serve four-year terms and representatives two-year terms. The Utah Legislature meets each year in January for an annual forty-five-day session.
The Utah Supreme Court is the court of last resort in Utah. It consists of five justices, who are appointed by the governor, and then subject to retention election. The Utah Court of Appeals handles cases from the trial courts. Trial level courts are the district courts and justice courts. All justices and judges, like those on the Utah Supreme Court, are subject to retention election after appointment.
Counties.
Utah is divided into political jurisdictions designated as "counties". Since 1918 there have been 29 counties in the state, ranging from 298 to.
Women's rights.
Utah granted full voting rights to women in 1870, 26 years before becoming a state. Among all U.S. states, only Wyoming granted suffrage to women earlier. However, in 1887 the initial Edmunds-Tucker Act was passed by Congress in an effort to curtail excessive Mormon influence in the territorial government. One of the provisions of the Act was the repeal of women's suffrage; full suffrage was not returned until Utah was admitted to the Union in 1896.
Utah is one of the 15 states that have not ratified the U.S. Equal Rights Amendment.
Constitution.
The constitution of Utah was enacted in 1895. Notably, the constitution outlawed polygamy, as requested by Congress when Utah had applied for statehood, and reestablished the territorial practice of women's suffrage. Utah's Constitution has been amended many times since its inception.
Alcohol, tobacco and gambling laws.
Utah's laws in regard to alcohol, tobacco and gambling are strict. Utah is an alcoholic beverage control state. The Utah Department of Alcoholic Beverage Control regulates the sale of alcohol; wine and spirituous liquors may only be purchased at state liquor stores, and local laws may prohibit the sale of beer and other alcoholic beverages on Sundays. The state bans the sale of fruity alcoholic drinks at grocery stores and convenience stores. The law states that such drinks must now have new state-approved labels on the front of the products that contain capitalized letters in bold type telling consumers the drinks contain alcohol and at what percentage. The Utah Indoor Clean Air Act is a statewide smoking ban, that prohibits smoking in many public places. Utah is one of few states to set a smoking age of 19, as opposed to 18, as in most other states.
Utah is also one of only two states in the United States to outlaw all forms of gambling; the other is Hawaii.
Same-sex marriage.
Same-sex marriage became legal in Utah on December 20, 2013 when judge Robert J. Shelby of the United States District Court for the District of Utah issued a ruling in "Kitchen v. Herbert." As of close of business December 26, more than 1,225 marriage licenses were issued, with at least 74 percent, or 905 licenses, issued to gay and lesbian couples. Utah's counties generated more than $49,000 in a three-and-a-half-day period from licenses, which cost $30–$50. The state Attorney General's office was granted a stay of the ruling by the United States Supreme Court on January 6, 2014 while the Tenth Circuit Court of Appeals considers the case. On Monday October 6, 2014, the Supreme Court of the United States declined a Writ of Certiorari, and the 10th Circuit Court issued their mandate later that day, lifting their stay. Same-sex marriages commenced again in Utah that day.
Politics.
In the late 19th century, the federal government took issue with polygamy in the LDS Church. The LDS Church discontinued plural marriage in 1890, and in 1896 Utah gained admission to the Union. Many new people settled the area soon after the Mormon pioneers. Relations have often been strained between the LDS population and the non-LDS population. These tensions have played a large part in Utah's history (Liberal Party vs. People's Party).
Utah was the single most Republican-leaning state in the country in every presidential election from 1976 to 2004, measured by the percentage point margin between the Republican and Democratic candidates. In 2008 Utah was only the third-most Republican state (after Wyoming and Oklahoma), but in 2012, with Mormon Mitt Romney atop the Republican ticket, Utah returned to its position as the most Republican state.
Both of Utah's U.S. Senators, Orrin Hatch and Mike Lee, are Republican. Four more Republicans, Rob Bishop, Chris Stewart, Jason Chaffetz, and Mia Love, represent Utah in the United States House of Representatives. After Jon Huntsman, Jr., resigned to serve as U.S. Ambassador to China, Gary Herbert was sworn in as governor on August 11, 2009. Herbert was elected to serve out the remainder of the term in a special election in 2010, defeating Democratic nominee Salt Lake County Mayor Peter Corroon with 64% of the vote. He won election to a full four-year term in 2012, defeating Democratic Businessman Peter Cooke with 68% of the vote.
The LDS Church maintains an official policy of neutrality with regard to political parties and candidates.
Utah votes predominately Republican. Self-identified Latter-day Saints are more likely to vote for the Republican ticket than non-Mormons, and Utah is one of the most Republican states in the nation.
In the 1970s, then-Apostle Ezra Taft Benson was quoted by the Associated Press that it would be difficult for a faithful Latter-day Saint to be a liberal Democrat. Although the LDS Church has officially repudiated such statements on many occasions, Democratic candidates—including LDS Democrats—believe that Republicans capitalize on the perception that the Republican Party is doctrinally superior. Political scientist and pollster Dan Jones explains this disparity by noting that the national Democratic Party is associated with liberal positions on gay marriage and abortion, both of which the LDS Church is against. The Republican Party in heavily Mormon Utah County presents itself as the superior choice for Latter-day Saints. Even though Utah Democratic candidates are predominantly LDS, socially conservative, and pro-life, no Democrat has won in Utah County since 1994.
David Magleby, dean of Social and Behavioral Sciences at Brigham Young University, a lifelong Democrat and a political analyst, asserts that the Republican Party actually has more conservative positions than the LDS Church. Magleby argues that the locally conservative Democrats are in better accord with LDS doctrine. For example, the Republican Party of Utah opposes almost all abortions while Utah Democrats take a more liberal approach, although more conservative than their national counterparts. On Second Amendment issues, the state GOP has been at odds with the LDS Church position opposing concealed firearms in places of worship and in public spaces.
In 1998 the church expressed concern that Utahns perceived the Republican Party as an LDS institution and authorized lifelong Democrat and Seventy Marlin Jensen to promote LDS bipartisanship.
Utah is much more conservative than the United States as a whole, particularly on social issues. Compared to other Republican-dominated states in the Mountain West such as Wyoming, Utah politics have a more moralistic and less libertarian character according to David Magleby.
About 80% of Utah's Legislature are members of The Church of Jesus Christ of Latter-day Saints, while they account for 61 percent of the population. Since becoming a state in 1896, Utah has had only two non-Mormon governors.
In 2006, the legislature passed legislation aimed at banning joint-custody for a non-biological parent of a child. The custody measure passed the legislature and was vetoed by the governor, a reciprocal benefits supporter.
Carbon County's Democrats are generally made up of members of the large Greek, Italian, and Southeastern European communities, whose ancestors migrated in the early 20th century to work in the extensive mining industry. The views common amongst this group are heavily influenced by labor politics, particularly of the New Deal Era.
The Democrats of Summit County are the by-product of the migration of wealthy families from California in the 1990s to the ski resort town of Park City; their views are generally supportive of the economic policies favored by unions and the social policies favored by the liberals.
The state's most Republican areas tend to be Utah County, which is the home to Brigham Young University in the city of Provo, and nearly all the rural counties. These areas generally hold socially conservative views in line with that of the national Religious Right.
The state has not voted for a Democrat for president since 1964. Historically, Republican presidential nominees score one of their best margins of victory here. Utah was the Republicans' best state in the 1976, 1980, 1984, 1988, 1996, 2000, and 2004 elections. In 1992, Utah was the only state in the nation where Democratic candidate Bill Clinton finished behind both Republican candidate George HW Bush and Independent candidate Ross Perot. In 2004, Republican George W. Bush won every county in the state and Utah gave him his largest margin of victory of any state. He won the state's five electoral votes by a margin of 46 percentage points with 71.5% of the vote. In the 1996 Presidential elections the Republican candidate received a smaller 54% of the vote while the Democrat earned 34%.
Major cities and towns.
Utah's population is concentrated in two areas, the Wasatch Front in the north-central part of the state, with a population of over 2 million; and Washington County, in southwestern Utah, locally known as "Dixie", with over 150,000 residents in the metropolitan area.
According the 2010 Census, Utah was the second-fastest growing state (at 23.8 percent) in the United States between 2000 and 2010 (behind Nevada). St. George, in the southwest, is the second-fastest growing metropolitan area in the United States, trailing Greeley, Colorado.
The three fastest-growing counties from 2000 to 2010 were Wasatch County (54.7%), Washington County (52.9%), and Tooele County (42.9%). However, Utah County added the most people (148,028). Between 2000 and 2010, Saratoga Springs (1,673%), Herriman (1,330%), Eagle Mountain (893%), Cedar Hills (217%), South Willard (168%), Nibley (166%), Syracuse (159%), West Haven (158%), Lehi (149%), Washington (129%), and Stansbury Park (116%) all at least doubled in population. West Jordan (35,376), Lehi (28,379), St. George (23,234), South Jordan (20,981), West Valley City (20,584), and Herriman (20,262) all added at least 20,000 people.
Sports.
Utah is the least populous U.S. state to have a major professional sports league franchise. The Utah Jazz of the National Basketball Association play at EnergySolutions Arena in Salt Lake City. The team moved to the city from New Orleans in 1979 and has been one of the most consistently successful teams in the league (although they have yet to win a championship). Salt Lake City was previously host to the Utah Stars, who competed in the ABA from 1970–76 and won 1 championship, and to the Utah Starzz of the WNBA from 1997 to 2003.
Real Salt Lake of Major League Soccer were founded in 2005 and play their home matches at Rio Tinto Stadium in Sandy. The team has enjoyed several years of success as the Utah outpost of the world's most popular sport. RSL remain the only Utah major league sports team to have won a major league national championship, having won the MLS Cup in 2009.
The Utah Blaze began play in the original AFL in 2006 that folded before the 2009 season, then returned to play when the league was re-founded in 2010. They compete at the Maverik Center in West Valley City.
Utah's highest level minor league baseball team is the Salt Lake Bees, who play at Smith's Ballpark in Salt Lake City and are part of the AAA level Pacific Coast League, one notch below Major League Baseball. Utah also has one minor league hockey team, the Utah Grizzlies, who play at the Maverik Center and compete in the ECHL (the third tier of U.S. hockey).
Utah has six universities that compete in Division I of the NCAA. Three of the schools have football programs that participate in the top-level Football Bowl Subdivision: Utah in the Pacific-12 Conference, Utah State in the Mountain West Conference, and BYU as an independent. Two more schools participate in FCS football: Weber State and Southern Utah (SUU) in the Big Sky Conference. Utah Valley, which has no football program, is a full member of the Great West Conference.
Salt Lake City hosted the 2002 Winter Olympics. After early financial struggles and scandal, the 2002 Olympics eventually became among the most successful Winter Olympics in history from a marketing and financial standpoint. Watched by over 2 billion viewers, the Games ended up with a profit of 40 million dollars.
Utah has hosted professional golf tournaments such as the Uniting Fore Care Classic and currently the Utah Championship.
Rugby has been growing quickly in the state of Utah, growing from 17 teams in 2009 to 70 teams as of 2013, including with over 3,000 players, and more than 55 high school varsity teams. The growth has been inspired in part by the 2008 movie Forever Strong. Utah fields two of the most competitive teams in the nation in college rugby — BYU and Utah.
Branding.
The state of Utah relies heavily on income from tourists and travelers taking advantage of the state's ski resorts and natural beauty, and thus the need to "brand" Utah and create an impression of the state throughout the world has led to several state slogans, the most famous of which being "The Greatest Snow on Earth", which has been in use in Utah officially since 1975 (although the slogan was in unofficial use as early as 1962) and now adorns nearly 50 percent of the state's license plates. In 2001, Utah Governor Mike Leavitt approved a new state slogan, "Utah! Where Ideas Connect", which lasted until March 10, 2006, when the Utah Travel Council and the office of Governor Jon Huntsman announced that "Life Elevated" would be the new state slogan.
Entertainment.
Utah is the setting of or the filming location for many books, films, television series, music videos, and video games. A selective list of each appears below.
Film.
"See "
Utah's Monument Valley has been location to several productions, such as "127 Hours", Tim Burton's "Planet of the Apes", and "Butch Cassidy and the Sundance Kid".
References.
 "This article incorporates public domain material from the website of the ".

</doc>
<doc id="31717" url="http://en.wikipedia.org/wiki?curid=31717" title="United Kingdom">
United Kingdom

The United Kingdom of Great Britain and Northern Ireland, commonly known as the United Kingdom (UK) or Britain, is a sovereign state in Europe. Lying off the north-western coast of the European mainland, the country includes the island of Great Britain—a term also applied loosely to refer to the whole country—the north-eastern part of the island of Ireland and many smaller islands. Northern Ireland is the only part of the UK that shares a land border with another state (the Republic of Ireland). Apart from this land border, the UK is surrounded by the Atlantic Ocean to its west and north, the North Sea to its east and the English Channel to its south. The Irish Sea lies between Great Britain and Ireland. The UK has an area of 93800 sqmi, making it the 80th-largest sovereign state in the world and the 11th-largest in Europe.
The United Kingdom is the 22nd-most populous country, with an estimated 64.5 million inhabitants. It is a constitutional monarchy with a parliamentary system of governance. Its capital city is London, an important global city and financial centre with an urban population of 10,310,000, the fourth-largest in Europe and second-largest in the European Union. The current monarch—since 6 February 1952—is Queen Elizabeth II. The UK consists of four countries: England, Scotland, Wales, and Northern Ireland. The latter three have devolved administrations, each with varying powers, based in their capitals, Edinburgh, Cardiff, and Belfast, respectively. Guernsey, Jersey, and the Isle of Man are not part of the United Kingdom, being Crown dependencies with the British Government responsible for defence and international representation.
The relationships among the countries of the United Kingdom have changed over time. Wales was annexed by the Kingdom of England under the Acts of Union of 1536 and 1543. A treaty between England and Scotland resulted in 1707 in a unified Kingdom of Great Britain, which merged in 1801 with the Kingdom of Ireland to form the United Kingdom of Great Britain and Ireland. In 1922, five-sixths of Ireland seceded from the country, leaving the present formulation of the United Kingdom of Great Britain and Northern Ireland. The UK has fourteen Overseas Territories. These are the remnants of the British Empire which, at its height in the 1920s, encompassed almost a quarter of the world's land mass and was the largest empire in history. British influence can be observed in the language, culture, and legal systems of many of its former colonies.
The United Kingdom is a developed country and has the world's fifth-largest economy by nominal GDP and tenth-largest economy by purchasing power parity. The UK is considered to have a high-income economy and is categorised as very high in the Human Development Index, currently ranking 14th in the world. It was the world's first industrialised country and the world's foremost power during the 19th and early 20th centuries. The UK remains a great power with considerable economic, cultural, military, scientific, and political influence internationally. It is a recognised nuclear weapons state and its military expenditure ranks fifth or sixth in the world. The UK has been a permanent member of the United Nations Security Council since its first session in 1946. It has been a member state of the European Union (EU) and its predecessor, the European Economic Community (EEC), since 1973; it is also a member of the Commonwealth of Nations, the Council of Europe, the G7, the G8, the G20, NATO, the Organisation for Economic Co-operation and Development (OECD), and the World Trade Organization (WTO).
Etymology and terminology.
The 1707 Acts of Union declared that the kingdoms of England and Scotland were "United into One Kingdom by the Name of Great Britain", though the new state is also referred to in the Acts as the "Kingdom of Great Britain", "United Kingdom of Great Britain" and "United Kingdom". However, the term "united kingdom" is only found in informal use during the 18th century and the country was only occasionally referred to as the "United Kingdom of Great Britain" — its full official name, from 1707 to 1800, being merely "Great Britain", without a "long form". The Acts of Union 1800 united the Kingdom of Great Britain and the Kingdom of Ireland in 1801, forming the United Kingdom of Great Britain and Ireland. The name "United Kingdom of Great Britain and Northern Ireland" was adopted following the independence of the Irish Free State, and the partition of Ireland, in 1922, which left Northern Ireland as the only part of the island of Ireland within the UK.
Although the United Kingdom, as a sovereign state, is a country, England, Scotland, Wales, and to a lesser degree, Northern Ireland, are also regarded as "countries", though they are not sovereign states. Scotland, Wales and Northern Ireland have devolved self-government. The British Prime Minister's website has used the phrase "countries within a country" to describe the United Kingdom. Some statistical summaries, such as those for the twelve NUTS 1 regions of the UK, also refer to Scotland, Wales and Northern Ireland as "regions". Northern Ireland is also referred to as a "province". With regard to Northern Ireland, the descriptive name used "can be controversial, with the choice often revealing one's political preferences."
The term "Britain" is often used as synonym for the United Kingdom. The term "Great Britain", by contrast, refers conventionally to the island of Great Britain, or politically to England, Scotland and Wales in combination. However, it is sometimes used as a loose synonym for the United Kingdom as a whole. "GB" and "GBR" are the standard country codes for the United Kingdom (see and ISO 3166-1 alpha-3) and are consequently used by international organisations to refer to the United Kingdom. Additionally, the United Kingdom's Olympic team competes under the name "Great Britain" or "Team GB".
The adjective "British" is commonly used to refer to matters relating to the United Kingdom. The term has no definite legal connotation, but is used in law to refer to UK citizenship and matters to do with nationality. People of the United Kingdom use a number of different terms to describe their national identity and may identify themselves as being British; or as being English, Scottish, Welsh, Northern Irish, or Irish; or as being both.
In 2006, a new design of British passport was introduced. Its first page shows the long form name of the state in English, Welsh and Scottish Gaelic. In Welsh, the long form name of the state is "Teyrnas Unedig Prydain Fawr a Gogledd Iwerddon" with "Teyrnas Unedig" being used as a short form name on government websites. (However it is usually abbreviated to "DU" for the mutated form "Y Deyrnas Unedig".) In Scottish Gaelic, the long form is "Rìoghachd Aonaichte Bhreatainn is Èireann a Tuath" and the short form "Rìoghachd Aonaichte".
History.
Before 1707.
Settlement by anatomically modern humans of what was to become the United Kingdom occurred in waves beginning by about 30,000 years ago. By the end of the region's prehistoric period, the population is thought to have belonged, in the main, to a culture termed Insular Celtic, comprising Brythonic Britain and Gaelic Ireland. The Roman conquest, beginning in 43 AD, and the 400-year rule of southern Britain, was followed by an invasion by Germanic Anglo-Saxon settlers, reducing the Brythonic area mainly to what was to become Wales and the historic Kingdom of Strathclyde. Most of the region settled by the Anglo-Saxons became unified as the Kingdom of England in the 10th century. Meanwhile, Gaelic-speakers in north west Britain (with connections to the north-east of Ireland and traditionally supposed to have migrated from there in the 5th century) united with the Picts to create the Kingdom of Scotland in the 9th century.
In 1066, the Normans invaded England from France and after its conquest, seized large parts of Wales, conquered much of Ireland and were invited to settle in Scotland, bringing to each country feudalism on the Northern French model and Norman-French culture. The Norman elites greatly influenced, but eventually assimilated with, each of the local cultures. Subsequent medieval English kings completed the conquest of Wales and made an unsuccessful attempt to annex Scotland. Following the Declaration of Arbroath, Scotland maintained its independence, albeit in near-constant conflict with England. The English monarchs, through inheritance of substantial territories in France and claims to the French crown, were also heavily involved in conflicts in France, most notably the Hundred Years War, while the Kings of Scots were in an alliance with the French during this period.
The early modern period saw religious conflict resulting from the Reformation and the introduction of Protestant state churches in each country. Wales was fully incorporated into the Kingdom of England, and Ireland was constituted as a kingdom in personal union with the English crown. In what was to become Northern Ireland, the lands of the independent Catholic Gaelic nobility were confiscated and given to Protestant settlers from England and Scotland.
In 1603, the kingdoms of England, Scotland and Ireland were united in a personal union when James VI, King of Scots, inherited the crowns of England and Ireland and moved his court from Edinburgh to London; each country nevertheless remained a separate political entity and retained its separate political, legal, and religious institutions.
In the mid-17th century, all three kingdoms were involved in a series of connected wars (including the English Civil War) which led to the temporary overthrow of the monarchy and the establishment of the short-lived unitary republic of the Commonwealth of England, Scotland and Ireland.
Although the monarchy was restored, it ensured (with the Glorious Revolution of 1688 and the subsequent Bill of Rights 1689, and the Claim of Right Act 1689) that, unlike much of the rest of Europe, royal absolutism would not prevail, and a professed Catholic could never accede to the throne. The British constitution would develop on the basis of constitutional monarchy and the parliamentary system. With the constitutional rights of Parliament legally established, no monarch has since entered the House of Commons when it is sitting meeting, which is annually commemorated at the State Opening of Parliament by the British monarch when the doors of the House of Commons are slammed in the face of the monarch's messenger, symbolising the rights of Parliament and its independence from the monarch. With the founding of the Royal Society in 1660, science was greatly encouraged. During this period, particularly in England, the development of naval power (and the interest in voyages of discovery) led to the acquisition and settlement of overseas colonies, particularly in North America.
Since the Acts of Union of 1707.
On 1 May 1707, the united Kingdom of Great Britain came into being, the result of Acts of Union being passed by the parliaments of England and Scotland to ratify the 1706 Treaty of Union and so unite the two kingdoms.
In the 18th century, cabinet government developed under Robert Walpole, in practice the first prime minister (1721–1742). A series of Jacobite Uprisings sought to remove the Protestant House of Hanover from the British throne and restore the Catholic House of Stuart. The Jacobites were finally defeated at the Battle of Culloden in 1746, after which the Scottish Highlanders were brutally suppressed. The British colonies in North America that broke away from Britain in the American War of Independence became the United States of America in 1783. British imperial ambition turned elsewhere, particularly to India.
During the 18th century, Britain was involved in the Atlantic slave trade. British ships transported an estimated 2 million slaves from Africa to the West Indies before banning the trade in 1807 and taking a leading role in the movement to abolish slavery worldwide by pressing other nations to end their trade with a series of treaties, and then formed the world's oldest international human rights organisation, Anti-Slavery International, in London in 1839. The term 'United Kingdom' became official in 1801 when the parliaments of Britain and Ireland each passed an Act of Union, uniting the two kingdoms and creating the United Kingdom of Great Britain and Ireland.
In the early 19th century, the British-led Industrial Revolution began to transform the country. It slowly led to a shift in political power away from the old Tory and Whig landowning classes towards the new industrialists. An alliance of merchants and industrialists with the Whigs would lead to a new party, the Liberals, with an ideology of free trade and "laissez-faire". In 1832 Parliament passed the Great Reform Act, which began the transfer of political power from the aristocracy to the middle classes. In the countryside, enclosure of the land was driving small farmers out. Towns and cities began to swell with a new urban working class. Few ordinary workers had the vote, and they created their own organisations in the form of trade unions.
After the defeat of France in the Revolutionary and Napoleonic Wars (1792–1815), the UK emerged as the principal naval and imperial power of the 19th century (with London the largest city in the world from about 1830). Unchallenged at sea, British dominance was later described as "Pax Britannica". By the time of the Great Exhibition of 1851, Britain was described as the "workshop of the world". The British Empire was expanded to include India, large parts of Africa and many other territories throughout the world. Alongside the formal control it exerted over its own colonies, British dominance of much of world trade meant that it effectively controlled the economies of many countries, such as China, Argentina and Siam. Domestically, political attitudes favoured free trade and laissez-faire policies and a gradual widening of the voting franchise. During the century, the population increased at a dramatic rate, accompanied by rapid urbanisation, causing significant social and economic stresses. After 1875, the UK's industrial monopoly was challenged by Germany and the USA. To seek new markets and sources of raw materials, the Conservative Party under Disraeli launched a period of imperialist expansion in Egypt, South Africa and elsewhere. Canada, Australia and New Zealand became self-governing dominions.
Social reform and home rule for Ireland were important domestic issues after 1900. The Labour Party emerged from an alliance of trade unions and small Socialist groups in 1900, and suffragettes campaigned for women's right to vote before 1914.
The UK fought with France, Russia and (after 1917) the US, against Germany and its allies in World War I (1914–18). The UK armed forces were engaged across much of the British Empire and in several regions of Europe, particularly on the Western front. The high fatalities of trench warfare caused the loss of much of a generation of men, with lasting social effects in the nation and a great disruption in the social order.
After the war, the UK received the League of Nations mandate over a number of former German and Ottoman colonies. The British Empire reached its greatest extent, covering a fifth of the world's land surface and a quarter of its population. However, the UK had suffered 2.5 million casualties and finished the war with a huge national debt. The rise of Irish Nationalism and disputes within Ireland over the terms of Irish Home Rule led eventually to the partition of the island in 1921, and the Irish Free State became independent with Dominion status in 1922. Northern Ireland remained part of the United Kingdom. A wave of strikes in the mid-1920s culminated in the UK General Strike of 1926. The UK had still not recovered from the effects of the war when the Great Depression (1929–32) occurred. This led to considerable unemployment and hardship in the old industrial areas, as well as political and social unrest in the 1930s. A coalition government was formed in 1931.
The UK entered World War II by declaring war on Germany in 1939, after it had invaded Poland and Czechoslovakia. In 1940, Winston Churchill became prime minister and head of a coalition government. Despite the defeat of its European allies in the first year of the war, the UK continued the fight alone against Germany. In 1940, the RAF defeated the German Luftwaffe in a struggle for control of the skies in the Battle of Britain. The UK suffered heavy bombing during the Blitz. There were also eventual hard-fought victories in the Battle of the Atlantic, the North Africa campaign and Burma campaign. UK forces played an important role in the Normandy landings of 1944, achieved with its ally the US. After Germany's defeat, the UK was one of the Big Three powers who met to plan the post-war world; it was an original signatory to the Declaration of the United Nations. The UK became one of the five permanent members of the United Nations Security Council. However, the war left the UK severely weakened and depending financially on Marshall Aid and loans from the United States.
In the immediate post-war years, the Labour government initiated a radical programme of reforms, which had a significant effect on British society in the following decades. Major industries and public utilities were nationalised, a Welfare State was established, and a comprehensive, publicly funded healthcare system, the National Health Service, was created. The rise of nationalism in the colonies coincided with Britain's now much-diminished economic position, so that a policy of decolonisation was unavoidable. Independence was granted to India and Pakistan in 1947. Over the next three decades, most colonies of the British Empire gained their independence. Many became members of the Commonwealth of Nations.
Although the UK was the third country to develop a nuclear weapons arsenal (with its first atomic bomb test in 1952), the new post-war limits of Britain's international role were illustrated by the Suez Crisis of 1956. The international spread of the English language ensured the continuing international influence of its literature and culture. From the 1960s onward, its popular culture was also influential abroad. As a result of a shortage of workers in the 1950s, the UK government encouraged immigration from Commonwealth countries. In the following decades, the UK became a multi-ethnic society. Despite rising living standards in the late 1950s and 1960s, the UK's economic performance was not as successful as many of its competitors, such as West Germany and Japan. In 1973, the UK joined the European Economic Community (EEC), and when the EEC became the European Union (EU) in 1992, it was one of the 12 founding members.
From the late 1960s, Northern Ireland suffered communal and paramilitary violence (sometimes affecting other parts of the UK) conventionally known as the Troubles. It is usually considered to have ended with the Belfast "Good Friday" Agreement of 1998.
Following a period of widespread economic slowdown and industrial strife in the 1970s, the Conservative Government of the 1980s initiated a radical policy of monetarism, deregulation, particularly of the financial sector (for example, Big Bang in 1986) and labour markets, the sale of state-owned companies (privatisation), and the withdrawal of subsidies to others. This resulted in high unemployment and social unrest, but ultimately also economic growth, particularly in the services sector. From 1984, the economy was helped by the inflow of substantial North Sea oil revenues.
Around the end of the 20th century there were major changes to the governance of the UK with the establishment of devolved administrations for Scotland, Wales and Northern Ireland. The statutory incorporation followed acceptance of the European Convention on Human Rights. The UK is still a key global player diplomatically and militarily. It plays leading roles in the EU, UN and NATO. However, controversy surrounds some of Britain's overseas military deployments, particularly in Afghanistan and Iraq.
The 2008 global financial crisis severely affected the UK economy. The coalition government of 2010 introduced austerity measures intended to tackle the substantial public deficits which resulted. In 2014 the Scottish Government held a referendum on Scottish independence, with 55% of voters rejecting the independence proposal and opting to remain within the United Kingdom.
Geography.
The total area of the United Kingdom is approximately 243610 km2. The country occupies the major part of the British Isles archipelago and includes the island of Great Britain, the northeastern one-sixth of the island of Ireland and some smaller surrounding islands. It lies between the North Atlantic Ocean and the North Sea with the south-east coast coming within 35 km of the coast of northern France, from which it is separated by the English Channel. In 1993 10% of the UK was forested, 46% used for pastures and 25% cultivated for agriculture. The Royal Greenwich Observatory in London is the defining point of the Prime Meridian.
The United Kingdom lies between latitudes 49° to 61° N, and longitudes 9° W to 2° E. Northern Ireland shares a 360 km land boundary with the Republic of Ireland. The coastline of Great Britain is 17820 km long. It is connected to continental Europe by the Channel Tunnel, which at 50 km (38 km underwater) is the longest underwater tunnel in the world.
England accounts for just over half of the total area of the UK, covering 130395 km2. Most of the country consists of lowland terrain, with mountainous terrain north-west of the Tees-Exe line; including the Cumbrian Mountains of the Lake District, the Pennines and limestone hills of the Peak District, Exmoor and Dartmoor. The main rivers and estuaries are the Thames, Severn and the Humber. England's highest mountain is Scafell Pike (978 m) in the Lake District. Its principal rivers are the Severn, Thames, Humber, Tees, Tyne, Tweed, Avon, Exe and Mersey.
Scotland accounts for just under a third of the total area of the UK, covering 78772 km2 and including nearly eight hundred islands, predominantly west and north of the mainland; notably the Hebrides, Orkney Islands and Shetland Islands. The topography of Scotland is distinguished by the Highland Boundary Fault – a geological rock fracture – which traverses Scotland from Arran in the west to Stonehaven in the east. The faultline separates two distinctively different regions; namely the Highlands to the north and west and the lowlands to the south and east. The more rugged Highland region contains the majority of Scotland's mountainous land, including Ben Nevis which at 1343 m is the highest point in the British Isles. Lowland areas – especially the narrow waist of land between the Firth of Clyde and the Firth of Forth known as the Central Belt – are flatter and home to most of the population including Glasgow, Scotland's largest city, and Edinburgh, its capital and political centre.
Wales accounts for less than a tenth of the total area of the UK, covering 20779 km2. Wales is mostly mountainous, though South Wales is less mountainous than North and mid Wales. The main population and industrial areas are in South Wales, consisting of the coastal cities of Cardiff, Swansea and Newport, and the South Wales Valleys to their north. The highest mountains in Wales are in Snowdonia and include Snowdon (Welsh: "Yr Wyddfa") which, at 1085 m, is the highest peak in Wales. The 14, or possibly 15, Welsh mountains over 3,000 feet (914 m) high are known collectively as the Welsh 3000s. Wales has over 1680 mi of coastline. Several islands lie off the Welsh mainland, the largest of which is Anglesey ("Ynys Môn") in the northwest.
Northern Ireland, separated from Great Britain by the Irish Sea and North Channel, has an area of 14160 km2 and is mostly hilly. It includes Lough Neagh which, at 388 km2, is the largest lake in the British Isles by area. The highest peak in Northern Ireland is Slieve Donard in the Mourne Mountains at 852 m.
Climate.
The United Kingdom has a temperate climate, with plentiful rainfall all year round. The temperature varies with the seasons seldom dropping below -11 C or rising above 35 C. The prevailing wind is from the south-west and bears frequent spells of mild and wet weather from the Atlantic Ocean, although the eastern parts are mostly sheltered from this wind since the majority of the rain falls over the western regions the eastern parts are therefore the driest. Atlantic currents, warmed by the Gulf Stream, bring mild winters; especially in the west where winters are wet and even more so over high ground. Summers are warmest in the south-east of England, being closest to the European mainland, and coolest in the north. Heavy snowfall can occur in winter and early spring on high ground, and occasionally settles to great depth away from the hills.
Administrative divisions.
Atlantic Ocean 
Channel 
North Sea 
Celtic Sea 
Netherlands 
France 
Belgium 
Norway 
Ireland 
Aberdeen 
Glasgow 
Edinburgh 
Newcastle 
Leeds 
Manchester 
Birmingham 
London 
Belfast 
Cardiff 
England 
Scotland 
Wales 
Northern Ireland 
 The four countries of the United Kingdom.
Each country of the United Kingdom has its own system of administrative and geographic demarcation, whose origins often pre-date the formation of the United Kingdom. Thus there is "no common stratum of administrative unit encompassing the United Kingdom". Until the 19th century there was little change to those arrangements, but there has since been a constant evolution of role and function. Change did not occur in a uniform manner and the devolution of power over local government to Scotland, Wales and Northern Ireland means that future changes are also unlikely to be uniform.
The organisation of local government in England is complex, with the distribution of functions varying according to local arrangements. Legislation concerning local government in England is the responsibility of the UK parliament and the Government of the United Kingdom, as England has no devolved parliament. The upper-tier subdivisions of England are the nine Government office regions or European Union government office regions. One region, Greater London, has had a directly elected assembly and mayor since 2000 following popular support for the proposal in a referendum. It was intended that other regions would also be given their own elected regional assemblies, but a proposed assembly in the North East region was rejected by a referendum in 2004. Below the regional tier, some parts of England have county councils and district councils and others have unitary authorities; while London consists of 32 London boroughs and the City of London. Councillors are elected by the first-past-the-post system in single-member wards or by the multi-member plurality system in multi-member wards.
For local government purposes, Scotland is divided into 32 council areas, with wide variation in both size and population. The cities of Glasgow, Edinburgh, Aberdeen and Dundee are separate council areas, as is the Highland Council which includes a third of Scotland's area but only just over 200,000 people. Local councils are made up of elected councillors, of whom there are currently 1,223; they are paid a part-time salary. Elections are conducted by single transferable vote in multi-member wards that elect either three or four councillors. Each council elects a Provost, or Convenor, to chair meetings of the council and to act as a figurehead for the area. Councillors are subject to a code of conduct enforced by the Standards Commission for Scotland. The representative association of Scotland's local authorities is the Convention of Scottish Local Authorities (COSLA).
Local government in Wales consists of 22 unitary authorities. These include the cities of Cardiff, Swansea and Newport which are unitary authorities in their own right. Elections are held every four years under the first-past-the-post system. The most recent elections were held in May 2012, except for the Isle of Anglesey. The Welsh Local Government Association represents the interests of local authorities in Wales.
Local government in Northern Ireland has since 1973 been organised into 26 district councils, each elected by single transferable vote. Their powers are limited to services such as collecting waste, controlling dogs and maintaining parks and cemeteries. On 13 March 2008 the executive agreed on proposals to create 11 new councils and replace the present system. The next local elections were postponed until 2016 to facilitate this.
Dependencies.
The United Kingdom has sovereignty over seventeen territories which do not form part of the United Kingdom itself: fourteen British Overseas Territories and three Crown dependencies.
The fourteen British Overseas Territories are: Anguilla; Bermuda; the British Antarctic Territory; the British Indian Ocean Territory; the British Virgin Islands; the Cayman Islands; the Falkland Islands; Gibraltar; Montserrat; Saint Helena, Ascension and Tristan da Cunha; the Turks and Caicos Islands; the Pitcairn Islands; South Georgia and the South Sandwich Islands; and Sovereign Base Areas on Cyprus. British claims in Antarctica are not universally recognised. Collectively Britain's overseas territories encompass an approximate land area of 667018 sqmi and a population of approximately 260,000 people. They are the remnants of the British Empire and several have specifically voted to remain British territories (Bermuda in 1995, Gibraltar in 2002 and the Falkland Islands in 2013).
The Crown dependencies are possessions of the Crown, as opposed to overseas territories of the UK. They comprise three independently administered jurisdictions: the Channel Islands of Jersey and Guernsey in the English Channel, and the Isle of Man in the Irish Sea. By mutual agreement, the British Government manages the islands' foreign affairs and defence and the UK Parliament has the authority to legislate on their behalf. However, internationally, they are regarded as "territories for which the United Kingdom is responsible". The power to pass legislation affecting the islands ultimately rests with their own respective legislative assemblies, with the assent of the Crown (Privy Council or, in the case of the Isle of Man, in certain circumstances the Lieutenant-Governor). Since 2005 each Crown dependency has had a Chief Minister as its head of government.
Politics.
The United Kingdom is a unitary state under a constitutional monarchy. Queen Elizabeth II is the head of state of the UK as well as monarch of fifteen other independent Commonwealth countries. The monarch has "the right to be consulted, the right to encourage, and the right to warn". The Constitution of the United Kingdom is uncodified and consists mostly of a collection of disparate written sources, including statutes, judge-made case law and international treaties, together with constitutional conventions. As there is no technical difference between ordinary statutes and "constitutional law", the UK Parliament can perform "constitutional reform" simply by passing Acts of Parliament, and thus has the political power to change or abolish almost any written or unwritten element of the constitution. However, no Parliament can pass laws that future Parliaments cannot change.
Government.
The UK has a parliamentary government based on the Westminster system that has been emulated around the world: a legacy of the British Empire. The parliament of the United Kingdom meets in the Palace of Westminster and has two houses: an elected House of Commons and an appointed House of Lords. All bills passed are given Royal Assent before becoming law.
The position of prime minister, the UK's head of government, belongs to the person most likely to command the confidence of the House of Commons; this individual is typically the leader of the political party or coalition of parties that holds the largest number of seats in that chamber. The prime minister chooses a cabinet and its members are formally appointed by the monarch to form Her Majesty's Government. By convention, the Queen respects the prime minister's decisions of government.
The cabinet is traditionally drawn from members of the prime minister's party or coalition and mostly from the House of Commons but always from both legislative houses, the cabinet being responsible to both. Executive power is exercised by the prime minister and cabinet, all of whom are sworn into the Privy Council of the United Kingdom, and become Ministers of the Crown. The current Prime Minister is David Cameron, who has been in office since 11 May 2010. Cameron is the leader of the Conservative Party and heads a coalition with the Liberal Democrats. For elections to the House of Commons, the UK is currently divided into 650 constituencies, each electing a single member of parliament (MP) by simple plurality. General elections are called by the monarch when the prime minister so advises. The Parliament Acts 1911 and 1949 require that a new election must be called no later than five years after the previous general election.
The UK's three major political parties are currently the Conservative Party (Tories), the Labour Party and the Liberal Democrats, representing the British traditions of conservatism, socialism and social liberalism, respectively. At the 2010 general election these three parties together won 622 out of 650 seats in the House of Commons. Most of the remaining seats were won by parties that contest elections only in one part of the UK: the Scottish National Party (Scotland only); Plaid Cymru (Wales only); and the Alliance Party, Democratic Unionist Party, Social Democratic and Labour Party and Sinn Féin (Northern Ireland only). In accordance with party policy, no elected Sinn Féin members of parliament have ever attended the House of Commons to speak on behalf of their constituents because of the requirement to take an oath of allegiance to the monarch.
Devolved administrations.
Scotland, Wales and Northern Ireland each have their own government or executive, led by a First Minister (or, in the case of Northern Ireland, a diarchal First Minister and deputy First Minister), and a devolved unicameral legislature. England, the largest country of the United Kingdom, has no such devolved executive or legislature and is administered and legislated for directly by the UK government and parliament on all issues. This situation has given rise to the so-called West Lothian question which concerns the fact that members of parliament from Scotland, Wales and Northern Ireland can vote, sometimes decisively, on matters that only affect England. The McKay Commission reported on this matter in March 2013 recommending that laws affecting only England should need support from a majority of English members of parliament.
The Scottish Government and Parliament have wide-ranging powers over any matter that has not been specifically reserved to the UK parliament, including education, healthcare, Scots law and local government. At the 2011 elections the Scottish National Party won re-election and achieved an overall majority in the Scottish parliament, with its leader, Alex Salmond, as First Minister of Scotland. In 2012, the UK and Scottish governments signed the Edinburgh Agreement setting out the terms for a referendum on Scottish independence in 2014, which was defeated 55% to 45%.
The Welsh Government and the National Assembly for Wales have more limited powers than those devolved to Scotland. The Assembly is able to legislate on devolved matters through Acts of the Assembly, which require no prior consent from Westminster. The 2011 elections resulted in a minority Labour administration led by Carwyn Jones.
The Northern Ireland Executive and Assembly have powers similar to those devolved to Scotland. The Executive is led by a diarchy representing unionist and nationalist members of the Assembly. Currently, Peter Robinson (Democratic Unionist Party) and Martin McGuinness (Sinn Féin) are First Minister and deputy First Minister respectively. Devolution to Northern Ireland is contingent on participation by the Northern Ireland administration in the North-South Ministerial Council, where the Northern Ireland Executive cooperates and develops joint and shared policies with the Government of Ireland. The British and Irish governments co-operate on non-devolved matters affecting Northern Ireland through the British–Irish Intergovernmental Conference, which assumes the responsibilities of the Northern Ireland administration in the event of its non-operation.
The UK does not have a codified constitution and constitutional matters are not among the powers devolved to Scotland, Wales or Northern Ireland. Under the doctrine of parliamentary sovereignty, the UK Parliament could, in theory, therefore, abolish the Scottish Parliament, Welsh Assembly or Northern Ireland Assembly. Indeed, in 1972, the UK Parliament unilaterally prorogued the Parliament of Northern Ireland, setting a precedent relevant to contemporary devolved institutions. In practice, it would be politically difficult for the UK Parliament to abolish devolution to the Scottish Parliament and the Welsh Assembly, given the political entrenchment created by referendum decisions. The political constraints placed upon the UK Parliament's power to interfere with devolution in Northern Ireland are even greater than in relation to Scotland and Wales, given that devolution in Northern Ireland rests upon an international agreement with the Government of Ireland.
Law and criminal justice.
The United Kingdom does not have a single legal system, as Article 19 of the 1706 Treaty of Union provided for the continuation of Scotland's separate legal system. Today the UK has three distinct systems of law: English law, Northern Ireland law and Scots law. A new Supreme Court of the United Kingdom came into being in October 2009 to replace the Appellate Committee of the House of Lords. The Judicial Committee of the Privy Council, including the same members as the Supreme Court, is the highest court of appeal for several independent Commonwealth countries, the British Overseas Territories and the Crown Dependencies.
Both English law, which applies in England and Wales, and Northern Ireland law are based on common-law principles. The essence of common law is that, subject to statute, the law is developed by judges in courts, applying statute, precedent and common sense to the facts before them to give explanatory judgements of the relevant legal principles, which are reported and binding in future similar cases ("stare decisis"). The courts of England and Wales are headed by the Senior Courts of England and Wales, consisting of the Court of Appeal, the High Court of Justice (for civil cases) and the Crown Court (for criminal cases). The Supreme Court is the highest court in the land for both criminal and civil appeal cases in England, Wales and Northern Ireland and any decision it makes is binding on every other court in the same jurisdiction, often having a persuasive effect in other jurisdictions.
Scots law is a hybrid system based on both common-law and civil-law principles. The chief courts are the Court of Session, for civil cases, and the High Court of Justiciary, for criminal cases. The Supreme Court of the United Kingdom serves as the highest court of appeal for civil cases under Scots law. Sheriff courts deal with most civil and criminal cases including conducting criminal trials with a jury, known as sheriff solemn court, or with a sheriff and no jury, known as sheriff summary Court. The Scots legal system is unique in having three possible verdicts for a criminal trial: "guilty", "not guilty" and "not proven". Both "not guilty" and "not proven" result in an acquittal.
Crime in England and Wales increased in the period between 1981 and 1995, though since that peak there has been an overall fall of 48% in recorded crime from 1995 to 2007/08, according to crime statistics. The prison population of England and Wales has almost doubled over the same period, to over 80,000, giving England and Wales the highest rate of incarceration in Western Europe at 147 per 100,000. Her Majesty's Prison Service, which reports to the Ministry of Justice, manages most of the prisons within England and Wales. Crime in Scotland fell to its lowest recorded level for 32 years in 2009/10, falling by ten per cent. At the same time Scotland's prison population, at over 8,000, is at record levels and well above design capacity. The Scottish Prison Service, which reports to the Cabinet Secretary for Justice, manages Scotland's prisons.
Foreign relations.
The UK is a permanent member of the United Nations Security Council, a member of NATO, the Commonwealth of Nations, G7, G8, G20, the OECD, the WTO, the Council of Europe, the OSCE, and is a member state of the European Union. The UK is said to have a "Special Relationship" with the United States and a close partnership with France—the "Entente cordiale"—and shares nuclear weapons technology with both countries. The UK is also closely linked with the Republic of Ireland; the two countries share a Common Travel Area and co-operate through the British-Irish Intergovernmental Conference and the British-Irish Council. Britain's global presence and influence is further amplified through its trading relations, foreign investments, official development assistance and military engagements.
Military.
The armed forces of the United Kingdom—officially, "Her Majesty's Armed Forces"—consist of three professional service branches: the Royal Navy and Royal Marines (forming the Naval Service), the British Army and the Royal Air Force. The forces are managed by the Ministry of Defence and controlled by the Defence Council, chaired by the Secretary of State for Defence. The Commander-in-Chief is the British monarch, Elizabeth II, to whom members of the forces swear an oath of allegiance. The Armed Forces are charged with protecting the UK and its overseas territories, promoting the UK's global security interests and supporting international peacekeeping efforts. They are active and regular participants in NATO, including the Allied Rapid Reaction Corps, as well as the Five Power Defence Arrangements, RIMPAC and other worldwide coalition operations. Overseas garrisons and facilities are maintained in Ascension Island, Belize, Brunei, Canada, Cyprus, Diego Garcia, the Falkland Islands, Germany, Gibraltar, Kenya, Qatar and Singapore.
The British armed forces played a key role in establishing the British Empire as the dominant world power in the 18th, 19th and early 20th centuries. Throughout its unique history the British forces have seen action in a number of major wars, such as the Seven Years' War, the Napoleonic Wars, the Crimean War, World War I and World War II—as well as many colonial conflicts. By emerging victorious from such conflicts, Britain has often been able to decisively influence world events. Since the end of the British Empire, the UK has nonetheless remained a major military power. Following the end of the Cold War, defence policy has a stated assumption that "the most demanding operations" will be undertaken as part of a coalition. Setting aside the intervention in Sierra Leone, recent UK military operations in Bosnia, Kosovo, Afghanistan, Iraq and, most recently, Libya, have followed this approach. The last time the British military fought alone was the Falklands War of 1982.
According to various sources, including the Stockholm International Peace Research Institute and the International Institute for Strategic Studies, the United Kingdom has the fifth- or sixth-highest military expenditure in the world. Total defence spending currently accounts for around 2.4% of total national GDP.
Economy.
The UK has a partially regulated market economy. Based on market exchange rates the UK is today the sixth-largest economy in the world and the third-largest in Europe after Germany and France, having fallen behind France for the first time in over a decade in 2008. HM Treasury, led by the Chancellor of the Exchequer, is responsible for developing and executing the British government's public finance policy and economic policy. The Bank of England is the UK's central bank and is responsible for issuing notes and coins in the nation's currency, the pound sterling. Banks in Scotland and Northern Ireland retain the right to issue their own notes, subject to retaining enough Bank of England notes in reserve to cover their issue. Pound sterling is the world's third-largest reserve currency (after the US Dollar and the Euro). Since 1997 the Bank of England's Monetary Policy Committee, headed by the Governor of the Bank of England, has been responsible for setting interest rates at the level necessary to achieve the overall inflation target for the economy that is set by the Chancellor each year.
The UK service sector makes up around 73% of GDP. London is one of the three "command centres" of the global economy (alongside New York City and Tokyo), it is the world's largest financial centre alongside New York, and it has the largest city GDP in Europe. Edinburgh is also one of the largest financial centres in Europe. Tourism is very important to the British economy and, with over 27 million tourists arriving in 2004, the United Kingdom is ranked as the sixth major tourist destination in the world and London has the most international visitors of any city in the world. The creative industries accounted for 7% GVA in 2005 and grew at an average of 6% per annum between 1997 and 2005.
The Industrial Revolution started in the UK with an initial concentration on the textile industry, followed by other heavy industries such as shipbuilding, coal mining and steelmaking. British merchants, shippers and bankers developed overwhelming advantage over those of other nations allowing the UK to dominate international trade in the 19th century. As other nations industrialised, coupled with economic decline after two world wars, the United Kingdom began to lose its competitive advantage and heavy industry declined, by degrees, throughout the 20th century. Manufacturing remains a significant part of the economy but accounted for only 16.7% of national output in 2003.
The automotive industry is a significant part of the UK manufacturing sector and employs over 800,000 people, with a turnover of some £52 billion, generating £26.6 billion of exports.
The aerospace industry of the UK is the second- or third-largest national aerospace industry in the world depending upon the method of measurement and has an annual turnover of around £20 billion. The wings for the Airbus A380 and the A350 XWB are designed and manufactured at Airbus UK's world-leading Broughton facility, whilst over a quarter of the value of the Boeing 787 comes from UK manufacturers including Eaton (fuel subsystem pumps), Messier-Bugatti-Dowty (the landing gear) and Rolls-Royce (the engines). Other key names include GKN Aerospace – an expert in metallic and composite aerostructures that's involved in almost every civil and military fixed and rotary wing aircraft in production and development today.
BAE Systems plays a critical role in some of the world's biggest defence aerospace projects. The company makes large sections of the Typhoon Eurofighter at its sub-assembly plant in Salmesbury and assembles the aircraft for the RAF at its Warton Plant, near Preston. It is also a principal subcontractor on the F35 Joint Strike Fighter—the world's largest single defence project—for which it designs and manufactures a range of components including the aft fuselage, vertical and horizontal tail and wing tips and fuel system. As well as this it manufactures the Hawk, the world's most successful jet training aircraft. Airbus UK also manufactures the wings for the A400 m military transporter. Rolls-Royce, is the world's second-largest aero-engine manufacturer. Its engines power more than 30 types of commercial aircraft and it has more than 30,000 engines currently in service across both the civil and defence sectors. Rolls-Royce is forecast to have more than 50% of the widebody market share by 2016, ahead of General Electric. Agusta Westland designs and manufactures complete helicopters in the UK.
The UK space industry is growing very fast. Worth £9.1bn in 2011 and employing 29,000 people, it is growing at a rate of some 7.5% annually, according to its umbrella organisation, the UK Space Agency. Government strategy is for the space industry to be a £40bn business for the UK by 2030, capturing a 10% share of the $250bn world market for commercial space technology. On 16 July 2013, the British government pledged £60 m to the Skylon project: this investment will provide support at a "crucial stage" to allow a full-scale prototype of the SABRE engine to be built.
The pharmaceutical industry plays an important role in the UK economy and the country has the third-highest share of global pharmaceutical R&D expenditures (after the United States and Japan).
Agriculture is intensive, highly mechanised and efficient by European standards, producing about 60% of food needs with less than 1.6% of the labour force (535,000 workers). Around two-thirds of production is devoted to livestock, one-third to arable crops. Farmers are subsidised by the EU's Common Agricultural Policy. The UK retains a significant, though much reduced fishing industry. It is also rich in a number of natural resources including coal, petroleum, natural gas, tin, limestone, iron ore, salt, clay, chalk, gypsum, lead, silica and an abundance of arable land.
In the final quarter of 2008 the UK economy officially entered recession for the first time since 1991. Unemployment increased from 5.2% in May 2008 to 7.6% in May 2009 and by January 2012 the unemployment rate among 18 to 24-year-olds had risen from 11.9% to 22.5%, the highest since current records began in 1992. Total UK government debt rose from 44.4% of GDP in 2007 to 82.9% of GDP in 2011. In February 2013, the UK lost its top AAA credit rating for the first time since 1978.
Inflation-adjusted wages in the UK fell by 3.2% between the third quarter of 2010 and the third quarter of 2012. Since the 1980s, economic inequality has grown faster in the UK than in any other developed country.
The poverty line in the UK is commonly defined as being 60% of the median household income. In 2007–2008 13.5 million people, or 22% of the population, lived below this line. This is a higher level of relative poverty than all but four other EU members. In the same year 4.0 million children, 31% of the total, lived in households below the poverty line after housing costs were taken into account. This is a decrease of 400,000 children since 1998–1999. The UK imports 40% of its food supplies. The Office for National Statistics has estimated that in 2011, 14 million people were at risk of poverty or social exclusion, and that one person in 20 (5.1%) was now experiencing "severe material depression," up from 3 million people in 1977.
Science and technology.
England and Scotland were leading centres of the Scientific Revolution from the 17th century and the United Kingdom led the Industrial Revolution from the 18th century, and has continued to produce scientists and engineers credited with important advances. Major theorists from the 17th and 18th centuries include Isaac Newton, whose laws of motion and illumination of gravity have been seen as a keystone of modern science; from the 19th century Charles Darwin, whose theory of evolution by natural selection was fundamental to the development of modern biology, and James Clerk Maxwell, who formulated classical electromagnetic theory; and more recently Stephen Hawking, who has advanced major theories in the fields of cosmology, quantum gravity and the investigation of black holes. Major scientific discoveries from the 18th century include hydrogen by Henry Cavendish; from the 20th century penicillin by Alexander Fleming, and the structure of DNA, by Francis Crick and others. Major engineering projects and applications by people from the UK in the 18th century include the steam locomotive, developed by Richard Trevithick and Andrew Vivian; from the 19th century the electric motor by Michael Faraday, the incandescent light bulb by Joseph Swan, and the first practical telephone, patented by Alexander Graham Bell; and in the 20th century the world's first working television system by John Logie Baird and others, the jet engine by Frank Whittle, the basis of the modern computer by Alan Turing, and the World Wide Web by Tim Berners-Lee.
Scientific research and development remains important in British universities, with many establishing science parks to facilitate production and co-operation with industry. Between 2004 and 2008 the UK produced 7% of the world's scientific research papers and had an 8% share of scientific citations, the third and second highest in the world (after the United States and China, and the United States, respectively). Scientific journals produced in the UK include "Nature", the "British Medical Journal" and "The Lancet".
Transport.
A radial road network totals 29145 mi of main roads, 2173 mi of motorways and 213750 mi of paved roads. The M25, encircling London, is the largest and busiest bypass in the world. In 2009 there were a total of 34 million licensed vehicles in Great Britain.
The UK has a railway network of 10072 mi in Great Britain and 189 mi in Northern Ireland. Railways in Northern Ireland are operated by NI Railways, a subsidiary of state-owned Translink. In Great Britain, the British Rail network was privatised between 1994 and 1997. Network Rail owns and manages most of the fixed assets (tracks, signals etc.). About 20 privately owned (and foreign state-owned railways including: Deutsche Bahn; SNCF and Nederlandse Spoorwegen) Train Operating Companies (including state-owned East Coast), operate passenger trains and carry over 18,000 passenger trains daily. There are also some 1,000 freight trains in daily operation. The UK government is to spend £30 billion on a new high-speed railway line, HS2, to be operational by 2025. Crossrail, under construction in London, Is Europe's largest construction project with a £15 billion projected cost.
In the year from October 2009 to September 2010 UK airports handled a total of 211.4 million passengers. In that period the three largest airports were London Heathrow Airport (65.6 million passengers), Gatwick Airport (31.5 million passengers) and London Stansted Airport (18.9 million passengers). London Heathrow Airport, located 15 mi west of the capital, has the most international passenger traffic of any airport in the world and is the hub for the UK flag carrier British Airways, as well as for BMI and Virgin Atlantic.
Energy.
In 2006, the UK was the world's ninth-largest consumer of energy and the 15th-largest producer. The UK is home to a number of large energy companies, including two of the six oil and gas "supermajors" – BP and Royal Dutch Shell – and BG Group. In 2011, 40% of the UK's electricity was produced by gas, 30% by coal, 19% by nuclear power and 4.2% by wind, hydro, biofuels and wastes.
In 2009, the UK produced 1.5 million barrels per day (bbl/d) of oil and consumed 1.7 million bbl/d. Production is now in decline and the UK has been a net importer of oil since 2005. In 2010[ [update]] the UK had around 3.1 billion barrels of proven crude oil reserves, the largest of any EU member state. In 2009, 66.5% of the UK's oil supply was imported.
In 2009, the UK was the 13th-largest producer of natural gas in the world and the largest producer in the EU. Production is now in decline and the UK has been a net importer of natural gas since 2004. In 2009, half of British gas was supplied from imports and this is expected to increase to at least 75% by 2015, as domestic reserves are depleted.
Coal production played a key role in the UK economy in the 19th and 20th centuries. In the mid-1970s, 130 million tonnes of coal was being produced annually, not falling below 100 million tonnes until the early 1980s. During the 1980s and 1990s the industry was scaled back considerably. In 2011, the UK produced 18.3 million tonnes of coal. In 2005 it had proven recoverable coal reserves of 171 million tons. The UK Coal Authority has stated there is a potential to produce between 7 billion tonnes and 16 billion tonnes of coal through underground coal gasification (UCG) or 'fracking', and that, based on current UK coal consumption, such reserves could last between 200 and 400 years. However, environmental and social concerns have been raised over chemicals getting into the water table and minor earthquakes damaging homes.
In the late 1990s, nuclear power plants contributed around 25% of total annual electricity generation in the UK, but this has gradually declined as old plants have been shut down and ageing-related problems affect plant availability. In 2012, the UK had 16 reactors normally generating about 19% of its electricity. All but one of the reactors will be retired by 2023. Unlike Germany and Japan, the UK intends to build a new generation of nuclear plants from about 2018.
Demographics.
A census is taken simultaneously in all parts of the UK every ten years. The Office for National Statistics is responsible for collecting data for England and Wales, the General Register Office for Scotland and the Northern Ireland Statistics and Research Agency each being responsible for censuses in their respective countries. In the 2011 census the total population of the United Kingdom was 63,181,775. It is the third-largest in the European Union, the fifth-largest in the Commonwealth and the 21st-largest in the world. 2010 was the third successive year in which natural change contributed more to population growth than net long-term international migration. Between 2001 and 2011 the population increased by an average annual rate of approximately 0.7%. This compares to 0.3% per year in the period 1991 to 2001 and 0.2% in the decade 1981 to 1991. The 2011 census also confirmed that the proportion of the population aged 0–14 has nearly halved (31% in 1911 compared to 18 in 2011) and the proportion of older people aged 65 and over has more than tripled (from 5 to 16%). It has been estimated that the number of people aged 100 or over will rise steeply to reach over 626,000 by 2080.
England's population in 2011 was found to be 53 million. It is one of the most densely populated countries in the world, with 383 people resident per square kilometre in mid-2003, with a particular concentration in London and the south-east. The 2011 census put Scotland's population at 5.3 million, Wales at 3.06 million and Northern Ireland at 1.81 million. In percentage terms England has had the fastest growing population of any country of the UK in the period from 2001 to 2011, with an increase of 7.9%.
In 2012 the average total fertility rate (TFR) across the UK was 1.92 children per woman. While a rising birth rate is contributing to current population growth, it remains considerably below the 'baby boom' peak of 2.95 children per woman in 1964, below the replacement rate of 2.1, but higher than the 2001 record low of 1.63. In 2012, Scotland had the lowest TFR at only 1.67, followed by Wales at 1.88, England at 1.94, and Northern Ireland at 2.03. In 2011, 47.3% of births in the UK were to unmarried women. A government figure estimated that there are 3.6 million homosexual people in Britain comprising 6% of the population.
Ethnic groups.
Historically, indigenous British people were thought to be descended from the various ethnic groups that settled there before the 11th century: the Celts, Romans, Anglo-Saxons, Norse and the Normans. Welsh people could be the oldest ethnic group in the UK. A 2006 genetic study shows that more than 50% of England's gene pool contains Germanic Y chromosomes. Another 2005 genetic analysis indicates that "about 75% of the traceable ancestors of the modern British population had arrived in the British isles by about 6,200 years ago, at the start of the British Neolithic or Stone Age", and that the British broadly share a common ancestry with the Basque people.
The UK has a history of small-scale non-white immigration, with Liverpool having the oldest Black population in the country dating back to at least the 1730s during the period of the African slave trade, and the oldest Chinese community in Europe, dating to the arrival of Chinese seamen in the 19th century. In 1950 there were probably fewer than 20,000 non-white residents in Britain, almost all born overseas.
Since 1948 substantial immigration from Africa, the Caribbean and South Asia has been a legacy of ties forged by the British Empire. Migration from new EU member states in Central and Eastern Europe since 2004 has resulted in growth in these population groups, although some of this migration has been temporary. Since the 1990s, there has been substantial diversification of the immigrant population, with migrants to the UK coming from a much wider range of countries than previous waves, which tended to involve larger numbers of migrants coming from a relatively small number of countries.
Academics have argued that the ethnicity categories employed in British national statistics, which were first introduced in the 1991 census, involve confusion between the concepts of ethnicity and race. In 2011[ [update]], 87.2% of the UK population identified themselves as white, meaning 12.8% of the UK population identify themselves as of one of number of ethnic minority groups. In the 2001 census, this figure was 7.9% of the UK population.
Because of differences in the wording of the census forms used in England and Wales, Scotland and Northern Ireland, data on the Other White group is not available for the UK as a whole, but in England and Wales this was the fastest growing group between the 2001 and 2011 censuses, increasing by 1.1 million (1.8 percentage points). Amongst groups for which comparable data is available for all parts of the UK level, there was considerable growth in the size of the Other Asian category, which increased from 0.4 to 1.4% of the population between 2001 and 2011. There was also considerable growth in the Mixed category. In 2001, people in this category accounted for 1.2% of the UK population; by 2011, the proportion was 2%.
Ethnic diversity varies significantly across the UK. 30.4% of London's population and 37.4% of Leicester's was estimated to be non-white in 2005[ [update]], whereas less than 5% of the populations of North East England, Wales and the South West were from ethnic minorities, according to the 2001 census. In 2011[ [update]], 26.5% of primary and 22.2% of secondary pupils at state schools in England were members of an ethnic minority.
Languages.
The UK's "de facto" official language is English. It is estimated that 95% of the UK's population are monolingual English speakers. 5.5% of the population are estimated to speak languages brought to the UK as a result of relatively recent immigration. South Asian languages, including Bengali, Tamil, Punjabi, Hindi and Gujarati, are the largest grouping and are spoken by 2.7% of the UK population. According to the 2011 census, Polish has become the second-largest language spoken in England and has 546,000 speakers.
Four Celtic languages are spoken in the UK: Welsh; Irish; Scottish Gaelic; and Cornish. All are recognised as regional or minority languages, subject to specific measures of protection and promotion under the European Charter for Regional or Minority Languages and the Framework Convention for the Protection of National Minorities. In the 2001 Census over a fifth (21%) of the population of Wales said they could speak Welsh, an increase from the 1991 Census (18%). In addition it is estimated that about 200,000 Welsh speakers live in England. In the same census in Northern Ireland 167,487 people (10.4%) stated that they had "some knowledge of Irish" (see Irish language in Northern Ireland), almost exclusively in the nationalist (mainly Catholic) population. Over 92,000 people in Scotland (just under 2% of the population) had some Gaelic language ability, including 72% of those living in the Outer Hebrides. The number of schoolchildren being taught through Welsh, Scottish Gaelic and Irish is increasing. Among emigrant-descended populations some Scottish Gaelic is still spoken in Canada (principally Nova Scotia and Cape Breton Island), and Welsh in Patagonia, Argentina.
Scots, a language descended from early northern Middle English, has limited recognition alongside its regional variant, Ulster Scots in Northern Ireland, without specific commitments to protection and promotion.
It is compulsory for pupils to study a second language up to the age of 14 in England, and up to age 16 in Scotland. French and German are the two most commonly taught second languages in England and Scotland. All pupils in Wales are taught Welsh as a second language up to age 16, or are taught in Welsh.
Religion.
Forms of Christianity have dominated religious life in what is now the United Kingdom for over 1,400 years. Although a majority of citizens still identify with Christianity in many surveys, regular church attendance has fallen dramatically since the middle of the 20th century, while immigration and demographic change have contributed to the growth of other faiths, most notably Islam. This has led some commentators to variously describe the UK as a multi-faith, secularised, or post-Christian society.
In the 2001 census 71.6% of all respondents indicated that they were Christians, with the next largest faiths (by number of adherents) being Islam (2.8%), Hinduism (1.0%), Sikhism (0.6%), Judaism (0.5%), Buddhism (0.3%) and all other religions (0.3%). 15% of respondents stated that they had no religion, with a further 7% not stating a religious preference. A Tearfund survey in 2007 showed only one in ten Britons actually attend church weekly. Between the 2001 and 2011 census there was a decrease in the amount of people who identified as Christian by 12%, whilst the percentage of those reporting no religious affiliation doubled. This contrasted with growth in the other main religious group categories, with the number of Muslims increasing by the most substantial margin to a total of about 5%.
The Church of England is the established church in England. It retains a representation in the UK Parliament and the British monarch is its Supreme Governor. In Scotland the Presbyterian Church of Scotland is recognised as the national church. It is not subject to state control, and the British monarch is an ordinary member, required to swear an oath to "maintain and preserve the Protestant Religion and Presbyterian Church Government" upon his or her accession. The (Anglican) Church in Wales was disestablished in 1920 and, as the (Anglican) Church of Ireland was disestablished in 1870 before the partition of Ireland, there is no established church in Northern Ireland. Although there are no UK-wide data in the 2001 census on adherence to individual Christian denominations, it has been estimated that 62% of Christians are Anglican, 13.5% Catholic, 6% Presbyterian, 3.4% Methodist with small numbers of other Protestant denominations such as Open Brethren, and Orthodox churches.
Migration.
The United Kingdom has experienced successive waves of migration. The Great Famine in Ireland, then part of the United Kingdom, resulted in perhaps a million people migrating to Great Britain. Unable to return to Poland at the end of World War II, over 120,000 Polish veterans remained in the UK permanently. After World War II, there was significant immigration from the colonies and newly independent former colonies, partly as a legacy of empire and partly driven by labour shortages. Many of these migrants came from the Caribbean and the Indian subcontinent. In 1841, 0.25% of the population of England and Wales was born in a foreign country. By 1931, this figure had risen to 2.6%, and by 1951 it was 4.4%.
One of the more recent trends in migration has been the arrival of workers from the new EU member states in Eastern Europe, known as the A8 countries. In 2010, there were 7.0 million foreign-born residents in the UK, corresponding to 11.3% of the total population. Of these, 4.76 million (7.7%) were born outside the EU and 2.24 million (3.6%) were born in another EU Member State. The proportion of foreign-born people in the UK remains slightly below that of many other European countries. However, immigration is now contributing to a rising population with arrivals and UK-born children of migrants accounting for about half of the population increase between 1991 and 2001. Analysis of Office for National Statistics (ONS) data shows that a net total of 2.3 million migrants moved to the UK in the 15 years from 1991 to 2006. In 2008 it was predicted that migration would add 7 million to the UK population by 2031, though these figures are disputed. The ONS reported that net migration rose from 2009 to 2010 by 21% to 239,000. In 2011 the net increase was 251,000: immigration was 589,000, while the number of people emigrating (for more than 12 months) was 338,000.
195,046 foreign nationals became British citizens in 2010, compared to 54,902 in 1999. A record 241,192 people were granted permanent settlement rights in 2010, of whom 51% were from Asia and 27% from Africa. 25.5% of babies born in England and Wales in 2011 were born to mothers born outside the UK, according to official statistics released in 2012.
Citizens of the European Union, including those of the UK, have the right to live and work in any EU member state. The UK applied temporary restrictions to citizens of Romania and Bulgaria, which joined the EU in January 2007. Research conducted by the Migration Policy Institute for the Equality and Human Rights Commission suggests that, between May 2004 and September 2009, 1.5 million workers migrated from the new EU member states to the UK, two-thirds of them Polish, but that many subsequently returned home, resulting in a net increase in the number of nationals of the new member states in the UK of some 700,000 over that period. The late-2000s recession in the UK reduced the economic incentive for Poles to migrate to the UK, the migration becoming temporary and circular. In 2009, for the first time since enlargement, more nationals of the eight central and eastern European states that had joined the EU in 2004 left the UK than arrived. In 2011, citizens of the new EU member states made up 13% of the immigrants entering the country.
The UK government has introduced a points-based immigration system for immigration from outside the European Economic Area to replace former schemes, including the Scottish Government's Fresh Talent Initiative. In June 2010 the UK government introduced a temporary limit of 24,000 on immigration from outside the EU, aiming to discourage applications before a permanent cap was imposed in April 2011. The cap has caused tension within the coalition: business secretary Vince Cable has argued that it is harming British businesses.
Emigration was an important feature of British society in the 19th century. Between 1815 and 1930 around 11.4 million people emigrated from Britain and 7.3 million from Ireland. Estimates show that by the end of the 20th century some 300 million people of British and Irish descent were permanently settled around the globe. Today, at least 5.5 million UK-born people live abroad, mainly in Australia, Spain, the United States and Canada.
Education.
Education in the United Kingdom is a devolved matter, with each country having a separate education system.
Whilst education in England is the responsibility of the Secretary of State for Education, the day-to-day administration and funding of state schools is the responsibility of local authorities. Universally free of charge state education was introduced piecemeal between 1870 and 1944. Education is now mandatory from ages five to sixteen (15 if born in late July or August). In 2011, the Trends in International Mathematics and Science Study (TIMSS) rated 13–14-year-old pupils in England and Wales 10th in the world for maths and 9th for science. The majority of children are educated in state-sector schools, a small proportion of which select on the grounds of academic ability. Two of the top ten performing schools in terms of GCSE results in 2006 were state-run grammar schools. Over half of students at the leading universities of Cambridge and Oxford had attended state schools. Despite a fall in actual numbers the proportion of children in England attending private schools has risen to over 7%. In 2010, more than 45% of places at the University of Oxford and 40% at the University of Cambridge were taken by students from private schools, even though they educate just 7% of the population. England has the two oldest universities in English-speaking world, Universities of Oxford and Cambridge (jointly known as "Oxbridge") with history of over eight centuries. The United Kingdom has 9 universities featured in the Times Higher Education top 100 rankings, making it second to the United States in terms of representation.
Education in Scotland is the responsibility of the Cabinet Secretary for Education and Lifelong Learning, with day-to-day administration and funding of state schools the responsibility of Local Authorities. Two non-departmental public bodies have key roles in Scottish education. The Scottish Qualifications Authority is responsible for the development, accreditation, assessment and certification of qualifications other than degrees which are delivered at secondary schools, post-secondary colleges of further education and other centres. The Learning and Teaching Scotland provides advice, resources and staff development to education professionals. Scotland first legislated for compulsory education in 1496. The proportion of children in Scotland attending private schools is just over 4%, and it has been rising slowly in recent years. Scottish students who attend Scottish universities pay neither tuition fees nor graduate endowment charges, as fees were abolished in 2001 and the graduate endowment scheme was abolished in 2008.
The Welsh Government has responsibility for education in Wales. A significant number of Welsh students are taught either wholly or largely in the Welsh language; lessons in Welsh are compulsory for all until the age of 16. There are plans to increase the provision of Welsh-medium schools as part of the policy of creating a fully bilingual Wales.
Education in Northern Ireland is the responsibility of the Minister of Education and the Minister for Employment and Learning, although responsibility at a local level is administered by five education and library boards covering different geographical areas. The Council for the Curriculum, Examinations & Assessment (CCEA) is the body responsible for advising the government on what should be taught in Northern Ireland's schools, monitoring standards and awarding qualifications.
A government commission's report in 2014 found that privately educated people comprise 7% of the general population of the UK but much larger percentages of the top professions, the most extreme case quoted being 71% of senior judges.
Healthcare.
Healthcare in the United Kingdom is a devolved matter and each country has its own system of private and publicly funded health care, together with alternative, holistic and complementary treatments. Public healthcare is provided to all UK permanent residents and is mostly free at the point of need, being paid for from general taxation. The World Health Organization, in 2000, ranked the provision of healthcare in the United Kingdom as fifteenth best in Europe and eighteenth in the world.
Regulatory bodies are organised on a UK-wide basis such as the General Medical Council, the Nursing and Midwifery Council and non-governmental-based, such as the Royal Colleges. However, political and operational responsibility for healthcare lies with four national executives; healthcare in England is the responsibility of the UK Government; healthcare in Northern Ireland is the responsibility of the Northern Ireland Executive; healthcare in Scotland is the responsibility of the Scottish Government; and healthcare in Wales is the responsibility of the Welsh Assembly Government. Each National Health Service has different policies and priorities, resulting in contrasts.
Since 1979 expenditure on healthcare has been increased significantly to bring it closer to the European Union average. The UK spends around 8.4% of its gross domestic product on healthcare, which is 0.5 percentage points below the Organisation for Economic Co-operation and Development average and about one percentage point below the average of the European Union.
Culture.
The culture of the United Kingdom has been influenced by many factors including: the nation's island status; its history as a western liberal democracy and a major power; as well as being a political union of four countries with each preserving elements of distinctive traditions, customs and symbolism. As a result of the British Empire, British influence can be observed in the language, culture and legal systems of many of its former colonies including Australia, Canada, India, Ireland, New Zealand, South Africa and the United States. The substantial cultural influence of the United Kingdom has led it to be described as a "cultural superpower."
Literature.
'British literature' refers to literature associated with the United Kingdom, the Isle of Man and the Channel Islands. Most British literature is in the English language. In 2005, some 206,000 books were published in the United Kingdom and in 2006 it was the largest publisher of books in the world.
The English playwright and poet William Shakespeare is widely regarded as the greatest dramatist of all time, and his contemporaries Christopher Marlowe and Ben Jonson have also been held in continuous high esteem. More recently the playwrights Alan Ayckbourn, Harold Pinter, Michael Frayn, Tom Stoppard and David Edgar have combined elements of surrealism, realism and radicalism.
Notable pre-modern and early-modern English writers include Geoffrey Chaucer (14th century), Thomas Malory (15th century), Sir Thomas More (16th century), John Bunyan (17th century) and John Milton (17th century). In the 18th century Daniel Defoe (author of "Robinson Crusoe") and Samuel Richardson were pioneers of the modern novel. In the 19th century there followed further innovation by Jane Austen, the gothic novelist Mary Shelley, the children's writer Lewis Carroll, the Brontë sisters, the social campaigner Charles Dickens, the naturalist Thomas Hardy, the realist George Eliot, the visionary poet William Blake and romantic poet William Wordsworth. 20th century English writers include the science-fiction novelist H. G. Wells; the writers of children's classics Rudyard Kipling, A. A. Milne (the creator of Winnie-the-Pooh), Roald Dahl and Enid Blyton; the controversial D. H. Lawrence; the modernist Virginia Woolf; the satirist Evelyn Waugh; the prophetic novelist George Orwell; the popular novelists W. Somerset Maugham and Graham Greene; the crime writer Agatha Christie (the best-selling novelist of all time); Ian Fleming (the creator of James Bond); the poets T.S. Eliot, Philip Larkin and Ted Hughes; the fantasy writers J. R. R. Tolkien, C. S. Lewis and J. K. Rowling; the graphic novelist Alan Moore.
Scotland's contributions include the detective writer Arthur Conan Doyle (the creator of Sherlock Holmes), romantic literature by Sir Walter Scott, the children's writer J. M. Barrie, the epic adventures of Robert Louis Stevenson and the celebrated poet Robert Burns. More recently the modernist and nationalist Hugh MacDiarmid and Neil M. Gunn contributed to the Scottish Renaissance. A more grim outlook is found in Ian Rankin's stories and the psychological horror-comedy of Iain Banks. Scotland's capital, Edinburgh, was UNESCO's first worldwide City of Literature.
Britain's oldest known poem, "Y Gododdin", was composed in "Yr Hen Ogledd" ("The Old North"), most likely in the late 6th century. It was written in Cumbric or Old Welsh and contains the earliest known reference to King Arthur. From around the seventh century, the connection between Wales and the Old North was lost, and the focus of Welsh-language culture shifted to Wales, where Arthurian legend was further developed by Geoffrey of Monmouth. Wales's most celebrated medieval poet, Dafydd ap Gwilym ("fl."1320–1370), composed poetry on themes including nature, religion and especially love. He is widely regarded as one of the greatest European poets of his age. Until the late 19th century the majority of Welsh literature was in Welsh and much of the prose was religious in character. Daniel Owen is credited as the first Welsh-language novelist, publishing "Rhys Lewis" in 1885. The best-known of the Anglo-Welsh poets are both Thomases. Dylan Thomas became famous on both sides of the Atlantic in the mid-20th century. He is remembered for his poetry – his "Do not go gentle into that good night; Rage, rage against the dying of the light." is one of the most quoted couplets of English language verse – and for his 'play for voices', "Under Milk Wood". The influential Church in Wales 'poet-priest' and Welsh nationalist R. S. Thomas was nominated for the Nobel Prize in Literature in 1996. Leading Welsh novelists of the twentieth century include Richard Llewellyn and Kate Roberts.
Authors of other nationalities, particularly from Commonwealth countries, the Republic of Ireland and the United States, have lived and worked in the UK. Significant examples through the centuries include Jonathan Swift, Oscar Wilde, Bram Stoker, George Bernard Shaw, Joseph Conrad, T.S. Eliot, Ezra Pound and more recently British authors born abroad such as Kazuo Ishiguro and Sir Salman Rushdie.
Music.
Various styles of music are popular in the UK from the indigenous folk music of England, Wales, Scotland and Northern Ireland to heavy metal. Notable composers of classical music from the United Kingdom and the countries that preceded it include William Byrd, Henry Purcell, Sir Edward Elgar, Gustav Holst, Sir Arthur Sullivan (most famous for working with the librettist Sir W. S. Gilbert), Ralph Vaughan Williams and Benjamin Britten, pioneer of modern British opera. Sir Peter Maxwell Davies is one of the foremost living composers and current Master of the Queen's Music. The UK is also home to world-renowned symphonic orchestras and choruses such as the BBC Symphony Orchestra and the London Symphony Chorus. Notable conductors include Sir Simon Rattle, John Barbirolli and Sir Malcolm Sargent. Some of the notable film score composers include John Barry, Clint Mansell, Mike Oldfield, John Powell, Craig Armstrong, David Arnold, John Murphy, Monty Norman and Harry Gregson-Williams. George Frideric Handel, although born German, was a naturalised British citizen and some of his best works, such as "Messiah", were written in the English language. Andrew Lloyd Webber has achieved enormous worldwide commercial success and is a prolific composer of musical theatre, works which have dominated London's West End for a number of years and have travelled to Broadway in New York.
The Beatles have international sales of over one billion units and are the biggest-selling and most influential band in the history of popular music. Other prominent British contributors to have influenced popular music over the last 50 years include; The Rolling Stones, Led Zeppelin, Pink Floyd, Queen, the Bee Gees, and Elton John, all of whom have world wide record sales of 200 million or more. The Brit Awards are the BPI's annual music awards, and some of the British recipients of the Outstanding Contribution to Music award include; The Who, David Bowie, Eric Clapton, Rod Stewart and The Police. More recent UK music acts that have had international success include Coldplay, Radiohead, Oasis, Spice Girls, Robbie Williams, Amy Winehouse and Adele.
A number of UK cities are known for their music. Acts from Liverpool have had more UK chart number one hit singles per capita (54) than any other city worldwide. Glasgow's contribution to music was recognised in 2008 when it was named a UNESCO City of Music, one of only three cities in the world to have this honour.
Visual art.
The history of British visual art forms part of western art history. Major British artists include: the Romantics William Blake, John Constable, Samuel Palmer and J.M.W. Turner; the portrait painters Sir Joshua Reynolds and Lucian Freud; the landscape artists Thomas Gainsborough and L. S. Lowry; the pioneer of the Arts and Crafts Movement William Morris; the figurative painter Francis Bacon; the Pop artists Peter Blake, Richard Hamilton and David Hockney; the collaborative duo Gilbert and George; the abstract artist Howard Hodgkin; and the sculptors Antony Gormley, Anish Kapoor and Henry Moore. During the late 1980s and 1990s the Saatchi Gallery in London helped to bring to public attention a group of multi-genre artists who would become known as the "Young British Artists": Damien Hirst, Chris Ofili, Rachel Whiteread, Tracey Emin, Mark Wallinger, Steve McQueen, Sam Taylor-Wood and the Chapman Brothers are among the better-known members of this loosely affiliated movement.
The Royal Academy in London is a key organisation for the promotion of the visual arts in the United Kingdom. Major schools of art in the UK include: the six-school University of the Arts London, which includes the Central Saint Martins College of Art and Design and Chelsea College of Art and Design; Goldsmiths, University of London; the Slade School of Fine Art (part of University College London); the Glasgow School of Art; the Royal College of Art; and The Ruskin School of Drawing and Fine Art (part of the University of Oxford). The Courtauld Institute of Art is a leading centre for the teaching of the history of art. Important art galleries in the United Kingdom include the National Gallery, National Portrait Gallery, Tate Britain and Tate Modern (the most-visited modern art gallery in the world, with around 4.7 million visitors per year).
Cinema.
The United Kingdom has had a considerable influence on the history of the cinema. The British directors Alfred Hitchcock, whose film "Vertigo" is considered by some critics as the best film of all time, and David Lean are among the most critically acclaimed of all-time. Other important directors including Charlie Chaplin, Michael Powell, Carol Reed and Ridley Scott. Many British actors have achieved international fame and critical success, including: Julie Andrews, Richard Burton, Michael Caine, Charlie Chaplin, Sean Connery, Vivien Leigh, David Niven, Laurence Olivier, Peter Sellers, Kate Winslet, Anthony Hopkins, and Daniel Day-Lewis. Some of the most commercially successful films of all time have been produced in the United Kingdom, including the two highest-grossing film franchises ("Harry Potter" and "James Bond"). Ealing Studios has a claim to being the oldest continuously working film studio in the world.
Despite a history of important and successful productions, the industry has often been characterised by a debate about its identity and the level of American and European influence. British producers are active in international co-productions and British actors, directors and crew feature regularly in American films. Many successful Hollywood films have been based on British people, stories or events, including "Titanic", "The Lord of the Rings", "Pirates of the Caribbean".
In 2009, British films grossed around $2 billion worldwide and achieved a market share of around 7% globally and 17% in the United Kingdom. UK box-office takings totalled £944 million in 2009, with around 173 million admissions. The British Film Institute has produced a poll ranking of what it considers to be the 100 greatest British films of all time, the BFI Top 100 British films. The annual British Academy Film Awards are hosted by the British Academy of Film and Television Arts.
Media.
The BBC, founded in 1922, is the UK's publicly funded radio, television and Internet broadcasting corporation, and is the oldest and largest broadcaster in the world. It operates numerous television and radio stations in the UK and abroad and its domestic services are funded by the television licence. Other major players in the UK media include ITV plc, which operates 11 of the 15 regional television broadcasters that make up the ITV Network, and News Corporation, which owns a number of national newspapers through News International such as the most popular tabloid "The Sun" and the longest-established daily "broadsheet" "The Times", as well as holding a large stake in satellite broadcaster British Sky Broadcasting. London dominates the media sector in the UK: national newspapers and television and radio are largely based there, although Manchester is also a significant national media centre. Edinburgh and Glasgow, and Cardiff, are important centres of newspaper and broadcasting production in Scotland and Wales respectively. The UK publishing sector, including books, directories and databases, journals, magazines and business media, newspapers and news agencies, has a combined turnover of around £20 billion and employs around 167,000 people.
In 2009, it was estimated that individuals viewed a mean of 3.75 hours of television per day and 2.81 hours of radio. In that year the main BBC public service broadcasting channels accounted for an estimated 28.4% of all television viewing; the three main independent channels accounted for 29.5% and the increasingly important other satellite and digital channels for the remaining 42.1%. Sales of newspapers have fallen since the 1970s and in 2009 42% of people reported reading a daily national newspaper. In 2010 82.5% of the UK population were Internet users, the highest proportion amongst the 20 countries with the largest total number of users in that year.
Philosophy.
The United Kingdom is famous for the tradition of 'British Empiricism', a branch of the philosophy of knowledge that states that only knowledge verified by experience is valid, and 'Scottish Philosophy', sometimes referred to as the 'Scottish School of Common Sense'. The most famous philosophers of British Empiricism are John Locke, George Berkeley and David Hume; while Dugald Stewart, Thomas Reid and William Hamilton were major exponents of the Scottish "common sense" school. Two Britons are also notable for a theory of moral philosophy utilitarianism, first used by Jeremy Bentham and later by John Stuart Mill in his short work "Utilitarianism".
Other eminent philosophers from the UK and the unions and countries that preceded it include Duns Scotus, John Lilburne, Mary Wollstonecraft, Sir Francis Bacon, Adam Smith, Thomas Hobbes, William of Ockham, Bertrand Russell and A.J. "Freddie" Ayer. Foreign-born philosophers who settled in the UK include Isaiah Berlin, Karl Marx, Karl Popper and Ludwig Wittgenstein.
Sport.
Major sports, including association football, tennis, rugby union, rugby league, golf, boxing, rowing and cricket, originated or were substantially developed in the UK and the states that preceded it. With the rules and codes of many modern sports invented and codified in late 19th century Victorian Britain, in 2012, the President of the IOC, Jacques Rogge, stated; "This great, sports-loving country is widely recognized as the birthplace of modern sport. It was here that the concepts of sportsmanship and fair play were first codified into clear rules and regulations. It was here that sport was included as an educational tool in the school curriculum".
In most international competitions, separate teams represent England, Scotland and Wales. Northern Ireland and the Republic of Ireland usually field a single team representing all of Ireland, with notable exceptions being association football and the Commonwealth Games. In sporting contexts, the English, Scottish, Welsh and Irish / Northern Irish teams are often referred to collectively as the Home Nations. There are some sports in which a single team represents the whole of United Kingdom, including the Olympics, where the UK is represented by the Great Britain team. The 1908, 1948 and 2012 Summer Olympics were held in London, making it the first city to host the games three times. Britain has participated in every modern Olympic Games to date and is third in the medal count.
A 2003 poll found that football is the most popular sport in the United Kingdom. England is recognised by FIFA as the birthplace of club football, and The Football Association is the oldest of its kind, with the rules of football first drafted in 1863 by Ebenezer Cobb Morley. Each of the Home Nations has its own football association, national team and league system. The English top division, the Premier League, is the most watched football league in the world. The first-ever international football match was contested by England and Scotland on 30 November 1872. England, Scotland, Wales and Northern Ireland compete as separate countries in international competitions. A Great Britain Olympic football team was assembled for the first time to compete in the London 2012 Olympic Games. However, the Scottish, Welsh and Northern Irish football associations declined to participate, fearing that it would undermine their independent status – a fear confirmed by FIFA president Sepp Blatter.
In 2003, rugby union was ranked the second most popular sport in the UK. The sport was created in Rugby School, Warwickshire, and the first rugby international took place on 27 March 1871 between England and Scotland. England, Scotland, Wales, Ireland, France and Italy compete in the Six Nations Championship; the premier international tournament in the northern hemisphere. Sport governing bodies in England, Scotland, Wales and Ireland organise and regulate the game separately. If any of the British teams or the Irish team beat the other three in a tournament, then it is awarded the Triple Crown.
Cricket was invented in England, and its laws were established by Marylebone Cricket Club in 1788. The England cricket team, controlled by the England and Wales Cricket Board, is the only national team in the UK with Test status. Team members are drawn from the main county sides, and include both English and Welsh players. Cricket is distinct from football and rugby where Wales and England field separate national teams, although Wales had fielded its own team in the past. Irish and Scottish players have played for England because neither Scotland nor Ireland have Test status and have only recently started to play in One Day Internationals. Scotland, England (and Wales), and Ireland (including Northern Ireland) have competed at the Cricket World Cup, with England reaching the finals on three occasions. There is a professional league championship in which clubs representing 17 English counties and 1 Welsh county compete.
Rugby league originated in Huddersfield and is generally played in Northern England. A single 'Great Britain Lions' team had competed in the Rugby League World Cup and Test match games, but this changed in 2008 when England, Scotland and Ireland competed as separate nations. Great Britain is still being retained as the full national team for Ashes tours against Australia, New Zealand and France. Super League is the highest level of professional rugby league in the UK and Europe. It consists of 11 teams from Northern England, 1 from London, 1 from Wales and 1 from France.
The modern game of tennis originated in Birmingham, England in the 1860s, before spreading around the world. The world's oldest tennis tournament, the Wimbledon championships, first occurred in 1877, and today the event takes place over two weeks in late June and early July.
Thoroughbred racing, which originated under Charles II of England as the "sport of kings", is popular throughout the UK with world-famous races including the Grand National, the Epsom Derby, Royal Ascot and the Cheltenham National Hunt Festival (including the Cheltenham Gold Cup). The UK has proved successful in the international sporting arena in rowing.
The UK is closely associated with motorsport. Many teams and drivers in Formula One (F1) are based in the UK, and the country has won more drivers' and constructors' titles than any other. The UK hosted the very first F1 Grand Prix in 1950 at Silverstone, the current location of the British Grand Prix held each year in July. The country also hosts legs of the Grand Prix motorcycle racing, World Rally Championship and FIA World Endurance Championship. The premier national auto racing event is the British Touring Car Championship (BTCC). Motorcycle road racing has a long tradition with races such as the Isle of Man TT and the North West 200.
Golf is the sixth-most popular sport, by participation, in the UK. Although The Royal and Ancient Golf Club of St Andrews in Scotland is the sport's home course, the world's oldest golf course is actually Musselburgh Links' Old Golf Course. In 1764, the standard 18 hole golf course was created at St Andrews when members modified the course from 22 to 18 holes. The oldest golf tournament in the world, and the first major championship in golf, The Open Championship, is played annually on the weekend of the third Friday in July.
Snooker is one of the UK's popular sporting exports, with the world championships held annually in Sheffield. The modern game of lawn tennis first originated in the city of Birmingham between 1859 and 1865. The Championships, Wimbledon are international tennis events held in Wimbledon in south London every summer and are regarded as the most prestigious event of the global tennis calendar. In Northern Ireland Gaelic football and hurling are popular team sports, both in terms of participation and spectating, and Irish expatriates in the UK and the US also play them. Shinty (or "camanachd") is popular in the Scottish Highlands.
Symbols.
The flag of the United Kingdom is the Union Flag (also referred to as the Union Jack). It was created in 1606 by the superimposition of the Flag of England on the Flag of Scotland and updated in 1801 with the addition of Saint Patrick's Flag. Wales is not represented in the Union Flag, as Wales had been conquered and annexed to England prior to the formation of the United Kingdom. The possibility of redesigning the Union Flag to include representation of Wales has not been completely ruled out. The national anthem of the United Kingdom is "God Save the King", with "King" replaced with "Queen" in the lyrics whenever the monarch is a woman.
Britannia is a national personification of the United Kingdom, originating from Roman Britain. Britannia is symbolised as a young woman with brown or golden hair, wearing a Corinthian helmet and white robes. She holds Poseidon's three-pronged trident and a shield, bearing the Union Flag. Sometimes she is depicted as riding on the back of a lion. Since the height of the British Empire in the late 19th century, Britannia has often been associated with British maritime dominance, as in the patriotic song "Rule, Britannia!". Up until 2008, the lion symbol was depicted behind Britannia on the British fifty pence coin and on the back of the British ten pence coin. It is also used as a symbol on the non-ceremonial flag of the British Army. The bulldog is sometimes used as a symbol of the United Kingdom and has been associated with Winston Churchill's defiance of Nazi Germany.

</doc>
<doc id="31718" url="http://en.wikipedia.org/wiki?curid=31718" title="Utopia">
Utopia

A utopia ( ) is a community or society possessing highly desirable or near perfect qualities. The word was coined by Sir Thomas More in Greek for his 1516 book "Utopia" (in Latin), describing a fictional island society in the Atlantic Ocean. The term has been used to describe both intentional communities that attempt to create an ideal society, and imagined societies portrayed in fiction. It has spawned other concepts, most prominently dystopia.
Etymology.
The term "" was coined in Greek by Sir Thomas More for his 1516 book "Utopia", describing a fictional island society in the Atlantic Ocean.
The word comes from the "Greek: οὐ" ("not") and "τόπος" ("place") and means "no-place", and strictly describes any non-existent society 'described in considerable detail'. However, in standard usage, the word's meaning has narrowed and now usually describes a non-existent society that is intended to be viewed as considerably better than contemporary society. "Eutopia", derived from the Greek "εὖ" ("good" or "well") and "τόπος" ("place"), means "good place", and is strictly speaking the correct term to describe a positive utopia. In English, "eutopia" and "utopia" are homophonous, which may have given rise to the change in meaning.
Varieties.
Chronologically, the first recorded utopian proposal is Plato's "Republic". Part conversation, part fictional depiction, and part policy proposal, it proposes a categorization of citizens into a rigid class structure of "golden," "silver," "bronze" and "iron" socioeconomic classes. The golden citizens are trained in a rigorous 50-year-long educational program to be benign oligarchs, the "philosopher-kings." Plato had stressed this many times in both quotes by him and in his published works, such as The Republic. The wisdom of these rulers will supposedly eliminate poverty and deprivation through fairly distributed resources, though the details on how to do this are unclear. The educational program for the rulers is the central notion of the proposal. It has few laws, no lawyers and rarely sends its citizens to war, but hires mercenaries from among its war-prone neighbors (these mercenaries were deliberately sent into dangerous situations in the hope that the more warlike populations of all surrounding countries will be weeded out, leaving peaceful peoples).
During the 16th century, Thomas More's book "Utopia" proposed an ideal society of the same name. Some readers, including utopian socialists, have chosen to accept this imaginary society as the realistic blueprint for a working nation, while others have postulated that More intended nothing of the sort. Some maintain the position that More's "Utopia" functions only on the level of a satire, a work intended to reveal more about the England of his time than about an idealistic society. This interpretation is bolstered by the title of the book and nation, and its apparent confusion between the Greek for "no place" and "good place": "utopia" is a compound of the syllable ou-, meaning "no", and topos, meaning place. But the homophonic prefix eu-, meaning "good," also resonates in the word, with the implication that the perfectly "good place" is really "no place."""'
Ecology.
Ecological utopian society describes new ways in which society should relate to nature.
They react to a perceived widening gap between the modern Western way of living that, allegedly, destroys nature and a more traditional way of living before industrialization, that is regarded by the ecologists to be more in harmony with nature. According to the Dutch philosopher Marius de Geus, ecological utopias could be sources of inspiration for green political movements.
Economics.
Particularly in the early 19th century, several utopian ideas arose, often in response to their belief that social disruption was created and caused by the development of commercialism and capitalism. These are often grouped in a greater "utopian socialist" movement, due to their shared characteristics: an egalitarian distribution of goods, frequently with the total abolition of money, and citizens only doing work which they enjoy and which is for the common good, leaving them with ample time for the cultivation of the arts and sciences. One classic example of such a utopia was Edward Bellamy's "Looking Backward". Another socialist utopia is William Morris' "News from Nowhere", written partially in response to the top-down (bureaucratic) nature of Bellamy's utopia, which Morris criticized. However, as the socialist movement developed it moved away from utopianism; Marx in particular became a harsh critic of earlier socialism he described as utopian. (For more information see the History of Socialism article.) In a utopian society, the economy, concurrent with the ongoing theme, is perfect; there is no inflation, and perfect social and financial equality. However, in 1905 H.G. Wells published "A Modern Utopia", which was widely read and admired and provoked much discussion. Also consider Eric Frank Russell's book "The Great Explosion" (1963) whose last section details an economic and social utopia. This forms the first mention of the idea of Local Exchange Trading Systems (LETS).
Politics and history.
A global utopia of world peace is often seen as one of the possible end results of world history. Within the localized political structures or spheres it presents, "polyculturalism" is the model-based adaptation of possible interactions with different cultures and identities in accordance with the principles of participatory society.
The Soviet writer Ivan Efremov produced, during the "Thaw" period, the science-fiction utopia "Andromeda" (1957) in which a united humanity communicates with a galaxy-wide Great Circle and develops its technology and culture within a social framework characterized by vigorous competition between alternative philosophies.
The English political philosopher James Harrington, author of the utopian work The Commonwealth of Oceana, inspired English country party republicanism and was influential in the design of three American colonies. His theories ultimately contributed to the idealistic principles of the American Founders. The colonies of Carolina (founded in 1670), Pennsylvania (founded in 1681), and Georgia (founded in 1733) were the only three English colonies in America that were planned as utopian societies with an integrated physical, economic, and social design. At the heart of the plan for Georgia was a concept of “agrarian equality” in which land was allocated equally and additional land acquisition through purchase or inheritance was prohibited; the plan was an early step toward the yeoman republic later envisioned by Thomas Jefferson.
The communes of the 1960s in the United States were often an attempt to greatly improve the way humans live together in communities. The back to the land movements and hippies inspired many to try to live in peace and harmony on farms, remote areas, and to set up new types of governance.
Intentional communities were organized and built all over the world with the hope of making a more perfect way of living together. However, many of these new small communities failed, but some are growing like the Twelve Tribes Communities that started in the United States and have grown to many tribes around the world.
Religious utopias.
Religious utopias can be intra-religious or inter-religious. 
Inter-religious utopias.
The inter-religious utopia borders on a concept like polyculturalism and is not deemed possible in the near future or the near-far future. Fledgling theories are generally canceled as impossible, but the ideology of God and religion used in inter-religious utopia is commonly stated by many people as their view of God. In more extended theories it goes up to the level of different religious leaders setting aside their differences and accepting harmony, peace and understanding to unite all religions within one another, thereby forming a utopian religion or a religion of humans with God any type of force that reigned before the birth of the universe. Religion and God are used as a self-motivating factor for people to believe in and to raise themselves out of difficult situations. 
Intra-religious utopias.
Intra-religious utopias are based on religious ideals, and are to date those most commonly found in human societies. Their members are usually required to follow and believe in the particular religious tradition that established the utopia. Some permit non-believers or non-adherents to take up residence within them;[] others (such as the community at Qumran) do not.
The Jewish, Christian, and Islamic ideas of the Garden of Eden and of Heaven/Paradise may be interpreted as forms of utopianism, especially in their folk-religious forms. Such religious utopias are often described as "gardens of delight", implying an existence free from worry in a state of bliss or enlightenment. They postulate freedom from sin, pain, poverty, and death; and often assume communion with beings such as angels or the houri. In a similar sense, the Hindu concept of moksha and the Buddhist concept of nirvana may be thought of as a kind of utopia. However, in Hinduism or Buddhism, utopia is not a place but a state of mind. a belief that if one is able to practice meditation without continuous stream of thoughts, one is able to reach enlightenment. This enlightenment promises exit from the cycle of life and death, relating back to the concept of utopia.
Some Jews believe that, at some point in the future, the prophet Elijah will return with the Messiah and set up a worldwide religious utopia, heralding in a Messianic Age.
In the United States and Europe during the Second Great Awakening (ca. 1790–1840) and thereafter, many radical religious groups formed utopian societies in which faith could govern all aspects of members' lives. These utopian societies included the Shakers, who originated in England in the 18th century and arrived in America in 1774. A number of religious utopian societies from Europe came to the United States from the 18th century throughout the 19th century, including the Society of the Woman in the Wilderness (led by Johannes Kelpius (1667–1708)), the Ephrata Cloister (established in 1732), and the Harmony Society, among others. The Harmony Society was a Christian theosophy and pietist group founded in Iptingen, Germany, in 1785. Due to religious persecution by the Lutheran Church and the government in Württemberg, the society moved to the United States on October 7, 1803, settled in Pennsylvania, and on February 15, 1805, they, together with about 400 followers, formally organized the Harmony Society, placing all their goods in common. The group lasted until 1905, making it one of the longest-running financially successful communes in American history. The Oneida Community, founded by John Humphrey Noyes in Oneida, New York, was a utopian religious commune that lasted from 1848 to 1881. Although this utopian experiment has become better known today for its manufacture of Oneida silverware, it was one of the longest-running communes in American history. The Amana Colonies were communal settlements in Iowa, started by radical German pietists, which lasted from 1855 to 1932. The Amana Corporation, manufacturer of refrigerators and household appliances, was originally started by the group. Other examples are Fountain Grove (founded in 1875), Riker's Holy City and other Californian utopian colonies between 1855 and 1955 (Hine), as well as Sointula in British Columbia, Canada. The Amish and Hutterites can also be considered an attempt towards religious utopia. A wide variety of intentional communities with some type of faith-based ideas have also started across the world.
The Book of Revelation in the Christian Bible depicts a hypothetical time in the future after the defeat of Satan and of evil. One interpretation of the text sees it as depicting Heaven on Earth, or a new Earth without sin. Many details of this hypothetical new Earth, where God and Jesus rule, remain unclear, although it is implied to be similar to the biblical Garden of Eden. Some theological philosophers believe that heaven will not be a physical realm, but instead an incorporeal place for souls.
Science and technology.
Scientific and technological utopias are set in the future, when it is believed that advanced science and technology will allow utopian living standards; for example, the absence of death and suffering; changes in human nature and the human condition. Technology has affected the way humans have lived to such an extent that normal functions, like sleep, eating or even reproduction, have been replaced by artificial means. Other examples include a society where humans have struck a balance with technology and it is merely used to enhance the human living condition (e.g. "Star Trek"). In place of the static perfection of a utopia, libertarian transhumanists envision an "extropia", an open, evolving society allowing individuals and voluntary groupings to form the institutions and social forms they prefer.
Buckminster Fuller presented a theoretical basis for technological utopianism and set out to develop a variety of technologies ranging from maps to designs for cars and houses which might lead to the development of such a utopia.
One notable example of a technological and libertarian socialist utopia is Scottish author Iain Banks' Culture.
Opposing this optimism is the prediction that advanced science and technology will, through deliberate misuse or accident, cause environmental damage or even humanity's extinction. Critics, such as Jacques Ellul and Timothy Mitchell advocate precautions against the premature embrace of new technologies, raising questions on responsibility and freedom brought by division of labour. Authors such as John Zerzan and Derrick Jensen consider that modern technology is progressively depriving humans of their autonomy, and advocate the collapse of the industrial civilization, in favor of small-scale organization, as a necessary path to avoid the threat of technology on human freedom and sustainability.
There are many examples of techno-dystopias portrayed in mainstream culture, such as the classics "Brave New World" and "Nineteen Eighty-Four", which have explored some of these topics.
Feminism.
Utopias have been used to explore the ramifications of gender's being either a societal construct, or a biologically "hard-wired" imperative, or some mix of the two. Socialist and economic utopias have tended to take the "woman question" seriously, and often to offer some form of equality between the sexes as part and parcel of their vision, whether this be by addressing misogyny, reorganizing society along separatist lines, creating a certain kind of androgynous equality that ignores gender, or in some other manner. For example, Edward Bellamy's "Looking Backward" (1887) responded, progressively for his day, to the contemporary women's suffrage and women's rights movements, which he supported, by incorporating the equality of women and men into his utopian world's structure, albeit by consigning women to a separate sphere of light industrial activity (due to women's lesser physical strength), and making various exceptions for them in order to make room (and praise) for motherhood. One of the earlier feminist utopias that imagines complete separatism is Charlotte Perkins Gilman's "Herland" (1915).
In science fiction and technological speculation, gender can be challenged on the biological as well as the social level. In Marge Piercy's "Woman on the Edge of Time," the utopian future offers equality between the genders and complete equality in sexuality (regardless of the gender of the lovers); birth-giving, often felt as the divider that cannot be avoided in discussions of women's rights and roles, has been shifted onto elaborate biological machinery that functions to offer an enriched embryonic experience; when a child is born, it spends most of its time in the children's ward with peers. Three "mothers" per child are the norm, and they are chosen in a gender neutral way (men as well as women may become "mothers") on the basis of their experience and ability. Technological advances also make possible the freeing of women from childbearing in Shulamit Firestone's "The Dialectic of Sex". The fictional aliens in Mary Gentle's "Golden Witchbreed" start out as gender-neutral children and do not develop into men and women until puberty, and gender has no bearing on social roles. In contrast, Doris Lessing's "The Marriages Between Zones Three, Four and Five" (1980) suggests that men's and women's values are inherent to the sexes and cannot be changed, making a compromise between them essential. In "My Own Utopia" (1961) by Elizabeth Mann Borghese, gender exists but is dependent upon age rather than sex — genderless children mature into women, some of whom eventually become men. "William Marston's Wonder Woman comics of the 1940s featured Paradise Island, a matriarchal all-female community of peace, loving submission, bondage, and giant space kangaroos."
Utopian single-gender worlds or single-sex societies have long been one of the primary ways to explore implications of gender and gender-differences. In speculative fiction, female-only worlds have been imagined to come about by the action of disease that wipes out men, along with the development of technological or mystical method that allow female parthenogenic reproduction. Charlotte Perkins Gilman's 1915 novel approaches this type of separate society. Many feminist utopias pondering separatism were written in the 1970s, as a response to the Lesbian separatist movement; examples include Joanna Russ's "The Female Man" and Suzy McKee Charnas's "Walk to the End of the World" and "Motherlines". Utopias imagined by male authors have often included equality between sexes, rather than separation, although as noted Bellamy's strategy includes a certain amount of "separate but equal". The use of female-only worlds allows the exploration of female independence and freedom from patriarchy. The societies may not necessarily be lesbian, or sexual at all — a famous early sexless example being "Herland" (1915) by Charlotte Perkins Gilman. Charlene Ball writes in "Women's Studies Encyclopedia" that use of speculative fiction to explore gender roles in future societies has been more common in the United States compared to Europe and elsewhere, although such efforts as Gert Brantenberg's "Egalia's Daughters" and Christa Wolf's portrayal of the land of Colchis in her "Medea: Voices "are certainly as influential and famous as any of the American feminist utopias.
Utopianism.
In many cultures, societies, and religions, there is some myth or memory of a distant past when humankind lived in a primitive and simple state, but at the same time one of perfect happiness and fulfillment. In those days, the various myths tell us, there was an instinctive harmony between humanity and nature. People's needs were few and their desires limited. Both were easily satisfied by the abundance provided by nature. Accordingly, there were no motives whatsoever for war or oppression. Nor was there any need for hard and painful work. Humans were simple and pious, and felt themselves close to the gods. According to one anthropological theory, hunter-gatherers were the original affluent society.
These mythical or religious archetypes are inscribed in many cultures, and resurge with special vitality when people are in difficult and critical times. However, the projection of the myth does not take place towards the remote past, but either towards the future or towards distant and fictional places, imagining that at some time in the future, at some point in space, or beyond death, there must exist the possibility of living happily.
These myths of the earliest stage of humankind have been referred to by various cultures, societies, and religions:
Golden Age
The Greek poet Hesiod, around the 8th century BC, in his compilation of the mythological tradition (the poem "Works and Days"), explained that, prior to the present era, there were four other progressively more perfect ones, the oldest of which was the Golden Age.
Plutarch, the Greek historian and biographer of the 1st century, dealt with the blissful and mythic past of the humanity.
Arcadia, e.g. in Sir Philip Sidney's prose romance "The Old Arcadia" (1580). Originally a region in the Peloponnesus, Arcadia became a synonym for any rural area that serves as a pastoral setting, as a "locus amoenus" ("delightful place"):
The Biblical Garden of Eden
The Biblical Garden of Eden as depicted in Genesis 2 (Authorized Version of 1611):
"And the Lord God planted a garden eastward in Eden; and there he put the man whom he had formed. And out of the ground made the Lord God to grow every tree that is pleasant to the sight, and good for food; the tree of life also in the midst of the garden, and the tree of knowledge of good and evil. [...]
And the Lord God took the man, and put him into the garden of Eden to dress it and to keep it. And the Lord God commanded the man, saying, Of every tree of the garden thou mayest freely eat: but of the tree of the knowledge of good and evil, thou shalt not eat of it: for in the day that thou eatest thereof thou shalt surely die. [...]
And the Lord God said, It is not good that the man should be alone; [...] And the Lord God caused a deep sleep to fall upon Adam, and he slept: and he took one of his ribs, and closed up the flesh instead thereof; and the rib, which the Lord God had taken from man, made he a woman, and brought her unto the man."
The Land of Cockaigne
The Land of Cockaigne (also Cockaygne, Cokaygne), was an imaginary land of idleness and luxury, famous in medieval story, and the subject of more than one poem, one of which, an early translation of a 13th-century French work, is given in Ellis's "Specimens of Early English Poets". In this, "the houses were made of barley sugar and cakes, the streets were paved with pastry, and the shops supplied goods for nothing." London has been so called (see Cockney), but Boileau applies the same to Paris.
The Peach Blossom Spring, a prose written by Tao Yuanming, describes a utopian place. The narrative goes that a fisherman from Wuling sailed upstream a river and came across a beautiful blossoming peach grove and lush green fields covered with blossom petals. Entranced by the beauty, he continued upstream. When he reached the end of the river, he stumbled onto a small grotto. Though narrow at first, he was able to squeeze through the passage and discovered an ethereal utopia, where the people led an ideal existence in harmony with nature. He saw a vast expanse of fertile lands, clear ponds, mulberry trees, bamboo groves, and the like with a community of people of all ages and houses in neat rows. The people explained that their ancestors escaped to this place during the civil unrest of the Qin Dynasty and they themselves had not left since or had contact with anyone from the outside. They had not even heard of the later dynasties of bygone times or the then-current Jin Dynasty. In the story, the community was secluded and unaffected by the troubles of the outside world. The sense of timelessness was also predominant in the story as a perfect utopian community remains unchanged, that is, it had no decline nor the need to improve. Eventually, the Chinese term "Peach Blossom Spring" (桃花源) came to be synonymous for the concept of utopia.
Datong is a traditional Chinese Utopia. The main description of it is found in the Chinese Classic of Rites, in the chapter called "Li Yun" (禮運). Later, Datong and its ideal of 'The World Belongs to Everyone/The World is Held in Common' 'Tianxia weigong/天下为公' 'influenced modern Chinese reformers and revolutionaries, such as Kang Youwei.
Schlaraffenland is an analogous German tradition. (.)
These myths also express some hope that the idyllic state of affairs they describe is not irretrievably and irrevocably lost to mankind, that it can be regained in some way or other.
One way might be a quest for an "earthly paradise"—a place like Shangri-La, hidden in the Tibetan mountains and described by James Hilton in his utopian novel "Lost Horizon" (1933). Christopher Columbus followed directly in this tradition in his belief that he had found the Garden of Eden when, towards the end of the 15th century, he first encountered the New World and its indigenous inhabitants.

</doc>
<doc id="31723" url="http://en.wikipedia.org/wiki?curid=31723" title="History of the United Kingdom">
History of the United Kingdom

The history of the United Kingdom as a unified sovereign state began in 1707 with the political union of the kingdoms of England and Scotland, into a united kingdom called Great Britain. On this new state the historian Simon Schama said "What began as a hostile merger would end in a full partnership in the most powerful going concern in the world... it was one of the most astonishing transformations in European history." A further Act of Union in 1800 added the Kingdom of Ireland to create the United Kingdom of Great Britain and Ireland.
The early years of the unified kingdom of Great Britain were marked by Jacobite risings which ended with defeat for the Stuart cause at Culloden in 1746. Later, in 1763, victory in the Seven Years War led to the dominance of the British Empire, which was to be the foremost global power for over a century and grew to become the largest empire in history. As a result, the culture of the United Kingdom, and its industrial, political, constitutional, educational and linguistic legacy, is widespread.
In 1922, following the Anglo-Irish Treaty, Ireland effectively seceded from the United Kingdom to become the Irish Free State; a day later, Northern Ireland seceded from the Free State and became part of the United Kingdom. As a result, in 1927 the United Kingdom changed its formal title to the "United Kingdom of Great Britain and Northern Ireland," usually shortened to the "United Kingdom", the "UK" or "Britain". Former parts of the British Empire became independent dominions.
In the Second World War, in which the Soviet Union, Nationalist China and the US joined Britain as allied powers, Britain and its Empire fought a successful war against Germany, Italy and Japan. The cost was high and Britain no longer had the wealth or the inclination to maintain an empire, so it granted independence to most of the Empire. The new states typically joined the Commonwealth of Nations. The United Kingdom has sought to be a leading member of the United Nations, the European Union and NATO. Since the 1990s, however, large-scale devolution movements in Northern Ireland, Scotland and Wales have brought into question the degree of unity of this constantly evolving political union.
18th century.
Birth of the Union.
The Kingdom of Great Britain came into being on 1 May 1707, as a result of the political union of the Kingdom of England (which included Wales) and the Kingdom of Scotland. The terms of the union had been negotiated the previous year, and laid out in the Treaty of Union. The parliaments of Scotland and of England then each ratified the treaty via respective Acts of Union.
Although politically separate states, England and Scotland had shared a monarch since 1603 when on the death of the childless Elizabeth I, James VI of Scotland became, additionally, James I of England, in an event known as the Union of the Crowns. Slightly more than one-hundred years later, the Treaty of Union enabled the two kingdoms to be combined into a single kingdom, merging the two parliaments into a single parliament of Great Britain. Queen Anne, who was reigning at the time of the union, had favoured deeper political integration between the two kingdoms and became the first monarch of Great Britain. The union was valuable to England's security because Scotland relinquished first, the right to choose a different monarch on Anne's death and second, the right to independently ally with a European power, which could then use Scotland as a base for the invasion of England.
Although now a single kingdom, certain aspects of the former independent kingdoms remained separate, as agreed in the terms in the Treaty of Union. Scottish and English law remained separate, as did the Presbyterian Church of Scotland and the Anglican Church of England. England and Scotland also continued to each have its own system of education.
The creation of Great Britain happened during the War of the Spanish Succession, in which just before his death in 1702 William III had reactivated the Grand Alliance against France. His successor, Anne, continued the war. The Duke of Marlborough won a series of brilliant victories over the French, England's first major battlefield successes on the Continent since the Hundred Years War. France was nearly brought to its knees by 1709, when King Louis XIV made a desperate appeal to the French people. Afterwards, his general Marshal Villars managed to turn the tide in favour of France. A more peace-minded government came to power in Great Britain, and the treaties of Utrecht and Rastadt in 1713–1714 ended the war.
Hanoverian kings.
Queen Anne died in 1714, and the Elector of Hanover, George Louis, became king as George I (1714–1727). He paid more attention to Hanover and surrounded himself with Germans, making him an unpopular king, However he did build up the army and created a more stable political system in Britain and helped bring peace to northern Europe. Jacobite factions seeking a Stuart restoration remained strong; they instigated a revolt in 1715–1716. The son of James II planned to invade England, but before he could do so, John Erskine, Earl of Mar, launched an invasion from Scotland, which was easily defeated.
George II (1727–1760) enhanced the stability of the constitutional system, with a government run by Sir Robert Walpole during the period 1730–42. He built up the first British Empire, strengthening the colonies in the Caribbean and North America. In coalition with the rising power Prussia, defeated France in the Seven Years' War (1756–1763), and won full control of Canada.
George III reigned 1760–1820; he was born in Britain, never visited Hanover, and spoke English as his first language. Frequently reviled by Americans as a tyrant and the instigator of the American War of Independence, he was insane off and on after 1788 as his eldest son served as regent. The last king to dominate government and politics, his long reign is noted for losing the first British Empire with a loss in the American Revolutionary War (1783), as France sought revenge for its defeat in the Seven Years War by aiding the Americans. The reign was notable for the building of a second empire based in India, Asia and Africa, the beginnings of the industrial revolution that made Britain an economic powerhouse, and above all the life and death struggle with the French, the French Revolutionary Wars 1793–1802, ending in a draw and a short truce, and the epic Napoleonic Wars (1803–1815), ending with the decisive defeat of Napoleon.
South Sea Bubble.
The era was prosperous as entrepreneurs extended the range of their business around the globe. The South Sea Bubble was a business enterprise that exploded in scandal. The South Sea Company was a private business corporation set up in London ostensibly to grant trade monopolies in South America. Its actual purpose was to re-negotiate previous high-interest government loans amounting to £31 million through market manipulation and speculation. It issued stock four times in 1720 that reached about 8,000 investors. Prices kept soaring every day, from £130 a share to £1,000, with insiders making huge paper profits. The Bubble collapsed overnight, ruining many speculators. Investigations showed bribes had reached into high places—even to the king. Robert Walpole managed to wind it down with minimal political and economic damage, although some losers fled to exile or committed suicide.
Warfare and finance.
From 1700 to 1850, Britain was involved in 137 wars or rebellions. It maintained a relatively large and expensive Royal Navy, along with a small standing army. When the need arose for soldiers it hired mercenaries or financed allies who fielded armies. The rising costs of warfare forced a shift in government financing from the income from royal agricultural estates and special imposts and taxes to reliance on customs and excise taxes and, after 1790, an income tax. Working with bankers in the City, the government raised large loans during wartime and paid them off in peacetime. The rise in taxes amounted to 20% of national income, but the private sector benefited from the increase in economic growth. The demand for war supplies stimulated the industrial sector, particularly naval supplies, munitions and textiles, which gave Britain an advantage in international trade during the postwar years.
The French Revolution polarized British political opinion in the 1790s, with conservatives outraged at killing of the king, the expulsion of the nobles, and the Reign of Terror. Britain was at war against France almost continuously from 1793 until the final defeat of Napoleon in 1815. Conservatives castigated every radical opinion in Britain as "Jacobin" (in reference to the leaders of the Terror), warning that radicalism threatened an upheaval of British society. The Anti-Jacobin sentiment, well expressed by Edmund Burke and many popular writers was strongest among the landed gentry and the upper classes.
British Empire.
The Seven Years' War, which began in 1756, was the first war waged on a global scale, fought in Europe, India, North America, the Caribbean, the Philippines and coastal Africa. The signing of the Treaty of Paris (1763) had important consequences for Britain and its empire. In North America, France's future as a colonial power there was effectively ended with the ceding of New France to Britain (leaving a sizeable French-speaking population under British control) and Louisiana to Spain. Spain ceded Florida to Britain. In India, the Carnatic War had left France still in control of its enclaves but with military restrictions and an obligation to support British client states, effectively leaving the future of India to Britain. The British victory over France in the Seven Years War therefore left Britain as the world's dominant colonial power.
During the 1760s and 1770s, relations between the Thirteen Colonies and Britain became increasingly strained, primarily because of opposition to Parliament's repeated attempts to tax American colonists without their consent. Disagreement turned to violence and in 1775 the American Revolutionary War began. In 1776 the Patriots expelled royal officials and declared the independence of the United States of America. After capturing a British invasion army in 1777, the US formed an alliance with France (and in turn Spain aided France), evening out the military balance. The British army controlled only a handful of coastal cities. 1780–81 was a low point for Britain. Taxes and deficits were high, government corruption was pervasive, and the war in America was entering its sixth year with no apparent end in sight. The Gordon Riots erupted in London during the spring of 1781, in response to increased concessions to Catholics by Parliament. In October 1781 Lord Cornwallis surrendered his army at Yorktown, Virginia. The Treaty of Paris was signed in 1783, formally terminating the war and recognising the independence of the United States.
The loss of the Thirteen Colonies, at the time Britain's most populous colonies, marked the transition between the "first" and "second" empires, in which Britain shifted its attention to Asia, the Pacific and later Africa. Adam Smith's "Wealth of Nations", published in 1776, had argued that colonies were redundant, and that free trade should replace the old mercantilist policies that had characterised the first period of colonial expansion, dating back to the protectionism of Spain and Portugal. The growth of trade between the newly independent United States and Britain after 1783 confirmed Smith's view that political control was not necessary for economic success.
During its first 100 years of operation, the focus of the British East India Company had been trade, not the building of an empire in India. Company interests turned from trade to territory during the 18th century as the Mughal Empire declined in power and the British East India Company struggled with its French counterpart, the "La Compagnie française des Indes orientales", during the Carnatic Wars of the 1740s and 1750s. The British, led by Robert Clive, defeated the French and their Indian allies in the Battle of Plassey, leaving the Company in control of Bengal and a major military and political power in India. In the following decades it gradually increased the size of the territories under its control, either ruling directly or indirectly via local puppet rulers under the threat of force of the Indian Army, 80% of which was composed of native Indian sepoys.
On 22 August 1770, James Cook discovered the eastern coast of Australia while on a scientific voyage to the South Pacific. In 1778, Joseph Banks, Cook's botanist on the voyage, presented evidence to the government on the suitability of Botany Bay for the establishment of a penal settlement, and in 1787 the first shipment of convicts set sail, arriving in 1788.
At the threshold to the 19th century, Britain was challenged again by France under Napoleon, in a struggle that, unlike previous wars, represented a contest of ideologies between the two nations.
The British government had somewhat mixed reactions to the outbreak of the French Revolution in 1789, and when war broke out on the Continent in 1792, it initially remained neutral. But the following January, Louis XVI was beheaded. This combined with a threatened invasion of the Netherlands by France spurred Britain to declare war. For the next 23 years, the two nations were at war except for a short period in 1802–1803. Britain alone among the nations of Europe never submitted to or formed an alliance with France. Throughout the 1790s, the British repeatedly defeated the navies of France and its allies, but were unable to perform any significant land operations. An Anglo-Russian invasion of the Netherlands in 1799 accomplished little except the capture of the Dutch fleet.
It was not only Britain's position on the world stage that was threatened: Napoleon threatened invasion of Britain itself, and with it, a fate similar to the countries of continental Europe that his armies had overrun.
1800 to 1837.
Union with Ireland.
On 1 January 1801, the Great Britain and Ireland joined to form the United Kingdom of Great Britain and Ireland.
Events that culminated in the union with Ireland had spanned several centuries. Invasions from England by the ruling Normans from 1170 led to centuries of strife in Ireland and successive Kings of England sought both to conquer and pillage Ireland, imposing their rule by force throughout the entire island. In the early 17th century, large-scale settlement by Protestant settlers from both Scotland and England began, especially in the province of Ulster, seeing the displacement of many of the native Roman Catholic Irish inhabitants of this part of Ireland. Since the time of the first Norman invaders from England, Ireland has been subject to control and regulation, firstly by England then latterly by Great Britain.
After the Irish Rebellion of 1641, Irish Roman Catholics were banned from voting or attending the Irish Parliament. The new English Protestant ruling class was known as the Protestant Ascendancy. Towards the end of the 18th century the entirely Protestant Irish Parliament attained a greater degree of independence from the British Parliament than it had previously held. Under the Penal Laws no Irish Catholic could sit in the Parliament of Ireland, even though some 90% of Ireland's population was native Irish Catholic when the first of these bans was introduced in 1691. This ban was followed by others in 1703 and 1709 as part of a comprehensive system disadvantaging the Catholic community, and to a lesser extent Protestant dissenters. In 1798, many members of this dissenter tradition made common cause with Catholics in a rebellion inspired and led by the Society of United Irishmen. It was staged with the aim of creating a fully independent Ireland as a state with a republican constitution. Despite assistance from France the Irish Rebellion of 1798 was put down by British forces.
Possibly influenced by the War of American Independence (1775–1783), a united force of Irish volunteers used their influence to campaign for greater independence for the Irish Parliament. This was granted in 1782, giving free trade and legislative independence to Ireland. However, the French revolution had encouraged the increasing calls for moderate constitutional reform. The Society of United Irishmen, made up of Presbyterians from Belfast and both Anglicans and Catholics in Dublin, campaigned for an end to British domination. Their leader Theobald Wolfe Tone (1763–98) worked with the Catholic Convention of 1792 which demanded an end to the penal laws. Failing to win the support of the British government, he travelled to Paris, encouraging a number of French naval forces to land in Ireland to help with the planned insurrections. These were slaughtered by government forces, but these rebellions convinced the British under Prime Minister William Pitt that the only solution was to end Irish independence once and for all.
The legislative union of Great Britain and Ireland was brought about by the Act of Union 1800, creating the "United Kingdom of Great Britain and Ireland". The Act was passed in both the Parliament of Great Britain and the Parliament of Ireland, dominated by the Protestant Ascendancy and lacking representation of the country's Roman Catholic population. Substantial majorities were achieved, and according to contemporary documents this was assisted by bribery in the form of the awarding of peerages and honours to opponents to gain their votes. Under the terms of the merger, the separate Parliaments of Great Britain and Ireland were abolished, and replaced by a united Parliament of the United Kingdom. Ireland thus became an integral part of the United Kingdom, sending around 100 MPs to the House of Commons at Westminster and 28 representative peers to the House of Lords, elected from among their number by the Irish peers themselves, except that Roman Catholic peers were not permitted to take their seats in the Lords. Part of the trade-off for the Irish Catholics was to be the granting of Catholic Emancipation, which had been fiercely resisted by the all-Anglican Irish Parliament. However, this was blocked by King George III, who argued that emancipating the Roman Catholics would breach his Coronation Oath. The Roman Catholic hierarchy had endorsed the Union. However the decision to block Catholic Emancipation fatally undermined the appeal of the Union.
Napoleonic wars.
During the War of the Second Coalition (1799–1801), Britain occupied most of the French and Dutch colonies (the Netherlands had been a satellite of France since 1796), but tropical diseases claimed the lives of over 40,000 troops. When the Treaty of Amiens ended the war, Britain was forced to return most of the colonies. The peace settlement was in effect only a ceasefire, and Napoleon continued to provoke the British by attempting a trade embargo on the country and by occupying the German city of Hanover (a fief of the British crown). In May 1803, war was declared again. Napoleon's plans to invade Britain failed due to the inferiority of his navy, and in 1805, Lord Nelson's fleet decisively defeated the French and Spanish at Trafalgar, which was the last significant naval action of the Napoleonic Wars.
The series of naval and colonial conflicts, including a large number of minor naval actions, resembled those of the French Revolutionary Wars and the preceding centuries of European warfare. Conflicts in the Caribbean, and in particular the seizure of colonial bases and islands throughout the wars, could potentially have some effect upon the European conflict. The Napoleonic conflict had reached the point at which subsequent historians could talk of a "world war". Only the Seven Years' War offered a precedent for widespread conflict on such a scale.
In 1806, Napoleon issued the series of Berlin Decrees, which brought into effect the Continental System. This policy aimed to weaken the British export economy closing French-controlled territory to its trade. The British army remained a minimal threat to France; the British standing army of just 220,000 at the height of the Napoleonic Wars hardly compared to France's army of a million men—in addition to the armies of numerous allies and several hundred thousand national guardsmen that Napoleon could draft into the military if necessary. Although the Royal Navy effectively disrupted France's extra-continental trade—both by seizing and threatening French shipping and by seizing French colonial possessions—it could do nothing about France's trade with the major continental economies and posed little threat to French territory in Europe. In addition, France's population and agricultural capacity far outstripped that of Britain.
Many in the French government believed that isolating Britain from the Continent would end its economic influence over Europe and isolate it. Though the French designed the Continental System to achieve this, it never succeeded in its objective. Britain possessed the greatest industrial capacity in Europe, and its mastery of the seas allowed it to build up considerable economic strength through trade to its possessions from its rapidly expanding new Empire. Britain's naval supremacy meant that France could never enjoy the peace necessary to consolidate its control over Europe, and it could threaten neither the home islands nor nor the main British colonies.
The Spanish uprising in 1808 at last permitted Britain to gain a foothold on the Continent. The Duke of Wellington and his army of British and Portuguese gradually pushed the French out of Spain and in early 1814, as Napoleon was being driven back in the east by the Prussians, Austrians, and Russians, Wellington invaded southern France. After Napoleon's surrender and exile to the island of Elba, peace appeared to have returned, but when he escaped back into France in 1815, the British and their allies had to fight him again. The armies of Wellington and Von Blucher defeated Napoleon once and for all at Waterloo.
Financing the war.
A key element in British success was its ability to mobilize the nation’s industrial and financial resources and apply them to defeating France. With a population of 16 million Britain was barely half the size of France with 30 million. In terms of soldiers the French numerical advantage was offset by British subsidies that paid for a large proportion of the Austrian and Russian soldiers, peaking at about 450,000 in 1813. Most important, the British national output remained strong and the well-organized business sector channeled products into what the military needed. The system of smuggling finished products into the continent undermined French efforts to ruin the British economy by cutting off markets. The British budget in 1814 reached £66 million, including £10 million for the Navy, £40 million for the Army, £10 million for the Allies, and £38 million as interest on the national debt. The national debt soared to £679 million, more than double the GDP. It was willingly supported by hundreds of thousands of investors and tax payers, despite the higher taxes on land and a new income tax. The whole cost of the war came to £831 million. By contrast the French financial system was inadequate and Napoleon’s forces had to rely in part on requisitions from conquered lands.
Napoleon also attempted economic warfare against Britain, especially in the Berlin Decree of 1806. It forbade the import of British goods into European countries allied with or dependent upon France, and installed the Continental System in Europe. All connections were to be cut, even the mail. British merchants smuggled in many goods and the Continental System was not a powerful weapon of economic war. There was some damage to Britain, especially in 1808 and 1811, but its control of the oceans helped ameliorate the damage. Even more damage was done to the economies of France and its allies, which lost a useful trading partner. Angry governments gained an incentive to ignore the Continental System, which led to the weakening of Napoleon's coalition.
War of 1812 with United States.
Simultaneous with the Napoleonic Wars, trade disputes and British impressment of American sailors led to the War of 1812 with the United States. The "second war of independence" for the American, it was little noticed in Britain, where all attention was focused on the struggle with France. The British could devote few resources to the conflict until the fall of Napoleon in 1814. American frigates also inflicted a series of embarrassing defeats on the British navy, which was short on manpower due to the conflict in Europe. A stepped-up war effort that year brought about some successes such as the burning of Washington, but many influential voices such as the Duke of Wellington argued that an outright victory over the US was impossible.
Peace was agreed to at the end of 1814, but Andrew Jackson, unaware of this, won a great victory over the British at the Battle of New Orleans in January 1815 (news took several weeks to cross the Atlantic before the advent of steam ships). Ratification of the Treaty of Ghent ended the war in February 1815. The major result was the permanent defeat of the Indian allies the British had counted upon. The US-Canadian border was demilitarised by both countries, and peaceful trade resumed, although worries of an American conquest of Canada persisted into the 1860s.
George IV and William IV.
Britain emerged from the Napoleonic Wars a very different country than it had been in 1793. As industrialisation progressed, society changed, becoming more urban and less rural. The postwar period saw an economic slump, and poor harvests and inflation caused widespread social unrest. Europe after 1815 was on guard against a return of Jacobinism, and even liberal Britain saw the passage of the Six Acts in 1819, which proscribed radical activities. By the end of the 1820s, along with a general economic recovery, many of these repressive laws were repealed and in 1828 new legislation guaranteed the civil rights of religious dissenters.
A weak ruler as regent (1811–20) and king (1820–30), George IV let his ministers take full charge of government affairs, playing a far lesser role than his father, George III. The principle now became established that the king accepts as prime minister the person who wins a majority in the House of Commons, whether the king personally favors him or not. His governments, with little help from the king, presided over victory in the Napoleonic Wars, negotiated the peace settlement, and attempted to deal with the social and economic malaise that followed. His brother William IV ruled (1830–37), but was little involved in politics. His reign saw several reforms: the poor law was updated, child labour restricted, slavery abolished in nearly all the British Empire, and, most important, the Reform Act 1832 refashioned the British electoral system.
There were no major wars until the Crimean War of 1853–56. While Prussia, Austria, and Russia, as absolute monarchies, tried to suppress liberalism wherever it might occur, the British came to terms with new ideas. Britain intervened in Portugal in 1826 to defend a constitutional government there and recognising the independence of Spain's American colonies in 1824. British merchants and financiers, and later railway builders, played major roles in the economies of most Latin American nations. The British intervened in 1827 on the side of the Greeks, who had been waging a war of independence against the Ottoman Empire since 1824.
Whig reforms of the 1830s.
The Whig Party recovered its strength and unity by supporting moral reforms, especially the reform of the electoral system, the abolition of slavery and emancipation of the Catholics. Catholic emancipation was secured in the Catholic Relief Act of 1829, which removed the most substantial restrictions on Roman Catholics in Britain.
The Whigs became champions of Parliamentary reform. They made Lord Grey prime minister 1830–1834, and the Reform Act of 1832 became their signature measure. It broadened the franchise and ended the system of "rotten borough" and "pocket boroughs" (where elections were controlled by powerful families), and instead redistributed power on the basis of population. It added 217,000 voters to an electorate of 435,000 in England and Wales. The main effect of the act was to weaken the power of the landed gentry, and enlarge the power of the professional and business middle-class, which now for the first time had a significant voice in Parliament. However, the great majority of manual workers, clerks, and farmers did not have enough property to qualify to vote. The aristocracy continued to dominate the government, the Army and Royal Navy, and high society. After parliamentary investigations demonstrated the horrors of child labour, limited reforms were passed in 1833.
Chartism emerged after the 1832 Reform Bill failed to give the vote to the working class. Activists denounced the 'betrayal' of the working class and the 'sacrificing' of their 'interests' by the 'misconduct' of the government. In 1838, Chartists issued the People's Charter demanding manhood suffrage, equal sized election districts, voting by ballots, payment of MPs (so poor men could serve), annual Parliaments, and abolition of property requirements. Elites saw the movement as pathological, so the Chartists were unable to force serious constitutional debate. Historians see Chartism as both a continuation of the 18th century fight against corruption and as a new stage in demands for democracy in an industrial society.
In 1832 Parliament abolished slavery in the Empire with the Slavery Abolition Act 1833. The government purchased the slaves for £20,000,000 (the money went to rich plantation owners who mostly lived in England), and freed the slaves, especially those in the Caribbean sugar islands.
Leadership.
Prime Ministers of the period included: William Pitt the Younger, Lord Grenville, Duke of Portland, Spencer Perceval, Lord Liverpool, George Canning, Lord Goderich, Duke of Wellington, Lord Grey, Lord Melbourne, and Sir Robert Peel.
Victorian era.
Victoria became queen in 1837 at age 18. Her long reign until 1901 saw Britain reach the zenith of its economic and political power. Exciting new technologies such as steam ships, railroads, photography, and telegraphs appeared, making the world much faster-paced. Britain again remained mostly inactive in Continental politics, and it was not affected by the wave of revolutions in 1848. The Victorian era saw the fleshing out of the second British Empire. Scholars debate whether the Victorian period—as defined by a variety of sensibilities and political concerns that have come to be associated with the Victorians—actually begins with her coronation or the earlier passage of the Reform Act 1832. The era was preceded by the Regency era and succeeded by the Edwardian period.
Historians like Bernard Porter have characterized the mid-Victorian era, (1850–1870) as Britain's 'Golden Years.'. There was peace and prosperity, as the national income per person grew by half. Much of the prosperity was due to the increasing industrialization, especially in textiles and machinery, as well as to the worldwide network of trade and engineering that produce profits for British merchants and experts from across the globe. There was peace abroad (apart from the short Crimean war, 1854–56), and social peace at home. Opposition to the new order melted away, says Porter. The Chartist movement, peaked as a democratic movement among the working class in 1848; its leaders moved to other pursuits, such as trade unions and cooperative societies. The working class ignored foreign agitators like Karl Marx in their midst, and joined in celebrating the new prosperity. Employers typically were paternalistic, and generally recognized the trade unions. Companies provided their employees with welfare services ranging from housing, schools and churches, to libraries, baths, and gymnasia. Middle-class reformers did their best to assist the working classes aspire to middle-class norms of 'respectability.'
There was a spirit of libertarianism, says Porter, as people felt they were free. Taxes were very low, and government restrictions were minimal. There were still problem areas, such as occasional riots, especially those motivated by anti-Catholicism. Society was still ruled by the aristocracy and the gentry, which controlled high government offices, both houses of Parliament, the church, and the military. Becoming a rich businessman was not as prestigious as inheriting a title and owning a landed estate. Literature was doing well, but the fine arts languished as the Great Exhibition of 1851 showcased Britain's industrial prowess rather than its sculpture, painting or music. The educational system was mediocre; the capstone universities (outside Scotland) were likewise mediocre.
Historian Llewellyn Woodward has concluded:
Free trade imperialism.
The Great London Exhibition of 1851 clearly demonstrated Britain's dominance in engineering and industry; that lasted until the rise of the United States and Germany in the 1890s. Using the imperial tools of free trade and financial investment, it exerted major influence on many countries outside Europe, especially in Latin America and Asia. Thus Britain had both a formal Empire based on British rule and an informal one based on the British pound.
Russia, France and the Ottoman Empire.
One nagging fear was the possible collapse of the Ottoman Empire. It was well understood that a collapse of that country would set off a scramble for its territory and possibly plunge Britain into war. To head that off Britain sought to keep the Russians from occupying Constantinople and taking over the Bosporous Straits, as well as from threatening India via Afghanistan. In 1853, Britain and France intervened in the Crimean War against Russia. Despite mediocre generalship, they managed to capture the Russian port of Sevastopol, compelling Tsar Nicholas I to ask for peace. A second Russo-Ottoman war in 1877 led to another European intervention, although this time at the negotiating table. The Congress of Berlin blocked Russia from imposing the harsh Treaty of San Stefano on the Ottoman Empire. Despite its alliance with the French in the Crimean War, Britain viewed the Second Empire of Napoleon III with some distrust, especially as the emperor constructed ironclad warships and began returning France to a more active foreign policy. But after Napoleon's downfall in the Franco-Prussian War in 1870, he was allowed to spent his last years exiled in Britain.
American Civil War.
During the American Civil War (1861–1865), British leaders personally disliked American republicanism and favoured the more aristocratic Confederacy, as it had been a major source of cotton for textile mills. Prince Albert was effective in defusing a war scare in late 1861. The British people, who depended heavily on American food imports, generally favoured the United States. What little cotton was available came from New York, as the blockade by the US Navy shut down 95% of Southern exports to Britain. In September 1862, during the Confederate invasion of Maryland, Britain (along with France) contemplated stepping in and negotiating a peace settlement, which could only mean war with the United States. But in the same month, US president Abraham Lincoln announced the Emancipation Proclamation. Since support of the Confederacy now meant support for slavery, there was no longer any possibility of European intervention.
Meanwhile the British sold arms to both sides, built blockade runners for a lucrative trade with the Confederacy, and surreptitiously allowed warships to be built for the Confederacy. The warships caused a major diplomatic row that was resolved in the Alabama Claims in 1872, in the Americans' favour.
Empire expands.
In 1867, Britain united most of its North American colonies as the Dominion of Canada, giving it self-government and responsibility for its own defence, but Canada did not have an independent foreign policy until 1931. Several of the colonies temporarily refused to join the Dominion despite pressure from both Canada and Britain; the last one, Newfoundland, held out until 1949.
The second half of the 19th century saw a huge expansion of Britain's colonial empire in Asia and Africa. In the latter continent, there was talk of the Union Jack flying from "Cairo to Cape Town", which only became a reality at the end of World War I. Having possessions on six continents, Britain had to defend all of its empire with a volunteer army, for it was the only power in Europe to have no conscription. Some questioned whether the country was overstretched.
The rise of the German Empire since 1871 posed a new challenge, for it (along with the United States) threatened to take Britain's place as the world's foremost industrial power. Germany acquired a number of colonies in Africa and the Pacific, but Chancellor Otto von Bismarck succeeded in achieving general peace through his balance of power strategy. When William II became emperor in 1888, he discarded Bismarck, began using bellicose language, and planned to build a navy to rival Britain's.
Ever since Britain had taken control of South Africa from the Netherlands in the Napoleonic Wars, it had run afoul of the Dutch settlers who further away and created two republics of their own. The British imperial vision called for control over the new countries and the Dutch-speaking "Boers" (or "Afrikaners") fought back in the War in 1899–1902. Outgunned by a mighty empire, the Boers waged a guerilla war, which gave the British regulars a difficult fight, but weight of numbers, superior equipment, and often brutal tactics eventually brought about a British victory. The war had been costly in human rights and was widely criticised by Liberals in Britain and worldwide. However, the United States gave its support. The Boer republics were merged into Union of South Africa in 1910; it had internal self-government but its foreign policy was controlled by London and was an integral part of the British Empire.
Free trade imperialism.
Britain in addition to taking control of new territories, developed an enormous power in economic and financial affairs in numerous independent countries, especially in Latin America and Asia. It lent money, built railways, and engaged in trade. The Great London Exhibition of 1851 clearly demonstrated Britain's dominance in engineering, communications and industry; that lasted until the rise of the United States and Germany in the 1890s.
In 1890–1902 under Salisbury Britain promoted a policy of Splendid isolation with no formal allies.
Ireland and the move to Home Rule.
Part of the agreement which led to the 1800 Act of Union stipulated that the Penal Laws in Ireland were to be repealed and Catholic Emancipation granted. However King George III blocked emancipation, arguing that to grant it would break his coronation oath to defend the Anglican Church. A campaign under lawyer and politician Daniel O'Connell, and the death of George III, led to the concession of Catholic Emancipation in 1829, allowing Catholics to sit in Parliament. O'Connell then mounted an unsuccessful campaign for the Repeal of the Act of Union.
When potato blight hit the island in 1846, much of the rural population was left without food because cash crops were being exported to pay rents. British politicians such as the Prime Minister Robert Peel were at this time wedded to the economic policy of laissez-faire, which argued against state intervention of any sort. While enormous sums were raised by private individuals and charities (American Indians sent supplies, while Queen Victoria personally gave the present-day equivalent €70,000), lack of adequate action let the problem become a catastrophe. The class of cottiers or farm labourers was virtually wiped out in what became known in Britain as 'The Irish Potato Famine' and in Ireland as the Great Hunger.
Most Irish people elected as their MPs Liberals and Conservatives who belonged to the main British political parties (note: the poor didn't have a vote at that time). A significant minority also elected Unionists, who championed the cause of the maintenance of the Act of Union. A former Tory barrister turned nationalist campaigner, Isaac Butt, established a new moderate nationalist movement, the Home Rule League, in the 1870s. After Butt's death the Home Rule Movement, or the Irish Parliamentary Party as it had become known, was turned into a major political force under the guidance of William Shaw and in particular a radical young Protestant landowner, Charles Stewart Parnell. The Irish Parliamentary Party dominated Irish politics, to the exclusion of the previous Liberal, Conservative and Unionist parties that had existed. Parnell's movement proved to be a broad church, from conservative landowners to the Land League which was campaigning for fundamental reform of Irish landholding, where most farms were held on rental from large aristocratic estates.
Parnell's movement campaigned for 'Home Rule', by which they meant that Ireland would govern itself as a region within the United Kingdom, in contrast to O'Connell who wanted complete independence subject to a shared monarch and Crown. Two Home Rule Bills (1886 and 1893) were introduced by Liberal Prime Minister Ewart Gladstone, but neither became law, mainly due to opposition from the House of Lords. The issue divided Ireland, for a significant unionist minority (largely though by no means exclusively based in Ulster), opposed Home Rule, fearing that a Catholic-Nationalist parliament in Dublin would discriminate against them and would also impose tariffs on industry; while most of Ireland was primarily agricultural, six counties in Ulster were the location of heavy industry and would be affected by any tariff barriers imposed.
Leadership.
Prime Ministers of the period included: Lord Melbourne, Sir Robert Peel, Lord John Russell, Lord Derby, Lord Aberdeen, Lord Palmerston, Benjamin Disraeli, William Ewart Gladstone, Lord Salisbury, and Lord Rosebery.
Queen Victoria.
The Queen gave her name to an era of British greatness, especially in the far-flung British Empire with which she identified. She played a small role in politics, but became the iconic symbol of the nation, the empire, and proper, restrained behaviour. Her strength lay in good common sense and directness of character; she expressed the qualities of the British nation which at that time made it preeminent in the world. As a symbol of domesticity, endurance and Empire, and as a woman holding the highest public office during an age when middle- and upper-class women were expected to beautify the home while men dominated the public sphere, Queen Victoria's influence has been enduring. Her success as ruler was due to the power of the self-images she successively portrayed of innocent young woman, devoted wife and mother, suffering and patient widow, and grandmotherly matriarch.
Disraeli.
Disraeli and Gladstone dominated the politics of the late 19th century, Britain's golden age of parliamentary government. They long were idolized, but historians in recent decades have become much more critical, especially regarding Disraeli.
Benjamin Disraeli (1804–1881), prime minister 1868 and 1874–80, remains an iconic hero of the Conservative Party. He was typical of the generation of British leaders who matured in the 1830s and 1840s. He was concerned with threats to established political, social, and religious values and elites; he emphasized the need for national leadership in response to radicalism, uncertainty, and materialism. He is especially known for his enthusiastic support for expanding and strengthening the British Empire in India and Africa as the foundation of British greatness, in contrast to Gladstone's negative attitude toward imperialism. Gladstone denounced Disraeli's policies of territorial aggrandizement, military pomp, and imperial symbolism (such as making the Queen Empress of India), saying it did not fit a modern commercial and Christian nation. Disraeli drummed up support by warnings of a supposed Russian threat to India that sank deep into the Conservative mindset. Disraeli's old reputation as the "Tory democrat" and promoter of the welfare state fell away as historians showed he that Disraeli had few proposals for social legislation in 1874–80, and that the 1867 Reform Act did not reflect a vision Conservatism for the unenfranchised working man. However he did work to reduce class antagonism, for as Perry notes, "When confronted with specific problems, he sought to reduce tension between town and country, landlords and farmers, capital and labour, and warring religious sects in Britain and Ireland—in other words, to create a unifying synthesis."
Gladstone.
William Ewart Gladstone (1809–1898) was the Liberal counterpart to Disraeli, serving as prime minister four times (1868–74, 1880–85, 1886, and 1892–94). His financial policies, based on the notion of balanced budgets, low taxes and laissez-faire, were suited to a developing capitalist society but could not respond effectively as economic and social conditions changed. Called the "Grand Old Man" later in life, he was always a dynamic popular orator who appealed strongly to British workers and lower middle class. The deeply religious Gladstone brought a new moral tone to politics with his evangelical sensibility and opposition to aristocracy. His moralism often angered his upper-class opponents (including Queen Victoria, who strongly favoured Disraeli), and his heavy-handed control split the Liberal party. His foreign policy goal was to create a European order based on cooperation rather than conflict and mutual trust instead of rivalry and suspicion; the rule of law was to supplant the reign of force and self-interest. This Gladstonian concept of a harmonious Concert of Europe was opposed to and ultimately defeated by the Germans with a Bismarckian system of manipulated alliances and antagonisms.
Historian Walter L. Arnstein, concludes:
Salisbury.
Historians portray Conservative prime Minister Lord Salisbury (1830–1903) as a talented leader who was an icon of traditional, aristocratic conservatism. Robert Blake considers Salisbury "a great foreign minister, [but] essentially negative, indeed reactionary in home affairs". Professor P.T. Marsh’s estimate is more favourable than Blake's, he portrays Salisbury as a leader who "held back the popular tide for twenty years." Professor Paul Smith argues that, "into the ‘progressive’ strain of modern Conservatism he simply will not fit." Professor H.C.G. Matthew points to "the narrow cynicism of Salisbury". One admirer of Salisbury, Maurice Cowling largely agrees with the critics and says Salisbury found the democracy born of the 1867 and 1884 Reform Acts as "perhaps less objectionable than he had expected—succeeding, through his public persona, in mitigating some part of its nastiness."
Morality.
The Victorian era is famous for the Victorian standards of personal morality. Historians generally agree that the middle classes held high personal moral standards (and usually followed them), but have debated whether the working classes followed suit. Moralists in the late 19th century such as Henry Mayhew decried the slums for their supposed high levels of cohabitation without marriage and illegitimate births. However new research using computerized matching of data files shows that the rates of cohabitation were quote low—under 5%—for the working class and the poor. By contrast in 21st century Britain, nearly half of all children are born outside marriage, and nine in ten newlyweds have been cohabitating.
Early 20th century.
Prime Ministers from 1900 to 1945: Marquess of Salisbury, Arthur Balfour, Sir Henry Campbell-Bannerman, Herbert Henry Asquith, David Lloyd George, Andrew Bonar Law, Stanley Baldwin, Ramsay MacDonald, Stanley Baldwin, Ramsay MacDonald, Stanley Baldwin, Neville Chamberlain and Winston Churchill.
Edwardian era 1901–1914.
Queen Victoria died in 1901 and her son Edward VII became king, inaugurating the Edwardian Era, which was characterised by great and ostentatious displays of wealth in contrast to the sombre Victorian Era. With the advent of the 20th century, things such as motion pictures, automobiles, and aeroplanes were coming into use. The new century was characterised by a feeling of great optimism. The social reforms of the last century continued into the 20th with the Labour Party being formed in 1900. Edward died in 1910, to be succeeded by George V, who reigned 1910–36. Scandal-free, hard working and popular, George V was the British monarch who, with Queen Mary, established the modern pattern of exemplary conduct for British royalty, based on middle-class values and virtues. He understood the overseas Empire better than any of his prime ministers and used his exceptional memory for figures and details, whether of uniforms, politics, or relations, to good effect in reaching out in conversation with his subjects.
The era was prosperous but political crises were escalating out of control. Dangerfield (1935) identified the "strange death of liberal England" as the multiple crisis that hit simultaneously in 1910–1914 with serious social and political instability arising from the Irish crisis, labor unrest, the women's suffrage movements, and partisan and constitutional struggles in Parliament. At one point it even seemed the Army might refuse orders dealing with Northern Ireland. No solution appeared in sight when the unexpected outbreak of the Great War in 1914 put domestic issues on hold.
McKibben argues that the political party system of the Edwardian era was in delicate balance on the eve of the war in 1914. The Liberals were in power with a progressive alliance of Labour and, off and on, Irish Nationalists. The coalition was committed to free trade (as opposed to the high tariffs the Conservatives sought), free collective bargaining for trades unions (which Conservatives opposed), an active social policy that was forging the welfare state, and constitutional reform to reduce the power of the House of Lords. The coalition lacked a long-term plan, because it was cobbled together from leftovers from the 1890s. The sociological basis was non-Anglican religion and non-English ethnicity rather than the emerging class conflict emphasized by Labour.
World War I.
Britain entered the war because of its implicit support for France, which had entered to support Russia, which in turn had entered to support Serbia. Even more important than that chain of links was Britain's determination to honor its commitment to defend Belgium. Britain was loosely part of the Triple Entente with France and Russia, which (with smaller allies) fought the Central Powers of Germany, Austria and the Ottoman Empire. After a few weeks the Western Front turned into a killing ground in which millions of men died but no army made a large advance.
The stalemate required an endless supply of men and munitions. By 1916, volunteering fell off, the government imposed conscription in Britain (but not in Ireland) to keep up the strength of the Army. After a rough start in industrial mobilisation, Britain replaced prime minister Asquith in December 1916 with the much more dynamic Liberal leader David Lloyd George. The nation now successfully mobilised its manpower, womanpower, industry, finances, Empire and diplomacy, in league with France and the U.S. to defeat the enemy. After defeating Russia, the Germans tried to win in the spring of 1918 before the millions of American soldiers arrived. They failed, and they were overwhelmed and finally accepted an Armistice in November 1918, that amounted to a surrender.
Britain eagerly supported the war, but in Ireland the Catholics were restless and plotted a rebellion in 1916. It failed but the brutal repression that followed turned that element against Britain. The economy grew about 14% from 1914 to 1918 despite the absence of so many men in the services; by contrast the German economy shrank 27%. The War saw a decline of civilian consumption, with a major reallocation to munitions. The government share of GDP soared from 8% in 1913 to 38% in 1918 (compared to 50% in 1943). The war forced Britain to use up its financial reserves and borrow large sums from New York banks. After the U.S. entered in April 1917, the Treasury borrowed directly from the U.S. government.
The Royal Navy dominated the seas, defeating the smaller German fleet in the only major naval battle of the war, the Battle of Jutland in 1916. Germany was blockaded, leading to an increasing shortage short of food. Germany's naval strategy increasingly turned towards use of U-Boats to strike back against the British, despite the risk of triggering war with the powerful neutral power, the United States. The waters around Britain were declared a war zone where any ship, neutral or otherwise, was a target. After the liner Lusitania was sunk in May 1915, drowning over 100 American passengers, protests by the United States led Germany to abandon unrestricted submarine warfare. With victory over Russia in 1917, Germany now calculated it could finally have numerical superiority on the Western Front. Planning for a massive spring offensive in 1918, it resumed the sinking of all merchant ships without warning. The US entered the war alongside the Allies (without actually joining them), and provided the needed money and supplies to sustain the Allies' war efforts. The U-boat threat was ultimately defeated by a convoy system across the Atlantic.
On other fronts, the British, French, Australians, and Japanese seized Germany's colonies. Britain fought the Ottoman Empire, suffering defeats in the Gallipoli Campaign) and in Mesopotamia, while arousing the Arabs who helped expel the Turks from their lands. Exhaustion and war-weariness were growing worse in 1917, as the fighting in France continued with no end in sight. The German spring offensives of 1918 failed, and with the arrival of the American in summer at the rate of 10,000 a day the Germans realized they were being overwhelmed. Germany agreed to an armistice—actually a surrender—on 11 November 1918.
Victorian attitudes and ideals that had continued into the first years of the 20th century changed during World War I. The army had traditionally never been a large employer in the nation, with the regular army standing at 247,432 at the start of the war. By 1918, there were about five million people in the army and the fledgling Royal Air Force, newly formed from the Royal Naval Air Service (RNAS) and the Royal Flying Corps (RFC), was about the same size of the pre-war army. The almost three million casualties were known as the "lost generation," and such numbers inevitably left society scarred; but even so, some people felt their sacrifice was little regarded in Britain, with poems like Siegfried Sassoon's "Blighters" criticising the ill-informed jingoism of the home front.
Postwar settlement.
The war had been won by Britain and its allies, but at a terrible human and financial cost, creating a sentiment that wars should never be fought again. The League of Nations was founded with the idea that nations could resolve their differences peacefully, but these hopes were unfulfilled. The harsh peace settlement imposed on Germany would leave it embittered and seeking revenge.
At the Paris Peace Conference of 1919, Lloyd George, American President Woodrow Wilson and French premier Georges Clemenceau made all the major decisions. They formed the League of Nations as a mechanism to prevent future wars. They sliced up the losers to form new nations in Europe, and divided up the German colonies and Ottoman holdings outside Turkey. They imposed what appeared to be heavy financial reparations (but with in the event were of modest size). They humiliated Germany by forcing it to declare its guilt for starting the war, a policy that caused deep resentment in Germany and helped fuel reactions such as Nazism. Britain gained the German colony of Tanganyika and part of Togoland in Africa, while its dominions added other colonies. Britain gained League of Nations mandates over Palestine, which had been partly promised as a homeland for Jewish settlers, and Iraq. Iraq became fully independent in 1932. Egypt, which had been a British protectorate since 1882, became independent in 1922, although the British remained there until 1952.
Irish independence and partition.
In 1912, the House of Lords managed to delay a Home Rule bill passed by the House of Commons. It was enacted as the Government of Ireland Act 1914. During these two years the threat of religious civil war hung over Ireland with the creation of the Unionist Ulster Volunteers opposed to the Act and their nationalist counterparts, the Irish Volunteers supporting the Act. The outbreak of World War I in 1914 put the crisis on political hold. A disorganized Easter Rising in 1916 was brutally suppressed by the British, which had the effect of galvanizing Catholic demands for independence. Prime Minister David Lloyd George failed to introduce Home Rule in 1918 and in the December 1918 General Election Sinn Féin won a majority of Irish seats. Its MPs refused to take their seats at Westminster, instead choosing to sit in the First Dáil parliament in Dublin. A declaration of independence was ratified by Dáil Éireann, the self-declared Republic's parliament in January 1919. An Anglo-Irish War was fought between Crown forces and the Irish Republican Army between January 1919 and June 1921. The war ended with the Anglo-Irish Treaty of December 1921 that established the Irish Free State. Six northern, predominantly Protestant counties became Northern Ireland and have remained part of Britain ever since, despite demands of the Catholic minority to unite with the Republic of Ireland. Britain officially adopted the name "United Kingdom of Great Britain and Northern Ireland" by the Royal and Parliamentary Titles Act 1927. In 1948 a working party chaired by the Cabinet Secretary recommended that the country's name be changed to the "United Kingdom of Great Britain and Ulster". However, after considering the potential problems this could cause with other Commonwealth countries especially with respect to changes to the King's title, the prime minister did not favour the change and it was not made.
Interwar era.
Historian Arthur Marwick sees a radical transformation of British society resulting from the Great War, a deluge that swept away many old attitudes and brought in a more equalitarian society. He sees the famous literary pessimism of the 1920s as misplaced, arguing there were major positive long-term consequences of the war to British society. He points to an energized self-consciousness among workers that quickly built up the Labour Party, the coming of partial woman suffrage, and an acceleration of social reform and state control of the economy. He sees a decline of deference toward the aristocracy and established authority in general, and the weakening among youth of traditional restraints on individual moral behavior. The chaperone faded away; village druggists sold contraceptives. Marwick says that class distinctions softened, national cohesion increased, and British society became more equal.
Politics and economics of 1920s.
Expanding the welfare state.
Two major programs that permanently expanded the welfare state passed in 1919 and 1920 with surprisingly little debate, even as the Conservatives dominated parliament. The Housing and Town Planning Act of 1919 set up a system of government housing that followed the 1918 campaign promises of “homes fit for heroes.” The Addison Act, named after the first Minister of Health Doctor Christopher Addison, required local authorities to survey their housing needs, and start building houses to replace slums. The treasury subsidized the low rents. In England and Wales, 214,000 houses were built, and the Ministry of Health became largely a ministry of housing.
The Unemployment Insurance Act of 1920 passed at a time of very little unemployment. It set up the dole system that provided 39 weeks of unemployment benefits to practically the entire civilian working population except domestic service, farm workers, and civil servants. Funded in part by weekly contributions from both employers and employed, it provided weekly payments of 15s for unemployed men and 12s for unemployed women. Historian Charles Mowat calls these two laws "Socialism by the back door," and notes how surprised politicians were when the costs to the Treasury soared during the high unemployment of 1921.
Conservative control.
The Lloyd-George coalition fell apart in 1922. Stanley Baldwin, as leader of the Conservative Party (1923–37) and as Prime Minister (in 1923–24, 1924–29 and 1935–37), dominated British politics. His mixture of strong social reforms and steady government proved a powerful election combination, with the result that the Conservatives governed Britain either by themselves or as the leading component of the National Government. He was the last party leader to win over 50% of the vote (in the general election of 1931). Baldwin's political strategy was to polarize the electorate so that voters would choose between the Conservatives on the right and the Labour Party on the left, squeezing out the Liberals in the middle. The polarization did take place and while the Liberals remained active under Lloyd George, they won few seats and were a minor factor until they joined a coalition with the Conservatives in 2010. Baldwin's reputation soared in the 1920s and 1930s, but crashed after 1945 as he was blamed for the appeasement policies toward Germany, and as admirers of Churchill made him the Conservative icon. Since the 1970s Baldwin's reputation has recovered somewhat.
Labour won the 1923 election, but in 1924 Baldwin and the Conservatives returned with a large majority.
McKibbin finds that the political culture of the interwar period was built around an anti-socialist middle class, supported by the Conservative leaders, especially Baldwin.
Economics.
Taxes rose sharply during the war and never returned to their old levels. A rich man paid 8% of his income in taxes before the war, and about a third afterwards. Much of the money went for the dole, the weekly unemployment benefits. About 5% of the national income every year was transferred from the rich to the poor. Taylor argues most people "were enjoying a richer life than any previously known in the history of the world: longer holidays, shorter hours, higher real wages."
The British economy was lackluster in the 1920s, with sharp declines and high unemployment in heavy industry and coal, especially in Scotland and Wales. Exports of coal and steel fell in half by 1939 and the business community was slow to adopt the new labour and management principles coming from the US, such as Fordism, consumer credit, eliminating surplus capacity, designing a more structured management, and using greater economies of scale. For over a century the shipping industry had dominated world trade, but it remained in the doldrums despite various stimulus efforts by the government. With the very sharp decline in world trade after 1929, its condition became critical.
Chancellor of the Exchequer Winston Churchill put Britain back on the gold standard in 1925, which many economists blame for the mediocre performance of the economy. Others point to a variety of factors, including the inflationary effects of the World War and supply-side shocks caused by reduced working hours after the war.
By the late 1920s, economic performance had stabilised, but the overall situation was disappointing, for Britain had fallen behind the United States as the leading industrial power. There also remained a strong economic divide between the north and south of England during this period, with the south of England and the Midlands fairly prosperous by the Thirties, while parts of south Wales and the industrial north of England became known as "distressed areas" due to particularly high rates of unemployment and poverty. Despite this, the standard of living continued to improve as local councils built new houses to let to families rehoused from outdated slums, with up to date facilities including indoor toilets, bathrooms and electric lighting now being included in the new properties. The private sector enjoyed a housebuilding boom during the 1930s.
Labour.
During the war trade unions were encouraged and their membership grew from 4.1 million in 1914 to 6.5 million in 1918. They peaked at 8.3 million in 1920 before relapsing to 5.4 million in 1923.
Coal was a sick industry; the best seams were being exhausted, raising the cost. Demand fell as oil began replacing coal for fuel. The 1926 general strike was a nine-day nationwide walkout of 1.3 million railwaymen, transport workers, printers, dockers, iron workers and steelworkers supporting the 1.2 million coal miners who had been locked out by the owners. The miners had rejected the owners' demands for longer hours and reduced pay in the face of falling prices. The Conservative government had provided a nine-month subsidy in 1925 but that was not enough to turn around a sick industry. To support the miners the Trades Union Congress (TUC), an umbrella organization of all trades unions, called out certain critical unions. The hope was the government would intervene to reorganize and rationalize the industry, and raise the subsidy. The Conservative government had stockpiled supplies and essential services continued with middle class volunteers. All three major parties opposed the strike. The Labour Party leaders did not approve and feared it would tar the party with the image of radicalism, for the Cominterm in Moscow had sent instructions for Communists to aggressively promote the strike. The general strike itself was largely non-violent, but the miners' lockout continued and there was violence in Scotland. It was the only general strike in British history, for TUC leaders such as Ernest Bevin considered it a mistake . Most historians treat it as a singular event with few long-term consequences, but Pugh says it accelerated the movement of working-class voters to the Labour Party, which led to future gains. The Trade Disputes and Trade Unions Act 1927 made general strikes illegal and ended the automatic payment of union members to the Labour Party. That act was largely repealed in 1946. The coal industry, used up the more accessible coal as costs rose output fell from 2567 million tons in 1924 to 183 million in 1945. The Labour government nationalised the mines in 1947.
Great Depression.
The Great Depression originated in the United States in late 1929 and quickly spread to the world. Britain had never experienced the boom that had characterized the US, Germany, Canada and Australia in the 1920s, so its bust appeared less severe. Britain's world trade fell in half (1929–33), the output of heavy industry fell by a third, employment profits plunged in nearly all sectors. At the depth in summer 1932, registered unemployed numbered 3.5 million, and many more had only part-time employment. Experts tried to remain optimistic. John Maynard Keynes, who had not predicted the slump, said, "'There will be no serious direct consequences in London. We find the look ahead decidedly encouraging."
Doomsayers on the left such as Sidney and Beatrice Webb, J.A. Hobson, and G.D.H. Cole repeated the dire warnings they had been making for years about the imminent death of capitalism, only now far more people paid attention. Starting in 1935 the Left Book Club provided a new warning every month, and built up the credibility of Soviet-style socialism as an alternative.
Particularly hardest hit by economic problems were the north of England, Scotland, Northern Ireland and Wales; unemployment reached 70% in some areas at the start of the 1930s (with more than 3 million out of work nationally) and many families depended entirely on payments from local government known as the dole.
In 1936, by which time unemployment was lower, 200 unemployed men made a highly publicized march from Jarrow to London in a bid to show the plight of the industrial poor. Although much romanticized by the Left, the Jarrow Crusade marked a deep split in the Labour Party and resulted in no government action. Unemployment remained high until the war absorbed all the job seekers. George Orwell's book "The Road to Wigan Pier" gives a bleak overview of the hardships of the time.
Appeasement.
Vivid memories of the horrors and deaths of the World War made Britain and its leaders strongly inclined to pacifism in the interwar era. The challenge came from dictators, first Benito Mussolini of Italy, then Adolf Hitler of a much more powerful Nazi Germany. The League of Nations proved disappointing to its supporters; it was unable to resolve any of the threats posed by the dictators. British policy was to "appease" them in the hopes they would be satiated. By 1938 it was clear that war was looming, and that Germany had the world's most powerful military. The final act of appeasement came when Britain and France sacrificed Czechoslovakia to Hitler's demands at the Munich Agreement of 1938. Instead of satiation Hitler menaced Poland, and at last Prime Minister Neville Chamberlain dropped appeasement and stood firm in promising to defend Poland. Hitler however cut a deal with Joseph Stalin to divide Eastern Europe; when Germany did invade Poland in September 1939, Britain and France declared war; the British Commonwealth followed London's lead.
Second World War.
Britain, along with the dominions and the rest of the Empire, declared war on Nazi Germany in 1939, after the German invasion of Poland. After a quiet period of "phoney war", the French and British armies collapsed under German onslaught in spring 1940. The British with the thinnest of margins rescued its main army from Dunkirk (as well as many French soldiers), leaving all their equipment and war supplies behind. Winston Churchill came to power, promising to fight the Germans to the very end. The Germans threatened an invasion—which the Royal Navy was prepared to repel. First the Germans tried to achieve air supremacy but were defeated by the Royal Air Force in the Battle of Britain in late summer 1940. Japan declared war in December 1941, and quickly seized Hong Kong, Malaya, Singapore, and Burma, and threatened Australia and India. Britain formed an alliance with the Soviet Union (starting in 1941) and very close ties to the United States (starting in 1940). The war was very expensive. It was paid for by high taxes, by selling off assets, and by accepting large amounts of Lend Lease from the U.S. and Canada. The US gave $40 billion in munitions; Canada also gave aid. (The American and Canadian aid did not have to be repaid, but there were also American loans that were repaid.)
Welfare state.
Welfare conditions, especially regarding food, improved during the war as the government imposed rationing and subsidized food prices. Conditions for housing worsened of course with the bombing, and clothing was in short supply.
A common theme called for an expansion of the welfare state as a reward to the people for their wartime sacrifices The goal was operationalized in a famous report by William Beveridge It recommended that the various income maintenance services that a grown-up piecemeal since 1911 be systematized and made universal. Unemployment benefits and sickness benefits were to be universal. There would be new benefits for maternity. The old-age pension system would be revised and expanded, and require that a person retired. A full-scale National Health Service would provide free medical care for everyone. All the major parties endorsed the principles and they were largely put into effect when peace returned.
Royals.
The media called it a "people's war"—a term that caught on and signified the popular demand for planning and an expanded welfare state. The Royal family played major symbolic roles in the war. They refused to leave London during the Blitz and were indefatigable in visiting troops, munition factories, dockyards, and hospitals all over the country. Princess Elizabeth joined the Auxiliary Territorial Service (ATS—a part of the army) and repaired trucks and jeeps. All social classes appreciated how the royals shared the hopes, fears and hardships of the people.
Memory.
The themes of equality and sacrifice were dominant both during the war, and in the memory of the war. There was little antiwar sentiment during or after the war. Furthermore Britain turned more toward the collective welfare state during the war, expanding it in the late 1940s and reaching a broad consensus supporting it across party lines. By the 1970s and 1980s, however, historians were exploring the subtle elements of continuing diversity and conflict in society during the war period. For example, at first historians emphasized that strikes became illegal in July 1940, and no trade union called one during the war. Later historians pointed to the many many localized unofficial strikes, especially in coal mining, shipbuilding, the metal trades, and engineering, with as many as 3.7 million man days lost in 1944.
Postwar.
Britain was a winner in the war, but it lost India in 1947 and nearly all the rest of the Empire by 1960. It debated its role in world affairs and joined the United Nations in 1945, NATO in 1949, where it became a close ally of the United States. After a long debate and initial rejection, it joined the European Union in 1973. Prosperity returned in the 1950s and London remained a world center of finance and culture, but the nation was no longer a major world power.
Austerity, 1945–1950.
The end of the war saw a landslide victory for Clement Attlee and the Labour Party. They were elected on a manifesto of greater social justice with left wing policies such as the creation of a National Health Service, an expansion of the provision of council housing and nationalisation of the major industries. Britain faced severe financial crises, and responded by reducing her international responsibilities and by sharing the hardships of an "age of austerity." Large loans from the United States and Marshall Plan grants helped rebuild and modernize its infrastructure and business practices. Rationing and conscription dragged on into the post war years, and the country suffered one of the worst winters on record. Nevertheless, morale was boosted by events such as the marriage of Princess Elizabeth in 1947 and the Festival of Britain.
Nationalisation.
Labour Party experts went into the files to find the detailed plans for nationalisation that had been developed. To their surprise, there were no plans. The leaders realized they had to act fast to keep up the momentum of the 1945 electoral landslide. They started with the Bank of England, civil aviation, coal, and cables and wireless. Then came railways, canals, road haulage and trucking, electricity, and gas. Finally came iron and steel, which was a special case because it was a manufacturing industry. Altogether, about one fifth of the economy had been nationalised. Labour dropped its plans to nationalise farmlands. The procedure used was developed by Herbert Morrison, who as Lord President chaired the Committee on the Socialization of Industries. He followed the model that was already in place of setting up public corporations such as the BBC in broadcasting (1927). As the owners of corporate stock were given government bonds, and the government took full ownership of each affected company, consolidating it into a national monopoly. The management remained the same, only now they became civil servants working for the government. For the Labour Party leadership, nationalisation was a method to consolidate economic planning in their own hands. It was not designed to modernise old industries, make them efficient, or transform their organisational structure. There was no money for modernisation, although the Marshall Plan, operated separately by American planners, did force many British businesses to adopt modern managerial techniques. Old line socialists were disappointed, as the nationalised industries seemed identical to the old private corporations, and national planning was made virtually impossible by the government’s financial constraints. Socialism was in place, but it did not seem to make a major difference. Rank-and-file workers had long been motivated to support Labour by tales of the mistreatment of workers by foremen and the management. The foremen and the managers were the same men as before with much the same power over the workplace. There was no worker control of industry. The unions resisted government efforts to set wages. By the time of the general elections in 1950 and 1951, Labour seldom boasted about nationalisation of industry. Instead it was the Conservatives who decried the inefficiency and mismanagement, and promised to reverse the takeover of steel and trucking.
Prosperity of 1950s.
As the country headed into the 1950s, rebuilding continued and a number of immigrants from the remaining British Empire, mostly the Caribbean and the Indian subcontinent, were invited to help the rebuilding effort. As the 1950s wore on, Britain lost its place as a superpower and could no longer maintain its large Empire. This led to decolonisation, and a withdrawal from almost all of its colonies by 1970. Events such as the Suez Crisis showed that the UK's status had fallen in the world. The 1950s and 1960s were, however, relatively prosperous times after the Second World War, and saw the beginning of a modernisation of the UK, with the construction of its first motorways for example, and also during the 1960s a great cultural movement began which expanded across the world. Unemployment was relatively low during this period and the standard of living continued to rise with more new private and council housing developments taking place and the number of slum properties diminishing.
The postwar period also witnessed a dramatic rise in the average standard of living, as characterised by a 40% rise in average real wages from 1950 to 1965. Earnings for men in industry rose by 95% between 1951 and 1964, while during that same period the official workweek was reduced and five reductions in income tax were made. Those in traditionally poorly paid semi-skilled and unskilled occupations saw a particularly marked improvement in their wages and living standards. As summed up by R. J. Unstead,
"Opportunities in life, if not equal, were distributed much more fairly than ever before and\ the weekly wage-earner, in particular, had gained standards of living that would have been almost unbelievable in the thirties."
In 1950, the UK standard of living was higher than in any EEC country apart from Belgium. It was 50% higher than the West German standard of living, and twice as high as the Italian standard of living. By the earlier Seventies, however, the UK standard of living was lower than all EEC countries apart from Italy (which, according to one calculation, was roughly equal to Britain). In 1951, the average weekly earnings of men over the age of 21 stood at £8 6s 0d, and nearly doubled a decade later to £15 7s 0d. By 1966, average weekly earnings stood at £20 6s 0d.
Between 1951 and 1963, wages rose by 72% while prices rose by 45%, enabling people to afford more consumer goods than ever before. Between 1955 and 1967, the average earnings of weekly-paid workers increased by 96% and those of salaried workers by 95%, while prices rose by about 45% in the same period. The rising affluence of the Fifties and Sixties was underpinned by sustained full employment and a dramatic rise in worker‘s wages. In 1950, the average weekly wage stood at £6.8s, compared with £11.2s.6d in 1959. As a result of wage rises, consumer spending also increased by about 20% during this same period, while economic growth remained at about 3%. In addition, food rations were lifted in 1954 while hire-purchase controls were relaxed in the same year. As a result of these changes, large numbers of the working classes were able to participate in the consumer market for the first time. As noted by Harriet Wilson
 “national wealth has grown considerably, and although the shareout of this among the social classes has remained substantially of the same proportions, it has meant a considerable rise in the standard of living of all classes. It is estimated that in Britain at the turn of the century average earnings in industry sufficed merely to meet the essential needs of a two-child family, today average earnings allow the industrial wage-earner to spend a third of his income on things other than basic needs.”
The significant real wage increases in the 1950s and 1960s contributed to a rapid increase in working-class consumerism, with British consumer spending rising by 45% between 1952 and 1964. In addition, entitlement to various fringe benefits was improved. In 1955, 96% of manual labourers were entitled to two weeks’ holiday with pay, compared with 61% in 1951. By the end of the 1950s, Britain had become one of the world's most affluent countries, and by the early Sixties, most Britons enjoyed a level of prosperity that had previously been known only to a small minority of the population. For the young and unattached, there was, for the first time in decades, spare cash for leisure, clothes, and luxuries. In 1959, "Queen" magazine declared that "Britain has launched into an age of unparalleled lavish living." Average wages were high while jobs were plentiful, and people saw their personal prosperity climb even higher. Prime Minister Harold Macmillan claimed that "the luxuries of the rich have become the necessities of the poor." Levels of disposable income rose steadily, with the spending power of the average family rising by 50% between 1951 and 1979, and by the end of the Seventies, 6 out of 10 families had come to own a car.
As noted by Martin Pugh,
"Keynesian economic management enabled British workers to enjoy a golden age of full employment which, combined with a more relaxed attitude towards working mothers, led to the spread of the two-income family. Inflation was around 4 per cent, money wages rose from an average of £8 a week in 1951 to £15 a week by 1961, home-ownership spread from 35 per cent in 1939 to 47 per cent by 1966, and the relaxation of credit controls boosted the demand for consumer goods."
By 1963, 82% of all private households had a television, 72% a vacuum cleaner, 45%a washing machine, and 30% a refrigerator. In addition, as noted by John Burnett,
“What was equally striking was that ownership of such things had spread down the social scale and the gap between professional and manual workers had considerably narrowed.”
A study of a slum area in Leeds (which was due for demolition) found that 74% of the households had a T.V., 41% a vacuum, and 38% a washing machine. In another slum area, St Mary’s in Oldham (where in 1970 few of the houses had fixed baths or a hot water supply and half shared outside toilets), 67% of the houses were rated as comfortably furnished and a further 24% furnished luxuriously, with smart modern furniture, deep pile carpeting, and decorations.
The provision of household amenities steadily improved during the second half of the Twentieth Century. From 1971 to 1983, households having the sole use of a fixed bath or shower rose from 88% to 97%, and those with an internal WC from 87% to 97%. In addition, the number of households with central heating almost doubled during that same period, from 34% to 64%. By 1983, 94% of all households had a refrigerator, 81% a colour television, 80% a washing machine, 57% a deep freezer, and 28% a tumble-drier.
Between 1950 and 1970, however, Britain was overtaken by most of the countries of the European Common Market in terms of the number of telephones, refrigerators, television sets, cars, and washing machines per 100 of the population (although Britain remained high in terms of bathrooms and lavatories per 100 people). Although the British standard of living was increasing, the standard of living in other countries increased faster. In 1976, UK wages were amongst the lowest in Western Europe, being half of West German rates and two-thirds of Italian rates. In addition, while educational opportunities for working-class people had widened significantly since the end of the Second World War, a number of developed countries came to overtake Britain in some educational indicators. By the early 1980s, some 80% to 90% of school leavers in France and West Germany received vocational training, compared with 40% in the United Kingdom. By the mid-1980s, over 80% of pupils in the United States and West Germany and over 90% in Japan stayed in education until the age of eighteen, compared with barely 33% of British pupils. In 1987, only 35% of 16- to 18-year-olds were in full-time education or training, compared with 80% in the United States, 77% in Japan, 69% in France, and 49% in the United Kingdom. There also remained gaps between manual and non-manual workers in areas such as fringe benefits and wage levels. In April 1978, for instance, male full-time manual workers aged 21 and above averaged a gross weekly wage of £80.70, while the equivalent for male white collar workers stood at £100.70.
Empire to Commonwealth.
Britain's control over its Empire loosened during the interwar period. Nationalism strengthened in other parts of the empire, particularly in India and in Egypt.
Between 1867 and 1910, the UK had granted Australia, Canada, and New Zealand "Dominion" status (near complete autonomy within the Empire). They became charter members of the British Commonwealth of Nations (known as the Commonwealth of Nations since 1949), an informal but close-knit association that succeeded the British Empire. Beginning with the independence of India and Pakistan in 1947, the remainder of the British Empire was almost completely dismantled. Today, most of Britain's former colonies belong to the Commonwealth, almost all of them as independent members. There are, however, 13 former British colonies, including Bermuda, Gibraltar, the Falkland Islands, and others, which have elected to continue rule by London and are known as British Overseas Territories.
From the Troubles to the Belfast Agreement.
In the 1960s, moderate unionist Prime Minister of Northern Ireland Terence O'Neill tried to reform the system and give a greater voice to Catholics who comprised 40% of the population of Northern Ireland. His goals were blocked by militant Protestants led by the Rev. Ian Paisley. The increasing pressures from nationalists for reform and from unionists to resist reform led to the appearance of the civil rights movement under figures like John Hume, Austin Currie and others. Clashes escalated out of control as the army could barely contain the Provisional Irish Republican Army (IRA) and the Ulster Defence Association. British leaders feared their withdrawal would give a "Doomsday Scenario," with widespread communal strife, followed by the mass exodus of hundreds of thousands of refugees. London shut down Northern Ireland's parliament and began direct rule. By the 1990s, the failure of the IRA campaign to win mass public support or achieve its aim of a British withdrawal led to negotiations that in 1998 produced the 'Good Friday Agreement'. It won popular support and largely ended the Troubles.
The economy in the late 20th century.
After the relative prosperity of the 1950s and 1960s, the UK experienced extreme industrial strife and stagflation through the 1970s following a global economic downturn; Labour had returned to government in 1964 under Harold Wilson to end 13 years of Conservative rule. The Conservatives were restored to government in 1970 under Edward Heath, who failed to halt the country's economic decline and was ousted in 1974 as Labour returned to power under Harold Wilson. The economic crisis deepened following Wilson's return and things fared little better under his successor James Callaghan.
A strict modernisation of its economy began under the controversial Conservative leader Margaret Thatcher following her election as prime minister in 1979, which saw a time of record unemployment as deindustrialisation saw the end of much of the country's manufacturing industries but also a time of economic boom as stock markets became liberalised and State-owned industries became privatised. Inflation also fell during this period and trade union power was reduced.
However the miners' strike of 1984–1985 sparked the end of most of the UK's coal mining. The exploitation of North Sea gas and oil brought in substantial tax and export revenues to aid the new economic boom. This was also the time that the IRA took the issue of Northern Ireland to Great Britain, maintaining a prolonged bombing campaign on the British mainland.
After the economic boom of the 1980s a brief but severe recession occurred between 1990 and 1992 following the economic chaos of Black Wednesday under government of John Major, who had succeeded Margaret Thatcher in 1990. However the rest of the 1990s saw the beginning of a period of continuous economic growth that lasted over 16 years and was greatly expanded under the New Labour government of Tony Blair following his landslide election victory in 1997, with a rejuvenated party having abandoned its commitment to policies including nuclear disarmament and nationalisation of key industries, and no reversal of the Thatcher-led union reforms.
From 1964 up until 1996, income per head had doubled, while ownership of various household goods had significantly increased. By 1996, two-thirds of households owned cars, 82% had central heating, most people owned a VCR, and one in five houses had a home computer. In 1971, 9% of households had no access to a shower or bathroom, compared with only 1% in 1990; largely due to demolition or modernisation of older properties which lacked such facilities. In 1971, only 35% had central heating, while 78% enjoyed this amenity in 1990. By 1990, 93% of households had colour television, 87% had telephones, 86% had washing machines, 80% had deep-freezers, 60% had video-recorders, and 47% had microwave ovens. Holiday entitlements had also become more generous. In 1990, nine out of ten full-time manual workers were entitled to more than four weeks of paid holiday a year, while twenty years previously only two-thirds had been allowed three weeks or more. The postwar period also witnessed significant improvements in housing conditions. In 1960, 14% of British households had no inside toilet, while in 1967 22% of all homes had no basic hot water supply. By the Nineties, however almost all homes had these amenities together with central heating, which was a luxury just two decades before. From 1996/7 to 2006/7, real median household income increased by 20% while real mean household incomes increased by 23%. There has also been a shift towards a service-based economy in the years following the end of the Second World War, with 11% of working people employed in manufacturing in 2006, compared with 25% in 1971.
Common Market (EEC), then EU, membership.
Britain's wish to join the Common Market (as the European Economic Community was known in Britain) was first expressed in July 1961 by the Macmillan government, was negotiated by Edward Heath as Lord Privy Seal, but was vetoed in 1963 by French President Charles de Gaulle. After initially hesitating over the issue, Harold Wilson's Labour Government lodged the UK's second application (in May 1967) to join the European Community, as it was now called. Like the first, though, it was vetoed by de Gaulle in November that year.
In 1973, as Conservative Party leader and Prime Minister, Heath negotiated terms for admission and Britain finally joined the Community, alongside Denmark and Ireland in 1973. In opposition, the Labour Party was deeply divided, though its Leader, Harold Wilson, remained in favour. In the 1974 General Election, the Labour Party manifesto included a pledge to renegotiate terms for Britain's membership and then hold a referendum on whether to stay in the EC on the new terms. This was a constitutional procedure without precedent in British history. In the subsequent referendum campaign, rather than the normal British tradition of "collective responsibility", under which the government takes a policy position which all cabinet members are required to support publicly, members of the Government (and the Conservative opposition) were free to present their views on either side of the question. A referendum was duly held on 5 June 1975, and the proposition to continue membership was passed with a substantial majority.
The Single European Act (SEA) was the first major revision of the 1957 Treaty of Rome. In 1987, the Conservative government under Margaret Thatcher enacted it into UK law.
The Maastricht Treaty transformed the European Community into the European Union. In 1992, the Conservative government under John Major ratified it, against the opposition of his backbench Maastricht Rebels.
The Treaty of Lisbon introduced many changes to the treaties of the Union. Prominent changes included more qualified majority voting in the Council of Ministers, increased involvement of the European Parliament in the legislative process through extended codecision with the Council of Ministers, eliminating the pillar system and the creation of a President of the European Council with a term of two and a half years and a High Representative of the Union for Foreign Affairs and Security Policy to present a united position on EU policies. The Treaty of Lisbon will also make the Union's human rights charter, the Charter of Fundamental Rights, legally binding. The Lisbon Treaty also leads to an increase in the voting weight of the UK in the Council of the European Union from 8.4% to 12.4%. In July 2008, the Labour government under Gordon Brown approved the treaty and the Queen ratified it.
Devolution for Scotland and Wales.
On 11 September 1997, (on the 700th anniversary of the Scottish victory over the English at the Battle of Stirling Bridge), a referendum was held on establishing a devolved Scottish Parliament. This resulted in an overwhelming 'yes' vote both to establishing the parliament and granting it limited tax varying powers. Two weeks later, a referendum in Wales on establishing a Welsh Assembly was also approved but with a very narrow majority. The first elections were held, and these bodies began to operate, in 1999. The creation of these bodies has widened the differences between the Countries of the United Kingdom, especially in areas like healthcare. It has also brought to the fore the so-called West Lothian question which is a complaint that devolution for Scotland and Wales but not England has created a situation where all the MPs in the UK parliament can vote on matters affecting England alone but on those same matters Scotland and Wales can make their own decisions.
21st century.
War in Afghanistan and Iraq, and terrorist attacks.
In the 2001 General Election, the Labour Party won a second successive victory, though voter turnout dropped to the lowest level for more than 80 years. Later that year, the September 11th attacks in the United States led to American President George W. Bush launching the War on Terror, beginning with the invasion of Afghanistan aided by British troops in October 2001. Thereafter, with the US focus shifting to Iraq, Tony Blair convinced the Labour and Conservative MPs to vote in favour of supporting the 2003 invasion of Iraq, despite huge anti-war marches held in London and Glasgow. Forty-six thousand British troops, one-third of the total strength of the Army's land forces, were deployed to assist with the invasion of Iraq and thereafter British armed forces were responsible for security in southern Iraq. All British forces were withdrawn in 2010.
The Labour Party won the 2005 general election and a third consecutive term. On 7 July 2005, a series of four suicide bombings struck London, killing 52 commuters, in addition to the four bombers.
Nationalist government in Scotland.
2007 saw the first ever election victory for the pro-independence Scottish National Party (SNP) in the Scottish Parliament elections. They formed a minority government with plans to hold a referendum before 2011 to seek a mandate "to negotiate with the Government of the United Kingdom to achieve independence for Scotland." Most opinion polls show minority support for independence, although support varies depending on the nature of the question. The response of the unionist parties was to establish the Calman Commission to examine further devolution of powers, a position that had the support of the Prime Minister.
Responding to the findings of the review, the UK government announced on 25 November 2009, that new powers would be devolved to the Scottish Government, notably on how it can raise tax and carry out capital borrowing, and the running of Scottish Parliament elections. These proposals were detailed in a white paper setting out a new Scotland Bill, to become law before the 2015 Holyrood elections. The proposal was criticised by the UK parliament opposition parties for not proposing to implement any changes before the next general election. Scottish Constitution Minister Michael Russell criticised the white paper, calling it "flimsy" and stating that their proposed Referendum (Scotland) Bill, 2010, whose own white paper was to be published five days later, would be "more substantial". According to "The Independent", the Calman Review white paper proposals fall short of what would normally be seen as requiring a referendum.
The 2011 election saw a decisive victory for the SNP which was able to form a majority government intent on delivering a referendum on independence. Within hours of the victory, Prime Minister David Cameron guaranteed that the UK government would not put any legal or political obstacles in the way of such a referendum. Some unionist politicians, including former Labour First Minister Henry McLeish, have responded to the situation by arguing that Scotland should be offered 'devo-max' as an alternative to independence, and First Minister Alex Salmond has signalled his willingness to include it on the referendum ballot paper.
The 2008 economic crisis.
In the wake of the global economic crisis of 2008, the United Kingdom economy contracted, experiencing negative economic growth throughout 2009. The announcement in November 2008 that the economy had shrunk for the first time since late 1992 brought an end to 16 years of continuous economic growth. Causes included an end to the easy credit of the preceding years, reduction in consumption and substantial depreciation of sterling (which fell 25% against the euro between January 2008 and January 2009), leading to increased import costs, notably of oil.
On 8 October 2008, the British Government announced a bank rescue package of around £500 billion ($850 billion at the time). The plan comprised three parts.: £200 billion to be made available to the banks in the Bank of England's Special Liquidity Scheme; the Government was to increase the banks' market capitalization, through the Bank Recapitalization Fund, with an initial £25 billion and another £25 billion to be provided if needed; and the Government was to temporarily underwrite any eligible lending between British banks up to around £250 billion. With the UK officially coming out of recession in the fourth quarter of 2009—ending six consecutive quarters of economic decline—the Bank of England decided against further quantitative easing.
The 2010 coalition government.
The United Kingdom General Election of 6 May 2010 resulted in the first hung parliament since 1974, with the Conservative Party winning the largest number of seats, but falling short of the 326 seats required for an overall majority. Following this, the Conservatives and the Liberal Democrats agreed to form the first coalition government for the UK since the end of the Second World War, with David Cameron becoming Prime Minister and Nick Clegg Deputy Prime Minister.
Under the coalition government, British military aircraft participated in the UN-mandated intervention in the 2011 Libyan civil war, flying a total of 3,000 air sorties against forces loyal to the Libyan dictator Muammar Gaddafi between March and October 2011. 2011 also saw England suffer unprecedented rioting in its major cities in early August, killing five people and causing over £200 million worth of property damage.
In late October 2011, the prime ministers of the Commonwealth realms voted to grant gender equality in the royal succession, ending the male-preference primogeniture that was mandated by the Act of Settlement 1701. The amendment, once enacted, will also end the ban on the monarch marrying a Catholic.
2015 election.
In 2014 Scotland held a referendum on becoming independent of the United Kingdom. The three major national parties were all strongly opposed, and won a majority defeating the separationists of the Scottish National Party (SNP). However, SNP successfully mobilized after the election, sweeping out the Labour Party which had long dominated Scotland. 
After years of austerity, the British economy was on an upswing in 2015, When Prime Minister David Cameron called a general election. The United Kingdom general election, 2015 was held on 7 May 2015. Pre-election polls had predicted a close race and a hung parliament, but the surprising result was clear victory by the Conservatives nationwide. The other three main parties were shocked and bitterly disappointed; their leaders resigned the next day. The Conservatives with 37% of the popular vote held a narrow majority with 331 of the 650 seats. The Scottish National Party (SNP) carried 56 of the 59 seats in Scotland, a gain of 50. Labour suffered its worst defeat since 1987, taking only 31% of the votes and losing 40 of its 41 seats in Scotland. The Liberal Democrats lost 49 of their 57 seats, as there coalition with the conservatives had alienated the great majority of their supporters. The UK Independence Party (UKIP), rallying voters against Europe and against immigration, did well with 13% of the vote count. It came in second in over 115 races but came in first in only one. Cameron now has a mandate for his austerity policies that shrink the size of government, and a challenge in dealing with Scotland.
Notes.
1    The terms "United Kingdom" and "United Kingdom of Great Britain" were used in the Treaty of Union and the 1707 Acts of Union. However, the actual "name" of the new state was "Great Britain".<br>
2    The name "Great Britain" (then spelled "Great Brittaine") was first used by James VI/I in October 1604, who indicated that henceforth he and his successors would be viewed as Kings of Great Britain, not Kings of England and Scotland. However the name was not applied to the "state" as a unit; both England and Scotland continued to be governed independently. Its validity as a name of the Crown is also questioned, given that monarchs continued using separate ordinals (e.g., James VI/I, James VII/II) in England and Scotland. To avoid confusion, historians generally avoid using the term "King of Great Britain" until 1707 and instead to match the ordinal usage call the monarchs kings or queens of England and Scotland. Separate ordinals were abandoned when the two states merged in accordance with the 1707 Acts of Union, with subsequent monarchs using ordinals apparently based on English not Scottish history (it might be argued that the monarchs have simply taken the higher ordinal, which to date has always been English). One example is Queen Elizabeth II of the United Kingdom, who is referred to as being "the Second" even though there never was an Elizabeth I of Scotland or Great Britain. Thus the term "Great Britain" is generally used from 1707.<br>
3    The number changed several times between 1801 and 1922.<br>
4    The Anglo-Irish Treaty was ratified by (i) The British Parliament (Commons, Lords & Royal Assent), (ii) Dáil Éireann, and the (iii) "at a meeting of members of the Parliament elected for constituencies in Southern Ireland", (though not the House of Commons of Southern Ireland – see this), the Treaty thus being ratified under both British and Irish constitutional theory

</doc>
<doc id="31724" url="http://en.wikipedia.org/wiki?curid=31724" title="Geography of the United Kingdom">
Geography of the United Kingdom

The United Kingdom is a sovereign state located off the north-western coast of continental Europe. With a total area of approximately 243610 km2, the UK occupies the major part of the British Isles archipelago and includes the island of Great Britain, the north-eastern one-sixth of the island of Ireland and many smaller surrounding islands. The mainland areas lie between latitudes 49°N and 59°N (the Shetland Islands reach to nearly 61°N), and longitudes 8°W to 2°E. The Royal Greenwich Observatory, in South East London, is the defining point of the Prime Meridian.
The UK lies between the North Atlantic and the North Sea, and comes within 35 km of the north-west coast of France, from which it is separated by the English Channel. It shares a 499 km international land boundary with the Republic of Ireland. The Channel Tunnel bored beneath the English Channel, now links the UK with France.
The British Overseas Territories and Crown Dependencies are covered in their own respective articles, see below.
Area.
The total area of the United Kingdom is approximately 245000 km2, comprising the island of Great Britain, the northeastern one-sixth of the island of Ireland (Northern Ireland) and many smaller islands. England is the largest country of the United Kingdom, at 130410 km2 accounting for just over half the total area of the UK. Scotland at 78772 km2, is second largest, accounting for about a third of the area of the UK. Wales and Northern Ireland are much smaller, covering 20758 and respectively.
The area of the countries of the United Kingdom is set out in the table below. Information about the area of England, the largest country, is also broken down by region.
The British Antarctic Territory, which covers an area of 1,709,400 km2 is geographically the largest of the British Overseas Territories followed by the Falkland Islands which covers an area of 12,173 km2. The remaining twelve overseas territories cover an area 5,997 km2.
Other countries with very similar land areas to the United Kingdom include Guinea (slightly larger), Uganda, Ghana and Romania (all slightly smaller). The UK is the world's 80th largest country by land area and the 10th largest in Europe (if European Russia is included).
Physical geography.
The physical geography of the UK varies greatly. England consists of mostly lowland terrain, with upland or mountainous terrain only found north-west of the Tees-Exe line. The upland areas include the Lake District, the Pennines, Exmoor and Dartmoor. The lowland areas are typically traversed by ranges of low hills, frequently composed of chalk. The physical geography of Scotland is distinguished by the Highland Boundary Fault which traverses the Scottish mainland from Helensburgh to Stonehaven. The faultline separates the two distinctively different regions of the Highlands to the north and west, and the lowlands to the south and east. Wales is mostly mountainous, though south Wales is less mountainous than north and mid Wales. The geography of Ireland includes the Mourne Mountains as well as Lough Neagh, at 388 km2, the largest body of water in the UK.
The overall geomorphology of the UK was shaped by the combined forces of tectonics and climate change, in particular glaciation.
The exact centre of the island of Great Britain is disputed. Depending upon how it is calculated it can be either Haltwhistle in Northumberland, or Dunsop Bridge in Lancashire.
Geology.
The geology of the UK is complex and diverse, a result of it being subject to a variety of plate tectonic processes over a very extended period of time. Changing latitude and sea levels have been important factors in the nature of sedimentary sequences, whilst successive continental collisions have affected its geological structure with major faulting and folding being a legacy of each orogeny (mountain-building period), often associated with volcanic activity and the metamorphism of existing rock sequences. As a result of this eventful geological history, the UK shows a rich variety of landscapes.
The oldest rocks in the British Isles are the Lewisian gneisses, metamorphic rocks found in the far north west of Scotland and in the Hebrides (with a few small outcrops elsewhere), which date from at least 2,700 Ma (Ma = million years ago). South of the gneisses are a complex mixture of rocks forming the North West Highlands and Grampian Highlands in Scotland. These are essentially the remains of folded sedimentary rocks that were deposited between 1,000 Ma and 670 Ma over the gneiss on what was then the floor of the Iapetus Ocean.
At 520 Ma, what is now Great Britain was split between two continents; the north of Scotland was located on the continent of Laurentia at about 20° south of the equator, while the rest of the country was on the continent of Gondwana near the Antarctic Circle. In Gondwana, England and Wales were largely submerged under a shallow sea studded with volcanic islands. The remains of these islands underlie much of central England with small outcrops visible in many places.
About 500 Ma southern Britain, the east coast of North America and south-east Newfoundland broke away from Gondwana to form the continent of Avalonia, which by 440 Ma had drifted to about 30° south. During this period north Wales was subject to volcanic activity. The remains of these volcanoes are still visible, one example of which is Rhobell Fawr dating from 510 Ma. Large quantities of volcanic lava and ash known as the Borrowdale Volcanics covered the Lake District and this can still be seen in the form of mountains such as Helvellyn and Scafell Pike.
Between 425 and 400 Ma Avalonia had joined with the continent of Baltica, and the combined landmass collided with Laurentia at about 20° south, joining the southern and northern halves of Great Britain together. The resulting Caledonian Orogeny produced an Alpine-style mountain range in much of north and west Britain.
The collision between continents continued during the Devonian period, producing uplift and subsequent erosion, resulting in the deposition of numerous sedimentary rock layers in lowlands and seas. The Old Red Sandstone and the contemporary volcanics and marine sediments found in Devon originated from these processes.
Around 360 Ma Great Britain was lying at the equator, covered by the warm shallow waters of the Rheic Ocean, during which time the Carboniferous Limestone was deposited, as found in the Mendip Hills and the Peak District of Derbyshire. Later, river deltas formed and the sediments deposited were colonised by swamps and rain forest. It was in this environment that the Coal Measures were formed, the source of the majority of Britain's extensive coal reserves.
Around 280 Ma the Variscan orogeny mountain-building period occurred, again due to collision of continental plates, causing major deformation in south west England. The general region of Variscan folding was south of an east–west line roughly from south Pembrokeshire to Kent. Towards the end of this period granite was formed beneath the overlying rocks of Devon and Cornwall, now exposed at Dartmoor and Bodmin Moor.
By the end of the Carboniferous period the various continents of the Earth had fused to form the super-continent of Pangaea. Britain was located in the interior of Pangea where it was subject to a hot arid desert climate with frequent flash floods leaving deposits that formed beds of red sedimentary rock.
As Pangaea drifted during the Triassic, Great Britain moved away from the equator until it was between 20° and 30° north. The remnants of the Variscan uplands in France to the south were eroded down, resulting in layers of the New Red Sandstone being deposited across central England.
Pangaea began to break up at the start of the Jurassic period. Sea levels rose and Britain drifted on the Eurasian Plate to between 30° and 40° north. Much Britain was under water again, and sedimentary rocks were deposited and can now be found underlying much of England from the Cleveland Hills of Yorkshire to the Jurassic Coast in Dorset. These include sandstones, greensands, oolitic limestone of the Cotswold Hills, corallian limestone of the Vale of White Horse and the Isle of Portland. The burial of algae and bacteria below the mud of the sea floor during this time resulted in the formation of North Sea oil and natural gas
The modern continents having formed, the Cretaceous saw the formation of the Atlantic Ocean, gradually separating northern Scotland from North America. The land underwent a series of uplifts to form a fertile plain. After 20 million years or so, the seas started to flood the land again until much of Britain was again below the sea, though sea levels frequently changed. Chalk and flints were deposited over much of Great Britain, now notably exposed at the White Cliffs of Dover and the Seven Sisters, and also forming Salisbury Plain.
Between 63 and 52 Ma, the last volcanic rocks in Great Britain were formed. The major eruptions at this time produced the Antrim Plateau, the basaltic columns of the Giant's Causeway and Lundy Island in the Bristol Channel.
The Alpine Orogeny that took place in Europe about 50 Ma, was responsible for the folding of strata in southern England, producing the London Basin syncline, the Weald-Artois Anticline to the south, the North Downs, South Downs and Chiltern Hills.
During the period the North Sea formed, Britain was uplifted. Some of this uplift was along old lines of weakness left from the Caledonian and Variscan Orogenies long before. The uplifted areas were then eroded, and further sediments, such as the London Clay, were deposited over southern England.
The major changes during the last 2 million years were brought about by several recent ice ages. The most severe was the Anglian Glaciation, with ice up to 1,000 m thick that reached as far south as London and Bristol. This took place between about 478,000 to 424,000 years ago, and was responsible for the diversion of the River Thames onto its present course. During the most recent Devensian glaciation, which ended a mere 10,000 years ago, the icesheet reached south to Wolverhampton and Cardiff. Among the features left behind by the ice are the fjords of the west coast of Scotland, the U-shaped valleys of the Lake District and erratics (blocks of rock) that have been transported from the Oslo region of Norway and deposited on the coast of Yorkshire.
Amongst the most significant geological features created during the last twelve thousand years are the peat deposits of Scotland, and of coastal and upland areas of England and Wales.
At the present time Scotland is continuing to rise as a result of the weight of Devensian ice being lifted. Southern and eastern England is sinking, generally estimated at 1 mm (1/25 inch) per year, with the London area sinking at double the speed partly due to the continuing compaction of the recent clay deposits.
Mountains and hills.
The ten tallest mountains in the UK are all found in Scotland. The highest peaks in each part of the UK are:
The ranges of mountains and hills in the UK include:
The lowest point of the UK is in the Fens of East Anglia, in England, parts of which lie up to 4 metres below sea level.
Rivers and lakes.
The longest river in the UK is the River Severn (220 mi) which flows through both Wales and England.
The longest rivers in the UK contained wholly within each of its constituent nations are:
The largest lakes (by surface area) in the UK by country are:
The deepest lake in the UK is Loch Morar with a maximum depth of 309 metres (Loch Ness is second at 228 metres deep). The deepest lake in England is Wastwater which achieves a depth of 79 m.
Artificial waterways.
"Main articles:" Waterways in the United Kingdom, Canals of Great Britain, Reservoirs and dams in the United Kingdom
As a result of its industrial history, the United Kingdom has an extensive system of canals, mostly built in the early years of the Industrial Revolution, before the rise of competition from the railways. The United Kingdom also has numerous dams and reservoirs to store water for drinking and industry. The generation of hydroelectric power is rather limited, supplying less than 2% of British electricity mainly from the Scottish Highlands.
Coastline.
The UK has a coastline which measures about 12,429 km. The heavy indentation of the coastline helps to ensure that no location is more than 125 km from tidal waters.
The UK claims jurisdiction over the continental shelf, as defined in continental shelf orders or in accordance with agreed upon boundaries, an exclusive fishing zone of 200 nmi, and territorial sea of 12 nmi.
Headlands.
The geology of the United Kingdom is such that there are many headlands along its coast. A list of headlands of the United Kingdom details many of them.
Islands.
In total, it is estimated that the UK is made up of over one thousand small islands, the majority located off the north and west coasts of Scotland. About 130 of these are inhabited according to the 2001 Census.
Climate.
The climate of the UK is generally temperate, although significant local variation occurs, particularly as a result of altitude and distance from the coast. In general the south of the country is warmer than the north, and the west wetter than the east. Due to the warming influence of the Gulf Stream, the UK is significantly warmer than some other locations at similar latitude, such as Newfoundland.
The prevailing winds are southwesterly, from the North Atlantic Current. More than 50% of the days are overcast. There are few natural hazards, although there can be strong winds and floods, especially in winter.
Average annual rainfall varies from over 3000 mm in the Scottish Highlands down to 553 mm in Cambridge. The county of Essex is one of the driest in the UK, with an average annual rainfall of around 600 mm, although it typically rains on over 100 days per year. In some years rainfall in Essex can be below 450 mm, less than the average annual rainfall in Jerusalem and Beirut.
The highest temperature recorded in the UK was 38.5 °C at Brogdale, near Faversham, in the county of Kent, on 10 August 2003. The lowest was -27.2 °C recorded at Braemar in the Grampian Mountains, Scotland, on 11 February 1895 and 10 January 1982 and Altnaharra, also in Scotland, on 30 December 1995.
Human geography.
Political geography.
National government.
The UK is governed as a whole by the Parliament of the United Kingdom. Of the four countries that make the UK, Scotland, Wales and Northern Ireland have devolved administrations and parliaments/assembly:
England has no devolved system of governmentthat is, the Parliament of the United Kingdom serves as (and historically was) the English Parliament. It is governed by UK government ministers and legislated for by the UK parliament. Within England, London has a devolved assembly but proposals for elected Regional Assemblies in England were rejected in the first referendum covering North East England. "See Government of England."
The UK (specifically, Northern Ireland) has an international land boundary with the Republic of Ireland of 499 km. There is also a boundary between the jurisdiction of France and the UK on the Channel Tunnel.
Local government.
Each part of the UK is subdivided in further local governmental regions:
Historically the UK was divided into counties or shires: administrative areas through which all civil responsibilities of the government were passed. Each county or shire had a county town as its administrative centre and was divided into individual parishes that were defined along ecclesiastic boundaries.
Between 1889 (1890 in Scotland) and 1974, the political boundaries were based on the traditional counties, but due to changes in population centres, the traditional counties became impractical as local government areas in certain highly urbanised areas. The Local Government Act 1972 created a new system of administrative counties, designed to take account of the widely differing populations across different parts of the country.
In the 1990s further population growth led to more political changes on a local level. Unitary authorities were formed across the entirety of Scotland and Wales, and in larger cities in England. Many unpopular administrative counties were also abolished at this time, leading to a mixture of two-tier and single-purpose authorities. Further reorganisations are planned if and when regional assemblies in England are revisited in the future.
Economic geography.
The economic geography of the UK reflects not only its current position in the global economy, but its long history both as a trading nation and an imperial power.
The UK led the industrial revolution and its highly urban character is a legacy of this, with all its major cities being current or former centres of various forms of manufacturing. However, this in turn was built on its exploitation of natural resources, especially coal and iron ore.
Primary industry.
The UK's primary industry was once dominated by the coal industry, heavily concentrated in the north, the Midlands and south Wales. This is all but gone and the major primary industry is North Sea oil. Its activity is concentrated on the UK Continental Shelf to the north-east of Scotland.
Manufacturing.
The UK's heavy manufacturing drove the industrial revolution. A map of the major UK cities gives a good picture of where this activity occurred, in particular Belfast, Birmingham, Glasgow, Liverpool, London, Manchester, Newcastle and Sheffield. Today there is no heavy manufacturing industry in which UK-based firms can be considered world leaders. However, areas of the UK still have a notable manufacturing base, including the Midlands which remains a strong manufacturing centre, and the North West which accounts for 60% of the United Kingdom's manufacturing output. More recently, high technology firms have concentrated largely along the M4 motorway, partly because of access to Heathrow Airport, but also because of agglomeration economies.
Finance and services.
Once, every large city had a stock exchange. Now, the UK financial industry is concentrated overwhelmingly in the City of London and Canary Wharf, with back office and administrative operations often dispersed around the south of England. London is one of the world's great financial centres and is usually referred to as a world city. There is also a significant legal and ebusiness industry in Leeds.
Regional disparity.
The effect of changing economic fortune has contributed to the creation of the so-called North-South divide, in which decaying industrial and ex-industrial areas of Northern England, Scotland and Wales contrast with the wealthy, finance and technology-led southern economy. This has led successive governments to develop regional policy to try to rectify the imbalance. However, this is not to say that the north-south divide is uniform; some of the worst pockets of deprivation can be found in London, whilst parts of Cheshire and North Yorkshire are very wealthy. Nor is the North-South divide limited to the economic sphere; cultural and political divisions weigh heavily too.
Natural resources.
Historically, much of the United Kingdom was forested. Since prehistoric times, man has deforested much of the United Kingdom.
Agriculture is intensive, highly mechanised, and efficient by European standards, producing about 60% of food needs with only 1% of the labour force. It contributes around 2% of GDP. Around two thirds of production is devoted to livestock, one third to arable crops.
In 1993, it was estimated that land use was:
The UK has a variety of natural resources including:
The UK has large coal, natural gas, and oil reserves; primary energy production accounts for 10% of GDP, one of the highest shares of any industrial nation. Due to the island location of the UK, the country has great potential for generating electricity from wave power and tidal power, although these have not yet been exploited on a commercial basis.
Environment.
Current issues.
England is one of the most densely populated countries/regions in the world, and the most densely populated major nation in Europe. The high population density (especially in the southeast of England) coupled with a changing climate, is likely to put extreme pressure on the United Kingdom's water resources in the future.
The United Kingdom is reducing greenhouse gas emissions. It has met Kyoto Protocol target of a 12.5% reduction from 1990 levels and intends to meet the legally binding target of a 20% cut in emissions by 2010. By 2015, to recycle or compost at least 33% of household waste. Between 1998-99 and 1999–2000, household recycling increased from 8.8% to 10.3% respectively.
International agreements.
The United Kingdom is a party to many international agreements, including:
Air Pollution, Air Pollution-Nitrogen Oxides, Air Pollution-Sulphur 94, Air Pollution-Volatile Organic Compounds, Antarctic-Environmental Protocol, Antarctic-Marine Living Resources, Antarctic Seals, Antarctic Treaty, Biodiversity, Climate Change, Climate Change-Kyoto Protocol, Desertification, Endangered Species, Environmental Modification, Hazardous Wastes, Law of the Sea, Marine Dumping, Marine Life Conservation, Nuclear Test Ban, Ozone Layer Protection, Ship Pollution, Tropical Timber 83, Tropical Timber 94, Wetlands and Whaling.
The UK has signed, but not ratified, the international agreement on Air Pollution-Persistent Organic Pollutants.

</doc>
<doc id="31725" url="http://en.wikipedia.org/wiki?curid=31725" title="Demography of the United Kingdom">
Demography of the United Kingdom

The UK population is considered an example of a population which has undergone the 'demographic transition' - that is, the transition from a (typically) pre-industrial population with high birth and mortality rates and only slow population growth, through a stage of falling mortality and faster rates of population growth, to a stage of low birth and mortality rates with, again, lower rates of population growth. This population growth through 'natural change' has been accompanied in the past two decades by growth through net international migration into the UK.
According to the 2011 census, the total population of the United Kingdom was around 63,182,000. It is the third-largest in the European Union (behind Germany and France) and the 22nd-largest in the world. Its overall population density is one of the highest in the world at 259 people per square kilometre, due to the particularly high population density in England. Almost one-third of the population lives in England's southeast, which is predominantly urban and suburban, with about 8 million in the capital city of London, the population density of which is just over 5,200 per square kilometre.
The United Kingdom's assumed high literacy rate (99% at age 15 and above) is attributable to universal public education introduced for the primary level in 1870 (Scotland 1872, free 1890) and secondary level in 1900. Parents are obliged to have their children educated from the ages of 5 to 16 (with legislation passed to raise this to 18), and can continue education free of charge in the form of A-Levels, vocational training or apprenticeship to age 18. About 40% of British students go on to post-secondary education (18+). The Church of England and the Church of Scotland function as the national churches in their respective countries, but all the major religions found in the world are represented in the United Kingdom.
The UK's population is predominantly White British. Being located close to continental Europe, the countries that formed the United Kingdom were subject to many invasions and migrations, especially from Scandinavia and the continent, including Roman occupation for several centuries. Historically, British people were thought to be descended mainly from the different ethnic stocks that settled there before the 11th century: pre-Celtic, Celtic, Anglo-Saxon, Viking and Norman. Although Celtic languages are partially spoken in Scotland, Cornwall, and Northern Ireland, the predominant language overall is English. In North and West Wales, Welsh is widely spoken as a first language, but much less so in the South East of the country, where English is the predominant language.
History.
During the Industrial Revolution, the life expectancy of children increased dramatically. The proportion of the children born in London who died before the age of five decreased from 74.5 per thousand in 1730–1749 to 31.8 per thousand in 1810–1829. According to Robert Hughes in "The Fatal Shore", the population of England and Wales, which had remained steady at 6 million from 1700 to 1740, rose dramatically after 1740.
The first Census in 1801 revealed that the population of Great Britain was 10.5 million. In 1800 the population of Ireland was between 4.5 and 5.5 million.
The 1841 UK Census counted the population of England and Wales to be 15.9 million. Ireland's population was 8.2 million in 1841. The population of Scotland was 2.6 million.
The Great Irish Famine, which began in the 1840s, caused the deaths of one million Irish people, and caused over a million to emigrate. Mass emigration became entrenched as a result of the famine and the population continued to decline until the mid-20th century.
The population of England had almost doubled from 16.8 million in 1851 to 30.5 million in 1901. Ireland’s population decreased rapidly, from 8.2 million in 1841 to less than 4.5 million in 1901.
Population.
The estimated population of the United Kingdom in the 2011 census was 63.182 million of whom 31.029m were men and 32.153m women.
Based on the 2011 census the population of England was 53.012m (84% of the UK), Scotland was estimated at 5.295m (8.4%), Wales was 3.063m (4.8%) and Northern Ireland 1,811m (2.9%).
More recent estimates for mid-2013 suggest that there are 53.9m in England, 5.3m in Scotland, 3.1m in Wales and 1.8m in Northern Ireland.
The UK Office for National Statistics' 2012-based National Population Projections indicated that if recent trends continue, the UK's population would increase to 73.3 million people by 2037. This is an average annual growth rate of 0.6% per annum. Projected population growth rates over that period vary for the different parts of the UK. For England it is 0.6% per annum, for Scotland and Wales, 0.3%, and for Northern Ireland 0.4%.
There are 13 urban areas which exceed 500,000 inhabitants, these being centred on London, Birmingham, Glasgow, Leeds and Bradford, Southampton and Portsmouth, Sheffield, Liverpool, Leicester, Manchester, Belfast, Bristol, Newcastle upon Tyne and Nottingham.
Age structure.
The key features of the age distribution profile for the UK population, as measured in the 2011 Census, were summarised in December 2012 by the Office for National Statistics in terms of peaks and wide bands of the pyramid reflecting high numbers of births in previous years particularly for people aged 60–64 born following the Second World War and those aged 40–49, born during the 1960s baby boom. There is a smaller number of children aged five to nine years than ten years ago which is a consequence of low numbers of births at the beginning of the 21st century, and the broadening of the pyramid in the 0–4 years category is due to a higher numbers of births in recent years. At higher ages, females outnumber males, reflecting the higher life expectancy of females. At lower ages there are more males than females, reflecting that there are slightly more boys than girls born each year.
The UK Office for National Statistics' 2012-based National Population Projections suggest that the average (median) population age would rise from 39.7 in 2012 to 42.8 in 2037 if current demographic trends continued.
Age structure for each five year band
 Age structure for men and women in 2011
UK Population change over time.
Population levels at census dates
Population density based on a calculated 243,820 km² area of the United Kingdom
Vital statistics since 1960.
Population density based on a calculated 243,820 km² area of the United Kingdom.
Current vital statistics.
Source:
Social issues.
Fertility.
In 2012 the UK's total fertility rate (TFR) was 1.92 children per woman, below the replacement rate, which in the UK is 2.075. In 2001, the TFR was at a record low of 1.63, but it then increased every year till reaching a peak of 1.96 in 2008, before decreasing again. The TFR was considerably higher during the 1960s 'baby boom', peaking at 2.95 children per woman in 1964.
In 2010 and again in 2012, England and Wales's TFR rose to 1.94. In Scotland however TFR is lower: it decreased from 1.75 in 2010 to 1.67 in 2012. Northern Ireland has the highest TFR in the UK, standing at 2.02 in 2010 and even 2.03 in 2012.
The TFR for British residents also varies by country of birth. In England and Wales in 1996, people born in the UK had a TFR of 1.67, India 2.21 and Pakistan and Bangladesh 4.90, for example.
Statistics for 2013 live births in England and Wales:
Other statistics:
LGBT.
There are known difficulties in producing reliable estimates of the lesbian, gay, bisexual and transgender population.
The Integrated Household Survey, published by the Office for National Statistics, provides the following estimates for the adult UK population as at 2011:
• 1.1 per cent (approximately 545,000 adults at the time of the survey) identify as Gay or Lesbian,
• 0.4 per cent (approximately 220,000 adults) identify as Bisexual,
• 0.3 per cent identify as ‘Other’,
• 3.6 per cent of those surveyed replied ‘Don’t know’ or refused to answer the question,
• 0.6 per cent of those surveyed provided ‘No response’ to the question.
• an estimated 2.7 per cent of 16 to 24 year olds in the UK identify themselves as Gay, Lesbian or Bisexual compared with 0.4 per cent of 65 year olds and over.
Other sources provide alternative estimates of the population by by sexual orientation.
One British journal published in 2004 estimated that approximately 5% of the British population is gay. A government figure estimated in 2005 that there are 3.6 million gay people in Britain comprising 6 percent of the population., though a report by the Equality and Human Rights Commission described that estimate as 'of questionable validity' when set against available survey estimates.
The (GIRES) estimated in 2009 that "56,000 might potentially be transsexual people". They note that it's very difficult to make a reliable estimate. This would be 0.09% of the population at the time.
It has also been estimated that approximately 0.4% of the UK population are nonbinary.
Ethnicity.
Census estimate for the main ethnic group categories
Note:<br>
Religion.
The traditional religion in the United Kingdom is Christianity. In England the established church is the Church of England (Anglican). In Scotland, the Church of Scotland (a Presbyterian Church) is regarded as the 'national church' but there is not an established church.
In Wales there is no established church, with the Church in Wales having been disestablished in 1920. Likewise, in Ireland the Church of Ireland was disestablished in 1871. In Northern Ireland and similarly in parts of Scotland, there is a sectarian divide between Roman Catholic and Protestant communities.
The table below shows data regarding religion for the 2001 and 2011 censuses:
These figures represent a decrease of 12% in the number of people identifying themselves as Christian in the 10-year period from 2001 to 2011, and an increase of 10% in the number of people stating that they have no religion.
In the 2011 Census, rather than select one of the specified religions offered on the Census form, many people chose to write in their own religion. Some of these religions were reassigned to one of the main religions offered. In England and Wales, 241,000 people belonged to religious groups which did not fall into any of the main religions. The largest of these were Pagans (57,000) and Spiritualists (39,000). The census also recorded 177,000 people stating their religion as Jedi Knight. These returns were classified as "No religion", along with Atheist, Agnostic, Heathen and those who ticked "Other" but did not write in any religion.
An Office for National Statistics survey of 450,000 Britons in 2010 found that 71% are Christian, 4% are Muslim and 21% have no religious affiliation.
Languages.
The United Kingdom's de facto official language is English which is spoken as a first language by 95% of the population. Six regional languages; Scots, Ulster-Scots, Welsh, Cornish, Irish and Scottish Gaelic are protected under the European Charter for Regional or Minority Languages. Abilities in these languages (other than Cornish) for those aged three and above were recorded in the UK census 2011 as follows.
Cornish is spoken by around 2,500 people. In the 2011 census 464 respondents aged three and over in Cornwall said that Cornish was their main language, amounting to 0.09% of the total population of Cornwall aged three and over.
After English, Polish was the second most common language given in the United Kingdom census 2011. 618,091 respondents aged three and over said that Polish was their main language, amounting to 1.01% of the total population of the United Kingdom aged three and over.
The French language is spoken in some parts of the Channel Islands although the islands, like the Isle of Man, are not part of the United Kingdom.
British Sign Language is also common.
National Identity.
Respondents to the 2011 UK census gave their national identities as follows.
Education.
Each country of the United Kingdom has a separate education system, with power over education matters in Scotland, Wales and Northern Ireland being devolved.
The Secretary of State for Education and the Secretary of State for Innovation, Universities and Skills are responsible to the UK Parliament for education in England. Depending on the status of state schools control of day-to-day administration and funding may either be the responsibility of the local education authorities or the school's own governing institution. Universal state education in England and Wales was introduced for primary level in 1870 and secondary level in 1900. Education is mandatory from ages five to sixteen (15 if born in late July or August). The majority of children are educated in state-sector schools, only a small proportion of which select on the grounds of academic ability. Despite a fall in actual numbers, the proportion of children in England attending private schools has risen to over 7%.
Just over half of students at the leading universities of Cambridge and Oxford had attended state schools. State schools which are allowed to select pupils according to intelligence and academic ability can achieve comparable results to the most selective private schools: out of the top ten performing schools in terms of GCSE results in 2006 two were state-run grammar schools. England has 4 Universities ranked amongst the top 10 in the 2011 THES - QS World University Rankings.
In Scotland, the Cabinet Secretary for Education and Lifelong Learning is responsible to the Scottish Parliament for education, with day-to-day administration and funding of state schools being the responsibility of Local Authorities. Scotland first legislated for universal provision of education in 1696. 
The proportion of children in Scotland attending private schools is just over 4% though it has been rising slowly in recent years. Scottish students who attend Scottish universities pay neither tuition fees nor graduate endowment charges as the fees were abolished in 2001 and the graduate endowment scheme was abolished in 2008.
The National Assembly for Wales has responsibility for education in Wales. A significant number of students in Wales are educated either wholly or largely through the medium of Welsh and lessons in the language are compulsory for all until the age of 16. There are plans to increase the provision of Welsh-medium schools as part of the policy of having a fully bilingual Wales.
The Northern Ireland Assembly is responsible for education in Northern Ireland though responsibility at a local level is administered by 5 Education and Library Boards covering different geographical areas.
The UK has some of the top universities in the world with Cambridge, Oxford and Imperial College ranked amongst the top 10 in the 2014-15 Times Higher Education World University Rankings.
Genetics.
The geneticist Stephen Oppenheimer carried out extensive research of Great Britain, finding that the Celtic and Anglo-Saxon influx had little effect, with the majority of British ethnicity tracing back from an ancient Palaeolithic Iberian migration, now represented by the Basques so that 75% of the modern British population could (in theory) trace their ancestry back 15,000 years.

</doc>
<doc id="31726" url="http://en.wikipedia.org/wiki?curid=31726" title="Politics of the United Kingdom">
Politics of the United Kingdom

The United Kingdom is a unitary democracy governed within the framework of a constitutional monarchy, in which the Monarch is the head of state and the Prime Minister of the United Kingdom is the head of government. Executive power is exercised by Her Majesty's Government, on behalf of and by the consent of the Monarch, as well as by the devolved Governments of Scotland and Wales, and the Northern Ireland Executive. Legislative power is vested in the two chambers of the Parliament of the United Kingdom, the House of Commons and the House of Lords, as well as in the Scottish parliament and Welsh and Northern Ireland assemblies. The judiciary is independent of the executive and the legislature. The highest court is the Supreme Court of the United Kingdom.
The UK political system is a multi-party system. Since the 1920s, the two largest political parties have been the Conservative Party and the Labour Party. Before the Labour Party rose in British politics, the Liberal Party was the other major political party along with the Conservatives. Though coalition and minority governments have been an occasional feature of parliamentary politics, the first-past-the-post electoral system used for general elections tends to maintain the dominance of these two parties, though each has in the past century relied upon a third party such as the Liberal Democrats to deliver a working majority in Parliament. A Conservative-Liberal Democrat coalition government held office from 2010 until 2015, the first coalition since 1945. The coalition ended following Parliamentary elections on May 7, 2015, in which the Conservative Party won an outright majority of 331 seats in the House of Commons, while their coalition partners lost all but eight seats.
With the partition of Ireland, Northern Ireland received home rule in 1920, though civil unrest meant direct rule was restored in 1972. Support for nationalist parties in Scotland and Wales led to proposals for devolution in the 1970s though only in the 1990s did devolution actually happen. Today, Scotland, Wales and Northern Ireland each possess a legislature and executive, with devolution in Northern Ireland being conditional on participation in certain all-Ireland institutions. The United Kingdom remains responsible for non-devolved matters and, in the case of Northern Ireland, co-operates with the Republic of Ireland.
It is a matter of dispute as to whether increased autonomy and devolution of executive and legislative powers has contributed to the increase in support for independence. The principal pro-independence party, the Scottish National Party, became a minority government in 2007 and then went on to win an overall majority of MSPs at the 2011 Scottish parliament elections and forms the Scottish Government administration. A 2014 referendum on independence led to a rejection of the proposal, but with 45% voting to secede. In Northern Ireland, the largest Pro-Belfast Agreement party, Sinn Féin, not only advocates Northern Ireland's unification with the Republic of Ireland, but also abstains from taking their elected seats in the Westminster government, as this would entail taking a pledge of allegiance to the British monarch.
The constitution of the United Kingdom is uncodified, being made up of constitutional conventions, statutes and other elements such as EU law. This system of government, known as the "Westminster system", has been adopted by other countries, especially those that were formerly parts of the British Empire.
The United Kingdom is also responsible for several dependencies, which fall into two categories: the Crown dependencies, in the immediate vicinity of the UK, and British Overseas Territories, which originated as colonies of the British Empire.
The Crown.
The British Monarch, currently Queen Elizabeth II, is the Chief of State of the United Kingdom. Though she takes little direct part in government, the Crown remains the fount in which ultimate executive power over Government lies. These powers are known as Royal Prerogative and can be used for a vast amount of things, such as the issue or withdrawal of passports, to the dismissal of the Prime Minister or even the Declaration of War. The powers are delegated from the Monarch personally, in the name of the Crown, and can be handed to various ministers, or other Officers of the Crown, and can purposely bypass the consent of Parliament.
The head of Her Majesty's Government; the Prime Minister, also has weekly meetings with the sovereign, where she may express her feelings, warn, or advise the Prime Minister in the Government's work.
According to the uncodified constitution of the United Kingdom, the monarch has the following powers:
Domestic Powers
Foreign Powers
Executive.
Executive power in the United Kingdom is exercised by the Sovereign, Queen Elizabeth II, via Her Majesty's Government and the devolved national authorities - the Scottish Government, the Welsh Assembly Government and the Northern Ireland Executive.
The United Kingdom Government.
The monarch appoints a Prime Minister as the head of Her Majesty's Government in the United Kingdom, guided by the strict convention that the Prime Minister should be the member of the House of Commons most likely to be able to form a Government with the support of that House. In practice, this means that the leader of the political party with an absolute majority of seats in the House of Commons is chosen to be the Prime Minister. If no party has an absolute majority, the leader of the largest party is given the first opportunity to form a coalition. The Prime Minister then selects the other Ministers which make up the Government and act as political heads of the various Government Departments. About twenty of the most senior government ministers make up the Cabinet and approximately 100 ministers in total comprise the government. In accordance with constitutional convention, all ministers within the government are either Members of Parliament or peers in the House of Lords.
As in some other parliamentary systems of government (especially those based upon the Westminster System), the executive (called "the government") is drawn from and is answerable to Parliament - a successful vote of no confidence will force the government either to resign or to seek a parliamentary dissolution and a general election. In practice, members of parliament of all major parties are strictly controlled by whips who try to ensure they vote according to party policy. If the government has a large majority, then they are very unlikely to lose enough votes to be unable to pass legislation.
The Prime Minister and the Cabinet.
The Prime Minister is the most senior minister in the Cabinet. They are responsible for chairing Cabinet meetings, selecting Cabinet ministers (and all other positions in Her Majesty's government), and formulating government policy. The Prime Minister is the de facto leader of the UK government; since they exercise executive functions that are nominally vested in the sovereign (by way of the Royal Prerogatives). Historically, the British monarch was the sole source of executive powers in the government. However, following the rule of the Hanoverian monarchs, an arrangement of a "Prime Minister" chairing and leading the Cabinet began to emerge. Over time, this arrangement became the effective executive branch of government, as it assumed the day-to-day functioning of the British government away from the sovereign.
Theoretically, the Prime Minister is "primus inter pares" (,i.e. Latin for "first among equals") among their Cabinet colleagues. While the Prime Minister is the senior Cabinet Minister, they are theoretically bound to make executive decisions in a collective fashion with the other Cabinet ministers. The Cabinet, along with the PM, consists of Secretaries of State from the various government departments, the Lord High Chancellor, the Lord Privy Seal, the President of the Board of Trade, the Chancellor of the Duchy of Lancaster and Ministers without portfolio. Cabinet meetings are typically held weekly, while Parliament is in session.
Government departments and the Civil Service.
The Government of the United Kingdom contains a number of ministries known mainly, though not exclusively as departments, e.g., Department for Education. These are politically led by a Government Minister who is often a Secretary of State and member of the Cabinet. He or she may also be supported by a number of junior Ministers. In practice, several government departments and Ministers have responsibilities that cover England alone, with devolved bodies having responsibility for Scotland, Wales and Northern Ireland, (for example - the Department of Health), or responsibilities that mainly focus on England (such as the Department for Education).
Implementation of the Minister's decisions is carried out by a permanent politically neutral organisation known as the civil service. Its constitutional role is to support the Government of the day regardless of which political party is in power. Unlike some other democracies, senior civil servants remain in post upon a change of Government. Administrative management of the Department is led by a head civil servant known in most Departments as a Permanent Secretary. The majority of the civil service staff in fact work in executive agencies, which are separate operational organisations reporting to Departments of State.
"Whitehall" is often used as a metonym for the central core of the Civil Service. This is because most Government Departments have headquarters in and around the former Royal Palace Whitehall.
Devolved national administrations.
Scottish Government.
The Scottish Government is responsible for all issues that are not explicitly reserved to the United Kingdom Parliament at Westminster, by the Scotland Act; including NHS Scotland, education, justice, rural affairs, and transport. It manages an annual budget of more than £25 billion. The government is led by the First Minister, assisted by various Ministers with individual portfolios and remits. The Scottish Parliament nominates a Member to be appointed as First Minister by the Queen. The First Minister then appoints their Ministers (now known as Cabinet Secretaries) and junior Ministers, subject to approval by the Parliament. The First Minister, the Ministers (but not junior ministers), the Lord Advocate and Solicitor General are the Members of the 'Scottish Executive', as set out in the Scotland Act 1998. They are collectively known as "the Scottish Ministers".
Welsh Government.
The Welsh Government and the National Assembly for Wales have more limited powers than those devolved to Scotland, although following the passing of the Government of Wales Act 2006 and the Welsh devolution referendum, 2011, the Assembly can now legislate in some areas through an Act of the National Assembly for Wales. Following the 2011 election, Welsh Labour held exactly half of the seats in the Assembly, falling just short of an overall majority. A Welsh Labour Government was subsequently formed headed by Carwyn Jones.
Northern Ireland Executive.
The Northern Ireland Executive and Assembly have powers closer to those already devolved to Scotland. The Northern Ireland Executive is led by a diarchy, currently First Minister Peter Robinson (Democratic Unionist Party) and deputy First Minister Martin McGuinness (Sinn Féin).
Legislatures.
The UK Parliament is the supreme legislative body in the United Kingdom (i.e., there is parliamentary sovereignty), and Government is drawn from and answerable to it. Parliament is bicameral, consisting of the House of Commons and the House of Lords. There is also a devolved Scottish Parliament and devolved Assemblies in Wales and Northern Ireland, with varying degrees of legislative authority.
UK Parliament.
House of Commons.
The Countries of the United Kingdom are divided into parliamentary constituencies of broadly equal population by the four Boundary Commissions. Each constituency elects a Member of Parliament (MP) to the House of Commons at General Elections and, if required, at by-elections. As of 2010 there are 650 constituencies (there were 646 before that year's general election). Of the 650 MPs, all but one - Lady Sylvia Hermon - belong to a political party.
In modern times, all Prime Ministers and Leaders of the Opposition have been drawn from the Commons, not the Lords. Alec Douglas-Home resigned from his peerages days after becoming Prime Minister in 1963, and the last Prime Minister before him from the Lords left in 1902 (the Marquess of Salisbury).
One party usually has a majority in Parliament, because of the use of the First Past the Post electoral system, which has been conducive in creating the current two party system. The monarch normally asks a person commissioned to form a government simply whether it can "survive" in the House of Commons, something which majority governments are expected to be able to do. In exceptional circumstances the monarch asks someone to 'form a government' "with a parliamentary minority" which in the event of no party having a majority requires the formation of a coalition government. This option is only ever taken at a time of national emergency, such as war-time. It was given in 1916 to Andrew Bonar Law, and when he declined, to David Lloyd George and in 1940 to Winston Churchill. A government is not formed by a vote of the House of Commons, it is a commission from the monarch. The House of Commons gets its first chance to indicate confidence in the new government when it votes on the Speech from the Throne (the legislative programme proposed by the new government).
House of Lords.
The House of Lords was previously a largely hereditary aristocratic chamber, although including life peers, and Lords Spiritual. It is currently mid-way through extensive reforms, the most recent of these being enacted in the House of Lords Act 1999. The house consists of two very different types of member, the Lords Temporal and Lords Spiritual. Lords Temporal include appointed members (life peers with no hereditary right for their descendants to sit in the house) and ninety-two remaining hereditary peers, elected from among, and by, the holders of titles which previously gave a seat in the House of Lords. The Lords Spiritual represent the established Church of England and number twenty-six: the Five Ancient Sees (Canterbury, York, London, Winchester and Durham), and the 21 next-most senior bishops.
The House of Lords currently acts to review legislation initiated by the House of Commons, with the power to propose amendments, and can exercise a suspensive veto. This allows it to delay legislation if it does not approve it for twelve months. However, the use of vetoes is limited by convention and by the operation of the Parliament Acts 1911 and 1949: the Lords may not veto the "money bills" or major manifesto promises (see Salisbury convention). Persistent use of the veto can also be overturned by the Commons, under a provision of the Parliament Act 1911. Often governments will accept changes in legislation in order to avoid both the time delay, and the negative publicity of being seen to clash with the Lords. However the Lords still retain a full veto in acts which would extend the life of Parliament beyond the 5-year term limit introduced by the Parliament Act 1911.
The Constitutional Reform Act 2005 outlined plans for a Supreme Court of the United Kingdom to replace the role of the Law Lords.
The House of Lords was replaced as the final court of appeal on civil cases within the United Kingdom on 1 October 2009, by the Supreme Court of the United Kingdom.
Devolved national legislatures.
Though the UK parliament remains the sovereign parliament, Scotland has a parliament and Wales and Northern Ireland have assemblies. De jure, each could have its powers broadened, narrowed or changed by an Act of the UK Parliament. The UK is a unitary state with a devolved system of government. This contrasts with a federal system, in which sub-parliaments or state parliaments and assemblies have a clearly defined constitutional "right" to exist and a "right" to exercise certain constitutionally guaranteed and defined functions and cannot be unilaterally abolished by Acts of the central parliament.
All three devolved institutions are elected by proportional representation: the Additional Member System is used in Scotland and Wales, and Single Transferable Vote is used in Northern Ireland.
England, therefore, is the only country in the UK not to have its own devolved parliament. However, senior politicians of all main parties have voiced concerns in regard to the West Lothian Question, which is raised where certain policies for England are set by MPs from all four constituent nations whereas similar policies for Scotland or Wales might be decided in the devolved assemblies by legislators from those countries alone. Alternative proposals for English regional government have stalled, following a poorly received referendum on devolved government for the North East of England, which had hitherto been considered the region most in favour of the idea, with the exception of Cornwall, where there is widespread support for a Cornish Assembly, including all five Cornish MPs. England is therefore governed according to the balance of parties across the whole of the United Kingdom.
The government has no plans to establish an English parliament or assembly although several pressure groups are calling for one. One of their main arguments is that MPs (and thus voters) from different parts of the UK have inconsistent powers. Currently an MP from Scotland can vote on legislation which affects only England but MPs from England (or indeed Scotland) cannot vote on matters devolved to the Scottish parliament. Indeed, the former Prime Minister Gordon Brown, who is an MP for a Scottish constituency, introduced some laws that only affect England and not his own constituency. This anomaly is known as the West Lothian question.
The policy of the UK Government in England was to establish elected regional assemblies with no legislative powers. The London Assembly was the first of these, established in 2000, following a referendum in 1998, but further plans were abandoned following rejection of a proposal for an elected assembly in North East England in a referendum in 2004. Unelected regional assemblies remain in place in eight regions of England.
Scottish Parliament.
The Scottish Parliament is the national, unicameral legislature of Scotland, located in the Holyrood area of the capital Edinburgh. The Parliament, informally referred to as "Holyrood" (cf. "Westminster"), is a democratically elected body comprising 129 members who are known as Members of the Scottish Parliament, or MSPs. Members are elected for four-year terms under the mixed member proportional representation system. As a result, 73 MSPs represent individual geographical constituencies elected by the plurality ("first past the post") system, with a further 56 returned from eight additional member regions, each electing seven MSPs.
The current Scottish Parliament was established by the Scotland Act 1998 and its first meeting as a devolved legislature was on 12 May 1999. The parliament has the power to pass laws and has limited tax-varying capability. Another of its roles is to hold the Scottish Government to account. The "devolved matters" over which it has responsibility include education, health, agriculture, and justice. A degree of domestic authority, and all foreign policy, remains with the UK Parliament in Westminster.
The public take part in Parliament in a way that is not the case at Westminster through Cross-Party Groups on policy topics which the interested public join and attend meetings of alongside Members of the Scottish Parliament (MSPs).
The resurgence in Celtic language and identity, as well as 'regional' politics and development, has contributed to forces pulling against the unity of the state. This was clearly demonstrated when - although some argue it was influenced by general public dillusionment with Labour - the Scottish National Party (SNP) became the largest party in the Scottish Parliament by one seat.
Alex Salmond (leader of SNP) has since made history by becoming the first First Minister of Scotland from a party other than Labour. The SNP governed as a minority administration at Holyrood following the 2007 Scottish Parliament election. Nationalism (support for breaking up the UK) has experienced a dramatic rise in popularity in recent years, with a pivotal moment coming at the 2011 Scottish Parliament election where the SNP capitalised on the collapse of the Liberal Democrat support to improve on their 2007 performance to win the first ever outright majority at Holyrood (despite the voting system being specifically designed to prevent majorities), with Labour remaining the largest opposition party. 
This election result prompted the leader of the three main opposition parties to resign. Iain Gray was succeeded as Scottish Labour leader by Johann Lamont, Scottish Conservative and Unionist leader, Annabel Goldie was replaced by Ruth Davidson, and Tavish Scott, leader of the Scottish Liberal Democrats was replaced by Willie Rennie. 
A major SNP manifesto pledge was to hold a referendum on Scottish Independence, which was duly granted by the UK Government and held on the 18th September 2014. When the nationalists came to power in 2011, opinion polls placed support for independence at around 31%, but in 2014, 45% voted to leave the union. In the wake of the referendum defeat, membership of the SNP surged to over 100,000, overtaking the Liberal Democrats as the third largest political party in the UK by membership, and in the general election of May 2015 the SNP swept the board and took 56 of the 59 Westminster constituencies in Scotland (far surpassing their previous best of 11 seats in the late 1970's) and winning more than 50% of the Scottish vote.
Alex Salmond resigned as First Minister of Scotland and leader of the SNP following the country's rejection of independence in September 2014, and was succeeded in both roles by the deputy First Minister and deputy leader of the SNP, Nicola Stugeon. Also in the wake of the referendum, Scottish Labour leader, Johann Lamont, stood down and Jim Murphy was elected to replace her. As Mr. Murphy is not currently an MSP, the Labour group in the Scottish Parliament is led by their deputy leader in Scotland, Kezia Dugdale.
National Assembly for Wales.
The National Assembly for Wales is the devolved assembly with power to make legislation in Wales. The Assembly comprises 60 members, who are known as Assembly Members, or AMs (Welsh: "Aelod y Cynulliad"). Members are elected for four-year terms under an additional members system, where 40 AMs represent geographical constituencies elected by the plurality system, and 20 AMs from five electoral regions using the d'Hondt method of proportional representation.
The Assembly was created by the Government of Wales Act 1998, which followed a referendum in 1997. On its creation, most of the powers of the Welsh Office and Secretary of State for Wales were transferred to it. The Assembly had no powers to initiate primary legislation until limited law-making powers were gained through the Government of Wales Act 2006. Its primary law-making powers were enhanced following a Yes vote in the referendum on 3 March 2011, making it possible for it to legislate without having to consult the UK parliament, nor the Secretary of State for Wales in the 20 areas that are devolved.
Northern Ireland Assembly.
The government of Northern Ireland was established as a result of the 1998 Good Friday Agreement. This created the Northern Ireland Assembly. The Assembly is a unicameral body consisting of 108 members elected under the Single Transferable Vote form of proportional representation. The Assembly is based on the principle of power-sharing, in order to ensure that both communities in Northern Ireland, unionist and nationalist, participate in governing the region. It has power to legislate in a wide range of areas and to elect the Northern Ireland Executive (cabinet). It sits at Parliament Buildings at Stormont in Belfast.
The Assembly has authority to legislate in a field of competences known as "transferred matters". These matters are not explicitly enumerated in the Northern Ireland Act 1998 but instead include any competence not explicitly retained by the Parliament at Westminster. Powers reserved by Westminster are divided into "excepted matters", which it retains indefinitely, and "reserved matters", which may be transferred to the competence of the Northern Ireland Assembly at a future date. Health, criminal law and education are "transferred" while royal relations are all "excepted".
While the Assembly was in suspension, due to issues involving the main parties and the Provisional Irish Republican Army (IRA), its legislative powers were exercised by the UK government, which effectively had power to legislate by decree. Laws that would normally be within the competence of the Assembly were passed by the UK government in the form of Orders-in-Council rather than legislative acts.
There has been a significant decrease in violence over the last twenty years, though the situation remains tense, with the more hard-line parties such as Sinn Féin and the Democratic Unionist Party now holding the most parliamentary seats (see Demographics and politics of Northern Ireland).
Judiciary.
The United Kingdom does not have a single legal system due to it being created by the political union of previously independent countries with the terms of the Treaty of Union guaranteeing the continued existence of Scotland's separate legal system. Today the UK has three distinct systems of law: English law, Northern Ireland law and Scots law. Recent constitutional changes saw a new Supreme Court of the United Kingdom come into being in October 2009 that took on the appeal functions of the Appellate Committee of the House of Lords. The Judicial Committee of the Privy Council, comprising the same members as the Supreme Court, is the highest court of appeal for several independent Commonwealth countries, the UK overseas territories, and the British crown dependencies.
England, Wales and Northern Ireland.
Both English law, which applies in England and Wales, and Northern Ireland law are based on common-law principles. The essence of common-law is that law is made by judges sitting in courts, applying their common sense and knowledge of legal precedent ("stare decisis") to the facts before them. The Courts of England and Wales are headed by the Senior Courts of England and Wales, consisting of the Court of Appeal, the High Court of Justice (for civil cases) and the Crown Court (for criminal cases). The Supreme Court of the United Kingdom is the highest court in the land for both criminal and civil cases in England, Wales, and Northern Ireland and any decision it makes is binding on every other court in the hierarchy.
Scotland.
Scots law, a hybrid system based on both common-law and civil-law principles, applies in Scotland. The chief courts are the Court of Session, for civil cases, and the High Court of Justiciary, for criminal cases. The Supreme Court of the United Kingdom serves as the highest court of appeal for civil cases under Scots law. Sheriff courts deal with most civil and criminal cases including conducting criminal trials with a jury, known that as Sheriff solemn Court, or with a Sheriff and no jury, known as (Sheriff summary Court). The Sheriff courts provide a local court service with 49 Sheriff courts organised across six Sheriffdoms.
Electoral systems.
Various electoral systems are used in the UK:
The use of the first-past-the-post to elect members of Parliament is unusual among European nations. The use of the system means that when three or more candidates receive a significant share of the vote, MPs are often elected from individual constituencies with a plurality (receiving more votes than any other candidate), but not an absolute majority (50 percent plus one vote).
Elections and political parties in the United Kingdom are affected by Duverger's law, the political science principle which states that plurality voting systems, such as first-past-the-post, tend to lead to the development of two-party systems. The UK, like several other states, has sometimes been called a "two-and-a-half" party system, because parliamentary politics is dominated by the Labour Party and Conservative Party, with the Liberal Democrats holding a significant number of seats (but still substantially less than Labour and the Conservatives), and several small parties (some of them regional or nationalist) trailing far behind in number of seats.
In the last few general elections, voter mandates for Westminster in the 30–40% ranges have been swung into 60% parliamentary majorities. No single party has won a majority of the popular vote since the Third National Government of Stanley Baldwin in 1935. On two occasions since World War II – 1951 and February 1974 – a party that came in second in the popular vote actually came out with the larger number of seats.
Electoral reform for parliamentary elections have been proposed many times. The Jenkins Commission report in October 1998 suggested implementing the Alternative Vote Top-up (also called alternative vote plus or AV+) in parliamentary elections. Under this proposal, most MPs would be directly elected from constituencies by the alternative vote, with a number of additional members elected from "top-up lists." However, no action was taken by the Labour government at the time. There are a number of groups in the UK campaigning for electoral reform, including the Electoral Reform Society, Make Votes Count Coalition and Fairshare.
The 2010 general election resulted in a hung parliament (no single party being able to command a majority in the House of Commons). This was only the second general election since World War II to return a hung parliament, the first being the February 1974 election. The Conservatives gained the most seats (ending 13 years of Labour government) and the largest percentage of the popular vote, but fell 20 seats short of a majority.
The Conservatives and Liberal Democrats entered into a new coalition government, headed by David Cameron. Under the terms of the coalition agreement the government committed itself to hold a referendum in May 2011 on whether to change parliamentary elections from first-past-the-post to AV. Electoral reform was a major priority for the Liberal Democrats, who favour proportional representation but were able to negotiate only a referendum on AV with the Conservatives. The coalition partners campaigned on opposite sides, with the Liberal Democrats supporting AV and the Conservatives opposing it. The referendum resulted in the Conservative's favour and the first-past-the-post system was maintained.
Political parties.
There are two main parties in the United Kingdom: the Conservative Party, and the Labour Party. The Scottish National Party is the third party in terms of representatives elected and party membership.
The modern Conservative Party was founded in 1834 and is an outgrowth of the Tory movement or party, which began in 1678. Today it is still colloquially referred to as the Tory Party and its members as "Tories". The Liberal Democrats were formed in 1988 by a merger of the Liberal Party and the Social Democratic Party (SDP), a Labour breakaway formed in 1981. The Liberals and SDP had contested elections together as the SDP–Liberal Alliance for seven years before. The modern Liberal Party had been founded in 1859 as an outgrowth of the Whig movement or party (which began at the same time as the Tory party and was its historical rival) as well as the Radical and Peelite tendencies.
The Liberal Party was one of the two dominant parties (along with the Conservatives) from its founding until the 1920s, when it rapidly declined and was supplanted on the left by the Labour Party, which was founded in 1900 and formed its first government in 1924. Since that time, the Labour and Conservatives parties have been dominant, with the Liberals (later Liberal Democrats) being the third largest party until 2015, when they lost 48 of their 57 seats, while the Scottish National Party went from 6 seats to 56. Founded in 1934, the SNP advocates for Scottish independence and has had continuous representation in Parliament since 1967. The SNP currently leads a majority government in the Scottish Parliament.
Minor parties also hold seats in parliament:
In the most recent general election in 2015, the Conservatives, who in the preious parliament had led a coalition with the Liberal Democrats, gained a majority of seats and went on to form the government.
Conservatives (Tories).
The Conservative Party won the largest number of seats at the 2015 general election, returning 331 MPs, enough to make an overall majority, and went on to form the government.
The Conservative party can trace its origin back to 1662, with the Court Party and the Country Party being formed in the aftermath of the English Civil War. The Court Party soon became known as the Tories, a name that has stuck despite the official name being 'Conservative'. The term "Tory" originates from the Exclusion Bill crisis of 1678-1681 - the Whigs were those who supported the exclusion of the Roman Catholic Duke of York from the thrones of England, Ireland and Scotland, and the Tories were those who opposed it. Both names were originally insults: a "whiggamore" was a horse drover (See Whiggamore Raid), and a "tory" ("Tóraidhe") was an Irish term for an outlaw, later applied to Irish Confederates and Irish Royalists, during the Wars of the Three Kingdoms.
Generally, the Tories were associated with lesser gentry and the Church of England, while Whigs were more associated with trade, money, larger land holders (or "land magnates"), expansion and tolerance of Catholicism.
The Rochdale Radicals were a group of more extreme reformists who were also heavily involved in the cooperative movement. They sought to bring about a more equal society, and are considered by modern standards to be left-wing.
After becoming associated with repression of popular discontent in the years after 1815, the Tories underwent a fundamental transformation under the influence of Robert Peel, himself an industrialist rather than a landowner, who in his 1834 "Tamworth Manifesto" outlined a new "Conservative" philosophy of reforming ills while conserving the good.
Though Peel's supporters subsequently split from their colleagues over the issue of free trade in 1846, ultimately joining the Whigs and the Radicals to form what would become the Liberal Party, Peel's version of the party's underlying outlook was retained by the remaining Tories, who adopted his label of Conservative as the official name of their party.
The crushing defeat of the 1997 election saw the Conservative Party lose over half their seats from 1992 and saw the party re-align with public perceptions of them.
In 2008, the Conservative Party formed a pact with the Ulster Unionist Party to select joint candidates for European and House of Commons elections; this angered the DUP as by splitting the Unionist vote, republican parties will be elected in some areas.
After thirteen years as the official opposition, the Party returned to power as part of a coalition with the Liberal Democrats in 2010, going on to form a majority government in 2015.
Historically, the party has been the mainland party most pre-occupied by British Unionism, as attested to by the party's full name, the Conservative & Unionist Party. This resulted in the merger between the Conservatives and Joseph Chamberlain's Liberal Unionist Party, composed of former Liberals who opposed Irish home rule. The unionist tendency is still in evidence today, manifesting sometimes as a scepticism or opposition to devolution, firm support for the continued existence of the United Kingdom in the face of movements advocating independence from the UK, and a historic link with the cultural unionism of Northern Ireland.
Labour.
The Labour Party won the second largest number of seats in the House of Commons at the 2015 general election, with 232 MPs.
The history of the Labour party goes back to 1900 when a Labour Representation Committee was established which changed its name to "The Labour Party" in 1906. After the First World War, this led to the demise of the Liberal Party as the main reformist force in British politics. The existence of the Labour Party on the left of British politics led to a slow waning of energy from the Liberal Party, which has consequently assumed third place in national politics. After performing poorly in the elections of 1922, 1923 and 1924, the Liberal Party was superseded by the Labour Party as the party of the left.
Following two brief spells in minority governments in 1924 and 1929–1931, the Labour Party had its first true victory after World War II in the 1945 "khaki election". Throughout the rest of the twentieth century, Labour governments alternated with Conservative governments. The Labour Party suffered the "wilderness years" of 1951-1964 (three straight General Election defeats) and 1979-1997 (four straight General Election defeats).
During this second period, Margaret Thatcher, who became leader of the Conservative party in 1975, made a fundamental change to Conservative policies, turning the Conservative Party into an economic neoliberal party. In the General Election of 1979 she defeated James Callaghan's troubled Labour government after the winter of discontent.
For most of the 1980s and the 1990s, Conservative governments under Thatcher and her successor John Major pursued policies of privatization, anti-trade-unionism, and, for a time, monetarism, now known collectively as Thatcherism.
The Labour Party elected left-winger Michael Foot as their leader after their 1979 election defeat, and he responded to dissatisfaction with the Labour Party by pursuing a number of radical policies developed by its grass-roots members. In 1981 several right-wing Labour MPs formed a breakaway group called the Social Democratic Party (SDP), a move which split Labour and is widely believed to have made Labour unelectable for a decade. The SDP formed an alliance with the Liberal Party which contested the 1983 and 1987 general elections as a centrist alternative to Labour and the Conservatives. After some initial success, the SDP did not prosper (partly due to its unfavourable distribution of votes in the FPTP electoral system), and was accused by some of splitting the anti-Conservative vote.
The SDP eventually merged with the Liberal Party to form the Liberal Democrats in 1988. Support for the new party has increased since then, and the Liberal Democrats (often referred to as LibDems) in 1997 and 2001 gained an increased number of seats in the House of Commons.
The Labour Party was badly defeated in the Conservative landslide of the 1983 general election, and Michael Foot was replaced shortly thereafter by Neil Kinnock as leader. Kinnock expelled the far left Militant tendency group (now called the Socialist Party of England and Wales) and moderated many of the party's policies. Yet he was in turn replaced by John Smith after Labour defeats in the 1987 and 1992 general elections.
Tony Blair became leader of the Labour party after John Smith's sudden death from a heart attack in 1994. He continued to move the Labour Party towards the 'centre' by loosening links with the unions and embracing many of Margaret Thatcher's liberal economic policies. This, coupled with the professionalising of the party machine's approach to the media, helped Labour win a historic landslide in the 1997 General Election, after 18 years of Conservative government. Some observers say the Labour Party had by then morphed from a democratic socialist party to a social democratic party, a process which delivered three general election victories but alienated some of its core base - leading to the formation of the Socialist Labour Party (UK).
Scottish National Party.
The Scottish National Party won the third largest number of seats in the House of Commons at the 2015 general election, with 56 MPs.
The SNP has enjoyed parliamentary representation continuously since 1967. Following the 2007 Scottish parliament elections, the SNP emerged as the largest party with 47 MSPs and formed a minority government with Alex Salmond the First Minister. After the 2011 Scottish election, the SNP won enough seats to form a majority government.
Members of the Scottish National Party and Plaid Cymru work together as a single parliamentary group following a formal pact signed in 1986. This group currently has 59 MPs.
Liberal Democrats.
The Liberal Democrats won the joint-fourth largest number of seats at the 2015 general election, returning 8 MPs.
The Liberal Democrats were formed in 1988 by a merger of the Liberal Party with the Social Democratic Party, but can trace their origin back to the Whigs and the Rochdale Radicals who evolved into the Liberal Party. The term 'Liberal Party' was first used officially in 1868, though it had been in use colloquially for decades beforehand. The Liberal Party formed a government in 1868 and then alternated with the Conservative Party as the party of government throughout the late 19th century and early 20th century.
The Liberal Democrats are heavily a party on Constitutional and Political Reforms, including changing the voting system for General Elections (UK Alternative Vote referendum, 2011), abolishing the House of Lords and replacing it with a 300-member elected Senate, introducing Fixed Five Year Parliaments, and introducing a National Register of Lobbyists. They also support what they see as greater fairness and social mobility. In government the party promoted legislation introducing a pupil premium - funding for schools directed at the poorest students to give them an equal chance in life. They also supported same sex marriage and increasing the income tax threshold to £10,000, a pre-election manifesto commitment.
Northern Ireland parties.
The Democratic Unionist Party had 8 MPs elected at the 2015 election. Founded in 1971 by Ian Paisley, it has grown to become the larger of the two main unionist political parties in Northern Ireland. Other Northern Ireland parties represented at Westminster include the Social Democratic and Labour Party (3 MPs), the Ulster Unionist Party, the Alliance Party of Northern Ireland (1 MP) and Sinn Féin (4 MPs). Sinn Féin MPs refuse to take their seats and sit in a 'foreign' parliament.
Plaid Cymru.
Plaid Cymru has enjoyed parliamentary representation continuously since 1974 and had 3 MPs elected at the 2015 election. Following the 2007 Welsh Assembly elections, they joined Labour as the junior partner in a coalition government, but have fallen down to the third largest party in the Assembly after the 2011 Assembly elections, and become an opposition party.
Other parliamentary parties.
The Green Party of England and Wales kept its only MP, Caroline Lucas, in the 2015 General Election (it had previously had an MP in 1992; Cynog Dafis, Ceredigion, who was elected on a joint Plaid Cymru/Green Party ticket). It also has seats in the European Parliament, two seats on the London Assembly and around 120 local councillors.
The UK Independence Party (UKIP) has 1 MP and 24 seats in the European Parliament as well as seats in the House of Lords and a number of local councillors. UKIP also has a MLA in the Northern Ireland Assembly. UKIP has become an emerging alternative party among some voters, gaining the third largest share of the vote in the 2015 General Election and the largest share of the vote of any party (27%) in the 2014 European elections. In 2014 UKIP gained its first MP following the defection and re-election of Douglas Carswell in the 2014 Clacton by-election. They campaign mainly on issues such as reducing immigration and EU withdrawal.
The Respect party, a left-wing group that came out of the anti-war movement had one MP, George Galloway between 2010 and 2015. It also has a small number of seats on local councils across the country.
There are usually a small number of Independent politicians in parliament with no party allegiance. In modern times, this has usually occurred when a sitting member leaves their party, and some such MPs have been re-elected as independents. The only current Independent MP is Lady Hermon, previously of the Ulster Unionist Party. However, since 1950 only two new members have been elected as independents without having ever stood for a major party:
Non-Parliamentary political parties.
Other UK political parties exist, but generally do not succeed in returning MPs to Parliament.
The Scottish Green Party has 2 MSPs in the Scottish Parliament and a number of local councillors.
The Green Party (Ireland) has one MLAs in the Northern Ireland Assembly as well as local councillors.
The British National Party (BNP) won two seats in the European Parliament in the 2009 European Elections, but currently has none. It also has a number of councillors.
The Libertarian Party was founded in 2008 and has contested several local elections and parliamentary constituencies, gaining some local councillors.
The English Democrats, which wants a parliament for England, has some local councillors and had its candidate elected mayor of Doncaster in 2009.
Other parties include: the Socialist Labour Party (UK), the Free England Party, the Communist Party of Britain, the Socialist Party (England and Wales), the Socialist Workers Party, the Scottish Socialist Party, the Liberal Party, Mebyon Kernow (a Cornish nationalist party) in Cornwall, Veritas, the Communist Left Alliance (in Fife) and the Pirate Party UK.
Several local parties contest only within a specific area, a single county, borough or district. Examples include the Better Bedford Independent Party, which was one of the dominant parties in Bedford Borough Council and led by Bedford's former Mayor, Frank Branston. The most notable local party is Health Concern, which controlled a single seat in the UK Parliament from 2001 to 2010.
The Jury Team, launched in March 2009 and described as a "non-party party", is an umbrella organisation seeking to increase the number of independent members of both domestic and European members of Parliament in Great Britain.
The Official Monster Raving Loony Party was founded in 1983. The OMRLP are distinguished by having a deliberately bizarre manifesto, which contains things that seem to be impossible or too absurd to implement – usually to highlight what they see as real-life absurdities. In spite of (or perhaps because of) a reputation more satirical than serious, they have routinely been successful in local elections.
Current political landscape.
Since winning the largest number of seats and votes in the 2010 general election, the Conservatives under David Cameron are now behind the Labour Party now led by Ed Miliband. Their coalition partners have also experienced a decline in support in opinion polls. At the same time, support for the UK Independence Party and the Green Party of England and Wales has advanced, with some polls now placing them ahead of the Liberal Democrats. Furthermore, in the Eastleigh by-election UKIP advanced by 24% to take second place from the Conservatives, less than 5% behind the Liberal Democrats who retained the seat.
In Scotland, the Scottish National Party won the Scottish parliamentary election in May 2007 and gaining support in most national opinion polls since then. In July 2008, the SNP won the by-election in Glasgow East, winning the third safest Labour seat in Scotland with a swing of 22.54%. In October of the same year, after public predictions by the SNP's leader Alex Salmond that they would win another by-election in Glenrothes, the seat was won by Labour with a majority of 6,737 and an increased share of the vote.
In the 2009 European Parliament election, the SNP received for the first time the largest share of the European election vote in Scotland, and in Wales the Conservative Party received more votes than the Labour Party for the first time since 1918. In Northern Ireland, the Democratic Unionist Party's (DUP) received its worst ever European election result, and for the first time an Irish Republican party, Sinn Féin, topped the poll in Northern Ireland.
In the 2010 General election the SNP won the six seats they had won in the previous General election of 2005. They then won an overall majority of seats in the 2011 Scottish parliamentary election, retaining control of the Scottish government in the process.
Membership.
All political parties have membership schemes that allow members of the public to actively influence the policy and direction of the party to varying degrees, though particularly at a local level. Membership of British political parties is around 1% of the British electorate, which is lower than in all European countries except for Poland and Latvia. Overall membership to a political party has been in decline since the 1950s. In 1951, the Conservative Party had 2.2 million members, and a year later in 1952 the Labour Party reached their peak of 1 million members (of an electorate of around 34 million).
The table below details the membership numbers of political parties that have more than 5,000 members.
No data could be collected for the four parties of Northern Ireland: the DUP, UUP, SDLP, and Sinn Féin. However in January 1997, it was estimated that the UUP had 10-12,000 members, and the DUP had 5,000 members.
Local government.
The UK is divided into a variety of different types of Local Authorities, with different functions and responsibilities.
England has a mix of two-tier and single-tier councils in different parts of the country. In Greater London, a unique two-tier system exists, with power shared between the London borough councils, and the Greater London Authority which is headed by an elected mayor.
Unitary Authorities are used throughout Scotland, Wales and Northern Ireland.
European Union.
The United Kingdom first joined the European Economic Community in January 1973, and has remained a member of the European Union (EU) that it evolved into; UK citizens, and other EU citizens resident in the UK, elect 73 members to represent them in the European Parliament in Brussels and Strasbourg.
The UK's membership in the Union has been objected to over questions of sovereignty, and in recent years there have been divisions in both major parties over whether the UK should form greater ties within the EU, or reduce the EU's supranational powers. Opponents of greater European integration are known as "Eurosceptics", while supporters are known as "Europhiles". Division over Europe is prevalent in both major parties, although the Conservative Party is seen as most divided over the issue, both whilst in Government up to 1997 and after 2010, and between those dates as the opposition. However, the Labour Party is also divided, with conflicting views over UK adoption of the euro whilst in Government (1997–2010), although the party is largely in favour of further integration where in the country's interest.
UK nationalists have long campaigned against European integration. The strong showing of the eurosceptic UK Independence Party (UKIP) since the 2004 European Parliament elections has shifted the debate over UK relations with the EU.
In March 2008, Parliament decided to not hold a referendum on the ratification of the Treaty of Lisbon, signed in December 2007. This was despite the Labour government promising in 2004 to hold a referendum on the previously proposed Constitution for Europe.

</doc>
<doc id="31727" url="http://en.wikipedia.org/wiki?curid=31727" title="Economy of the United Kingdom">
Economy of the United Kingdom

 
The United Kingdom has the fifth-largest national economy (and second largest in Europe) measured by nominal GDP and ninth largest in the world (and third largest in Europe) measured by purchasing power parity (PPP). The UK economy comprises (in descending order of size) the economies of England, Scotland, Wales and Northern Ireland. In 2013 the UK was the fourth-largest exporter in the world and the fourth largest importer, and had the second largest stock of inward foreign direct investment and the second-largest stock of outward foreign direct investment. The UK is one of the world's most globalised economies.
The service sector dominates the UK economy, contributing around 78% of GDP; the financial services industry is particularly important and London is the world's largest financial centre (tied withNew York) The British aerospace industry is the second- or third-largest national aerospace industry depending on the method of measurement. The pharmaceutical industry plays an important role in the economy and the UK has the third-highest share of global pharmaceutical R&D. The automotive industry is also a major employer and exporter. The British economy is boosted by North Sea oil and gas production; its reserves were valued at an estimated £250 billion in 2007. There are significant regional variations in prosperity, with the South East of England and southern Scotland the richest areas per capita. London has the largest city GDP in Europe.
In the 18th century the UK was the first country to industrialise and during the 19th century had a dominant role in the global economy. From the late 19th century the Second Industrial Revolution in the United States and Germany presented an increasing economic challenge, and the costs of fighting World War I and World War II further weakened the UK's relative position. However it still maintains a significant role in the world economy, particularly in financial services and the knowledge economy.
More recently, the UK entered a recession during the financial crisis of 2007–08, its first for nearly two decades, and initially experienced a deeper downturn than all of the G7 except Japan. However, since 2013 the UK has been in a nascent economic recovery and is firmly in expansion territory. Since 2010, The Government has been pursuing an austerity program aimed at cutting the budget deficit. In the financial year 2009–2010 this was 11% of GDP, it is now 5%.
Government involvement in the British economy is primarily exercised by HM Treasury, headed by the Chancellor of the Exchequer, and the Department for Business, Innovation and Skills. Since 1979 management of the UK economy has followed a broadly laissez-faire approach. The Bank of England is the UK's central bank and its Monetary Policy Committee is responsible for setting interest rates. The currency of the UK is the pound sterling, which is also the world's third-largest reserve currency after the US dollar and the euro, and also the fourth-most-valued currency in the world, behind the Kuwaiti Dinar, Bahraini Dinar, and Omani Rial, and the most valued currency outside the Middle East. The UK is a member of the Commonwealth of Nations, the European Union, the G7, the G8, the G20, the International Monetary Fund, the Organisation for Economic Co-operation and Development, the World Bank, the World Trade Organisation, Asian Infrastructure Investment Bank and the United Nations.
History.
1945 to 1980.
Following the end of the Second World War, the United Kingdom enjoyed a long period without a major recession (from 1945 to 1973) and a rapid growth in prosperity in the 1950s and 1960s, with unemployment staying low and not exceeding 500,000 until the second half of the 1960s. According to the OECD, the annual rate of growth (percentage change) between 1960 and 1973 averaged 2.9%, although this figure was far behind the rates of other European countries such as France, West Germany and Italy.
However, following the 1973 oil crisis and the 1973–1974 stock market crash, the British economy fell into recession and the government of Edward Heath was ousted by the Labour Party under Harold Wilson, which had previously governed from 1964 to 1970. Wilson formed a minority government on 4 March 1974 after the general election on 28 February ended in a hung parliament. Wilson subsequently secured a three-seat majority in a second election in October that year.
The UK recorded weaker growth than many other European nations in the 1970s; even after the early 1970s recession ended, the economy was still blighted by rising unemployment and double-digit inflation, which exceeded 20% more than once after 1973 and was rarely below 10% after this date.
In 1976, the UK was forced to request a loan of £2.3 billion from the International Monetary Fund. The then Chancellor of the Exchequer Denis Healey was required to implement public spending cuts and other economic reforms in order to secure the loan, and for a while the British economy improved. However, following the Winter of Discontent, when the UK was hit by numerous public sector strikes, the government of James Callaghan lost a vote of no confidence in March 1979. This triggered the May 1979 general election which resulted in Margaret Thatcher's Conservative Party forming a new government.
1979 to 1997.
A new period of neo-liberal economics began in 1979 with the election of Margaret Thatcher who won the general election on 3 May that year to return the Conservative Party to government after five years of Labour government.
During the 1980s most state-owned enterprises were privatised, taxes cut, union reforms passed and markets deregulated. GDP fell 5.9% initially but growth subsequently returned and rose to 5% at its peak in 1988, one of the highest rates of any European nation.
However, Thatcher's modernisation of the British economy was far from trouble free; her battle against inflation resulted in a substantial increase in unemployment from 1.5 million in 1979 to over 3 million by the start of 1982, peaking at nearly 3.3 million in 1984. In spite of this, Thatcher was re-elected in June 1983 with a landslide majority, as the Labour Party suffered its worst general election result in decades and the recently formed SDP-Liberal Alliance almost matched Labour in terms of votes, if not seats.
The increase in unemployment was substantially due to government economic policy which resulted in the closure of outdated factories and coalpits which were no longer economically viable; this process continued for most of the 1980s, with newer industries and the service sector enjoying significant growth. Unemployment had fallen below 3 million by the time of Thatcher's third successive election victory in June 1987 and by the end of 1989 it was down to 1.6 million. However, the British economy slid into another recession in late 1990, concurrently with a global recession, and this caused the economy to shrink by a total of 8% from peak to trough and unemployment to increase from around 1.6 million in the spring of 1990 to nearly 3 million by the end of 1992. The subsequent economic recovery was extremely strong, and unlike after the early 1980s recession, the recovery saw a rapid and substantial fall in unemployment, which was down to 1.7 million by 1997, although the popularity of the Conservative government failed to improve with the economic upturn.
1997 to 2008.
The Labour Party, led by Tony Blair since the death of his predecessor John Smith three years earlier, returned to power in May 1997 after 18 years in opposition. During Blair's 10 years in office there were 40 successive quarters of economic growth, lasting until the second quarter of 2008, helped by Blair's decision to keep taxes relatively low and abandon traditional Labour policies including public ownership of industries and utilities. The previous 15 years had seen one of the highest economic growth rates of major developed economies during that time and certainly the strongest of any European nation. GDP growth had briefly reached 4% per year in the early 1990s, gently declining thereafter. Peak growth was relatively anaemic compared to prior decades, such as the 6.5% pa peak in the early 1970s, although growth was smoother and more consistent. Annual growth rates averaged 2.68% between 1992 and 2007 according to the IMF, with the finance sector accounting for a greater part than previously.
This extended period of growth ended in 2008 when the United Kingdom suddenly entered a recession – its first for nearly two decades – brought about by the global financial crisis. Beginning with the collapse of Northern Rock, which was taken into public ownership in February 2008, other banks had to be partly nationalised. The Royal Bank of Scotland Group, which at its peak was the fifth-largest in the world by market capitalisation, was effectively nationalised on 13 October 2008. By mid-2009, HM Treasury had a 70.33% controlling shareholding in RBS, and a 43% shareholding, through UK Financial Investments Limited, in Lloyds Banking Group. The recession saw unemployment rise from just over 1.6 million in January 2008 to nearly 2.5 million by October 2009.
The UK economy had been one of the strongest economies in terms of inflation, interest rates and unemployment, all of which remained relatively low until the 2008–09 recession. Unemployment has since reached a peak of just under 2.5 million (7.8%), the highest level since the early 1990s, although still far lower than some other European nations. However, interest rates have reduced to 0.5% pa. During August 2008 the IMF warned that the UK economic outlook had worsened due to a twin shock: financial turmoil and rising commodity prices. Both developments harm the UK more than most developed countries, as the UK obtains revenue from exporting financial services while recording deficits in finished goods and commodities, including food. In 2007, the UK had the world's third largest current account deficit, due mainly to a large deficit in manufactured goods. During May 2008, the IMF advised the UK government to broaden the scope of fiscal policy to promote external balance. The UK's "labour productivity per hour worked" is currently on a par with the average for the "old" EU (15 countries). In 2010, the United Kingdom ranked 26th on the Human Development Index.
2008 to present day.
The UK entered a recession in Q2 of 2008 and exited it in Q4 of 2009, according to figures produced by the Office for National Statistics. The subsequently revised ONS figures show that the UK suffered six consecutive quarters of negative growth, shrinking 7.2% from peak to trough, making it the longest recession since records began.
Support for the Labour government (led by Gordon Brown after Tony Blair's resignation in June 2007) slumped during the financial crisis of 2008 and 2009, and the general election of May 2010 ended in a hung parliament. The Conservatives, led by David Cameron since the end of 2005, had the largest number of seats, but came 20 seats short of an overall majority. This resulted in a coalition being formed with the Liberal Democrats in order for the Conservatives to take government within four days of the election results being announced. In order to ease the large deficit created under the previous Labour government, the Conservative-led government has made deep spending cuts since taking office. Within three years, this had led to public sector job losses well into six figures, but the private sector has enjoyed strong job growth and by October 2013 unemployment had fallen back below 2.5 million for the first time in four years.
In Q1 of 2012, the UK economy was thought to have entered a double-dip recession by posting two consecutive negative quarters of growth. However, revised figures by the Office for National Statistics
 show that in fact the UK economy stagnated in Q1 with growth at 0.0%, thereby not fulfilling the technical requirement of two consecutive quarters of negative growth for a recession.
The Office for National Statistics estimates that UK growth in Q1 of 2015 was 0.3%, and that the volume of GDP is 4.0% above its pre-recession peak.
A report released by the Office for National Statistics on 14 May 2013 revealed that over the six-year period between 2005 and 2011, the UK dropped from 5th place to 12th place in terms of household income on an international scale—the drop was partially attributed to the devaluation of sterling over this time frame. However, the report also concluded that, during this period, inflation was relatively less volatile, the UK labour market was more resilient in comparison to other recessions, and household spending and wealth in the UK remained relatively strong in comparison with other OECD countries.
Government spending and economic management.
Government involvement throughout the economy is primarily exercised by HM Treasury, headed by the Chancellor of the Exchequer. In recent years, the UK economy has been managed in accordance with principles of market liberalisation and low taxation and regulation. Since 1997, the Bank of England's Monetary Policy Committee, headed by the Governor of the Bank of England, has been responsible for setting interest rates at the level necessary to achieve the overall inflation target for the economy that is set by the Chancellor each year. The Scottish Government, subject to the approval of the Scottish Parliament, has the power to vary the basic rate of income tax payable in Scotland by plus or minus 3 pence in the pound, though this power has not yet been exercised.
In the 20-year period from 1986/87 to 2006/07 government spending in the UK averaged around 40% of GDP. As a result of the 2007–2010 financial crisis and the late-2000s global recession government spending increased to a historically high level of 48% of GDP in 2009–10, partly as a result of the cost of a series of bank bailouts.
In terms of net government debt as % of GDP, at the end of June 2014 public sector net debt excluding financial sector interventions was £1304.6 billion, equivalent to 77.3% of GDP. In July 2007, the UK had government debt at 35.5% of GDP.
For the financial year of 2013–2014 public sector net borrowing was £93.7 billion. This was £13.0 billion higher than in the financial year of 2012–2013.
Taxation.
Taxation in the United Kingdom may involve payments to at least two different levels of government: local government and central government (HM Revenue & Customs). Local government is financed by grants from central government funds, business rates, council tax and increasingly from fees and charges such as those from on-street parking. Central government revenues are mainly income tax, national insurance contributions, value added tax, corporation tax and fuel duty.
Sectors.
Agriculture.
Agriculture in the UK is intensive, highly mechanised, and efficient by European standards, producing about 60% of food needs, with less than 1.6% of the labour force (535,000 workers). It contributes around 0.6% of British national value added. Around two-thirds of the production is devoted to livestock, one-third to arable crops. Agriculture is subsidised by the European Union's Common Agricultural Policy.
The UK retains a significant, though reduced, fishing industry. Its fleets, based in towns such as Kingston upon Hull, Grimsby, Fleetwood, Newlyn, Great Yarmouth, Peterhead, Fraserburgh, and Lowestoft, bring home fish ranging from sole to herring.
The Blue Book 2013 reports that "Agriculture" added gross value of £9,438 million to the UK economy in 2011.
The UK is also rich in a number of natural resources including coal, petroleum, natural gas, tin, limestone, iron ore, salt, clay, chalk, gypsum, lead, silica and an abundance of arable land.
Construction.
The construction industry of the United Kingdom contributed gross value of £86,789 million to the UK economy in 2011. The industry employed around 2.2 million people in the fourth quarter of 2009. There were around 194,000 construction firms in the United Kingdom in 2009, of which around 75,400 employed just one person and 62 employed over 1,200 people. In 2009 the construction industry in the UK received total orders of around £18.7 billion from the private sector and £15.1 billion from the public sector.
The largest construction project in the UK is Crossrail. Due to open in 2018, it will be a new railway line running east to west through London and into the surrounding countryside with a branch to Heathrow Airport. The main feature of the project is construction of 42 km (26 mi) of new tunnels connecting stations in central London. It is also Europe's biggest construction project with a £15 billion projected cost.
Prospective construction projects include the High Speed 2 line between London and the West Midlands and Crossrail 2.
Production industries.
Electricity, gas and water supply.
The Blue Book 2013 reports that this sector added gross value of £33,289 million to the UK economy in 2011. The United Kingdom is expected to launch the building of new nuclear reactors to replace existing generators and to boost UK's energy reserves.
Manufacturing.
In 2011 the UK manufacturing sector generated approximately £140,539 million in gross value added and employed around 2.6 million people. Of the approximately £16 billion invested in R&D by UK businesses in 2008, approximately £12 billion was by manufacturing businesses. In 2008, the UK was the sixth-largest manufacturer in the world measured by value of output.
In 2008 around 180,000 people in the UK were directly employed in the UK automotive manufacturing sector. In that year the sector had a turnover of £52.5 billion, generated £26.6 billion of exports and produced around 1.45 million passenger vehicles and 203,000 commercial vehicles. The UK is a major centre for engine manufacturing, and in 2008 around 3.16 million engines were produced in the country.
The aerospace industry of the UK is the second- or third-largest aerospace industry in the world, depending upon the method of measurement. The industry employs around 113,000 people directly and around 276,000 indirectly and has an annual turnover of around £20 billion. British companies with a major presence in the industry include BAE Systems (the world's second-largest defence contractor) and Rolls-Royce (the world's second-largest aircraft engine maker). Foreign aerospace companies active in the UK include EADS and its Airbus subsidiary, which employs over 13,000 people in the UK.
The pharmaceutical industry employs around 67,000 people in the UK and in 2007 contributed £8.4 billion to the UK's GDP and invested a total of £3.9 billion in research and development. In 2007 exports of pharmaceutical products from the UK totalled £14.6 billion, creating a trade surplus in pharmaceutical products of £4.3 billion. The UK is home to GlaxoSmithKline and AstraZeneca, respectively the world's third- and seventh-largest pharmaceutical companies.
Mining and quarrying.
The Blue Book 2013 reports that this sector added gross value of £31,380 million to the UK economy in 2011. In 2007 the UK had a total energy output of 9.5 quadrillion Btus, of which the composition was oil (38%), natural gas (36%), coal (13%), nuclear (11%) and other renewables (2%). In 2009, the UK produced 1.5 million barrels per day (bbl/d) of oil and consumed 1.7 million bbl/d. Production is now in decline and the UK has been a net importer of oil since 2005. As of 2010 the UK has around 3.1 billion barrels of proven crude oil reserves, the largest of any EU member state.
In 2009 the UK was the 13th largest producer of natural gas in the world and the largest producer in the EU. Production is now in decline and the UK has been a net importer of natural gas since 2004. In 2009 the UK produced 19.7 million tons of coal and consumed 60.2 million tons. In 2005 it had proven recoverable coal reserves of 171 million tons. It has been estimated that identified onshore areas have the potential to produce between 7 billion tonnes and 16 billion tonnes of coal through underground coal gasification (UCG). Based on current UK coal consumption, these volumes represent reserves that could last the UK between 200 and 400 years.
The UK is home to a number of large energy companies, including two of the six oil and gas "supermajors" – BP and Royal Dutch Shell – and BG Group.
Service industries.
The service sector is the dominant sector of the UK economy, and contributes around 77.8% of GDP as of Q1 2014.
Creative industries.
The creative industries accounted for 7% GVA in 2005 and grew at an average of 6% per annum between 1997 and 2005. Key areas include London and the North West of England which are the two largest creative industry clusters in Europe. According to the British Fashion Council, the fashion industry’s contribution to the UK economy in 2014 is ₤26 billion, up from ₤21 billion pounds in 2009.
Education, health and social work.
According to The Blue Book 2013 the education sector added gross value of £84,556 million in 2011 whilst Human health and social work activities added £104,026 million in 2011.
In the UK the majority of the healthcare sector consists of the state funded and operated National Health Service (NHS), which accounts for over 80% of all healthcare spending in the UK and has a workforce of around 1.7 million, making it the largest employer in Europe, and putting it amongst the largest employers in the world. The NHS operates independently in each of the four constituent countries of the UK. The NHS in England is by far the largest of the four parts and had a turnover of £92.5 billion in 2008.
In 2007/08 higher education institutions in the UK had a total income of £23 billion and employed a total of 169,995 staff. In 2007/08 there were 2,306,000 higher education students in the UK (1,922,180 in England, 210,180 in Scotland, 125,540 in Wales and 48,200 in Northern Ireland).
Financial and business services.
The UK financial services industry added gross value of £116,363 million to the UK economy in 2011. The UK's exports of financial and business services make a significant positive contribution towards the country's balance of payments.
London is a major centre for international business and commerce and is one of the three "command centres" of the global economy (alongside New York City and Tokyo). There are over 500 banks with offices in London, and it is the leading international centre for banking, insurance, Eurobonds, foreign exchange trading and energy futures. London's financial services industry is primarily based in the City of London and Canary Wharf. The City houses the London Stock Exchange, the London International Financial Futures and Options Exchange, the London Metal Exchange, Lloyds of London, and the Bank of England. Canary Wharf began development in the 1980s and is now home to major financial institutions such as Barclays Bank, Citigroup and HSBC, as well as the UK Financial Services Authority. London is also a major centre for other business and professional services, and four of the six largest law firms in the world are headquartered there.
Several other major UK cities have large financial sectors and related services. Edinburgh has one of the largest financial centres in Europe and is home to the headquarters of the Royal Bank of Scotland Group and Standard Life. Leeds is now the UK's largest centre for business and financial services outside London, and the largest centre for legal services in the UK after London.
Hotels and restaurants.
The Blue Book 2013 reports that this industry added gross value of £36,554 million to the UK economy in 2011.
Public administration and defence.
The Blue Book 2013 reports that this sector added gross value of £70,400 million to the UK economy in 2011.
Real estate and renting activities.
The real estate and renting activities sector includes the letting of dwellings and other related business support activities. The Blue Book 2013 reports that real estate industry added gross value of £143,641 million in 2011. Notable real estate companies in the United Kingdom include British Land and The Peel Group.
The UK property market boomed for the seven years up to 2008 and in some areas property trebled in value over that period. The increase in property prices had a number of causes: low interest rates, credit growth, economic growth, rapid growth in buy to-let property investment, foreign property investment in London and planning restrictions on the supply of new housing.
Tourism.
Tourism is very important to the British economy. With over 27 million tourists arriving in 2004, the United Kingdom is ranked as the sixth major tourist destination in the world. London, by a considerable margin, is the most visited city in the world with 15.6 million visitors in 2006, ahead of 2nd placed Bangkok (10.4 million visitors) and 3rd placed Paris (9.7 million).
Transport, storage and communication.
The transport and storage industry added gross value of £59,179 million to the UK economy in 2011 and the telecommunication industry added a gross value of £25,098 million in the same year.
The UK has a radial road network of 46904 km of main roads, with a motorway network of 3497 km. There are a further 213750 km of paved roads. The railway infrastructure company Network Rail owns and operates the majority of the 16,116 km railway lines in Great Britain though not the rolling stock; a further 303 route km (189 route mi) in Northern Ireland is owned and operated by Northern Ireland Railways. Urban rail networks are well developed in major cities including Glasgow, Liverpool and London. Plans are now being considered to build new high speed lines linking all major cities by 2025.
The Highways Agency is the executive agency responsible for trunk roads and motorways in England apart from the privately owned and operated M6 Toll. The Department for Transport states that traffic congestion is one of the most serious transport problems and that it could cost England an extra £22 billion in wasted time by 2025 if left unchecked. According to the government-sponsored Eddington report of 2006, congestion is in danger of harming the economy, unless tackled by road pricing and expansion of the transport network.
In the year from October 2009 to September 2010 UK airports handled a total of 211.4 million passengers. In that period the three largest airports were London Heathrow Airport (65.6 million passengers), Gatwick Airport (31.5 million passengers) and London Stansted Airport (18.9 million passengers). London Heathrow Airport, located 24 km west of the capital, has the most international passenger traffic of any airport in the world. and is the hub for the UK flag carrier British Airways, as well as BMI and Virgin Atlantic. London's six commercial airports form the world's largest city airport system measured by passenger traffic.
Wholesale and retail trade.
This sector includes the motor trade, auto repairs, personal and household goods industries. The Blue Book 2013 reports that this sector added gross value of £151,785 million to the UK economy in 2011.
The UK grocery market is dominated by four companies – Asda (owned by Wal-Mart Stores), Morrisons, Sainsbury's and Tesco.
London is a major retail centre and in 2010 had the highest non-food retail sales of any city in the world, with a total spend of around £64.2 billion. The UK-based Tesco is the third-largest retailer in the world measured by revenues (after Wal-Mart Stores and Carrefour) and is the current leader in the UK market with around a 30% share.
Currency.
London is the world capital for foreign exchange trading. The highest daily volume, counted in trillions of dollars US, is reached when New York enters the trade. The currency of the UK is the pound sterling, represented by the symbol £. The Bank of England is the central bank, responsible for issuing currency. Banks in Scotland and Northern Ireland retain the right to issue their own notes, subject to retaining enough Bank of England notes in reserve to cover the issue. Pound sterling is also used as a reserve currency by other governments and institutions, and is the third-largest after the US dollar and the euro.
The UK chose not to join the euro at the currency's launch. The government of former Prime Minister Tony Blair had pledged to hold a public referendum for deciding membership should "five economic tests" be met. Until relatively recently there was debate over whether or not the UK should abolish its currency Pound Sterling and join the Euro. In 2007 the British Prime Minister, Gordon Brown, pledged at the time to hold a public referendum based on certain tests he set as Chancellor of the Exchequer. When assessing the tests, Gordon Brown concluded that while the decision was close, the United Kingdom should not yet join the Euro. He ruled out membership for the foreseeable future, saying that the decision not to join had been right for the UK and for Europe. In particular, he cited fluctuations in house prices as a barrier to immediate entry. Public opinion polls have shown that a majority of Britons have been opposed to joining the single currency for some considerable time and this position has now hardened further. In 2005, more than half (55%) of the UK were against adopting the currency, while 30% were in favour. The current government, a Conservative and Liberal Democrat coalition, is opposed to membership.
Exchange rates.
"(average for of each year)", in USD (US dollar) and EUR (euro) per GBP; and inversely: GBP per USD and EUR. (Synthetic Euro XEU before 1999). "Caution: these averages conceal wide intra-year spreads. The coefficient of variation gives an indication of this. It also shows the extent to which the pound tracks the euro or the dollar." Note the effect of Black Wednesday in late 1992 by comparing the averages for 1992 with the averages for 1993.
1 GBP in USD since 1971
Economy by region.
The strength of the UK economy varies from country to country and from region to region. Excluding the effects of North Sea Oil and Gas (officially included in the Extra-regio), England has the highest Gross value added (GVA) with Scotland close behind.
Within England, GVA per capita is highest in London. The following table shows the GVA (2009) per capita of the 9 statistical regions of England (NUTS).
Two of the are in the United Kingdom. Inner London is number 1 with a GDP per capita of €65 138, and Berkshire, Buckinghamshire and Oxfordshire is number 7 with a GDP per capita of €37 379. Edinburgh is also one of the largest financial centres in Europe.
At the other end of the scale, Cornwall has the lowest GVA per head of any county or unitary authority in England, and it has received EU Convergence funding (formerly Objective One funding) since 2000.
Data.
Gross Domestic Product.
The money Gross Domestic Product (GDP) for the United Kingdom, at market prices, in 2009 was £1,396 billion (or $2,003 billion) according to the Office for National Statistics in February 2010.
Below is a table of the trend of gross domestic product of United Kingdom at market prices by the International Monetary Fund, with figures in millions of pounds sterling. Percentages in the table can be misleading as they are not corrected for the exchange rate.
For purchasing power parity comparisons, the US Dollar is exchanged at £0.66
Consumer prices index (2006 to present).
Below is a table of the trend of CPI of United Kingdom:
Trade.
UK Trade Office for National Statistics shows the extent of import and export activity, which is a key contributor to the overall economic growth in the UK.
Seasonally adjusted, the UK’s deficit on trade in goods and services was estimated to have been £2.8 billion in March 2015, compared with £3.3 billion in February 2015. This reflects a deficit of £10.1 billion on goods, partially offset by an estimated surplus of £7.3 billion on services. 
In quarter 1 January to March 2015, the UK's deficit on trade in goods and services was estimated to have been £7.5 billion; widening by £1.5 billion from the previous quarter.
In quarter 1 January to March 2015, the trade in goods deficit widened by £0.8 billion to £29.9 billion. The widening reflects a £2.7 billion fall in exports and a £1.9 billion fall in imports. At the commodity level, the fall in exports in quarter 1 reflects a £2.2 billion decrease in exports of fuels; specifically, oil exported to EU countries, which fell by £2.1 billion from the previous quarter. Over the same period, imports of fuels fell by £2.2 billion, reflecting a £1.8 billion fall in imports of oil from outside the EU.
Foreign direct investment.
In 2013 the UK was the leading country in Europe for inward foreign direct investment (FDI) with $26.51bn. This gave it a 19.31% market share in Europe. In contrast, the UK was second in Europe for outward FDI, with $42.59bn, giving a 17.24% share of the European market.
European Union membership.
As a member of the European Union, UK is bound to numerous EU-wide trade and market policies. According to the 2014 report within the "Balance of EU competences" review, majority of the EU trade policies have been beneficial for UK, despite the proportion of the country's exports going to the EU falling from 54 percent to 47 percent over the past decade. The total value of exports however has increased in the same period from £130 billion (€160 billion) to £240 billion (€275 billion). The report on research and development concluded that "The balance of competence for the free movement of goods and IP [intellectual property] works in the UK's interests".
Poverty.
The United Kingdom is a developed country with social welfare infrastructure, thus discussions surrounding poverty tend to be of relative poverty rather than absolute poverty. According to the OECD, the UK is in the lower half of developed country rankings for poverty rates, doing better than Italy and the US but less well than France, Austria, Hungary, Slovakia and the Nordic countries.
The poverty line in the UK is commonly defined as being 60% of the median household income. In 2007–2008, this was calculated to be £115 per week for single adults with no dependent children; £199 per week for couples with no dependent children; £195 per week for single adults with two dependent children under 14; and £279 per week for couples with two dependent children under 14. In 2007–2008, 13.5 million people, or 22% of the population, lived below this line. This is a higher level of relative poverty than all but four other EU members. In the same year, 4.0 million children, 31% of the total, lived in households below the poverty line, after housing costs were taken into account. This is a decrease of 400,000 children since 1998–1999.

</doc>
<doc id="31728" url="http://en.wikipedia.org/wiki?curid=31728" title="Telecommunications in the United Kingdom">
Telecommunications in the United Kingdom

Until 1982, the main civil telecommunications system in the UK was a state monopoly known (since reorganisation in 1969) as Post Office Telecommunications. Broadcasting of radio and television was a duopoly of the BBC and Independent Broadcasting Authority (IBA): these two organisations controlled all broadcast services, and directly owned and operated the broadcast transmitter sites. Mobile phone and Internet services did not then exist.
National Telephone Company (NTC) was a British telephone company from 1881 until 1911 which brought together smaller local companies in the early years of the telephone. Under the Telephone Transfer Act 1911 it was taken over by the General Post Office (GPO) in 1912.
British Rail Telecommunications was created by British Rail (BR). It was the largest private telecoms network in Britain, consisting of 17,000 route kilometres of fibre optic and copper cable which connected every major city and town in the country and provided links to continental Europe through the Channel Tunnel.
BR also operated its own national trunked radio network providing dedicated train-to-shore mobile communications, and in the early 1980s BR helped establish Mercury Communications’, now C&WC, core infrastructure by laying a resilient ‘figure-of-eight’ fibre optic network alongside Britain’s railway lines, spanning London, Bristol, Birmingham, Leeds and Manchester.
The civil telecoms monopoly ended when Mercury Communications arrived in 1983. The Post Office system evolved into British Telecom and was privatised in 1984.
Broadcast transmitters, which belonged to the BBC and IBA, were privatised during the 1990s and now belong to Babcock International and Arqiva.
Regulation of communications has changed many times during the same period, and most of the bodies have been merged into Ofcom, the independent regulator and competition authority for the UK communications industries .
Infrastructure.
Domestic trunk infrastructure.
All communications trunks are now digital. Most are carried via national optical fibre networks. There are several companies with national fibre networks, including BT, Level 3 Communications, Virgin Media, Cable & Wireless, Easynet and Thus. Microwave links are used up to the 155 Mbit/s level, but are seldom cost-effective at higher bit rates.
International trunks.
The UK is a focal point for many of the world's submarine communications cables, which are now mostly digital optical fibre cables. There are many satellite links too, but these now provide a relatively small part of the international bandwidth.
Broadcast transmission.
Most broadcasting organisations, BBC and commercial, lease transmission facilities from one or more of the transmission companies. The main exception is the smaller local radio stations, some of which find it more cost-effective to provide their own.
Fixed phone lines.
BT is still the main provider of fixed telephones lines, both POTS and ISDN, and it has a universal service obligation, although companies can now contract Openreach to install a phoneline on their behalf, rather than telling the customer to get BT to install it, then transfer over.
Virgin Media is the second biggest player in the residential telephone line market.
Other companies provide fixed telephone lines too, but mainly to large companies in the major cities.
There are many other providers who sell fixed telephone services carried over BT lines. They have no network infrastructure of their own.
Mobile phone networks.
First generation networks.
Both companies ran ETACS analogue mobile phone networks.
Third generation networks.
The four 2G companies all won 3G licences in a competitive auction, as did new entrant known as Hutchison 3G, which branded its network as 3. They have now rolled out their networks. Hutchinson 3G does not operate a 2G network, but has an agreement with Orange whereby customers who lose a 3G signal roam with Orange. They previously had an agreement with O2 to provide the same service.
The third generation stems from technological improvements and is in essence an improvement of the available bandwidth, enabling new services to be provided to customers. Such services include streaming of live radio or video, video calls and live TV.
Fourth generation networks.
Long-term evolution (LTE) services are currently being rolled out. EE launched their 4G network in October 2012, using part of their existing 1800 MHz spectrum. O2 and Vodafone will use the 800 MHz and 2600 MHz bands for their services. O2 launched its 4G network on 29 August 2013, initially in London, Leeds and Bradford with a further 13 cities added by the end of 2013. Vodafone commenced its 4G services on 29 August 2013, initially in London with 12 more cities to be added by the end of 2013. 3 commenced LTE services in London, Birmingham, Manchester, Reading, Wolverhampton and the Black country in December 2013 albeit with a limited number of subscribers to evaluate its implementation. Full rollout to remaining subscribers commenced on 5 February 2014 on a phased basis via a silent SIM update. A 50 further cities and over 200 towns are scheduled to receive LTE coverage by the end of 2014. As a condition of acquiring part of EE's 1800 MHz spectrum for 4G use, 3 were unable to use it until October 2013.
Services.
Telephones.
Fixed telephones.
In the UK, there were 35 million (2002) main line telephones.
The telephone service in the United Kingdom was originally provided by private companies and local city councils, but by 1912–13 all except the telephone service of Kingston upon Hull, Yorkshire and Guernsey had been bought out by the General Post Office. Post Office Telephones also operated telephone services in Jersey and the Isle of Man until 1969 when the islands took over responsibility for their own postal and telephone services.
Post Office Telephones was reorganised in 1980–81 as "British Telecommunications" ("British Telecom", or "BT"), and was the first nationalised industry to be privatised by the Conservative government. The Hull Telephone Department was itself sold by Hull City Council as "Kingston Communications" in the late 1990s and celebrated its .
Mobile telephones.
There are more mobile phones than people in the UK. In 2011 there were 82 million subscriptions in the UK. There were 76 million in 2008 and 55 million in January 2005.
Each of the main network operators sells mobile phone services to the public. In addition, companies such as Virgin Mobile UK and Tesco Mobile act as mobile virtual network operators, using the infrastructure of other companies.
Numbering.
There is a set numbering plan for phone numbers within the United Kingdom, which is regulated by the Office of Communications (Ofcom), which replaced the Office of Telecommunications (Oftel) in 2003. Each number consists of an area code—one for each of the large towns and cities and their surroundings—and a subscriber number—the individual number.
Mobile IMSI is the actual number assigned to it the mobile telephone number, and provided with individual license to the MNOs.
Television and radio broadcasting.
Radio.
In 1998 there were 663 radio broadcast stations: 219 on AM, 431 on FM and 3 on shortwave. There were 84.5 million radio receiver sets (1997). Today there are around 600 licensed radio stations in the UK.
Television.
In 1997 there were 30.5 million households with television sets.
Analogue television broadcasts ceased in the UK in 2012, replaced by the Digital Terrestrial Service Freeview which operates via the DVB-T and DVB-T2 (for HD broadcasts) standards. Digital Satellite is provided by BSkyB (subscription and free services) and Freesat (free to air services only) from services at 28.2° East. Digital cable is primarily provided by Virgin Media.
Internet.
The country code top-level domain for United Kingdom web pages is codice_1. Nominet UK is the codice_1. Network Information Centre and second-level domains must be used.
At the end of 2004, 52% of households (12.6 million) were reported to have access to the internet (Source: Office for National Statistics Omnibus Survey). broadband connections accounted for 50.7% of all internet connections in July 2005, with one broadband connection being created every ten seconds. Broadband connections grew by nearly 80% in 2004. In 1999, there were 364 Internet service providers (ISPs). Public libraries also provide access to the internet, sometimes for a fee.

</doc>
<doc id="31729" url="http://en.wikipedia.org/wiki?curid=31729" title="Transport in the United Kingdom">
Transport in the United Kingdom

Transport in the United Kingdom is facilitated with road, air, rail, and water networks. A radial road network totals 29145 mi of main roads, 2173 mi of motorways and 213750 mi of paved roads. The National Rail network of 10,072 route miles (16,116 km) in Great Britain and 189 route miles (303 route km) in Northern Ireland carries over 18,000 passenger and 1,000 freight trains daily. Urban rail networks exist in London, Manchester, Birmingham, Edinburgh, Glasgow, Cardiff, Belfast, Leeds and Liverpool. There are many regional and international airports, with Heathrow Airport in London being one of the busiest in the world. The UK also has a network of ports which received over 558 million tons of goods in 2003–2004.
Transport trends.
Since 1952 (the earliest date for which comparable figures are available), the United Kingdom has seen a growth of car use, increasing its modal share, while the use of buses has declined, and railway use has grown more slowly.
In 1952 27% of distance travelled was by car or taxi; with 42% being by bus or coach and 18% by rail. A further 11% was by bicycle and 3% by motorcycle. The distance travelled by air was negligible.
By 2003 85% of distance travelled was by car or taxi; with 6% being by bus and 6% by rail. Air, pedal cycle and motorcycle accounted for roughly 1% each. In terms of journeys, slightly over 1 billion are made per annum by main line rail, 1 billion by light rail, 4.5 billion by bus, and 21 million on domestic air flights.
Passenger transport has grown in recent years. Figures from the DTI show that total passenger travel inside the United Kingdom has risen from 403 billion passenger kilometres in 1970 to 797 billion in 2004.
Freight transport has undergone similar changes, increasing in volume and shifting from railways onto the road. In 1953 89 billion tonne kilometres of goods were moved, with rail accounting for 42%, road 36% and water 22%. By 2002 the volume of freight moved had almost trebled to 254 billion tonne kilometres, of which 7.5% was moved by rail, 26% by water, 4% by pipeline and 62% by road. Despite the growth in tonne kilometres, the environmental external costs of trucks and lorries in the UK have reportedly decreased. Between 1990 and 2000, there has been a move to heavier goods vehicles due to major changes in the haulage industry including a shift in sales to larger articulated vehicles. A larger than average fleet turnover has ensured a swift introduction of new and cleaner vehicles in the UK.
Although the decline in railway use led to a reduction in the length of the rail network, the length of the road network has not increased in proportion to the increase in road use. Whereas the rail network has halved from 19471 mi in 1950 to 10014 mi today, the major road network only increased from 44710 mi in 1951 to 50893 mi in 1990, and reduced slightly to 50265 mi by 2010. In 2008, the Department for Transport stated that traffic congestion is one of the most serious transport problems facing the United Kingdom. According to the government-sponsored Eddington report of 2006, bottleneck roads are in serious danger of becoming so congested that it may damage the economy.
Air transport.
There are 471 airports and airfields in the United Kingdom, of which 334 are paved. There are also 11 heliports. "(2004 CIA estimates)"
BAA is the United Kingdom's largest airport operator, its flagship being London Heathrow Airport, the largest traffic volume international airport in the world and is the world's busiest airports. Gatwick Airport, the second largest, is owned by Global Infrastructure Partners and was previously owned by BAA. The third largest is Manchester Airport, in Manchester, which is run by Manchester Airport Group, which also owns various other airports.
Other major airports include London Stansted Airport in Essex, about 30 mi north of London, Birmingham Airport in Solihull, Newcastle Airport, Liverpool Airport, and Bristol Airport.
Outside England, Cardiff Airport, Edinburgh Airport and Belfast International Airport, are the busiest airports serving Wales, Scotland and Northern Ireland respectively.
The largest airline in the United Kingdom is British Airways. Others include easyJet, Monarch Airlines, Flybe, Jet2, Thomson Airways and Virgin Atlantic.
Railways.
The rail network in the United Kingdom consists of two independent parts, that of Northern Ireland and that of Great Britain. Since 1994, the latter has been connected to mainland Europe via the Channel Tunnel. The network of Northern Ireland is connected to that of the Republic of Ireland. The National Rail network of 10072 mi in Great Britain and 189 route miles (303 route km) in Northern Ireland carries over 18,000 passenger trains and 1,000 freight trains daily. Urban rail networks are also well developed in London and several other cities. There were once over 30000 mi of rail network in the UK; however, most of this was reduced over a time period from 1955 to 1975, much of it after a report by a government advisor Richard Beeching in the mid-1960s (known as the Beeching Axe).
Great Britain.
The rail network in Great Britain is the oldest such network in the world. The system consists of five high-speed main lines (the West Coast, East Coast, Midland, Great Western and Great Eastern), which radiate from London to the rest of the country, augmented by regional rail lines and dense commuter networks within the major cities. High Speed 1 is operationally separate from the rest of the network, and is built to the same standard as the TGV system in France.
The world's first passenger railway running on steam was the Stockton and Darlington Railway, opened on 27 September 1825. Just under five years later the world's first intercity railway was the Liverpool and Manchester Railway, designed by George Stephenson and opened by the Prime Minister, the Duke of Wellington on 15 September 1830. The network grew rapidly as a patchwork of literally hundreds of separate companies during the Victorian era, which eventually was consolidated into just four by 1922, as the boom in railways ended and they began to lose money. Eventually the entire system came under state control in 1948, under the British Transport Commission's Railway Executive. After 1962 it came under the control of the British Railways Board; then British Railways (later British Rail), and the network was reduced to less than half of its original size by the infamous Beeching cuts of the 1960s when many unprofitable branch lines were closed. Several stations have been reopened in Wales. British Rail during 1986 electrified the Hastings Line.
In 1994 and 1995, British Rail was split into infrastructure, maintenance, rolling stock, passenger and freight companies, which were privatised from 1996 to 1997. The privatisation has delivered very mixed results with healthy passenger growth, mass refurbishment of infrastructure, investment in new rolling stock, and safety improvements being offset by concerns over network capacity and the overall cost to the taxpayer. It has also led to some confusion as to who looks after different aspects of the rail service among the general public. This is because for example, different companies run the tracks to those that run the trains and locomotives.
In Britain, the infrastructure (track, stations, depots and signalling chiefly) is owned and maintained by Network Rail, a not-for-profit company. Network Rail replaced Railtrack, which became bankrupt in 2002 following the Hatfield rail crash in 2000. Passenger services are operated by train operating companies (TOCs), which are franchises awarded by the United Kingdom Government or the Scottish Government. Examples include First Group, East Coast and Virgin Trains. Freight trains are operated by Freight Operating Companies, such as DB Schenker Rail (UK), which are commercial operations unsupported by government. Most Train Operating Companies do not own the locomotives and coaches that they use to operate passenger services. Instead, they are required to lease these from the three Rolling Stock Operating Companies (ROSCOs), with train maintenance carried out by companies such as Bombardier and Alstom.
In Great Britain there is 10274 mi of gauge track, reduced from a historic peak of over 30000 mi. Of this, 3062 mi is electrified and 7823 mi is double or multiple tracks. The maximum scheduled speed on the regular network has historically been around 125 mph on the InterCity lines. On High Speed 1, trains are now able to reach the speeds of French TGVs.
A High Speed 2 line running north from London has been proposed and the legislation needed for the first phase is (as of May 2014) proceeding through parliament. Network Rail are considering reopening a line in south-west England connecting Tavistock to Okehampton and Exeter as an alternative to the coastal mainline which was damaged at Dawlish by coastal storms in February 2014, causing widespread disruption.
Northern Ireland.
In Northern Ireland, Northern Ireland Railways (NIR) both owns the infrastructure and operates passenger rail services. The Northern Ireland rail network is one of the few networks in Europe that carry no freight. It is publicly owned. NIR was united in 1996 with Northern Ireland's two publicly owned bus operators — Ulsterbus and Metro (formally Citybus) — under the brand Translink.
In Northern Ireland there is 212 mi of track at gauge. 118 mi of it is double track.
International rail services.
Eurostar operates trains via the Channel Tunnel to France and Belgium and Enterprise trains link Northern Ireland and the Republic of Ireland.
Rapid transit.
Three cities in the United Kingdom have rapid transit systems. The most well known is the London Underground (commonly known as the Tube), the oldest rapid transit system in the world (opened 1863). Another system also in London is the separate Docklands Light Railway (opened 1987. though this is more of an elevated light metro system due to its lower passenger capacities; further, it is integrated with the Underground in many ways). Outside of London, there is the Glasgow Subway which is the third oldest rapid transit system in the world (opened 1896). One other system, Newcastle's Tyne and Wear Metro (opened 1980), has many similarities to a rapid transit system including underground stations, but is sometimes considered to be light rail.
Urban rail.
Urban commuter rail networks are focused on many of the country's major cities: 
They consist of several railway lines connecting city centre stations of major cities to suburbs and surrounding towns. Train services and ticketing are fully integrated with the national rail network and are not considered separate.
In London, a route for Crossrail 2 has been safeguarded.
Trams and light rail.
Tram systems were popular in the United Kingdom in the late 19th and early 20th century. However, with the rise of the car they began to be widely dismantled in the 1950s. By 1962 only the Blackpool tramway and the Glasgow Corporation Tramways remained; the final Glasgow service was withdrawn on 2 September 1962. Recent years have seen a revival the United Kingdom, as in other countries, of trams together with light rail systems.
Roads.
The road network in Great Britain, in 2006, consists of 7596 mi of trunk roads (including 2176 mi of motorway), 23658 mi of principal roads (including 34 mi of motorway), 71244 mi of "B" and "C" roads, and 145017 mi of unclassified roads (mainly local streets and access roads) — totalling 247523 mi.
Road is the most popular method of transportation in the United Kingdom, carrying over 90% of motorised passenger travel and 65% of domestic freight. The major motorways and trunk roads, many of which are dual carriageway, form the trunk network which links all cities and major towns, these carry about one third of the nation's traffic, and occupy about 0.16% of its land area.
The motorway system, which was constructed from the 1950s onwards, is stated by the British Chambers of Commerce to be, by virtually every measurement of motorway capacity, well below the capacity of other leading
European nations. They give comparative figures for a selection of nations of (units = km/million population): Luxembourg 280, Spain 225, Austria 200, France 185, Belgium 165, Denmark 165, Sweden 165, Netherlands 140, Germany 140, Italy 115, Finland 100, Portugal 80, United Kingdom 60, Greece 45 and Ireland 30.
The Highways Agency (an Executive Agency of the Department for Transport) is responsible for maintaining motorways and trunk roads in England. Other English roads are maintained by local authorities. In Scotland and Wales roads are the responsibility of Transport Scotland, an Executive Agency of the Scottish Government, and the Welsh Assembly Government respectively. Northern Ireland's roads are overseen by the Roads Service Northern Ireland, a section of the Department for Regional Development. In London, Transport for London is responsible for all trunk roads and other major roads, which are part of the Transport for London Road Network.
Toll roads are rare in the United Kingdom, though there are many toll bridges such as the Severn crossing. Road traffic congestion has been identified as a key concern for the future prosperity of the United Kingdom, and policies and measures are being investigated and developed by the government to ameliorate its effects. In 2003, the United Kingdom's first toll motorway, the M6 Toll, opened in the West Midlands area to relieve the congested M6 motorway. Rod Eddington, in his 2006 "Transport’s role in sustaining the UK’s productivity and competitiveness" report, recommended that the congestion problem should be tackled with a "sophisticated policy mix" of congestion-targeted road pricing and improving the capacity and performance of the transport network through infrastructure investment and better use of the existing network. Congestion charging systems do operate in the cities of London and Durham and on the Dartford Crossing. In 2005, the Government published proposals for a United Kingdom-wide road pricing scheme. This was designed to be revenue neutral with other motoring taxes to be reduced to compensate. The plans have been extremely controversial with 1.8 million people signing a petition against them.
Driving is on the left. The maximum speed limit is 70 miles per hour (112 km/h) on motorways and dual carriageways.
Road passenger transport.
Buses.
Local bus services cover the whole country. Since deregulation the majority (80% by the late 1990s) of these local bus companies have been taken over by one of the "Big Five" private transport companies: Arriva (part of Deutsche Bahn AG), First Group, Go-Ahead Group, National Express Group (owners of National Express) and Stagecoach Group. In Northern Ireland coach, bus (and rail) services remain state-owned and are provided by Translink.
Coaches.
Coaches provide long-distance links throughout the United Kingdom: in England and Wales the majority of coach services are provided by National Express. Megabus run no-frills coach services in competition with National Express and services in Scotland in co-operation with Scottish Citylink.
Water transport.
Due to the United Kingdom's island nature, before the Channel Tunnel and the advent of air travel the only way to enter or leave the country was on water, except at the border between Northern Ireland and the Republic of Ireland.
Ports and harbours.
Approximately 95% of freight enters the United Kingdom by sea (75% by value). Three major ports handle most freight traffic:
There are many other ports and harbours around the United Kingdom, including the following towns and cities:
Aberdeen, Avonmouth, Barrow, Barry, Belfast, Boston, Bristol, Cardiff, Dover, Edinburgh/Leith, Falmouth, Felixstowe, Fishguard, Glasgow, Gloucester, Grangemouth, Grimsby, Harwich, Heysham, Holyhead, Hull, Kirkwall, Larne, Liverpool, Londonderry, Manchester, Oban, Peterhead, Plymouth, Poole, Port Talbot, Portishead, Portsmouth, Scapa Flow, Southampton, Stornoway, Stranraer, Sullom Voe, Swansea, Tees (Middlesbrough), Tyne (Newcastle).
Merchant marine.
For long periods of the last millennium Britain had the largest merchant fleet in the world, but it has slipped down the rankings as the use of Flags of convenience has grown. There are 429 ships of  gross register tons (GRT) or over, making a total of  GRT ( tonnes deadweight (DWT)). These are split into the following types: bulk carrier 18, cargo ship 55, chemical tanker 48, container ship 134, liquefied gas 11, passenger ship 12, passenger/cargo ship 64, petroleum tanker 40, refrigerated cargo ship 19, roll-on/roll-off 25, vehicle carrier 3. There are also 446 ships registered in other countries, and 202 foreign-owned ships registered in the United Kingdom. "(2005 CIA estimate)"
Ferries.
Ferries, both passenger only and passengers and vehicles, operate within the United Kingdom across rivers and stretches of water. In London the Woolwich Ferry links the North and South circular roads. Gosport and Portsmouth are linked by the Gosport Ferry; Southampton and Isle of Wight are linked by ferry and fast Catamaran ferries; North Shields and South Shields on Tyneside are linked by the Shields Ferry; and the Mersey has the Mersey Ferry.
In Scotland, Caledonian MacBrayne provides passenger and RO-RO ferry services in the Firth of Clyde, to various islands from Oban and within the inner and outer Hebrides. Orkney Ferries provides services within the Orkney Isles; and NorthLink Ferries provides services from the Scottish mainland to Orkney and Shetland, mainly from Aberdeen although other ports are also used. Ferries operate to Northern Ireland from Stranraer and Cairnryan to Larne and Belfast.
Holyhead, in Wales, is the principal ferry port for Wales and England to Dublin and Dún Laoghaire, in Ireland.
Passenger ferries operate internationally to nearby countries such as France, the Republic of Ireland, Belgium, the Netherlands, Denmark and Spain. Ferries usually originate from one of the following:
More services from Ramsgate, Liverpool, Newhaven, Southampton, Lymington and Fishguard operate to Ireland, France, Belgium and the Isle of Wight.
Waterbuses operate on rivers in some of the country's largest cities such as London (London River Services and Thames Clippers), Cardiff (Cardiff Waterbus) and Bristol (Bristol Ferry Boat).
Other shipping.
Cruise ships depart from the United Kingdom for destinations worldwide, many heading for ports around the Mediterranean and Caribbean.
The Cunard Line still offer a scheduled transatlantic service between Southampton and New York with RMS Queen Mary 2.
The Solent is a world centre for yachting and home to largest number of private yachts in the world.
Inland waterways.
"see canals of the United Kingdom"
Major canal building began in the United Kingdom after the onset of the Industrial revolution in the 18th century. A large canal network was built and it became the primary method of transporting goods throughout the country; however, by the 1830s with the development of the railways, the canal network began to go into decline.
There are currently 1988 mi of waterways in the United Kingdom and the primary use is recreational. 385 mi is used for commerce. "(2004 CIA estimate)"
Education and professional development.
The United Kingdom also has a well-developed network of organisations offering education and professional development in the transport and logistics sectors.
Educational organisations.
A number of Universities offer degree programmes in transport, usually covering transportation planning, engineering of transport infrastructure, and management of transport and logistics services. The Institute for Transport Studies, University of Leeds is one such organisation.
Professional development.
Professional development for those working in the transport and logistics sectors is provided by a number of Professional Institutes representing specific sectors. These include:
Through these professional bodies, transport planners and engineers can train for a number of professional qualifications, including:
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="31730" url="http://en.wikipedia.org/wiki?curid=31730" title="British Armed Forces">
British Armed Forces

 
The British Armed Forces form the military of the United Kingdom, tasked with defence of the country, its overseas territories and the Crown dependencies; as well as promoting the UK's wider interests, supporting international peacekeeping efforts, and providing humanitarian aid. They consist of: the Royal Navy, a blue-water navy with a fleet of 77 commissioned ships; the Royal Marines, a highly specialised amphibious light infantry force; the British Army, the UK's principal land warfare branch; and the Royal Air Force, a technologically sophisticated air force with a diverse operational fleet consisting of both fixed-wing and rotary aircraft.
The Commander-in-chief of the British Armed Forces is the British monarch, Queen Elizabeth II, to whom members of the forces swear allegiance. However the British parliament maintains the armed forces during times of peace with the passing of quinquennial armed forces acts. The armed forces are managed by the Defence Council of the Ministry of Defence, headed by the Secretary of State for Defence.
The UK is an active and regular participant in NATO and other coalition operations. The country is also party to the Five Power Defence Arrangements. Recent operations have included wars in Afghanistan and Iraq, the 2000 intervention in Sierra Leone, peacekeeping responsibilities in the Balkans and Cyprus, and participation in the UN-mandated no-fly zone over Libya. Overseas garrisons and facilities are maintained at Ascension Island, Belize, Brunei, Canada, Diego Garcia, the Falkland Islands, Germany, Gibraltar, Kenya, Qatar and Cyprus.
The United Kingdom tested its first nuclear weapon under Operation Hurricane in 1952, becoming the third nation in the world to achieve the status of a nuclear power. s of 2012[ [update]], Britain remains one of five recognised nuclear powers, with a total of 225 nuclear warheads. Of those, no more than 160 are deployed and active. Its nuclear deterrence system is based on Trident missiles on board ballistic missile submarines.
History.
Upon the Act of Union in 1707, the armed forces of England and Scotland were merged into the armed forces of the Kingdom of Great Britain. By 1815, with the defeat of Napoleon at the Battle of Waterloo Britain had risen to become the world's dominant superpower, and the British Empire subsequently presided over a period of relative peace, known as Pax Britannica, until the outbreak of World War I in 1914. Between 1707 and 1914, British forces played a prominent role in notable conflicts including the Seven Years' War, the Napoleonic Wars and the Crimean War.
The current structure of defence management in Britain was set in place in 1964 when the modern day Ministry of Defence (MoD) was created (an earlier form had existed since 1940). The MoD assumed the roles of the Admiralty, the War Office and the Air Ministry.
Cold War.
Post–World War II economic and political decline, as well as changing attitudes in British society and government, were reflected by the Armed Forces' contracting global role. Britain's protracted decline was dramatically epitomised by its political defeat during the Suez War of 1956. The 1957 Defence White Paper abolished conscription and reduced the size of the Armed Forces from 690,000 to 375,000 by 1962. Seeking an inexpensive alternative to maintaining a large conventional military, the government pursued a doctrine of nuclear deterrence. This initially consisted of free-fall bombs operated by the RAF, but these were eventually superseded by the submarine-launched Polaris ballistic missile. While assurances had been made to the United States that Britain would maintain a presence "East of Suez", a process of gradual withdrawal from its eastern commitments was undertaken in the 1960s, primarily for economic reasons. By the mid-1970s, Britain had withdrawn permanently deployed forces from Aden, Bahrain, Malaysia, Mauritius, Oman, Sharjah, and Singapore. Agreements with Malta (expired 1979) and South Africa (terminated 1975) also ended.
With a permanent presence east of Suez effectively reduced to Hong Kong (up to 1997) and Brunei, the Armed Forces reconfigured to focus on the responsibilities allocated to the services during the Cold War. Substantial forces thus became committed to NATO in Europe and elsewhere; by 1985, 72,929 personnel were stationed in Continental Europe. The British Army of the Rhine and RAF Germany consequently represented the largest and most important overseas commitments that the British Armed Forces had during this period. The Royal Navy's fleet developed an anti-submarine warfare specialisation, with a particular focus on countering Soviet submarines in the Eastern Atlantic and North Sea. In the process of this transition and due to economic constraints, four conventional aircraft carriers and two "commando" carriers were decommissioned between 1967 and 1984. With the cancellation of the CVA-01 project, three "Invincible"-class STOVL aircraft carriers, originally designed as "Through-Deck Cruisers", became their ultimate replacements.
While this focus on NATO obligations increased in prominence during the 1970s, low-intensity conflicts in Northern Ireland and Oman emerged as the primary operational concerns of the British Armed Forces. These conflicts had followed a spate of insurgencies against British colonial occupation in Aden, Cyprus, Kenya and Malaysia. An undeclared war with Indonesia had also occurred in Borneo during the 1960s, and recurring civil unrest in the declining number of British colonies often required military assistance.
Recent history.
Four major reviews of the British Armed Forces have been conducted since the end of the Cold War. All three services experienced considerable reductions in manpower, equipment, and infrastructure during this period while re-structuring to deliver a greater focus on expeditionary warfare.
The Conservative government produced the Options for Change review in the 1990s, seeking to benefit from a perceived post–Cold War "peace dividend". Though the Soviet Union had disintegrated, a presence in Germany was retained in the reduced form of British Forces Germany. Experiences during the First Gulf War prompted renewed efforts to enhance joint operational cohesion and efficiency among the services by establishing a Permanent Joint Headquarters in 1996.
An increasingly international role for the British Armed Forces has been pursued since the Cold War's end. This entailed the Armed Forces often constituting a major component in peacekeeping missions under the auspices of the United Nations or NATO, and other multinational operations. Consistent under-manning and the reduced size of the Armed Forces highlighted the problem of 'overstretch' during the conflicts in Iraq and Afghanistan. This reportedly contributed to personnel retention difficulties and challenged the military's ability to sustain its overseas commitments.
A Strategic Defence Review (SDR)—described as "foreign-policy-led"—was published in 1998. Expeditionary warfare and tri-service integration were central to the review, which sought to improve efficiency and reduce expenditure by consolidating resources. Most of the Armed Forces' helicopters were collected under a single command and a Joint Force Harrier was established in 2000, containing the Navy and RAF's fleet of Harrier Jump Jets. A Joint Rapid Reaction Force was formed in 1999, with significant tri-service resources at its disposal.
The first major post-11 September restructuring was announced in the 2004 Delivering Security in a Changing World: Future Capabilities review, continuing a vision of "mobility" and "expeditionary warfare" articulated in the SDR. Future equipment projects reflecting this direction featured in the review, including the procurement of two large aircraft carriers and a series of medium-sized vehicles for the Army. Reductions in manpower, equipment, and infrastructure were also announced. The decision to reduce the Army's regular infantry to 36 battalions (from 40) and amalgamate the remaining single-battalion regiments was controversial, especially in Scotland and among former soldiers. Envisaging a rebalanced composition of more rapidly deployable light and medium forces, the review announced that a regiment of Challenger 2 main battle tanks and a regiment of AS-90 self-propelled artillery would be converted to lighter roles.
There were more than 30,000 members of the British Armed Forces deployed abroad in January 2007, serving in various capacities. Peacekeeping, humanitarian aid, and disaster relief tasks increased in the 2000s, many under the auspices of the United Nations and NATO. The Armed Forces contributed to the international humanitarian and reconstruction efforts in the aftermath of the 2004 tsunami and 2005 earthquake in Pakistan.
Within the United Kingdom, there were approximately 140,000 personnel stationed in England, 13,200 in Scotland, 7,000 in Northern Ireland, and 6,200 in Wales. The conflict in Northern Ireland had required the Armed Forces to provide "Military aid to the civil power" from 1969, with a presence that peaked at over 20,000 regular personnel in 1972. Sectarian and paramilitary violence subsided after the Good Friday Agreement was signed in 1998. and the IRA declared an end to its campaign in 2005. Operational support for the Police Service of Northern Ireland, known as Operation Banner, officially ended on 1 August 2007, resulting in the reduction of the military presence to the size of a peacetime garrison.
As a result of the Strategic Defence and Security Review conducted in October 2010, Prime Minister David Cameron signed a 50-year treaty with French President Nicolas Sarkozy that would have the two countries co-operate intensively in military matters.
Today.
The British Armed Forces is a professional force with a strength in January 2015 of 156,940 Regular and 30,000 Volunteer Reserve personnel. This gives a total strength of 186,940 Service Personnel. In addition, all ex-Regular personnel retain a "statutory liability for service" and are liable to be recalled (under Section 52 of the Reserve Forces Act (RFA) 1996) for duty in a time of need, this is known as the Regular Reserve. However, MoD publications since April 2013 no longer report the entire strength of the Regular Reserve, instead they only give a figure for Regular Reserves who serve under a fixed-term reserve contract. These contracts are similar in nature to those of the Volunteer Reserve. As of 2014, regular Reserves serving under a fixed-term contract numbered 45,110 personnel in 2014. All personnel figures exclude the Military Provost Guard Service and the University Training Units; the University Royal Naval Unit, the Officers' Training Corps and the University Air Squadron.
Britain has the fifth or sixth-largest defence budget in the world, with the country spending more than countries like Germany or Japan but more or less comparable to that of France or Saudi Arabia. In September 2011, according to the Royal United Services Institute, current "planned levels of defence spending should be enough for the United Kingdom to maintain its position as one of the world's top military powers, as well as being one of NATO-Europe's top military powers. Its edge – not least its qualitative edge – in relation to rising Asian powers seems set to erode, but will remain significant well into the 2020’s, and possibly beyond."
In the 2013 Spending Review, the Chancellor of the Exchequer, George Osborne, reinforced the government's commitment to the 2010 SDSR and stated the £38bn "black hole" in the defence budget had been filled, that at over 2% of GDP, the defence budget will remain one of the largest in the world, the equipment budget will grow by 1% in real terms every year after 2015, that there will be further reductions in the civilian MoD workforce, PFI contracts signed in the last decade would be renegotiated, the way equipment is purchased is to be overhauled, and a greater commitment to cyber warfare.
Command organisation.
As Sovereign and head of state, Queen Elizabeth II is Head of the Armed Forces and their Commander-in-Chief. Long-standing constitutional convention, however, has vested "de facto" executive authority, by the exercise of Royal Prerogative powers, in the Prime Minister and the Secretary of State for Defence, and the Prime Minister (acting with the support of the Cabinet) makes the key decisions on the use of the armed forces. The Queen, however, remains the ultimate authority of the military, with officers and personnel swearing allegiance to the monarch. It has been claimed that this includes the power to prevent unconstitutional use of the armed forces, including its nuclear weapons.
The Ministry of Defence is the Government department and highest level of military headquarters charged with formulating and executing defence policy for the Armed Forces; it currently employs over 60,000 civilians as of 2014. This number will be reduced to just 55,000 by 2015 (a reduction of 25,000 as per the October 2010 SDSR) and then again to 48,000 by 2020 (a reduction of 7,000 as per the Three Month Review in 2011). The department is controlled by the Secretary of State for Defence and contains three deputy appointments: Minister of State for the Armed Forces, Minister for Defence Procurement, and Minister for Veterans' Affairs.
Responsibility for the management of the forces is delegated to a number of committees: the Defence Council, Chiefs of Staff Committee, Defence Management Board and three single-service boards. The Defence Council, composed of senior representatives of the services and the Ministry of Defence, provides the "formal legal basis for the conduct of defence". The three constituent single-service committees (Admiralty Board, Army Board and Air Force Board) are chaired by the Secretary of State for Defence.
The Chief of the Defence Staff is the professional head of the Armed Forces and is an appointment that can be held by an Admiral, Air Chief Marshal or General. Before the practice was discontinued in the 1990s, those who were appointed to the position of CDS had been elevated to the most senior rank in their respective service (a 5-star rank). The CDS, along with the Permanent Under Secretary, are the principal advisers to the departmental minister. The three services have their own respective professional chiefs: the First Sea Lord, the Chief of the General Staff and the Chief of the Air Staff.
Personnel are based in a number of overseas territories, though internal security for the majority is provided solely by small police forces. Garrisons and facilities exist in Ascension Island, Diego Garcia, the Falkland Islands, Gibraltar, and the Sovereign Base Areas in Cyprus. These deployments accounted for over 5,000 personnel in 2006. Locally-raised units are maintained in Bermuda (The Bermuda Regiment), the Falkland Islands (Falkland Islands Defence Force), and Gibraltar (Royal Gibraltar Regiment). Though their primary mission is "home defence", individuals have volunteered for operational duties. The Royal Gibraltar Regiment has recently mobilised section-sized units for attachment to regiments deployed to Iraq.
Weapons of mass destruction.
The United Kingdom is one of only five recognised nuclear weapon states under the Non-Proliferation Treaty and maintains an independent nuclear deterrent, currently consisting of four "Vanguard"-class ballistic missile submarines. This is known as the UK Trident programme and delivers a 'continuous at sea deterrent' capability. Nomenclature of the UK deterrent is after the UGM-133 Trident II submarine-launched ballistic missile that is used to deliver the nuclear warheads. Estimates of the United Kingdoms nuclear stockpile put it at approximately 225 nuclear warheads in total, with 160 of those being active.
A successor programme is currently in its early stages with a final decision to be made in 2016 after the 2015 general election. It primarily seeks to replace the "Vangaurd"-class submarines with a new generation of SSBNs, however, the programme will also extend the life of the UGM-133 Trident II ballistic missiles, refurbish the nuclear warheads and modernise existing infrastructure associated with the deterrent.
Former weapons of mass destruction possessed by the United Kingdom include both biological and chemical weapons. These were renounced in 1956 and subsequently destroyed.
UK Joint Expeditionary Force.
The UK Joint Expeditionary Force, not to be confused with the similarly named UK-French Combined Joint Expeditionary Force, was announced in December 2012 by the Chief of the Defence Staff, General Sir David Richards. It is designed to be an integrated joint force, with capabilities across the spectrum at sea, on land and in the air, with the aspiration being greater levels of integration than previously achieved especially when combined with other nations' armed forces. Of variable size, it is intended to be the basis of all the UK armed forces' combined joint training; a framework into which other nations will fit. It will be the core of the UK's contribution to any military action, whether NATO, coalition or independent. Together with Command and Control elements including HQ Allied Rapid Reaction Corps and the maritime component HQ at Northwood, the force is designed to meet the UK's obligations to NATO.
Royal Navy.
The Royal Navy is a technologically sophisticated naval force, and as of April 2015 consists of 77 commissioned ships. Command of deployable assets is exercised by the Fleet Commander of the Naval Service. Personnel matters are the responsibility of the Second Sea Lord/Commander-in-Chief Naval Home Command, an appointment usually held by a vice-admiral.
The Surface Fleet consists of amphibious warfare ships, destroyers, frigates, patrol vessels, mine-countermeasure vessels, and other miscellaneous vessels. The Surface Fleet has been structured around a single fleet since the abolition of the Eastern and Western fleets in 1971. The recently built Type 45 destroyers are technologically advanced air-defence destroyers. The Royal Navy is building two "Queen Elizabeth" class aircraft carriers, embarking an air-group including the advanced fifth-generation multi-role fighter, the F-35B.
A submarine service has existed within the Royal Navy for more than 100 years. The Submarine Service's four "Vanguard"-class nuclear-powered submarines carry Lockheed Martin's Trident II ballistic missiles, forming the United Kingdom's nuclear deterrent. The service possessed a combined fleet of diesel-electric and nuclear-powered submarines until the early 1990s. Following the Options for Change defence review, the Upholder class diesel-electric submarines were withdrawn and the attack submarine flotilla is now exclusively nuclear-powered. Seven "Astute" class nuclear-powered attack submarines have been ordered, with two completed, and four under construction. The "Astute" class are the most advanced and largest fleet submarines ever built for the Royal Navy, and will maintain Britain's nuclear-powered submarine fleet capabilities for decades to come.
Royal Marines.
The Royal Marines are the Royal Navy's amphibious troops. Consisting of a single manoeuvre brigade (3 Commando) and various independent units, the Royal Marines specialise in amphibious, arctic, and mountain warfare. Contained within 3 Commando Brigade are three attached army units; 1st Battalion, The Rifles, an infantry battalion based at Beachley Barracks near Chepstow (from April 2008), 29 Commando Regiment Royal Artillery, an artillery regiment based in Plymouth, and 24 Commando Regiment Royal Engineers. The Commando Logistic Regiment consists of personnel from the Army, Royal Marines, and Royal Navy.
British Army.
The British Army is made up of the Regular Army and the Army Reserve. The army has a single command structure based at Andover and known as "Army Headquarters". Deployable combat formations consist of two divisions (1st Armoured and 3rd Mechanised) and eight brigades. Within the United Kingdom, operational and non-deployable units are administered by three regionally-defined "regenerative" divisions (2nd, 4th, and 5th) and London District.
The Army has 50 battalions (36 regular and 14 territorial) of regular and territorial infantry, organised into 17 regiments. The majority of infantry regiments contains multiple regular and territorial battalions. Modern infantry have diverse capabilities and this is reflected in the varied roles assigned to them. There are four operational roles that infantry battalions can fulfil: air assault, armoured infantry, mechanised infantry, and light role infantry.
Regiments and battalions e.g.: the Parachute Regiment, exist within every corps of the Army, functioning as administrative or tactical formations. Armoured regiments are equivalent to an infantry battalion. There are 11 armoured regiments within the regular army, of which six are designated as "Armoured" and five as "Formation Reconnaissance". With the exception of the Household Cavalry, armoured regiments and their Territorial counterparts are grouped under the Royal Armoured Corps. Arms and support units are also formed into similar collectives organised around specific purposes, such as the Corps of Royal Engineers, Army Air Corps and Royal Army Medical Corps.
Royal Air Force.
The Royal Air Force has a large operational fleet that fulfils various roles, consisting of both fixed-wing and rotary aircraft. Frontline aircraft are controlled by Air Command, which is organised into three groups defined by function: 1 Group (Air Combat), 2 Group (Air Support) and 22 Group (training aircraft and ground facilities). In addition 83 Expeditionary Air Group directs formations in the Middle East. Deployable formations consist of Expeditionary Air Wings and squadrons—the basic unit of the Air Force. Independent flights are deployed to facilities in Afghanistan, the Falkland Islands, Iraq, and the United States.
The Royal Air Forces operates multi-role and single-role fighters, reconnaissance and patrol aircraft, tankers, transports, helicopters, unmanned aerial vehicles, and various types of training aircraft. Ground units are also maintained by the Royal Air Force, most prominently the RAF Police and the Royal Air Force Regiment (RAF Regt). The Royal Air Force Regiment essentially functions as the local ground defence force of the RAF. Roled principally as ground defence for RAF facilities, the regiment contains nine regular squadrons, supported by five squadrons of the Royal Auxiliary Air Force Regiment. By March 2008, the three remaining "Air Defence" squadrons had disbanded or re-roled and their responsibilities transferred to the British Army's Royal Artillery.
Civilian agencies of the Ministry of Defence.
The British Armed Forces are supported by civilian agencies owned by the MoD. Although they are civilian, they play a vital role in supporting Armed Forces operations, and in certain circumstances are under military discipline.
Royal Fleet Auxiliary.
The 13 ships of the Royal Fleet Auxiliary (RFA) primarily serve to replenish Royal Navy warships at sea, and also augment the Royal Navy's amphibious warfare capabilities through its three Bay-class landing ship dock vessels. It is manned by 1,850 civilian personnel and is funded and run by the Ministry of Defence.
UK Hydrographic Office.
For more Information about Civilian Agencies of or within the MoD see Ministry of Defence (United Kingdom).
Recruitment.
All three services of the British Armed Forces recruit primarily from within the United Kingdom, although citizens from the Commonwealth of Nations and the Republic of Ireland are equally eligible to join. The minimum recruitment age is 16 years (although personnel may not serve on armed operations below 18 years); the maximum recruitment age depends whether the application is for a regular or reserve role; there are further variations in age limit for different corps/regiments. The normal term of engagement is 22 years; however, the minimum service required before resignation is 4 years, plus, in the case of the Army, any service person below the age of 18. At present, the yearly intake into the armed forces is 11,880 (per the 12 months to 31 March 2014).
Excluding the Brigade of Gurkhas and the Royal Irish Regiment, as of 1 April 2014 there are approximately 11,200 Black and Minority Ethnic (BME) persons serving as Regulars across the three service branches - of those, 6,610 were recruited from outside the United Kingdom. In total, Black and Minority Ethnic persons represent 7.1% of all service personnel, an increase from 6.6% in 2010.
Women have been integrated into the British Armed Forces since the early 1990s; however, they remain excluded from primarily combat units in the Army, Royal Marines, and Royal Air Force Regiment. As of the 1 April 2014, there are approximately 15,840 women serving in the Armed Forces, representing 9.9% of all service personnel. The first female military pilot was Flight Lieutenant Julie Ann Gibson while Flight Lieutenants Jo Salter and Kirsty Moore were the first fast-jet pilots, the former flying a Tornado GR1 on missions patrolling the then Northern Iraqi No-Fly Zone. Flight Lieutenant Juliette Fleming and Squadron Leader Nikki Thomas recently were the first Tornado GR4 crew. While enforcing the Libyan No-Fly Zone, Flight Lieutenant Helen Seymour was identified as the first female Eurofighter Typhoon pilot. As of August 2011, it was announced that a female Lieutenant Commander, Sarah West, will command the frigate HMS Portland.
Since the year 2000, sexual orientation has not been a factor considered in recruitment, and homosexuals can serve openly in the armed forces. All branches of the forces have actively recruited at Gay Pride events. The forces keep no formal figures concerning the number of gay and lesbian serving soldiers, saying that the sexual orientation of personnel is considered irrelevant and not monitored.

</doc>
<doc id="31731" url="http://en.wikipedia.org/wiki?curid=31731" title="Foreign relations of the United Kingdom">
Foreign relations of the United Kingdom

The diplomatic foreign relations of the United Kingdom are implemented by the Foreign and Commonwealth Office. The Prime Minister and numerous other agencies play a role in setting policy, and many institutions and businesses have a voice and a role. Great Britain was the world's foremost power during the 18th, 19th and early 20th centuries. Until the Suez crisis of 1956, the country was considered a 'superpower'. After 1956 however, with the loss of the empire, its dominant role in global affairs was gradually diminished. Nevertheless, the United Kingdom remains a major power and a permanent member of the United Nations Security Council, a Member state of the European Union, and a founding member of the G7, G8, G20, NATO, OECD, WTO, Council of Europe, OSCE, and the Commonwealth of Nations, which is a legacy of the British Empire.
History.
British foreign relations since 1600 have focused on achieving a balance of power, with no country controlling the continent of Europe. The chief enemy, from the Hundred Years' War until the defeat of Napoleon (1337-1815) was France, a larger country with a more powerful army. The British were generally successful in their many wars, with the notable exception of the American War of Independence (1775–1783), when Britain, without any major allies, was defeated by the colonials who had the support of France, the Netherlands and Spain. A favoured diplomatic strategy was subsidising the armies of continental allies, such as Prussia, thereby turning London's enormous financial power to military advantage. Britain relied heavily on its Royal Navy for security, seeking to keep it the most powerful fleet afloat with a full complement of bases across the globe. Historians agree that Lord Salisbury as foreign minister and prime minister in the late 19th century was a strong and effective leader in foreign affairs. He had a superb grasp of the issues, and proved:
The British built up a very large worldwide British Empire, which peaked in size in the 1920-40 era and in wealth around 1900, then began to shrink until by the 1970s almost nothing was left but a "Commonwealth of Nations" that had little to do. Britain finally turned its attention to the continent, joining the European Union.
After 1900 Britain ended its "splendid isolation" by developing friendly relations with the United States and Japan 1902. Even more important—by forming the Triple Entente with France (1904) and Russia (1907), thus forging the anti-German alliance that fought the First World War (1914-1918). The "special relationship" with the U.S. endured. It played a pivotal role in the Second World War and the Cold War, and is in effect today through NATO. By 2014, however, the debate was underway whether Britain should reduce or cut its ties with the EU, and whether Scotland should leave the UK.
Recent history.
After 1945 Britain systematically reduced its overseas commitments. Practically all the colonies became independent. Britain reduced its involvements in the Middle east, with the humiliating Suez Crisis of 1956 marking the end of its status as a superpower. However Britain did forge close military ties with the United States, and with traditional foes such as France and Germany, in the NATO military alliance. After years of debate (and rebuffs), Britain joined the Common Market in 1973; it is now the European Union. However it did not merge financially, and kept the pound separate from the Euro, which kept it partly isolated from the EU financial crisis of 2011.
Lunn et al. (2008) argue:
Commonwealth of Nations & Ireland.
The UK has varied relationships with the countries that make up the Commonwealth of Nations which originated from the British Empire. Elizabeth II of the United Kingdom is the head of the Commonwealth and is head of state of 16 of its 53 member states. Those that retain the Queen as head of state are called Commonwealth realms. Over time several countries have been suspended from the Commonwealth for various reasons. Zimbabwe was suspended because of the authoritarian rule of its President and so too was Pakistan, but it has since returned. Countries which become republics are still eligible for membership of the Commonwealth so long as they are deemed democratic. Commonwealth nations such as Malaysia enjoyed no export duties in trade with the UK before the UK concentrated its economic relationship with EU member states.
The UK was once a dominant colonial power in many countries on the continent of Africa and its multinationals remain large investors in sub-Saharan Africa. Nowadays the UK, as a leading member of the Commonwealth of Nations, seeks to influence Africa through its foreign policies. Current UK disputes are with Zimbabwe over human rights violations. Tony Blair set up the Africa Commission and urged rich countries to cease demanding developing countries repay their large debts. Relationships with developed (often former dominion) nations are strong with numerous cultural, social and political links, mass inter-migration trade links as well as calls for Commonwealth free trade.
 Australia.
Australia–United Kingdom relations are close, marked by shared history, culture, institutions and language, extensive people-to-people links, aligned security interests, and vibrant trade and investment cooperation. The long-standing relationship between the United Kingdom and Australia formally began in 1901 when the six British Crown colonies in Australia federated, and the Commonwealth of Australia was formed as a Dominion of the British Empire. Australia fought alongside Britain in World War I, notably at Gallipoli, and again in World War II. Andrew Fisher, Australian prime minister from 1914 to 1916, declared that Australia would defend the United Kingdom "to the last man and the last shilling." Until 1949, the United Kingdom and Australia shared a common nationality code. The final constitutional ties between United Kingdom and Australia ended in 1986 with the passing of the Australia Act 1986. Currently, roughly 1/4 of the Australian population was born in the UK, giving strong mutual relations. Furthermore, investment and trade between the two countries, is still important.
 Barbados.
The two countries are related through common history, the Commonwealth of Nations and their sharing of the same Head of State, Queen Elizabeth II as their Monarch. As one of the first English colonies, the initial permanent European settlement took place in the early seventeenth century by English settlers. Barbados thereafter remained as a territory of the United Kingdom until it negotiated independence in 1966. In recent years, increasing numbers of British nationals have purchased secondary homes in Barbados, and the islands ranked as the Caribbean regions' fourth largest export market of the United Kingdom. The British High Commission was established in Bridgetown, Barbados in 1966 and there is also a Barbadian High Commission in London.
 Brunei.
In 1888, Brunei became a British protectorate, gaining its independence from British rule less than 100 years later in 1984.
The UK and Brunei have a long-standing and strong bilateral relationship, particularly on defence co-operation, trade and education. The UK continues to play a strong role in developing Brunei’s oil and gas sector, and the Brunei Investment Agency is a significant investor in the UK, with their largest overseas operations in the City of London. The UK remains the destination of choice for Bruneian students, with about 1,220 of them enrolled in higher education in the UK in 2006-07.
The United Kingdom has a high commission in Bandar Seri Begawan, and Brunei has a high commission in London. Both countries are full members of the Commonwealth of Nations.
 Canada.
London and Ottawa enjoy cooperative and intimate contact; the two countries are related through history, the Commonwealth of Nations, and their sharing of the same Head of State and monarch. Both countries fought together in both World Wars, the Korean War, and more recently cooperate in the coalition in the War in Afghanistan. Both are founding members of NATO, and also belong to the G7 (and the G8). Winston Churchill said Canada was the "linchpin of the English-speaking world", as it connects two other anglophone countries: the US and the UK. These three countries were the first to share the knowledge of the atom bomb with each other, as all three worked on the Manhattan Project together. Despite this shared history, the UK and Canada have grown apart economically. The UK was Canada's largest trade partner is the 19th and early 20th centuries, but is now well down the list. The two nations now find themselves in separate trade blocs, the EU for the UK and NAFTA for Canada. However relations are still strong, with large migration between the two countries, as well as Canada having the highest favourable public opinion of the UK in the world.
 Cyprus.
The UK maintains two sovereign area military bases on the island of Cyprus. The UK is also a signatory to a treaty with Greece and Turkey concerning the independence of Cyprus, the Treaty of Guarantee, which maintains that Britain is a "guarantor power" of the island's independence.
Both countries are members of the European Union.
 India.
India has a high commission in London and two consulates-general in Birmingham and Edinburgh. The United Kingdom has a high commission in New Delhi and three deputy high commissions in Mumbai, Chennai and Kolkata. Although the Sterling Area no longer exists and the Commonwealth is much more an informal forum, India and the UK still have many enduring links. This is in part due to the significant number of people of Indian origin living in the UK. The large South Asian population in the UK results in steady travel and communication between the two countries. The British Raj allowed for both cultures to imbibe tremendously from the other. The English language and cricket are perhaps the two most evident British exports, whilst in the UK food from the Indian subcontinent are very popular. The United Kingdom's favourite food is often reported to be Indian cuisine, although no official study reports this.
Economically the relationship between Britain and India is also strong. India is the second largest investor in Britain after the US, this being demonstrated by Tata Motors' ownership of British-based Jaguar Land Rover. Britain is also one of the largest investors in India.
 Ireland.
Despite a long history of conflict from English Tudor plantation in Ireland to the Irish War of independence, the UK presently works closely with the government of the Republic of Ireland in areas concerning the peace process in Northern Ireland as well as on many security issues. In 1949 the Irish Houses of Parliament passed the Republic of Ireland Act, making the Republic of Ireland officially fully independent; the country withdrew from the Commonwealth. Under the Ireland Act 1949 Irish citizens are treated as though they are Commonwealth citizens and not aliens for the purposes of law. Until 1998, the Republic of Ireland claimed Northern Ireland, but this was rescinded under the Belfast Agreement through an amendment of the Irish Constitution, which now states an aspiration to peaceful unity. There is an ongoing dispute that also involves Denmark and Iceland, over the status of the ocean floor surrounding Rockall. However, this is for the most part a trivial issue that rarely makes it onto British-Irish meeting agendas.
Both countries are members of the European Union.
 Malaysia.
The United Kingdom has a high commission in Kuala Lumpur, and Malaysia has a high commission in London. Both countries are full members of the Commonwealth of Nations. Both the UK and Malaysia are part of the Five Powers Defence Arrangements. Malaysia is a strong partner of Britain in the Far East. Britain has made numerous military sacrifices in guaranteeing a stable independent Malaysia, for example the Malaysian Emergency and the protection of the country during high tensions with Indonesia-Konfrontasi.
The Yang di-Pertuan Agong Sultan Abdul Halim of Kedah paid a state visit to the United Kingdom in July 1974. The Yang di-Pertuan Agong Sultan Azlan Shah of Perak paid a state visit to the United Kingdom in November 1993. HM Queen Elizabeth II of the United Kingdom paid state visits to Malaysia in October 1989, and in September 1998.
 Malta.
In the 1950s and 1960s, serious consideration was given in both countries to the idea of a political union between the United Kingdom and Malta. However, this plan for "Integration with Britain" foundered, and Malta gained its independence from the United Kingdom in 1964. British Monarch Queen Elizabeth II remained Queen of Malta until the country became a Republic in 1974. There is a small Maltese community in the United Kingdom. In addition, the British overseas territory of Gibraltar has been influenced by significant 18th and 19th Century immigration from Malta (see "History of the Maltese in Gibraltar").
Both countries are members of the European Union.
 Nauru.
Nauru was part of the British Western Pacific Territories from September 1914 and June 1921. The British Government had ceased to exercise any direct role in the governance of Nauru by 1968, when the island achieved its independence. The Nauruan government maintains an Hon. Consul, Martin W I Weston. The British High Commission in Suva is responsible for the United Kingdom's bilateral relations with Nauru.
 New Zealand.
Up to about the 1960s, New Zealand also had extremely close economic relations with the United Kingdom, especially considering the distance at which trade took place. As an example, in 1955, Britain took 65.3 percent of New Zealand's exports, and only during the following decades did this dominant position begin to decline as the United Kingdom oriented itself more towards the European Union, with the share of exports going to Britain having fallen to only 6.2 percent in 2000. Historically, some industries, such as dairying, a major economic factor in the former colony, had even more dominant trade links, with 80-100% of all cheese and butter exports going to Britain from around 1890 to 1940. This strong bond also supported the mutual feelings for each other in other areas.
 Nigeria.
Nigeria, formerly a colony, gained independence from Britain in 1960. Large numbers of Nigerians have since emigrated to Britain. The British government played an important role in resolving the Nigerian Civil War. Trade and investment between the two countries are strong, many British multinational companies are active in Nigeria, especially Shell in oil and gas production.
 Pakistan.
Pakistan before partition was part of Indian Empire from 1 November 1858 to 13 August 1947. Both UK and Pakistan are active members of the Commonwealth of Nations. Favourable opinion of Britain is much lower in Pakistan, relative to many other Commonwealth countries, this is because the UK is seen as an ally of the US. However, large numbers of Pakistanis live, work and study in the UK and the British government has refused to support US infringements into northern Pakistan during the Afghanistan War, thinking it wrong to violate Pakistani sovereignty as so.
 Papua New Guinea.
Papua New Guinea and the United Kingdom share Queen Elizabeth as their head of state. They have had relations since 1975 when Papua New Guinea gained independence from Australia (then still a British Dominion).
 Singapore.
Singapore and the United Kingdom share a friendly relationship since Singapore became independent from the United Kingdom in 1959. Singapore retained the Judicial Committee of the Privy Council as the final court of appeal up till 1989 (fully abolished in 1994) due to political reasons.
United States.
The United Kingdom and the United States are close military allies. The two countries share cultural similarities, as well as military research and intelligence facilities. The UK has purchased military technology from the USA such as Tomahawk cruise missiles and Trident nuclear missiles, and the US has purchased equipment from Britain (e.g. Harrier Jump Jet). The USA also maintains a large number of military personnel in the UK. In recent years, the Prime Minister of the United Kingdom and the President of the United States have often been close friends, for example Tony Blair and Bill Clinton (and later Blair and George W. Bush), and the often like-minded Margaret Thatcher and Ronald Reagan. Present British policy is that The United Kingdom's relationship with the United States represents Britain's "most important bilateral relationship".
Europe.
The UK has had good relations with the rest of Europe since the Second World War. It became a member of the European Economic Community in 1973, which eventually evolved into the European Union through the Maastricht Treaty twenty years later. Although the UK does not use the Euro and is not a member of the Eurozone, it still plays a leading role in the day-to-day workings of the EU. However it has been referred to as a "peculiar" member of the EU, due to its often strained relations with the organisation.
Britain has a century-long alliance with France, through the Entente Cordiale, which was reconfirmed through the November 2010 Defence and Security Co-operation Treaty – setting up a joint expeditionary force, joint naval battlegroup capability and some nuclear collaboration – as well as extremely close cooperation with France over the 2011 Libyan civil war and Libyan no-fly zone.
International Organisations.
The United Kingdom is a member of the following international organisations:
ADB (nonregional member), AfDB (nonregional member), Arctic Council (observer), Australia Group, BIS, Commonwealth of Nations, CBSS (observer), CDB, Council of Europe, CERN, EAPC, EBRD, EIB, ESA, European Union, FAO, FATF, G-20, G-5, G7, G8, G-10, IADB, IAEA, IBRD (also known as the World Bank), ICAO, ICC, ICCt, ICRM, IDA, IEA, IFAD, IFC, IFRCS, IHO, ILO, IMF, IMO, IMSO, Interpol, IOC, IOM, IPU, ISO, ITSO, ITU, ITUC, MIGA, MONUSCO, NATO, NEA, NSG, OAS (observer), OECD, OPCW, OSCE, Paris Club, PCA, PIF (partner), SECI (observer), UN, United Nations Security Council, UNCTAD, UNESCO, UNFICYP, UNHCR, UNIDO, UNMIS, UNRWA, UPU, WCO, WHO, WIPO, WMO, WTO, Zangger Committee

</doc>
<doc id="31734" url="http://en.wikipedia.org/wiki?curid=31734" title="Urea">
Urea

Urea or carbamide is an organic compound with the chemical formula CO(NH2)2. The molecule has two —NH2 groups joined by a carbonyl (C=O) functional group.
Urea serves an important role in the metabolism of nitrogen-containing compounds by animals and is the main nitrogen-containing substance in the urine of mammals. It is a colorless, odorless solid, highly soluble in water and practically non-toxic (LD50 is 15 g/kg for rats). Dissolved in water, it is neither acidic nor alkaline. The body uses it in many processes, the most notable one being nitrogen excretion. Urea is widely used in fertilizers as a convenient source of nitrogen. Urea is also an important raw material for the chemical industry.
The discovery by Friedrich Wöhler in 1828 that urea can be produced from inorganic starting materials was an important conceptual milestone in chemistry, as it showed for the first time that a substance previously known only as a byproduct of life could be synthesized in the laboratory without any biological starting materials, contradicting the widely held doctrine of vitalism.
Related compounds.
The terms urea is also used for a "class" of chemical compounds sharing the same functional group RR'N—CO—NRR', namely a carbonyl group attached to two organic amine residues. Examples include carbamide peroxide, allantoin, and hydantoin. Ureas are closely related to biurets and related in structure to amides, carbamates, carbodiimides, and thiocarbamides.
History.
Urea was first discovered in urine in 1727 by the Dutch scientist Herman Boerhaave, though this discovery is often attributed to the French chemist Hilaire Rouelle.
In 1828, the German chemist Friedrich Wöhler obtained urea artificially by treating silver cyanate with ammonium chloride.
This was the first time an organic compound was artificially synthesized from inorganic starting materials, without the involvement of living organisms. The results of this experiment implicitly discredited vitalism — the theory that the chemicals of living organisms are fundamentally different from those of inanimate matter. This insight was important for the development of organic chemistry. His discovery prompted Wöhler to write triumphantly to Berzelius: "I must tell you that I can make urea without the use of kidneys, either man or dog. Ammonium cyanate is urea." For this discovery, Wöhler is considered by many to be the father of organic chemistry.
Physiology.
Amino acids from ingested food that are not used for the synthesis of proteins and other biological substances — or produced from catabolism of muscle protein — are oxidized by the body, yielding urea and carbon dioxide, as an alternative source of energy. The oxidation pathway starts with the removal of the amino group by a transaminase; the amino group is then fed into the urea cycle. The first step in the conversion of amino acids from protein into metabolic waste in the liver is removal of the alpha-amino nitrogen, which results in ammonia. Because ammonia is toxic, it is excreted immediately by fish, converted into uric acid by birds, and converted into urea by mammals.
Ammonia (NH3) is a common byproduct of the metabolism of nitrogenous compounds. Ammonia is smaller, more volatile and more mobile than urea. If allowed to accumulate, ammonia would raise the pH in cells to toxic levels. Therefore many organisms convert ammonia to urea, even though this synthesis has a net energy cost. Being practically neutral and highly soluble in water, urea is a safe vehicle for the body to transport and excrete excess nitrogen.
Urea is synthesized in the body of many organisms as part of the urea cycle, either from the oxidation of amino acids or from ammonia. In this cycle, amino groups donated by ammonia and L-aspartate are converted to urea, while L-ornithine, citrulline, L-argininosuccinate, and L-arginine act as intermediates. Urea production occurs in the liver and is regulated by N-acetylglutamate. Urea is then dissolved into the blood (in the reference range of 2.5 to 6.7 mmol/liter) and further transported and excreted by the kidney as a component of urine. In addition, a small amount of urea is excreted (along with sodium chloride and water) in sweat.
In water, the amine groups undergo slow displacement by water molecules, producing ammonia, ammonium ion, and bicarbonate ion. For this reason, old, stale urine has a stronger odor than fresh urine.
In humans.
The cycling of and excretion of urea by the kidneys is a vital part of mammalian metabolism. Besides its role as carrier of waste nitrogen, urea also plays a role in the countercurrent exchange system of the nephrons, that allows for re-absorption of water and critical ions from the excreted urine. Urea is reabsorbed in the inner medullary collecting ducts of the nephrons, thus raising the osmolarity in the medullary interstitium surrounding the thin ascending limb of the loop of Henle, which in turn causes water to be reabsorbed. By action of the urea transporter 2, some of this reabsorbed urea will eventually flow back into the thin ascending limb of the tubule, through the collecting ducts, and into the excreted urine.
This mechanism, which is controlled by the antidiuretic hormone, allows the body to create hyperosmotic urine, that has a higher concentration of dissolved substances than the blood plasma. This mechanism is important to prevent the loss of water, to maintain blood pressure, and to maintain a suitable concentration of sodium ions in the blood plasma.
The equivalent nitrogen content (in gram) of urea (in mmol) can be estimated by the conversion factor 0.028 g/mmol. Furthermore, 1 gram of nitrogen is roughly equivalent to 6.25 grams of protein, and 1 gram of protein is roughly equivalent to 5 grams of muscle tissue. In situations such as muscle wasting, 1 mmol of excessive urea in the urine (as measured by urine volume in litres multiplied by urea concentration in mmol/l) roughly corresponds to a muscle loss of 0.67 gram.
In other species.
In aquatic organisms the most common form of nitrogen waste is ammonia, whereas land-dwelling organisms convert the toxic ammonia to either urea or uric acid. Urea is found in the urine of mammals and amphibians, as well as some fish. Birds and saurian reptiles have a different form of nitrogen metabolism that requires less water, and leads to nitrogen excretion in the form of uric acid. It is noteworthy that tadpoles excrete ammonia but shift to urea production during metamorphosis. Despite the generalization above, the urea pathway has been documented not only in mammals and amphibians but in many other organisms as well, including birds, invertebrates, insects, plants, yeast, fungi, and even microorganisms.
Production.
Urea is produced on an industrial scale: In 2012, worldwide production capacity was approximately 184 million tonnes.
Industrial methods.
For use in industry, urea is produced from synthetic ammonia and carbon dioxide. As large quantities of carbon dioxide are produced during the ammonia manufacturing process as a byproduct from hydrocarbons (predominantly natural gas, less often petroleum derivatives), or occasionally from coal, urea production plants are almost always located adjacent to the site where the ammonia is manufactured. Although natural gas is both the most economical and the most widely available ammonia plant feedstock, plants using it do not produce quite as much carbon dioxide from the process as is needed to convert their entire ammonia output into urea. In recent years new technologies such as the KM-CDR process have been developed to recover supplementary carbon dioxide from the combustion exhaust gases produced in the fired reforming furnace of the ammonia synthesis gas plant, allowing operators of stand-alone nitrogen fertilizer complexes to avoid the need to handle and market ammonia as a separate product and also to reduce their 'greenhouse gas' emissions to the atmosphere.
Synthesis.
The basic process, developed in 1922, is also called the Bosch–Meiser urea process after its discoverers. The various commercial urea processes are characterized by the conditions under which urea formation takes place and the way in which unconverted reactants are further processed. The process consists of two main equilibrium reactions, with incomplete conversion of the reactants. The first is carbamate formation: the fast exothermic reaction of liquid ammonia with gaseous carbon dioxide (CO2) at high temperature and pressure to form ammonium carbamate (H2N-COONH4):
The second is urea conversion: the slower endothermic decomposition of ammonium carbamate into urea and water:
The overall conversion of NH3 and CO2 to urea is exothermic, the reaction heat from the first reaction driving the second. Like all chemical equilibria, these reactions behave according to Le Chatelier's principle, and the conditions that most favour carbamate formation have an unfavourable effect on the urea conversion equilibrium. The process conditions are, therefore, a compromise: the ill-effect on the first reaction of the high temperature (around 190°C) needed for the second is compensated for by conducting the process under high pressure (140–175 bar), which favours the first reaction. Although it is necessary to compress gaseous carbon dioxide to this pressure, the ammonia is available from the ammonia plant in liquid form, which can be pumped into the system much more economically. To allow the slow urea formation reaction time to reach equilibrium a large reaction space is needed, so the synthesis reactor in a large urea plant tends to be a massive pressure vessel.
Because the urea conversion is incomplete, the product has to be separated from unchanged ammonium carbamate. In early "straight-through" urea plants this was done by letting down the system pressure to atmospheric so as to allow the carbamate to decompose back to ammonia and carbon dioxide. Originally, because it was not economic to recompress the ammonia and carbon dioxide for recycle, the ammonia at least would be used for the manufacture of other products, for example ammonium nitrate or sulfate. (The carbon dioxide would be wasted, as likely as not.) Later process schemes were developed to allow recycling of the unused ammonia and carbon dioxide. This was accomplished by depressurizing the reaction solution in stages (first to 18–25 bar and then to 2–5 bar) and passing it at each stage through a steam-heated "carbamate decomposer", then recombining the resultant carbon dioxide and ammonia in a falling-film "carbamate condenser" and pumping the carbamate solution into the previous stage.
The stripping concept.
There are two main disadvantages to the "total recycle" concept just outlined. The first is the complexity of the flow scheme and, consequently, the amount of process equipment needed; the second is the amount of water recycled in the carbamate solution, which has an adverse effect on the equilibrium in the urea conversion reaction and thus on the overall efficiency of the plant. The stripping concept, developed in the early 1960s by Stamicarbon in The Netherlands, addressed both problems. It also improved heat recovery and reuse in the process.
The position of the equilibrium in the carbamate formation/decomposition depends on the product of the partial pressures of the reactants. In the total recycle processes carbamate decomposition is promoted by reducing the overall pressure, which reduces the partial pressure of both ammonia and carbon dioxide. But it is possible to achieve a similar effect without lowering the overall pressure by suppressing the partial pressure of just one of the reactants. Instead of being fed directly to the reactor along with the ammonia, as in the total recycle process, in the stripping process the carbon dioxide gas is first routed through a stripper (a carbamate decomposer operating under the full system pressure which is configured to provide maximum gas-liquid contact), flushing out free ammonia and thus reducing its partial pressure over the liquid surface, and carrying it directly to a carbamate condenser (also operating at full system pressure), from which reconstituted ammonium carbamate liquor passes directly to the reactor. That allows the medium-pressure stage of the total recycle process to be omitted altogether.
The stripping concept proved to be such a major advance that competitors such as Snamprogetti – now Saipem – (Italy), the former Montedison (Italy), Toyo Engineering Corporation (Japan) and Urea Casale (Switzerland) all developed their own versions of it. Today effectively all new urea plants use the principle, and many total recycle urea plants have been converted to stripping processes. No radical alternative to it has been proposed; the main thrust of technological development today, in response to industry demands for ever larger individual plants, is directed at reconfiguring and reorientating major items in the plant to reduce their size and the overall height of the plant, as well as at meeting ever more challenging environmental performance targets.
Side reactions.
It is fortunate that the urea conversion reaction is a slow one, because if it were not it would go into reverse in the stripper. As it is, the succeeding stages of the process must be designed to minimize residence times, at least until the temperature has been reduced to the point where the reversion reaction is very slow.
Two reactions produce impurities. Biuret is formed when two molecules of urea combine with the loss of a molecule of ammonia.
Normally this reaction is suppressed in the synthesis reactor by maintaining an excess of ammonia, but after the stripper it will occur until the temperature is reduced. Biuret is undesirable in fertilizer urea because it is toxic to crop plants, although to what extent depends on the nature of the crop and the method of application of the urea. (Biuret is actually welcome in urea when is used as a cattle feed supplement.)
Isocyanic acid results from the thermal decomposition of ammonium cyanate, which is in chemical equilibrium with urea:
This reaction is at its worst when the urea solution is heated at low pressure, which happens when the solution is concentrated for prilling or granulation (see below). The reaction products mostly volatilize into the overhead vapours and recombine when these are condensed to form urea again, which contaminates the process condensate.
Corrosion.
Ammonium carbamate solutions are notoriously corrosive towards metallic materials of construction, even the more resistant forms of stainless steel, especially in the hottest parts of the plant such as the stripper. Traditionally corrosion has been minimized (although not eliminated) by continuously injecting a minor amount of oxygen (as air) into the plant to establish and maintain a passive oxide layer on exposed stainless steel surfaces. But because the carbon dioxide feed is recovered from ammonia synthesis gas it contains traces of hydrogen which can mingle with the passivation air to form an explosive mixture if allowed to accumulate in the plant. In the mid 1990s two duplex (ferritic-austenitic) stainless steels were introduced (DP28W, jointly developed by Toyo Engineering and Sumitomo Metals Industries and Safurex, jointly developed by Stamicarbon and Sandvik Materials Technology (Sweden) ) which have allowed the amount of passivation oxygen to be drastically reduced and can, in theory, operate without oxygen at all.
Saipem now uses either zirconium stripper tubes or bimetallic tubes comprising a titanium body (which is cheaper but less erosion-resistant) to which is metallurgically bonded an internal lining of zirconium. These tubes are fabricated by ATI Wah Chang (USA), a leading specialist in refractory and 'reactive' metals, using its Omegabond technique.
Finishing.
Urea can be produced as prills, granules, pellets, crystals, and solutions.
Solid forms.
For its main use as a fertilizer urea is mostly marketed in solid form, either as prills or granules. The advantage of prills is that, in general, they can be produced more cheaply than granules and that the technique was firmly established in industrial practice long before a satisfactory urea granulation process was commercialized. However, on account of the limited size of particles that can be produced with the desired degree of sphericity and their low crushing and impact strength, the performance of prills during bulk storage, handling and use is generally (with some exceptions) considered inferior to that of granules.
High-quality compound fertilizers containing nitrogen co-granulated with other components such as phosphates have been produced routinely since the beginnings of the modern fertilizer industry, but on account of the low melting point and hygroscopic nature of urea it took courage to apply the same kind of technology to granulate urea on its own. But at the end of the 1970s three companies began to develop fluidized-bed granulation. The first in the field was Nederlandse Stikstof Maatschappij, which later became part of Hydro Agri (now Yara International). Yara eventually sold this technology to Uhde GmbH, whose Uhde Fertilizer Technology (UFT) subsidiary now markets it. Around the same time Toyo Engineering Corporation developed its spouted-bed process, comprising a fluidized bed deliberately agitated to produce turbulent ebullation. Stamicarbon also undertook development work on its own fluidized-bed granulation system, using film sprays rather than atomizing sprays to introduce the urea melt, but shelved it until the 1990s, when there was for a time considerable doubt about the commercial future of the Hydro (UFT) process. As a result, the Stamicarbon technology is now commercialized and highly successful. More recently, Urea Casale has introduced a fluidized-bed granulation system with a difference: the urea is sprayed in laterally from the side walls of the granulator instead of from the bottom so that the bed organizes into two cylindrical masses contrarotating on parallel longitudinal axes. The raw product is stated to be so uniform that screens are unnecessary.
Surprisingly, perhaps, considering the product particles are anything but spherical, pastillation using a Rotoform steel-belt pastillator is beginning to gain ground as a urea particle-forming process as a result of development work by Stamicarbon in collaboration with Sandvik Process Systems (Germany). Single-machine capacity is limited to 175 t/d, but the machines are simple and need little maintenance, specific power consumption is much lower than for granulation, and the product is very uniform. The robustness of the product appears to make up for its very non-spherical shape.
UAN solutions.
In admixture, the combined solubility of ammonium nitrate and urea is so much higher than that of either component alone that it is possible to obtain a stable solution (known as UAN) with a total nitrogen content (32%) approaching that of solid ammonium nitrate (33.5%), though not, of course, that of urea itself (46%). Given the ongoing safety and security concerns surrounding fertilizer-grade solid ammonium nitrate, UAN provides a considerably safer alternative without entirely sacrificing the agronomic properties that make ammonium nitrate more attractive than urea as a fertilizer in areas with short growing seasons. It is also more convenient to store and handle than a solid product and easier to apply accurately to the land by mechanical means.
Laboratory preparation.
Ureas in the more general sense can be accessed in the laboratory by reaction of phosgene with primary or secondary amines, proceeding through an isocyanate intermediate. Non-symmetric ureas can be accessed by reaction of primary or secondary amines with an isocyanate.
Also, urea is produced when phosgene reacts with ammonia:
Historical process.
Urea was first noticed by Hermann Boerhaave in the early 18th century from evaporates of urine. In 1773, Hilaire Rouelle obtained crystals containing urea from human urine by evaporating it and treating it with alcohol in successive filtrations. This method was aided by Carl Wilhelm Scheele's discovery that urine treated by concentrated nitric acid precipitated crystals. Antoine François, comte de Fourcroy and Louis Nicolas Vauquelin discovered in 1799 that the nitrated crystals were identical to Rouelle's substance and invented the term "urea." Berzelius made further improvements to its purification and finally William Prout, in 1817, succeeded in obtaining and determining the chemical composition of the pure substance. In the evolved procedure, urea was precipitated as urea nitrate by adding strong nitric acid to urine. To purify the resulting crystals, they were dissolved in boiling water with charcoal and filtered. After cooling, pure crystals of urea nitrate form. To reconstitute the urea from the nitrate, the crystals are dissolved in warm water, and barium carbonate added. The water is then evaporated and anhydrous alcohol added to extract the urea. This solution is drained off and allowed to evaporate resulting in pure urea.
Chemical properties.
Molecular and crystal structure.
The urea molecule is planar in the crystal structure, but the geometry around the nitrogens is pyramidal in the gas-phase minimum-energy structure. In solid urea, the oxygen center is engaged in two N-H-O hydrogen bonds. The resulting dense and energetically favourable hydrogen-bond network is probably established at the cost of efficient molecular packing: The structure is quite open, the ribbons forming tunnels with square cross-section. The carbon in urea is described as sp2 hybridized, the C-N bonds have significant double bond character, and the carbonyl oxygen is basic compared to, say, formaldehyde. Urea's high aqueous solubility reflects its ability to engage in extensive hydrogen bonding with water.
By virtue of its tendency to form a porous frameworks, urea has the ability to trap many organic compounds. In these so-called clathrates, the organic "guest" molecules are held in channels formed by interpenetrating helices composed of hydrogen-bonded urea molecules. This behaviour can be used to separate mixtures, e.g. in the production of aviation fuel and lubricating oils, and in the separation of hydrocarbons.
As the helices are interconnected, all helices in a crystal must have the same molecular handedness. This is determined when the crystal is nucleated and can thus be forced by seeding. The resulting crystals have been used to separate racemic mixtures.
Reactions.
Urea reacts with alcohols to form urethanes. Urea reacts with malonic esters to make barbituric acids.
Uses.
Agriculture.
More than 90% of world industrial production of urea is destined for use as a nitrogen-release fertilizer. Urea has the highest nitrogen content of all solid nitrogenous fertilizers in common use. Therefore, it has the lowest transportation costs per unit of nitrogen nutrient. The standard crop-nutrient rating (NPK rating) of urea is 46-0-0.
Many soil bacteria possess the enzyme urease, which catalyzes the conversion of the urea to ammonia or ammonium ion and bicarbonate ion, thus urea fertilizers are very rapidly transformed to the ammonium form in soils. Among soil bacteria known to carry urease, some ammonia-oxidizing bacteria (AOB), such as species of "Nitrosomonas", are also able to assimilate the carbon dioxide released by the reaction to make biomass via the Calvin Cycle, and harvest energy by oxidizing ammonia (the other product of urease) to nitrite, a process termed nitrification. Nitrite-oxidizing bacteria, especially "Nitrobacter", oxidize nitrite to nitrate, which is extremely mobile in soils because of its negative charge and is a major cause of water pollution from agriculture. Ammonium and nitrate are readily absorbed by plants, and are the dominant sources of nitrogen for plant growth. Urea is also used in many multi-component solid fertilizer formulations. Urea is highly soluble in water and is, therefore, also very suitable for use in fertilizer solutions (in combination with ammonium nitrate: UAN), e.g., in 'foliar feed' fertilizers. For fertilizer use, granules are preferred over prills because of their narrower particle size distribution, which is an advantage for mechanical application.
The most common impurity of synthetic urea is biuret, which impairs plant growth.
Urea is usually spread at rates of between 40 and 300 kg/ha but rates vary. Smaller applications incur lower losses due to leaching. During summer, urea is often spread just before or during rain to minimize losses from volatilization (process wherein nitrogen is lost to the atmosphere as ammonia gas). Urea is not compatible with other fertilizers.
Because of the high nitrogen concentration in urea, it is very important to achieve an even spread. The application equipment must be correctly calibrated and properly used. Drilling must not occur on contact with or close to seed, due to the risk of germination damage. Urea dissolves in water for application as a spray or through irrigation systems.
In grain and cotton crops, urea is often applied at the time of the last cultivation before planting. In high rainfall areas and on sandy soils (where nitrogen can be lost through leaching) and where good in-season rainfall is expected, urea can be side- or top-dressed during the growing season. Top-dressing is also popular on pasture and forage crops. In cultivating sugarcane, urea is side-dressed after planting, and applied to each ratoon crop.
In irrigated crops, urea can be applied dry to the soil, or dissolved and applied through the irrigation water. Urea will dissolve in its own weight in water, but it becomes increasingly difficult to dissolve as the concentration increases. Dissolving urea in water is endothermic, causing the temperature of the solution to fall when urea dissolves.
As a practical guide, when preparing urea solutions for fertigation (injection into irrigation lines), dissolve no more than 3 g urea per 1 L water.
In foliar sprays, urea concentrations of 0.5% – 2.0% are often used in horticultural crops. Low-biuret grades of urea are often indicated.
Urea absorbs moisture from the atmosphere and therefore is typically stored either in closed/sealed bags on pallets or, if stored in bulk, under cover with a tarpaulin. As with most solid fertilizers, storage in a cool, dry, well-ventilated area is recommended.
Overdose or placing Urea near seed is harmful. 
Chemical industry.
Urea is a raw material for the manufacture of two main classes of materials: urea-formaldehyde resins and urea-melamine-formaldehyde used in marine plywood.
Explosive.
Urea can be used to make urea nitrate, a high explosive that is used industrially and as part of some improvised explosive devices. It is a stabilizer in nitrocellulose explosives.
Automobile systems.
Urea is used in SNCR and SCR reactions to reduce the NOx pollutants in exhaust gases from combustion from Diesel, dual fuel, and lean-burn natural gas engines. The BlueTec system, for example, injects a water-based urea solution into the exhaust system. The ammonia produced by the hydrolysis of the urea reacts with the nitrogen oxide emissions and is converted into nitrogen and water within the catalytic converter. Trucks and cars using these catalytic converters need to carry a supply of diesel exhaust fluid (DEF, also known as AdBlue), a mixture of urea and water.
Laboratory uses.
Urea in concentrations up to 10 M is a powerful protein denaturant as it disrupts the noncovalent bonds in the proteins. This property can be exploited to increase the solubility of some proteins.
A mixture of urea and choline chloride is used as a deep eutectic solvent, a type of ionic liquid.
Urea can in principle serve as a hydrogen source for subsequent power generation in fuel cells. Urea present in urine/wastewater can be used directly (though bacteria normally quickly degrade urea.) Producing hydrogen by electrolysis of urea solution occurs at a lower voltage (0.37V) and thus consumes less energy than the electrolysis of water (1.2V).
Urea in concentrations up to 8 M can be used to make fixed brain tissue transparent to visible light while still preserving fluorescent signals from labeled cells. This allows for much deeper imaging of neuronal processes then previously obtainable using conventional one photon or two photon confocal microscopes.
Medical use.
Urea-containing creams are used as topical dermatological products to promote rehydration of the skin. Urea 40% is indicated for psoriasis, xerosis, onychomycosis, ichthyosis, eczema, keratosis, keratoderma, corns, and calluses. If covered by an occlusive dressing, 40% urea preparations may also be used for nonsurgical debridement of nails. Urea 40% "dissolves the intercellular matrix" of the nail plate. Only diseased or dystrophic nails are removed, as there is no effect on healthy portions of the nail. This drug is also used as an earwax removal aid.
Urea can also be used as a diuretic. It was first used as a diuretic by a Dr. W. Friedrich in 1892. In a 2010 study of ICU patients in Belgium, urea was used as a diuretic to treat euvolemic hyponatremia and was found to be a safe, inexpensive and simple treatment.
Certain types of instant cold packs (or ice packs) contain water and separated urea crystals. Rupturing the internal water bag starts an endothermic reaction and allows the pack to be used to reduce swelling.
Like saline, urea injection is used to perform abortion.
Urea is the main component of an alternative medicinal treatment referred to as urine therapy.
The blood urea nitrogen (BUN) test is a measure of the amount of nitrogen in the blood that comes from urea. It is used as a marker of renal function, though it is inferior to other markers such as creatinine because blood urea levels are influenced by other factors such as diet and dehydration.
Urea labeled with carbon-14 or carbon-13 is used in the urea breath test, which is used to detect the presence of the bacteria "Helicobacter pylori" ("H. pylori") in the stomach and duodenum of humans, associated with peptic ulcers. The test detects the characteristic enzyme urease, produced by "H. pylori", by a reaction that produces ammonia from urea. This increases the pH (reduces acidity) of the stomach environment around the bacteria. Similar bacteria species to "H. pylori" can be identified by the same test in animals such as apes, dogs, and cats (including big cats).
Analysis.
Urea is readily quantified by a number of different methods, such as the diacetyl monoxime colorimetric method, and the Berthelot reaction (after initial conversion of urea to ammonia via urease). These methods are amenable to high throughput instrumentation, such as automated flow injection analyzers and 96-well micro-plate spectrophotometers.
Safety.
Urea can be irritating to skin, eyes, and the respiratory tract. Repeated or prolonged contact with urea in fertilizer form on the skin may cause dermatitis.
High concentrations in the blood can be damaging. Ingestion of low concentrations of urea, such as are found in typical human urine, are not dangerous with additional water ingestion within a reasonable time-frame. Many animals (e.g., dogs) have a much more concentrated urine and it contains a higher urea amount than normal human urine; this can prove dangerous as a source of liquids for consumption in a life-threatening situation (such as in a desert).
Urea can cause algal blooms to produce toxins, and its presence in the runoff from fertilized land may play a role in the increase of toxic blooms.
The substance decomposes on heating above melting point, producing toxic gases, and reacts violently with strong oxidants, nitrites, inorganic chlorides, chlorites and perchlorates, causing fire and explosion.

</doc>
<doc id="31736" url="http://en.wikipedia.org/wiki?curid=31736" title="Uric acid">
Uric acid

Uric acid is a heterocyclic compound of carbon, nitrogen, oxygen, and hydrogen with the formula C5H4N4O3. It forms ions and salts known as urates and acid urates, such as ammonium acid urate. Uric acid is a product of the metabolic breakdown of purine nucleotides. High blood concentrations of uric acid can lead to gout. The chemical is associated with other medical conditions including diabetes and the formation of ammonium acid urate kidney stones.
Chemistry.
Uric acid is a diprotic acid with pKa1=5.4 and pKa2=10.3. Thus in strong alkali at high pH, it forms the dually charged full urate ion, but at biological pH or in the presence of carbonic acid or carbonate ions, it forms the singly charged hydrogen or acid urate ion, as its pKa1 is lower than the pKa1 of carbonic acid. As its second ionization is so weak, the full urate salts tend to hydrolyze back to hydrogen urate salts and free base at pH values around neutral. It is aromatic because of the purine functional group.
As a bicyclic, heterocyclic purine derivative, uric acid does not protonate as an oxygen [-OH] like carboxylic acids does. X-ray diffraction studies on the hydrogen urate ion in crystals of ammomium hydrogen urate, formed "in vivo" as gouty deposits, reveal the keto-oxygen in the 2 position of a tautomer of the purine structure exists as a hydroxyl group and the two flanking nitrogen atoms at the 1 and 3 positions share the ionic charge in the six-membered pi-resonance-stabilized ring.
Thus, while most organic acids are deprotonated by the ionization of a polar hydrogen-to-oxygen bond, usually accompanied by some form of resonance stabilization (resulting in a carboxylate ion), uric acid is deprotonated at a nitrogen atom and uses a tautomeric keto/hydroxy group as an electron-withdrawing group to increase the pK1 value. The five-membered ring also possesses a keto group (in the 8 position), flanked by two secondary amino groups (in the 7 and 9 positions), and deprotonation of one of these at high pH could explain the pK2 and behavior as a diprotic acid. Similar tautomeric rearrangement and pi-resonance stabilization would then give the ion some degree of stability.
Uric acid was first isolated from kidney stones in 1776 by Scheele. As far as laboratory synthesis is concerned, in 1882, Ivan Horbaczewski claimed to have prepared uric acid by melting urea hydrogen peroxide with glycine, trichlorolactic acid, and its amide. Soon after, repetition by Eduard Hoffmann shows that this preparation with glycine gives no trace of uric acid, but trichlorolacetamide produces some uric acid. Thus, Hoffmann was the first to synthesize uric acid.
Solubility.
In general, the water solubility of uric acid and its alkali metal and alkaline earth salts is rather low. All these salts exhibit greater solubility in hot water than cold, allowing for easy recrystallization. This low solubility is significant for the etiology of gout. The solubility of the acid and its salts in ethanol is very low or negligible. In ethanol water mixtures, the solubilities are somewhere between the end values for pure ethanol and pure water.
The figures given indicate what mass of water is required to dissolve a unit mass of compound indicated. The lower the number the more soluble the substance in the said solvent.
Biology.
The enzyme xanthine oxidase makes uric acid from xanthine and hypoxanthine, which in turn are produced from other purines. Xanthine oxidase is a large enzyme whose active site consists of the metal molybdenum bound to sulfur and oxygen. Within cells, xanthine oxidase can exist as xanthine dehydrogenase and xanthine oxireductase, which has also been purified from bovine milk and spleen extracts. Uric acid is released in hypoxic conditions.
In humans and higher primates, uric acid is the final oxidation (breakdown) product of purine metabolism and is excreted in urine. In most other mammals, the enzyme uricase further oxidizes uric acid to allantoin. The loss of uricase in higher primates parallels the similar loss of the ability to synthesize ascorbic acid, leading to the suggestion that urate may partially substitute for ascorbate in such species. Both uric acid and ascorbic acid are strong reducing agents (electron donors) and potent antioxidants. In humans, over half the antioxidant capacity of blood plasma comes from uric acid.
The Dalmatian dog has a genetic defect in uric acid uptake by the liver and kidneys, resulting in decreased conversion to allantoin, so this breed excretes uric acid, and not allantoin, in the urine.
In birds and reptiles, and in some desert dwelling mammals (e.g., the kangaroo rat), uric acid also is the end-product of purine metabolism, but it is excreted in feces as a dry mass. This involves a complex metabolic pathway that is energetically costly in comparison to processing of other nitrogenous wastes such as urea (from urea cycle) or ammonia, but has the advantages of reducing water loss and, hence, reducing the need for water.
In humans, about 70% of daily uric acid disposal occurs via the kidneys, and in 5-25% of humans, impaired renal (kidney) excretion leads to hyperuricemia.
Genetics.
A proportion of people have mutations in the proteins responsible for the excretion of uric acid by the kidneys. Variants within a number of genes have so far been identified:
"SLC2A9"; "ABCG2"; "SLC17A1"; "SLC22A11"; "SLC22A12";
"SLC16A9"; "GCKR"; "LRRC16A"; and "PDZK1".
 "SLC2A9" is known to transport both uric acid and fructose.
Medicine.
In human blood plasma, the reference range of uric acid is typically 3.4-7.2 mg/dL (200-430 µmol/L) for men (1 mg/dL=59.48 µmol/L), and 2.4-6.1 mg/dL for women (140-360 µmol/L). However, blood test results should always be interpreted using the range provided by the laboratory that performed the test. Uric acid concentrations in blood plasma above and below the normal range are known, respectively, as hyperuricemia and hypouricemia. Likewise, uric acid concentrations in urine above and below normal are known as hyperuricosuria and hypouricosuria. Such abnormal concentrations of uric acid are not medical conditions, but are associated with a variety of medical conditions.
High uric acid.
High levels of uric acid is called hyperuricemia and can lead to gout.
Gout.
Excess serum accumulation of uric acid in the blood can lead to a type of arthritis known as gout. This painful condition is the result of needle-like crystals of uric acid precipitating in joints, capillaries, skin, and other tissues. Kidney stones can also form through the process of formation and deposition of sodium urate microcrystals.
A study found that men who drink two or more sugar-sweetened beverages a day have an 85% higher chance of developing gout than those who drank such beverages infrequently.
Gout can occur where serum uric acid levels are as low as 6 mg/dL (~357 µmol/L), but an individual can have serum values as high as 9.6 mg/dL (~565 µmol/L) and not have gout.
One treatment for gout, in the 19th century, had been administration of lithium salts; lithium urate is more soluble. Today, inflammation during attacks is more commonly treated with NSAIDs or corticosteroids, and urate levels are managed with allopurinol. Allopurinol, developed over 30 years ago by Elion et al., weakly inhibits xanthine oxidase. It is an analog of hypoxanthine that is hydroxylated by xanthine oxidoreductase at the 2-position to give oxipurinol. Oxipurinol has been supposed to bind tightly to the reduced molybdenum ion in the enzyme and, thus, inhibits uric acid synthesis.
Lesch-Nyhan syndrome.
Lesch-Nyhan syndrome, an extremely rare inherited disorder, is also associated with very high serum uric acid levels. Spasticity, involuntary movement, and cognitive retardation as well as manifestations of gout are seen in cases of this syndrome.
Cardiovascular disease.
Although uric acid can act as an antioxidant, excess serum accumulation is often associated with cardiovascular disease. It is not known whether this is causative (e.g., by acting as a prooxidant) or a protective reaction taking advantage of urate's antioxidant properties. The same may account for the putative role of uric acid in the etiology of stroke.
Type 2 diabetes.
The association of high serum uric acid with insulin resistance has been known since the early part of the 20th century, but the hypothesis that high serum uric acid is a risk factor for diabetes has long been a matter of debate. In fact, hyperuricemia was presumed to be a consequence of insulin resistance rather than its precursor. However, a prospective follow-up study showed high serum uric acid is associated with higher risk of type 2 diabetes, independent of obesity, dyslipidemia, and hypertension.
Metabolic syndrome.
Hyperuricemia is associated with components of metabolic syndrome. A study has suggested fructose-induced hyperuricemia may play a pathogenic role in the metabolic syndrome. This is consistent with the increased consumption in recent decades of fructose-containing beverages (such as fruit juices and soft drinks sweetened with sugar and high-fructose corn syrup) and the epidemic of diabetes and obesity.
Uric acid stone formation.
Saturation levels of uric acid in blood may result in one form of kidney stones when the urate crystallizes in the kidney. These uric acid stones are radiolucent and so do not appear on an abdominal plain X-ray, and thus their presence must be diagnosed by ultrasound for this reason or stone protocol CT. Very large stones may be detected on X-ray by their displacement of the surrounding kidney tissues.
Uric acid stones, which form in the absence of secondary causes such as chronic diarrhea, vigorous exercise, dehydration, and animal protein loading, are felt to be secondary to obesity and insulin resistance seen in metabolic syndrome. Increased dietary acid leads to increased endogenous acid production in the liver and muscles, which in turn leads to an increased acid load to the kidneys. This load is handled more poorly because of renal fat infiltration and insulin resistance, which are felt to impair ammonia excretion (a buffer). The urine is, therefore, quite acidic, and uric acid becomes insoluble, crystallizes and stones form. In addition, naturally present promoter and inhibitor factors may be affected. This explains the high prevalence of uric stones and unusually acidic urine seen in patients with type 2 diabetes. Uric acid crystals can also promote the formation of calcium oxalate stones, acting as "seed crystals" (heterogeneous nucleation).
Low Uric Acid.
Causes of low uric acid.
Low uric acid (hypouricemia) can have numerous causes.
Low dietary zinc intakes cause lower uric acid levels. This effect can be even more pronounced in women taking oral contraceptive medication.
Xanthine oxidase is an Fe-Mo enzyme, so people with Fe deficiency (the most common cause of anemia in young women) or Mo deficiency can experience hypouricemia.
Xanthine oxidase loses its function and gains ascorbase function when some of the Fe atoms in XO are replaced with Cu atoms. As such, people with high Cu/Fe can experience hypouricemia and vitamin C deficiency, resulting in oxidative damage. Since estrogen increases the half-life of Cu, women with very high estrogen levels and intense blood loss during menstruation are likely to have a high Cu/Fe and present with hypouricemia.
Sevelamer, a drug indicated for prevention of hyperphosphataemia in patients with chronic renal failure, can significantly reduce serum uric acid.
But the main cause of congenitally low uric acid, sometimes as low as zero, remains the Molybdenum cofactor deficiency.
Multiple sclerosis.
Lower serum values of uric acid have been associated with multiple sclerosis (MS). MS patients have been found to have serum levels ~194 µmol/L, with patients in relapse averaging ~160 µmol/L and patients in remission averaging ~230 µmol/L. Serum uric acid in healthy controls was ~290 µmol/L. Conversion factor: 1 mg/dL=59.48 µmol/L
A 1998 study completed a statistical analysis of 20 million patient records, comparing serum uric acid values in patients with gout and patients with multiple sclerosis. Almost no overlap between the groups was found.
Uric acid has been successfully used in the treatment and prevention of the animal (murine) model of MS. A 2006 study found elevation of serum uric acid values in multiple sclerosis patients, by oral supplementation with inosine, resulted in lower relapse rates, and no adverse effects.
Normalizing low uric acid.
Correcting low or deficient zinc levels can help elevate serum uric acid. Inosine can be used to elevate uric acid levels.
Zn inhibits Cu absorption, helping to reduce the high Cu/Fe in some people with hypouricemia. Fe supplements can ensure adequate Fe reserves (ferritin above 25 ng/dl), also correcting the high Cu/Fe.
Oxidative stress.
Uric acid may be a marker of oxidative stress, and may have a potential therapeutic role as an antioxidant. On the other hand, like other strong reducing substances such as ascorbate, uric acid can also act as a prooxidant. Thus, it is unclear whether elevated levels of uric acid in diseases associated with oxidative stress such as stroke and atherosclerosis are a protective response or a primary cause.
For example, some researchers propose hyperuricemia-induced oxidative stress is a cause of metabolic syndrome. On the other hand, plasma uric acid levels correlate with longevity in primates and other mammals. This is presumably a function of urate's antioxidant properties.
Correlations with creative output.
Havelock Ellis found in his "A Study of British Genius (1904)" that there was an unusually high rate of gout among eminent men in his study, and gout is associated with higher volumes of uric acid in the blood. He therefore suggested that it might have something to do with it. Later investigators have examined this relationship, and there is indeed a correlation. A review is Jensen & Sinha (1993), which found only a slight correlation between IQ and serum urate level (SUL), however there was a stronger correlation between SUL and scholastic achievement, even after controlling for IQ. Another study found a correlation of +.37 between serum urate level and publication rates of university professors. Jensen speculates that it may be due to uric acid's having a similar chemical structure to that of caffeine, and thus acting as a natural stimulant.

</doc>
<doc id="31737" url="http://en.wikipedia.org/wiki?curid=31737" title="Supreme Court of the United States">
Supreme Court of the United States

The Supreme Court of the United States (SCOTUS) was established pursuant to Article III of the United States Constitution in 1789 as the highest federal court in the United States. It has ultimate (and largely discretionary) appellate jurisdiction over all federal courts and over state court cases involving issues of federal law, plus original jurisdiction over a small range of cases. In the legal system of the United States, the Supreme Court is the final interpreter of federal constitutional law, although it may only act within the context of a case in which it has jurisdiction.
The Court consists of the Chief Justice of the United States and eight associate justices who are nominated by the President and confirmed by the Senate. Once appointed, justices have life tenure unless they resign, retire, take senior status, or are removed after impeachment (though no justice has ever been removed). In modern discourse, the justices are often categorized as having conservative, moderate, or liberal philosophies of law and of judicial interpretation. Each justice has one vote, and while many cases are decided unanimously, many of the highest profile cases often expose ideological beliefs that track with those philosophical or political categories. The Court meets in the United States Supreme Court Building in Washington, D.C.
History.
The ratification of the United States Constitution established the Supreme Court in 1789. Its powers are detailed in Article Three of the Constitution. The Supreme Court is the only court specifically established by the Constitution, and all the others were created by Congress. Congress is also responsible for conferring the title "justice" upon the associate justices, who have been known to scold lawyers for instead using the term "judge", which is the term used by the Constitution.
The Court first convened on February 2, 1790, by which time five of its six initial positions had been filled. The sixth member (James Iredell) joined on May 12, 1790. Because the full Court had only six members, every decision that it made by a majority was also made by two-thirds (voting four to two). However, Congress has always allowed less than the Court's full membership to make decisions, starting with a quorum of four judges in 1789.
Earliest beginnings to Marshall.
Under Chief Justices Jay, Rutledge, and Ellsworth (1789–1801), the Court heard few cases; its first decision was "West v. Barnes" (1791), a case involving a procedural issue. The Court lacked a home of its own and had little prestige, a situation not helped by the highest-profile case of the era, "Chisholm v. Georgia" (1793), which was reversed within two years by the adoption of the Eleventh Amendment.
The Court's power and prestige waxed during the Marshall Court (1801–1835). Under Marshall, the Court established the power of judicial review over acts of Congress, including specifying itself as the supreme expositor of the Constitution ("Marbury v. Madison") and made several important constitutional rulings giving shape and substance to the balance of power between the federal government and the states (prominently, "Martin v. Hunter's Lessee", "McCulloch v. Maryland" and "Gibbons v. Ogden").
The Marshall Court also ended the practice of each justice issuing his opinion "seriatim", a remnant of British tradition, and instead issuing a single majority opinion. Also during Marshall's tenure, although beyond the Court's control, the impeachment and acquittal of Justice Samuel Chase in 1804–1805 helped cement the principle of judicial independence.
From Taney to Taft.
The Taney Court (1836–1864) made several important rulings, such as "Sheldon v. Sill", which held that while Congress may not limit the subjects the Supreme Court may hear, it may limit the jurisdiction of the lower federal courts to prevent "them" from hearing cases dealing with certain subjects. Nevertheless, it is primarily remembered for its ruling in "Dred Scott v. Sandford", which may have helped precipitate the Civil War. In the Reconstruction era, the Chase, Waite, and Fuller Courts (1864–1910) interpreted the new Civil War amendments to the Constitution and developed the doctrine of substantive due process ("Lochner v. New York"; "Adair v. United States").
Under the White and Taft Courts (1910–1930), the Court held that the Fourteenth Amendment had incorporated some guarantees of the Bill of Rights against the states ("Gitlow v. New York"),
grappled with the new antitrust statutes ("Standard Oil Co. of New Jersey v. United States"), upheld the constitutionality of military conscription ("Selective Draft Law Cases") and brought the substantive due process doctrine to its first apogee ("Adkins v. Children's Hospital").
The New Deal era.
During the Hughes, Stone, and Vinson Courts (1930–1953), the Court gained its own accommodation in 1935 and changed its interpretation of the Constitution, giving a broader reading to the powers of the federal government to facilitate President Franklin Roosevelt's New Deal (most prominently "West Coast Hotel Co. v. Parrish, Wickard v. Filburn", "United States v. Darby" and "United States v. Butler"). During World War II, the Court continued to favor government power, upholding the internment of Japanese citizens ("Korematsu v. United States") and the mandatory pledge of allegiance ("Minersville School District v. Gobitis"). Nevertheless, "Gobitis" was soon repudiated ("West Virginia State Board of Education v. Barnette"), and the "Steel Seizure Case" restricted the pro-government trend.
Warren and Burger.
The Warren Court (1953–1969) dramatically expanded the force of Constitutional civil liberties. It held that segregation in public schools violates equal protection ("Brown v. Board of Education", "Bolling v. Sharpe" and "Green v. County School Bd.") and that traditional legislative district boundaries violated the right to vote ("Reynolds v. Sims"). It created a general right to privacy ("Griswold v. Connecticut"), limited the role of religion in public school (most prominently "Engel v. Vitale" and "Abington School District v. Schempp"), incorporated most guarantees of the Bill of Rights against the States—prominently "Mapp v. Ohio" (the exclusionary rule) and "Gideon v. Wainwright" (right to appointed counsel),—and required that criminal suspects be apprised of all these rights by police ("Miranda v. Arizona"); At the same time, however, the Court limited defamation suits by public figures ("New York Times v. Sullivan") and supplied the government with an unbroken run of antitrust victories.
The Burger Court (1969–1986) expanded "Griswold"'s right to privacy to strike down abortion laws ("Roe v. Wade"), but divided deeply on affirmative action ("Regents of the University of California v. Bakke") and campaign finance regulation ("Buckley v. Valeo"), and dithered on the death penalty, ruling first that most applications were defective ("Furman v. Georgia"), then that the death penalty itself was "not" unconstitutional ("Gregg v. Georgia").
Rehnquist and Roberts.
The Rehnquist Court (1986–2005) was noted for its revival of judicial enforcement of federalism, emphasizing the limits of the Constitution's affirmative grants of power ("United States v. Lopez") and the force of its restrictions on those powers ("Seminole Tribe v. Florida", "City of Boerne v. Flores"). It struck down single-sex state schools as a violation of equal protection ("United States v. Virginia"), laws against sodomy as violations of substantive due process ("Lawrence v. Texas"), and the line item veto ("Clinton v. New York"), but upheld school vouchers ("Zelman v. Simmons-Harris") and reaffirmed "Roe"'s restrictions on abortion laws ("Planned Parenthood v. Casey"). The Court's decision in "Bush v. Gore", which ended the electoral recount during the presidential election of 2000, was controversial.
The Roberts Court (2005–present) is regarded by some as more conservative than the Rehnquist Court. Some of its major rulings have concerned federal preemption ("Wyeth v. Levine"), civil procedure ("Twombly-Iqbal"), abortion ("Gonzales v. Carhart"), climate change ("Massachusetts v. EPA"), and the Bill of Rights, prominently "Citizens United v. Federal Election Commission" (First Amendment), "Heller-McDonald" (Second Amendment), and "Baze v. Rees" (Eighth Amendment).
Composition.
Size of the Court.
Article III of the United States Constitution leaves it to Congress to fix the number of justices. The Judiciary Act of 1789 called for the appointment of six justices, and as the nation's boundaries grew, Congress added justices to correspond with the growing number of judicial circuits: seven in 1807, nine in 1837, and ten in 1863.
In 1866, at the behest of Chief Justice Chase, Congress passed an act providing that the next three justices to retire would not be replaced, which would thin the bench to seven justices by attrition. Consequently, one seat was removed in 1866 and a second in 1867. In 1869, however, the Circuit Judges Act returned the number of justices to nine, where it has since remained.
President Franklin D. Roosevelt attempted to expand the Court in 1937. His proposal envisioned appointment of one additional justice for each incumbent justice who reached the age of 70 years 6 months and refused retirement, up to a maximum bench of 15 justices. The proposal was ostensibly to ease the burden of the docket on elderly judges, but the actual purpose was widely understood as an effort to pack the Court with justices who would support Roosevelt's New Deal. The plan, usually called the "Court-packing Plan", failed in Congress. Nevertheless, the Court's balance began to shift within months when Justice van Devanter retired and was replaced by Senator Hugo Black. By the end of 1941, Roosevelt had appointed seven justices and elevated Harlan Fiske Stone to Chief Justice.
Appointment and confirmation.
The President of the United States appoints justices "by and with the advice and consent of the Senate." Most presidents nominate candidates who broadly share their ideological views, although a justice's decisions may end up being contrary to a president's expectations. Because the Constitution sets no qualifications for service as a justice, a president may nominate anyone to serve, subject to Senate confirmation.
In modern times, the confirmation process has attracted considerable attention from the press and advocacy groups, which lobby senators to confirm or to reject a nominee depending on whether their track record aligns with the group's views. The Senate Judiciary Committee conducts hearings and votes on whether the nomination should go to the full Senate with a positive, negative or neutral report. The committee's practice of personally interviewing nominees is relatively recent. The first nominee to appear before the committee was Harlan Fiske Stone in 1925, who sought to quell concerns about his links to Wall Street, and the modern practice of questioning began with John Marshall Harlan II in 1955. Once the committee reports out the nomination, the full Senate considers it. Rejections are relatively uncommon; the Senate has explicitly rejected twelve Supreme Court nominees, most recently Robert Bork in 1987.
Nevertheless, not every nominee has received a floor vote in the Senate. Although Senate rules do not necessarily allow a negative vote in committee to block a nomination, a nominee may be filibustered once debate has begun in the full Senate. No nomination for associate justice has ever been filibustered, but President Lyndon Johnson's nomination of sitting Associate Justice Abe Fortas to succeed Earl Warren as Chief Justice was successfully filibustered in 1968. A president may also withdraw a nomination before the actual confirmation vote occurs, typically because it is clear that the Senate will reject the nominee, most recently Harriet Miers in 2006.
Once the Senate confirms a nomination, the president must prepare and sign a commission, to which the Seal of the Department of Justice must be affixed, before the new justice can take office. The seniority of an associate justice is based on the commissioning date, not the confirmation or swearing-in date.
Before 1981, the approval process of justices was usually rapid. From the Truman through Nixon administrations, justices were typically approved within one month. From the Reagan administration to the present, however, the process has taken much longer. Some believe this is because Congress sees justices as playing a more political role than in the past.
Recess appointments.
When the Senate is in recess, a president may make temporary appointments to fill vacancies. Recess appointees hold office only until the end of the next Senate session (at most, less than two years). The Senate must confirm the nominee for them to continue serving; of the two chief justices and six associate justices who have received recess appointments, only Chief Justice John Rutledge was not subsequently confirmed.
No president since Dwight D. Eisenhower has made a recess appointment to the Court, and the practice has become rare and controversial even in lower federal courts. In 1960, after Eisenhower had made three such appointments, the Senate passed a "sense of the Senate" resolution that recess appointments to the Court should only be made in "unusual circumstances." Such resolutions are not legally binding but are an expression of Congress's views in the hope of guiding executive action.
Tenure.
The Constitution provides that justices "shall hold their offices during good behavior" (unless appointed during a Senate recess). The term "good behavior" is understood to mean justices may serve for the remainder of their lives, unless they are impeached and convicted by Congress, resign or retire. Only one justice has been impeached by the House of Representatives (Samuel Chase, March 1804), but he was acquitted in the Senate (March 1805). Moves to impeach sitting justices have occurred more recently (for example, William O. Douglas was the subject of hearings twice, in 1953 and again in 1970; and Abe Fortas resigned while hearings were being organized), but they did not reach a vote in the House. No mechanism exists for removing a justice who is permanently incapacitated by illness or injury, but unable (or unwilling) to resign.
Because justices have indefinite tenure, timing of vacancies can be unpredictable. Sometimes vacancies arise in quick succession, as in the early 1970s when Lewis Franklin Powell, Jr. and William Rehnquist were nominated to replace Hugo Black and John Marshall Harlan II, who retired within a week of each other. Sometimes a great length of time passes between nominations, such as the eleven years between Stephen Breyer's nomination in 1994 to succeed Harry Blackmun and the nomination of John Roberts in 2005 to fill the seat of Sandra Day O'Connor (though Roberts' nomination was withdrawn and resubmitted for the role of Chief Justice after Rehnquist died).
Despite the variability, all but four Presidents have been able to appoint at least one justice. William Henry Harrison died a month after taking office, though his successor (John Tyler) made an appointment during that presidential term. Likewise, Zachary Taylor died early in his term, but his successor (Millard Fillmore) also made a Supreme Court nomination before the end of that term. Andrew Johnson, who became President after the assassination of Abraham Lincoln, was denied the opportunity to appoint a justice by a reduction in the size of the Court. Jimmy Carter is the only President to complete at least one term in office without making a nomination to the Court during his presidency.
Three presidents have appointed justices who collectively served more than 100 years: Franklin D. Roosevelt, Andrew Jackson and Abraham Lincoln.
Membership.
Current justices.
Court demographics.
The Court currently has six male and three female justices. One justice is African American, one is Latino, and two are Italian-Americans; six justices are Roman Catholics, and three are Jewish. The average age is . Every current justice has an Ivy League background. Four justices are from the state of New York, two from New Jersey, two from California, and one from Georgia.
In the 19th century, every justice was a male of European descent, almost always Protestant and of Northern European descent, and concerns about diversity focused on geography, to represent all regions of the country, rather than ethnic, religious, or gender diversity. Thurgood Marshall became the first African American Justice in 1967, and Sandra Day O'Connor became the first female Justice in 1981. O'Connor, whose appointment fulfilled Ronald Reagan's campaign promise to place a woman on the Court, was later joined by Ruth Bader Ginsburg, appointed by Bill Clinton in 1993. Marshall was succeeded by Clarence Thomas in 1991, who is the second African American to serve on the Supreme Court. In 1986, Antonin Scalia became the first Italian-American to serve on the Court. After O'Connor had in 2006 been succeeded by Samuel Alito, Ginsburg was in 2009 joined by Sonia Sotomayor, the first Latino justice, and in 2010 by Elena Kagan, so that there were three female justices.
Most justices have been Protestants, including 35 Episcopalians, 19 Presbyterians, 10 Unitarians, five Methodists, and three Baptists. The first Catholic justice was Roger Taney in 1836, and 1916 saw the appointment of the first Jewish justice, Louis Brandeis. In recent years this situation has reversed: after the retirement of Justice Stevens in June 2010, the Court is without a Protestant for the first time in its history.
Retired justices.
There are currently three living retired justices of the Supreme Court of the United States: John Paul Stevens, Sandra Day O'Connor, and David Souter. As retired justices, they no longer participate in the work of the Supreme Court, but may be designated for temporary assignments to sit on lower federal courts, usually the United States Courts of Appeals. Such assignments are formally made by the Chief Justice, on request of the Chief Judge of the lower court and with the consent of the retired Justice. In recent years, Justice O'Connor has sat with several Courts of Appeals around the country, and Justice Souter has frequently sat on the First Circuit, the court of which he was briefly a member before joining the Supreme Court.
The status of a retired Justice is analogous to that of a Circuit or District Judge who has taken senior status, and eligibility of a Supreme Court Justice to assume retired status (rather than simply resign from the bench) is governed by the same age and service criteria.
Justices sometimes strategically plan their decisions to leave the bench, with personal, institutional, and partisan factors playing a role. The fear of mental decline and death often motivates justices to step down. The desire to maximize the Court's strength and legitimacy through one retirement at a time, when the Court is in recess, and during non-presidential election years suggests a concern for institutional health. Finally, especially in recent decades, many justices have timed their departure to coincide with a compatible president holding office to ensure that a like-minded successor would be appointed.
Seniority and seating.
Many of the internal operations of the Court are organized by the seniority of the justices; the Chief Justice is considered the most senior member of the Court, regardless of the length of his or her service. The Associate Justices are then ranked by the length of their service.
During Court sessions, the justices sit according to seniority, with the Chief Justice in the center, and the Associate Justices on alternating sides, with the most senior Associate Justice on the Chief Justice's immediate right, and the most junior Associate Justice seated on the left farthest away from the Chief Justice. Therefore, the current court sits as follows from left to right, from the perspective of those facing the Court: Sotomayor, Breyer, Thomas, Scalia (most senior Associate Justice), Roberts (Chief Justice), Kennedy, Ginsburg, Alito, and Kagan (most junior Associate Justice). In the official yearly Court photograph, justices are arranged similarly, with the five most senior members sitting in the front row in the same order as they would sit during Court sessions (Thomas, Scalia, Roberts, Kennedy, Ginsburg), and the four most junior justices standing behind them, again in the same order as they would sit during Court sessions (Sotomayor, Breyer, Alito, Kagan).
In the justices' private conferences, the current practice is for them to speak and vote in order of seniority from the Chief Justice first to the most junior Associate Justice last. The most junior Associate Justice in these conferences is charged with any menial tasks the justices may require as they convene alone, such as answering the door of their conference room, serving coffee, and transmitting the orders of the Court to the court's clerk.
Justice Joseph Story served the longest as the junior justice, from February 3, 1812, to September 1, 1823, for a total of 4,228 days. Justice Stephen Breyer follows close behind, with 4,199 days when Samuel Alito joined the court on January 31, 2006.
Salary.
For the years 2009 through 2012, associate justices were paid $213,900 and the chief justice $223,500. Article III, Section 1 of the U.S. Constitution prohibits Congress from reducing the pay for incumbent justices. Once a justice meets age and service requirements, the justice may retire. Judicial pensions are based on the normal formula for federal employees, but a justice's pension will never be less than their salary at time of retirement. (The same procedure applies to judges of other federal courts.)
Judicial leanings.
While justices do not represent or receive official endorsements from political parties, as is accepted practice in the legislative and executive branches, jurists are informally categorized in legal and political circles as being judicial conservatives, moderates, or liberals. Such leanings, however, generally refer to legal outlook rather than a political or legislative one.
As of the October 2012 term, the Court consists of five justices appointed by Republican presidents and four appointed by Democratic presidents. It is popularly accepted that Chief Justice Roberts and justices Scalia, Thomas, and Alito (appointed by Republican presidents) comprise the Court's conservative wing. Justices Ginsburg, Breyer, Sotomayor, and Kagan (appointed by Democratic presidents) comprise the Court's liberal wing. Justice Kennedy (appointed by President Reagan) is generally considered "a conservative who has occasionally voted with liberals", and is often the swing vote that determines the outcome of close cases.
Tom Goldstein argued in an article in SCOTUSblog in 2010, that the popular view of the Supreme Court as sharply divided along ideological lines and each side pushing an agenda at every turn is "in significant part a caricature designed to fit certain preconceptions." He points out that in the 2009 term, almost half the cases were decided unanimously, and only about 20% were decided by a 5-to-4 vote. Barely one in ten cases involved the narrow liberal/conservative divide (fewer if the cases where Sotomayor recused herself are not included). He also pointed to several cases that defy the popular conception of the ideological lines of the Court.
Goldstein further argued that the large number of pro-criminal-defendant summary dismissals (usually cases where the justices decide that the lower courts significantly misapplied precedent and reverse the case without briefing or argument) are an illustration that the conservative justices have not been aggressively ideological. Likewise, Goldstein stated that the critique that the liberal justices are more likely to invalidate acts of Congress, show inadequate deference to the political process, and be disrespectful of precedent, also lacks merit: Thomas has most often called for overruling prior precedent (even if long standing) that he views as having been wrongly decided, and during the 2009 term Scalia and Thomas voted most often to invalidate legislation.
According to statistics compiled by SCOTUSblog, in the twelve terms from 2000 to 2011, an average of 19 of the opinions on major issues (22%) were decided by a 5–4 vote, with an average of 70% of those split opinions decided by a Court divided along the traditionally perceived ideological lines (about 15% of all opinions issued). Over that period, the conservative bloc has been in the majority about 62% of the time that the Court has divided along ideological lines, which represents about 44% of all the 5–4 decisions.
In the October 2010 term, the Court decided 86 cases, including 75 signed opinions and 5 summary reversals (where the Court reverses a lower court without arguments and without issuing an opinion on the case). Four were decided with unsigned opinions, two cases affirmed by an equally divided Court, and two cases were dismissed as improvidently granted. Justice Kagan recused herself from 26 of the cases due to her prior role as United States Solicitor General. Of the 80 cases, 38 (about 48%, the highest percentage since the October 2005 term) were decided unanimously (9–0 or 8–0), and 16 decisions were made by a 5–4 vote (about 20%, compared to 18% in the October 2009 term, and 29% in the October 2008 term). However, in fourteen of the sixteen 5–4 decisions, the Court divided along the traditional ideological lines (with Ginsburg, Breyer, Sotomayor, and Kagan on the liberal side, and Roberts, Scalia, Thomas, and Alito on the conservative, and Kennedy providing the "swing vote"). This represents 87% of those 16 cases, the highest rate in the past 10 years. The conservative bloc, joined by Kennedy, formed the majority in 63% of the 5–4 decisions, the highest cohesion rate of that bloc in the Roberts court.
In the October 2011 term, the Court decided 75 cases. Of these, 33 (about 44%) were decided unanimously, and 15 (about 20%, the same percentage as in the previous term) were decided by a vote of 5–4. Of the latter 15, the Court divided along the perceived ideological lines 10 times, with Justice Kennedy siding with the conservative justices (Roberts, Scalia, Thomas, and Alito) five times, and with the liberal justices (Ginsburg, Breyer, Sotomayor, and Kagan) five times.
In the October 2012 term, the Court decided 78 cases. Five of them were decided in unsigned opinions. 38 out of the 78 decisions (representing 49% of the decisions) were unanimous in judgement, with 24 decisions being completely unanimous (a single opinion with every justice that participated joined). This was the largest percentage of unanimous decisions that the Court had in ten years, since the October 2002 term (when 51% of the decisions handed down were unanimous). The Court split 5-4 in 23 cases (29% of the total); of these, 16 broke down along the traditionally perceived ideological lines, with Chief Justice Roberts and Justices Scalia, Thomas, and Alito on one side, Justices Ginsburg, Breyer, Sotomayor and Kagan on the other, and Justice Kennedy holding the balance. Of these 16 cases, Justice Kennedy sided with the conservatives on 10 cases, and with the liberals on 6. Three cases were decided by an interesting alignment of justices, with Chief Justice Roberts joined by Justices Kennedy, Thomas, Breyer and Alito in the majority, with Justices Scalia, Ginsburg, Sotomayor, and Kagan in the minority. The greatest agreement between justices was between Ginsburg and Kagan, who agreed on 72 of the 75 cases in which both voted; the lowest agreement between justices was between Ginsburg and Alito, who agreed only on 45 out of 77 cases in which they both participated.
Justice Kennedy was in the majority of 5-4 decisions on 20 out of the 24 cases, and in 71 of the 78 cases of the term, in line with his position as the "swing-vote" of the Court.
Facilities.
The Supreme Court first met on February 1, 1790, at the Merchants' Exchange Building in New York City. When Philadelphia became the capital, the Court met briefly in Independence Hall before settling in Old City Hall from 1791 until 1800. After the government moved to Washington, D.C., the Court occupied various spaces in the United States Capitol building until 1935, when it moved into its own purpose-built home. The four-story building was designed by Cass Gilbert in a classical style sympathetic to the surrounding buildings of the Capitol and Library of Congress, and is clad in marble. The building includes the courtroom, justices' chambers, an extensive law library, various meeting spaces, and auxiliary services including a gymnasium. The Supreme Court building is within the ambit of the Architect of the Capitol, but maintains its own police force separate from the Capitol Police.
Located across the street from the United States Capitol at One First Street NE and Maryland Avenue, the building is open to the public from 9 am to 4:30 pm weekdays but closed on weekends and holidays. Visitors may not tour the actual courtroom unaccompanied. There is a cafeteria, a gift shop, exhibits, and a half-hour informational film. When the Court is not in session, lectures about the courtroom are held hourly from 9:30 am to 3:30 pm and reservations are not necessary. When the Court is in session the public may attend oral arguments, which are held twice each morning (and sometimes afternoons) on Mondays, Tuesdays, and Wednesdays in two-week intervals from October through late April, with breaks during December and February. Visitors are seated on a first-come first-served basis. One estimate is there are about 250 seats available. The number of open seats varies from case to case; for important cases, some visitors arrive the day before and wait through the night. From mid-May until the end of June, the court releases orders and opinions beginning at 10 am, and these 15 to 30-minute sessions are open to the public on a similar basis. Supreme Court Police are available to answer questions.
Jurisdiction.
Section 2 of Article Three of the United States Constitution outlines the jurisdiction of the federal courts of the United States:
The judicial Power shall extend to all Cases, in Law and Equity, arising under this Constitution, the Laws of the United States, and Treaties made, or which shall be made, under their Authority; to all Cases affecting Ambassadors, other public Ministers and Consuls; to all Cases of admiralty and maritime Jurisdiction; to Controversies to which the United States shall be a Party; to Controversies between two or more States; between a State and Citizens of another State; between Citizens of different States; between Citizens of the same State claiming Lands under Grants of different States, and between a State, or the Citizens thereof, and foreign States, Citizens or Subjects.
The jurisdiction of the federal courts was further limited by the Eleventh Amendment to the United States Constitution, which forbade federal courts from hearing cases "commenced or prosecuted against [a State] by Citizens of another State, or by Citizens or Subjects of any Foreign State." However, states may waive this immunity, and Congress may abrogate the states' immunity in certain circumstances (see Sovereign immunity). In addition to constitutional constraints, Congress is authorized by Article III to regulate the court's appellate jurisdiction. The federal courts may hear cases only if one or more of the following conditions are met:
Exercise of this power can become controversial (see jurisdiction stripping). For example, #redirect , as amended by the Detainee Treatment Act, provides that "No court, justice, or judge shall have jurisdiction to hear or consider an application for a writ of habeas corpus filed by or on behalf of an alien detained by the United States who has been determined by the United States to have been properly detained as an enemy combatant or is awaiting such determination."
The Constitution specifies that the Supreme Court may exercise original jurisdiction in cases affecting ambassadors and other diplomats, and in cases in which a state is a party. In all other cases, however, the Court has only appellate jurisdiction. It considers cases based on its original jurisdiction very rarely; almost all cases are brought to the Supreme Court on appeal. In practice, the only original jurisdiction cases heard by the Court are disputes between two or more states.
The power of the Supreme Court to consider appeals from state courts, rather than just federal courts, was created by the Judiciary Act of 1789 and upheld early in the Court's history, by its rulings in "Martin v. Hunter's Lessee" (1816) and "Cohens v. Virginia" (1821). The Supreme Court is the only federal court that has jurisdiction over direct appeals from state court decisions, although there are several devices that permit so-called "collateral review" of state cases.
Since Article Three of the United States Constitution stipulates that federal courts may only entertain "cases" or "controversies", the Supreme Court avoids deciding cases that are moot and does not render advisory opinions, as the supreme courts of some states may do. For example, in "DeFunis v. Odegaard", 416 U.S. (1974), the Court dismissed a lawsuit challenging the constitutionality of a law school affirmative action policy because the plaintiff student had graduated since he began the lawsuit, and a decision from the Court on his claim would not be able to redress any injury he had suffered. The mootness exception is not absolute. If an issue is "capable of repetition yet evading review", the Court will address it even though the party before the Court would not himself be made whole by a favorable result. In "Roe v. Wade", 410 U.S. (1973), and other abortion cases, the Court addresses the merits of claims pressed by pregnant women seeking abortions even if they are no longer pregnant because it takes longer than the typical human gestation period to appeal a case through the lower courts to the Supreme Court.
Justices as Circuit Justices.
The United States is divided into thirteen circuit courts of appeals, each of which is assigned a "Circuit Justice" from the Supreme Court. Although this concept has been in continuous existence throughout the history of the republic, its meaning has changed through time.
Under the Judiciary Act of 1789, each Justice was required to "ride circuit", or to travel within the assigned circuit and consider cases alongside local judges. This practice encountered opposition from many Justices, who cited the difficulty of travel. Moreover, several individuals opposed it because a Justice could not be expected to be impartial in an appeal if he had previously decided the same case while riding circuit. Circuit riding was abolished in 1891.
Today, the Circuit Justice for each circuit is responsible for dealing with certain types of applications that, under the Court's rules, may be addressed by a single Justice. These include applications for emergency stays (including stays of execution in death-penalty cases) and injunctions pursuant to the All Writs Act arising from cases within that circuit, as well as routine requests such as requests for extensions of time. In the past, Circuit Justices also sometimes ruled on motions for bail in criminal cases, writs of habeas corpus, and applications for writs of error granting permission to appeal. Ordinarily, a Justice will resolve such an application by simply endorsing it "Granted" or "Denied" or entering a standard form of order. However, the Justice may elect to write an opinion—referred to as an in-chambers opinion—in such matters if he or she wishes.
A Circuit Justice may sit as a judge on the Court of Appeals of that circuit, but over the past hundred years, this has rarely occurred. A Circuit Justice sitting with the Court of Appeals has seniority over the Chief Judge of the circuit.
The Chief Justice has traditionally been assigned to the District of Columbia Circuit, the Fourth Circuit (which includes Maryland and Virginia, the states surrounding the District of Columbia), and since it was established, the Federal Circuit. Each Associate Justice is assigned to one or two judicial circuits.
As of September 28, 2010, the allotment of the justices among the circuits is:
Four of the current Justices are assigned to circuits on which they previously sat as circuit judges: Chief Justice Roberts (D.C. Circuit), Justice Breyer (First Circuit), Justice Alito (Third Circuit), and Justice Kennedy (Ninth Circuit).
Process.
A term of the Supreme Court commences on the first Monday of each October, and continues until June or early July of the following year. Each term consists of alternating periods of approximately two weeks known as "sittings" and "recesses." Justices hear cases and deliver rulings during sittings; they discuss cases and write opinions during recesses.
Case selection.
Nearly all cases come before the court by way of petitions for writs of certiorari, commonly referred to as "cert". The Court may review any case in the federal courts of appeals "by writ of "certiorari" granted upon the petition of any party to any civil or criminal case". The Court may only review "final judgments rendered by the highest court of a state in which a decision could be had" if those judgments involve a question of federal statutory or constitutional law. The party that appealed to the Court is the "petitioner" and the non-mover is the "respondent". All case names before the Court are styled "petitioner" v. "respondent", regardless of which party initiated the lawsuit in the trial court. For example, criminal prosecutions are brought in the name of the state and against an individual, as in "State of Arizona v. Ernesto Miranda". If the defendant is convicted, and his conviction then is affirmed on appeal in the state supreme court, when he petitions for cert the name of the case becomes "Miranda v. Arizona".
There are situations where the Court has original jurisdiction, such as when two states have a dispute against each other, or when there is a dispute between the United States and a state. In such instances, a case is filed with the Supreme Court directly. Examples of such cases include "United States v. Texas", a case to determine whether a parcel of land belonged to the United States or to Texas, and "Virginia v. Tennessee", a case turning on whether an incorrectly drawn boundary between two states can be changed by a state court, and whether the setting of the correct boundary requires Congressional approval. Although it has not happened since 1794 in the case of "Georgia v. Brailsford", parties in an action at law in which the Supreme Court has original jurisdiction may request that a jury determine issues of fact. Two other original jurisdiction cases involve colonial era borders and rights under navigable waters in "New Jersey v. Delaware", and water rights between riparian states upstream of navigable waters in "Kansas v. Colorado".
A cert petition is voted on at a session of the court called a "conference". A conference is a private meeting of the nine Justices by themselves; the public and the Justices' clerks are excluded. If four Justices vote to grant the petition, the case proceeds to the briefing stage; otherwise, the case ends. Except in death penalty cases and other cases in which the Court orders briefing from the respondent, the respondent may, but is not required to, file a response to the cert petition.
The court grants a petition for cert only for "compelling reasons", spelled out in the court's Rule 10. Such reasons include:
When a conflict of interpretations arises from differing interpretations of the same law or constitutional provision issued by different federal circuit courts of appeals, lawyers call this situation a "circuit split". If the court votes to deny a cert petition, as it does in the vast majority of such petitions that come before it, it does so typically without comment. A denial of a cert petition is not a judgment on the merits of a case, and the decision of the lower court stands as the final ruling in the case.
To manage the high volume of cert petitions received by the Court each year (of the more than 7,000 petitions the Court receives each year, it will usually request briefing and hear oral argument in 100 or fewer), the Court employs an internal case management tool known as the "cert pool." Currently, all justices except for Justice Alito participate in the cert pool.
Oral argument.
When the Court grants a cert petition, the case is set for oral argument. Both parties will file briefs on the merits of the case, as distinct from the reasons they may have argued for granting or denying the cert petition. With the consent of the parties or approval of the Court, "amici curiae", or "friends of the court", may also file briefs. The Court holds two-week oral argument sessions each month from October through April. Each side has thirty minutes to present its argument (the Court may choose to give more time, though this is rare), and during that time, the Justices may interrupt the advocate and ask questions. The petitioner gives the first presentation, and may reserve some time to rebut the respondent's arguments after the respondent has concluded. "Amici curiae" may also present oral argument on behalf of one party if that party agrees. The Court advises counsel to assume that the Justices are familiar with and have read the briefs filed in a case.
The Supreme Court bar.
In order to plead before the court, an attorney must first be admitted to the court's bar. Approximately 4,000 lawyers join the bar each year. The bar contains an estimated 230,000 members. In reality, pleading is limited to several hundred attorneys. The rest join for a one-time fee of $200, earning the court about $750,000 annually. Attorneys can be admitted as either individuals or as groups. The group admission is held before the current justices of the Supreme Court, wherein the Chief Justice approves a motion to admit the new attorneys. The lawyers mostly apply for the trophy of a certificate for their office, an addition for their resume, and access to better seating if they wish to attend an oral argument. Members of the Supreme Court Bar are also granted access to the collections of the Supreme Court Library.
Decision.
At the conclusion of oral argument, the case is submitted for decision. Cases are decided by majority vote of the Justices. It is the Court's practice to issue decisions in all cases argued in a particular Term by the end of that Term. Within that Term, however, the Court is under no obligation to release a decision within any set time after oral argument. At the conclusion of oral argument, the Justices retire to another conference at which the preliminary votes are tallied, and the most senior Justice in the majority assigns the initial draft of the Court's opinion to a Justice on his or her side. Drafts of the Court's opinion, as well as any concurring or dissenting opinions, circulate among the Justices until the Court is prepared to announce the judgment in a particular case.
It is possible that, through recusals or vacancies, the Court divides evenly on a case. If that occurs, then the decision of the court below is affirmed, but does not establish binding precedent. In effect, it results in a return to the "status quo ante". For a case to be heard, there must be a quorum of at least six justices. If a quorum is not available to hear a case and a majority of qualified justices believes that the case cannot be heard and determined in the next term, then the judgment of the court below is affirmed as if the Court had been evenly divided. For cases brought directly to the Supreme Court by direct appeal from a United States District Court, the Chief Justice may order the case remanded to the appropriate U.S. Court of Appeals for a final decision there. This has only occurred once in U.S. history, in the case of "United States v. Alcoa".
Published opinions.
The Court's opinions are published in three stages. First, a slip opinion is made available on the Court's web site and through other outlets. Next, several opinions are bound together in paperback form, called a preliminary print of "United States Reports", the official series of books in which the final version of the Court's opinions appears. About a year after the preliminary prints are issued, a final bound volume of "U.S. Reports" is issued. The individual volumes of "U.S. Reports" are numbered so that users may cite this set of reports—or a competing version published by another commercial legal publisher—to allow those who read their pleadings and other briefs to find the cases quickly and easily.
As of the beginning of October Term 2014, there are:
s of 2012[ [update]], the "U.S. Reports" have published a total of 30,161 Supreme Court opinions, covering the decisions handed down from February 1790 to March 2012. This figure does not reflect the number of cases the Court has taken up, as several cases can be addressed by a single opinion (see, for example, "Parents v. Seattle", where "Meredith v. Jefferson County Board of Education" was also decided in the same opinion; by a similar logic, "Miranda v. Arizona" actually decided not only "Miranda" but also three other cases: "Vignera v. New York", "Westover v. United States," and "California v. Stewart"). A more unusual example is The Telephone Cases, which comprise a single set of interlinked opinions that take up the entire 126th volume of the "U.S. Reports".
Opinions are also collected and published in two unofficial, parallel reporters: "Supreme Court Reporter", published by West (now a part of Thomson Reuters), and "United States Supreme Court Reports, Lawyers' Edition" (simply known as "Lawyers' Edition"), published by LexisNexis. In court documents, legal periodicals, and other legal media, case citations generally contain the cites from each of the three reporters; for example, the citation to "Citizens United v. Federal Election Commission" is presented as "Citizens United v. Federal Election Com'n", 585 U.S. 50, 130 S. Ct. 876, 175 L. Ed. 2d 753 (2010), with "S. Ct." representing the "Supreme Court Reporter", and "L. Ed." representing the "Lawyers' Edition".
Citations to published opinions.
Lawyers use an abbreviated format to cite cases, in the form "vol U.S. page, pin (year)", where vol is the volume number, page is the page number on which the opinion begins, and year is the year in which the case was decided. Optionally, pin is used to "pinpoint" to a specific page number within the opinion. For instance, the citation for "Roe v. Wade" is 410 U.S. 113 (1973) and it means the case was decided in 1973 and appears on page 113 of volume 410 of "U.S. Reports". For hot-from-the-press judgments, the volume and page numbers are replaced with "___".
Institutional powers and constraints.
The Federal court system and the judicial authority to interpret the Constitution received little attention in the debates over the drafting and ratification of the Constitution. The power of judicial review, in fact, is nowhere mentioned in it, Over the ensuing years, the question of whether the power of judicial review was even intended by the drafters of the Constitution was quickly frustrated by the lack of evidence bearing on the question either way. Nevertheless, the power of judiciary to overturn laws and executive actions it determines are unlawful or unconstitutional is a well-established precedent. Many of the Founding Fathers accepted the notion of judicial review; in Federalist No. 78, Alexander Hamilton wrote: "A Constitution is, in fact, and must be regarded by the judges, as a fundamental law. It therefore belongs to them to ascertain its meaning, as well as the meaning of any particular act proceeding from the legislative body. If there should happen to be an irreconcilable variance between the two, that which has the superior obligation and validity ought, of course, to be preferred; or, in other words, the Constitution ought to be preferred to the statute."
The Supreme Court firmly established its power to declare laws unconstitutional in "Marbury v. Madison" (1803), consummating the American system of checks and balances. In explaining the power of judicial review, Chief Justice John Marshall stated that the authority to interpret the law was the particular province of the courts, part of the "duty of the judicial department to say what the law is." His contention was not that the Court had privileged insight into constitutional requirements, but that it was the constitutional duty of the judiciary, as well as the other branches of government, to read and obey the dictates of the Constitution.
Since the founding of the republic, there has been a tension between the practice of judicial review and the democratic ideals of egalitarianism, self-government, self-determination and freedom of conscience. At one pole are those who view the Federal Judiciary and especially the Supreme Court as being "the most separated and least checked of all branches of government." Indeed federal judges and justices on the Supreme Court are not required to stand for election by virtue of their tenure "during good behavior", and their pay may "not be diminished" while they hold their position (). Though subject to the process of impeachment, only one Justice has ever been impeached and no Supreme Court Justice has been removed from office. At the other pole are those who view the judiciary as the least dangerous branch, with little ability to resist the exhortations of the other branches of government. The Supreme Court, it is noted, cannot directly enforce its rulings; instead, it relies on respect for the Constitution and for the law for adherence to its judgments. One notable instance of nonacquiescence came in 1832, when the state of Georgia ignored the Supreme Court's decision in "Worcester v. Georgia". President Andrew Jackson, who sided with the Georgia courts, is supposed to have remarked, "John Marshall has made his decision; now let him enforce it!"; however, this alleged quotation has been disputed. Some state governments in the South also resisted the desegregation of public schools after the 1954 judgment "Brown v. Board of Education". More recently, many feared that President Nixon would refuse to comply with the Court's order in "United States v. Nixon" (1974) to surrender the Watergate tapes. Nixon, however, ultimately complied with the Supreme Court's ruling.
Supreme Court decisions can be (and have been) purposefully overturned by constitutional amendment, which has happened on five occasions:
When the Court rules on matters involving the interpretation of laws rather than of the Constitution, simple legislative action can reverse the decisions (for example, in 2009 Congress passed the Lilly Ledbetter act, superseding the limitations given in "Ledbetter v. Goodyear Tire & Rubber Co." in 2007). Also, the Supreme Court is not immune from political and institutional restraints: lower federal courts and state courts sometimes resist doctrinal innovations, as do law enforcement officials.
In addition, the other two branches can restrain the Court through other mechanisms. Congress can increase the number of justices, giving the President power to influence future decisions by appointments (as in Roosevelt's Court Packing Plan discussed above). Congress can pass legislation that restricts the jurisdiction of the Supreme Court and other federal courts over certain topics and cases: this is suggested by language in of Article Three, where the appellate jurisdiction is granted "with such Exceptions, and under such Regulations as the Congress shall make." The Court sanctioned such congressional action in the Reconstruction case "ex parte McCardle" (1869), though it rejected Congress' power to dictate how particular cases must be decided in "United States v. Klein" (1871).
On the other hand, through its power of judicial review, the Supreme Court has defined the scope and nature of the powers and separation between the legislative and executive branches of the federal government; for example, in "United States v. Curtiss-Wright Export Corp." (1936), "Dames & Moore v. Regan" (1981), and notably in "Goldwater v. Carter" (1979), (where it effectively gave the Presidency the power to terminate ratified treaties without the consent of Congress or the Senate). The Court's decisions can also impose limitations on the scope of Executive authority, as in "Humphrey's Executor v. United States" (1935), the "Steel Seizure Case" (1952), and "United States v. Nixon" (1974).
Law clerks.
Each Supreme Court justice hires several law clerks to review petitions for writ of certiorari, research them, prepare bench memorandums, and draft opinions. Associate justices are allowed four clerks. The chief justice is allowed five clerks, but Chief Justice Rehnquist hired only three per year, and Chief Justice Roberts usually hires only four. Generally, law clerks serve a term of one to two years.
The first law clerk was hired by Associate Justice Horace Gray in 1882. Oliver Wendell Holmes, Jr. and Louis Brandeis were the first Supreme Court justices to use recent law school graduates as clerks, rather than hiring a "stenographer-secretary". Most law clerks are recent law school graduates.
The first female clerk was Lucile Lomen, hired in 1944 by Justice William O. Douglas. The first African-American, William T. Coleman, Jr., was hired in 1948 by Justice Felix Frankfurter. A disproportionately large number of law clerks have obtained law degrees from elite law schools, especially Harvard, Yale, the University of Chicago, Columbia, and Stanford. From 1882 to 1940, 62% of law clerks were graduates of Harvard Law School. Those chosen to be Supreme Court law clerks usually have graduated in the top of their law school class and were often an editor of the law review or a member of the moot court board. In recent times, clerking previously for a judge in a federal circuit court has been a prerequisite to clerking for a Supreme Court justice.
Six Supreme Court justices previously clerked for other justices: Byron White clerked for Frederick M. Vinson, John Paul Stevens clerked for Wiley Rutledge, Stephen Breyer clerked for Arthur Goldberg, William H. Rehnquist clerked for Robert H. Jackson, John G. Roberts, Jr. clerked for William H. Rehnquist, and Elena Kagan clerked for Thurgood Marshall. Many of the justices have also clerked in the federal Courts of Appeals. Justice Samuel Alito clerked for Judge Leonard I. Garth of the United States Court of Appeals for the Third Circuit and Elena Kagan clerked for Judge Abner J. Mikva of the United States Court of Appeals for the District of Columbia Circuit.
Politicization of the Court.
Clerks hired by each of the justices of the Supreme Court are often given considerable leeway in the opinions they draft. "Supreme Court clerkship appeared to be a nonpartisan institution from the 1940s into the 1980s", according to a study published in 2009 by the law review of Vanderbilt University Law School. "As law has moved closer to mere politics, political affiliations have naturally and predictably become proxies for the different political agendas that have been pressed in and through the courts", former federal court of appeals judge J. Michael Luttig said. David J. Garrow, professor of history at the University of Cambridge, stated that the Court had thus begun to mirror the political branches of government. "We are getting a composition of the clerk workforce that is getting to be like the House of Representatives", Professor Garrow said. "Each side is putting forward only ideological purists."
According to the "Vanderbilt Law Review" study, this politicized hiring trend reinforces the impression that the Supreme Court is "a superlegislature responding to ideological arguments rather than a legal institution responding to concerns grounded in the rule of law."
A poll conducted in June 2012 by The New York Times and CBS News showed that just 44 percent of Americans approve of the job the Supreme Court is doing. Three-quarters said the justices' decisions are sometimes influenced by their political or personal views.
Criticism.
Some criticisms leveled at the Supreme Court are:
References.
Bibliography.
</dl>
Further reading.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Supreme Court of the United States" article dated 2006-08-05, and does not reflect subsequent edits to the article. ()
More spoken articles
class="navbox nowraplinks collapsible autocollapse" style="width:100%"
! colspan="2" class="navbox-title"

</doc>
<doc id="31739" url="http://en.wikipedia.org/wiki?curid=31739" title="Chief Justice of the United States">
Chief Justice of the United States

The Chief Justice of the United States is the head of the United States federal court system (the judicial branch of the federal government of the United States) and the chief judge of the Supreme Court of the United States. The Chief Justice is one of nine Supreme Court justices; the other eight are the Associate Justices of the Supreme Court of the United States. From 1789 until 1866, the office was known as the Chief Justice of the Supreme Court.
The Chief Justice is the highest judicial officer in the country, and acts as a chief administrative officer for the federal courts and as head of the Judicial Conference of the United States appoints the director of the Administrative Office of the United States Courts. The Chief Justice also serves as a spokesperson for the judicial branch.
The Chief Justice leads the business of the Supreme Court and presides over oral arguments before the court. When the court renders an opinion, the Chief Justice—when in the majority—decides who writes the court's opinion. The Chief Justice also has significant agenda-setting power over the court's meetings. In the case of an impeachment of a President of the United States, which has occurred twice, the Chief Justice presides over the trial in the Senate. In modern tradition, the Chief Justice has the ceremonial duty of administering the oath of office of the President of the United States.
The first Chief Justice was John Jay. The 17th and current Chief Justice is John G. Roberts, Jr.
Origin, title, and appointment to the post.
The United States Constitution does not explicitly establish the office of Chief Justice, but presupposes its existence with a single reference in : "When the President of the United States is tried, the Chief Justice shall preside." Nothing more is said in the Constitution regarding the office, including any distinction between the Chief Justice and Associate Justices of the Supreme Court, who are not mentioned in the Constitution. 
The office was originally known as "Chief Justice of the Supreme Court" and is still informally referred to using that title. However, #redirect specifies that the title is "Chief Justice of the United States". The title was changed from Chief Justice of the Supreme Court by Congress in 1866 at the suggestion of the sixth Chief Justice, Salmon P. Chase. Chase wished to emphasize the Supreme Court's role as a co-equal branch of government. The first Chief Justice commissioned using the new title was Melville Fuller in 1888. Use of the previous title when referring to Chief Justices John Jay through Roger B. Taney is technically correct, as that was the legal title during their time on the court, but the newer title is frequently used retroactively for all Chief Justices.
The other eight members of the court are officially Associate Justices of the Supreme Court of the United States, not "Associate Justices of the United States." The Chief Justice is the only member of the court to whom the Constitution refers as a "Justice," and only in Article I. Article III of the Constitution refers to all members of the Supreme Court (and of other federal courts) simply as "Judges."
The Chief Justice is nominated by the President of the United States and confirmed to sit on the Court by the United States Senate. The U.S. Constitution states that all justices of the court "shall hold their offices during good behavior," meaning that the appointments end only when a justice dies in office, resigns, or is impeached by the United States House of Representatives and convicted at trial by the Senate. The salary of the Chief Justice is set by Congress; the Constitution prohibits Congress from lowering the salary of any judge, including the Chief Justice, while that judge holds his or her office. s of 2015[ [update]], the salary is $258,100 per year, which is slightly higher than that of the Associate Justices, which is $246,800. 
While the Chief Justice is appointed by the President, there is no specific constitutional prohibition against using another method to select the Chief Justice from among those Justices properly appointed and confirmed to the Supreme Court, and at least one scholar has proposed that presidential appointment should be done away with, and replaced by a process that permits the Justices to select their own Chief Justice.
Three serving Associate Justices have received promotions to Chief Justice: Edward Douglass White in 1910, Harlan Fiske Stone in 1941, and William Rehnquist in 1986. Associate Justice Abe Fortas was nominated to the position of Chief Justice of the United States, but his nomination was filibustered by Senate Republicans in 1968. Despite the failed nomination, Fortas remained an Associate Justice until his resignation the following year. Most Chief Justices, including John Roberts, have been nominated to the highest position on the Court without any previous experience on the Supreme Court; indeed some, such as Earl Warren, received confirmation despite having no prior judicial experience.
There have been 21 individuals nominated for Chief Justice, of whom 17 have been confirmed by the Senate, although a different 17 have served. The second Chief Justice, John Rutledge, served in 1795 on a recess appointment, but did not receive Senate confirmation. Associate Justice William Cushing received nomination and confirmation as Chief Justice in January 1796, but declined the office; President Washington then nominated, and the Senate confirmed, Oliver Ellsworth, who served instead. The Senate subsequently confirmed President Adams's nomination of John Jay to replace Ellsworth, but Jay declined to resume his former office, citing the burden of riding circuit and its impact on his health, and his perception of the Court's lack of prestige. Adams then nominated John Marshall, whom the Senate confirmed shortly afterward.
When the Chief Justice dies in office or is otherwise unwilling or unable to serve, the duties of the Chief Justice temporarily are performed by the most senior sitting associate justice, who acts as Chief Justice until a new Chief Justice is confirmed. Currently, Antonin Scalia is the most senior associate justice.
Duties.
Along with the duties of the associate justices, the Chief Justice has several unique duties.
Impeachment trials.
Article I, section 3 of the U.S. Constitution stipulates that the Chief Justice shall preside over impeachment trials of the President of the United States in the U.S. Senate. Two Chief Justices, Salmon P. Chase and William Rehnquist, have presided over the trial in the Senate that follows an impeachment of the president – Chase in 1868 over the proceedings against President Andrew Johnson and Rehnquist in 1999 over the proceedings against President Bill Clinton. Both presidents were subsequently acquitted.
Seniority.
The Chief Justice is considered to be the justice with most seniority, independent of the number of years of service in the Supreme Court. As a result, the Chief Justice chairs the conferences where cases are discussed and voted on by the justices. The Chief Justice normally speaks first, and so has influence in framing the discussion.
The Chief Justice sets the agenda for the weekly meetings where the justices review the petitions for certiorari, to decide whether to hear or deny each case. The Supreme Court agrees to hear less than one percent of the cases petitioned to it. While associate justices may append items to the weekly agenda, in practice this initial agenda-setting power of the Chief Justice has significant influence over the direction of the court.
Despite the seniority and added prestige, the Chief Justice's vote carries the same legal weight as each of the other eight justices. In any decision, he has no legal authority to overrule the verdicts or interpretations of the other eight judges or tamper with them. However, in any vote, the most senior justice in the majority decides who will write the Opinion of the Court. This power to determine the opinion author (including the option to select oneself) allows a Chief Justice in the majority to influence the historical record. Two justices in the same majority, given the opportunity, might write very different majority opinions (as evidenced by many concurring opinions); being assigned the opinion may also cement the vote of an associate who is viewed as only marginally in the majority (a tactic that was reportedly used to some effect by Earl Warren). A Chief Justice who knows the associate justices can therefore do much—by the simple act of selecting the justice who writes the opinion of the court—to affect the "flavor" of the opinion, which in turn can affect the interpretation of that opinion in cases before lower courts in the years to come. It is said that some Chief Justices, notably Earl Warren and Warren E. Burger, sometimes switched votes to a majority they disagreed with to be able to use this prerogative of the Chief Justice to dictate who would write the opinion.
Oath of office.
The Chief Justice typically administers the oath of office at the inauguration of the President of the United States. This is a traditional rather than constitutional responsibility of the Chief Justice; the Constitution does not require that the oath be administered by anyone in particular, simply that it be taken by the president. Law empowers any federal and state judge, as well as notaries public, to administer oaths and affirmations.
If the Chief Justice is ill or incapacitated, the oath is usually administered by the next senior member of the Supreme Court. Seven times, someone other than the Chief Justice of the United States administered the oath of office to the President. Robert Livingston, as Chancellor of the State of New York (the state's highest ranking judicial office), administered the oath of office to George Washington at his first inauguration; there was no Chief Justice of the United States, nor any other federal judge prior to their appointments by President Washington in the months following his inauguration. William Cushing, an associate justice of the Supreme Court, administered Washington's second oath of office in 1793. Calvin Coolidge's father, a notary public, administered the oath to his son after the death of Warren Harding. This, however, was contested upon Coolidge's return to Washington and his oath was re-administered by Judge Adolph A. Hoehling, Jr. of the District of Columbia Supreme Court. John Tyler and Millard Fillmore were both sworn in on the death of their predecessors by Chief Justice William Cranch of the Circuit Court of the District of Columbia. Chester A. Arthur and Theodore Roosevelt's initial oaths reflected the unexpected nature of their taking office. On November 22, 1963, after the assassination of President John F. Kennedy, Judge Sarah T. Hughes, a federal district court judge of the United States District Court for the Northern District of Texas, administered the oath of office to then Vice President Lyndon B. Johnson aboard the presidential airplane. 
In addition, the Chief Justice ordinarily administers the oath of office to newly appointed and confirmed associate justices, whereas the senior associate justice will normally swear in a new Chief Justice or vice president.
Other duties.
The Chief Justice also:
Unlike Senators and Representatives who are constitutionally prohibited from holding any other "office of trust or profit" of the United States or of any state while holding their congressional seats, the Chief Justice and the other members of the federal judiciary are not barred from serving in other positions. Chief Justice John Jay served as a diplomat to negotiate the so-called Jay Treaty (also known as the Treaty of London of 1794), and Chief Justice Earl Warren chaired The President's Commission on the Assassination of President Kennedy. As described above, the Chief Justice holds office in the Smithsonian Institution and the Library of Congress.
Disability or vacancy.
Under 28 USC , when the Chief Justice is unable to discharge his functions, or that office is vacant, his duties are carried out by the most senior associate justice who is able to act, until the disability or vacancy ends.
List of Chief Justices.
Data based on:
Further reading.
</dl>

</doc>
<doc id="31740" url="http://en.wikipedia.org/wiki?curid=31740" title="University of Michigan">
University of Michigan

The University of Michigan (UM, U-M, UMich, or U of M), frequently referred to simply as Michigan, is a public research university located in Ann Arbor, Michigan, United States. Originally, founded in 1817 in Detroit as the "Catholepistemiad", or University of Michigania, 20 years before the Michigan Territory officially became a state, the University of Michigan is the state's oldest university. The university moved to Ann Arbor in 1837 onto 40 acre of what is now known as Central Campus. Since its establishment in Ann Arbor, the university campus has expanded to include more than 584 major buildings with a combined area of more than 34 million gross square feet (781 acres or 3.16 km²), and has two satellite campuses located in Flint and Dearborn. The University was one of the founding members of the Association of American Universities.
Considered one of the foremost research universities in the United States, the university has very high research activity and its comprehensive graduate program offers doctoral degrees in the humanities, social sciences, and STEM fields (Science, Technology, Engineering and Mathematics) as well as professional degrees in medicine, law, pharmacy, nursing, social work and dentistry. Michigan's body of living alumni (as of 2012) comprises more than 500,000. Besides academic life, Michigan's athletic teams compete in Division I of the NCAA and are collectively known as the Wolverines. They are members of the Big Ten Conference.
History.
The University of Michigan was established in Detroit in 1817 as the "Catholepistemiad", or University of Michigania, by the governor and judges of Michigan Territory. The Rev. John Monteith was one of the university's founders and its first President. Ann Arbor had set aside 40 acre that it hoped would become the site for a new state capitol, but it offered this land to the university when Lansing was chosen as the state capital. What would become the university moved to Ann Arbor in 1837 thanks to governor Stevens T. Mason. The original 40 acre became part of the current Central Campus. The first classes in Ann Arbor were held in 1841, with six freshmen and a sophomore, taught by two professors. Eleven students graduated in the first commencement in 1845. By 1866, enrollment increased to 1,205 students, many of whom were Civil War veterans. Women were first admitted in 1870. James Burrill Angell, who served as the university's president from 1871 to 1909, aggressively expanded U-M's curriculum to include professional studies in dentistry, architecture, engineering, government, and medicine. U-M also became the first American university to use the seminar method of study.
From 1900 to 1920 the university constructed many new facilities, including buildings for the dental and pharmacy programs, chemistry, natural sciences, Hill Auditorium, large hospital and library complexes, and two residence halls. In 1920 the university reorganized the College of Engineering and formed an advisory committee of 100 industrialists to guide academic research initiatives. The university became a favored choice for bright Jewish students from New York in the 1920s and 1930s when the Ivy League schools had quotas restricting the number of Jews to be admitted. As a result, U-M gained the nickname "Harvard of the West," which became commonly parodied in reverse after John F. Kennedy referred to himself as "a graduate of the Michigan of the East, Harvard University" in his speech proposing the formation of the Peace Corps while on the front steps of the Michigan Union. During World War II, U-M's research grew to include U.S. Navy projects such as proximity fuzes, PT boats, and radar jamming.
By 1950, enrollment had reached 21,000, of which more than one third (or 7,700) were veterans supported by the G.I. Bill. As the Cold War and the Space Race took hold, U-M became a major recipient of government grants for strategic research and helped to develop peacetime uses for nuclear energy. Much of that work, as well as research into alternative energy sources, is pursued via the Memorial Phoenix Project.
Lyndon B. Johnson's speech outlining his Great Society program was given during U-M's 1964 spring commencement ceremony. During the 1960s, there were numerous protests against the Vietnam War and related to other issues at U-M. On March 24, 1965, a group of U-M faculty members and 3,000 students held the nation's first ever faculty-led "teach-in" to protest against American policy in Southeast Asia. In response to a series of sit-ins in 1966 by "Voice", the campus political party of Students for a Democratic Society, U-M's administration banned sit-ins. In response, 1,500 students participated in a one-hour sit-in inside the LSA Building, which housed administrative offices.
Former U-M student and noted architect Alden B. Dow designed the current Fleming Administration Building, which was completed in 1968. The building's plans were drawn in the early 1960s, before student activism prompted a concern for safety, but the Fleming Building's narrow windows, all located above the first floor, and fortress-like exterior led to a campus rumor that it was designed to be riot-proof. Dow denied those rumors, claiming the small windows were designed to be energy efficient.
During the 1970s, severe budget constraints challenged the university's physical development; but, in the 1980s, the university received increased grants for research in the social and physical sciences. The university's involvement in the anti-missile Strategic Defense Initiative and investments in South Africa caused controversy on campus. During the 1980s and 1990s, the university devoted substantial resources to renovating its massive hospital complex and improving the academic facilities on the North Campus. In its 2011 annual financial report, the university announced that it had dedicated $497 million per year in each of the prior 10 years to renovate buildings and infrastructure around the campus. The university also emphasized the development of computer and information technology throughout the campus.
In the early 2000s, U-M also faced declining state funding due to state budget shortfalls. At the same time, the university attempted to maintain its high academic standing while keeping tuition costs affordable. There were disputes between U-M's administration and labor unions, notably with the Lecturers' Employees Organization (LEO) and the Graduate Employees Organization (GEO), the union representing graduate student employees. These conflicts led to a series of one-day walkouts by the unions and their supporters. The university is currently engaged in a $2.5 billion construction campaign.
In 2003, two lawsuits involving U-M's affirmative action admissions policy reached the U.S. Supreme Court ("Grutter v. Bollinger" and "Gratz v. Bollinger"). President George W. Bush publicly opposed the policy before the court issued a ruling. The court found that race may be considered as a factor in university admissions in all public universities and private universities that accept federal funding. But, it ruled that a point system was unconstitutional. In the first case, the court upheld the Law School admissions policy, while in the second it ruled against the university's undergraduate admissions policy.
The debate continues because in November 2006, Michigan voters passed Proposal 2, banning most affirmative action in university admissions. Under that law, race, gender, and national origin can no longer be considered in admissions. U-M and other organizations were granted a stay from implementation of the passed proposal soon after that election, and this has allowed time for proponents of affirmative action to decide legal and constitutional options in response to the election results. The university has stated it plans to continue to challenge the ruling; in the meantime, the admissions office states that it will attempt to achieve a diverse student body by looking at other factors, such as whether the student attended a disadvantaged school, and the level of education of the student's parents.
On May 1, 2014, University of Michigan was named one of 55 higher education institutions under investigation by the Office of Civil Rights “for possible violations of federal law over the handling of sexual violence and harassment complaints" by Barack Obama's White House Task Force To Protect Students from Sexual Assault.
The University of Michigan became more selective in the early 2010s. The acceptance rate declined from 50.6% in 2010 to 32.2% in 2014. The rate of new freshman enrollment has been fairly stable since 2010. 
Campus.
The Ann Arbor campus is divided into four main areas: the North, Central, Medical, and South Campuses. The physical infrastructure includes more than 500 major buildings, with a combined area of more than 34 million square feet or 781 acre. The Central and South Campus areas are contiguous, while the North Campus area is separated from them, primarily by the Huron River. There is also leased space in buildings scattered throughout the city, many occupied by organizations affiliated with the University of Michigan Health System. An East Medical Campus has recently been developed on Plymouth Road, with several university-owned buildings for outpatient care, diagnostics, and outpatient surgery.
In addition to the U-M Golf Course on South Campus, the university operates a second golf course on Geddes Road, Radrick Farms Golf Course. The golf course is only open to faculty, staff, and alumni. Another off-campus facility is the Inglis House, which the university has owned since the 1950s. The Inglis House is a 10,000 sqft mansion used to hold various social events, including meetings of the board of regents, and to host visiting dignitaries. The university also operates a large office building called Wolverine Tower in southern Ann Arbor near Briarwood Mall. Another major facility is the Matthaei Botanical Gardens, which is located on the eastern outskirts of Ann Arbor.
All four campus areas are connected by bus services, the majority of which connect the North and Central Campuses. There is a shuttle service connecting the University Hospital, which lies between North and Central Campuses, with other medical facilities throughout northeastern Ann Arbor.
Central Campus.
Central Campus was the original location of U-M when it moved to Ann Arbor in 1837. It originally had a school and dormitory building (where Mason Hall now stands) and several houses for professors on forty acres of land bounded by North University Avenue, South University Avenue, East University Avenue, and State Street. The President's House, located on South University Avenue, is the oldest building on campus as well as the only surviving building from the original forty acre campus. Because Ann Arbor and Central Campus developed simultaneously, there is no distinct boundary between the city and university, and some areas contain a mixture of private and university buildings. Residence halls located on Central Campus are split up into two groups: the Hill Neighborhood and Central Campus.
Central Campus is the location of the College of Literature, Science and the Arts, and is immediately adjacent to the medical campus. Most of the graduate and professional schools, including the Ross School of Business, the Gerald R. Ford School of Public Policy, the Law School and the School of Dentistry, are on Central Campus. Two prominent libraries, the Harlan Hatcher Graduate Library and the Shapiro Undergraduate Library (which are connected by a skywalk), are also on Central Campus, as well as museums housing collections in archaeology, anthropology, paleontology, zoology, dentistry, and art. Ten of the buildings on Central Campus were designed by Detroit-based architect Albert Kahn between 1904 and 1936. The most notable of the Kahn-designed buildings are the Burton Memorial Tower and nearby Hill Auditorium.
North Campus.
North Campus is the most contiguous campus, built independently from the city on a large plot of farm land—approximately 800 acre—that the university bought in 1952. It is newer than Central Campus, and thus has more modern architecture, whereas most Central Campus buildings are classical or gothic in style. The architect Eero Saarinen, based in Birmingham, Michigan, created one of the early master plans for North Campus and designed several of its buildings in the 1950s, including the Earl V. Moore School of Music Building. North and Central Campuses each have unique bell towers that reflect the predominant architectural styles of their surroundings. Each of the bell towers houses a grand carillon. The North Campus tower is called Lurie Tower. The University of Michigan's largest residence hall, Bursley Hall, is located on North Campus.
North Campus houses the College of Engineering, the School of Music, Theatre & Dance, the School of Art & Design, the Taubman College of Architecture and Urban Planning, and an annex of the School of Information. The campus is served by the Duderstadt Center, which houses the Art, Architecture and Engineering Library. The Duderstadt Center also contains multiple computer labs, video editing studios, electronic music studios, an audio studio, a video studio, multimedia workspaces, and a 3D virtual reality room. Other libraries located on North Campus include the Gerald R. Ford Presidential Library and the Bentley Historical Library.
South Campus.
South Campus is the site for the athletic programs, including major sports facilities such as Michigan Stadium, Crisler Center, and Yost Ice Arena. South Campus is also the site of the Buhr library storage facility, Revelli Hall, home of the Michigan Marching Band, the Institute for Continuing Legal Education, and the Student Theatre Arts Complex, which provides shop and rehearsal space for student theatre groups. The university's departments of public safety and transportation services offices are located on South Campus.
U-M's golf course is located south of Michigan Stadium and Crisler Arena. It was designed in the late 1920s by Alister MacKenzie, the designer of Augusta National Golf Club in Augusta, Georgia (home of The Masters Tournament). The course opened to the public in the spring of 1931. The University of Michigan Golf Course was included in a listing of top holes designed by what "Sports Illustrated" calls "golf's greatest course architect." The U-M Golf Course's signature No. 6 hole—a 310 yd par 4, which plays from an elevated tee to a two-tiered, kidney-shaped green protected by four bunkers—is the second hole on the Alister MacKenzie Dream 18 as selected by a five-person panel that includes three-time Masters champion Nick Faldo and golf course architect Tom Doak. The listing of "the best holes ever designed by Augusta National architect Alister MacKenzie" is featured in SI's Golf Plus special edition previewing the Masters on April 4, 2006.
Organization and administration.
The University of Michigan consists of a flagship campus in Ann Arbor, with two regional campuses in Dearborn and Flint. The Board of Regents, which governs the university and was established by the Organic Act of March 18, 1837, consists of eight members elected at large in biennial state elections for overlapping eight-year terms. Between the establishment of the University of Michigan in 1837 and 1850, the Board of Regents ran the university directly; although they were, by law, supposed to appoint a Chancellor to administer the university, they never did. Instead a rotating roster of professors carried out the day-to-day administration duties.
The President of the University of Michigan is the principal executive officer of the university. The office was created by the Michigan Constitution of 1850, which also specified that the president was to be appointed by the Regents of the University of Michigan and preside at their meetings, but without a vote. Today, the president's office is at the Ann Arbor campus, and the president has the privilege of living in the President's House, the university's oldest building located on Central Campus in Ann Arbor. Mark Schlissel is the 14th and current president of the university and has served since July 2014.
There are thirteen undergraduate schools and colleges. By enrollment, the three largest undergraduate units are the College of Literature, Science, and the Arts, the College of Engineering, and the Ross School of Business. At the graduate level, the Rackham Graduate School serves as the central administrative unit of graduate education at the university. There are 18 graduate schools and colleges, the largest of which are the College of Literature, Science, and the Arts, the College of Engineering, the Law School, and the Ross School of Business. Professional degrees are conferred by the Schools of Public Health, Dentistry, Law, Medicine, and Pharmacy. The Medical School is partnered with the University of Michigan Health System, which comprises the university's three hospitals, dozens of outpatient clinics, and many centers for medical care, research, and education.
Endowment.
As of March 2014, U-M's financial endowment (the "University Endowment Fund") was valued at $9.47 billion. In 2013, Michigan's endowment was the eighth largest endowment in the U.S. and the third-largest among U.S public universities at that time; it has been the fastest growing endowment in the nation over the last 21 years. The endowment is primarily used according to the donors' wishes, which include the support of teaching and research. In mid-2000, U-M embarked on a massive fund-raising campaign called "The Michigan Difference," which aimed to raise $2.5 billion, with $800 million designated for the permanent endowment. Slated to run through December 2008, the university announced that the campaign had reached its target 19 months early in May 2007. Ultimately, the campaign raised $3.2 billion over 8 years. Over the course of the capital campaign, 191 additional professorships were endowed, bringing the university total to 471 as of 2009. Like nearly all colleges and universities, U-M suffered significant realized and unrealized losses in its endowment during the second half of 2008. In February 2009, a university spokesperson estimated losses of between 20 and 30 percent.
In November 2013, the university launched the "Victors for Michigan" campaign, which with a $4 billion goal, is its largest fundraising campaign to date.
Student government.
Housed in the Michigan Union, the Central Student Government (CSG) is the central student government of the University. With representatives from each of the University's colleges and schools, CSG represents students and manages student funds on the campus. CSG is a 501(c)(3) organization, independent from the University of Michigan. In recent years CSG has organized airBus, a transportation service between campus and the Detroit Metropolitan Wayne County Airport, and has led the university's efforts to register its student population to vote, with its Voice Your Vote Commission (VYV) registering 10,000 students in 2004. VYV also works to improve access to non-partisan voting-related information and increase student voter turnout. CSG was successful at reviving Homecoming activities, including a carnival and parade, for students after a roughly eleven-year absence in October 2007, and during the 2013-14 school year, was instrumental in persuading the University to rescind an unpopular change in student football seating policy at Michigan Stadium.
There are student governance bodies in each college and school. The two largest colleges at the University of Michigan are the College of Literature, Science, and the Arts (LS&A) and the College of Engineering. Undergraduate students in the LS&A are represented by the LS&A Student Government (LSA SG). Engineering Student Government (ESG) manages undergraduate student government affairs for the College of Engineering. Graduate students enrolled in the Rackham Graduate School are represented by the Rackham Student Government (RSG). In addition, the students that live in the residence halls are represented by the University of Michigan Residence Halls Association (RHA).
A longstanding goal of the student government is to create a student-designated seat on the Board of Regents, the university's governing body. Such a designation would achieve parity with other Big Ten schools that have student regents. In 2000, students Nick Waun and Scott Trudeau ran for the board on the statewide ballot as third-party nominees. Waun ran for a second time in 2002, along with Matt Petering and Susan Fawcett. Although none of these campaigns has been successful, a poll conducted by the State of Michigan in 1998 concluded that a majority of Michigan voters would approve of such a position if the measure were put before them. A change to the board's makeup would require amending the Michigan Constitution.
Academics.
The University of Michigan is a large, four-year, residential research university accredited by the North Central Association of Colleges and Schools. The four year, full-time undergraduate program comprises the majority of enrollments and emphasizes instruction in the arts, sciences, and professions and there is a high level of coexistence between graduate and undergraduate programs. The university has "very high" research activity and the "comprehensive" graduate program offers doctoral degrees in the humanities, social sciences, and STEM fields as well as professional degrees in medicine, law, and dentistry. U-M has been included on Richard Moll's list of Public Ivies. With over 200 undergraduate majors, 100 doctoral and 90 master's programs, U-M conferred 6,490 undergraduate degrees, 4,951 graduate degrees, and 709 first professional degrees in 2011-2012.
National honor societies such as Phi Beta Kappa, Phi Kappa Phi, and Tau Beta Pi have chapters at U-M. Degrees "with Highest Distinction" are recommended to students who rank in the top 3% of their class, "with High Distinction" to the next 7%, and "with Distinction" to the next 15%. Students earning a minimum overall GPA of 3.4 who have demonstrated high academic achievement and capacity for independent work may be recommended for a degree "with Highest Honors," "with High Honors," or "with Honors." Those students who earn all A's for two or more consecutive terms in a calendar year are recognized as James B. Angell Scholars and are invited to attend the annual Honors Convocation, an event which recognizes undergraduate students with distinguished academic achievements.
Out-of-state undergraduate students pay between US $36,001.38 and $43,063.38 annually for tuition alone while in-state undergraduate students paid between US $11,837.38 and $16,363.38 annually. U-M provides financial aid in the form of need-based loans, grants, scholarships, work study, and non-need based scholarships, with 77% of undergraduates in 2007 receiving financial aid. For undergraduates in 2008, 46% graduated with about $25,586 of debt. The university is attempting to increase financial aid availability to students by devoting over $1.53 billion in endowment funds to support financial aid.
Student body.
In Fall 2012, the university had an enrollment of 43,625 students: 28,395 undergraduate students, 12,565 academic degree-seeking graduate students, and 2,665 first professional students in a total of 600 academic programs. Of all students, 36,650 (87.4 percent) are U.S. citizens or permanent residents and 5,274 (12.6 percent) are international students. Each year, some 45,000 people apply for freshman admission; just under a third of applicants are admitted and approximately 6,000 new students enroll. Students come from all 50 U.S. states and more than 100 countries. Approximately 95 percent of the university's incoming class of 2013 had an unweighted high school GPA of 3.5 and higher, with the average accepted unweighted GPA being a 3.85. The middle 50 percent of admitted applicants reported an SAT score of 2030-2250 (Critical Reading 650-740, Math 680-780, Writing 660-760) and an ACT score of 30-33. Full-time students make up about 97 percent of the student body. Among full-time students, the university has a first-time student retention rate of 97 percent.
In 2012, undergraduates were enrolled in 12 schools: About 62 percent in the College of Literature, Science, and the Arts; 21 percent in the College of Engineering; 4 percent in the Ross School of Business; 3 percent in the School of Kinesiology; 3 percent in the School of Music, Theatre & Dance; and 2 percent in the School of Nursing. Small numbers of undergraduates were enrolled in the colleges or schools of Art & Design, Architecture & Urban Planning, Dentistry, Education, Pharmacy, and Public Policy. In 2014, the School of Information opened to undergraduates, with the new Bachelor of Science in Information degree. Among undergraduates, 70 percent graduate with a bachelor's degree within four years, 86 percent graduate within five years and 88 percent graduating within six years.
Of the university's 12,714 non-professional graduate students, 5,367 are seeking academic doctorates and 6,821 are seeking master's degrees. The largest number of master's degree students are enrolled in the Ross School of Business (1,812 students seeking MBA or Master of Accounting degrees) and the College of Engineering (1,456 students seeking M.S. or M.Eng. degrees). The largest number of doctoral students are enrolled in the College of Literature, Science, and the Arts (2,076) and College of Engineering (1,496). While the majority of U-M's graduate degree-granting schools and colleges have both undergraduate and graduate students, a few schools only issue graduate degrees. Presently, the School of Natural Resources and Environment, School of Public Health, and School of Social Work only have graduate students.
In Fall 2010, 2,709 Michigan students were enrolled in U-M's professional schools: the School of Dentistry (439 students), Law School (1,182 students), Medical School (802 students), and College of Pharmacy (439 students).
Research.
The university is one of the founding members (1900) of the Association of American Universities. With over 6,200 faculty members, 73 of whom are members of the National Academy and 471 of whom hold an endowed chair in their discipline, the university manages one of the largest annual collegiate research budgets of any university in the United States, totaling about $1 billion in 2009. The Medical School spent the most at over $445 million, while the College of Engineering was second at more than $160 million. U-M also has a technology transfer office, which is the university conduit between laboratory research and corporate commercialization interests. In 2009, the university consummated a deal to purchase a facility formerly owned by Pfizer. The acquisition includes over 170 acre of property, and 30 major buildings comprising roughly 1600000 sqft of wet laboratory space, and 400000 sqft of administrative space. As of the purchase date, the university's intentions for the space were not announced, but the expectation is that the new space will allow the university to ramp up its research and ultimately employ in excess of 2,000 people.
The university is also a major contributor to the medical field with the EKG, gastroscope, and the announcement of Jonas Salk's polio vaccine. The university's 13000 acre biological station in the Northern Lower Peninsula of Michigan is one of only 47 Biosphere Reserves in the United States.
In the mid-1960s U-M researchers worked with IBM to develop a new virtual memory architectural model that became part of IBM's Model 360/67 mainframe computer (the 360/67 was initially dubbed the 360/65M where the "M" stood for Michigan). The Michigan Terminal System (MTS), an early time-sharing computer operating system developed at U-M, was the first system outside of IBM to use the 360/67's virtual memory features.
U-M is home to the National Election Studies and the University of Michigan Consumer Sentiment Index. The Correlates of War project, also located at U-M, is an accumulation of scientific knowledge about war. The university is also home to major research centers in optics, reconfigurable manufacturing systems, wireless integrated microsystems, and social sciences. The University of Michigan Transportation Research Institute and the Life Sciences Institute are located at the university. The Institute for Social Research (ISR), the nation's longest-standing laboratory for interdisciplinary research in the social sciences, is home to the Survey Research Center, Research Center for Group Dynamics, Center for Political Studies, Population Studies Center, and Inter-Consortium for Political and Social Research. Undergraduate students are able to participate in various research projects through the Undergraduate Research Opportunity Program (UROP) as well as the UROP/Creative-Programs.
The U-M library system comprises nineteen individual libraries with twenty-four separate collections—roughly 13.3 million volumes. U-M was the original home of the JSTOR database, which contains about 750,000 digitized pages from the entire pre-1990 backfile of ten journals of history and economics, and has initiated a book digitization program in collaboration with Google. The University of Michigan Press is also a part of the U-M library system.
In the late 1960s U-M, together with Michigan State University and Wayne State University, founded the Merit Network, one of the first university computer networks. The Merit Network was then and remains today administratively hosted by U-M. Another major contribution took place in 1987 when a proposal submitted by the Merit Network together with its partners IBM, MCI, and the State of Michigan won a national competition to upgrade and expand the National Science Foundation Network (NSFNET) backbone from 56,000 to 1.5 million, and later to 45 million bits per second. In 2006, U-M joined with Michigan State University and Wayne State University to create the University Research Corridor. This effort was undertaken to highlight the capabilities of the state's three leading research institutions and drive the transformation of Michigan's economy. The three universities are electronically interconnected via the Michigan LambdaRail (MiLR, pronounced 'MY-lar'), a high-speed data network providing 10 Gbit/s connections between the three university campuses and other national and international network connection points in Chicago.
The University of Michigan is a participant in the Committee on Institutional Cooperation (CIC), an academic consortium of the universities in the Big Ten Conference plus former conference member the University of Chicago. The initiative also allows students at participating institutions to take distance courses at other participating institutions and forms a partnership of research. Students at participating schools are also allowed "in-house" viewing privileges at other participating schools' libraries.
Student life.
Residential life.
The University of Michigan's campus housing system can accommodate up to 10,900 people, or nearly 30 percent of the total student population at the university. The residence halls are located in three distinct geographic areas on campus: Central Campus, Hill Area (between Central Campus and the University of Michigan Medical Center) and North Campus. Family housing is located on North Campus and mainly serves graduate students. The largest residence hall has a capacity of 1,240 students, while the smallest accommodates 25 residents. A majority of upper-division and graduate students live in off-campus apartments, houses, and cooperatives, with the largest concentrations in the Central and South Campus areas.
The residential system has a number of "living-learning communities" where academic activities and residential life are combined. These communities focus on areas such as research through the Michigan Research Community, medical sciences, community service and the German language. The Michigan Research Community and the Women in Science and Engineering Residence Program are housed in Mosher-Jordan Hall. The Residential College (RC), a living-learning community that is a division of the College of Literature, Science and the Arts, also has its principal instructional space in East Quad. Also housed in East Quad is the Michigan Community Scholars Program, which is dedicated to civic engagement, community service learning and intercultural understanding and dialogue. The Lloyd Hall Scholars Program (LHSP) is located in Alice Lloyd Hall. The Health Sciences Scholars Program (HSSP) is located in Couzens Hall. The North Quad complex houses two additional living-learning communities: the Global Scholars Program and the Max Kade German Program. It is "technology-rich," and houses communication-related programs, including the School of Information, the Department of Communication Studies, and the Department of Screen Arts and Cultures. North Quad is also home to services such as the Language Resource Center and the Sweetland Center for Writing.
The residential system also has a number of "theme communities" where students have the opportunity to be surrounded by students in a residential hall who share similar interests. These communities focus on global leadership, the college transition experience, and internationalism. The Adelia Cheever Program is housed in the Helen Newberry House. The First Year Experience is housed in the Baits II Houses, Northwood Houses, and Markley Hall. The Sophomore Experience is housed in Stockwell Hall and the Transfer Year Experience is housed in Northwood III. The newly organized International Impact program is housed in North Quad.
Groups and activities.
The University lists 1,438 student organizations. With a history of student activism, some of the most visible groups include those dedicated to causes such as civil rights and labor rights. One group is Students for a Democratic Society, which recently reformed with a new chapter on campus as of February 2007. Another student labor campaign organization recently established on campus is the United Students Against Sweatshops (USAS). This group seeks to hold accountable multinational companies that exploit their workers in factories around the world where college apparel is produced. Though the student body generally leans toward left-wing politics, there are also conservative groups, such as Young Americans for Freedom, and non-partisan groups, such as the Roosevelt Institution.
There are also several engineering projects teams, including the University of Michigan Solar Car Team, which has placed first in the North American Solar Challenge six times and third in the World Solar Challenge four times. Michigan Interactive Investments, the TAMID Israel Investment Group, and the Michigan Economics Society are also affiliated with the university.
The university also showcases many community service organizations and charitable projects, including Foundation for International Medical Relief of Children, Dance Marathon at the University of Michigan, The Detroit Partnership, Relay For Life, U-M Stars for the Make-A-Wish Foundation, InnoWorks at the University of Michigan, SERVE, Letters to Success, PROVIDES, Circle K, Habitat for Humanity, and Ann Arbor Reaching Out. Intramural sports are popular, and there are recreation facilities for each of the three campuses.
Fraternities and sororities play a role in the university's social life; approximately 18 percent of undergraduates are involved in Greek life. Membership numbers for the 2009-2010 school year reached the highest in the last two decades. Four different Greek councils—the Interfraternity Council, Multicultural Greek Council, National Pan-Hellenic Council, and Panhellenic Association—represent most Greek organizations. Each council has a different recruitment process.
The Michigan Union and Michigan League are student activity centers located on Central Campus; Pierpont Commons is on North Campus. The Michigan Union houses a majority of student groups, including the student government. The William Monroe Trotter House, located east of Central Campus, is a multicultural student center operated by the university's Office of Multi-Ethnic Student Affairs. The University Activities Center (UAC) is a student-run programming organization and is composed of 14 committees. Each group involves students in the planning and execution of a variety of events both on and off campus.
The Michigan Marching Band, composed of more than 350 students from almost all of U-M's schools, is the university's marching band. Over 100 years old, the band performs at every home football game and travels to at least one away game a year. The student-run and led University of Michigan Pops Orchestra is another musical ensemble that attracts students from all academic backgrounds. It performs regularly in the Michigan Theater. The University of Michigan Men's Glee Club, founded in 1859 and the second oldest such group in the country, is a men's chorus with over 100 members. Its eight-member subset a cappella group, the University of Michigan Friars, which was founded in 1955, is the oldest currently running "a cappella" group on campus.
The University of Michigan also encourages many cultural and ethnic student organizations on campus. There are currently over 317 organizations under this category. There are organizations for almost every culture from the to to even the 
. These organizations hope to promote various aspects of their culture along with raising political and social awareness around campus by hosting an assortment of events throughout the school year. These clubs also help students make this large University into a smaller community to help find people with similar interests and backgrounds.
Media and publications.
The student newspaper is "The Michigan Daily", founded in 1890 and editorially and financially independent of the university. "The Daily" is published five days a week during academic year, and weekly from May to August. Other student publications at the university include the conservative "The Michigan Review" and the progressive "Michigan Independent". The humor publications "Gargoyle" and "The Michigan Every Three Weekly" are also published by Michigan students.
WCBN-FM (88.3 FM) is the student-run college radio station which plays in freeform format. WOLV-TV is the student-run television station that is primarily shown on the university's cable television system.
Several academic journals are published at the university:
Athletics.
The University of Michigan's sports teams are called the Wolverines. They participate in the NCAA's Football Bowl Subdivision (formerly Division I-A) and in the Big Ten Conference in all sports except women's water polo, which is a member of the Collegiate Water Polo Association. U-M boasts 27 varsity sports, including 13 men's teams and 14 women's teams. In 10 of the past 14 years concluding in 2009, U-M has finished in the top five of the NACDA Director's Cup, a ranking compiled by the National Association of Collegiate Directors of Athletics to tabulate the success of universities in competitive sports. U-M has finished in the top 10 of the Directors' Cup standings in 14 of the award's 16 seasons and has placed in the top six in nine of the last 10 seasons.
The Michigan football program ranks first in NCAA history in both total wins (903 through the end of the 2012 season) and winning percentage (.735). The team won the first Rose Bowl game in 1902. U-M had 40 consecutive winning seasons from 1968 to 2007, including consecutive bowl game appearances from 1975 to 2007. The Wolverines have won a record 42 Big Ten championships. The program has eleven national championships, most recently in 1997, and has produced three Heisman Trophy winners: Tom Harmon, Desmond Howard and Charles Woodson.
Michigan Stadium is the largest college football stadium in the nation and one of the largest football-only stadiums in the world, with an official capacity of 109,901 (the extra seat is said to be "reserved" for Fritz Crisler) though attendance—frequently over 111,000 spectators—regularly exceeds the official capacity. The NCAA's record-breaking attendance has become commonplace at Michigan Stadium, especially since the arrival of head coach Bo Schembechler. U-M has fierce rivalries with many teams, including Michigan State, Notre Dame, and Ohio State; ESPN has referred to the Michigan-Ohio State rivalry as the greatest rivalry in American sports. U-M also has all-time winning records against Michigan State, Notre Dame, and Ohio State.
The men's ice hockey team, which plays at Yost Ice Arena, has won nine national championships, while the men's basketball team, which plays at the Crisler Center, has appeared in five Final Fours and won the national championship in 1989. The men's basketball program became involved in a scandal involving payments from a booster during the 1990s. This led to the program being placed on probation for a four-year period. The program also voluntarily vacated victories from its 1992–1993 and 1995–1999 seasons in which the payments took place, as well as its 1992 and 1993 Final Four appearances.
The men's wrestling, men's gymnastics, and women's volleyball teams compete at the Cliff Keen Arena, dedicated and named after longtime wrestling coach Cliff Keen in 1990.
Through the 2008 Summer Olympic Games, 178 U-M students and coaches had participated in the Olympics, winning medals in every Summer Olympics except 1896, and winning gold medals in all but four Olympiads. U-M students have won a total of 133 Olympic medals: 65 gold, 30 silver, and 38 bronze.
School songs.
The University of Michigan's fight song, "The Victors," was written by student Louis Elbel in 1898 following the last-minute football victory over the University of Chicago that won a league championship. The song was declared by John Philip Sousa as "the greatest college fight song ever written." The song refers to the university as being "the Champions of the West." At the time, U-M was part of the Western Conference, which would later become the Big Ten Conference. Michigan was considered to be on the Western Frontier when it was founded in the old Northwest Territory. Although mainly used at sporting events, the fight song can be heard at other events. President Gerald Ford had it played by the United States Marine Band as his entrance anthem during his term as president from 1974 to 1977, in preference over the more traditional "Hail to the Chief", and the Michigan Marching Band performed a slow-tempo variation on the fight song at his funeral. The fight song is also sung during graduation commencement ceremonies. The university's alma mater song is "The Yellow and Blue." A common rally cry is "Let's Go Blue!," which had a complementary short musical arrangement written by former students Joseph Carl, a sousaphonist, and Albert Ahronheim, a drum major.
Before "The Victors" was officially the University's fight song, the song "There'll Be a Hot Time in the Old Town Tonight" was considered to be the school song. After Michigan temporarily withdrew from the Western Conference in 1907, a new Michigan fight song "Varsity" was written in 1911 because the line "champions of the West" was no longer appropriate.
Alumni.
In addition to the late U.S. president Gerald Ford, the university has produced twenty-six Rhodes Scholars. As of 2012, the university has almost 500,000 living alumni.
More than 250 Michigan graduates have served as legislators as either a United States Senator (40 graduates) or as a Congressional representative (over 200 graduates), including former House Majority Leader Dick Gephardt and U.S. Representative Justin Amash, who represents Michigan's Third Congressional District. Mike Duggan, Mayor of Detroit, earned his bachelor and law degree at Michigan, while Michigan Governor Rick Snyder earned his bachelor, M.B.A., and J.D. degrees from Michigan. U-M's contributions to aeronautics include aircraft designer Clarence "Kelly" Johnson of Lockheed Skunk Works fame, Lockheed president Willis Hawkins, and several astronauts including the all-U-M crews of both Gemini 4 and Apollo 15. U-M counts among its matriculants twenty-one billionaires and prominent company founders and co-founders including Google co-founder Larry Page and Dr. J. Robert Beyster, who founded Science Applications International Corporation (SAIC) in 1969. Several U-M graduates contributed greatly to the field of computer science, including Claude Shannon (who made major contributions to the mathematics of information theory), and Turing Award winners Edgar Codd, Stephen Cook, Frances E. Allen and Michael Stonebraker. Marjorie Lee Browne received her M.S. in 1939 and her doctoral degree in 1950, becoming the third African American woman to earn a PhD in mathematics.
Notable writers who attended U-M include playwright Arthur Miller, essayists Susan Orlean and Sven Birkerts, journalists and editors Mike Wallace, Jonathan Chait of "The New Republic", Daniel Okrent, and Sandra Steingraber, food critics Ruth Reichl and Gael Greene, novelists Brett Ellen Block, Elizabeth Kostova, Marge Piercy, Brad Meltzer, Betty Smith, and Charles Major, screenwriter Judith Guest, Pulitzer Prize-winning poet Theodore Roethke, National Book Award winners Keith Waldrop and Jesmyn Ward, composer/author/puppeteer Forman Brown, and Alireza Jafarzadeh (a Middle East analyst, author, and TV commentator).
In Hollywood, famous alumni include actors James Earl Jones, David Alan Grier, actresses Lucy Liu, Gilda Radner, and Selma Blair, and filmmaker Lawrence Kasdan. Many Broadway and musical theatre actors, including Gavin Creel, Andrew Keenan-Bolger, and his sister Celia Keenan-Bolger, attended U-M for musical theatre. The creators of "A Very Potter Musical", known as StarKid Productions, also graduated from the University of Michigan. A member of Starkid, actor and singer Darren Criss, is a series regular on the television series "Glee".
Musical graduates include operatic soprano Jessye Norman, singer Joe Dassin, jazz guitarist Randy Napoleon, and Mannheim Steamroller founder Chip Davis. Classical composer Frank Ticheli and Broadway composer Andrew Lippa attended. Pop Superstar Madonna and rock legend Iggy Pop attended but did not graduate.
Other U-M graduates include HH Holmes (noted serial killer), Donald Kohn (past Vice Chairman of the Board of Governors of the Federal Reserve System), Temel Kotil (president and CEO of Turkish Airlines), current Dean of Harvard Law School Martha Minow, assisted-suicide advocate Dr. Jack Kevorkian, Weather Underground radical activist Bill Ayers, activist Tom Hayden, architect Charles Moore, Rensis Likert (a sociologist who specialized in management styles and developed the Likert scale), the Swedish Holocaust hero Raoul Wallenberg, and Benjamin D. Pritchard (the Civil War general who captured Jefferson Davis). Neurosurgeon and CNN chief medical correspondent Sanjay Gupta attended both college and medical school at U-M. Clarence Darrow attended law school at U-M at a time when many lawyers did not receive any formal education. Frank Murphy, who was mayor of Detroit, governor of Michigan, attorney general of the United States, and Supreme Court justice was also a graduate of the Law School. Conservative pundit Ann Coulter is another U-M law school graduate (J.D. 1988).
Vaughn R. Walker, a federal district judge in California who overturned the controversial California Proposition 8 in 2010 and ruled it unconstitutional, received his undergraduate degree from U-M in 1966.
Some more notorious graduates of the University are 1910 convicted murderer (though perhaps wrongfully so) Dr. Harvey Crippen, late 19th-century American serial killer Herman Mudgett, and "Unabomber" Ted Kaczynski.
U-M athletes have starred in Major League Baseball, the National Football League and National Basketball Association as well as other professional sports. Notable among recent players is Tom Brady of the New England Patriots. Three players have won college football's Heisman Trophy, awarded to the player considered the best in the nation: Tom Harmon (1940), Desmond Howard (1991) and Charles Woodson (1997). Professional golfer John Schroeder and Olympic swimmer Michael Phelps also attended the University of Michigan, with the latter studying Sports Marketing and Management. Phelps also swam competitively for Club Wolverine, a swimming club associated with the university. National Hockey League players Marty Turco, Chris Summers, Max Pacioretty, Carl Hagelin, Brendan Morrison, Jack Johnson, and Michael Cammalleri all played for U-M's ice hockey team. Baseball Hall of Famers George Sisler and Barry Larkin also played baseball at the university.
The university claims the only alumni association with a chapter on the moon, established in 1971 when the crew of Apollo 15 placed a charter plaque for a new U-M Alumni Association on the lunar surface. The plaque states: "The Alumni Association of The University of Michigan. Charter Number One. This is to certify that The University of Michigan Club of The Moon is a duly constituted unit of the Alumni Association and entitled to all the rights and privileges under the Association's Constitution." According to the Apollo 15 astronauts, several small U-M flags were brought on the mission. The presence of a U-M flag on the moon is a long-held campus myth.
References.
General.
</dl>

</doc>
<doc id="31741" url="http://en.wikipedia.org/wiki?curid=31741" title="Unemployment">
Unemployment

Unemployment occurs when people are without work and actively seeking work. The unemployment rate is a measure of the prevalence of unemployment and it is calculated as a percentage by dividing the number of unemployed individuals by all individuals currently in the labor force. During periods of recession, an economy usually experiences a relatively high unemployment rate. According to International Labour Organization report, more than 200 million people globally or 6% of the world's workforce were without a job in 2012.
There remains considerable theoretical debate regarding the causes, consequences and solutions for unemployment. Classical economics, New classical economics, and the Austrian School of economics argue that market mechanisms are reliable means of resolving unemployment. These theories argue against interventions imposed on the labor market from the outside, such as unionization, bureaucratic work rules, minimum wage laws, taxes, and other regulations that they claim discourage the hiring of workers.
Keynesian economics emphasizes the cyclical nature of unemployment and recommends government interventions in the economy that it claims will reduce unemployment during recessions. This theory focuses on recurrent shocks that suddenly reduce aggregate demand for goods and services and thus reduce demand for workers. Keynesian models recommend government interventions designed to increase demand for workers; these can include financial stimuli, publicly funded job creation, and expansionist monetary policies. Keynes believed that the root cause of unemployment is the desire of investors to receive more money rather than produce more products, which is not possible without public bodies producing new money.
In addition to these comprehensive theories of unemployment, there are a few categorizations of unemployment that are used to more precisely model the effects of unemployment within the economic system. The main types of unemployment include structural unemployment which focuses on structural problems in the economy and inefficiencies inherent in labour markets, including a mismatch between the supply and demand of laborers with necessary skill sets. Structural arguments emphasize causes and solutions related to disruptive technologies and globalization. Discussions of frictional unemployment focus on voluntary decisions to work based on each individuals' valuation of their own work and how that compares to current wage rates plus the time and effort required to find a job. Causes and solutions for frictional unemployment often address job entry threshold and wage rates. Behavioral economists highlight individual biases in decision making, and often involve problems and solutions concerning sticky wages and efficiency wages.
Definitions, types, and theories.
Economists distinguish between various overlapping types of and theories of unemployment, including cyclical or Keynesian unemployment, frictional unemployment, structural unemployment and classical unemployment. Some additional types of unemployment that are occasionally mentioned are seasonal unemployment, hardcore unemployment, and hidden unemployment.
Though there have been several definitions of "voluntary" and "involuntary unemployment" in the economics literature, a simple distinction is often applied. Voluntary unemployment is attributed to the individual's decisions, whereas involuntary unemployment exists because of the socio-economic environment (including the market structure, government intervention, and the level of aggregate demand) in which individuals operate. In these terms, much or most of frictional unemployment is voluntary, since it reflects individual search behavior. Voluntary unemployment includes workers who reject low wage jobs whereas involuntary unemployment includes workers fired due to an economic crisis, industrial decline, company bankruptcy, or organizational restructuring.
On the other hand, cyclical unemployment, structural unemployment, and classical unemployment are largely involuntary in nature. However, the existence of structural unemployment may reflect choices made by the unemployed in the past, while classical (natural) unemployment may result from the legislative and economic choices made by labour unions or political parties. So, in practice, the distinction between voluntary and involuntary unemployment is hard to draw.
The clearest cases of involuntary unemployment are those where there are fewer job vacancies than unemployed workers even when wages are allowed to adjust, so that even if all vacancies were to be filled, some unemployed workers would still remain. This happens with cyclical unemployment, as macroeconomic forces cause microeconomic unemployment which can boomerang back and exacerbate these macroeconomic forces.
Classical unemployment.
Classical or real-wage unemployment occurs when real wages for a job are set above the market-clearing level, causing the number of job-seekers to exceed the number of vacancies.
Many economists have argued that unemployment increases with increased governmental regulation. For example, minimum wage laws raise the cost of some low-skill laborers above market equilibrium, resulting in increased unemployment as people who wish to work at the going rate cannot (as the new and higher enforced wage is now greater than the value of their labor). Laws restricting layoffs may make businesses less likely to hire in the first place, as hiring becomes more risky.
However, this argument overly simplifies the relationship between wage rates and unemployment, ignoring numerous factors, which contribute to unemployment. Some, such as Murray Rothbard, suggest that even social taboos can prevent wages from falling to the market-clearing level.
In "Out of Work: Unemployment and Government in the Twentieth-Century America", economists Richard Vedder and Lowell Gallaway argue that the empirical record of wages rates, productivity, and unemployment in American validates classical unemployment theory. Their data shows a strong correlation between adjusted real wage and unemployment in the United States from 1900 to 1990. However, they maintain that their data does not take into account exogenous events.
Cyclical unemployment.
Cyclical, deficient-demand, or Keynesian unemployment, occurs when there is not enough aggregate demand in the economy to provide jobs for everyone who wants to work. Demand for most goods and services falls, less production is needed and consequently fewer workers are needed, wages are sticky and do not fall to meet the equilibrium level, and mass unemployment results. Its name is derived from the frequent shifts in the business cycle although unemployment can also be persistent as occurred during the Great Depression of the 1930s.
With cyclical unemployment, the number of unemployed workers is exceeding the number of job vacancies, so that even if full employment were attained and all open jobs were filled, some workers would still remain unemployed. Some associate cyclical unemployment with frictional unemployment because the factors that cause the friction are partially caused by cyclical variables. For example, a surprise decrease in the money supply may shock rational economic factors and suddenly inhibit aggregate demand.
Keynesian economists on the other hand see the lack of demand for jobs as potentially resolvable by government intervention. One suggested interventions involves deficit spending to boost employment and demand. Another intervention involves an expansionary monetary policy that increases the supply of money which should reduce interest rates which should lead to an increase in non-governmental spending.
Marxian theory of unemployment.
 It is in the very nature of the capitalist mode of production to overwork some workers while keeping the rest as a reserve army of unemployed paupers.
 — Marx, "Theory of Surplus Value", 
Marxists also share the Keynesian viewpoint of the relationship between economic demand and employment, but with the caveat that the market system's propensity to slash wages and reduce labor participation on an enterprise level causes a requisite decrease in aggregate demand in the economy as a whole, causing crises of unemployment and periods of low economic activity before the capital accumulation (investment) phase of economic growth can continue.
According to Karl Marx, unemployment is inherent within the unstable capitalist system and periodic crises of mass unemployment are to be expected. The function of the proletariat within the capitalist system is to provide a "reserve army of labour" that creates downward pressure on wages. This is accomplished by dividing the proletariat into surplus labour (employees) and under-employment (unemployed). This reserve army of labour fight among themselves for scarce jobs at lower and lower wages.
At first glance, unemployment seems inefficient since unemployed workers do not increase profits. However, unemployment is profitable within the global capitalist system because unemployment lowers wages which are costs from the perspective of the owners. From this perspective low wages benefit the system by reducing economic rents. Yet, it does not benefit workers. Capitalist systems unfairly manipulate the market for labour by perpetuating unemployment which lowers laborers' demands for fair wages. Workers are pitted against one another at the service of increasing profits for owners.
According to Marx, the only way to permanently eliminate unemployment would be to abolish capitalism and the system of forced competition for wages and then shift to a socialist or communist economic system. For contemporary Marxists, the existence of persistent unemployment is proof of the inability of capitalism to ensure full employment.
Full employment.
In demand-based theory, it is possible to abolish cyclical unemployment by increasing the aggregate demand for products and workers. However, eventually the economy hits an "inflation barrier" imposed by the four other kinds of unemployment to the extent that they exist. Historical experience suggests that low unemployment affects inflation in the short term but not the long term. In the long term, the velocity of money supply measures such as the MZM ("money zero maturity," representing cash and equivalent demand deposits) velocity is far more predictive of inflation than low unemployment.
Some demand theory economists see the inflation barrier as corresponding to the natural rate of unemployment. The "natural" rate of unemployment is defined as the rate of unemployment that exists when the labour market is in equilibrium and there is pressure for neither rising inflation rates nor falling inflation rates. An alternative technical term for this rate is the NAIRU or the Non-Accelerating Inflation Rate of Unemployment. No matter what its name, demand theory holds that this means that if the unemployment rate gets "too low," inflation will accelerate in the absence of wage and price controls (incomes policies).
One of the major problems with the NAIRU theory is that no one knows exactly what the NAIRU is (while it clearly changes over time). The margin of error can be quite high relative to the actual unemployment rate, making it hard to use the NAIRU in policy-making.
Another, normative, definition of full employment might be called the "ideal" unemployment rate. It would exclude all types of unemployment that represent forms of inefficiency. This type of "full employment" unemployment would correspond to only frictional unemployment (excluding that part encouraging the McJobs management strategy) and would thus be very low. However, it would be impossible to attain this full-employment target using only demand-side Keynesian stimulus without getting below the NAIRU and causing accelerating inflation (absent incomes policies). Training programs aimed at fighting structural unemployment would help here.
To the extent that hidden unemployment exists, it implies that official unemployment statistics provide a poor guide to what unemployment rate coincides with "full employment".
Structural unemployment.
Structural unemployment occurs when a labour market is unable to provide jobs for everyone who wants one because there is a mismatch between the skills of the unemployed workers and the skills needed for the available jobs. Structural unemployment is hard to separate empirically from frictional unemployment, except to say that it lasts longer. As with frictional unemployment, simple demand-side stimulus will not work to easily abolish this type of unemployment.
Structural unemployment may also be encouraged to rise by persistent cyclical unemployment: if an economy suffers from long-lasting low aggregate demand, it means that many of the unemployed become disheartened, while their skills (including job-searching skills) become "rusty" and obsolete. Problems with debt may lead to homelessness and a fall into the vicious circle of poverty.
This means that they may not fit the job vacancies that are created when the economy recovers. The implication is that sustained "high" demand may "lower" structural unemployment. This theory of persistence in structural unemployment has been referred to as an example of path dependence or "hysteresis".
Much "technological unemployment", due to the replacement of workers by machines, might be counted as structural unemployment. Alternatively, technological unemployment might refer to the way in which steady increases in labour productivity mean that fewer workers are needed to produce the same level of output every year. The fact that aggregate demand can be raised to deal with this problem suggests that this problem is instead one of cyclical unemployment. As indicated by Okun's Law, the demand side must grow sufficiently quickly to absorb not only the growing labour force but also the workers made redundant by increased labour productivity.
Seasonal unemployment may be seen as a kind of structural unemployment, since it is a type of unemployment that is linked to certain kinds of jobs (construction work, migratory farm work). The most-cited official unemployment measures erase this kind of unemployment from the statistics using "seasonal adjustment" techniques. This results in substantial, permanent structural unemployment.
Frictional unemployment.
Frictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. It is sometimes called search unemployment and can be voluntary based on the circumstances of the unemployed individual. Frictional unemployment is always present in an economy, so the level of involuntary unemployment is properly the unemployment rate minus the rate of frictional unemployment, which means that increases or decreases in unemployment are normally under-represented in the simple statistics.
Frictional unemployment exists because both jobs and workers are heterogeneous, and a mismatch can result between the characteristics of supply and demand. Such a mismatch can be related to skills, payment, work-time, location, seasonal industries, attitude, taste, and a multitude of other factors. New entrants (such as graduating students) and re-entrants (such as former homemakers) can also suffer a spell of frictional unemployment.
Workers as well as employers accept a certain level of imperfection, risk or compromise, but usually not right away; they will invest some time and effort to find a better match. This is in fact beneficial to the economy since it results in a better allocation of resources. However, if the search takes too long and mismatches are too frequent, the economy suffers, since some work will not get done. Therefore, governments will seek ways to reduce unnecessary frictional unemployment through multiple means including providing education, advice, training, and assistance such as daycare centers.
The frictions in the labour market are sometimes illustrated graphically with a Beveridge curve, a downward-sloping, convex curve that shows a correlation between the unemployment rate on one axis and the vacancy rate on the other. Changes in the supply of or demand for labour cause movements along this curve. An increase (decrease) in labour market frictions will shift the curve outwards (inwards).
Hidden unemployment.
Hidden, or covered, unemployment is the unemployment of potential workers that is not reflected in official unemployment statistics, due to the way the statistics are collected. In many countries only those who have no work but are actively looking for work (and/or qualifying for social security benefits) are counted as unemployed. Those who have given up looking for work (and sometimes those who are on Government "retraining" programs) are not officially counted among the unemployed, even though they are not employed.
The statistic also does not count the "underemployed" – those working fewer hours than they would prefer or in a job that doesn't make good use of their capabilities. In addition, those who are of working age but are currently in full-time education are usually not considered unemployed in government statistics. Traditional unemployed native societies who survive by gathering, hunting, herding, and farming in wilderness areas, may or may not be counted in unemployment statistics. Official statistics often underestimate unemployment rates because of hidden unemployment.
Long-term unemployment.
This is defined in European Union statistics, as unemployment lasting for longer than one year. The United States Bureau of Labor Statistics (BLS), which reports current long-term unemployment rate at 1.9 percent, defines this as unemployment lasting 27 weeks or longer. Long-term unemployment is a component of structural unemployment, which results in long-term unemployment existing in every social group, industry, occupation, and all levels of education. Current long- term unemployment is a result of a 6-year period of weak business hiring, which is the cause of an aggregate demand shortfall. Another factor of current long-term unemployment is the stigma attached to it that makes it harder for people seeking jobs to find employment in low and medium skill jobs because those employers care about long-term unemployment, while high skill jobs mainly focus on an applicants past experiences instead of their long-term unemployment. In response to current rates of long-term unemployment in the United States, which accounts for 31.9% of total unemployment, President Barack Obama implemented policies in January 2014 to assist those who desire to re-enter the work place but are struggling. As of 15 October 2014, the Department of Labor's H-1B funds are providing 23 grants, a total of $170 million, for programs in 20 states and Puerto Rico to help the long-term unemployed re-enter the workforce. The grants were divided between non-profits, local government, and employers to train and match long-term unemployed job seekers for in-demand jobs. The Obama Administration announced a call to action for over 300 businesses to implement best practices for hiring and recruiting the long-term unemployed to provide these candidates an equal chance throughout the hiring process. Along with businesses, the Office of Personnel Management is providing guidance to Federal agencies to establish a trend of fair treatment and consideration for people who are long-term unemployed candidates applying for employment by Federal agencies.
Measurement.
There are also different ways national statistical agencies measure unemployment. These differences may limit the validity of international comparisons of unemployment data. To some degree these differences remain despite national statistical agencies increasingly adopting the definition of unemployment by the International Labour Organization. To facilitate international comparisons, some organizations, such as the OECD, Eurostat, and International Labor Comparisons Program, adjust data on unemployment for comparability across countries.
Though many people care about the number of unemployed individuals, economists typically focus on the unemployment rate. This corrects for the normal increase in the number of people employed due to increases in population and increases in the labour force relative to the population. The unemployment rate is expressed as a percentage, and is calculated as follows:
As defined by the International Labour Organization, "unemployed workers" are those who are currently not working but are willing and able to work for pay, currently available to work, and have actively searched for work.
Individuals who are actively seeking job placement must make the effort to: be in contact with an employer, have job interviews, contact job placement agencies, send out resumes, submit applications, respond to advertisements, or some other means of active job searching within the prior four weeks. Simply looking at advertisements and not responding will not count as actively seeking job placement. Since not all unemployment may be "open" and counted by government agencies, official statistics on unemployment may not be accurate. In the United States, for example, the unemployment rate does not take into consideration those individuals who are not actively looking for employment, such as those still attending college.
The ILO describes 4 different methods to calculate the unemployment rate:
The primary measure of unemployment, U3, allows for comparisons between countries. Unemployment differs from country to country and across different time periods. For example, during the 1990s and 2000s, the United States had lower unemployment levels than many countries in the European Union, which had significant internal variation, with countries like the UK and Denmark outperforming Italy and France. However, large economic events such as the Great Depression can lead to similar unemployment rates across the globe.
European Union (Eurostat).
Eurostat, the statistical office of the European Union, defines unemployed as those persons age 15 to 74 who are not working, have looked for work in the last four weeks, and ready to start work within two weeks, which conform to ILO standards. Both the actual count and rate of unemployment are reported. Statistical data are available by member state, for the European Union as a whole (EU28) as well as for the euro area (EA19). Eurostat also includes a long-term unemployment rate. This is defined as part of the unemployed who have been unemployed for an excess of 1 year.
The main source used is the European Union Labour Force Survey (EU-LFS). The EU-LFS collects data on all member states each quarter. For monthly calculations, national surveys or national registers from employment offices are used in conjunction with quarterly EU-LFS data. The exact calculation for individual countries, resulting in harmonized monthly data, depend on the availability of the data.
United States Bureau of Labor statistics.
The Bureau of Labor Statistics measures employment and unemployment (of those over 15 years of age) using two different labour force surveys conducted by the United States Census Bureau (within the United States Department of Commerce) and/or the Bureau of Labor Statistics (within the United States Department of Labor) that gather employment statistics monthly. The Current Population Survey (CPS), or "Household Survey", conducts a survey based on a sample of 60,000 households. This Survey measures the unemployment rate based on the ILO definition.
The Current Employment Statistics survey (CES), or "Payroll Survey", conducts a survey based on a sample of 160,000 businesses and government agencies that represent 400,000 individual employers. This survey measures only civilian nonagricultural employment; thus, it does not calculate an unemployment rate, and it differs from the ILO unemployment rate definition. These two sources have different classification criteria, and usually produce differing results. Additional data are also available from the government, such as the unemployment insurance weekly claims report available from the Office of Workforce Security, within the U.S. Department of Labor Employment & Training Administration. The Bureau of Labor Statistics provides up-to-date numbers via a PDF linked here. The BLS also provides a readable concise current Employment Situation Summary, updated monthly.
The Bureau of Labor Statistics also calculates six alternate measures of unemployment, U1 through U6, that measure different aspects of unemployment:
"Note: "Marginally attached workers" are added to the total labour force for unemployment rate calculation for U4, U5, and U6." The BLS revised the CPS in 1994 and among the changes the measure representing the official unemployment rate was renamed U3 instead of U5. In 2013, Representative Hunter proposed that the Bureau of Labor Statistics use the U5 rate instead of the current U3 rate.
Statistics for the U.S. economy as a whole hide variations among groups. For example, in January 2008 U.S. unemployment rates were 4.4% for adult men, 4.2% for adult women, 4.4% for Caucasians, 6.3% for Hispanics or Latinos (all races), 9.2% for African Americans, 3.2% for Asian Americans, and 18.0% for teenagers. Also, the U.S. unemployment rate would be at least 2% higher if prisoners and jail inmates were counted.
The unemployment rate is included in a number of major economic indexes including the United States' Conference Board's Index of Leading Indicators a macroeconomic measure of the state of the economy.
Alternatives.
Limitations of the unemployment definition.
Some critics believe that current methods of measuring unemployment are inaccurate in terms of the impact of unemployment on people as these methods do not take into account the 1.5% of the available working population incarcerated in U.S. prisons (who may or may not be working while incarcerated), those who have lost their jobs and have become discouraged over time from actively looking for work, those who are self-employed or wish to become self-employed, such as tradesmen or building contractors or IT consultants, those who have retired before the official retirement age but would still like to work (involuntary early retirees), those on disability pensions who, while not possessing full health, still wish to work in occupations suitable for their medical conditions, those who work for payment for as little as one hour per week but would like to work full-time.
These people are "involuntary part-time" workers, those who are underemployed, e.g., a computer programmer who is working in a retail store until he can find a permanent job, involuntary stay-at-home mothers who would prefer to work, and graduate and Professional school students who were unable to find worthwhile jobs after they graduated with their Bachelor's degrees.
Internationally, some nations' unemployment rates are sometimes muted or appear less severe due to the number of self-employed individuals working in agriculture. Small independent farmers are often considered self-employed; so, they cannot be unemployed. The impact of this is that in non-industrialized economies, such as the United States and Europe during the early 19th century, overall unemployment was approximately 3% because so many individuals were self-employed, independent farmers; yet, unemployment outside of agriculture was as high as 80%.
Many economies industrialize and experience increasing numbers of non-agricultural workers. For example, the United States' non-agricultural labour force increased from 20% in 1800, to 50% in 1850, to 97% in 2000. The shift away from self-employment increases the percentage of the population who are included in unemployment rates. When comparing unemployment rates between countries or time periods, it is best to consider differences in their levels of industrialization and self-employment.
Additionally, the measures of employment and unemployment may be "too high". In some countries, the availability of unemployment benefits can inflate statistics since they give an incentive to register as unemployed. People who do not really seek work may choose to declare themselves unemployed so as to get benefits; people with undeclared paid occupations may try to get unemployment benefits in addition to the money they earn from their work.
However, in countries such as the United States, Canada, Mexico, Australia, Japan and the European Union, unemployment is measured using a sample survey (akin to a Gallup poll). According to the BLS, a number of Eastern European nations have instituted labour force surveys as well. The sample survey has its own problems because the total number of workers in the economy is calculated based on a sample rather than a census.
It is possible to be neither employed nor unemployed by ILO definitions, i.e., to be outside of the "labour force." These are people who have no job and are not looking for one. Many of these are going to school or are retired. Family responsibilities keep others out of the labour force. Still others have a physical or mental disability which prevents them from participating in labour force activities. And of course some people simply elect not to work, preferring to be dependent on others for sustenance.
Typically, employment and the labour force include only work done for monetary gain. Hence, a homemaker is neither part of the labour force nor unemployed. Nor are full-time students nor prisoners considered to be part of the labour force or unemployment. The latter can be important. In 1999, economists Lawrence F. Katz and Alan B. Krueger estimated that increased incarceration lowered measured unemployment in the United States by 0.17% between 1985 and the late 1990s.
In particular, as of 2005, roughly 0.7% of the U.S. population is incarcerated (1.5% of the available working population). Additionally, children, the elderly, and some individuals with disabilities are typically not counted as part of the labour force in and are correspondingly not included in the unemployment statistics. However, some elderly and many disabled individuals are active in the labour market
In the early stages of an economic boom, unemployment often rises. This is because people join the labour market (give up studying, start a job hunt, etc.) because of the improving job market, but until they have actually found a position they are counted as unemployed. Similarly, during a recession, the increase in the unemployment rate is moderated by people leaving the labour force or being otherwise discounted from the labour force, such as with the self-employed.
For the fourth quarter of 2004, according to OECD, (source ), normalized unemployment for men aged 25 to 54 was 4.6% in the U.S. and 7.4% in France. At the same time and for the same population the employment rate (number of workers divided by population) was 86.3% in the U.S. and 86.7% in France. This example shows that the unemployment rate is 60% higher in France than in the U.S., yet more people in this demographic are working in France than in the U.S., which is counterintuitive if it is expected that the unemployment rate reflects the health of the labour market.
Due to these deficiencies, many labour market economists prefer to look at a range of economic statistics such as labour market participation rate, the percentage of people aged between 15 and 64 who are currently employed or searching for employment, the total number of full-time jobs in an economy, the number of people seeking work as a raw number and not a percentage, and the total number of person-hours worked in a month compared to the total number of person-hours people would like to work. In particular the NBER does not use the unemployment rate but prefer various employment rates to date recessions.
Labor force participation rate.
The labor force participation rate is the ratio between the labor force and the overall size of their cohort (national population of the same age range). In the West during the later half of the 20th century, the labor force participation rate increased significantly, due to an increase in the number of women who entered the workplace.
In the United States, there have been four significant stages of women's participation in the labor force - increases in the 20th century and decreases in the 21st century. Male labor force participation decreased from 1953 until 2013. Since October 2013 men have been increasingly joining the labor force.
During the late 19th century through the 1920s, very few women worked outside the home. They were young single women who typically withdrew from the labor force at marriage unless family needed two incomes. These women worked primarily in the textile manufacturing industry or as domestic workers. This profession empowered women and allowed them to earn a living wage. At times, they were a financial help to their families.
Between 1930 and 1950, female labor force participation increased primarily due to the increased demand for office workers, women's participation in the high school movement, and due to electrification which reduced the time spent on household chores. Between the 1950s to the early 1970s, most women were secondary earners working mainly as secretaries, teachers, nurses, and librarians (pink-collar jobs).
Between the mid 1970s to the late 1990s there was a period of revolution of women in the labor force brought on by a source of different factors. Women more accurately planned for their future in the work force, investing in more applicable majors in college that prepared them to enter and compete in the labor market. In the United States, the female labor force participation rate rose from approximately 33% in 1948 to a peak of 60.3% in 2000. As of April 2015 the female labor force participation is at 56.6%, the male labor force participation rate is at 69.4% and the total is 62.8%. 
A common theory in modern economics claims that the rise of women participating in the U.S. labor force in the 1950s through to the 1990s was due to the introduction of a new contraceptive technology, birth control pills, and the adjustment of age of majority laws. The use of birth control gave women the flexibility of opting to invest and advance their career while maintaining a relationship. By having control over the timing of their fertility, they were not running a risk of thwarting their career choices. However, only 40% of the population actually used the birth control pill.
This implies that other factors may have contributed to women choosing to invest in advancing their careers. One factor may be that more and more men delayed the age of marriage, allowing women to marry later in life without worrying about the quality of older men. Other factors include the changing nature of work, with machines replacing physical labor, eliminating many traditional male occupations, and the rise of the service sector, where many jobs are gender neutral.
Another factor that may have contributed to the trend was The Equal Pay Act of 1963, which aimed at abolishing wage disparity based on sex. Such legislation diminished sexual discrimination and encouraged more women to enter the labor market by receiving fair remuneration to help raising families and children.
At the turn of the 21st century the labor force participation began to reverse its long period of increase. Reasons for this change include a rising share of older workers, an increase in school enrollment rates among young workers and a decrease in female labor force participation.
The labor force participation rate can decrease when the rate of growth of the population outweighs that of the employed and unemployed together. The labor force participation rate is a key component in long-term economic growth, almost as important as productivity.
A historic shift began around the end of the great recession as women began leaving the labor force in the United States and other developed countries. The female labor force participation rate in the United States has steadily decreased since 2009 and as of April 2015 the female labor force participation rate has gone back down to 1988 levels of 56.6%. 
Participation rates are defined as follows:
The labor force participation rate explains how an increase in the unemployment rate can occur simultaneously with an increase in employment. If a large amount of new workers enter the labor force but only a small fraction become employed, then the increase in the number of unemployed workers can outpace the growth in employment.
Unemployment ratio.
The unemployment ratio calculates the share of unemployed for the whole population. Particularly many young people between 15 and 24 are studying full-time and are therefore neither working nor looking for a job. This means they are not part of the labour force which is used as the denominator for calculating the unemployment rate. The youth unemployment ratios in the European Union range from 5.2 (Austria) to 20.6 percent (Spain). These are considerably lower than the standard youth unemployment rates, ranging from 7.9 (Germany) to 57.9 percent (Greece).
Effects.
High and persistent unemployment, in which economic inequality increases, has a negative effect on subsequent long-run economic growth. Unemployment can harm growth not only because it is a waste of resources, but also because it generates redistributive pressures and subsequent distortions, drives people to poverty, constrains liquidity limiting labor mobility, and erodes self-esteem promoting social dislocation, unrest and conflict. 2013 Economics Nobel prize winner Robert J. Shiller said that rising inequality in the United States and elsewhere is the most important problem.
Costs.
Individual.
Unemployed individuals are unable to earn money to meet financial obligations. Failure to pay mortgage payments or to pay rent may lead to homelessness through foreclosure or eviction. Across the United States the growing ranks of people made homeless in the foreclosure crisis are generating tent cities.
Unemployment increases susceptibility to malnutrition, illness, mental stress, and loss of self-esteem, leading to depression. According to a study published in Social Indicator Research, even those who tend to be optimistic find it difficult to look on the bright side of things when unemployed. Using interviews and data from German participants aged 16 to 94 – including individuals coping with the stresses of real life and not just a volunteering student population – the researchers determined that even optimists struggled with being unemployed.
In 1979, Brenner found that for every 10% increase in the number of unemployed there is an increase of 1.2% in total mortality, a 1.7% increase in cardiovascular disease, 1.3% more cirrhosis cases, 1.7% more suicides, 4.0% more arrests, and 0.8% more assaults reported to the police.
A study by Ruhm, in 2000, on the effect of recessions on health found that several measures of health actually improve during recessions. As for the impact of an economic downturn on crime, during the Great Depression the crime rate did not decrease. The unemployed in the U.S. often use welfare programs such as Food Stamps or accumulating debt because unemployment insurance in the U.S. generally does not replace a majority of the income one received on the job (and one cannot receive such aid indefinitely).
Not everyone suffers equally from unemployment. In a prospective study of 9570 individuals over four years, highly conscientious people suffered more than twice as much if they became unemployed. The authors suggested this may be due to conscientious people making different attributions about why they became unemployed, or through experiencing stronger reactions following failure. There is also possibility of reverse causality from poor health to unemployment.
Some hold that many of the low-income jobs are not really a better option than unemployment with a welfare state (with its unemployment insurance benefits). But since it is difficult or impossible to get unemployment insurance benefits without having worked in the past, these jobs and unemployment are more complementary than they are substitutes. (These jobs are often held short-term, either by students or by those trying to gain experience; turnover in most low-paying jobs is high.)
Another cost for the unemployed is that the combination of unemployment, lack of financial resources, and social responsibilities may push unemployed workers to take jobs that do not fit their skills or allow them to use their talents. Unemployment can cause underemployment, and fear of job loss can spur psychological anxiety. As well as anxiety, it can cause depression, lack of confidence, and huge amounts of stress. They will begin to lose social contacts, and good social skills.
Social.
An economy with high unemployment is not using all of the resources, specifically labour, available to it. Since it is operating below its production possibility frontier, it could have higher output if all the workforce were usefully employed. However, there is a trade-off between economic efficiency and unemployment: if the frictionally unemployed accepted the first job they were offered, they would be likely to be operating at below their skill level, reducing the economy's efficiency.
During a long period of unemployment, workers can lose their skills, causing a loss of human capital. Being unemployed can also reduce the life expectancy of workers by about seven years.
High unemployment can encourage xenophobia and protectionism as workers fear that foreigners are stealing their jobs. Efforts to preserve existing jobs of domestic and native workers include legal barriers against "outsiders" who want jobs, obstacles to immigration, and/or tariffs and similar trade barriers against foreign competitors.
High unemployment can also cause social problems such as crime; if people have less disposable income than before, it is very likely that crime levels within the economy will increase.
A 2015 study published in "The Lancet" estimates that unemployment causes 45,000 suicides a year globally.
Socio-political.
High levels of unemployment can be causes of civil unrest, in some cases leading to revolution, and particularly totalitarianism. The fall of the Weimar Republic in 1933 and Adolf Hitler's rise to power, which culminated in World War II and the deaths of tens of millions and the destruction of much of the physical capital of Europe, is attributed to the poor economic conditions in Germany at the time, notably a high unemployment rate of above 20%; see Great Depression in Central Europe for details.
Note that the hyperinflation in the Weimar Republic is not directly blamed for the Nazi rise – the Inflation in the Weimar Republic occurred primarily in the period 1921–23, which was contemporary with Hitler's Beer Hall Putsch of 1923, and is blamed for damaging the credibility of democratic institutions, but the Nazis did not assume government until 1933, ten years after the hyperinflation but in the midst of high unemployment.
Rising unemployment has traditionally been regarded by the public and media in any country as a key guarantor of electoral defeat for any government which oversees it. This was very much the consensus in the United Kingdom until 1983, when Margaret Thatcher's Conservative government won a landslide in the general election, despite overseeing a rise in unemployment from 1,500,000 to 3,200,000 since its election four years earlier.
Benefits.
The primary benefit of unemployment is that people are available for hire, without being headhunted away from their existing employers. This permits new and old businesses to take on staff.
Unemployment is argued to be "beneficial" to the people who are not unemployed in the sense that it averts inflation, which itself has damaging effects, by providing (in Marxian terms) a reserve army of labour, that keeps wages in check. However, the direct connection between full local employment and local inflation has been disputed by some due to the recent increase in international trade that supplies low-priced goods even while local employment rates rise to full employment.
 Full employment cannot be achieved because workers would shirk if they were not threatened with the possibility of unemployment. The curve for the no-shirking condition (labeled NSC) goes to infinity at full employment as a result. The inflation-fighting benefits to the "entire economy" arising from a presumed optimum level of unemployment has been studied extensively. The Shapiro–Stiglitz model suggests that wages are not bid down sufficiently to ever reach 0% unemployment. This occurs because employers know that when wages decrease, workers will shirk and expend less effort. Employers avoid shirking by preventing wages from decreasing so low that workers give up and become unproductive. These higher wages perpetuate unemployment while the threat of unemployment reduces shirking.
Before current levels of world trade were developed, unemployment was demonstrated to reduce inflation, following the Phillips curve, or to decelerate inflation, following the NAIRU/natural rate of unemployment theory, since it is relatively easy to seek a new job without losing one's current one. And when more jobs are available for fewer workers (lower unemployment), it may allow workers to find the jobs that better fit their tastes, talents, and needs.
As in the Marxian theory of unemployment, special interests may also benefit: some employers may expect that employees with no fear of losing their jobs will not work as hard, or will demand increased wages and benefit. According to this theory, unemployment may promote general labour productivity and profitability by increasing employers' rationale for their monopsony-like power (and profits).
Optimal unemployment has also been defended as an environmental tool to brake the constantly accelerated growth of the GDP to maintain levels sustainable in the context of resource constraints and environmental impacts. However the tool of denying jobs to willing workers seems a blunt instrument for conserving resources and the environment – it reduces the consumption of the unemployed across the board, and only in the short term. Full employment of the unemployed workforce, all focused toward the goal of developing more environmentally efficient methods for production and consumption might provide a more significant and lasting cumulative environmental benefit and reduced resource consumption. If so the future economy and workforce would benefit from the resultant structural increases in the sustainable level of GDP growth.
Some critics of the "culture of work" such as anarchist Bob Black see employment as overemphasized culturally in modern countries. Such critics often propose quitting jobs when possible, working less, reassessing the cost of living to this end, creation of jobs which are "fun" as opposed to "work," and creating cultural norms where work is seen as unhealthy. These people advocate an "anti-work" ethic for life.
Decline in work hours.
As a result of productivity the work week declined considerably over the 19th century. By the 1920s in the U.S. the average work week was 49 hours, but the work week was reduced to 40 hours (after which overtime premium was applied) as part of the National Industrial Recovery Act of 1933. At the time of the Great Depression of the 1930s it was understood that with the enormous productivity gains due to electrification, mass production and agricultural mechanization, there was no need for a large number of previously employed workers.
Controlling or reducing unemployment.
Societies try a number of different measures to get as many people as possible into work, and various societies have experienced close to full employment for extended periods, particularly during the Post-World War II economic expansion. The United Kingdom in the 1950s and 60s averaged 1.6% unemployment, while in Australia the 1945 "White Paper on Full Employment in Australia" established a government policy of full employment, which policy lasted until the 1970s when the government ran out of money.
However, mainstream economic discussions of full employment since the 1970s suggest that attempts to reduce the level of unemployment below the natural rate of unemployment will fail, resulting only in less output and more inflation.
Demand-side solutions.
Increases in the demand for labour will move the economy along the demand curve, increasing wages and employment. The demand for labour in an economy is derived from the demand for goods and services. As such, if the demand for goods and services in the economy increases, the demand for labour will increase, increasing employment and wages.
There are many ways to stimulate demand for goods and services. Increasing wages to the working class (those more likely to spend the increased funds on goods and services, rather than various types of savings, or commodity purchases) is one theory proposed. Increased wages is believed to be more effective in boosting demand for goods and services than central banking strategies that put the increased money supply mostly into the hands of wealthy persons and institutions. Monetarists suggest that increasing money supply in general will increase short-term demand. Long-term the increased demand will be negated by inflation. A rise in fiscal expenditures is another strategy for boosting aggregate demand.
Providing aid to the unemployed is a strategy used to prevent cutbacks in consumption of goods and services which can lead to a vicious cycle of further job losses and further decreases in consumption/demand. Many countries aid the unemployed through social welfare programs. These unemployment benefits include unemployment insurance, unemployment compensation, welfare and subsidies to aid in retraining. The main goal of these programs is to alleviate short-term hardships and, more importantly, to allow workers more time to search for a job.
A direct demand-side solution to unemployment is government-funded employment of the able-bodied poor. This was notably implemented in Britain from the 17th century until 1948 in the institution of the workhouse, which provided jobs for the unemployed with harsh conditions and poor wages to dissuade their use. A modern alternative is a job guarantee, where the government guarantees work at a living wage.
Temporary measures can include public works programs such as the Works Progress Administration. Government-funded employment is not widely advocated as a solution to unemployment, except in times of crisis; this is attributed to the public sector jobs' existence depending directly on the tax receipts from private sector employment.
In the U.S., the unemployment insurance allowance one receives is based solely on previous income (not time worked, family size, etc.) and usually compensates for one-third of one's previous income. To qualify, one must reside in their respective state for at least a year and, of course, work. The system was established by the Social Security Act of 1935. Although 90% of citizens are covered by unemployment insurance, less than 40% apply for and receive benefits. However, the number applying for and receiving benefits increases during recessions. In cases of highly seasonal industries the system provides income to workers during the off seasons, thus encouraging them to stay attached to the industry.
According to classical economic theory, markets reach equilibrium where supply equals demand; everyone who wants to sell at the market price can. Those who do not want to sell at this price do not; in the labour market this is classical unemployment. Monetary policy and fiscal policy can both be used to increase short-term growth in the economy, increasing the demand for labour and decreasing unemployment.
Supply-side solutions.
However, the labour market is not 100% efficient: It does not clear, though it may be more efficient than bureaucracy. Some argue that minimum wages and union activity keep wages from falling, which means too many people want to sell their labour at the going price but cannot. This assumes perfect competition exists in the labour market, specifically that no single entity is large enough to affect wage levels and that employees are similar in ability.
Advocates of supply-side policies believe those policies can solve this by making the labour market more flexible. These include removing the minimum wage and reducing the power of unions. Supply-siders argue the reforms increase long-term growth by reducing labour costs. This increased supply of goods and services requires more workers, increasing employment. It is argued that supply-side policies, which include cutting taxes on businesses and reducing regulation, create jobs, reduce unemployment and decrease labour's share of national income. Other supply-side policies include education to make workers more attractive to employers.
History.
There are relatively limited historical records on unemployment because it has not always been acknowledged or measured systematically. Industrialization involves economies of scale that often prevent individuals from having the capital to create their own jobs to be self-employed. An individual who cannot either join an enterprise or create a job is unemployed. As individual farmers, ranchers, spinners, doctors and merchants are organized into large enterprises, those who cannot join or compete become unemployed.
Recognition of unemployment occurred slowly as economies across the world industrialized and bureaucratized. Before this, traditional self sufficient native societies have no concept of unemployment. The recognition of the concept of "unemployment" is best exemplified through the well documented historical records in England. For example, in 16th century England no distinction was made between vagrants and the jobless; both were simply categorized as "sturdy beggars", to be punished and moved on.
The closing of the monasteries in the 1530s increased poverty, as the church had helped the poor. In addition, there was a significant rise in enclosure during the Tudor period. Also the population was rising. Those unable to find work had a stark choice: starve or break the law. In 1535, a bill was drawn up calling for the creation of a system of public works to deal with the problem of unemployment, to be funded by a tax on income and capital. A law passed a year later allowed vagabonds to be whipped and hanged.
In 1547, a bill was passed that subjected vagrants to some of the more extreme provisions of the criminal law, namely two years servitude and branding with a "V" as the penalty for the first offense and death for the second. During the reign of Henry VIII, as many as 72,000 people are estimated to have been executed. In the 1576 Act each town was required to provide work for the unemployed.
The Elizabethan Poor Law of 1601, one of the world's first government-sponsored welfare programs, made a clear distinction between those who were unable to work and those able-bodied people who refused employment. Under the Poor Law systems of England and Wales, Scotland and Ireland a workhouse was a place where people who were unable to support themselves, could go to live and work.
Industrial Revolution to late 19th century.
Poverty was a highly visible problem in the eighteenth century, both in cities and in the countryside. In France and Britain by the end of the century, an estimated 10 percent of the people depended on charity or begging for their food.—Jackson J. Spielvogel (2008), Cengage Learning. p.566. ISBN 0-495-50287-1
 By 1776 some 1,912 parish and corporation workhouses had been established in England and Wales, housing almost 100,000 paupers.
A description of the miserable living standards of the mill workers in England in 1844 was given by Fredrick Engels in "The Condition of the Working-Class in England in 1844". In the preface to the 1892 edition Engels notes that the extreme poverty he wrote about in 1844 had largely disappeared. David Ames Wells also noted that living conditions in England had improved near the end of the 19th century and that unemployment was low.
The scarcity and high price of labor in the U.S. during the 19th century was well documented by contemporary accounts, as in the following:
"The laboring classes are comparatively few in number, but this is counterbalanced by, and indeed, may be one of the causes of the eagerness by which they call in the use of machinery in almost every department of industry. Wherever it can be applied as a substitute for manual labor, it is universally and willingly resorted to ...It is this condition of the labor market, and this eager resort to machinery wherever it can be applied, to which, under the guidance of superior education and intelligence, the remarkable prosperity of the United States is due." Joseph Whitworth, 1854
Scarcity of labor was a factor in the economics of slavery in the U.S.
As new territories were opened and Federal land sales conducted, land had to be cleared and new homesteads established. Hundreds of thousands of immigrants annually came to the U.S. and found jobs digging canals and building railroads. Almost all work during most of the 19th century was done by hand or with horses, mules, or oxen, because there was very little mechanization. The workweek during most of the 19th century was 60 hours. Unemployment at times was between one and two percent.
The tight labor market was a factor in productivity gains allowing workers to maintain or increase their nominal wages during the secular deflation that caused real wages to rise at various times in the 19th century, especially in the final decades.
20th century.
There were labor shortages during WW I. Ford Motor Co. doubled wages to reduce turnover. After 1925 unemployment began to gradually rise.
Great Depression.
The decade of the 1930s saw the Great Depression impact unemployment across the globe. One Soviet trading corporation in New York averaged 350 applications a day from Americans seeking jobs in the Soviet Union. In Germany the unemployment rate reached nearly 25% in 1932.
In some towns and cities in the north east of England, unemployment reached as high as 70%; the national unemployment level peaked at more than 22% in 1932. Unemployment in Canada reached 27% at the depth of the Depression in 1933. In 1929, the U.S. unemployment rate averaged 3%. In 1933, 25% of all American workers and 37% of all nonfarm workers were unemployed.
In the U.S., the WPA (1935–43) was the largest make-work program. It hired men (and some women) off the relief roles ("dole") typically for unskilled labor.
In Cleveland, Ohio, the unemployment rate was 60%; in Toledo, Ohio, 80%. There were two million homeless people migrating across the United States. Over 3 million unemployed young men were taken out of the cities and placed into 2600+ work camps managed by the CCC.
Unemployment in the United Kingdom fell later in the 1930s as the depression eased, and remained low (in six figures) after World War II.
Fredrick Mills found that in the U.S., 51% of the decline in work hours was due to the fall in production and 49% was from increased productivity.
By 1972 unemployment in the UK had crept back up above 1,000,000, and was even higher by the end of the decade, with inflation also being high. Although the monetarist economic policies of Margaret Thatcher's Conservative government saw inflation reduced after 1979, unemployment soared in the early 1980s, exceeding 3,000,000 – a level not seen for some 50 years – by 1982. This represented one in eight of the workforce, with unemployment exceeding 20% in some parts of the United Kingdom which had relied on the now-declining industries such as coal mining.
However, this was a time of high unemployment in all major industrialised nations. By the spring of 1983, unemployment in the United Kingdom had risen by 6% in the previous 12 months; compared to 10% in Japan, 23% in the United States of America and 34% in West Germany (seven years before reunification).
Unemployment in the United Kingdom remained above 3,000,000 until the spring of 1987, by which time the economy was enjoying a boom. By the end of 1989, unemployment had fallen to 1,600,000. However, inflation had reached 7.8% and the following year it reached a nine-year high of 9.5%; leading to increased interest rates.
Another recession began during 1990 and lasted until 1992. Unemployment began to increase and by the end of 1992 nearly 3,000,000 in the United Kingdom were unemployed. Then came a strong economic recovery. With inflation down to 1.6% by 1993, unemployment then began to fall rapidly, standing at 1,800,000 by early 1997.
21st century.
The official unemployment rate in the 16 EU countries that use the euro rose to 10% in December 2009 as a result of another recession. Latvia had the highest unemployment rate in EU at 22.3% for November 2009. Europe's young workers have been especially hard hit. In November 2009, the unemployment rate in the EU27 for those aged 15–24 was 18.3%. For those under 25, the unemployment rate in Spain was 43.8%. Unemployment has risen in two-thirds of European countries since 2010.
Into the 21st century, unemployment in the United Kingdom remained low and the economy remaining strong, while at this time several other European economies – namely, France and Germany (reunified a decade earlier) – experienced a minor recession and a substantial rise in unemployment.
In 2008, when the recession brought on another increase in the United Kingdom, after 15 years of economic growth and no major rises in unemployment. Early in 2009, unemployment passed the 2,000,000 mark, by which time economists were predicting it would soon reach 3,000,000. However, the end of the recession was declared in January 2010 and unemployment peaked at nearly 2,700,000 in 2011, appearing to ease fears of unemployment reaching 3,000,000. The unemployment rate of Britain's young black people was 47.4% in 2011. 2013/2014 has seen the employment rate increase from 1,935,836 to 2,173,012 as supported by showing the UK is creating more job opportunities and forecasts the rate of increase in 2014/2015 will be another 7.2%
An 26 April 2005 "Asia Times" article notes that, "In regional giant South Africa, some 300,000 textile workers have lost their jobs in the past two years due to the influx of Chinese goods". The increasing U.S. trade deficit with China cost 2.4 million American jobs between 2001 and 2008, according to a study by the Economic Policy Institute (EPI). From 2000 to 2007, the United States lost a total of 3.2 million manufacturing jobs.
About 25 million people in the world's 30 richest countries will have lost their jobs between the end of 2007 and the end of 2010 as the economic downturn pushes most countries into recession. In April 2010, the U.S. unemployment rate was 9.9%, but the government's broader U-6 unemployment rate was 17.1%. In April 2012, the unemployment rate was 4.6% in Japan. In a 2012 news story, the "Financial Post" reported, "Nearly 75 million youth are unemployed around the world, an increase of more than 4 million since 2007. In the European Union, where a debt crisis followed the financial crisis, the youth unemployment rate rose to 18% last year from 12.5% in 2007, the ILO report shows."

</doc>
