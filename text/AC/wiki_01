<doc id="26197" url="http://en.wikipedia.org/wiki?curid=26197" title="Radiocarbon dating">
Radiocarbon dating

Radiocarbon dating (also referred to as carbon dating or carbon-14 dating) is a method of determining the age of an object containing organic material by using the properties of radiocarbon (14C), a radioactive isotope of carbon. The method was invented by Willard Libby in the late 1940s and soon became a standard tool for archaeologists. Libby received the Nobel Prize for his work in 1960. The radiocarbon dating method is based on the fact that radiocarbon is constantly being created in the atmosphere by the interaction of cosmic rays with atmospheric nitrogen. The resulting radiocarbon combines with atmospheric oxygen to form radioactive carbon dioxide, which is incorporated into plants by photosynthesis; animals then acquire 14C by eating the plants. When the animal or plant dies, it stops exchanging carbon with its environment, and from that point onwards the amount of 14C it contains begins to reduce as the 14C undergoes radioactive decay. Measuring the amount of 14C in a sample from a dead plant or animal such as piece of wood or a fragment of bone provides information that can be used to calculate when the animal or plant died. The older a sample is, the less 14C there is to be detected, and because the half-life of 14C (the period of time after which half of a given sample will have decayed) is about 5,730 years, the oldest dates that can be reliably measured by radiocarbon dating are around 50,000 years ago, although special preparation methods occasionally permit dating of older samples.
The idea behind radiocarbon dating is straightforward, but years of work were required to develop the technique to the point where accurate dates could be obtained. Research has been ongoing since the 1960s to determine what the proportion of 14C in the atmosphere has been over the past fifty-thousand years. The resulting data, in the form of a calibration curve, is now used to convert a given measurement of radiocarbon in a sample into an estimate of the sample's calendar age. Other corrections must be made to account for the proportion of 14C in different types of organisms (fractionation), and the varying levels of 14C throughout the biosphere (reservoir effects). Additional complications come from the burning of fossil fuels such as coal and oil, and from the above-ground nuclear tests done in the 1950s and 1960s. Fossil fuels contain little detectable 14C, and as a result there was a noticeable drop in the proportion of 14C in the atmosphere beginning in the late 19th century. Conversely, nuclear testing increased the amount of 14C in the atmosphere, to a maximum (reached in 1963) of almost twice what it had been before the testing began.
Measurement of radiocarbon was originally done by beta-counting devices, which counted the amount of beta radiation emitted by decaying 14C atoms in a sample. Samples were converted to solid carbon for the earliest devices, but it was quickly discovered that converting them to gas or liquid form gave more accurate results. More recently, accelerator mass spectrometry has become the method of choice; it can be used with much smaller samples (as small as individual plant seeds), and gives results much more quickly. Dates are often reported in years "before present", or BP; this actually refers to a baseline of 1950 AD, so that a date of 500 BP means 1450 AD.
The development of radiocarbon dating has had a profound impact on archaeology. In addition to permitting more accurate dating within archaeological sites than did previous methods, it allows comparison of dates of events across great distances. Histories of archaeology often refer to its impact as the "radiocarbon revolution". Occasionally, the method is used for items of popular interest such as the Shroud of Turin, which is claimed to show an image of the body of Jesus Christ. A sample of linen from the shroud was tested in 1988 and found to date from the 13th or 14th century, casting doubt on its authenticity.
Background.
History.
In the early 1930s Willard Libby was a chemistry student at the University of Berkeley, receiving his Ph.D. in 1933. He remained there as an instructor until the end of the decade. In 1939 the Radiation Laboratory at Berkeley began experiments to determine if any of the elements common in organic matter had isotopes with half-lives long enough to be of value in biomedical research. It was soon discovered that 14C's half-life was far longer than had been previously thought, and in 1940 this was followed by proof that the interaction of slow neutrons with 14N was the main pathway by which 14C was created. It had previously been thought that 14C would be more likely to be created by deuterons interacting with 13C. At some time during World War II Libby read a paper by W. E. Danforth and S. A. Korff, published in 1939, which predicted the creation of 14C in the atmosphere by neutrons from cosmic rays that had been slowed down by collisions with molecules of atmospheric gas. It was this paper that gave Libby the idea that radiocarbon dating might be possible.
In 1945, Libby moved to the University of Chicago. He published a paper in 1946 in which he proposed that the carbon in living matter might include 14C as well as non-radioactive carbon. Libby and several collaborators proceeded to experiment with methane collected from sewage works in Baltimore, and after isotopically enriching their samples they were able to demonstrate that they contained radioactive 14C. By contrast, methane created from petroleum showed no radiocarbon activity. The results were summarized in a paper in "Science" in 1947, in which the authors commented that their results implied it would be possible to date materials containing carbon of organic origin.
Libby and James Arnold proceeded to experiment with samples of wood of known age. For example, two samples taken from the tombs of two Egyptian kings, Zoser and Sneferu, independently dated to 2625 BC plus or minus 75 years, were dated by radiocarbon measurement to an average of 2800 BC plus or minus 250 years. These results were published in "Science" in 1949. In 1960, Libby was awarded the Nobel Prize in Chemistry for this work.
Physical and chemical details.
In nature, carbon exists as two stable, nonradioactive isotopes: carbon-12 (12C), and carbon-13 (13C), and a radioactive isotope, carbon-14 (14C), also known as "radiocarbon". The half-life of 14C (the time it takes for half of a given amount of 14C to decay) is about 5,730 years, so its concentration in the atmosphere might be expected to reduce over thousands of years, but 14C is constantly being produced in the lower stratosphere and upper troposphere by cosmic rays, which generate neutrons that in turn create 14C when they strike nitrogen-14 (14N) atoms. The following nuclear reaction creates 14C:
where n represents a neutron and p represents a proton.
Once produced, the 14C quickly combines with the oxygen in the atmosphere to form carbon dioxide (CO2). Carbon dioxide produced in this way diffuses in the atmosphere, is dissolved in the ocean, and is taken up by plants via photosynthesis. Animals eat the plants, and ultimately the radiocarbon is distributed throughout the biosphere. The ratio of 14C to 12C is approximately 1.5 parts of 14C to 1012 parts of 12C. In addition, about 1% of the carbon atoms are of the stable isotope 13C.
The equation for the radioactive decay of 14C is:
By emitting a beta particle (an electron, e−) and an electron antineutrino (#redirect ), one of the neutrons in the 14C nucleus changes to a proton and the 14C nucleus reverts to the stable (non-radioactive) isotope 14N.
Principles.
During its life, a plant or animal is exchanging carbon with its surroundings, so the carbon it contains will have the same proportion of 14C as the atmosphere. Once it dies, it ceases to acquire 14C, but the 14C within its biological material at that time will continue to decay, and so the ratio of 14C to 12C in its remains will gradually decrease. Because 14C decays at a known rate, the proportion of radiocarbon can be used to determine how long it has been since a given sample stopped exchanging carbon – the older the sample, the less 14C will be left.
The equation governing the decay of a radioactive isotope is:
where "N"0 is the number of atoms of the isotope in the original sample (at time "t" = 0, when the organism from which the sample was taken died), and "N" is the number of atoms left after time "t". "λ" is a constant that depends on the particular isotope; for a given isotope it is equal to the reciprocal of the mean-life – i.e. the average or expected time a given atom will survive before undergoing radioactive decay. The mean-life, denoted by "τ", of 14C is 8,267 years, so the equation above can be rewritten as:
The sample is assumed to have originally had the same 14C/12C ratio as the ratio in the atmosphere, and since the size of the sample is known, the total number of atoms in the sample can be calculated, yielding "N"0, the number of 14C atoms in the original sample. Measurement of "N", the number of 14C atoms currently in the sample, allows the calculation of "t", the age of the sample, using the equation above.
The half-life of a radioactive isotope (usually denoted by t1/2) is a more familiar concept than the mean-life, so although the equations above are expressed in terms of the mean-life, it is more usual to quote the value of 14C's half-life than its mean-life. The currently accepted value for the half-life of 14C is 5,730 years. This means that after 5,730 years, only half of the initial 14C will have remained; a quarter will have remained after 11,460 years; an eighth after 17,190 years; and so on.
The above calculations make several assumptions, such as that the level of 14C in the atmosphere has remained constant over time. In fact, the level of 14C in the atmosphere has varied significantly and as a result the values provided by the equation above have to be corrected by using data from other sources. This is done by calibration curves, which convert a measurement of 14C in a sample into an estimated calendar age. The calculations involve several steps and include an intermediate value called the "radiocarbon age", which is the age in "radiocarbon years" of the sample: an age quoted in radiocarbon years means that no calibration curve has been used − the calculations for radiocarbon years assume that the 14C/12C ratio has not changed over time. Calculating radiocarbon ages also requires the value of the half-life for 14C, which for more than a decade after Libby's initial work was thought to be 5,568 years. This was revised in the early 1960s to 5,730 years, which meant that many calculated dates in papers published prior to this were incorrect (the error is about 3%). For consistency with these early papers, and to avoid the risk of a double correction for the incorrect half-life, radiocarbon ages are still calculated using the incorrect half-life value. A correction for the half-life is incorporated into calibration curves, so even though radiocarbon ages are calculated using a half-life value that is known to be incorrect, the final reported calibrated date, in calendar years, is accurate. When a date is quoted, the reader should be aware that if it is an uncalibrated date (a term used for dates given in radiocarbon years) it may differ substantially from the best estimate of the actual calendar date, both because it uses the wrong value for the half-life of 14C, and because no correction (calibration) has been applied for the historical variation of 14C in the atmosphere over time.
Carbon exchange reservoir.
Carbon is distributed throughout the atmosphere, the biosphere, and the oceans; these are referred to collectively as the carbon exchange reservoir, and each component is also referred to individually as a carbon exchange reservoir. The different elements of the carbon exchange reservoir vary in how much carbon they store, and in how long it takes for the 14C generated by cosmic rays to fully mix with them. This affects the ratio of 14C to 12C in the different reservoirs, and hence the radiocarbon ages of samples that originated in each reservoir. The atmosphere, which is where 14C is generated, contains about 1.9% of the total carbon in the reservoirs, and the 14C it contains mixes in less than seven years. The ratio of 14C to 12C in the atmosphere is taken as the baseline for the other reservoirs: if another reservoir has a lower ratio of 14C to 12C, it indicates that the carbon is older and hence that some of the 14C has decayed. The ocean surface is an example: it contains 2.4% of the carbon in the exchange reservoir, but there is only about 95% as much 14C as would be expected if the ratio were the same as in the atmosphere. The time it takes for carbon from the atmosphere to mix with the surface ocean is only a few years, but the surface waters also receive water from the deep ocean, which has more than 90% of the carbon in the reservoir. Water in the deep ocean takes about 1,000 years to circulate back through surface waters, and so the surface waters contain a combination of older water, with depleted 14C, and water recently at the surface, with 14C in equilibrium with the atmosphere.
Creatures living at the ocean surface have the same 14C ratios as the water they live in, and as a result of the reduced 14C/12C ratio, the radiocarbon age of marine life is typically about 440 years. Organisms on land are in closer equilibrium with the atmosphere and have the same 14C/12C ratio as the atmosphere. These organisms contain about 1.3% of the carbon in the reservoir; sea organisms have a mass of less than 1% of those on land and are not shown on the diagram. Accumulated dead organic matter, of both plants and animals, exceeds the mass of the biosphere by a factor of nearly 3, and since this matter is no longer exchanging carbon with its environment, it has a 14C/12C ratio lower than that of the biosphere.
Dating considerations.
The variation in the 14C/12C ratio in different parts of the carbon exchange reservoir means that a straightforward calculation of the age of a sample based on the amount of 14C it contains will often give an incorrect result. There are several other possible sources of error that need to be considered. The errors are of four general types:
Atmospheric variation.
In the early years of using the technique, it was understood that it depended on the atmospheric 14C/12C ratio having remained the same over the preceding few thousand years. To verify the accuracy of the method, several artefacts that were datable by other techniques were tested; the results of the testing were in reasonable agreement with the true ages of the objects. Over time, however, discrepancies began to appear between the known chronology for the oldest Egyptian dynasties and the radiocarbon dates of Egyptian artefacts. Neither the pre-existing Egyptian chronology nor the new radiocarbon dating method could be assumed to be accurate, but a third possibility was that the 14C/12C ratio had changed over time. The question was resolved by the study of tree rings: comparison of overlapping series of tree rings allowed the construction of a continuous sequence of tree-ring data that spanned 8,000 years. (Since that time the tree-ring data series has been extended to 13,900 years.) In the 1960s, Hans Suess was able to use the tree-ring sequence to show that the dates derived from radiocarbon were consistent with the dates assigned by Egyptologists. This was possible because although annual plants, such as corn, have a 14C/12C ratio that reflects the atmospheric ratio at the time they were growing, trees only add material to their outermost tree ring in any given year, while the inner tree rings don't get their 14C replenished and instead start losing 14C through decay. Hence each ring preserves a record of the atmospheric 14C/12C ratio of the year it grew in. Carbon-dating the wood from the tree rings themselves provides the check needed on the atmospheric 14C/12C ratio: with a sample of known date, and a measurement of the value of "N" (the number of atoms of 14C remaining in the sample), the carbon-dating equation allows the calculation of "N"0 – the number of atoms of 14C in the sample at the time the tree ring was formed – and hence the 14C/12C ratio in the atmosphere at that time. Armed with the results of carbon-dating the tree rings, it became possible to construct calibration curves designed to correct the errors caused by the variation over time in the 14C/12C ratio. These curves are described in more detail below.
Coal and oil began to be burned in large quantities during the 19th century. Both are sufficiently old that they contain little detectable 14C and, as a result, the CO2 released substantially diluted the atmospheric 14C/12C ratio. Dating an object from the early 20th century hence gives an apparent date older than the true date. For the same reason, 14C concentrations in the neighbourhood of large cities are lower than the atmospheric average. This fossil fuel effect (also known as the Suess effect, after Hans Suess, who first reported it in 1955) would only amount to a reduction of 0.2% in 14C activity if the additional carbon from fossil fuels were distributed throughout the carbon exchange reservoir, but because of the long delay in mixing with the deep ocean, the actual effect is a 3% reduction.
A much larger effect comes from above-ground nuclear testing, which released large numbers of neutrons and created 14C. From about 1950 until 1963, when atmospheric nuclear testing was banned, it is estimated that several tonnes of 14C were created. If all this extra 14C had immediately been spread across the entire carbon exchange reservoir, it would have led to an increase in the 14C/12C ratio of only a few per cent, but the immediate effect was to almost double the amount of 14C in the atmosphere, with the peak level occurring in about 1965. The level has since dropped, as the "bomb carbon" (as it is sometimes called) percolates into the rest of the reservoir.
Isotopic fractionation.
Photosynthesis is the primary process by which carbon moves from the atmosphere into living things. In photosynthetic pathways 12C is absorbed slightly more easily than 13C, which in turn is more easily absorbed than 14C. The differential uptake of the three carbon isotopes leads to 13C/12C and 14C/12C ratios in plants that differ from the ratios in the atmosphere. This effect is known as isotopic fractionation.
To determine the degree of fractionation that takes place in a given plant, the amounts of both 12C and 13C isotopes are measured, and the resulting 13C/12C ratio is then compared to a standard ratio known as PDB. The 13C/12C ratio is used instead of 14C/12C because the former is much easier to measure, and the latter can be easily derived: the depletion of 13C relative to 12C is proportional to the difference in the atomic masses of the two isotopes, so the depletion for 14C is twice the depletion of 13C. The fractionation of 13C, known as δ13C, is calculated as follows:
where the ‰ sign indicates parts per thousand. Because the PDB standard contains an unusually high proportion of 13C, most measured δ13C values are negative.
For marine organisms, the details of the photosynthesis reactions are less well understood, and the δ13C values for marine photosynthetic organisms are dependent on temperature. At higher temperatures, CO2 has poor solubility in water, which means there is less CO2 available for the photosynthetic reactions. Under these conditions, fractionation is reduced, and at temperatures above 14 °C the δ13C values are correspondingly higher, while at lower temperatures, CO2 becomes more soluble and hence more available to marine organisms. The δ13C value for animals depends on their diet. An animal that eats food with high δ13C values will have a higher δ13C than one that eats food with lower δ13C values. The animal's own biochemical processes can also impact the results: for example, both bone minerals and bone collagen typically have a higher concentration of 13C than is found in the animal's diet, though for different biochemical reasons. The enrichment of bone 13C also implies that excreted material is depleted in 13C relative to the diet.
Since 13C makes up about 1% of the carbon in a sample, the 13C/12C ratio can be accurately measured by mass spectrometry. Typical values of δ13C have been found by experiment for many plants, as well as for different parts of animals such as bone collagen, but when dating a given sample it is better to determine the δ13C value for that sample directly than to rely on the published values.
The carbon exchange between atmospheric CO2 and carbonate at the ocean surface is also subject to fractionation, with 14C in the atmosphere more likely than 12C to dissolve in the ocean. The result is an overall increase in the 14C/12C ratio in the ocean of 1.5%, relative to the 14C/12C ratio in the atmosphere. This increase in 14C concentration almost exactly cancels out the decrease caused by the upwelling of water (containing old, and hence 14C depleted, carbon) from the deep ocean, so that direct measurements of 14C radiation are similar to measurements for the rest of the biosphere. Correcting for isotopic fractionation, as is done for all radiocarbon dates to allow comparison between results from different parts of the biosphere, gives an apparent age of about 440 years for ocean surface water.
Reservoir effects.
Libby's original exchange reservoir hypothesis assumed that the 14C/12C ratio in the exchange reservoir is constant all over the world, but it has since been discovered that there are several causes of variation in the ratio across the reservoir.
Marine effect
The CO2 in the atmosphere transfers to the ocean by dissolving in the surface water as carbonate and bicarbonate ions; at the same time the carbonate ions in the water are returning to the air as CO2. This exchange process brings14C from the atmosphere into the surface waters of the ocean, but the 14C thus introduced takes a long time to percolate through the entire volume of the ocean. The deepest parts of the ocean mix very slowly with the surface waters, and the mixing is uneven. The main mechanism that brings deep water to the surface is upwelling, which is more common in regions closer to the equator. Upwelling is also influenced by factors such as the topography of the local ocean bottom and coastlines, the climate, and wind patterns. Overall, the mixing of deep and surface waters takes far longer than the mixing of atmospheric CO2 with the surface waters, and as a result water from some deep ocean areas has an apparent radiocarbon age of several thousand years. Upwelling mixes this "old" water with the surface water, giving the surface water an apparent age of about several hundred years (after correcting for fractionation). This effect is not uniform – the average effect is about 440 years, but there are local deviations of several hundred years for areas that are geographically close to each other. The effect also applies to marine organisms such as shells, and marine mammals such as whales and seals, which have radiocarbon ages that appear to be hundreds of years old.
Hemisphere effect
The northern and southern hemispheres have atmospheric circulation systems that are sufficiently independent of each other that there is a noticeable time lag in mixing between the two. The atmospheric 14C/12C ratio is lower in the southern hemisphere, with an apparent additional age of 30 years for radiocarbon results from the south as compared to the north. This is probably because the greater surface area of ocean in the southern hemisphere means that there is more carbon exchanged between the ocean and the atmosphere than in the north. Since the surface ocean is depleted in 14C because of the marine effect, 14C is removed from the southern atmosphere more quickly than in the north.
Other effects
If the carbon in freshwater is partly acquired from aged carbon, such as rocks, then the result will be a reduction in the 14C/12C ratio in the water. For example, rivers that pass over limestone, which is mostly composed of calcium carbonate, will acquire carbonate ions. Similarly, groundwater can contain carbon derived from the rocks through which it has passed. These rocks are usually so old that they no longer contain any measurable 14C, so this carbon lowers the 14C/12C ratio of the water it enters, which can lead to apparent ages of thousands of years for both the affected water and the plants and freshwater organisms that live in it. This is known as the hard water effect because it is often associated with calcium ions, which are characteristic of hard water; other sources of carbon such as humus can produce similar results. The effect varies greatly and there is no general offset that can be applied; additional research is usually needed to determine the size of the offset, for example by comparing the radiocarbon age of deposited freshwater shells with associated organic material.
Volcanic eruptions eject large amounts of carbon into the air. The carbon is of geological origin and has no detectable 14C, so the 14C/12C ratio in the vicinity of the volcano is depressed relative to surrounding areas. Dormant volcanoes can also emit aged carbon. Plants that photosynthesize this carbon also have lower 14C/12C ratios: for example, plants on the Greek island of Santorini, near the volcano, have apparent ages of up to a thousand years. These effects are hard to predict – the town of Akrotiri, on Santorini, was destroyed in a volcanic eruption thousands of years ago, but radiocarbon dates for objects recovered from the ruins of the town show surprisingly close agreement with dates derived from other means. If the dates for Akrotiri are confirmed, it would indicate that the volcanic effect in this case was minimal.
Contamination.
Any addition of carbon to a sample of a different age will cause the measured date to be inaccurate. Contamination with modern carbon causes a sample to appear to be younger than it really is: the effect is greater for older samples. If a sample that is 17,000 years old is contaminated so that 1% of the sample is modern carbon, it will appear to be 600 years younger; for a sample that is 34,000 years old the same amount of contamination would cause an error of 4,000 years. Contamination with old carbon, with no remaining 14C, causes an error in the other direction independent of age – a sample contaminated with 1% old carbon will appear to be about 80 years older than it really is, regardless of the date of the sample.
Samples.
Samples for dating need to be converted into a form suitable for measuring the 14C content; this can mean conversion to gaseous, liquid, or solid form, depending on the measurement technique to be used. Before this can be done, the sample must be treated to remove any contamination and any unwanted constituents. This includes removing visible contaminants, such as rootlets that may have penetrated the sample since its burial. Alkali and acid washes can be used to remove humic acid and carbonate contamination, but care has to be taken to avoid destroying or damaging the sample.
Preparation and size.
Particularly for older samples, it may be useful to enrich the amount of 14C in the sample before testing. This can be done with a thermal diffusion column. The process takes about a month and requires a sample about ten times as large as would be needed otherwise, but it allows more precise measurement of the 14C/12C ratio in old material and extends the maximum age that can be reliably reported.
Once contamination has been removed, samples must be converted to a form suitable for the measuring technology to be used. Where gas is required, CO2 is widely used. For samples to be used in liquid scintillation counters, the carbon must be in liquid form; the sample is typically converted to benzene. For accelerator mass spectrometry, solid graphite targets are the most common, although iron carbide and gaseous CO2 can also be used.
The quantity of material needed for testing depends on the sample type and the technology being used. There are two types of testing technology: detectors that record radioactivity, known as beta counters, and accelerator mass spectrometers. For beta counters, a sample weighing at least 10 grams is typically required. Accelerator mass spectrometry (AMS) is much more sensitive, and samples as small as 0.5 milligrams can be used.
Measurement and results.
For decades after Libby performed the first radiocarbon dating experiments, the only way to measure the 14C in a sample was to detect the radioactive decay of individual carbon atoms. In this approach, what is measured is the activity, in number of decay events per unit mass per time period, of the sample. This method is also known as "beta counting", because it is the beta particles emitted by the decaying 14C atoms that are detected. In the late 1970s an alternative approach became available: directly counting the number of 14C and 12C atoms in a given sample, via accelerator mass spectrometry, usually referred to as AMS. AMS counts the 14C/12C ratio directly, instead of the activity of the sample, but measurements of activity and 14C/12C ratio can be converted into each other exactly. For some time, beta counting methods were more accurate than AMS, but as of 2014 AMS is more accurate and has become the method of choice for radiocarbon measurements. In addition to improved accuracy, AMS has two further significant advantages over beta counting: it can perform accurate testing on samples much too small for beta counting; and it is much faster – an accuracy of 1% can be achieved in minutes with AMS, which is far quicker than would be achievable with the older technology.
Beta counting.
Libby's first detector was a Geiger counter of his own design. He converted the carbon in his sample to lamp black (soot) and coated the inner surface of a cylinder with it. This cylinder was inserted into the counter in such a way that the counting wire was inside the sample cylinder, in order that there should be no material between the sample and the wire. Any interposing material would have interfered with the detection of radioactivity, since the beta particles emitted by decaying 14C are so weak that half are stopped by a 0.01 mm thickness of aluminium.
Libby's method was soon superseded by gas proportional counters, which were less affected by bomb carbon (the additional 14C created by nuclear weapons testing). These counters record bursts of ionization caused by the beta particles emitted by the decaying 14C atoms; the bursts are proportional to the energy of the particle, so other sources of ionization, such as background radiation, can be identified and ignored. The counters are surrounded by lead or steel shielding, to eliminate background radiation and to reduce the incidence of cosmic rays. In addition, anticoincidence detectors are used; these record events outside the counter, and any event recorded simultaneously both inside and outside the counter is regarded as an extraneous event and ignored.
The other common technology used for measuring 14C activity is liquid scintillation counting, which was invented in 1950, but which had to wait until the early 1960s, when efficient methods of benzene synthesis were developed, to become competitive with gas counting; after 1970 liquid counters became the more common technology choice for newly constructed dating laboratories. The counters work by detecting flashes of light caused by the beta particles emitted by 14C as they interact with a fluorescing agent added to the benzene. Like gas counters, liquid scintillation counters require shielding and anticoincidence counters.
For both the gas proportional counter and liquid scintillation counter, what is measured is the number of beta particles detected in a given time period. Since the mass of the sample is known, this can be converted to a standard measure of activity in units of either counts per minute per gram of carbon (cpm/g C), or becquerels per kg (Bq/kg C, in SI units). Each measuring device is also used to measure the activity of a blank sample – a sample prepared from carbon old enough to have no activity. This provides a value for the background radiation, which must be subtracted from the measured activity of the sample being dated to get the activity attributable solely to that sample's 14C. In addition, a sample with a standard activity is measured, to provide a baseline for comparison.
Accelerator mass spectrometry.
AMS counts the atoms of 14C and 12C in a given sample, determining the 14C/12C ratio directly. The sample, often in the form of graphite, is made to emit C− ions (carbon atoms with a single negative charge), which are injected into an accelerator. The ions are accelerated and passed through a stripper, which removes several electrons so that the ions emerge with a positive charge. The C3+ ions are then passed through a magnet that curves their path; the heavier ions are curved less than the lighter ones, so the different isotopes emerge as separate streams of ions. A particle detector then records the number of ions detected in the 14C stream, but since the volume of 12C (and 13C, needed for calibration) is too great for individual ion detection, counts are determined by measuring the electric current created in a Faraday cup. Some AMS facilities are also able to evaluate a sample's fractionation, another piece of data necessary for calculating the sample's radiocarbon age.
The use of AMS, as opposed to simpler forms of mass spectrometry, is necessary because of the need to distinguish the carbon isotopes from other atoms or molecules that are very close in mass, such as 14N and 13CH. As with beta counting, both blank samples and standard samples are used. Two different kinds of blank may be measured: a sample of dead carbon that has undergone no chemical processing, to detect any machine background, and a sample known as a process blank made from dead carbon that is processed into target material in exactly the same way as the sample which is being dated. Any 14C signal from the machine background blank is likely to be caused either by beams of ions that have not followed the expected path inside the detector, or by carbon hydrides such as 12CH2 or 13CH. A 14C signal from the process blank measures the amount of contamination introduced during the preparation of the sample. These measurements are used in the subsequent calculation of the age of the sample.
Calculations.
The calculations to be performed on the measurements taken depend on the technology used, since beta counters measure the sample's radioactivity whereas AMS determines the ratio of the three different carbon isotopes in the sample.
To determine the age of a sample whose activity has been measured by beta counting, the ratio of its activity to the activity of the standard must be found. To determine this, a blank sample (of old, or dead, carbon) is measured, and a sample of known activity is measured. The additional samples allow errors such as background radiation and systematic errors in the laboratory setup to be detected and corrected for. The most common standard sample material is oxalic acid, such as the HOxII standard, 1,000 lb of which was prepared by NIST in 1977 from French beet harvests.
The results from AMS testing are in the form of ratios of 12C, 13C, and 14C, which are used to calculate Fm, the "fraction modern". This is defined as the ratio between the 14C/12C ratio in the sample and
the 14C/12C ratio in modern carbon, which is in turn defined as the 14C/12C ratio that would have been measured in 1950 had there been no fossil fuel effect.
Both beta counting and AMS results have to be corrected for fractionation. This is necessary because different materials of the same age, which because of fractionation have naturally different 14C/12C ratios, will appear to be of different ages because the 14C/12C ratio is taken as the indicator of age. To avoid this, all radiocarbon measurements are converted to the measurement that would have been seen had the sample been made of wood, which has a known δ13C
value of −25‰.
Once the corrected 14C/12C ratio is known, a "radiocarbon age" is calculated using:
formula_6
The calculation uses Libby's half-life of 5,568 years, not the more accurate modern value of 5,730 years. Libby’s value for the half-life is used to maintain consistency with early radiocarbon testing results; calibration curves include a correction for this, so the accuracy of final reported calendar ages is assured.
Errors and reliability.
The reliability of the results can be improved by lengthening the testing time. For example, if counting beta decays for 250 minutes is enough to give an error of ± 80 years, with 68% confidence, then doubling the counting time to 500 minutes will allow a sample with only half as much 14C to be measured with the same error term of 80 years.
Radiocarbon dating is generally limited to dating samples no more than 50,000 years old, as samples older than that have insufficient 14C to be measurable. Older dates have been obtained by using special sample preparation techniques, large samples, and very long measurement times. These techniques can allow dates up to 60,000 and in some cases up to 75,000 years before the present to be measured.
Radiocarbon dates are generally presented with a range of one standard deviation (usually represented by the Greek letter sigma: σ) on either side of the mean. This obscures the fact that the true age of the object being measured may lie outside the range of dates quoted. This was demonstrated in 1970 by an experiment run by the British Museum radiocarbon laboratory, in which weekly measurements were taken on the same sample for six months. The results varied widely (though consistently with a normal distribution of errors in the measurements), and included multiple date ranges (of 1σ confidence) that did not overlap with each other. The extreme measurements included one with a maximum age of under 4,400 years, and another with a minimum age of more than 4,500 years.
Errors in procedure can also lead to errors in the results. If 1% of the benzene in a modern reference sample accidentally evaporates, scintillation counting will give a radiocarbon age that is too young by about 80 years.
Calibration.
The calculations given above produce dates in radiocarbon years: i.e. dates that represent the age the sample would be if the 14C/12C ratio had been constant historically. Although Libby had pointed out as early as 1955 the possibility that this assumption was incorrect, it was not until discrepancies began to accumulate between measured ages and known historical dates for artefacts that it became clear that a correction would need to be applied to radiocarbon ages to obtain calendar dates.
To produce a curve that can be used to relate calendar years to radiocarbon years, a sequence of securely dated samples is needed which can be tested to determine their radiocarbon age. The study of tree rings led to the first such sequence: individual pieces of wood show characteristic sequences of rings that vary in thickness because of environmental factors such as the amount of rainfall in a given year. These factors affect all trees in an area, so examining tree-ring sequences from old wood allows the identification of overlapping sequences. In this way, an uninterrupted sequence of tree rings can be extended far into the past. The first such published sequence, based on bristlecone pine tree rings, was created by Wesley Ferguson. Hans Suess used this data to publish the first calibration curve for radiocarbon dating in 1967. The curve showed two types of variation from the straight line: a long term fluctuation with a period of about 9,000 years, and a shorter term variation, often referred to as "wiggles", with a period of decades. Suess said he drew the line showing the wiggles by "cosmic "schwung"", by which he meant that the variations were caused by extraterrestrial forces. It was unclear for some time whether the wiggles were real or not, but they are now well-established. These short term fluctuations in the calibration curve are now known as de Vries effects, after Hessel de Vries.
A calibration curve is used by taking the radiocarbon date reported by a laboratory, and reading across from that date on the vertical axis of the graph. The point where this horizontal line intersects the curve will give the calendar age of the sample on the horizontal axis. This is the reverse of the way the curve is constructed: a point on the graph is derived from a sample of known age, such as a tree ring; when it is tested, the resulting radiocarbon age gives a data point for the graph.Over the next thirty years many calibration curves were published using a variety of methods and statistical approaches. These were superseded by the INTCAL series of curves, beginning with INTCAL98, published in 1998, and updated in 2004, 2009, and 2013. The improvements to these curves are based on new data gathered from tree rings, varves, coral, plant macrofossils, speleothems, and foraminifera. The INTCAL13 data includes separate curves for the northern and southern hemispheres, as they differ systematically because of the hemisphere effect; there is also a separate marine calibration curve. For a set of samples with a known sequence and separation in time such as a sequence of tree rings, the samples' radiocarbon ages form a small subset of the calibration curve. The resulting curve can then be matched to the actual calibration curve by identifying where, in the range suggested by the radiocarbon dates, the wiggles in the calibration curve best match the wiggles in the curve of sample dates. This "wiggle-matching" technique can lead to more precise dating than is possible with individual radiocarbon dates. Wiggle-matching can be used in places where there is a plateau on the calibration curve, and hence can provide a much more accurate date than the intercept or probability methods are able to produce. The technique is not restricted to tree rings; for example, a stratified tephra sequence in New Zealand, known to predate human colonization of the islands, has been dated to 1314 AD ± 12 years by wiggle-matching. The wiggles also mean that reading a date from a calibration curve can give more than one answer: this occurs when the curve wiggles up and down enough that the radiocarbon age intercepts the curve in more than one place, which may lead to a radiocarbon result being reported as two separate age ranges, corresponding to the two parts of the curve that the radiocarbon age intercepted.
Bayesian statistical techniques can be applied when there are several radiocarbon dates to be calibrated. For example, if a series of radiocarbon dates is taken from different levels in a given stratigraphic sequence, Bayesian analysis can help determine if some of the dates should be discarded as anomalies, and can use the information to improve the output probability distributions. When Bayesian analysis was introduced, its use was limited by the need to use mainframe computers to perform the calculations, but the technique has since been implemented on programs available for personal computers, such as OxCal.
Reporting dates.
Several formats for citing radiocarbon results have been used since the first samples were dated. As of 2014, the standard format required by the journal "Radiocarbon" is as follows.
Uncalibrated dates should be reported as "<laboratory>: <14C year> ± <range> BP", where:
For example, the uncalibrated date "UtC-2020: 3510 ± 60 BP" indicates that the sample was tested by the Utrecht van der Graaf Laboratorium, where it has a sample number of 2020, and that the uncalibrated age is 3510 years before present, ± 60 years. Related forms are sometimes used: for example, "10 ka BP" means 10,000 radiocarbon years before present (i.e. 8,050 BC), and 14C yr BP might be used to distinguish the uncalibrated date from a date derived from another dating method such as thermoluminescence.
Calibrated 14C dates are frequently reported as cal BP, cal BC, or cal AD, again with BP referring to the year 1950 as the zero date. "Radiocarbon" gives two options for reporting calibrated dates. A common format is "cal <date-range> <confidence>", where:
For example, "cal 1220–1281 AD (1σ)" means a calibrated date for which the true date lies between 1220 AD and 1281 AD, with the confidence level given as 1σ, or one standard deviation. Calibrated dates can also be expressed as BP instead of using BC and AD. The curve used to calibrate the results should be the latest available INTCAL curve. Calibrated dates should also identify any programs, such as OxCal, used to perform the calibration. In addition, an article in "Radiocarbon" in 2014 about radiocarbon date reporting conventions recommends that information should be provided about sample treatment, including the sample material, pretreatment methods, and quality control measurements; that the citation to the software used for calibration should specify the version number and any options or models used; and that the calibrated date should be given with the associated probabilities for each range.
Use in archaeology.
Interpretation.
A key concept in interpreting radiocarbon dates is archaeological association: what is the true relationship between two or more objects at an archaeological site? It frequently happens that a sample for radiocarbon dating can be taken directly from the object of interest, but there are also many cases where this is not possible. Metal grave goods, for example, cannot be radiocarbon dated, but they may be found in a grave with a coffin, charcoal, or other material which can be assumed to have been deposited at the same time. In these cases a date for the coffin or charcoal is indicative of the date of deposition of the grave goods, because of the direct functional relationship between the two. There are also cases where there is no functional relationship, but the association is reasonably strong: for example, a layer of charcoal in a rubbish pit provides a date which has a relationship to the rubbish pit.
Contamination is of particular concern when dating very old material obtained from archaeological excavations and great care is needed in the specimen selection and preparation. In 2014, Tom Higham and co-workers suggested that many of the dates published for Neanderthal artefacts are too recent because of contamination by "young carbon".
As a tree grows, only the outermost tree ring exchanges carbon with its environment, so the age measured for a wood sample depends on where the sample is taken from. This means that radiocarbon dates on wood samples can be older than the date at which the tree was felled. In addition, if a piece of wood is used for multiple purposes, there may be a significant delay between the felling of the tree and the final use in the context in which it is found. This is often referred to as the "old wood" problem. One example is the Bronze Age trackway at Withy Bed Copse, in England; the trackway was built from wood that had clearly been worked for other purposes before being re-used in the trackway. Another example is driftwood, which may be used as construction material. It is not always possible to recognize re-use. Other materials can present the same problem: for example, bitumen is known to have been used by some Neolithic communities to waterproof baskets; the bitumen's radiocarbon age will be greater than is measurable by the laboratory, regardless of the actual age of the context, so testing the basket material will give a misleading age if care is not taken. A separate issue, related to re-use, is that of lengthy use, or delayed deposition. For example, a wooden object that remains in use for a lengthy period will have an apparent age greater than the actual age of the context in which it is deposited.
Notable applications.
Pleistocene/Holocene boundary in Two Creeks Fossil Forest.
The Pleistocene is a geological epoch that began about 2.6 million years ago. The Holocene, the current geological epoch, begins about 11,700 years ago, when the Pleistocene ends. Establishing the date of this boundary − which is defined by sharp climatic warming − as accurately as possible has been a goal of geologists for much of the 20th century. At Two Creeks, in Wisconsin, a fossil forest was discovered (Two Creeks Buried Forest State Natural Area), and subsequent research determined that the destruction of the forest was caused by the Valders ice readvance, the last southward movement of ice before the end of the Pleistocene in that area. Before the advent of radiocarbon dating, the fossilized trees had been dated by correlating sequences of annually deposited layers of sediment at Two Creeks with sequences in Scandinavia. This led to estimates that the trees were between 24,000 and 19,000 years old, and hence this was taken to be the date of the last advance of the Wisconsin glaciation before its final retreat marked the end of the Pleistocene in North America. In 1952 Libby published radiocarbon dates for several samples from the Two Creeks site and two similar sites nearby; the dates were averaged to 11,404 BP with a standard error of 350 years. This result was uncalibrated, as the need for calibration of radiocarbon ages was not yet understood. Further results over the next decade supported an average date of 11,350 BP, with the results thought to be most accurate averaging 11,600 BP. There was initial resistance to these results on the part of Ernst Antevs, the palaeobotanist who had worked on the Scandinavian varve series, but his objections were eventually discounted by other geologists. In the 1990s samples were tested with AMS, yielding (uncalibrated) dates ranging from 11,640 BP to 11,800 BP, both with a standard error of 160 years. Subsequently a sample from the fossil forest was used in an interlaboratory test, with results provided by over 70 laboratories. These tests produced a median age of 11,788 ± 8 BP (2σ confidence) which when calibrated gives a date range of 13,730 to 13,550 cal BP. The Two Creeks radiocarbon dates are now regarded as a key result in developing the modern understanding of North American glaciation at the end of the Pleistocene.
Dead Sea Scrolls.
In 1947, scrolls were discovered in caves near the Dead Sea that proved to contain writing in Hebrew and Aramaic, most of which are thought to have been produced by the Essenes, a small Jewish sect. These scrolls are of great significance in the study of Biblical texts because many of them contain the earliest known version of books of the Hebrew bible. A sample of the linen wrapping from one of these scrolls, the Great Isaiah Scroll, was included in an 1955 analysis by Libby, with an estimated age of 1,917 ± 200 years. Based on an analysis of the writing style, palaeographic estimates were made of the age of 21 of the scrolls, and samples from most of these, along with other scrolls which had not been palaeographically dated, were tested by two AMS laboratories in the 1990s. The results ranged in age from the early 4th century BC to the mid 4th century AD. In many cases the scrolls were determined to be older than the palaeographically determined age. The Isaiah scroll was included in the testing and was found to have two possible date ranges at a 2σ confidence level, because of the shape of the calibration curve at that point: there is a 15% chance that it dates from 355−295 BC, and an 84% chance that it dates from 210−45 BC. Subsequently these dates were criticized on the grounds that before the scrolls were tested, they had been treated with modern castor oil in order to make the writing easier to read; it was argued that failure to remove the castor oil sufficiently would have caused the dates to be too young. Multiple papers have been published both supporting and opposing the criticism.
Impact.
Soon after the publication of Libby's 1949 paper in "Science", radiocarbon dating laboratories were being established at universities around the world, and by the end of the 1950s there were more than 20 active 14C research laboratories. It was quickly apparent that the principles of radiocarbon dating were valid, despite certain discrepancies, the causes of which were then unknown.
The development of radiocarbon dating has had a profound impact on archaeology; it is often described as the "radiocarbon revolution". In the words of anthropologist R. E. Taylor, "14C data made a "world" prehistory possible by contributing a time scale that transcends local, regional and continental boundaries". It provides more accurate dating within sites than previous methods, which were usually derived from either stratigraphy or typologies (e.g. of stone tools or pottery); it also allows comparison and synchronization of events across great distances. The advent of radiocarbon dating may even have led to better field methods in archaeology, since better data recording leads to firmer association of objects with the samples to be tested. These improved field methods were sometimes motivated by attempts to prove that a 14C date was incorrect. Taylor also suggests that the availability of definite date information freed archaeologists from the need to focus so much of their energy on determining the dates of their finds, and led to an expansion of the questions archaeologists were willing to research. For example, questions about the evolution of human behaviour were much more frequently seen in archaeology, beginning in the 1970s.
The dating framework provided by radiocarbon led to a change in the prevailing view of how innovations spread through prehistoric Europe. It had previously been thought that many ideas spread by diffusion through the continent, or by invasions of peoples bringing new cultural ideas with them. As radiocarbon dates began to prove these ideas wrong in many instances, it became apparent that these innovations must sometimes have arisen locally. This has been described as a "second radiocarbon revolution", and with regard to British prehistory, archaeologist Richard Atkinson has characterized the impact of radiocarbon dating as "radical ... therapy" for the "progressive disease of invasionism". More broadly, the success of radiocarbon dating stimulated interest in analytical and statistical approaches to archaeological data. Taylor has also described the impact of AMS, and the ability to obtain accurate measurements from very small samples, as ushering in a third radiocarbon revolution.
Occasionally, radiocarbon dating techniques are used to date an object of popular interest. An example is the Shroud of Turin, a piece of linen cloth thought by some to bear an image of Jesus Christ after his crucifixion. The Shroud of Turin was tested in 1988; the results, from three separate laboratories, dated the sample of linen tested to the 14th century, raising doubts about the shroud's authenticity.
Other radioactive isotopes created by cosmic rays have been studied to determine if they could also be used to assist in dating objects of archaeological interest; they include 3He, 10Be, 21Ne, 26Al, and 36Cl. With the development of AMS in the 1980s it became possible to measure these isotopes precisely enough for them to be the basis of useful dating techniques, which have been primarily applied to dating rocks. Naturally occurring radioactive isotopes can also form the basis of dating methods: this is the case with potassium-argon dating, argon-argon dating, and uranium series dating. Other dating techniques of interest to archaeologists include thermoluminescence, optically stimulated luminescence, electron spin resonance dating, and fission track dating, as well as techniques that depend on annual bands or layers, such as dendrochronology, tephrochronology, and varve chronology.

</doc>
<doc id="26199" url="http://en.wikipedia.org/wiki?curid=26199" title="Roald Amundsen">
Roald Amundsen

Roald Engelbregt Gravning Amundsen (]; 16 July 1872 – c. 18 June 1928) was a Norwegian explorer of polar regions. He led the Antarctic expedition (1910–12) that was the first to reach the South Pole, on 14 December 1911. In 1926 he was the first expedition leader to be recognized without dispute as having reached the North Pole. He is also known as having the first expedition to traverse the Northwest Passage (1903–06) in the Arctic. 
He disappeared in June 1928 in the Arctic while taking part in a rescue mission by plane. Amundsen was among key expedition leaders, including Douglas Mawson, Robert Falcon Scott, and Ernest Shackleton, during the Heroic Age of Antarctic Exploration.
Early life.
Amundsen was born to a family of Norwegian shipowners and captains in Borge, between the towns Fredrikstad and Sarpsborg. His parents were Jens Amundsen and Hanna Sahlqvist. Roald was the fourth son in the family. His mother wanted him to avoid the family maritime trade and encouraged him to become a doctor, a promise that Amundsen kept until his mother died when he was aged 21. He promptly quit university for a life at sea.
Amundsen had hidden a lifelong desire inspired by Fridtjof Nansen's crossing of Greenland in 1888 and Franklin's lost expedition. He decided on a life of intense exploration of wilderness places.
Polar treks.
Belgian Antarctic Expedition (1897–99).
Amundsen joined the Belgian Antarctic Expedition (1897–99) as first mate. This expedition, led by Adrien de Gerlache using the ship the RV "Belgica", became the first expedition to winter in Antarctica. The "Belgica", whether by mistake or design, became locked in the sea ice at 70°30′S off Alexander Island, west of the Antarctic Peninsula. The crew endured a winter for which they were poorly prepared. By Amundsen's own estimation, the doctor for the expedition, the American Frederick Cook, probably saved the crew from scurvy by hunting for animals and feeding the crew fresh meat. In cases where citrus fruits are lacking, fresh meat from animals that make their own vitamin C (which most do) contains enough of the vitamin to prevent scurvy, and even partly treat it. This was an important lesson for Amundsen's future expeditions.
Northwest Passage (1903–1906).
In 1903, Amundsen led the first expedition to successfully traverse Canada's Northwest Passage between the Atlantic and Pacific oceans. He planned a small expedition of six men in a 45-ton fishing vessel, "Gjøa," in order to have flexibility. His ship had relatively shallow draft. His technique was to use a small ship and hug the coast. Amundsen had the ship outfitted with a small gasoline engine. They traveled via Baffin Bay, the Parry Channel and then south through Peel Sound, James Ross Strait, Simpson Strait and Rae Strait. They spent two winters (1903-1904 and 1904-1905) at King William Island in the harbor of what is today Gjoa Haven, Nunavut, Canada. During this time, Amundsen and the crew learned from the local Netsilik Inuit people about Arctic survival skills, which he found invaluable in his later expedition to the South Pole. For example, he learned to use sled dogs for transportation of goods and to wear animal skins in lieu of heavy, woolen parkas, which could not deter cold when wet.
Leaving Gjoa Haven, he sailed west and passed Cambridge Bay, which had been reached from the west by Richard Collinson in 1852. Continuing to the south of Victoria Island, the ship cleared the Canadian Arctic Archipelago on 17 August 1905. It had to stop for the winter before going on to Nome on the Alaska District's Pacific coast. Five hundred miles (800 km) away, Eagle City, Alaska, had a telegraph station; Amundsen traveled there (and back) overland to wire a success message (collect) on 5 December 1905. His team reached Nome in 1906. Because the water along the route was sometimes as shallow as 3 ft, a larger ship could not have made the voyage.
At this time that Amundsen learned that Norway had formally become independent of Sweden and had a new king. The explorer sent the new King Haakon VII news that his traversing the Northwest Passage "was a great achievement for Norway". He said he hoped to do more and signed it "Your loyal subject, Roald Amundsen." The crew returned to Oslo in November 1906, after almost 3.5 years abroad. It took until 1972 to have the "Gjøa" returned to Norway. After a 45-day trip from San Francisco on a bulk carrier, the "Gjøa" was placed in her current location on land, outside the Fram Museum in Oslo.
South Pole Expedition (1910–12).
Amundsen planned next to take an expedition to the North Pole and explore the Arctic Basin. Finding it difficult to raise funds, when he heard in 1909 that the Americans Frederick Cook and Robert Peary had claimed to reach the North Pole as a result of two different expeditions, he decided to reroute to Antarctica. He was not clear about his intentions, and the Englishman Robert F. Scott and the Norwegian supporters felt misled. Scott was planning his own expedition to the South Pole that year. Using the ship "Fram" ("Forward"), earlier used by Fridtjof Nansen, Amundsen left Oslo for the south on 3 June 1910. At Madeira, Amundsen alerted his men that they would be heading to Antarctica, and sent a telegram to Scott, notifying him simply: "BEG TO INFORM YOU FRAM PROCEEDING ANTARCTIC--AMUNDSEN."
Nearly six months later, the expedition arrived at the eastern edge of the Ross Ice Shelf (then known as "the Great Ice Barrier"), at a large inlet called the Bay of Whales, on 14 January 1911. Amundsen established his base camp there, calling it "Framheim." Amundsen eschewed the heavy wool clothing worn on earlier Antarctic attempts in favour of adopting Inuit-style furred skins.
Using skis and dog sleds for transportation, Amundsen and his men created supply depots at 80°, 81° and 82° South on the Barrier, along a line directly south to the Pole. Amundsen also planned to kill some of his dogs on the way and use them as a source for fresh meat. A small group, including Hjalmar Johansen, Kristian Prestrud and Jørgen Stubberud, set out on 8 September 1911, but had to abandon their trek due to extreme temperatures. The painful retreat caused a quarrel within the group, and Amundsen sent Johansen and the other two men to explore King Edward VII Land.
A second attempt, with a team made up of Olav Bjaaland, Helmer Hanssen, Sverre Hassel, Oscar Wisting, and Amundsen, departed base camp on 19 October 1911. They took four sledges and 52 dogs. Using a route along the previously unknown Axel Heiberg Glacier, they arrived at the edge of the Polar Plateau on 21 November after a four-day climb. On 14 December 1911, the team of five, with 16 dogs, arrived at the Pole (90° 0′ S). They arrived 33–34 days before Scott’s group. Amundsen named their South Pole camp "Polheim," meaning "Home on the Pole." Amundsen renamed the Antarctic Plateau as King Haakon VII’s Plateau. They left a small tent and letter stating their accomplishment, in case they did not return safely to Framheim.
The team returned to Framheim on 25 January 1912, with 11 surviving dogs. They made their way off the continent and to Hobart, Australia, where Amundsen publicly announced his success on 7 March 1912. He telegraphed news to backers.
Amundsen's expedition benefited from his careful preparation, good equipment, appropriate clothing, a simple primary task (Amundsen did no surveying on his route south and is known to have taken only two photographs), an understanding of dogs and their handling, and the effective use of skis. In contrast to the misfortunes of Scott’s team, Amundsen’s trek proved rather smooth and uneventful.
In Amundsen's own words:
I may say that this is the greatest factor—the way in which the expedition is equipped—the way in which every difficulty is foreseen, and precautions taken for meeting or avoiding it. Victory awaits him who has everything in order—luck, people call it. Defeat is certain for him who has neglected to take the necessary precautions in time; this is called bad luck.—from "The South Pole," by Roald Amundsen
Amundsen wrote about the expedition in "The South Pole: An Account of the Norwegian Antarctic Expedition in the 'Fram,' 1910–12" (1912).
Northeast Passage (1918–20).
In 1918, Amundsen began an expedition with a new ship "Maud", lasted until 1925. "Maud" sailed west to east through the Northeast Passage, now called the "Northern Route" (1918–20).
With him on this expedition were Oscar Wisting and Helmer Hanssen, both of whom had been part of the team to reach the South Pole. In addition, Henrik Lindstrøm was included as a cook. He suffered a stroke and was so physically reduced that he could not participate.
The goal of the expedition was to explore the unknown areas of the Arctic Ocean, strongly inspired by Fridtjof Nansen's earlier expedition with "Fram". The plan was to sail along the coast of Siberia and go into the ice farther to the north and east than Nansen had. In contrast to Amundsen's earlier expeditions, this was expected to yield more material for academic research, and he carried the geophysicist Harald Sverdrup on board.
The voyage was to the northeasterly direction over the Kara Sea. Amundsen planned to freeze the "Maud" into the polar ice cap and drift towards the North Pole (as Nansen had done with the "Fram"), and he did so off Cape Chelyuskin. But, the ice became so thick that the ship was unable to break free, although it was designed for such a journey in heavy ice. In September 1919, the crew got the ship loose from the ice, but it froze again after eleven days somewhere between the New Siberian Islands and Wrangel Island.
During this time, Amundsen participated little in the work outdoors, such as sleigh rides and hunting, because he had suffered numerous injuries. He had a broken arm and had been attacked by polar bears. Hanssen and Wisting, along with two other men, embarked on an expedition by dog sled to Nome, Alaska, more than 1,000 kilometres away. But they found that the ice was not frozen solid in the Bering Strait, and it could not be crossed. They sent a telegram from Anadyr to signal their location.
After two winters frozen in the ice, without having achieved the goal of drifting over the North Pole, Amundsen decided to go to Nome to repair the ship and buy provisions. Several of the crew ashore there, including Hanssen, did not return on time to the ship. Amundsen considered Hanssen to be in breach of contract, and dismissed him from the crew.
During the third winter, "Maud" was frozen in the western Bering Strait. She finally became free and the expedition sailed south, reaching Seattle, Washington in the US Pacific Northwest in 1921 for repairs. Amundsen returned to Norway, needing to put his finances in order. He took with him two young indigenous girls, the adopted four-year-old Kakonita and her companion Camilla. When Amundsen went bankrupt two years later, however, he sent the girls to be cared for by Camilla's father, who lived in eastern Russia.
In June 1922 Amundsen returned to "Maud", which had been sailed to Nome. He decided to shift from the planned naval expedition to aerial ones, and arranged to charter a plane. He divided the expedition team in two: one part was to survive the winter and prepare for an attempt to fly over the pole. This part was led by Amundsen. The second team on "Maud", under the command of Wisting, was to resume the original plan to drift over the North Pole in the ice. The ship drifted in the ice for three years east of the New Siberian Islands, never reaching the North Pole. It was finally seized by Amundsen's creditors as collateral for his mounting debt.
The attempt to fly over the Pole failed, too. Amundsen and Oskar Omdal, of the Royal Norwegian Navy, tried to fly from Wainwright, Alaska, to Spitsbergen across the North Pole. When their aircraft was damaged, they abandoned the journey. To raise additional funds, Amundsen traveled around the United States in 1924 on a lecture tour. 
Although he was unable to reach the North Pole, the scientific results of the expedition, mainly the work of Sverdrup, have proven to be of considerable value. Many of these carefully collected scientific data were lost during the ill-fated journey of Peter Tessem and Paul Knutsen, two crew members sent on a mission by Amundsen. The scientific materials were later retrieved by Russian scientist Nikolay Urvantsev from where they had been abandoned on the shores of the Kara Sea.
Reaching the North Pole.
In 1925 accompanied by Lincoln Ellsworth, pilot Hjalmar Riiser-Larsen, and three other team members, Amundsen took two Dornier Do J flying boats, the N-24 and N-25, to 87° 44′ north. It was the northernmost latitude reached by plane up to that time. The aircraft landed a few miles apart without radio contact, yet the crews managed to reunite. The N-24 was damaged. Amundsen and his crew worked for more than three weeks to clean up an airstrip to take off from ice. They shovelled 600 tons of ice while consuming only one pound (400 g) of daily food rations. In the end, six crew members were packed into the N-25. In a remarkable feat, Riiser-Larsen took off, and they barely became airborne over the cracking ice. They returned triumphant when everyone thought they had been lost forever.
In 1926 Amundsen and 15 other men (including Ellsworth, Riiser-Larsen, Oscar Wisting, and the Italian air crew led by aeronautical engineer Umberto Nobile) made the first crossing of the Arctic in the airship "Norge," designed by Nobile. They left Spitzbergen on 11 May 1926, and they landed in Alaska two days later. The three previous claims to have arrived at the North Pole: Frederick Cook in 1908; Robert Peary in 1909; and Richard E. Byrd in 1926 (just a few days before the "Norge") are all disputed, as being either of dubious accuracy or outright fraud. If their claims are false, the crew of the "Norge" would be the first verified explorers to have reached the North Pole. If the "Norge" expedition was the first to the North Pole, Amundsen and Oscar Wisting were the first men to reach each geographical pole, by ground or by air.
Disappearance and death.
Amundsen disappeared with five crew on 18 June 1928 while flying on a rescue mission in the Arctic. His team included Norwegian pilot Leif Dietrichson, French pilot René Guilbaud, and three more Frenchmen. They were seeking missing members of Nobile's crew, whose new airship "Italia" had crashed while returning from the North Pole. Afterward, a wing-float and bottom gasoline tank from Amundsen's French Latham 47 flying boat, adapted as a replacement wing-float, were found near the Tromsø coast. It is believed that the plane crashed in fog in the Barents Sea, and that Amundsen and his crew were killed in the crash, or died shortly afterward. The search for Amundsen and team was called off in September 1928 by the Norwegian Government and the bodies were never found.
In 2004 and in late August 2009, the Royal Norwegian Navy used the unmanned submarine "Hugin 1000" to search for the wreckage of Amundsen's plane. The searches focused on a 40 sqmi area of the sea floor, and were documented by the German production company ContextTV. They found nothing from the Amundsen flight.
Legacy.
A number of places have been named after Amundsen:
Several ships are named after him:
Other tributes include:
European-Inuit descendant claims.
Some Inuit people in Gjøa Haven with European ancestry have claimed to be descendants of Amundsen (or one of his six crew, whose names have not remained as well known), from the period of their extended winter stay on King Williams Island from 1903 to 1905. Accounts by members of the expedition told of their relations with Inuit women, and historians have speculated that Amundsen might also have taken a partner, although he wrote a warning against this. Specifically, half brothers Bob Konona and Paul Ikuallaq say that their father Luke Ikuallaq (b. 1904) told them on his deathbed that he was the son of Amundsen. Konona said that their father Ikuallaq was left out on the ice to die after his birth, as his European ancestry made him illegitimate to the Inuit, threatening their community. His Inuit grandparents saved him. In 2012, Y-DNA analysis, with the families' permission, showed that Ikuallaq (and his sons) was not a match to the direct male line of Amundsen. Not all descendants claiming European ancestry have been tested for a match to Amundsen, nor has there been a comparison of Ikuallaq's DNA to that of other European members of Amundsen's crew.
Notes.
Notes
Citations
Sources
External links.
Works by Amundsen

</doc>
<doc id="26200" url="http://en.wikipedia.org/wiki?curid=26200" title="Richard Lovelace">
Richard Lovelace

Richard Lovelace (1617–1657) was an English poet in the seventeenth century. He was a cavalier poet who fought on behalf of the king during the Civil War. His best known works are "To Althea, from Prison," and "To Lucasta, Going to the Warres."
Biography.
Early life and family.
Richard Lovelace was born on 9 December 1617. His exact birthplace is unknown, and may have been Woolwich, Kent, or Holland. He was the oldest son of Sir William Lovelace and Anne Barne Lovelace. He had four brothers and three sisters. His father was from a distinguished military and legal family; the Lovelace family owned a considerable amount of property in Kent.
His father, Sir William Lovelace, knt., was a member of the Virginia Company and an incorporator in the second Virginia Company in 1609. He was a soldier and died during the war with Spain and Holland in the Siege of Groenlo (1627) a few days before the town fell. Richard was only 9 years old when his father died.
Richard's father was the son of Sir William Lovelace and Elizabeth Aucher who was the daughter of Mabel Wroths and Edward Aucher, Esq. who inherited, under his father's Will, the manors of Bishopsbourne and Hautsborne. Elizabeth's nephew was Sir Anthony Aucher (1614 – 31 May 1692) an English politician and Cavalier during the English Civil War. He was the son of her brother Sir Anthony Aucher and his wife Hester Collett.
Richard Lovelace's mother, Anne Barne (1587–1633), was the daughter of Sir William Barne and the granddaughter of Sir George Barne III (1532- d. 1593), the Lord Mayor of London and a prominent merchant and public official from London during the reign of Elizabeth I; and Anne Gerrard, daughter of Sir William Garrard, who was Lord Mayor of London in 1555.
Richard Lovelace's mother was also the daughter of Anne Sandys and the granddaughter of Cicely Wilford and the Most Reverend Dr. Edwin Sandys, an Anglican church leader who successively held the posts of the Bishop of Worcester (1559–1570), Bishop of London (1570–1576), and the Archbishop of York (1576–1588). He was one of the translators of the Bishops' Bible.
Anne Barne Lovelace married as her second husband, on 20 January 1630, at Greenwich, England, the Very Rev. Dr. Jonathan Browne They were the parents of one child, Anne Browne, who married Herbert Crofte, S.T.P. and D.D and were the parents of Sir Herbert Croft, 1st Baronet see Croft baronets.
His brother, Francis Lovelace (1621–1675), was the second governor of the New York colony appointed by the Duke of York, later King James II of England. He was also the great nephew of both George Sandys (2 March 1577 – March 1644), an English traveller, colonist and poet; and of Sir Edwin Sandys (9 December 1561 – October 1629), an English statesman and one of the founders of the London Company.
In 1629, when Lovelace was eleven, he went to Sutton’s Foundation at Charterhouse School, then located in London. However, there is not a clear record that Lovelace actually attended because it is believed that he studied as a "boarder" because he did not need financial assistance like the "scholars". He spent five years at Charterhouse, three of which were spent with Richard Crashaw, who also became a poet. On 5 May 1631, Lovelace was sworn in as a "Gentleman Wayter Extraordinary" to the King. This was an "honorary position for which one paid a fee". He then went on to Gloucester Hall, Oxford, in 1634.
Collegiate career.
Richard Lovelace attended the University of Oxford and was praised by one of his contemporaries, Anthony Wood. for being "the most amiable and beautiful person that ever eye beheld; a person also of innate modesty, virtue and courtly deportment, which made him then, but especially after, when he retired to the great city, much admired and adored by the female sex" At the age of eighteen, during a three-week celebration at Oxford, he was granted the degree of Master of Arts. While at school, he tried to portray himself more as a social connoisseur rather than a scholar, continuing his image of being a Cavalier. Being a Cavalier poet, Lovelace wrote to praise a friend or fellow poet, to give advice in grief or love, to define a relationship, to articulate the precise amount of attention a man owes a woman, to celebrate beauty, and to persuade to love. Lovelace wrote a comedy, "The Scholars", while at Oxford. He then left for the University of Cambridge for a few months where he met Lord Goring, who led him into political trouble.
Politics and prison.
Lovelace’s was often influenced by his experiences with politics and association with important figures of his time. At the age of thirteen, Lovelace became a "Gentlemen Wayter Extraordinary" to the King and at nineteen he contributed a verse to a volume of elegies commemorating Princess Katharine. In 1639 Lovelace joined the regiment of Lord Goring, serving first as a senior ensign and later as a captain in the Bishops’ Wars. This experience inspired the "Sonnet. To Generall Goring", the poem "To Lucasta, Going to the Warres" and the tragedy "The Soldier". Upon his return to his home in Kent in 1640, Lovelace served as a country gentleman and a justice of the peace where he encountered firsthand the civil turmoil regarding religion and politics.
In 1641 Lovelace led a group of men to seize and destroy a petition for the abolition of Episcopal rule, which had been signed by fifteen thousand people. The following year he presented the House of Commons with Dering’s pro-Royalist petition which was supposed to have been burned. These actions resulted in Lovelace’s first imprisonment. Shortly thereafter, he was released on bail with the stipulation that he avoid communication with the House of Commons without permission. This prevented Lovelace, who had done everything to prove himself during the Bishops’ Wars, from participating in the first phase of the English Civil War. However, this first experience of imprisonment did result in some good, as it brought him to write one of his finest and most beloved lyrics, "To Althea, from Prison," in which he illustrates his noble and paradoxical nature. Lovelace did everything he could to remain in the king’s favor despite his inability to participate in the war.
Richard Lovelace did his part again during the political chaos of 1648, though it is unclear specifically what his actions were. He did, however, manage to warrant himself another prison sentence; this time for nearly a year. When he was released in April 1649, the king had been executed and Lovelace’s cause seemed lost. As in his previous incarceration, this experience led to creative production—this time in the form of spiritual freedom, as reflected in the release of his first volume of poetry, "Lucasta".
Literature.
Richard Lovelace first started writing while he was a student at Oxford and wrote almost 200 poems from that time until his death. His first work was a drama titled "The Scholars". The play was never published; however, it was performed at college and then in London. In 1640, he wrote a tragedy titled "The Soldier" which was based on his own military experience. When serving in the Bishops' Wars, he wrote the sonnet "To Generall Goring," which is a poem of Bacchanalian celebration rather than a glorification of military action. One of his extremely famous poems is "To Lucasta, Going to the Warres," written in 1640 and exposed in his first political action. During his first imprisonment in 1642, he wrote his most famous poem "To Althea, From Prison." Later on that year during his travels to Holland with General Goring, he wrote "The Rose," following with "The Scrutiny" and on 14 May 1649, "Lucasta" was published. He also wrote poems analyzing the details of many simple insects. "The Ant," 'The Grasse-hopper," "The Snayl," "The Falcon," "The Toad and Spyder." Of these poems, "The Grasse-hopper" is his most well-known. In 1660, after Lovelace died, "Lucasta: Postume Poems" was published; it contains "A Mock-Song," which has a much darker tone than his previous works.
William Winstanley, who praised much of Richard Lovelace's works, thought highly of him and compared him to an idol; "I can compare no Man so like this Colonel Lovelace as Sir Philip Sidney,” of which it is in an Epitaph made of him;
His most quoted excerpts are from the beginning of the last stanza of "To Althea, From Prison":
and the end of "To Lucasta. Going to the Warres":

</doc>
<doc id="26201" url="http://en.wikipedia.org/wiki?curid=26201" title="Reduced instruction set computing">
Reduced instruction set computing

Reduced instruction set computing, or RISC (pronounced 'risk'), is a CPU design strategy based on the insight that a simplified instruction set (as opposed to a complex set) provides higher performance when combined with a microprocessor architecture capable of executing those instructions using fewer microprocessor cycles per instruction. A computer based on this strategy is a "reduced instruction set computer", also called "RISC". The opposing architecture is called complex instruction set computing, i.e. CISC.
Various suggestions have been made regarding a precise definition of RISC, but the general concept is that of a system that uses a small, highly optimized set of instructions, rather than a more versatile set of instructions often found in other types of architectures. Another common trait is that RISC systems use the load/store architecture, where memory is normally accessed only through specific instructions, rather than accessed as part of other instructions like codice_1.
Although a number of systems from the 1960s and 70s have been identified as being forerunners of RISC, the modern version of the design dates to the 1980s. In particular, two projects at Stanford University and University of California, Berkeley are most associated with the popularization of this concept. Stanford's design would go on to be commercialized as the successful MIPS architecture, while Berkeley's RISC gave its name to the entire concept, commercialized as the SPARC. Another success from this era were IBM's efforts that eventually led to the Power Architecture. As these projects matured, a wide variety of similar designs flourished in the late 1980s and especially the early 1990s, representing a major force in the Unix workstation market as well as embedded processors in laser printers, routers and similar products.
Well-known RISC families include DEC Alpha, AMD Am29000, ARC, ARM, Atmel AVR, Blackfin, Intel i860 and i960, MIPS, Motorola 88000, PA-RISC, Power (including PowerPC), RISC-V, SuperH, and SPARC. In the 21st century, the use of ARM architecture processors in smart phones and tablet computers such as the iPad, Android, and Windows RT tablets provided a wide user base for RISC-based systems. RISC processors are also used in supercomputers such as the K computer, the fastest on the TOP500 list in 2011, second at the 2012 list, and fourth at the 2013 list, and Sequoia, the fastest in 2012 and third in the 2013 list.
History and development.
A number of systems, going back to the 1970s (and even 1960s) have been credited as the first RISC architecture, partly based on their use of load/store approach. The term RISC was coined by David Patterson of the Berkeley RISC project, although somewhat similar concepts had appeared before.
The CDC 6600 designed by Seymour Cray in 1964 used a load/store architecture with only two addressing modes (register+register, and register+immediate constant) and 74 opcodes, with the basic clock cycle/instruction issue rate being 10 times faster than the memory access time. Partly due to the optimized load/store architecture of the CDC 6600 Jack Dongarra states that it can be considered as a forerunner of modern RISC systems, although a number of other technical barriers needed to be overcome for the development of a modern RISC system.
Michael J. Flynn views the first RISC system as the IBM 801 design which began in 1975 by John Cocke, and completed in 1980. The 801 was eventually produced in a single-chip form as the ROMP in 1981, which stood for 'Research OPD [Office Products Division] Micro Processor'. As the name implies, this CPU was designed for "mini" tasks, and was also used in the IBM RT-PC in 1986, which turned out to be a commercial failure. But the 801 inspired several research projects, including new ones at IBM that would eventually lead to the IBM POWER instruction set architecture.
The most public RISC designs, however, were the results of university research programs run with funding from the DARPA VLSI Program. The VLSI Program, practically unknown today, led to a huge number of advances in chip design, fabrication, and even computer graphics. The Berkeley RISC project started in 1980 under the direction of David Patterson and Carlo H. Sequin.
Berkeley RISC was based on gaining performance through the use of pipelining and an aggressive use of a technique known as register windowing. In a traditional CPU, one has a small number of registers, and a program can use any register at any time. In a CPU with register windows, there are a huge number of registers, e.g. 128, but programs can only use a small number of them, e.g. eight, at any one time. A program that limits itself to eight registers per procedure can make very fast procedure calls: The call simply moves the window "down" by eight, to the set of eight registers used by that procedure, and the return moves the window back. The Berkeley RISC project delivered the RISC-I processor in 1982. Consisting of only 44,420 transistors (compared with averages of about 100,000 in newer CISC designs of the era) RISC-I had only 32 instructions, and yet completely outperformed any other single-chip design. They followed this up with the 40,760 transistor, 39 instruction RISC-II in 1983, which ran over three times as fast as RISC-I.
The MIPS architecture grew out of a graduate course by John L. Hennessy at Stanford University in 1981, resulted in a functioning system in 1983, and could run simple programs by 1984. The MIPS approach emphasized an aggressive clock cycle and the use of the pipeline, making sure it could be run as "full" as possible. The MIPS system was followed by the MIPS-X and in 1984 Hennessy and his colleagues formed MIPS Computer Systems. The commercial venture resulted in the R2000 microprocessor in 1985, and was followed by the R3000 in 1988.
In the early 1980s, significant uncertainties surrounded the RISC concept, and it was uncertain if it could have a commercial future, but by the mid-1980s the concepts had matured enough to be seen as commercially viable. In 1986 Hewlett Packard started using an early implementation of their PA-RISC in some of their computers. In the meantime, the Berkeley RISC effort had become so well known that it eventually became the name for the entire concept and in 1987 Sun Microsystems began shipping systems with the SPARC processor, directly based on the Berkeley RISC-II system.
The US government Committee on Innovations in Computing and Communications credits the acceptance of the viability of the RISC concept to the success of the SPARC system. The success of SPARC renewed interest within IBM, which released new RISC systems by 1990 and by 1995 RISC processors were the foundation of a $15 billion server industry.
Since 2010 a new open source, ISA, RISC-V, is under development at the University of California, Berkeley, for research purposes and as a free alternative to proprietary ISA's . As of 2014 version 2 of the userspace ISA is fixed. The ISA is designed to be extensible from a barebones core sufficient for a small embedded processor to supercomputer and cloud computing use with standard and chip designer defined extensions and coprocessors. It has been tested in silicon design with the ROCKET SoC which is also available as an open source processor generator in the CHISEL language.
Characteristics and design philosophy.
Instruction set.
A common misunderstanding of the phrase "reduced instruction set computer" is the mistaken idea that instructions are simply eliminated, resulting in a smaller set of instructions.
In fact, over the years, RISC instruction sets have grown in size, and today many of them have a larger set of instructions than many CISC CPUs. Some RISC processors such as the PowerPC have instruction sets as large as the CISC IBM System/370, for example; conversely, the DEC PDP-8—clearly a CISC CPU because many of its instructions involve multiple memory accesses—has only 8 basic instructions and a few extended instructions.
The term "reduced" in that phrase was intended to describe the fact that the amount of work any single instruction accomplishes is reduced—at most a single data memory cycle—compared to the "complex instructions" of CISC CPUs that may require dozens of data memory cycles in order to execute a single instruction. In particular, RISC processors typically have separate instructions for I/O and data processing.
Hardware utilization.
For any given level of general performance, a RISC chip will typically have far fewer transistors dedicated to the core logic which originally allowed designers to increase the size of the register set and increase internal parallelism.
Other features that are typically found in RISC architectures are:
Exceptions abound, of course, within both CISC and RISC.
RISC designs are also more likely to feature a Harvard memory model, where the instruction stream and the data stream are conceptually separated; this means that modifying the memory where code is held might not have any effect on the instructions executed by the processor (because the CPU has a separate instruction and data cache), at least until a special synchronization instruction is issued. On the upside, this allows both caches to be accessed simultaneously, which can often improve performance.
Many early RISC designs also shared the characteristic of having a branch delay slot. A branch delay slot is an instruction space immediately following a jump or branch. The instruction in this space is executed, whether or not the branch is taken (in other words the effect of the branch is delayed). This instruction keeps the ALU of the CPU busy for the extra time normally needed to perform a branch. Nowadays the branch delay slot is considered an unfortunate side effect of a particular strategy for implementing some RISC designs, and modern RISC designs generally do away with it (such as PowerPC and more recent versions of SPARC and MIPS).
Some aspects attributed to the first RISC-"labeled" designs around 1975 include the observations that the memory-restricted compilers of the time were often unable to take advantage of features intended to facilitate "manual" assembly coding, and that complex addressing modes take many cycles to perform due to the required additional memory accesses. It was argued that such functions would be better performed by sequences of simpler instructions if this could yield implementations small enough to leave room for many registers, reducing the number of slow memory accesses. In these simple designs, most instructions are of uniform length and similar structure, arithmetic operations are restricted to CPU registers and only separate "load" and "store" instructions access memory. These properties enable a better balancing of pipeline stages than before, making RISC pipelines significantly more efficient and allowing higher clock frequencies.
In the early days of the computer industry, programming was done in assembly language or machine code, which encouraged powerful and easy-to-use instructions. CPU designers therefore tried to make instructions that would do as much work as feasible. With the advent of higher level languages, computer architects also started to create dedicated instructions to directly implement certain central mechanisms of such languages. Another general goal was to provide every possible addressing mode for every instruction, known as orthogonality, to ease compiler implementation. Arithmetic operations could therefore often have results as well as operands directly in memory (in addition to register or immediate).
The attitude at the time was that hardware design was more mature than compiler design so this was in itself also a reason to implement parts of the functionality in hardware or microcode rather than in a memory constrained compiler (or its generated code) alone. After the advent of RISC, this philosophy became retroactively known as complex instruction set computing, or CISC.
CPUs also had relatively few registers, for several reasons:
An important force encouraging complexity was very limited main memories (on the order of kilobytes). It was therefore advantageous for the code density—the density of information held in computer programs—to be high, leading to features such as highly encoded, variable length instructions, doing data loading as well as calculation (as mentioned above). These issues were of higher priority than the ease of decoding such instructions.
An equally important reason was that main memories were quite slow (a common type was ferrite core memory); by using dense information packing, one could reduce the frequency with which the CPU had to access this slow resource. Modern computers face similar limiting factors: main memories are slow compared to the CPU and the fast cache memories employed to overcome this are limited in size. This may partly explain why highly encoded instruction sets have proven to be as useful as RISC designs in modern computers.
RISC was developed as an alternative to what is now known as CISC. Over the years, other strategies have been implemented as alternatives to RISC and CISC. Some examples are VLIW, MISC, OISC, massive parallel processing, systolic array, reconfigurable computing, and dataflow architecture.
In the mid-1970s, researchers (particularly John Cocke) at IBM (and similar projects elsewhere) demonstrated that the majority of combinations of these orthogonal addressing modes and instructions were not used by most programs generated by compilers available at the time. It proved difficult in many cases to write a compiler with more than limited ability to take advantage of the features provided by conventional CPUs.
It was also discovered that, on microcoded implementations of certain architectures, complex operations tended to be "slower" than a sequence of simpler operations doing the same thing. This was in part an effect of the fact that many designs were rushed, with little time to optimize or tune every instruction, but only those used most often. One infamous example was the VAX's codice_2 instruction.
As mentioned elsewhere, core memory had long since been slower than many CPU designs. The advent of semiconductor memory reduced this difference, but it was still apparent that more registers (and later caches) would allow higher CPU operating frequencies. Additional registers would require sizeable chip or board areas which, at the time (1975), could be made available if the complexity of the CPU logic was reduced.
Yet another impetus of both RISC and other designs came from practical measurements on real-world programs. Andrew Tanenbaum summed up many of these, demonstrating that processors often had oversized immediates. For instance, he showed that 98% of all the constants in a program would fit in 13 bits, yet many CPU designs dedicated 16 or 32 bits to store them. This suggests that, to reduce the number of memory accesses, a fixed length machine could store constants in unused bits of the instruction word itself, so that they would be immediately ready when the CPU needs them (much like immediate addressing in a conventional design). This required small opcodes in order to leave room for a reasonably sized constant in a 32-bit instruction word.
Since many real-world programs spend most of their time executing simple operations, some researchers decided to focus on making those operations as fast as possible. The clock rate of a CPU is limited by the time it takes to execute the slowest "sub-operation" of any instruction; decreasing that cycle-time often accelerates the execution of other instructions. The focus on "reduced instructions" led to the resulting machine being called a "reduced instruction set computer" (RISC). The goal was to make instructions so simple that they could "easily" be pipelined, in order to achieve a "single clock" throughput at "high frequencies".
Later, it was noted that one of the most significant characteristics of RISC processors was that external memory was only accessible by a "load" or "store" instruction. All other instructions were limited to internal registers. This simplified many aspects of processor design: allowing instructions to be fixed-length, simplifying pipelines, and isolating the logic for dealing with the delay in completing a memory access (cache miss, etc.) to only two instructions. This led to RISC designs being referred to as "load/store" architectures.
One more issue is that some complex instructions are difficult to restart, e.g. following a page fault. In some cases, restarting from the beginning will work (although wasteful), but in many cases this would give incorrect results. Therefore the machine needs to have some hidden state to remember which parts went through and what remains to be done. With a load/store machine, the program counter is sufficient to describe the state of the machine.
The main distinguishing feature of RISC is that the instruction set is optimized for a highly regular instruction pipeline flow.
All the other features associated with RISC—branch delay slots, separate instruction and data caches, load/store architecture, large register set, etc.—may seem to be a random assortment of unrelated features,
but each of them is helpful in maintaining a regular pipeline flow that completes an instruction every clock cycle.
Comparison to other architectures.
Some CPUs have been specifically designed to have a very small set of instructions – but these designs are very different from classic RISC designs, so they have been given other names such as minimal instruction set computer (MISC), or transport triggered architecture (TTA), etc.
Despite many successes, RISC has made few inroads into the desktop PC and commodity server markets, where Intel's x86 platform remains the dominant processor architecture. There are three main reasons for this:
Outside of the desktop arena, however, the ARM architecture (RISC and born at about the same time as SPARC) has to a degree broken the Intel stranglehold with its widespread use in smartphones, tablets and many forms of embedded device. It is also the case that since the Pentium Pro (P6) Intel has been using an internal RISC processor core for its processors.
While early RISC designs differed significantly from contemporary CISC designs, by 2000 the highest performing CPUs in the RISC line were almost indistinguishable from the highest performing CPUs in the CISC line.
RISC: from cell phones to supercomputers.
RISC architectures are now used across a wide range of platforms, from cellular telephones and tablet computers to some of the world's fastest supercomputers such as the K computer, the fastest on the TOP500 list in 2011.
Low end and mobile systems.
By the beginning of the 21st century, the majority of low end and mobile systems relied on RISC architectures. Examples include:

</doc>
<doc id="26202" url="http://en.wikipedia.org/wiki?curid=26202" title="Ralph Waldo Emerson">
Ralph Waldo Emerson

Ralph Waldo Emerson (May 25, 1803 – April 27, 1882) was an American essayist, lecturer, and poet, who led the Transcendentalist movement of the mid-19th century. He was seen as a champion of individualism and a prescient critic of the countervailing pressures of society, and he disseminated his thoughts through dozens of published essays and more than 1,500 public lectures across the United States.
Emerson gradually moved away from the religious and social beliefs of his contemporaries, formulating and expressing the philosophy of Transcendentalism in his 1836 essay, "Nature". Following this ground-breaking work, he gave a speech entitled "The American Scholar" in 1837, which Oliver Wendell Holmes, Sr. considered to be America's "Intellectual Declaration of Independence". 
Emerson wrote most of his important essays as lectures first, then revised them for print. His first two collections of essays – ' and ', published respectively in 1841 and 1844 – represent the core of his thinking, and include such well-known essays as "Self-Reliance", "The Over-Soul", "Circles", "The Poet" and "Experience". Together with "Nature", these essays made the decade from the mid-1830s to the mid-1840s Emerson's most fertile period.
Emerson wrote on a number of subjects, never espousing fixed philosophical tenets, but developing certain ideas such as individuality, freedom, the ability for humankind to realize almost anything, and the relationship between the soul and the surrounding world. Emerson's "nature" was more philosophical than naturalistic: "Philosophically considered, the universe is composed of Nature and the Soul." Emerson is one of several figures who "took a more pantheist or pandeist approach by rejecting views of God as separate from the world."
He remains among the linchpins of the American romantic movement, and his work has greatly influenced the thinkers, writers and poets that have followed him. When asked to sum up his work, he said his central doctrine was "the infinitude of the private man." Emerson is also well known as a mentor and friend of fellow Transcendentalist Henry David Thoreau.
Early life, family, and education.
Emerson was born in Boston, Massachusetts, on May 25, 1803, son of Ruth Haskins and the Rev. William Emerson, a Unitarian minister. He was named after his mother's brother Ralph and the father's great-grandmother Rebecca Waldo. Ralph Waldo was the second of five sons who survived into adulthood; the others were William, Edward, Robert Bulkeley, and Charles. Three other children—Phebe, John Clarke, and Mary Caroline–died in childhood.
The young Ralph Waldo Emerson's father died from stomach cancer on May 12, 1811, less than two weeks before Emerson's eighth birthday. Emerson was raised by his mother, with the help of the other women in the family; his aunt Mary Moody Emerson in particular had a profound effect on Emerson. She lived with the family off and on, and maintained a constant correspondence with Emerson until her death in 1863.
Emerson's formal schooling began at the Boston Latin School in 1812 when he was nine. In October 1817, at 14, Emerson went to Harvard College and was appointed freshman messenger for the president, requiring Emerson to fetch delinquent students and send messages to faculty. Midway through his junior year, Emerson began keeping a list of books he had read and started a journal in a series of notebooks that would be called "Wide World". He took outside jobs to cover his school expenses, including as a waiter for the Junior Commons and as an occasional teacher working with his uncle Samuel in Waltham, Massachusetts. By his senior year, Emerson decided to go by his middle name, Waldo. Emerson served as Class Poet; as was custom, he presented an original poem on Harvard's Class Day, a month before his official graduation on August 29, 1821, when he was 18. He did not stand out as a student and graduated in the exact middle of his class of 59 people.
In 1826, faced with poor health, Emerson went to seek out warmer climates. He first went to Charleston, South Carolina, but found the weather was still too cold. He then went further south, to St. Augustine, Florida, where he took long walks on the beach, and began writing poetry. While in St. Augustine, he made the acquaintance of Prince Achille Murat. Murat, the nephew of Napoleon Bonaparte, was only two years his senior; they became extremely good friends and enjoyed one another's company. The two engaged in enlightening discussions on religion, society, philosophy, and government, and Emerson considered Murat an important figure in his intellectual education.
While in St. Augustine, Emerson had his first experience of slavery. At one point, he attended a meeting of the Bible Society while there was a slave auction taking place in the yard outside. He wrote, "One ear therefore heard the glad tidings of great joy, whilst the other was regaled with 'Going, gentlemen, going'!"
Early career.
After Harvard, Emerson assisted his brother William in a school for young women established in their mother's house, after he had established his own school in Chelmsford, Massachusetts; when his brother William went to Göttingen to study divinity, Emerson took charge of the school. Over the next several years, Emerson made his living as a schoolmaster, then went to Harvard Divinity School. Emerson's brother Edward, two years younger than he, entered the office of lawyer Daniel Webster, after graduating Harvard first in his class. Edward's physical health began to deteriorate and he soon suffered a mental collapse as well; he was taken to McLean Asylum in June 1828 at age 23. Although he recovered his mental equilibrium, he died in 1834 from apparently longstanding tuberculosis. Another of Emerson's bright and promising younger brothers, Charles, born in 1808, died in 1836, also of tuberculosis, making him the third young person in Emerson's innermost circle to die in a period of a few years.
Emerson met his first wife, Ellen Louisa Tucker, in Concord, New Hampshire on Christmas Day, 1827, and married her when she was 18. The couple moved to Boston, with Emerson's mother Ruth moving with them to help take care of Ellen, who was already sick with tuberculosis. Less than two years later, Ellen died at the age of 20 on February 8, 1831, after uttering her last words: "I have not forgotten the peace and joy." Emerson was heavily affected by her death and visited her grave in Roxbury daily. In a journal entry dated March 29, 1832, Emerson wrote, "I visited Ellen's tomb & opened the coffin."
Boston's Second Church invited Emerson to serve as its junior pastor and he was ordained on January 11, 1829. His initial salary was $1,200 a year, increasing to $1,400 in July, but with his church role he took on other responsibilities: he was chaplain to the Massachusetts legislature, and a member of the Boston school committee. His church activities kept him busy, though during this period, facing the imminent death of his wife, he began to doubt his own beliefs.
After his wife's death, he began to disagree with the church's methods, writing in his journal in June 1832: "I have sometimes thought that, in order to be a good minister, it was necessary to leave the ministry. The profession is antiquated. In an altered age, we worship in the dead forms of our forefathers." His disagreements with church officials over the administration of the Communion service and misgivings about public prayer eventually led to his resignation in 1832. As he wrote, "This mode of commemorating Christ is not suitable to me. That is reason enough why I should abandon it." As one Emerson scholar has pointed out, "Doffing the decent black of the pastor, he was free to choose the gown of the lecturer and teacher, of the thinker not confined within the limits of an institution or a tradition."
Emerson toured Europe in 1833 and later wrote of his travels in "English Traits" (1856). He left aboard the brig "Jasper" on Christmas Day, 1832, sailing first to Malta. During his European trip, he spent several months in Italy, visiting Rome, Florence and Venice, among other cities. When in Rome, he met with John Stuart Mill, who gave him a letter of recommendation to meet Thomas Carlyle. He went to Switzerland, and had to be dragged by fellow passengers to visit Voltaire's home in Ferney, "protesting all the way upon the unworthiness of his memory." He then went on to Paris, a "loud modern New York of a place," where he visited the Jardin des Plantes. He was greatly moved by the organization of plants according to Jussieu's system of classification, and the way all such objects were related and connected. As Richardson says, "Emerson's moment of insight into the interconnectedness of things in the Jardin des Plantes was a moment of almost visionary intensity that pointed him away from theology and toward science."
Moving north to England, Emerson met William Wordsworth, Samuel Taylor Coleridge, and Thomas Carlyle. Carlyle in particular was a strong influence on Emerson; Emerson would later serve as an unofficial literary agent in the United States for Carlyle, and in March 1835, he tried to convince Carlyle to come to America to lecture. The two would maintain correspondence until Carlyle's death in 1881.
Emerson returned to the United States on October 9, 1833, and lived with his mother in Newton, Massachusetts, until October, 1834, when he moved to Concord, Massachusetts, to live with his step-grandfather Dr. Ezra Ripley at what was later named The Old Manse. Seeing the budding Lyceum movement, which provided lectures on all sorts of topics, Emerson saw a possible career as a lecturer. On November 5, 1833, he made the first of what would eventually be some 1,500 lectures, discussing The Uses of Natural History in Boston. This was an expanded account of his experience in Paris. In this lecture, he set out some of his important beliefs and the ideas he would later develop in his first published essay Nature:
Nature is a language and every new fact one learns is a new word; but it is not a language taken to pieces and dead in the dictionary, but the language put together into a most significant and universal sense. I wish to learn this language, not that I may know a new grammar, but that I may read the great book that is written in that tongue.
On January 24, 1835, Emerson wrote a letter to Lydia Jackson proposing marriage. Her acceptance reached him by mail on the 28th. In July 1835, he bought a house on the Cambridge and Concord Turnpike in Concord, Massachusetts which he named "Bush"; it is now open to the public as the Ralph Waldo Emerson House. Emerson quickly became one of the leading citizens in the town. He gave a lecture to commemorate the 200th anniversary of the town of Concord on September 12, 1835. Two days later, he married Lydia Jackson in her home town of Plymouth, Massachusetts, and moved to the new home in Concord together with Emerson's mother on September 15.
Emerson quickly changed his wife's name to Lidian, and would call her Queenie, and sometimes Asia, and she called him Mr. Emerson. Their children were Waldo, Ellen, Edith, and Edward Waldo Emerson. Edward Waldo Emerson was the father of Raymond Emerson. Ellen was named for his first wife, at Lidian's suggestion.
Emerson was poor when he was at Harvard, and later supported his family for much of his life. He inherited a fair amount of money after his first wife's death, though he had to file a lawsuit against the Tucker family in 1836 to get it. He received $11,600 in May 1834, and a further $11,674.49 in July 1837. In 1834, he considered that he had an income of $1,200 a year from the initial payment of the estate, equivalent to what he had earned as a pastor.
Literary career and Transcendentalism.
On September 8, 1836, the day before the publication of "Nature", Emerson met with Frederic Henry Hedge, George Putnam and George Ripley to plan periodic gatherings of other like-minded intellectuals. This was the beginning of the Transcendental Club, which served as a center for the movement. Its first official meeting was held on September 19, 1836. On September 1, 1837, women attended a meeting of the Transcendental Club for the first time. Emerson invited Margaret Fuller, Elizabeth Hoar and Sarah Ripley for dinner at his home before the meeting to ensure that they would be present for the evening get-together. Fuller would prove to be an important figure in Transcendentalism.
Emerson anonymously published his first essay, "Nature", on September 9, 1836. A year later, on August 31, 1837, Emerson delivered his now-famous Phi Beta Kappa address, "The American Scholar", then known as "An Oration, Delivered before the Phi Beta Kappa Society at Cambridge"; it was renamed for a collection of essays (which included the first general publication of "Nature") in 1849. Friends urged him to publish the talk, and he did so, at his own expense, in an edition of 500 copies, which sold out in a month. In the speech, Emerson declared literary independence in the United States and urged Americans to create a writing style all their own and free from Europe. James Russell Lowell, who was a student at Harvard at the time, called it "an event without former parallel on our literary annals". Another member of the audience, Reverend John Pierce, called it "an apparently incoherent and unintelligible address".
In 1837, Emerson befriended Henry David Thoreau. Though they had likely met as early as 1835, in the fall of 1837, Emerson asked Thoreau, "Do you keep a journal?" The question went on to be a lifelong inspiration for Thoreau. Emerson's own journal comes to 16 large volumes, in the definitive Harvard University Press edition published between 1960 and 1982. Some scholars consider the journal to be Emerson's key literary work.
In March 1837, Emerson gave a series of lectures on The Philosophy of History at Boston's Masonic Temple. This was the first time he managed a lecture series on his own, and was the beginning of his serious career as a lecturer. The profits from this series of lectures were much larger than when he was paid by an organization to talk, and Emerson continued to manage his own lectures often throughout his lifetime. He would eventually give as many as 80 lectures a year, traveling across the northern part of the United States. He traveled as far as St. Louis, Des Moines, Minneapolis, and California.
On July 15, 1838, Emerson was invited to Divinity Hall, Harvard Divinity School for the school's graduation address, which came to be known as his "Divinity School Address". Emerson discounted Biblical miracles and proclaimed that, while Jesus was a great man, he was not God: historical Christianity, he said, had turned Jesus into a "demigod, as the Orientals or the Greeks would describe Osiris or Apollo". His comments outraged the establishment and the general Protestant community. For this, he was denounced as an atheist, and a poisoner of young men's minds. Despite the roar of critics, he made no reply, leaving others to put forward a defense. He was not invited back to speak at Harvard for another thirty years.
The Transcendental group began to publish its flagship journal, "The Dial", in July 1840. They planned the journal as early as October 1839, but work did not begin until the first week of 1840. George Ripley was its managing editor and Margaret Fuller was its first editor, having been hand-chosen by Emerson after several others had declined the role. Fuller stayed on for about two years and Emerson took over, utilizing the journal to promote talented young writers including Ellery Channing and Thoreau.
It was in 1841 that Emerson published "Essays", his second book, which included the famous essay, "Self-Reliance". His aunt called it a "strange medley of atheism and false independence", but it gained favorable reviews in London and Paris. This book, and its popular reception, more than any of Emerson's contributions to date laid the groundwork for his international fame.
In January 1842 Emerson's first son Waldo died from scarlet fever. Emerson wrote of his grief in the poem "Threnody" ("For this losing is true dying"), and the essay "Experience". That same month, William James was born, and Emerson agreed to be his godfather.
Bronson Alcott announced his plans in November 1842 to find "a farm of a hundred acres in excellent condition with good buildings, a good orchard and grounds". Charles Lane purchased a 90 acre farm in Harvard, Massachusetts, in May 1843 for what would become Fruitlands, a community based on Utopian ideals inspired in part by Transcendentalism. The farm would run based on a communal effort, using no animals for labor; its participants would eat no meat and use no wool or leather. Emerson said he felt "sad at heart" for not engaging in the experiment himself. Even so, he did not feel Fruitlands would be a success. "Their whole doctrine is spiritual", he wrote, "but they always end with saying, Give us much land and money". Even Alcott admitted he was not prepared for the difficulty in operating Fruitlands. "None of us were prepared to actualize practically the ideal life of which we dreamed. So we fell apart", he wrote. After its failure, Emerson helped buy a farm for Alcott's family in Concord which Alcott named "Hillside".
"The Dial" ceased publication in April 1844; Horace Greeley reported it as an end to the "most original and thoughtful periodical ever published in this country". (An unrelated magazine of the same name would be published in several periods through 1929.)
In 1844, Emerson published his second collection of essays, entitled "Essays: Second Series." This collection included "The Poet," "Experience," "Gifts," and an essay entitled "Nature," a different work from the 1836 essay of the same name.
Emerson made a living as a popular lecturer in New England and much of the rest of the country. He had begun lecturing in 1833; by the 1850s he was giving as many as 80 per year. He addressed the Boston Society for the Diffusion of Useful Knowledge and the Gloucester Lyceum, among others. Emerson spoke on a wide variety of subjects and many of his essays grew out of his lectures. He charged between $10 and $50 for each appearance, bringing him as much as $2,000 in a typical winter "season". This was more than his earnings from other sources. In some years, he earned as much as $900 for a series of six lectures, and in another, for a winter series of talks in Boston, he netted $1,600. He eventually gave some 1,500 lectures in his lifetime. His earnings allowed him to expand his property, buying 11 acre of land by Walden Pond and a few more acres in a neighboring pine grove. He wrote that he was "landlord and waterlord of 14 acres, more or less".
Emerson was introduced to Indian philosophy when reading the works of French philosopher Victor Cousin. In 1845, Emerson's journals show he was reading the "Bhagavad Gita" and Henry Thomas Colebrooke's "Essays on the Vedas." Emerson was strongly influenced by Vedanta, and much of his writing has strong shades of nondualism. One of the clearest examples of this can be found in his essay "The Over-soul":
We live in succession, in division, in parts, in particles. Meantime within man is the soul of the whole; the wise silence; the universal beauty, to which every part and particle is equally related, the eternal ONE. And this deep power in which we exist and whose beatitude is all accessible to us, is not only self-sufficing and perfect in every hour, but the act of seeing and the thing seen, the seer and the spectacle, the subject and the object, are one. We see the world piece by piece, as the sun, the moon, the animal, the tree; but the whole, of which these are shining parts, is the soul.
From 1847 to 1848, he toured England, Scotland, and Ireland. He also visited Paris between the French Revolution and the bloody June Days. When he arrived, he saw the stumps where trees had been cut down to form barricades in the February riots. On May 21 he stood on the Champ de Mars in the midst of mass celebrations for concord, peace and labor. He wrote in his journal: "At the end of the year we shall take account, & see if the Revolution was worth the trees." The trip left an important imprint on Emerson's later work. His 1856 book "English Traits" is based largely on observations recorded in his travel journals and notebooks. Emerson later came to see the American Civil War as a 'revolution' that shared common ground with the European revolutions of 1848.
In February 1852 Emerson and James Freeman Clarke and William Henry Channing edited an edition of the works and letters of Margaret Fuller, who had died in 1850. Within a week of her death, her New York editor Horace Greeley suggested to Emerson that a biography of Fuller, to be called "Margaret and Her Friends", be prepared quickly "before the interest excited by her sad decease has passed away". Published with the title "The Memoirs of Margaret Fuller Ossoli", Fuller's words were heavily censored or rewritten. The three editors were not concerned about accuracy; they believed public interest in Fuller was temporary and that she would not survive as a historical figure. Even so, for a time, it was the best-selling biography of the decade and went through thirteen editions before the end of the century.
Walt Whitman published the innovative poetry collection "Leaves of Grass" in 1855 and sent a copy to Emerson for his opinion. Emerson responded positively, sending a flattering five-page letter as a response. Emerson's approval helped the first edition of "Leaves of Grass" stir up significant interest and convinced Whitman to issue a second edition shortly thereafter. This edition quoted a phrase from Emerson's letter, printed in gold leaf on the cover: "I Greet You at the Beginning of a Great Career". Emerson took offense that this letter was made public and later became more critical of the work.
Civil War years.
Emerson was staunchly anti-slavery, but he did not appreciate being in the public limelight and was hesitant about lecturing on the subject. He did, however, give a number of lectures during the pre-Civil War years, beginning as early as November, 1837. A number of his friends and family members were more active abolitionists than he, at first, but from 1844 on, he took a more active role in opposing slavery. He gave a number of speeches and lectures, and notably welcomed John Brown to his home during Brown's visits to Concord. He voted for Abraham Lincoln in 1860, but Emerson was disappointed that Lincoln was more concerned about preserving the Union than eliminating slavery outright. Once the American Civil War broke out, Emerson made it clear that he believed in immediate emancipation of the slaves.
Around this time, in 1860, Emerson published "The Conduct of Life", his seventh collection of essays. In this book, Emerson "grappled with some of the thorniest issues of the moment," and "his experience in the abolition ranks is a telling influence in his conclusions." These essays also find Emerson strongly embracing the idea of war as a means of national rebirth: "Civil war, national bankruptcy, or revolution, [are] more rich in the central tones than languid years of prosperity," Emerson writes.
Emerson visited Washington, D.C, at the end of January 1862. He gave a public lecture at the Smithsonian on January 31, 1862, and declared: "The South calls slavery an institution... I call it destitution... Emancipation is the demand of civilization". The next day, February 1, his friend Charles Sumner took him to meet Lincoln at the White House. Lincoln was familiar with Emerson's work, having previously seen him lecture. Emerson's misgivings about Lincoln began to soften after this meeting. In 1865, he spoke at a memorial service held for Lincoln in Concord: "Old as history is, and manifold as are its tragedies, I doubt if any death has caused so much pain as this has caused, or will have caused, on its announcement." Emerson also met a number of high-ranking government officials, including Salmon P. Chase, the secretary of the treasury, Edward Bates, the attorney general, Edwin M. Stanton, the secretary of war, Gideon Welles, the secretary of the navy, and William Seward, the secretary of state.
On May 6, 1862, Emerson's protégé Henry David Thoreau died of tuberculosis at the age of 44 and Emerson delivered his eulogy. Emerson would continuously refer to Thoreau as his best friend, despite a falling out that began in 1849 after Thoreau published "A Week on the Concord and Merrimack Rivers". Another friend, Nathaniel Hawthorne, died two years after Thoreau in 1864. Emerson served as one of the pallbearers as Hawthorne was buried in Concord, as Emerson wrote, "in a pomp of sunshine and verdure". He was elected a Fellow of the American Academy of Arts and Sciences in 1864.
Final years and death.
Starting in 1867, Emerson's health began declining; he wrote much less in his journals. Beginning as early as the summer of 1871 or in the spring of 1872, Emerson started having memory problems and suffered from aphasia. By the end of the decade, he forgot his own name at times and, when anyone asked how he felt, he responded, "Quite well; I have lost my mental faculties, but am perfectly well".
In the spring of 1871 Emerson took a trip on the transcontinental railroad, barely two years after its completion. Along the way and in California he met a number of dignitaries, including Brigham Young during a stopover in Salt Lake City. Part of his California visit included a trip to Yosemite, and while there he met a young and unknown John Muir, a signature event in Muir's career.
Emerson's Concord home caught fire on July 24, 1872; Emerson called for help from neighbors and, giving up on putting out the flames, all attempted to save as many objects as possible. The fire was put out by Ephraim Bull, Jr., the one-armed son of Ephraim Wales Bull. Donations were collected by friends to help the Emersons rebuild, including $5,000 gathered by Francis Cabot Lowell, another $10,000 collected by LeBaron Russell Briggs, and a personal donation of $1,000 from George Bancroft. Support for shelter was offered as well; though the Emersons ended up staying with family at the Old Manse, invitations came from Anne Lynch Botta, James Elliot Cabot, James Thomas Fields and Annie Adams Fields. The fire marked an end to Emerson's serious lecturing career; from then on, he would lecture only on special occasions and only in front of familiar audiences.
While the house was being rebuilt, Emerson took a trip to England, continental Europe, and Egypt. He left on October 23, 1872, along with his daughter Ellen while his wife Lidian spent time at the Old Manse and with friends. Emerson and his daughter Ellen returned to the United States on the ship "Olympus" along with friend Charles Eliot Norton on April 15, 1873. Emerson's return to Concord was celebrated by the town and school was canceled that day.
In late 1874 Emerson published an anthology of poetry called "Parnassus", which included poems by Anna Laetitia Barbauld, Julia Caroline Dorr, Jean Ingelow, Lucy Larcom, Jones Very, as well as Thoreau and several others. The anthology was originally prepared as early as the fall of 1871 but was delayed when the publishers asked for revisions.
The problems with his memory had become embarrassing to Emerson and he ceased his public appearances by 1879. As Holmes wrote, "Emerson is afraid to trust himself in society much, on account of the failure of his memory and the great difficulty he finds in getting the words he wants. It is painful to witness his embarrassment at times".
On April 21, 1882, Emerson was found to be suffering from pneumonia. He died on April 27, 1882. Emerson is buried in Sleepy Hollow Cemetery, Concord, Massachusetts. He was placed in his coffin wearing a white robe given by American sculptor Daniel Chester French.
Lifestyle and beliefs.
Emerson's religious views were often considered radical at the time. He believed that all things are connected to God and, therefore, all things are divine. Critics believed that Emerson was removing the central God figure; as Henry Ware, Jr. said, Emerson was in danger of taking away "the Father of the Universe" and leaving "but a company of children in an orphan asylum". Emerson was partly influenced by German philosophy and Biblical criticism. His views, the basis of Transcendentalism, suggested that God does not have to reveal the truth but that the truth could be intuitively experienced directly from nature. When asked his religious belief, Emerson stated, “I am more of a Quaker than anything else. I believe in the ‘still, small voice,’ and that voice is Christ within us.”
Emerson did not become an ardent abolitionist until 1844, though his journals show he was concerned with slavery beginning in his youth, even dreaming about helping to free slaves. In June 1856, shortly after Charles Sumner, a United States Senator, was beaten for his staunch abolitionist views, Emerson lamented that he himself was not as committed to the cause. He wrote, "There are men who as soon as they are born take a bee-line to the axe of the inquisitor... Wonderful the way in which we are saved by this unfailing supply of the moral element". After Sumner's attack, Emerson began to speak out about slavery. "I think we must get rid of slavery, or we must get rid of freedom", he said at a meeting at Concord that summer. Emerson used slavery as an example of a human injustice, especially in his role as a minister. In early 1838, provoked by the murder of an abolitionist publisher from Alton, Illinois named Elijah Parish Lovejoy, Emerson gave his first public antislavery address. As he said, "It is but the other day that the brave Lovejoy gave his breast to the bullets of a mob, for the rights of free speech and opinion, and died when it was better not to live". John Quincy Adams said the mob-murder of Lovejoy "sent a shock as of any earthquake throughout this continent". However, Emerson maintained that reform would be achieved through moral agreement rather than by militant action. By August 1, 1844, at a lecture in Concord, he stated more clearly his support for the abolitionist movement. He stated, "We are indebted mainly to this movement, and to the continuers of it, for the popular discussion of every point of practical ethics".
Emerson is often known as one of the most liberal democratic thinkers of his time who believed that through the democratic process, slavery should be abolished. While being an avid abolitionist who was known for his criticism of the legality of slavery, Emerson struggled with the implications of race. His usual liberal leanings did not clearly translate when it came to believing that all races had equal capability or function, which was a common conception for the period in which he lived. Many critics believe that it was his views on race are what inhibited him from becoming earlier in his life and also inhibited him being more active in the anti-slavery movement. Much of his early life, he was silent on the topic of race and slavery. It was not until well into his 30s that Emerson began to publish writings on race and slavery, and it was not until his late 40s and 50s that he became known as an anti-slavery activist.
During his early life, Emerson seems to develop a hierarchy of races based on faculty to reason or rather, whether African slaves were distinguishably equal to white men based on their ability to reason. In a journal entry written in 1822, Emerson writes about a personal observation:
"It can hardly be true that the difference lies in the attribute of reason. I saw ten, twenty, a hundred large lipped, lowbrowed black men in the streets who, except in the mere matter of language, did not exceed the sagacity of the elephant. Now is it true that these were created superior to this wise animal, and designed to control it? And in comparison with the highest orders of men, the Africans will stand so low as to make the difference which subsists between themselves & the sagacious beasts inconsiderable"
As with many supporters of slavery, during his early years, Emerson seems to have thought that the faculties of African slaves were not equal to their owners, the white men. But this belief in racial inferiorities did not make Emerson a supporter of slavery. Emerson wrote later that year that, "No ingenious sophistry can ever reconcile the unperverted mind to the pardon of Slavery; nothing but tremendous familiarity, and the bias of private interest." Emerson saw the removal of people from their homeland, the treatment of slaves, and the self-seeking benefactors of slaves as gross injustices. For Emerson, slavery was a moral issue while superiority of the races was an issue Emerson tried to analyze from a scientific perspective based what he believe to be inherited traits.
Emerson saw himself as a man of Saxon descent. In a speech given in 1835 titled “Permanent Traits of the English National Genius”, Emerson said the following, “The inhabitants of the United States, especially of the Northern portion, are descended from the people of England and have inherited the traits of their national character.” Emerson saw direct ties between race based on national identity and the inherit nature of the human being. White Americans who were native-born in the United States, Emerson categorizes this population as its own “race”, had a unique position of being superior to other members of the nations. Emerson’s idea of race was more based on a shared culture, environment, and history, not the scientific traits that modern science defines as race. He even believed the native-born Americans were superior to European immigrants, such as the Irish, French, and German.
Later in his life, Emerson's ideas on race changed when he became more involved in the abolitionist movement while at the same time he began to more thoroughly analyze the philosophical implications of race and racial hierarchies. His beliefs shifted focus to the potential outcomes of racial conflicts. Emerson's racial views were closely related to his views on nationalism and national superiority, specifically that of the Saxons. Emerson used contemporary theories of race and natural science to support a theory of race development. He believed that the current political battle and the current enslavement of other races was an inevitable racial struggle, one that would result in the inevitable union of the United States. Such conflicts were necessary for the dialectic of change that would eventually allow the progress of the nation. In much of his later work, Emerson seems to allow the notion that different races will eventually mix in America. This hybridization process would lead to a superior race that would be to the advantage of the superiority of the United States. (Field 9). It is unclear whether or not Emerson believed that Africans would be involved in this hybridization, since much of his work on race seems to indicate that he believed in an early-Darwinist “survival of the fittest” theory, in which less superior races would eventually become obsolete.
Emerson may have had erotic thoughts about at least one man. During his early years at Harvard, he found himself attracted to a young freshman named Martin Gay about whom he wrote sexually charged poetry. He also had a number of romantic interests in various women throughout his life, such as Anna Barker and Caroline Sturgis.
Legacy.
As a lecturer and orator, Emerson—nicknamed the Concord Sage—became the leading voice of intellectual culture in the United States. James Russell Lowell, editor of the Atlantic Monthly and the North American Review, commented in his My Study Windows (1871), that Emerson was not only the “most steadily attractive lecturer in America,” but also “one of the pioneers of the lecturing system.” Herman Melville, who had met Emerson in 1849, originally thought he had "a defect in the region of the heart" and a "self-conceit so intensely intellectual that at first one hesitates to call it by its right name", though he later admitted Emerson was "a great man". Theodore Parker, a minister and Transcendentalist, noted Emerson's ability to influence and inspire others: "the brilliant genius of Emerson rose in the winter nights, and hung over Boston, drawing the eyes of ingenuous young people to look up to that great new star, a beauty and a mystery, which charmed for the moment, while it gave also perennial inspiration, as it led them forward along new paths, and towards new hopes".
Emerson's work not only influenced his contemporaries, such as Walt Whitman and Henry David Thoreau, but would continue to influence thinkers and writers in the United States and around the world down to the present. Notable thinkers who recognize Emerson's influence include Nietzsche and William James, Emerson's godson.
"There is little disagreement that Emerson was the most influential writer of 19th-century America, though these days he is largely the concern of scholars. Walt Whitman, Henry David Thoreau and William James were all positive Emersonians, while Herman Melville, Nathaniel Hawthorne and Henry James were Emersonians in denial — while they set themselves in opposition to the sage, there was no escaping his influence. To T. S. Eliot, Emerson’s essays were an “encumbrance.” Waldo the Sage was eclipsed from 1914 until 1965, when he returned to shine, after surviving in the work of major American poets like Robert Frost, Wallace Stevens and Hart Crane."
In his book "The American Religion," Harold Bloom repeatedly refers to Emerson as "The prophet of the American Religion," which in the context of the book refers to indigenously American religions such as Mormonism and Christian Science, which arose largely in Emerson's lifetime, but also to Mainline Protestant churches that Bloom says have become in the United States more gnostic than their European counterparts. In "The Western Canon", Harold Bloom compares Emerson to Michel de Montaigne: "The only equivalent reading experience that I know is to reread endlessly in the notebooks and journals of Ralph Waldo Emerson, the American version of Montaigne." Several of Emerson's poems were included in Bloom's "The Best Poems of the English Language", although he wrote that none of the poems are as outstanding as the best of Emerson's essays, which Bloom listed as "Self-Reliance", "Circles", "Experience", and "nearly all of "Conduct of Life"". In his belief that line lengths, and rhythms, and phrases are determined by breath Emerson's poetry foreshadowed the theories of Charles Olsen.
In film.
In the 2015 documentary film "The Gettysburg Address", Emerson is portrayed by actor David Strathairn.
In comics.
Emerson is featured prominently in the Johnny Woodruff arc of the newspaper comic Mr. Gnu by Travis Dandro.
Selected works.
See also: .
Collections
Individual essays
Poems
Letters

</doc>
<doc id="26204" url="http://en.wikipedia.org/wiki?curid=26204" title="Women in Judaism">
Women in Judaism

The role of women in Judaism is determined by the Hebrew Bible, the Oral Law (the corpus of rabbinic literature), by custom, and by non-religious cultural factors. Although the Hebrew Bible and rabbinic literature mention various female role models, religious law treats women differently in various circumstances.
Gender has a bearing on familial lines: in traditional Judaism, Jewishness is passed down through the mother, although the father's name is used to describe sons and daughters in the Torah, e.g., "Dinah, daughter of Jacob".
Biblical times.
Relatively few women are mentioned in the Bible by name and role, suggesting that they were rarely in the forefront of public life. There are a number of exceptions to this rule, including the Matriarchs Sarah, Rebecca, Rachel, and Leah, Miriam the prophetess, Deborah the Judge, Huldah the prophetess, Abigail who married David, Rahab and Esther. In the Biblical account these women did not meet with opposition for the relatively public presence they had.
According to Jewish tradition, a covenant was formed between the Israelites and the God of Abraham at Mount Sinai. The Torah relates that both Israelite men and Israelite women were present at Sinai, however, the covenant was worded in such a way that it bound men to act upon its requirements and to ensure that the members of their household (wives, children, and slaves) met these requirements as well. In this sense, the covenant bound women as well, though indirectly.
Marriage and family law in biblical times favored men over women. For example, a husband could divorce a wife if he chose to, but a wife could not divorce a husband without his consent. The practice of levirate marriage applied to widows of childless deceased husbands, but not to widowers of childless deceased wives. Laws concerning the loss of female virginity have no male equivalent. These and other gender differences found in the Torah suggest that women were subordinate to men during biblical times, however, they also suggest that biblical society viewed continuity, property, and family unity as paramount. However, men had specific obligations they were required to perform for their wives. These included the provision of clothing, food, and sexual relations to their wives.
Women also had a role in ritual life. Women (as well as men) were required to make a pilgrimage to the Temple in Jerusalem once a year and offer the Passover sacrifice. They would also do so on special occasions in their lives such as giving a "todah" ("thanksgiving") offering after childbirth. Hence, they participated in many of the major public religious roles that non-levitical men could, albeit less often and on a somewhat smaller and generally more discreet scale.
Women depended on men economically. Women generally did not own property except in the rare case of inheriting land from a father who didn't bear sons. Even "in such cases, women would be required to remarry within the tribe so as not to reduce its land holdings."
According to John Bowker (theologian), traditionally, Jewish "men and women pray separately. This goes back to ancient times when women could go only as far as the second court of the Temple."
Talmudic times.
Classical Jewish rabbinical literature contains quotes that may be seen as both laudatory and derogatory of women. The Talmud states that:
While few women are mentioned by name in rabbinic literature, and none are known to have authored a rabbinic work, those who are mentioned are portrayed as having a strong influence on their husbands. Occasionally they have a public persona. Examples are Bruriah, the wife of the Tanna Rabbi Meir; Rachel, the wife of Rabbi Akiva; and Yalta, the wife of Rabbi Nachman. Rabbi Elazar's wife (of Mishnaic times) counselled her husband in assuming leadership over the Sanhedrin. When R' Elazar ben Azarya was asked to assume the role of "Nasi" ("Prince" or President of the Sanhedrin), he replied that he must first take counsel with his wife, which he did.
Middle Ages.
Jews lived all across the medieval world. They lived in both Muslim controlled lands such as Iran, Iraq, Egypt, Spain, North Africa, as well as Christian controlled lands such as France, Germany, Italy, and England. Mark Cohen, in his book, "Under Crescent and Cross", argues that Jews were more accepted and a part of the Islamic world than Jews living in Christian lands. He contends that Jews who lived in the Islamic ruled lands lived in a place that was culturally and professionally accepting towards Jews. This was mainly due to the fact that Jews were seen as a separate ethnicity (see Dhimmi). Jews were seen as a separate ethnicity due to visual cues (clothing, speech/language, calendar, religious space) and this gave Muslims the ability to tolerate them. In contrast, the Christian world was a place of persecution for the Jews and they were not seen as a separate ethnic group. Cohen even states that the excesses of persecution and violence did not have a counterpart in the Islamic world. During this time there was a conflict between Judaism’s lofty religious expectations of women and the reality of society in which these Jewish women lived; this is similar to the lives of Christian women in the same period.
Since Jews were seen as second class citizens in the Christian and Muslim world, it was even harder for Jewish women to establish their own status. Avraham Grossman argues in his book "Pious and Rebellious: Jewish Women in Medieval Europe" that three factors affected how Jewish women were perceived by the society around them:“ the biblical and talmudic heritage; the situation in the non-Jewish society within which the Jews lived and functioned; and the economic status of the Jews, including the woman’s role in supporting the family.” Grossman uses all three factors to argue that women’s status overall during this period actually rose.
Religious Life.
The basis for Jewish beliefs can be seen in central texts of Judaism: Talmud, Mishnah, Tanach, and responsa. These are rich sources to help in understanding what ordinary people thought and how they acted during the Middle Ages. From the sources we do have, we can see how Jewish women in the medieval world lived.
The synagogue was an important part of the Jewish community; it was used for more than just prayer. For the most part Jewish communities were able to govern themselves.They had their own governing council made up of lay as well as religious leaders.The council could oversee everything from the running the synagogue to reprimanding those who participated in sinful activities, such as gambling. The synagogue was the center of the Jewish community which many believe was key to its continuity in this period.
Religious developments during the medieval period included relaxation on prohibitions against teaching women Torah, and the rise of women's prayer groups. One place that women participated in Jewish practices publicly was the synagogue. Women probably learned how to read the liturgy in Hebrew. In most synagogues they were given their own section, most likely a balcony; some synagogues had a separate building. Separation from the men was created by the Rabbis in the Mishnah and the Talmud. The reasoning behind the Halacha was that a woman and her body would distract men and give them impure thoughts during prayer. Due to this rabbinical interpretation, scholars have seen the women’s role in the synagogue as limited and sometimes even non-existent. However, recent research has shown that women actually had a larger role in the synagogue and the community at large. Women usually attended synagogue, for example, on the Sabbath and the holidays. Depending on the location of the women in the synagogue, they may have followed the same service as the men or they conducted their own services. Since the synagogues were large, there would be a designated woman who would be able to follow the cantor and repeat the prayers aloud for the women. Women had always attended services on Shabbat and holidays, but beginning in the eleventh century, women became more involved in the synagogue and its rituals. Women sitting separately from the men became a norm in synagogues around the beginning of the thirteenth century. Women, however, did much more than pray in the synagogue. One of the main jobs for women was to beautify the building. There are Torah ark curtains and Torah covers that women sewed and survive today. The synagogue was a communal place for both men and women where worship, learning and community activities occurred.
The rise and increasing popularity of Kabbalah, which emphasized the shechinah and female aspects of the divine presence and human-divine relationship, and which saw marriage as a holy covenant between partners rather than a civil contract, had great influence. Kabbalists explained the phenomenon of menstruation as expressions of the demonic or sinful character of the menstruant. 
These changes were accompanied by increased pietistic strictures, including greater requirements for modest dress, and greater strictures during the period of menstruation. At the same time, there was a rise in philosophical and midrashic interpretations depicting women in a negative light, emphasizing a duality between matter and spirit in which femininity was associated, negatively, with earth and matter. The gentile society was also seen as a negative influence on the Jewish community. For example, it seems that Jews would analyze the modesty of their non-Jewish neighbors before officially moving into a new community because they knew that their children would be influenced by the local gentiles.
After the expulsion of the Jews from Spain in 1492, women became virtually the only source of Jewish ritual and tradition in the Catholic world in a phenomenon known as crypto-Judaism. Crypto-Jewish women would slaughter their own animals and made sure to keep as many of the Jewish dietary laws and life cycle rituals as possible without raising suspicion. Occasionally, these women were prosecuted by Inquisition officials for suspicious behavior such as lighting candles to honor the Sabbath or refusing to eat pork when it was offered to them. The Inquisition targeted crypto-Jewish women at least as much as it targeted crypto-Jewish men because women were accused of perpetuating Jewish tradition while men were merely permitting their wives and daughters to organize the household in this manner.
Jewish women were also apart of the social phenomenon of martyrdom of the First Crusade. Most of the violence from the First Crusade towards Jews was due to the People’s Crusade. Inspired by the Pope’s call, Christians in Roven, Trier, Metz, Cologne, Mainz, Worms, Prague, and Bohemia, among others, massacred thousands of Jews. The local governments did not, at first, sanction the mass murder of Jews as part of the fervor of the Crusades. However, popular anxiety overcame many towns and villages and lead towards the local government’s support of killing Jews. Although many Jews did convert, many rather chose to die. Through the sources, such as chronicles and poems, we see that Jewish women were often martyred with their families. In contrast, most Christian women martyrs were members of a convent or religious order when they were martyred (See for more information).
Domestic Life.
Marriage, Domestic Violence and Divorce are all topics discussed by Jewish sages of the Medieval world. Marriage is an important institution in Judaism (see Marriage in Judaism). The sages of this period discussed this topic at length.
Rabbeinu Gershom instituted a rabbinic decree (Takkanah) prohibiting polygamy among Ashkenazic Jews. The rabbis instituted legal methods to enable women to petition a rabbinical court to compel a divorce. Maimonides ruled that a woman who found her husband "repugnant" could compel a divorce, "because she is not like a captive, to be subjected to intercourse with one who is hateful to her." Divorce for Christian women was technically not an option. By the tenth century, Christianity considered marriage a sacrament and could not be dissolved (see Divorce in Medieval Europe).
The rabbis also instituted and tightened prohibitions on domestic violence. Rabbi Peretz ben Elijah ruled, "The cry of the daughters of our people has been heard concerning the sons of Israel who raise their hands to strike their wives. Yet who has given a husband the authority to beat his wife?"Rabbi Meir of Rothenberg ruled that "For it is the way of the Gentiles to behave thus, but Heaven forbid that any Jew should do so. And one who beats his wife is to be excommunicated and banned and beaten." Rabbi Meir of Rothenberg also ruled that a battered wife could petition a rabbinical court to compel a husband to grant a divorce, with a monetary fine owed her on top of the regular ketubah money. These rulings occurred in the midst of societies where wife-beating was legally sanctioned and routine.
Education.
Jewish women had a limited education. They were taught to read, write, run a household. They were also given some education in religious law that was essential to their daily lives, such as keeping kosher. Both Christian and Jewish girls were educated in the home. Although Christian girls may have had a male or female tutor, most Jewish girls had a female tutor. Higher learning was uncommon for both Christian and Jewish women. Christian women could enter a convent in order to achieve a higher education (See Female Education in the Medieval Period). 
There are more sources of education for Jewish women living in Muslim controlled lands. Middle Eastern Jewry, on the other hand, had an abundance of female literates. The Cairo Geniza is filled with correspondences written (sometimes dictated) between family members and spouses. Many of these letters are pious and poetic and express a desire to be in closer or more frequent contact with a loved one that is far enough away to only be reached by written correspondence. There are also records of wills and other personal legal documents as well as written petitions to officials in cases of spouse spousal abuse or other conflicts between family members written or dictated by women.
Many women gained enough education to help their husbands out in business or even hold their own. Just like Christian women who ran their own business, Jewish women were engaged in their own occupations as well as helping their husbands. Jewish women seem to have lent money to Christian women throughout Europe. Women were also copyists, midwives, spinners and weavers.
Views on the education of women.
From certain contexts of the Mishnah and Talmud it can be derived that women should not study Mishnah. There were female Tannaitic Torah jurists such as Rabbi Meir's wife, Rabbi Meir's daughter, and the daughter of Haninyah ben Teradyon Haninyah's daughter is again mentioned as a sage in the non-Talmud 3rd-century text Tractate Semahot verse 12:13. Rabbi Meir's wife is credited with teaching him how to understand some verses from Isaiah. In the Mishnah there is also a reference to certain women teaching men the Torah from behind a curtain, so that no man would be offended.
A "yeshiva", or school for Talmudic studies, is an "exclusively masculine environment" because of absence of women from these studies.
Beruryah.
Beruryah (her name is a standard Jewish female name meaning 'the clarity of God') is a Tanna mentioned by name in the Talmud, who has a female name, has orally been transmitted as a female, and is referred to in the text using the nekeva (feminine Hebrew and Aramaic) adjectives and adverbs. Originally she was believed to be either Rabbi Meir's wife mentioned above, or Rabbi Chaninyah's daughter mentioned above, however over the past three to four centuries Rabbinic scholars have realized that these generations do not correspond to Beruryah's law decisions, and life, therefore she today is just 'Beruryah' and of heretofore unknown lineage.
Her law decisions were minor but set a crucial ancient precedent for modern Jewish women. She is mentioned at least four times in the Talmudic discourse regarding her law decrees first Babylonian Talmud Berakhot 10a then in Tosefta Pesahim 62b in Babylonian Talmud Eruvin 53b–54a and Babylonian Talmud Avodah Zarah 18b. In one case she paskinned din on "klaustra" a rare Greek word referring to an object, used in the Talmud, unfortunately Rabbi Yehudah Hanassi did not believe women could be credited with paskining din, as it says 'do not speak too much to women' (Tannah Rabbi Jesse the Galilean), and therefore credited the law to Rabbi Joshua who may have been her father.
Beruryah however was actually remembered with great respect in the Talmud where she is lauded to have been reputed as such a genius that she studied “three hundred Halachot from three hundred sages in just one day” (Pesachim 62b). Clearly contradicting the injunction against women studying Torah.
Rashi's Daughters.
Rashi had no sons and taught the Mishnah and Talmud to his daughters, until they knew it by heart as Jewish tradition teaches; they then transferred their knowledge of original Mishnah commentary to the Ashkenazi men of the next generation.
Haim Yosef David Azulai, AKA 'The Hid'aa'.
The Hida, wrote (Tuv Ayin, no. 4) woman should not study Mishnah only if they do not want to.'We cannot force a woman to learn, like we do to boys'. However, if she wants to learn then not only may she do so on her own, but men may originally teach her, and she can then teach other women if they so choose. According to the Hida, the prohibition against teaching women does not apply to a motivated woman or girl. Other Mizrahi Rabbis disputed this with him.
His response to detractors was that indeed, in truth, there is a prohibition against teaching Mishnah to any student—male or female—who one knows is not properly prepared and motivated, referred to a talmid she-eino hagun (Shulhan Arukh, Yoreh De'ah 246:7). Babylonian Talmud Berakhos 28a relates that Rabban Gamliel would announce that any student who is not pure enough so that 'his outer self is like his inner self' may not enter the study hall. While this approach, requiring absolute purity, was rejected by other ancient Rabbis, for example 'he who is not for the name of God, will become for the name of God', and a middle approach was adopted by Jews as standard. If one has knowledge that a particular Mishnayot student is definitely bad then he may not be taught. He claimed that 'it seems that for women there is a higher standard and she must be motivated in order to have this permission to learn' in his response to the Mizrahi tradition.
Yisrael Meir Kagan.
One of the most important Ashkenazic rabbanim of the past century, Yisrael Meir Kagan, known poularly as the Chofetz Chaim. favored Torah education for girls to counteract the French "finishing schools" prevalent in his day for the daughters of the bourgeoisie."It would appear that all [these sexist laws] were intended for earlier generations when everyone dwelt in the place of their familial ancestral home and ancestral tradition was very powerful among all to follow the path of their fathers... under such circumstances we could maintain that a woman not study Mishnayos and, for guidance, rely on her righteous parents, but presently, due to our myriad sins, ancestral tradition has become exceptionally weak and it is common that people do not dwell in proximity to the family home, and especially those women who devote themselves to mastering the vernacular, surely it is a now a great mitzvah to teach them Scripture and the ethical teachings of our sages such as Pirkei Avos, Menoras Ha-Ma'or and the like so that they will internalize our sacred faith because [if we do not do so] they are prone to abandon the path of God and violate all principles of [our] faith."
Joseph Solovetchik.
Rabbi Yoseph Solovetchik 'amened' the teachings of The Hafetz Haim. Rabbi Solovetchik taught all religious Ashkenazi Jews with the exception of hardline Hasidim, not should, or if they show motivation, but must teach their female children Gemarah like the boy school children. He among others fully institutionlized the teaching of Mishnah and Talmud to girls, from an autobiography on him by Rabbi Mayor Twersky called "A Glimpse of the Rav" in R. Menachem Genack ed., Rabbi Joseph B. Soloveitchik: Man of Halacha, Man of Faith, page 113: "The halakha prohibiting Torah study for women is not indiscriminate or all-encompassing. There is complete unanimity that women are obligated to study halakhot pertaining to mitsvot which are incumbent upon them... The prohibition of teaching Torah she-Ba'al Pe to women relates to optional study. If ever circumstances dictate that study of Torah sh-Ba'al Pe is necessary to provide a firm foundation for faith, such study becomes obligatory and obviously lies beyond the pale of any prohibition." Undoubtedly, the Rav's prescription was more far-reaching that that of the Hafets Hayim and others. But the difference in magnitude should not obscure their fundamental agreement [on changing the attitudes Halachically].
Present day.
Orthodox Judaism.
Orthodox Judaism is based on gendered understandings of Jewish practice—i.e., that there are different roles for men and women in religious life. There are different opinions among Orthodox Jews concerning these differences. Most claim that men and women have complementary, yet different roles in religious life, resulting in different religious obligations. Others believe that some of these differences are not a reflection of religious law, but rather of cultural, social, and historical causes. In the area of education, women were historically exempted from any study beyond an understanding of the practical aspects of Torah, and the rules necessary in running a Jewish household – both of which they have an obligation to learn. Until the twentieth century, women were often discouraged from learning Talmud and other advanced Jewish texts. In the past 100 years, Orthodox Jewish education for women has advanced tremendously.
There have been many areas in which Orthodox women have been working towards change within religious life over the past 20 years: promoting advanced women's learning and scholarship, promoting women's ritual inclusion in synagogue, promoting women's communal and religious leadership, and more. Women have been advancing change despite often vocal opposition by rabbinic leaders. Some Orthodox rabbis try to discount changes by claiming that women are motivated by sociological reasons and not by "true" religious motivation. For example, Orthodox, Haredi, and Hasidic rabbis discourage women from wearing a yarmulke, tallit or tefillin.
In most Orthodox synagogues, women still do not give a "d'var Torah" (brief discourse, generally on the weekly Torah portion) after or between services. Furthermore, a few Modern Orthodox synagogues have mechitzot dividing the left and right sides of the synagogue (rather than the usual division between the front and back of the synagogue, with women sitting in the back), with the women's section on one side and the men's section on the other.
Rules of modesty.
The importance of modesty in dress and conduct is particularly stressed among girls and women in Orthodox society. Many Orthodox women only wear skirts and avoid wearing trousers, and some married Orthodox women cover their hair with a wig, hat, or scarf. Judaism prescribes modesty for both men and women.
Rules of family purity.
In accordance with Jewish Law, Orthodox Jewish women refrain from contact with their husbands while they are menstruating, and for a period of 7 clean days after menstruating, and after the birth of a child. The Israeli Rabbinate has recently approved women acting as "yoatzot", halakhic advisers on sensitive personal matters such as family purity.
Modern Orthodox Judaism.
Rabbi Joseph B. Soloveitchik, a leader of profound influence in modern Orthodoxy in the United States, discouraged women from serving as presidents of synagogues or any other official positions of leadership, from performing other mitzvot (commandments) traditionally performed by males exclusively, such as wearing a tallit or tefillin. Soloveitchik wrote that while women do not lack the capability to perform such acts, there is no "mesorah" (Jewish tradition) that permits it. In making his decision, he relied upon Jewish oral law, including a mishnah in Chulin 2a and a Beit Yoseph in the Tur Yoreh Deah stating that a woman can perform a specific official communal service for her own needs but not those of others.
Women's issues garnered more interest with the advent of feminism. Many Modern Orthodox Jewish women and Modern Orthodox rabbis sought to provide greater and more advanced Jewish education for women. Since most Modern Orthodox women attend college, and many receive advanced degrees in a variety of fields, Modern Orthodox communities generally promote women's secular education. A few Modern Orthodox Synagogues have women serving as clergy, including Gilah Kletenik at Congregation Kehilath Jeshurun. In 2013, Yeshivat Maharat, located in the United States, became the first Orthodox institution to consecrate female clergy. The graduates of Yeshivat Maharat do not call themselves "rabbis." The title they are given is "maharat." Also in 2013 Malka Schaps became the first female Haredi dean at an Israeli university when she was appointed dean of Bar Ilan University's Faculty of Exact Sciences. Also in 2013, the first class of female halachic advisers trained to practice in the US graduated; they graduated from the North American branch of Nishmat’s yoetzet halacha program in a ceremony at Congregation Sheartith Israel, Spanish and Portuguese Synagogue in Manhattan and SAR High School in Riverdale, New York began allowing girls to wrap tefillin during Shacharit-morning prayer; it is probably the first Modern Orthodox high school in the U.S. to do so.
In 2014 the first ever book of halachic decisions written by women who were ordained to serve as poskot (Idit Bartov and Anat Novoselsky) was published. The women were ordained by the municipal chief rabbi of Efrat, Rabbi Shlomo Riskin, after completing Midreshet Lindenbaum women’s college’s five-year ordination course in advanced studies in Jewish law, as well as passing examinations equivalent to the rabbinate’s requirement for men.
In 2015 Jennie Rosenfeld became the first female Orthodox spiritual advisor in Israel (specifically, she became the spiritual advisor, also called manhiga ruchanit, for the community of Efrat.)
Women's prayer groups.
Separate Jewish women's prayer groups were a sanctioned custom among German Jews in the Middle Ages. The "Kol Bo" provides, in the laws for Tisha B'Av:
In Germany, in the 12th and 13th centuries, women's prayer groups were led by female cantors. Rabbi Eliezar of Worms, in his elegy for his wife Dulca, praised her for teaching the other women how to pray and embellishing the prayer with music. The gravestone of Urania of Worms, who died in 1275, contains the inscription "who sang "piyyutim" for the women with musical voice." In the Nurnberg Memorial Book, one Richenza was inscribed with the title "prayer leader of the women."
Orthodox women more recently began holding organized women's "tefila" (prayer) groups beginning in the 1970s. While no Orthodox legal authorities agree that women can form a "minyan" (prayer quorum) for the purpose of regular services, women in these groups read the prayers and study Torah. A number of leaders from all segments of Orthodox Judaism have commented on this issue, but it has had little impact on Haredi and Sephardi Judaism. However, the emergence of this phenomenon has enmeshed Modern Orthodox Judaism in a debate which still continues today. There are three schools of thought on this issue:
In 2013 the Israeli Orthodox rabbinical organization Beit Hillel issued a halachic ruling which allows women, for the first time, to say the Kaddish prayer in memory of their deceased parents.
Women as witnesses.
Traditionally, women are not generally permitted to serve as witnesses in an Orthodox Beit Din (rabbinical court), although they have recently been permitted to serve as "toanot" (advocates) in those courts. This limitation has exceptions which have required exploration under rabbinic law as the role of women in society and the obligations of religious groups under external civil law have been subject to increasing recent scrutiny.
The recent case of Rabbi Mordecai Tendler, the first rabbi to be expelled from the Rabbinical Council of America following allegations of sexual harassment, illustrated the importance of clarification of Orthodox halakha in this area. Rabbi Tendler claimed that the tradition of exclusion of women's testimony should compel the RCA to disregard the allegations. He argued that since the testimony of a woman could not be admitted in Rabbinical court, there were no valid witnesses against him, and hence the case for his expulsion had to be thrown out for lack of evidence. In a ruling of importance for Orthodox women's capacity for legal self-protection under Jewish law, Haredi Rabbi Benzion Wosner, writing on behalf of the "Shevet Levi" Beit Din (Rabbinical court) of Monsey, New York, identified sexual harassment cases as coming under a class of exceptions to the traditional exclusion, under which "even children or women" have not only a right but an obligation to testify, and can be relied upon by a rabbinical court as valid witnesses:
The Rabbinical Council of America, while initially relying on its own investigation, chose to rely on the Halakhic ruling of the Haredi Rabbinical body as authoritative in the situation.
Orthodox approaches to change.
Leaders of the Haredi community have been steadfast in their opposition to a change in the role of women, arguing that the religious and social constraints on women, as dictated by traditional Jewish texts, are timeless and are not affected by contemporary social change. Many also argue that giving traditionally male roles to women will only detract from both women's and men's ability to lead truly fulfilling lives. Haredim have also sometimes perceived arguments for liberalization as in reality stemming from antagonism to Jewish law and beliefs generally, arguing that preserving faith requires resisting secular and "un-Jewish" ideas.
Modern Orthodox Judaism, particularly in its more liberal variants, has tended to look at proposed changes in the role of women on a specific, case-by-case basis, focusing on arguments regarding the religious and legal role of specific prayers, rituals and activities individually. Such arguments have tended to focus on cases where the Talmud and other traditional sources express multiple or more liberal viewpoints, particularly where the role of women in the past was arguably broader than in more recent times. Feminist advocates within Orthodoxy have tended to stay within the traditional legal process of argumentation, seeking a gradualist approach, and avoiding wholesale arguments against the religious tradition as such. Nevertheless, a growing Orthodox feminist movement seeks to address gender inequalities.
Agunot.
Agunot (lit. "chained women") are women who are wish to divorce their husbands, but whose husbands refuse to give them a writ of divorce (a "get"). In Orthodox Judaism, only a man is able to serve a "get."
Conservative Judaism.
Although the position of Conservative Judaism toward women originally differed little from the Orthodox position, it has in recent years minimized legal and ritual differences between men and women. The Committee on Jewish Law and Standards (CJLS) of the Rabbinical Assembly has approved a number of decisions and responsa on this topic. These provide for women's active participation in areas such as:
A rabbi may or may not decide to adopt particular rulings for the congregation; thus, some Conservative congregations will be more or less egalitarian than others. However, there are other areas where legal differences remain between men and women, including:
A Conservative Jewish "ketuba" includes a clause that puts a husband and wife on more equal footing when it comes to marriage and divorce law within "halacha".
The CJLS recently reaffirmed the obligation of Conservative women to observe "niddah" (sexual abstinence during and after menstruation) and "mikvah" (ritual immersion) following menstruation, although somewhat liberalizing certain details. Such practices, while requirements of Conservative Judaism, are not widely observed among Conservative laity.
Changes in the Conservative position.
Prior to 1973, Conservative Judaism had more limited roles for women and was more similar to current Modern Orthodoxy, with changes on issues including mixed seating, synagogue corporate leadership, and permitting women to be called to the Torah. In 1973, the CJLS of the Rabbinical Assembly voted, without issuing an opinion, that women could count in a "minyan". In 1983, the Jewish Theological Seminary of America (JTSA) faculty voted, also without accompanying opinion, to ordain women as rabbis and as cantors.
In 2002, the CJLS adapted a responsum by Rabbi David Fine, , which provides an official religious-law foundation for women counting in a minyan and explains the current Conservative approach to the role of women in prayer. This responsum holds that although Jewish women do not traditionally have the same obligations as men, Conservative women have, as a collective whole, voluntarily undertaken them. Because of this collective undertaking, the Fine responsum holds that Conservative women are eligible to serve as agents and decision-makers for others. The responsum also holds that traditionally-minded communities and individual women can opt out without being regarded by the Conservative movement as sinning. By adopting this responsum, the CJLS found itself in a position to provide a considered Jewish-law justification for its egalitarian practices, without having to rely on potentially unconvincing arguments, undermine the religious importance of community and clergy, ask individual women intrusive questions, repudiate the "halakhic" tradition, or label women following traditional practices as sinners.
In 2006, the CJLS adopted three responsa on the subject of niddah, which reaffirmed an obligation of Conservative women to abstain from sexual relations during and following menstruation and to immerse in a mikvah prior to resumption, while liberalizing observance requirements including shortening the length of the niddah period, lifting restrictions on non-sexual contact during niddah, and reducing the circumstances under which spotting and similar conditions would mandate abstinence.
In all cases continuing the Orthodox approach was also upheld as an option. Individual Conservative rabbis and synagogues are not required to adopt any of these changes, and a small number have adopted none of them.
Conservative approaches to change.
Prior to 1973, Conservative approaches to change were generally on an individual, case-by-case basis. Between 1973 and 2002, the Conservative movement adapted changes through its official organizations, but without issuing explanatory opinions. Since 2002, the Conservative movement has coalesced around a single across-the board approach to the role of women in Jewish law.
In 1973, 1983, and 1993, individual rabbis and professors issued six major opinions which influenced change in the Conservative approach, the first and second Sigal, Blumenthal, Rabinowitz, and Roth responsa, and the Hauptman article. These opinions sought to provide for a wholesale shift in women's public roles through a single, comprehensive legal justification. Most such opinions based their positions on an argument that Jewish women always were, or have become, legally obligated to perform the same "mitzvot" as men and to do so in the same manner.
The first Sigal and the Blumenthal responsa were considered by the CJLS as part of its decision on prayer roles in 1973. They argued that women have always had the same obligations as men. The first Sigal responsum used the Talmud's general prayer obligation and examples of cases in which women were traditionally obligated to say specific prayers and inferred from them a public prayer obligation identical to that of men. The Blumenthal responsum extrapolated from a minority authority that a "minyan" could be formed with nine men and one woman in an emergency. The Committee on Jewish Law and Standards (CJLS) declined to adopt either responsum. Rabbi Siegel reported to the Rabbinical Assembly membership that many on the CJLS, while agreeing with the result, found the arguments unconvincing.
The Rabinowitz, Roth, and second Sigal responsa were considered by the JTSA faculty as part of its decision to ordain women as rabbis in 1983. The Rabbinowitz responsum sidestepped the issue of obligation, arguing that there is no longer a religious need for a community representative in prayer and hence there is no need to decide whether a woman can "halakhically" serve as one. The CJLS felt that an argument potentially undermining the value of community and clergy was unconvincing: "We should not be afraid to recognize that the function of clergy is to help our people connect with the holy." The Roth and second Sigal responsa accepted that time-bound "mitzvot" were traditionally optional for women, but argued that women in modern times could change their traditional roles. The Roth responsum argued that women could individually voluntarily assume the same obligations as men, and that women who do so (e.g. pray three times a day regularly) could count in a "minyan" and serve as agents. The JTSA accordingly required female rabbinical students wishing to train as rabbis to personally obligate themselves, but synagogue rabbis, unwilling to inquire into individual religiosity, found it impractical. The second Sigal responsum called for a "takkanah", or rabbinical edict, "that would serve as a "halakhic" ERA," overruling all non-egalitarian provisions in law or, in the alternative, a new approach to "halakhic" interpretation independent of legal precedents. The CJLS, unwilling to use either an intrusive approach or a repudiation of the traditional legal process as bases for action, did not adopt either and let the JTS faculty vote stand unexplained.
In 1993, Professor Judith Hauptman of JTS issued an influential paper arguing that women had historically always been obligated in prayer, using more detailed arguments than the Blumenthal and first Sigal responsa. The paper suggested that women who followed traditional practices were failing to meet their obligations. Rabbi Roth argued that Conservative Judaism should think twice before adopting a viewpoint labeling its most traditional and often most committed members as sinners. The issue was again dropped.
In 2002, the CJLS returned to the issue of justifying its actions regarding women's status, and adopted a single authoritative approach, the Fine responsum, as the definitive Conservative halakha on role-of-women issues. This responsum holds that although Jewish women do not traditionally have the same obligations as men, Conservative women have, as a collective whole, voluntarily undertaken them. Because of this collective undertaking, the Fine responsum holds that Conservative women are eligible to serve as agents and decision-makers for others. The Responsum also holds that traditionally-minded communities and individual women can opt out without being regarded by the Conservative movement as sinning. By adopting this Responsum, the CJLS found itself in a position to provide a considered Jewish-law justification for its egalitarian practices, without having to rely on potentially unconvincing arguments, undermine the religious importance of community and clergy, ask individual women intrusive questions, repudiate the "halakhic" tradition, or label women following traditional practices as sinners.
Reform Judaism.
Reform Judaism believes in the equality of men and women. The Reform movement rejects the idea that halakha (Jewish law) is the sole legitimate form of Jewish decision making, and holds that Jews can and must consider their conscience and ethical principles inherent in the Jewish tradition when deciding upon a right course of action. There is widespread consensus among Reform Jews that traditional distinctions between the role of men and women are antithetical to the deeper ethical principles of Judaism. This has enabled Reform communities to allow women to perform many rituals traditionally reserved for men, such as:
Concerns about intermarriage have also influenced the Reform Jewish position on gender. In 1983, the Central Conference of American Rabbis passed a resolution waiving the need for formal conversion for anyone with at least one Jewish parent who has made affirmative acts of Jewish identity. This departed from the traditional position requiring formal conversion to Judaism for children without a Jewish mother.
The 1983 resolution of the American Reform movement has had a mixed reception in Reform Jewish communities outside of the United States. Most notably, the Israel Movement for Progressive Judaism has rejected patrilineal descent and requires formal conversion for anyone without a Jewish mother.
Liberal prayerbooks tend increasingly to avoid male-specific words and pronouns, seeking that all references to God in translations be made in gender-neutral language. For example, the UK Liberal movement's "Siddur Lev Chadash" (1995) does so, as does the UK Reform Movement's "Forms of Prayer" (2008). In Mishkan T'filah, the American Reform Jewish prayer book released in 2007, references to God as “He” have been removed, and whenever Jewish patriarchs are named (Abraham, Isaac, and Jacob), so also are the matriarchs (Sarah, Rebecca, Rachel, and Leah.) In 2015 the Reform Jewish High Holy Days prayer book Mishkan HaNefesh was released; it is intended as a companion to Mishkan T'filah. It includes a version of the High Holy Days prayer Avinu Malkeinu that refers to God as both "Loving Father" and "Compassionate Mother." Other notable changes are replacing a line from the Reform movement’s earlier prayerbook, "Gates of Repentance," that mentioned the joy of a bride and groom specifically, with the line "rejoicing with couples under the chuppah [wedding canopy]", and adding a third, non-gendered option to the way worshippers are called to the Torah, offering “mibeit,” Hebrew for “from the house of,” in addition to the traditional “son of” or “daughter of.”
Reform approaches to change.
Reform Judaism generally holds that the various differences between the roles of men and women in traditional Jewish law are not relevant to modern conditions and not applicable today. Accordingly, there has been no need to develop legal arguments analogous to those made within the Orthodox and Conservative movements.
Reconstructionist Judaism.
The equality of women and men is a central tenet and hallmark of Reconstructionist Judaism. From the beginning, Reconstructionist Jewish ritual allowed men and women to pray together — a decision based on egalitarian philosophy. It was on this basis that Rabbi Mordecai Kaplan called for the full equality of women and men, despite the obvious difficulties reconciling this stance with norms of traditional Jewish practice. The Reconstructionist Movement ordained women rabbis from the start. In 1968, women were accepted into the Reconstructionist Rabbinical College, under the leadership of Ira Eisenstein. The first ordained female Reconstructionist rabbi, Sandy Eisenberg Sasso, served as rabbi of the Manhattan Reconstructionist Congregation in 1976 and gained a pulpit in 1977 at Beth El Zedeck congregation in Indianapolis. Sandy Eisenberg Sasso was accepted without debate or subsequent controversy. In 2005, 24 out of the movement's 106 synagogues in the US had women as senior or assistant rabbis. In 2013 Rabbi Deborah Waxman was elected as the President of the Reconstructionist Rabbinical College. As the President, she is believed to be the first woman and first lesbian to lead a Jewish congregational union, and the first female rabbi and first lesbian to lead a Jewish seminary; the Reconstructionist Rabbinical College is both a congregational union and a seminary.
The Reconstructionist Community began including women in the minyan and allowing them to come up to the Torah for aliyot. They also continued the practice of bat mitzvah. Reconstructionist Judaism also allowed women to perform other traditional male tasks, such as serving as witnesses, leading services, public Torah reading, and wearing ritual prayer garments like kippot and tallitot. Female Reconstructionist rabbis have been instrumental in the creation of rituals, stories, and music that have begun to give women's experience a voice in Judaism. Most of the focus has been on rituals for life-cycle events. New ceremonies have been created for births, weddings, divorces, conversions, weaning, and the onset of menarche and menopause. The Reconstructionist movement as a whole has been committed to creating liturgy that is in consonance with gender equality and the celebration of women's lives. Another major step: The Federation of Reconstructionist Congregations has also developed educational programs that teach the full acceptance of lesbians, as well as rituals that affirm lesbian relationships. Reconstructionist rabbis officiate at same-sex weddings. Reconstructionist Judaism also allows openly LGBT men and women to be ordained as rabbis and cantors.
Several prominent members of the Reconstructionist community have focused on issues like domestic violence. Others have devoted energy to helping women gain the right of divorce in traditional Jewish communities. Many have spoken out for the right of Jewish women to pray aloud and read from the Torah at the Western Wall in Jerusalem, the Women of the Wall group.
When the roles of women in religion change, there may also be changed roles for men. With their advocacy of patrilineal descent in the 1970s, the Reconstructionist Rabbinical Association supported the principle that a man who takes responsibility for raising a Jewish child can pass Judaism on to the next generation as well as a woman. All children who receive a Jewish education are considered Jewish in Reconstructionist Judaism regardless of whatever is the sex of their Jewish parent.
Jewish Renewal.
Jewish Renewal is a recent movement in Judaism which endeavors to reinvigorate modern Judaism with Kabbalistic, Hasidic, musical and meditative practices; it describes itself as "a worldwide, transdenominational movement grounded in Judaism’s prophetic and mystical traditions." The Jewish Renewal movement ordains women as well as men as rabbis and cantors. Lynn Gottlieb became the first female rabbi in Jewish Renewal in 1981, and Avitall Gerstetter, who lives in Germany, became the first female cantor in Jewish Renewal (and the first female cantor in Germany) in 2002. In 2009 and 2012 respectively, OHALAH (Association of Rabbis for Jewish Renewal) issued a board statement and a resolution supporting Women of the Wall. The Statement of Principles of OHALAH states in part, "Our local communities will embody egalitarian and inclusive values, manifested in a variety of leadership and decision-making structures, ensuring that women and men are full and equal partners in every aspect of our communal Jewish life." In 2014 OHALAH issued a Board Statement stating in part, "Therefore, be it resolved that: OHALAH supports the observance of Women's History Month, International Women’s Day, and Women's Equality Day; OHALAH condemns all types of sexism; OHALAH is committed to gender equality, now and in all generations to come; and OHALAH supports equal rights regardless of gender." Also in 2014, ALEPH: Alliance for Jewish Renewal issued a statement stating, "ALEPH: Alliance for Jewish Renewal supports the observance of Women's History Month, International Women’s Day, and Women's Equality Day, condemns all types of sexism, is committed to gender equality, now and in all generations to come, and supports equal rights regardless of gender, in recognition and allegiance to the view that we are all equally created in the Divine Image." 
Humanistic Judaism.
Humanistic Judaism is a movement in Judaism that offers a nontheistic alternative in contemporary Jewish life. It ordains both men and women as rabbis, and its first rabbi was a woman, Tamara Kolton, who was ordained in 1999. Its first cantor was also a woman, Deborah Davis, ordained in 2001; however, Humanistic Judaism has since stopped ordaining cantors. The Society for Humanistic Judaism issued a statement in 1996 stating in part, "we affirm that a woman has the moral right and should have the continuing legal right to decide whether or not to terminate a pregnancy in accordance with her own ethical standards. Because a decision to terminate a pregnancy carries serious, irreversible consequences, it is one to be made with great care and with keen awareness of the complex psychological, emotional, and ethical implications." They also issued a statement in 2011 condemning the then-recent passage of the “No Taxpayer Funding for Abortion Act” by the U.S. House of Representatives, which they called "a direct attack on a women’s right to choose". In 2012, they issued a resolution opposing conscience clauses that allow religious-affiliated institutions to be exempt from generally applicable requirements mandating reproductive healthcare services to individuals or employees. In 2013 they issued a resolution stating in part, "Therefore, be it resolved that: The Society for Humanistic Judaism wholeheartedly supports the observance of Women's Equality Day on August 26 to commemorate the anniversary of the passage of the Nineteenth Amendment to the U.S. Constitution allowing women to vote; The Society condemns gender discrimination in all its forms, including restriction of rights, limited access to education, violence, and subjugation; and The Society commits itself to maintain vigilance and speak out in the fight to bring gender equality to our generation and to the generations that follow." 
Women as soferim.
A Sofer, Sopher, Sofer SeTaM, or Sofer ST"M (Heb: "scribe", סופר סת״ם) is a Jewish scribe who can transcribe Torah scrolls, tefillin and mezuzot, and other religious writings. (ST"M, סת״ם, is an abbreviation for Sefer Torahs, Tefillin, and Mezuzot. The plural of sofer is "soferim", סופרים.) Forming the basis for the discussion of women becoming soferim, Talmud Gittin 45b states: "Sifrei Torah, tefillin and mezuzot written by a heretic, a star-worshipper, a slave, a woman, a minor, a Cuthean, or an apostate Jew, are unfit for ritual use." The rulings on Mezuzah and Tefillin are virtually undisputed among those who hold to the Talmudic Law. While Arba'ah Turim does not include women in its list of those ineligible to write Sifrei Torah, some see this as proof that women are permitted to write a Torah scroll. However today, virtually all Orthodox (both Modern and Ultra) authorities contest the idea that a woman is permitted to write a Sefer Torah. Yet women are permitted to inscribe Ketubot (marriage contracts), STaM not intended for ritual use, and other writings of Sofrut beyond simple STaM. In 2003 Canadian Aviel Barclay became the world's first known traditionally trained female sofer. In 2007 Jen Taylor Friedman, a British woman, became the first female sofer to scribe a Sefer Torah. In 2010 the first Sefer Torah scribed by a group of women (six female sofers, who were from Brazil, Canada, Israel, and the United States) was completed; this was known as the Women's Torah Project.
From October 2010 until spring 2011, Julie Seltzer, one of the female sofers from the Women's Torah Project, scribed a Sefer Torah as part of an exhibition at the Contemporary Jewish Museum in San Francisco. This makes her the first American female sofer to scribe a Sefer Torah; Julie Seltzer was born in Philadelphia and is non-denominationally Jewish. From spring 2011 until August 2012 she scribed another Sefer Torah, this time for the Reform congregation Beth Israel in San Diego. Seltzer was taught mostly by Jen Taylor Friedman. On September 22, 2013, Congregation Beth Elohim of New York dedicated a new Torah, which members of Beth Elohim said was the first Torah in New York City to be completed by a woman. The Torah was scribed by Linda Coppleson. As of 2014, there are an estimated 50 female sofers in the world.
External links.
General
Publications
Particular issues

</doc>
<doc id="26207" url="http://en.wikipedia.org/wiki?curid=26207" title="Robert Herrick">
Robert Herrick

Robert Herrick may refer to:

</doc>
<doc id="26211" url="http://en.wikipedia.org/wiki?curid=26211" title="Racketeer Influenced and Corrupt Organizations Act">
Racketeer Influenced and Corrupt Organizations Act

The Racketeer Influenced and Corrupt Organizations Act, commonly referred to as the RICO Act or simply RICO, is a United States federal law that provides for extended criminal penalties and a civil cause of action for acts performed as part of an ongoing criminal organization. The RICO Act focuses specifically on racketeering, and it allows the "leaders" of a syndicate to be tried for the crimes which they "ordered" others to do or assisted them, closing a perceived loophole that allowed someone who told a man to, for example, murder, to be exempt from the trial because he did not actually commit the crime personally.
RICO was enacted by section 901(a) of the Organized Crime Control Act of 1970 (, 84 Stat. , enacted  15, 1970). RICO is codified as Chapter 96 of Title 18 of the United States Code, 18 U.S.C. § 1961–1968. G. Robert Blakey, an adviser to the United States Senate Government Operations Committee, drafted the law under the close supervision of the committee's chairman, Senator John Little McClellan. It was enacted as Title IX of the Organized Crime Control Act of 1970, and signed into law by Richard M. Nixon. While its original use in the 1970s was to prosecute the Mafia as well as others who were actively engaged in organized crime, its later application has been more widespread.
Beginning in 1972, 33 States adopted state RICO laws to be able to prosecute similar conduct.
Summary.
Under RICO, a person who has committed "at least two acts of racketeering activity" drawn from a list of 35 crimes—27 federal crimes and 8 state crimes—within a 10-year period can be charged with racketeering if such acts are related in one of four specified ways to an "enterprise". Those found guilty of racketeering can be fined up to $25,000 and sentenced to 20 years in prison per racketeering count. In addition, the racketeer must forfeit all ill-gotten gains and interest in any business gained through a pattern of "racketeering activity."
When the U.S. Attorney decides to indict someone under RICO, he or she has the option of seeking a pre-trial restraining order or injunction to temporarily seize a defendant's assets and prevent the transfer of potentially forfeitable property, as well as require the defendant to put up a performance bond. This provision was placed in the law because the owners of Mafia-related shell corporations often absconded with the assets. An injunction and/or performance bond ensures that there is something to seize in the event of a guilty verdict.
In many cases, the threat of a RICO indictment can force defendants to plead guilty to lesser charges, in part because the seizure of assets would make it difficult to pay a defense attorney. Despite its harsh provisions, a RICO-related charge is considered easy to prove in court, as it focuses on patterns of behavior as opposed to criminal acts.
RICO also permits a private individual "damaged in his business or property" by a "racketeer" to file a civil suit. The plaintiff must prove the existence of an "enterprise". The defendant(s) are not the enterprise; in other words, the defendant(s) and the enterprise are not one and the same. There must be one of four specified relationships between the defendant(s) and the enterprise: either the defendant(s) invested the proceeds of the pattern of racketeering activity into the enterprise; or the defendant(s) acquired or maintained an interest in, or control over, the enterprise through the pattern of racketeering activity; or the defendant(s) conducted or participated in the affairs of the enterprise "through" the pattern of racketeering activity; or the defendant(s) conspired to do one of the above. In essence, the enterprise is either the 'prize,' 'instrument,' 'victim,' or 'perpetrator' of the racketeers. A civil RICO action can be filed in state or federal court.
Both the criminal and civil components allow the recovery of treble damages (damages in triple the amount of actual/compensatory damages).
Although its primary intent was to deal with organized crime, Blakey said that Congress never intended it to merely apply to the Mob. He once told "Time," "We don't want one set of rules for people whose collars are blue or whose names end in vowels, and another set for those whose collars are white and have Ivy League diplomas."
Initially, prosecutors were skeptical of using RICO, mainly because it was unproven. However, during the 1980s and 1990s, federal prosecutors utilized the law to bring charges against several Mafia figures. The first major success was the Mafia Commission Trial, which resulted in several top leaders of New York City's Five Families getting what amounted to life sentences. By the turn of the century, RICO cases resulted in virtually all of the top leaders of the New York Mafia being sent to prison.
State laws.
Beginning in 1972, 33 states, as well as Puerto Rico and the US Virgin Islands, adopted state RICO laws to cover additional state offenses under a similar scheme.
RICO predicate offenses.
Under the law, the meaning of racketeering activity is set out at #redirect . As currently amended it includes:
Pattern of racketeering activity requires at least two acts of racketeering activity, one of which occurred after the effective date of this chapter and the last of which occurred within ten years (excluding any period of imprisonment) after the commission of a prior act of racketeering activity. The U.S. Supreme Court has instructed federal courts to follow the continuity-plus-relationship test in order to determine whether the facts of a specific case give rise to an established pattern. Predicate acts are related if they "have the same or similar purposes, results, participants, victims, or methods of commission, or otherwise are interrelated by distinguishing characteristics and are not isolated events." ("H.J. Inc. v. Northwestern Bell Telephone Co.") Continuity is both a closed and open ended concept, referring to either a closed period of conduct, or to past conduct that by its nature projects into the future with a threat of repetition.
Application of RICO laws.
Although some of the RICO predicate acts are extortion and blackmail, one of the most successful applications of the RICO laws has been the ability to indict or sanction individuals for their behavior and actions committed against witnesses and victims in alleged retaliation or retribution for cooperating with federal law enforcement or intelligence agencies.
Violations of the RICO laws can be alleged in civil lawsuit cases or for criminal charges. In these instances charges can be brought against individuals or corporations in retaliation for said individuals or corporations working with law enforcement. Further, charges can also be brought against individuals or corporations who have sued or filed criminal charges against a defendant.
Anti-SLAPP (strategic lawsuit against public participation) laws can be applied in an attempt to curb alleged abuses of the legal system by individuals or corporations who utilize the courts as a weapon to retaliate against whistle blowers, victims, or to silence another's speech. RICO could be alleged if it can be shown that lawyers and/or their clients conspired and collaborated to concoct fictitious legal complaints solely in retribution and retaliation for themselves having been brought before the courts.
Although the RICO laws may cover drug trafficking crimes in addition to other more traditional RICO predicate acts such as extortion, blackmail, and racketeering, large-scale and organized drug networks are now commonly prosecuted under the Continuing Criminal Enterprise Statute, also known as the "Kingpin Statute". The CCE laws target only traffickers who are responsible for long-term and elaborate conspiracies; whereas the RICO law covers a variety of organized criminal behaviors.
Famous cases.
Hells Angels Motorcycle Club.
In 1979, the United States federal government went after Sonny Barger and several members and associates of the Oakland charter of the Hells Angels using RICO. In "United States v. Barger", the prosecution team attempted to demonstrate a pattern of behavior to convict Barger and other members of the club of RICO offenses related to guns and illegal drugs. The jury acquitted Barger on the RICO charges with a hung jury on the predicate acts: "There was no proof it was part of club policy, and as much as they tried, the government could not come up with any incriminating minutes from any of our meetings mentioning drugs and guns."
Frank Tieri.
On November 21, 1980, Genovese crime family boss Frank "Funzi" Tieri was the first Mafia boss to be convicted under the RICO Act.
Catholic sex abuse cases.
In some jurisdictions, RICO suits have been filed against Catholic dioceses, using racketeering laws to prosecute the highers-up in the episcopacy for abuses committed by those under their authority. A Cleveland grand jury cleared two bishops of racketeering charges, finding that their mishandling of sex abuse claims did not amount to criminal racketeering. Certain lawyers and abuse advocates have openly wondered why a similar suit was not filed against archbishop Bernard Law prior to his getting reassigned to Vatican City.
Gil Dozier.
Louisiana Commissioner of Agriculture and Forestry Gil Dozier, in office from 1976 to 1980, faced indictment with violations of both the Hobbs and the RICO laws. He was accused of compelling companies doing business with his department to make campaign contributions on his behalf. On September 23, 1980, the Baton Rouge-based United States District Court for the Middle District of Louisiana convicted Dozier of five counts of extortion and racketeering. The sentence of ten years imprisonment, later upgraded to eighteen when other offenses were determined, and a $25,000 fine was suspended pending appeal, and Dozier remained free on bail. He eventually served nearly four years until a presidential commutation freed him in 1986.
Key West PD.
About June 1984, the Key West Police Department located in the County of Monroe, Florida, was declared a criminal enterprise under the federal RICO statutes after a lengthy United States Department of Justice investigation. Several high-ranking officers of the department, including Deputy Police Chief Raymond Cassamayor, were arrested on federal charges of running a protection racket for illegal cocaine smugglers. At trial, a witness testified he routinely delivered bags of cocaine to the Deputy Chief's office at City Hall.
Michael Milken.
On March 29, 1989, American financier Michael Milken was indicted on 98 counts of racketeering and fraud relating to an investigation into an allegation of insider trading and other offenses. Milken was accused of using a wide-ranging network of contacts to manipulate stock and bond prices. It was one of the first occasions that a RICO indictment was brought against an individual with no ties to organized crime. Milken pled guilty to six lesser felonies of securities fraud and tax evasion rather than risk spending the rest of his life in prison, and ended up serving 22 months in prison. Milken was also ordered banned for life from the securities industry.
On September 7, 1988, Milken's employer, Drexel Burnham Lambert, was threatened with RICO charges "respondat superior", the legal doctrine that corporations are responsible for their employees' crimes. Drexel avoided RICO charges by entering an Alford plea to lesser felonies of stock parking and stock manipulation. In a carefully worded plea, Drexel said it was "not in a position to dispute the allegations" made by the government. If Drexel had been indicted for RICO violations, it would have had to post a performance bond of up to $1 billion to avoid having its assets frozen. This would have taken precedence over all of the firm's other obligations—including the loans that provided 96 percent of its capital. If the bond ever had to be paid, its shareholders would have been practically wiped out. Since banks will not extend credit to a firm indicted under RICO, an indictment would have likely put Drexel out of business. By at least one estimate, a RICO indictment would have destroyed the firm within a month. Years later, Drexel president and CEO Fred Joseph said that Drexel had no choice but to plead guilty because "a financial institution cannot survive a RICO indictment."
Major League Baseball.
In 2002, the former minority owners of the Montreal Expos baseball team filed charges under the RICO Act against Major League Baseball commissioner Bud Selig and former Expos owner Jeffrey Loria, claiming that Selig and Loria deliberately conspired to devalue the team for personal benefit in preparation for a move. If found liable, Major League Baseball could have been responsible for up to $300 million in punitive damages. The case lasted two years, successfully stalling the Expos' move to Washington or contraction during that time. It was eventually sent to arbitration where the arbiters ruled in favor of Major League Baseball, permitting the move to Washington to take place.
Pro-life activists.
RICO laws were successfully cited in "NOW v. Scheidler", 510 U.S. 249, 114 S. Ct. 798, 127 L.Ed. 2d 99 (1994), a suit in which certain parties, including the National Organization for Women, sought damages and an injunction against pro-life activists who physically block access to abortion clinics. The Court held that a RICO enterprise does not need an economic motive, and that the Pro-Life Action Network could therefore qualify as a RICO enterprise. The Court remanded for consideration of whether PLAN committed the requisite acts in a pattern of racketeering activity.
Los Angeles Police Department.
In April 2000, Federal judge William J. Rea in Los Angeles, ruling in one Rampart scandal case, said that the plaintiffs could pursue RICO claims against the LAPD, an unprecedented finding. The idea that a police organization could be characterized as a racketeering enterprise shook up City Hall and further damaged the already-tarnished image of the LAPD. However, in July 2001, U.S. District Judge Gary A. Feess said that the plaintiffs do not have standing to sue the LAPD under RICO because they are alleging personal injuries, rather than economic or property damage.
Mohawk Industries.
On April 26, 2006, the Supreme Court heard "Mohawk Industries, Inc. v. Williams", No. , 547 U.S. (2006), which concerned what sort of corporations fell under the scope of RICO. Mohawk Industries had allegedly hired illegal aliens, in violation of RICO. The court was asked to decide whether Mohawk Industries, along with recruiting agencies, constitutes an 'enterprise' that can be prosecuted under RICO, but in June of that year dismissed the case and remanded it to Court of Appeals.
Latin Kings.
On August 20, 2006, in Tampa, Florida, most of the state leadership members of the street gang, the Latin Kings, were arrested in connection with RICO conspiracy charges to engage in racketeering and currently await trial. The operation, called "Broken Crown", targeted statewide leadership of the Latin Kings. The raid occurred at the Caribbean American Club. Along with Hillsborough County Sheriff’s Office, Tampa Police Department, the State Attorney’s Office, the FBI, Immigration and Customs Enforcement, and the federal Bureau of Alcohol, Tobacco and Firearms were involved in the operation. Included in the arrest were leader Gilberto Santana from Brooklyn NY, Captain Luis Hernandez from Miami FL, Affiliate Celina Hernandez, Affiliate Michael Rocca, Affiliate Jessica Ramirez, Affiliate Reinaldo Arroyo, Affiliate Samual Alvarado, Omari Tolbert, Edwin DeLeon, and many others, totaling 39.
Gambino crime family.
Also, in Tampa, on October 16, 2006, four members of the Gambino crime family (Capo Ronald Trucchio, Terry Scaglione, Steven Catallono and associate Kevin McMahon) were tried under RICO statutes, found guilty and sentenced to life in prison.
Lucchese Crime Family.
In the mid 1990s, prosecuting attorneys Gregory O’Connell and Charles Rose used RICO charges to bring down the Lucchese family within an 18-month period. Dismantling the Lucchese family had a profound financial impact on previously Mafia held businesses such as construction, garment, and garbage hauling. Here they dominated and extorted money through taxes, dues, and fees. An example of this extortion was through the garbage business. Hauling of garbage from the World Trade Center cost the building owners $1.2 million per year to be removed when the Mafia monopolized the business, as compared to $150,000 per year when competitive bids could be sought.
Chicago Outfit.
In 2005, the U.S. Department of Justice's Operation Family Secrets indicted 15 Outfit members and associates under RICO predicates. Five defendants were convicted of RICO violations and other crimes. Six plead guilty, two died before trial and one was too sick to be tried.
Michael Conahan and Mark Ciavarella.
A federal grand jury in the Middle District of Pennsylvania handed down a 48-count indictment against former Luzerne County Court of Common Pleas Judges Michael Conahan and Mark Ciavarella. The judges were charged with RICO after allegedly committing acts of wire fraud, mail fraud, tax evasion, money laundering, and honest services fraud. The judges were accused of taking kickbacks for housing juveniles, that the judges convicted of mostly petty crimes, at a private detention center. The incident was dubbed by many local and national newspapers as the "Kids for cash scandal". On February 18, 2011, a federal jury found Michael Ciavarella guilty of racketeering because of his involvement in accepting illegal payments from Robert Mericle, the developer of PA Child Care, and Attorney Robert Powell, a co-owner of the facility. Ciavarella is facing 38 other counts in federal court.
Scott W. Rothstein.
Scott W. Rothstein is a disbarred lawyer and the former managing shareholder, chairman, and chief executive officer of the now-defunct Rothstein Rosenfeldt Adler law firm. He is accused of funding his philanthropy, political contributions, law firm salaries, and an extravagant lifestyle with a massive 1.2 billion dollar Ponzi scheme. On December 1, 2009, Rothstein turned himself in to federal authorities and was subsequently arrested on charges related to the Racketeer Influenced and Corrupt Organizations Act (RICO). Although his arraignment plea was not guilty, Rothstein cooperated with the government and reversed his plea to guilty of five federal crimes on January 27, 2010. Bond was denied by U.S. Magistrate Judge Robin Rosenbaum, who ruled that due to his ability to forge documents, he was considered a flight risk. On June 9, 2010, Rothstein received a 50-year prison sentence after a hearing in federal court in Fort Lauderdale.
AccessHealthSource.
Eleven defendants were indicted on RICO charges for allegedly assisting AccessHealthSource, a local health care provider, in obtaining and maintaining lucrative contracts with local and state government entities in the city of El Paso, Texas, “through bribery of and kickbacks to elected officials or himself and others, extortion under color of authority, fraudulent schemes and artifices, false pretenses, promises and representations and deprivation of the right of citizens to the honest services of their elected local officials” (see indictment).
International equivalents to RICO.
The US RICO legislation has other equivalents in the rest of the world. In spite of Interpol having a standardized definition of RICO-like crimes, the interpretation and national implementation in legislation (and enforcement) widely varies. Most nations do cooperate with the US on RICO enforcement only where their own related laws are specifically broken, but this is in line with the Interpol protocols for such matters.
By nation, alphabetically
Without other nations enforcing similar legislation to RICO many cross border RICO cases would not be possible. In the overall body of RICO cases that went to trial, at least 50% have had some non-US enforcement component to them. The offshoring of money away from the US finance system as part racketeering (and especially money laundering) is typically a major contributing factor to this.
However, other countries have laws that enable the government to seize property with unlawful origins. Mexico and Colombia both have specific laws that define the participation in criminal organizations as a separate crime, and separate laws that allow the seizure of goods related with these crimes. This latter provides a specific chapter titled "International Cooperation", which instructs Mexican authorities to cooperate with foreign authorities with respect to organized crime assets within Mexico, and provides the framework by which Mexican authorities may politely request the cooperation of foreign authorities with respect to assets located outside of Mexico, in terms of any international instruments they may be party to.
Arguably, this may be construed as allowing the application of the RICO Act in Mexico, provided the relevant international agreements exist among Mexico and countries with RICO or RICO-equivalent provisions.

</doc>
<doc id="26213" url="http://en.wikipedia.org/wiki?curid=26213" title="Rhombicuboctahedron">
Rhombicuboctahedron

In geometry, the rhombicuboctahedron, or small rhombicuboctahedron, is an Archimedean solid with eight triangular and eighteen square faces. There are 24 identical vertices, with one triangle and three squares meeting at each. (Note that six of the squares only share vertices with the triangles while the other twelve share an edge.) The polyhedron has octahedral symmetry, like the cube and octahedron. Its dual is called the deltoidal icositetrahedron or trapezoidal icositetrahedron, although its faces are not really true trapezoids.
The name "rhombicuboctahedron" refers to the fact that twelve of the square faces lie in the same planes as the twelve faces of the rhombic dodecahedron which is dual to the cuboctahedron. "Great rhombicuboctahedron" is an alternative name for a truncated cuboctahedron, whose faces are parallel to those of the (small) rhombicuboctahedron.
It can also be called an "expanded cube" or "cantellated cube" or a "cantellated octahedron" from truncation operations of the uniform polyhedron.
Geometric relations.
There are distortions of the rhombicuboctahedron that, while some of the faces are not regular polygons, are still vertex-uniform. Some of these can be made by taking a cube or octahedron and cutting off the edges, then trimming the corners, so the resulting polyhedron has six square and twelve rectangular faces. These have octahedral symmetry and form a continuous series between the cube and the octahedron, analogous to the distortions of the rhombicosidodecahedron or the tetrahedral distortions of the cuboctahedron. However, the rhombicuboctahedron also has a second set of distortions with six rectangular and sixteen trapezoidal faces, which do not have octahedral symmetry but rather Th symmetry, so they are invariant under the same rotations as the tetrahedron but different reflections.
The lines along which a Rubik's Cube can be turned are, projected onto a sphere, similar, topologically identical, to a rhombicuboctahedron's edges. In fact, variants using the Rubik's Cube mechanism have been produced which closely resemble the rhombicuboctahedron.
The rhombicuboctahedron is used in three uniform space-filling tessellations: the cantellated cubic honeycomb, the runcitruncated cubic honeycomb, and the runcinated alternated cubic honeycomb.
Dissection.
The rhombicuboctahedron dissected into two square cupolae and a central octagonal prism. A rotation of one cupola creates the "pseudo­rhombi­cubocta­hedron". Both of these polyhedra have the same vertex figure: 3.4.4.4".
There are three pairs of parallel planes that each intersect the rhombicuboctahedron in a regular octagon. The rhombicuboctahedron may be divided along any of these to obtain an octagonal prism with regular faces and two additional polyhedra called square cupolae, which count among the Johnson solids; it is thus an "elongated square orthobicupola". These pieces can be reassembled to give a new solid called the elongated square gyrobicupola or "pseudorhombicuboctahedron", with the symmetry of a square antiprism. In this the vertices are all locally the same as those of a rhombicuboctahedron, with one triangle and three squares meeting at each, but are not all identical with respect to the entire polyhedron, since some are closer to the symmetry axis than others.
Orthogonal projections.
The "rhombicuboctahedron" has six special orthogonal projections, centered, on a vertex, on two types of edges, and three types of faces: triangles, and two squares. The last two correspond to the B2 and A2 Coxeter planes.
Spherical tiling.
The rhombicuboctahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.
Pyritohedral symmetry.
A half symmetry form of the rhombicuboctahedron, , exists with pyritohedral symmetry, [4,3+], (3*2) as Coxeter diagram , Schläfli symbol s2{3,4}, and can be called a "cantic snub octahedron". This form can be visualized by alternatingly coloring the edges of the 6 squares. These squares can then be distorted into rectangles, while the 8 triangles remain equilateral. The 12 diagonal square faces will become isosceles trapezoids. In the limit, the rectangles can be reduced to edges, and the trapezoids become triangles, and a icosahedron is formed, by a "snub octahedron" construction, , s{3,4}.
Algebraic properties.
Cartesian coordinates.
Cartesian coordinates for the vertices of a rhombicuboctahedron centred at the origin, with edge length 2 units, are all permutations of 
If the original rhombicuboctahedron has unit edge length, its dual strombic icositetrahedron has edge lengths
Area and volume.
The area "A" and the volume "V" of the rhombicuboctahedron of edge length "a" are:
Close-packing density.
The optimal packing fraction of rhombicuboctahedra is given by
It was noticed that this optimal value is obtained in a Bravais lattice by de Graaf (2011). Since the rhombicuboctahedron is contained in a rhombic dodecahedron whose
inscribed sphere is identical to its own inscribed sphere, the value of the optimal packing fraction is a corollary of the Kepler conjecture: it can be achieved by putting a rhombicuboctahedron in each cell of the rhombic dodecahedral honeycomb, and it cannot be surpassed, since otherwise the optimal packing density of spheres could be surpassed by putting a sphere in each rhombicuboctahedron of the hypothetical packing which surpasses it.
In the arts.
The large polyhedron in the 1495 portrait of Luca Pacioli, traditionally though controversially attributed to Jacopo de' Barbari, is a glass rhombicuboctahedron half-filled with water.
The first printed version of the rhombicuboctahedron was by Leonardo da Vinci and appeared in his 1509 "Divina Proportione".
A spherical 180×360° panorama can be projected onto any polyhedron; but the rhombicuboctahedron provides a good enough approximation of a sphere while being easy to build. This type of projection, called "Philosphere", is possible from some panorama assembly software. It consists of two images that are printed separately and cut with scissors while leaving some flaps for assembly with glue.
Games and toys.
The Freescape games "Driller" and "Dark Side" both had a game map in the form of a rhombicuboctahedron.
A level in the videogame "Super Mario Galaxy" has a planet in the similar shape of a rhombicuboctahedron.
"Sonic the Hedgehog 3"'s Icecap Zone features pillars topped with rhombicuboctahedra.
During the Rubik's Cube craze of the 1980s, one combinatorial puzzle sold had the form of a rhombicuboctahedron (the mechanism was of course that of a Rubik's Cube).
The Rubik's Snake toy was usually sold in the shape of a stretched rhombicuboctahedron (12 of the squares being replaced with 1:√2 rectangles).
Related polyhedra.
The rhombicuboctahedron is one of a family of uniform polyhedra related to the cube and regular octahedron.
This polyhedron is topologically related as a part of sequence of cantellated polyhedra with vertex figure (3.4.n.4), and continues as tilings of the hyperbolic plane. These vertex-transitive figures have (*n32) reflectional symmetry.
Vertex arrangement.
It shares its vertex arrangement with three nonconvex uniform polyhedra: the stellated truncated hexahedron, the small rhombihexahedron (having the triangular faces and six square faces in common), and the small cubicuboctahedron (having twelve square faces in common).
Rhombicuboctahedral graph.
In the mathematical field of graph theory, a rhombicuboctahedral graph is the graph of vertices and edges of the rhombicuboctahedron, one of the Archimedean solids. It has 24 vertices and 48 edges, and is a quartic graph Archimedean graph.
References.
</dl>

</doc>
<doc id="26214" url="http://en.wikipedia.org/wiki?curid=26214" title="Reverse transcriptase">
Reverse transcriptase

A Reverse transcriptase (RT) is an enzyme used to generate complementary DNA (cDNA) from an RNA template, a process termed "reverse transcription". It is mainly associated with retroviruses. It should be noted however that also non-retroviruses use RT (for example, the hepatitis B virus, a member of the Hepadnaviridae, which are dsDNA-RT viruses, while retroviruses are ssRNA viruses). RT inhibitors are widely used as antiretroviral drugs. RT activities are also associated with the replication of chromosome ends (telomerase) and some mobile genetic elements (retrotransposons).
Retroviral RT has three sequential biochemical activities: 
These activities are used by the retrovirus to convert single-stranded genomic RNA into double-stranded cDNA which can integrate into the host genome, potentially generating a long-term infection that can be very difficult to eradicate. The same sequence of reactions is widely used in the laboratory to convert RNA to DNA for use in molecular cloning, RNA sequencing, polymerase chain reaction (PCR), or genome analysis.
Well studied reverse transcriptases include:
History.
Reverse transcriptases were discovered by Howard Temin at the University of Wisconsin–Madison in RSV virions, and independently isolated by David Baltimore in 1970 at MIT from two RNA tumour viruses: R-MLV and again RSV. For their achievements, both shared the 1975 Nobel Prize in Physiology or Medicine (with Renato Dulbecco).
The idea of reverse transcription was very unpopular at first as it contradicted the central dogma of molecular biology which states that DNA is transcribed into RNA which is then translated into proteins. However, in 1970 when the scientists Howard Temin and David Baltimore both independently discovered the enzyme responsible for reverse transcription, named reverse transcriptase, the possibility that genetic information could be passed on in this manner was finally accepted.
Function in viruses.
The enzymes are encoded and used by reverse-transcribing viruses, which use the enzyme during the process of replication. Reverse-transcribing RNA viruses, such as retroviruses, use the enzyme to reverse-transcribe their RNA genomes into DNA, which is then integrated into the host genome and replicated along with it. Reverse-transcribing DNA viruses, such as the hepadnaviruses, can allow RNA to serve as a template in assembling, and making DNA strands. HIV infects humans with the use of this enzyme. Without reverse transcriptase, the viral genome would not be able to incorporate into the host cell, resulting in failure to replicate.
Process of reverse transcription.
Reverse transcriptase creates single-stranded DNA from an RNA template.
In virus species with reverse transcriptase lacking DNA-dependent DNA polymerase activity, creation of double-stranded DNA can possibly be done by host-encoded DNA polymerase δ, mistaking the viral DNA-RNA for a primer and synthesizing a double-stranded DNA by similar mechanism as in primer removal, where the newly synthesized DNA displaces the original RNA template.
The process of reverse transcription is extremely error-prone and it is during this step that mutations may occur. Such mutations may cause drug resistance.
Retroviral reverse transcription.
Retroviruses, also referred to as class VI ssRNA-RT viruses, are RNA reverse transcribing viruses with a DNA intermediate. Their genomes consist of two molecules of positive-sense single stranded RNA with a 5' cap and 3' polyadenylated tail. Examples of retroviruses include the human immunodeficiency virus (HIV) and the human T-lymphotropic virus (HTLV). Creation of double-stranded DNA occurs in the cytosol as a series of these steps:
Creation of double-stranded DNA also involves "strand transfer", in which there is a translocation of short DNA product from initial RNA dependent DNA synthesis to acceptor template regions at the other end of the genome, which are later reached and processed by the reverse transcriptase for its DNA-dependent DNA activity.
Retroviral RNA is arranged in 5’ terminus to 3’ terminus. The site where the primer is annealed to viral RNA is called the primer-binding site (PBS). The RNA 5’end to the PBS site is called U5, and the RNA 3’ end to the PBS is called the leader. The tRNA primer is unwound between 14 and 22 nucleotides and forms a base-paired duplex with the viral RNA at PBS. The fact that the PBS is located near the 5’ terminus of viral RNA is unusual because reverse transcriptase synthesize DNA from 3’ end of the primer in the 5’ to 3’ direction (with respect to the RNA template).Therefore, the primer and reverse transcriptase must be relocated to 3’ end of viral RNA. In order to accomplish this reposition, multiple steps and various enzymes including DNA polymerase, ribonuclease H(RNase H) and polynucleotide unwinding are needed.
The HIV reverse transcriptase also has ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that copies the sense cDNA strand into an "antisense" DNA to form a double-stranded viral DNA intermediate (vDNA).
In eukaryotes.
Self-replicating stretches of eukaryotic genomes known as retrotransposons utilize reverse transcriptase to move from one position in the genome to another via an RNA intermediate. They are found abundantly in the genomes of plants and animals. Telomerase is another reverse transcriptase found in many eukaryotes, including humans, which carries its own RNA template; this RNA is used as a template for DNA replication.
In prokaryotes.
Initial reports of reverse transcriptase in prokaryotes came as far back as 1971 (Beljanski et al., 1971a, 1972). These have since been broadly described as part of bacterial Retron msr RNAs, distinct sequences which code for reverse transcriptase, and are used in the synthesis of msDNA. In order to initiate synthesis of DNA, a primer is needed. In bacteria, the primer is synthesized during replication.
Evolutionary role.
Valerian Dolja of Oregon State argues that viruses due to their diversity have played an evolutionary role in the development of cellular life, with reverse transcriptase playing a central role.
Structure.
Reverse transcriptase enzymes include an RNA-dependent DNA polymerase and a DNA-dependent DNA polymerase, which work together to perform transcription. In addition to the transcription function, retroviral reverse transcriptases have a domain belonging to the RNase H family which is vital to their replication.
Replication fidelity.
There are three different replication systems during the life cycle of a retrovirus. First of all, the reverse transcriptase synthesizes viral DNA from viral RNA, and then from newly made complementary DNA strand. The second replication process occurs when host cellular DNA polymerase replicates the integrated viral DNA. Lastly, RNA polymerase II transcribes the proviral DNA into RNA which will be packed into virions. Therefore, mutation can occur during one or all of these replication steps.
Reverse transcriptase has a high error rate when transcribing RNA into DNA since, unlike most other DNA polymerases, it has no proofreading ability. This high error rate allows mutations to accumulate at an accelerated rate relative to proofread forms of replication. The commercially available reverse transcriptases produced by Promega are quoted by their manuals as having error rates in the range of 1 in 17,000 bases for AMV and 1 in 30,000 bases for M-MLV
Other than creating single nucleotide polymorphisms, reverse transcriptases have also been shown to be involved in processes such as transcript fusions, exon shuffling and creating artificial antisense transcripts. It has been speculated that this template switching activity of Reverse Transcriptase, which can be demonstrated completely "in vivo", may have been one of the causes for finding several thousand unannotated transcripts in the genomes of model organisms.
Applications.
Antiviral drugs.
As HIV uses reverse transcriptase to copy its genetic material and generate new viruses (part of a retrovirus proliferation circle), specific drugs have been designed to disrupt the process and thereby suppress its growth. Collectively, these drugs are known as reverse transcriptase inhibitors and include the nucleoside and nucleotide analogues zidovudine (trade name Retrovir), lamivudine (Epivir) and tenofovir (Viread), as well as non-nucleoside inhibitors, such as nevirapine (Viramune).
Molecular biology.
Reverse transcriptase is commonly used in research to apply the polymerase chain reaction technique to RNA in a technique called reverse transcription polymerase chain reaction (RT-PCR). The classical PCR technique can be applied only to DNA strands, but, with the help of reverse transcriptase, RNA can be transcribed into DNA, thus making PCR analysis of RNA molecules possible. Reverse transcriptase is used also to create cDNA libraries from mRNA. The commercial availability of reverse transcriptase greatly improved knowledge in the area of molecular biology, as, along with other enzymes, it allowed scientists to clone, sequence, and characterise RNA.
Reverse transcriptase has also been employed in insulin production. By inserting eukaryotic mRNA for insulin production along with reverse transcriptase into bacteria, the mRNA could be inserted into the prokaryote's genome. Large amounts of insulin can then be created, sidestepping the need to harvest pig pancreas and other such traditional sources. Directly inserting eukaryotic DNA into bacteria would not work because it carries introns, so would not translate successfully using the bacterial ribosomes. Processing in the eukaryotic cell during mRNA production removes these introns to provide a suitable template. Reverse transcriptase converted this edited RNA back into DNA so it could be incorporated in the genome.

</doc>
<doc id="26215" url="http://en.wikipedia.org/wiki?curid=26215" title="Riemann mapping theorem">
Riemann mapping theorem

In complex analysis, the Riemann mapping theorem states that if "U" is a non-empty simply connected open subset of the complex number plane C which is not all of C, then there exists a biholomorphic mapping "f" (i.e. a bijective holomorphic mapping whose inverse is also holomorphic) from "U" onto the open unit disk 
This mapping is known as a Riemann mapping.
Intuitively, the condition that "U" be simply connected means that "U" does not contain any “holes”. The fact that "f" is biholomorphic implies that it is a conformal map and therefore angle-preserving. Intuitively, such a map preserves the shape of any sufficiently small figure, while possibly rotating and scaling (but not reflecting) it.
Henri Poincaré proved that the map "f" is essentially unique: if "z"0 is an element of "U" and φ is an arbitrary angle, then there exists precisely one "f" as above such that "f"("z"0) = 0 and that the argument of the derivative of "f" at the point "z"0 is equal to φ. This is an easy consequence of the Schwarz lemma.
As a corollary of the theorem, any two simply connected open subsets of the Riemann sphere which both lack at least two points of the sphere can be conformally mapped into each other (because conformal equivalence is an equivalence relation).
History.
The theorem was stated (under the assumption that the boundary of "U" is piecewise smooth) by Bernhard Riemann in 1851 in his PhD thesis. Lars Ahlfors wrote once, concerning the original formulation of the theorem, that it was “ultimately formulated in terms which would defy any attempt of proof, even with modern methods”. Riemann's flawed proof depended on the Dirichlet principle (which was named by Riemann himself), which was considered sound at the time. However, Karl Weierstrass found that this principle was not universally valid. Later, David Hilbert was able to prove that, to a large extent, the Dirichlet principle is valid under the hypothesis that Riemann was working with. However, in order to be valid, the Dirichlet principle needs certain hypotheses concerning the boundary of "U" which are not valid for simply connected domains in general. Simply connected domains with arbitrary boundaries were first treated by William Fogg Osgood (1900).
The first proof of the theorem is due to Constantin Carathéodory, who published it in 1912. His proof used Riemann surfaces and it was simplified by Paul Koebe two years later in a way which did not require them.
Another proof, due to Lipót Fejér and to Frigyes Riesz, was published in 1922 and it was rather shorter than the previous ones. In this proof, like in Riemann's proof, the desired mapping was obtained as the solution of an extremal problem. The Fejér–Riesz proof was further simplified by Alexander Ostrowski and by Carathéodory.
Importance.
The following points detail the uniqueness and power of the Riemann mapping theorem:
A proof sketch.
Given "U" and a point "z"0 in "U", we want to construct a function "f" which maps "U" to the unit disk and "z"0 to 0. For this sketch, we will assume that "U" is bounded and its boundary is smooth, much like Riemann did. Write
where "g" = "u" + "iv" is some (to be determined) holomorphic function with real part "u" and imaginary part "v". It is then clear that "z"0 is the only zero of "f". We require |"f"("z")| = 1 for "z" ∈ ∂"U", so we need 
on the boundary. Since "u" is the real part of a holomorphic function, we know that "u" is necessarily a harmonic function; i.e., it satisfies Laplace's equation.
The question then becomes: does a real-valued harmonic function "u" exist that is defined on all of "U" and has the given boundary condition? The positive answer is provided by the Dirichlet principle. Once the existence of "u" has been established, the Cauchy–Riemann equations for the holomorphic function "g" allow us to find "v" (this argument depends on the assumption that "U" be simply connected). Once "u" and "v" have been constructed, one has to check that the resulting function "f" does indeed have all the required properties.
Uniformization theorem.
The Riemann mapping theorem can be generalized to the context of Riemann surfaces: If "U" is a simply-connected open subset of a Riemann surface, then "U" is biholomorphic to one of the following: the Riemann sphere, C or "D". This is known as the uniformization theorem.
Smooth Riemann mapping theorem.
In the case of a simply connected bounded domain with smooth boundary, the Riemann mapping function and all its derivatives extend by continuity to the closure of the domain. This can be proved using regularity properties of solutions of the Dirichlet boundary value problem, which follow either from the theory of Sobolev spaces for planar domains or from classical potential theory. Other methods for proving the smooth Riemann mapping theorem include the theory of kernel functions or the Beltrami equation.

</doc>
<doc id="26219" url="http://en.wikipedia.org/wiki?curid=26219" title="Rhodesia">
Rhodesia

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1970–1975
 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1978 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |style="padding-left:0;text-align:left;"| 1978 est.
 |- class="mergedbottomrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
Rhodesia (, ), commonly known from 1970 onwards as the Republic of Rhodesia, was an unrecognised state in southern Africa from 1965 to 1979. It comprised the region now known as Zimbabwe. The country, with its capital in Salisbury, was considered a "de facto" successor state to the former British colony of Southern Rhodesia (which had achieved responsible government in 1923).
During an effort to delay an immediate transition to black majority rule, Rhodesia's predominantly white government issued its own Unilateral Declaration of Independence (UDI) from the United Kingdom on 11 November 1965. The UDI administration initially sought recognition as an autonomous realm within the Commonwealth of Nations, but reconstituted itself as a republic in 1970.
Following a brutal guerrilla war fought with two rival militant African nationalist organisations (Robert Mugabe's ZANU and Joshua Nkomo's ZAPU), Rhodesian premier Ian Smith conceded to bi-racial democracy in 1978. However, a provisional government subsequently headed by Smith and his moderate colleague Abel Muzorewa failed in appeasing international critics or halting the bloodshed.
By December 1979, Muzorewa had replaced Smith as Prime Minister and secured an agreement with the militant nationalists, allowing Rhodesia to briefly revert to colonial status pending elections under a universal franchise. It finally achieved internationally recognised independence in April 1980; the nation was concurrently renamed the Republic of Zimbabwe.
A wholly landlocked area, Rhodesia was bordered by South Africa to the south, Bechuanaland (later Botswana) to the southwest, Zambia to the northwest, and Mozambique (a Portuguese province until 1975) to the east. The state was originally named after Cecil Rhodes, whose British South Africa Company acquired the land in the late 19th century.
Nomenclature.
The official name of the country, according to the constitution adopted concurrently with the Unilateral Declaration of Independence (UDI) in 1965, was Rhodesia. This was not the case under British law, however, which considered the territory's legal name to be Southern Rhodesia, the name given to the country in 1898 during the British South Africa Company's administration of the Rhodesias, and retained by the self-governing colony of Southern Rhodesia after the end of Company rule in 1923.
This naming dispute dated back to October 1964, when Northern Rhodesia became independent from Britain and concurrently changed its name to Zambia. The Southern Rhodesian colonial government in Salisbury felt that in the absence of a "Northern" Rhodesia, the continued use of "Southern" was superfluous. It passed legislation to become simply Rhodesia, but the British government refused to approve this on the grounds that the country's name was defined by British legislation and so could not be altered by the colonial government. Salisbury went on using the shortened name in an official manner nevertheless, while the British government continued referring to the country as Southern Rhodesia. This situation continued throughout the UDI period.
The British government maintained this stance regarding the June–December 1979 successor state of Zimbabwe Rhodesia, and when Zimbabwe Rhodesia returned to colonial status from December 1979 to April 1980, it was as "Southern Rhodesia". Southern Rhodesia subsequently gained international recognition of its independence in April 1980, when it became the Republic of Zimbabwe.
History.
Background.
Until after World War II, the landlocked British possession of Southern Rhodesia was not developed as an indigenous African territory, but rather as a unique state which reflected its multiracial character. This scenario certainly made it very different from other lands which existed under colonial rule, as many Europeans had arrived to make permanent homes, populating the towns as traders or settling to farm the most productive soils. In 1922, faced with the decision to join the Union of South Africa as a fifth province or accept nearly full internal self-autonomy, the electorate cast its vote against South African integration.
In view of the outcome of the referendum, the territory was annexed by the United Kingdom on 12 September 1923. Shortly after annexation, on 1 October 1923, the first constitution for the new Colony of Southern Rhodesia came into force. Under this constitution, Southern Rhodesia was given the right to elect its own thirty-member legislature, premier, and cabinet—although the British Crown retained a formal veto over measures affecting natives and dominated foreign policy. White residents, meanwhile, provided most of the colony's administrative, industrial, scientific, and farming skills in addition to owning half the land. They also established a relatively balanced economy, transforming what was once a primary producer dependent on backwoods farming into an industrial giant which spawned a strong manufacturing sector, iron and steel industries, and modern mining ventures. These economic successes owed little to foreign aid.
The Rhodesian authorities resisted the temptation to nationalise major enterprises without paying proper compensation, consistently refused to radically alleviate unemployment, and shied from filling civil service posts with partisan appointees. By colonial standards, public services were well-organised and praised for their efficiency.
In 1953, Southern Rhodesia merged with the two other British Central African states to form the Federation of Rhodesia and Nyasaland – a loose association that placed defence and economic direction under a central government but left many domestic affairs under the control of its constituent territories. As it began to appear that decolonisation was inevitable and indigenous black populations were pressing heavily for change, the federation was dissolved in 1963.
Unilateral Declaration of Independence (1965).
Although prepared to grant formal independence to Southern Rhodesia (now Rhodesia), the British government had adopted a policy of "no independence before majority rule", dictating that colonies with a substantial population of European settlers would not receive independence except under conditions of majority rule. Rhodesian colonials initially balked at the suggestion; some felt they had a right to absolute political control, at least for the time being, despite their relatively small numbers. The authorities were also disturbed by the post-independence chaos which was plaguing other African nations at the time. However, once Rhodesia had been introduced as a topic for discussion in international bodies, extension of the status quo became a matter of concern to the world community and a serious embarrassment to the United Kingdom.
After the federal break-up in 1963, then-Prime Minister Alec Douglas-Home insisted that preconditions on independence talks hinge what he termed the "five principles" – unimpeded progress to majority rule, assurance against any future legislation decidedly detrimental to black interests, "improvement in the political status" of local Africans, moves towards ending racial discrimination, and agreement on a settlement which could be "acceptable to the whole population". Harold Wilson and his incoming Labour government took an even harder line on demanding that these points be legitimately addressed before an independence agenda could be set.
By 1964, growing dissatisfaction with the ongoing negotiations ousted Salisbury's incumbent Winston Field, replacing him with Ian Smith, deputy chairman of the conservative Rhodesian Front party. Smith, the colony's first Rhodesian-born leader, soon came to personify resistance to liberals in British government and those agitating for change at home. He ruled out acceptance for all five of the proposed principles as they stood, implying instead that Rhodesia was already legally entitled to independence—a claim which was overwhelmingly endorsed by registered voters in a referendum.
Emboldened by the results of this referendum and the subsequent general election, Rhodesia now threatened to assume her own sovereignty without British consent. Harold Wilson countered by warning that such an irregular procedure would be considered treasonous, although he specifically rejected using armed force against the English "kith and kin" in Africa. Wilson's refusal to consider a military option encouraged Smith to proceed with his plans. Talks quickly broke down, and final efforts in October to achieve a settlement floundered; the Rhodesian Front remained unwilling to accept what were regarded as unacceptably drastic terms and the British would settle for nothing less – it was a formula doomed to failure.
On 11 November 1965, following a brief but solemn consensus, Rhodesia's leading statesmen issued their country's unilateral declaration of independence (UDI). This was immediately denounced as an "act of rebellion against the Crown" in the United Kingdom, and Wilson promised that the illegal action would be short-lived. However, few seemed to initially realise that Rhodesia was no longer within the Commonwealth's direct sphere of influence and British rule was now a constitutional fiction; Salisbury remained virtually immune to credible metropolitan leverage.
In October 1965, the United Nations Security Council had warned Whitehall about the possibility of UDI, urging Wilson to use all means at his disposal (including military pressure) to prevent the Rhodesian Front from asserting independence. After UDI was proclaimed, UN officials branded Ian Smith's government as an "illegal racist minority regime" and called on member states to sever economic ties with Rhodesia, recommending sanctions on petroleum products and military hardware. In December 1966, these measures became mandatory, extending to bar the purchase of Rhodesian tobacco, chrome, copper, asbestos, sugar, meat, and hides.
Britain, having already adopted extensive sanctions of its own, dispatched a Royal Navy squadron to monitor oil deliveries in the port of Beira, from which a strategic pipeline ran to Umtali. The warships were to deter "by force, if necessary, vessels reasonably believed to be carrying oil destined for (Southern) Rhodesia".
Some nations, such as Switzerland, and West Germany, which were not UN members, conducted business legally with Rhodesia – the latter remained the Smith government's largest trading partner in Western Europe until 1973, when Bonn joined the UN. Japan continued to accept more Rhodesian exports than any other nation, and Iran provided oil. The Portuguese government marketed Rhodesian products as its own, via false certificates of origin and disguised trade channels. South Africa openly refused to observe the UN sanctions. A 1971 amendment passed in the United States permitted American firms to go on importing Rhodesian chromium and nickel as normal.
Despite the poor showing of sanctions, Rhodesia found it nearly impossible to obtain diplomatic recognition abroad. In 1970, the US government had made it clear that the UDI would not be recognised "under [any] circumstances". Even Smith's ideological allies in Pretoria, although sympathetic, failed to recognise the new country on an equal level.
Initially, the state retained its pledged loyalty to Elizabeth II of the United Kingdom, recognising her as Queen of Rhodesia. After trying and failing to have Deputy Prime Minister Clifford Dupont named as Governor-General of Rhodesia, Smith designated Dupont as "Officer Administering the Government." The colonial Governor, Sir Humphrey Gibbs sacked the entire cabinet on orders from Whitehall and condemned the UDI as an act of treason. However, he was unable to enact any concrete actions to foster a return to legality. Government ministers simply ignored his notices, pointing out that UDI made his office obsolete. Even so, Gibbs continued to occupy his residence in Salisbury until 1970, when he vacated the premises and left Rhodesia following the declaration of a republic.
In September 1968, the Appellate Division of the Rhodesian High Court ruled that Ian Smith's administration had become the "de jure" government of the country, not merely the "de facto" one. To support his decision, Chief Justice Sir Hugh Beadle used several statements made by Hugo Grotius, who maintained that there was no way in which a nation could rightly claim to be governing a particular territory – if it was waging a war against that territory. Beadle argued that due to Britain's economic war against Rhodesia, she could not (at the same point) be described as "governing" Rhodesia. Resulting court decisions held that the rebel government "could lawfully do anything its predecessors could lawfully have done".
A Salisbury commission chaired by prominent lawyer W.R. Waley was appointed to study constitutional options open to the Rhodesian authorities as of April 1968, but reaching a further settlement with the British was ruled out early on. Waley, although insistent that "Europeans must surrender any belief in permanent European domination", also testified that majority rule was not immediately desirable and conceded that the government ought to remain in responsible hands for the near future.
Talks aimed at easing the differences between Rhodesia and the United Kingdom were carried out aboard Royal Navy vessels once in December 1966 and again in October 1968. Both efforts failed to achieve agreement, although Harold Wilson added a sixth principle to the five he had previously enunciated: "it would be necessary to ensure that, regardless of race, there was no oppression of the majority by the minority or of [any] minority by the majority." Rhodesian resolve stiffened following a failure to reach a new settlement, with more radical elements of the Rhodesian Front calling for a republican constitution.
During a two-proposition referendum held in 1969, the proposal for severing all remaining ties to the British Crown passed by a majority of 61,130 votes to 14,327. Rhodesia declared itself a republic on 2 March 1970. Under the new constitution, a president served as ceremonial head of state, with the prime minister nominally reporting to him. Some in Rhodesian government had vainly hoped that the declaration of a republic would finally prompt other nations to grant recognition.
Impact of UDI.
The years following Rhodesia's UDI saw an unfolding series of economic, military, and political pressures placed on the country which eventually brought about majority rule, a totality of these factors rather than any one the reason for introducing change. In 2005, a conference at the London School of Economics which discussed Rhodesia's independence concluded that UDI was sparked by an existing racial conflict complicated by Cold War intrigues.
Critics of UDI sought to maintain that Ian Smith intended only to safeguard the privileges of an entrenched colonial elite at the expense of the impoverished African community. According to this logic, UDI created a vacuum of oppression which was eventually filled by Robert Mugabe's present dictatorship. Smith and his supporters continued to defend their actions, however, by claiming that the Rhodesian majority was too inexperienced at the time to manage effectively a developed nation.
At large, the European population's emerging attitude to UDI was tense. Many white Rhodesians had seen themselves as nothing less than fully fledged members of the British Empire, carrying on the same rugged values and frontier spirit of the early Englishmen who had settled in 1890. But such confidence was rudely shaken by Whitehall's refusal to grant independence on their terms. After 1965, there were those who continued to claim that they were collectively upholders of principle and defenders of such values against the twin threats of communism, manifested through the militant black nationalists, and – ironically – the decadence of Britain herself. Often repeated appeals to the Christian heritage of their pioneer ancestors in "defending the free world" reflected these beliefs.
African parties displayed initial horror at Smith's declaration, with one ZANU official stating, "...for all those who cherish freedom and a meaningful life, UDI has set a collision course which cannot be altered. 11 November 1965 [has] marked the turning point of the struggle for freedom in that land from a constitutional and political one to primarily a military struggle." It would, however, be several years before even the most radical nationalists chose to develop a coherent strategy revolving around armed resistance, preferring instead to create opportunities for external intervention.
Because Rhodesian exports were generally competitive and had previously been entitled to preferential treatment on the British market, the former colony did not recognise the need for escalating the pace of diversification before independence. Following the UDI, however, Rhodesia began to demonstrate that she had the potential to develop a greater degree of economic self-sufficiency. After the Rhodesian Front began introducing incentives accorded to domestic production, industrial output expanded dramatically. A rigid system of countermeasures enacted to combat sanctions succeeded in blunting their impact for at least a decade. Over the next nine years Rhodesian companies, spiting the freezing of their assets and blocking of overseas accounts, also perfected cunning techniques of sanctions evasion through both local and foreign subsidiaries, which operated on a clandestine trade network.
From 1968 until 1970, there was virtually no further dialogue between Rhodesia and the UK. This altered immediately after the ascension of Edward Heath, who reopened negotiations. Smith remained optimistic that Heath would do his utmost to remedy Anglo-Rhodesian relations, although disappointed that he continued to adhere publicly to the original "five principles" proposed by Alec Douglas-Home, now foreign secretary. In November 1971, Douglas-Home renewed contacts with Salisbury and announced a proposed agreement which would be satisfactory to both sides – it recognised Rhodesia's 1969 constitution as the legal frame of government, while agreeing that gradual legislative representation was an acceptable formula for unhindered advance to majority rule. Nevertheless, the new settlement, if approved, would also implement an immediate improvement in black political status, offer a means to terminate racial discrimination, and provide a solid guarantee against retrogressive constitutional amendments.
Implementation of the proposed settlement hinged on popular acceptance, but the Rhodesian government consistently refused to submit it to a universal referendum. A twenty four-member commission headed by an eminent jurist, Lord Pearce, was therefore tasked with ascertaining public opinion on the subject. In 1972, the commission began interviewing interest groups and sampling opinions – although concern was expressed over the widespread apathy encountered. According to every published finding, whites were in favour of the settlement, and Rhodesians of mixed or Asian background generally pleased, while black reaction was resoundingly negative. As many as thirty African chiefs and politicians voiced their opposition, prompting Britain to withdraw from the proposals on the grounds of the commission's report.
The Bush War.
As early as 1960, minority rule in Southern Rhodesia was already being challenged by a rising tide of political violence led by African nationalists such as Joshua Nkomo and Ndabaningi Sithole. After their public campaigns were initially suppressed, many believed that negotiation was completely incapable of meeting their aspirations. Petrol bombings by radicals became increasingly common, with the "Zimbabwe Review" observing in 1961, "for the first time home-made petrol bombs were used by freedom fighters in Salisbury against settler establishments." It was officially noted that between January and September 1962 alone, 33 bombings were carried out, in addition to 27 acts of attempted sabotage on communications. In that same period, nationalists were implicated in arson targeting 18 schools and 10 churches. Nkomo's Zimbabwe African People's Union (ZAPU) subsequently disclosed that it had formed a military wing (Zimbabwe People's Revolutionary Army, ZIPRA), and 'the decision to start bringing in arms and ammunition and to send young men away for sabotage training' had already been made. The Rhodesian authorities responded by banning ZAPU and driving its supporters underground. Frustrated by their repeated failures, nationalists also conducted a campaign of terror against black Africans, murdering those who had either identified with the colonial administration or had simply failed to demonstrate their allegiance to the cause. To protect civilians, emergency laws were imposed, broadening the legal definition of unlawful gatherings and giving the police greater powers to crack down on agitators or subversives. The death sentence was also introduced for terrorism involving explosives and arson.
A crisis of confidence soon resulted across ZAPU, which was already suffering from poor morale, compounded by tribal and ideological factionalism. In 1963, party dissidents rejected Joshua Nkomo's authority to form their own organisation, the Zimbabwe African National Union (ZANU) – which worked out its own strategy for impressing international opinion, undermining white assurance, and achieving a complete breakdown of order. By August 1964, ZANU, which had brutally intimidated neutrals and opponents alike, was outlawed by authorities as well.
ZANU's agenda was inward-looking, leftist, and pan-Africanist in nature. Ndabaningi Sithole and avowed Marxist Robert Mugabe, its most prominent leaders, demanded a one party Zimbabwean state with total black rule and a public monopoly on land. After being forced from Rhodesia, they continued to operate in exile, creating occupation groups representing urban workers, miners, and peasant farmers. ZANU also attracted professionals, students, and feminists to its ranks. While ZAPU theoretically continued to command the allegiance of most Ndebele and Shona activists, Sithole and Mugabe drew their support from the Mashonaland countryside.
After the UDI, ZANU officials mapped an elaborate plan for the "liberation of Zimbabwe" which called for attacks on white farmers, destruction of cash crops, disrupting electricity in urban areas, and petrol bombings. Sithole and Nkomo both insisted on the need for armed struggle, but disagreed on the means to go about it. For example, ZAPU tended to follow Soviet thinking, placing an emphasis on sophisticated weaponry in the hopes of winning a conventional battle like the Viet Minh at Dien Bien Phu. ZANU preferred to politicise populations in areas which they intended to seize. Neither force, however, had acquired basic knowledge of guerrilla warfare. Debate on political theory and insurgent tactics became the obsession of nationalists at this stage.
In April 1966, two Zimbabwe African National Liberation Army (ZANLA – the military wing of ZANU) units, having received prior training at Nanjing Military College, crossed into Rhodesia from Zambia. They were armed with SKS carbines, Chinese hand grenades, explosives, and communist pamphlets, having been issued vague instructions to sabotage important installations before killing white persons indiscriminately. At least five guerrillas were simply arrested before getting very far. Another seven hoped to destroy a pylon carrying electricity to Sinoia in the northwest. Their faulty demolitions were uncovered by the Rhodesian Security Forces and the men easily tracked to a nearby ranch on 28 April, where they were shot resisting capture. This event is considered to be the first engagement of what came to be known as the "Bush War" in Rhodesia and the "Second "Chimurenga"" (or "rebellion" in Shona) by supporters of the guerrillas.
The campaign proper is generally considered to have started in 1972 with the Attack on Altena Farm, despite the minor threat already represented by the nationalist movements in the 1960s.
After unsuccessful appeals to Britain and the United States for military assistance to liberate Zimbabwe, Robert Mugabe, who was based in Mozambique after that country's independence from Portugal in 1975, led ZANU to seek support from the People's Republic of China and countries of the Soviet Bloc. Joshua Nkomo, based in Zambia and also supported by the Soviet Union, led ZAPU. ZANU and ZAPU together formed 'the Patriotic Front'. Broadly, ZANLA recruited mainly from Mashonaland and Manicaland provinces, whilst the ZIPRA recruited from Mashonaland West, Midlands and Matabeleland provinces of Zimbabwe.
After the collapse of Portuguese rule in Mozambique in 1974–75, it was no longer viable for the Smith regime to sustain white minority rule indefinitely. Even the South African Apartheid regime considered sustaining white minority rule in a nation in which blacks outnumbered whites by 22:1 as untenable. In 1978 there were 270,000 Rhodesians of European descent and more than six million Africans.
International business groups involved in the country (e.g. Lonrho) transferred their support from the Rhodesian government to black nationalist parties. Business leaders and politicians feted Nkomo on his visits to Europe. ZANU also attracted business supporters who saw the course that future events were likely to take. Funding and arms support provided by supporters, particularly from the Soviet Union and its allies in the latter 1970s, allowed both ZIPRA and the ZANLA to acquire more sophisticated weaponry, thereby increasing the military pressure that the guerrillas were able to place on Rhodesia.
Until 1972, containing the guerrillas was little more than a police action. Even as late as August 1975 when Rhodesian government and black nationalist leaders met at Victoria Falls for negotiations brokered by South Africa and Zambia, the talks never got beyond the procedural phase. Rhodesian representatives made it clear they were prepared to fight an all out war to prevent majority rule. However, the situation changed dramatically after the end of Portuguese colonial rule in Mozambique in 1975. Rhodesia now found itself almost entirely surrounded by hostile states and even South Africa, its only real ally, pressed for a settlement.
At this point, ZANU's alliance with FRELIMO (the Liberation Front of Mozambique) and the porous border between Mozambique and eastern Rhodesia enabled large-scale training and infiltration of ZANU/ZANLA fighters. The governments of Zambia and Botswana were also emboldened sufficiently to allow resistance movement bases to be set up in their territories. Guerrillas began to launch operations deep inside Rhodesia, attacking roads, railways, economic targets and isolated security force positions, in 1976.
The government adopted a 'strategic hamlets' policy of the kind used in Malaya and Vietnam to restrict the influence of insurgents over the population of rural areas. Local people were forced to relocate to protected villages (PVs) which were strictly controlled and guarded by the government against rebel atrocities. The protected villages were compared by the guerrillas to concentration camps. Some contemporary accounts claim that this interference in the lives of local residents induced many of them who had previously been neutral to support the guerrillas.
The war degenerated into rounds of increasing brutality from all three parties involved (ZANU and ZAPU, and the Rhodesian Army fighting off their attacks). Mike Subritzky, a former NZ Army ceasefire monitor in Rhodesia, in 1980 described the war as "both bloody and brutal and brought out the very worst in the opposing combatants on all three sides."
End of the Bush War.
Rhodesia began to lose vital economic and military support from South Africa, which, while sympathetic to the white minority government, never accorded it diplomatic recognition. The South African government placed limits on the fuel and munitions they supplied to the Rhodesian military. They also withdrew the personnel and equipment that they had previously provided to aid the war effort, though covert military support continued.
In 1976, the South African government and United States governments worked together to place pressure on Smith to agree to a form of majority rule. In response to the initiative of US Secretary of State Henry Kissinger, in 1976 Ian Smith accepted the principle of black majority rule within two years. The Rhodesians now offered more concessions, but those concessions, focused on reaching an "internal settlement" with moderate black leaders, were insufficient to end the war.
At the time, some Rhodesians said the still embittered history between the British-dominated Rhodesia and the Afrikaner-dominated South Africa partly led South African government to withdraw its aid to Rhodesia. Ian Smith said in his memoirs that even though many white South Africans supported Rhodesia, South African Prime Minister John Vorster's policy of détente with the Black African states ended up with Rhodesia being offered as the "sacrificial lamb" to buy more time for South Africa. Other observers perceived South Africa's distancing itself from Rhodesia as being an early move in the process that led to majority rule in South Africa itself.
In the latter 1970s, the militants had successfully put the economy of Rhodesia under significant pressure while the numbers of guerrillas in the country were steadily increasing. The government abandoned its early strategy of trying to defend the borders in favour of trying to defend key economic areas and lines of communication with South Africa, while the rest of the countryside became a patchwork of "no-go areas".
Late 1970s.
By the late 1970s, Rhodesia's front-line forces contained about 25,000 regular troops and police – backed up by relatively strong army and police reserves. Its armoured vehicles largely consisted of light armoured cars, complemented by eight tanks (Polish built T-55LD tanks), delivered in the last year of the war. The Rhodesian air force, in turn, operated an assortment of both Canberra light bombers, Hawker Hunter fighter bombers, older de Havilland Vampire jets as well as a somewhat antiquated, but still potent, helicopter arm. These forces, including highly trained special operations units, were capable of launching devastating raids on resistance movement camps outside the country, as in Operation Dingo in 1977 and other similar operations.
Nevertheless, guerrilla pressure inside the country itself was steadily increasing in the latter 1970s. By 1978–79, the war had become a contest between the guerrilla warfare placing ever increasing pressure on the Rhodesian regime and civil population and the Rhodesian government's strategy of trying to hold off the militants until external recognition for a compromise political settlement with moderate black leaders could be secured.
By this time, the need to cut a deal was apparent to most Rhodesians, but not to all. Ian Smith had dismissed his intransigent Defence Minister, P. K. van der Byl, as early as 1976. "PK" had been a hard-line opponent of any form of compromise with domestic opposition or the international community since before UDI.
Van der Byl eventually retired to his country estate outside Cape Town, but there were elements in Rhodesia, mainly embittered former security force personnel, who forcibly opposed majority rule up to and well beyond independence. New white immigrants continued to arrive in Rhodesia right up to the eve of independence.
Alleged biological attacks had little impact on the fighting capability of ZANLA, but caused considerable distress to the local population. Some former officers of the Rhodesian Security Forces stated that anthrax was covertly used by Rhodesian psy-ops units during the late 1970s, though not produced in Rhodesia; they stressed that Rhodesia did not have the capacity to produce such weapons, and that the cholera and anthrax had come from the South African government's top-secret chemical and biological weapons programme, Project Coast, which was using Rhodesia as a "laboratory".
The work of journalists such as Lord Richard Cecil, son of the Marquess of Salisbury, stiffened the morale of Rhodesians and their overseas supporters. Lord Richard produced regular news reports such as the Thames TV 'Frontline Rhodesia' features. These reports typically contrasted the incompetent insurgents with the "superbly professional" government troops. A group of ZANLA fighters killed Lord Richard on 20 April 1978 when he was accompanying Rhodesian airborne unit employed in Fire Force Operations.
The shooting down on 3 September 1978 of the civilian Vickers Viscount airliner "Hunyani", Air Rhodesia Flight RH825, in the Kariba area by ZIPRA fighters using a surface-to-air missile, with the subsequent massacre of its survivors, is widely considered to be the event that finally destroyed the Rhodesians' will to continue the war. Although militarily insignificant, the loss of this aircraft (and a second Viscount, the "Umniati", in 1979) demonstrated the reach of resistance movements extended to Rhodesian civil society.
The Rhodesians' means to continue the war were also eroding fast. In December 1978, a ZANLA unit penetrated the outskirts of Salisbury and fired a volley of rockets and incendiary device rounds into the main oil storage depot – the most heavily defended economic asset in the country. The storage tanks burned for five days, giving off a column of smoke that could be seen 80 mi away. Half a million barrels of petroleum product (comprising Rhodesia's strategic oil reserve) were lost.
The government's defence spending increased from R$30 million, 8.5% of the national budget in 1971 to 1972, to R$400 m in 1978 to 1979, 47% of the national budget. In 1980, the post-independence government of Zimbabwe inherited a US$500 million national debt.
End of UDI (1979).
The Rhodesian army continued its "mobile counter-offensive" strategy of holding key positions ("vital asset ground") while carrying out raids into the no-go areas and into neighbouring countries. While often extraordinarily successful in inflicting heavy guerrilla casualties, such raids also on occasion failed to achieve their objectives. In April 1979 special forces carried out a raid on Joshua Nkomo's residence in Lusaka (Zambia) with the stated intention of assassinating him. Nkomo and his family left hastily a few hours before the raid – having clearly been warned that the raid was coming. Rumours of treachery circulated within Rhodesia. It was variously suggested that the army command had been penetrated by British MI6 or that people in the Rhodesian establishment were positioning themselves for life after independence. The loyalty of the country's Central Intelligence Organization became suspect. 
In 1979, some special forces units were accused of using counterinsurgent operations as cover for ivory poaching and smuggling. Colonel Reid-Daly (commander of the Selous Scouts) discovered that his phone was bugged and after challenging a superior officer on this issue was court martialled for insubordination. He received the lightest sentence possible, a caution, but he continued to fight his conviction and eventually resigned his commission and left the Army.
By 1978–79, up to 70% of the regular army was composed of black soldiers (though both the army and police reserves remained overwhelmingly white). By 1979 there were also 30 black commissioned officers in the regular army. While there was never any suggestion of disloyalty among the soldiers from predominantly black units (in particular within the Selous Scouts or the Rhodesian African Rifles – RAR), some argue that, by the time of the 1980 election, many of the RAR soldiers voted for Robert Mugabe.
As the result of an internal settlement between the Rhodesian government and some urban-based African nationalist parties, which were not in exile and not involved in the war, elections were held in April 1979. The United African National Council (UANC) party won a majority in this election, and its leader, Abel Muzorewa (a United Methodist Church bishop), became the country's prime minister on 1 June 1979. The country's name was changed to Zimbabwe Rhodesia. The internal settlement left control of the country's police, security forces, civil service and judiciary in white hands, for the moment. It assured whites of about one third of the seats in parliament. It was essentially a power-sharing arrangement between whites and blacks which, in the eyes of many, particularly the insurgents, did not amount to majority rule. However, the United States Senate voted to end economic sanctions against Zimbabwe Rhodesia on 12 June.
While the 1979 election was described by the Rhodesian government as non-racial and democratic, it did not include the main nationalist parties ZANU and ZAPU. In spite of offers from Ian Smith, the latter parties declined to participate in an election in which their political position would be insecure and under a proposed constitution which they had played no part in drafting and which was perceived as retaining strong white minority privilege.
Bishop Muzorewa's government did not receive international recognition. The Bush War continued unabated and sanctions were not lifted. The international community refused to accept the validity of any agreement which did not incorporate the main nationalist parties. The British Government (then led by the recently elected Margaret Thatcher) issued invitations to all parties to attend a peace conference at Lancaster House. These negotiations took place in London in late 1979. The three-month-long conference almost failed to reach conclusion, due to disagreements on land reform, but resulted in the Lancaster House Agreement. UDI ended, and Rhodesia temporarily reverted to the status of a British colony (the'Colony of Southern Rhodesia').
The outcome was an internationally supervised general election in early 1980. ZANU (PF) led by Robert Mugabe won this election, some alleged, by terrorising opposition to ZANU, including supporters of ZAPU, through elements of ZANU that had not confined themselves to the designated guerrilla assembly points, as stipulated by the Lancaster House Agreement. The observers and the newly installed governor Lord Soames were accused of looking the other way, and Mugabe's victory was certified. Nevertheless, few could doubt that Mugabe's support within his majority Shona tribal group was extremely strong. The Rhodesian military seriously considered mounting a coup against a perceived stolen election ("Operation Quartz") to prevent ZANU from taking over the country. The alleged coup was to include the assassination of Mugabe and coordinated assaults on guerrilla assembly points throughout the country. The plan was eventually scuttled, as it was obvious that Mugabe enjoyed widespread support from the black majority despite voter intimidation, as well as the fact that the coup would gain no external support, and a conflagration which would engulf the country was seen as inevitable.
Republic of Zimbabwe (1980).
Mugabe and the victorious black nationalists were rather less concerned by Operation Quartz than by the possibility that there might be a mass exodus of the white community of the kind that had caused chaos in Mozambique five years earlier. Such an exodus had been prepared for by the South African government. With the agreement of the British Governor of Rhodesia, South African troops had entered the country to secure the road approaches to the Beit Bridge border crossing point. Refugee camps had been prepared in the Transvaal. On the day the election results became known, most white families had prepared contingency plans for flight, including the packing of cars and suitcases.
However, after a meeting with Robert Mugabe and the central committee of ZANU (PF), Ian Smith was reassured that whites could, and should stay in the new Zimbabwe. Mugabe promised that he would abide strictly by the terms of the Lancaster House Agreement and that changes in Zimbabwe would be made gradually and by proper legal process. In a CBS news interview, Mugabe claimed that Rhodesian whites "...are still in control of the economy, the majority being commercial farmers.", Mugabe however, would reverse his commitment to these agreements some years later; the regime began confiscating white owned farm lands. This is widely blamed for leading to the deterioration of the Zimbabwean economy which plagues the country today.
On 18 April 1980 the country became independent as the Republic of Zimbabwe, and its capital, Salisbury, was renamed Harare two years later.
Politics.
Although Southern Rhodesia never gained full Dominion status within the old Commonwealth, Southern Rhodesians ruled themselves from the attainment of 'Responsible Government' in 1923. Its electoral register had property and education qualifications. Over the years various electoral arrangements made at a national and municipal level upheld these standards. For example, the franchise for the first Legislative Council election in 1899 contained the following requirement:
Following Cecil Rhodes' dictum of "equal rights for all civilised men", there was no overt racial component to the franchise. However, the requirement excluded a majority of native blacks from the electorate.
Up until the 1950s, Southern Rhodesia had a vibrant political life with right and left wing parties competing for power. The Rhodesia Labour Party held seats in the Assembly and in municipal councils throughout the 1920s and 1930s. From 1953 to 1958, the prime minister was Garfield Todd, a liberal who did much to promote the development of the Black community through investment in education, housing and healthcare. However, the government forced Todd from office because his proposed reforms were seen by many whites as too radical.
From 1958 onwards, white settler politics consolidated and ossified around resistance to majority rule, setting the stage for UDI. The 1961 Constitution governed Southern Rhodesia and independent Rhodesia up until 1969, using the Westminster Parliamentary System modified by a system of separate voter rolls with differing property and education qualifications, without regard to race. Whites ended up with the majority of Assembly seats.
The 1969 republican constitution established a bicameral Parliament consisting of an indirectly elected Senate and a directly elected House of Assembly, effectively reserving the majority of seats for whites. The office of President had only ceremonial significance with the Prime Minister holding executive power.
The Constitution of the short-lived Zimbabwe Rhodesia, which saw a black-led government elected for the first time, reserved 28 of the 100 parliamentary seats for whites. The independence constitution agreed at Lancaster House watered those provisions down and reserved 20 out of 100 seats for whites in the House of Assembly and 8 out of 40 seats in the Senate. The constitution prohibited Zimbabwe authorities from altering the Constitution for seven years without unanimous consent and required a three-quarters vote in Parliament for a further three years. The government amended the Constitution in 1987 to abolish the seats reserved for whites, and replace the office of Prime Minister with an executive President. In 1990 the government abolished the Senate.
Society.
Satirical novelist Evelyn Waugh wrote humorously after a visit to Rhodesia (in 1958):
Military and police.
Rhodesian Security Forces consisted of the Rhodesian Army, Royal Rhodesian Air Force, British South Africa Police, Rhodesian Ministry of Internal Affairs (INTAF) and the Guard Force. Despite the impact of economic and diplomatic sanctions, Rhodesia was able to develop and maintain a potent and professional military capability. "Time" magazine reported in June 1977 that "man for man, the Rhodesian army ranks among the world's finest fighting units."
Foreign relations.
Throughout the period of its Unilateral Declaration of Independence (1965 to 1979), Rhodesia pursued a foreign policy of attempting to secure recognition as an independent country, and insisting that its political system would include 'gradual steps to majority rule.' Ardently anti-communist, Rhodesia tried to present itself to the West as a front-line state against communist expansion in Africa, to little avail. Rhodesia received little international recognition during its existence; recognition only occurred after elections in 1980 and a transition to black African rule.
Rhodesia wished to retain its economic prosperity and also feared communist elements in the rebel forces, and thus felt their policy of a gradual progression to black majority rule was justified. However, the international community refused to accept this rationale, believing that their policies were perpetuating racism. This attitude was part of the larger decolonisation context, during which Western powers such as the United Kingdom, France, and Belgium hastened to grant independence to their colonies in Africa.
Britain and the UDI.
Rhodesia was originally a British colony. Although decolonisation in Africa had begun after World War II, it began accelerating in the early 1960s, causing Britain to negotiate independence rapidly with several of its colonies. During this period, it adopted a foreign policy called NIBMAR, or No Independence Before Majority African Rule, mandating democratic reforms that placed governance in the hands of the majority black Africans. The governing white minority of Rhodesia, led by Ian Smith, opposed the policy and its implications. On 11 November 1965, Rhodesia's minority white government made a unilateral declaration of independence (UDI) from the United Kingdom, as it became apparent that negotiations would not lead to independence under the white regime.
The United Kingdom government immediately brought in legislation (Southern Rhodesia Act 1965) which formally abolished all Rhodesian government institutions. This move made life difficult for Rhodesian citizens who wished to travel internationally as passports issued by Rhodesia's UDI administration were not recognised as valid; in January 1966, the British issued a statement accepting as valid any passport issued before the declaration of independence and allowing six-month United Kingdom passports to be granted when they expired – provided that the bearer declared they did not intend to aid the UDI Rhodesian government.
Until late 1969, Rhodesia still recognised Queen Elizabeth II as head of state, even though it opposed the British government itself for hindering its goals of independence. The Queen, however, refused to accept the title "Queen of Rhodesia." Eventually, the Smith government abandoned attempts to remain loyal to the Crown, and in 1969, a majority of the electorate voted in referendum to declare Rhodesia a republic. They hoped that this move would facilitate recognition as an independent state by the international community, but the issues of white minority control remained and hindered this effort, and like the UDI before it, the proclamation of a republic lacked international recognition.
Sanctions.
After the declaration of independence, and indeed for the entire duration of its existence, Rhodesia did not receive official recognition from any state, although it did maintain diplomatic relations with South Africa, another white minority regime (but did not recognise Rhodesia due to its wish to preserve its fragile positions with other nations but frequently assisted the republic), and Portugal, which ceased relations with Rhodesia after its Carnation Revolution in 1974. The day following the declaration of independence, the United Nations Security Council passed a resolution (S/RES/216) calling upon all states to not accord Rhodesia recognition, and to refrain from any assistance. The Security Council also imposed selective mandatory economic sanctions, which were later made comprehensive.
Despite the mounting sanctions imposed by the United Nations, the Rhodesian economy prospered. By the 1970s, Rhodesia's economic strength was second only to that of South Africa's. Rhodesia's Europeans had one of the highest standards of living in the world, whilst Rhodesia's Africans had the highest standard of health, education and housing when compared to any other African people. Rhodesia's economic success in the face of such adversity was truly unprecedented.
International perspective.
Rhodesia's Unilateral Declaration of Independence from the United Kingdom on 11 November 1965 was promptly condemned by the international community. The United Nations Security Council Resolution 216 of 12 November 1965 called "upon all States not to recognise this illegal racist minority regime in Southern Rhodesia."
Rhodesia campaigned for international acceptance and invoked the doctrine of non-intervention in internal affairs as justification for rebuking external criticism of its internal policies. However, the emerging doctrine of self-determination in colonial situations meant that most nations regarded Rhodesia as illegitimate.
Zambia, formerly Northern Rhodesia, took a pragmatic approach towards Rhodesia. Kenneth Kaunda, heavily dependent on access through Rhodesia for his nation's copper ore exports, fuel, and power imports unofficially worked with the Rhodesian government. Rhodesia still allowed Zambia to export and import its goods through its territory to Mozambique ports, despite the Zambian government's official policy of hostility and non-recognition of the post-UDI Smith Administration.
The United States, like all other Western nations, refused to recognise Rhodesia, but unlike others allowed its Consulate-General to function as a communications conduit between the American government in Washington, D.C., and the Rhodesian government in Salisbury. When Rhodesia set up an information office in Washington, D.C., OAS nations loudly protested. The US government responded by saying the Rhodesian mission and its staff had no official diplomatic status and violated no US laws.
Portugal pursued a middle path with Rhodesia. While not officially recognising Rhodesia under Ian Smith, the government of António Salazar did permit Rhodesia to establish a representative mission in Lisbon, and permitted Rhodesian exports and imports through their colony of Mozambique. The Portuguese government in power at that time, authoritarian and ardently anti-communist, gave active behind-the-scenes support in Rhodesia's fight against the guerrilla groups.
South Africa, itself under international pressure as a white minority government, pursued a policy of détente with the black African states at the time. These states wanted South Africa to pressure Ian Smith to accept a faster transition to majority rule in Rhodesia, in return for pledges of non-interference in South Africa's internal affairs. Prime Minister John Vorster, believing majority rule in Rhodesia would lead to international acceptance for South Africa, used a number of tactics to pressure Smith. The South African government held up shipments of fuel and ammunition and pulled out friendly South African forces from Rhodesia. The combined loss of Mozambique and the loss of support from South Africa dealt critical blows to the Rhodesian government.
Legations.
After the UDI, Rhodesia House in London, (the Rhodesian High Commission), now the Embassy of Zimbabwe in London, simply became a representative office with no official diplomatic status. Other locations which had Rhodesian representative offices were:
The most important representative offices for Rhodesia were Lisbon and Pretoria.
Results.
Continuing civil war and a lack of international support eventually led the Rhodesian government to submit to an agreement with the UK in 1979. This led to internationally supervised elections, won by Zimbabwe African National Union - Patriotic Front and Robert Mugabe, establishing the internationally recognised Zimbabwe.
Legacy.
After recognised independence in April 1980, the history of Rhodesia became that of Zimbabwe. However, many of the issues associated with UDI and the Bush War were not resolved immediately. In the early 1980s, South Africa sought to secure its position in the region by various means including the destabilisation of neighbouring states through support for dissident groups such as the National Union for the Total Independence of Angola UNITA (in Angola) and Renamo (in Mozambique). In Zimbabwe, the South African intelligence service promoted ZIPRA dissidents in what became known as the super-ZAPU insurgency in Matabeleland.
During the Bush War of the 1970s some white farmers were able to carry on operations by paying protection money to commanders. The super-ZAPU insurgency of the early 1980s was much less manageable. Super-ZAPU targeted white farmers, missionaries and tourists on the grounds that their murders would make "international headlines."
The insurgency was equipped and coordinated by South African intelligence, working through white former members of the Rhodesian security services. The super-ZAPU insurgency was eventually resolved at a military level by the Zimbabwe army Fifth Brigade's sweep through Matabeleland in 1983 (operation "Gukurahundi") and at a political level by the Unity Accord of 1987. Operation Gukurahundi was associated with the massacre of between four and twenty thousand civilians. Those last figures are estimated by sources ranging from the Catholic Commission for Justice and Peace to Parade magazine.
In the ten years after independence, around 60% of the white population of Zimbabwe emigrated, most to South Africa and to other mainly white, English speaking countries where they formed expatriate communities. Politically within Zimbabwe, the consolidation of power by Robert Mugabe continued through the 1980s. Parliamentary seats reserved for the white population were abolished in 1987 and a new constitution promulgated with Mugabe in the position of state president. Many expatriates and some of the whites who stayed in Zimbabwe became deeply nostalgic for Rhodesia. These individuals are known as "Rhodies." Native whites who are more accepting of the new order are known as "Zimbos."
While as Rhodesia, the country was once considered the breadbasket of Africa. Today, Zimbabwe is a net importer of foodstuffs, with the European Union and United States providing emergency food relief as humanitarian aid on a regular basis. The nation has suffered profound economic and social decline in the past twenty years. Part of the issue is due to a marked decrease in agricultural production as fertile farmland once cultivated by trained white farmers and their employees sometimes from other African countries has been forcibly relocated to black Zimbabweans by political background rather than farming experience, many whom are untrained in agricultural land management, as compensation for military service and/or political loyalty. In such cases, production usually falls to less than half of its estimated capacity and fertile land lies fallow due to neglect. Not only is production reduced, but the jobs associated with operating a viable enterprise are lost. The Zimbabwe government blame the crippling sanctions imposed by the Western governments to force a regime change, for the failures in the economy. Recently the agriculture sector has started to do well since the availability of expertise and machines has improved supported mainly by China.
Zimbabwe also suffered from a crippling inflation rate, as the Reserve Bank of Zimbabwe had a policy of printing money to satisfy government debts, which introduces excessive currency into the economic system which led to the demise of the local currency. This policy caused the inflation rate to soar from 32% in 1998 (considered extremely high by most economic standards) to an astonishing 11,200,000% by 2007. Monetary aid by the International Monetary Fund has been suspended due to the Zimbabwe government's defaulting on past loans, inability to stabilise its own economy, and its inability to stem corruption and advance human rights.
In 2009, Zimbabwe abandoned its currency, relying instead on foreign currencies such as the South African rand, the US dollar, the Botswana pula, the euro and the British pound, among others.
In 2008 elections, Mugabe garnered 41%, Simba Makoni 10% and Morgan Tsvangirai 48% of the votes cast for president forcing a runoff election called by the Zimbabwe Electoral Commission (ZEC). In the months leading to the run-off, instances of extreme violence between the two major parties (ZANU PF and MDC) led Tsvangirai to withdraw from the election. In February 2009, a power-sharing accord was reached which resulted in the Zimbabwe Government of National Unity of 2009. The accord was, essentially, to create the position of "Prime Minister" for Tsvangirai, who served in that role from 2009 to 2013. Mugabe retained the title of President.
Rhodesian media.
Among the news magazines published in Rhodesia under UDI was the "Rhodesia Herald" and "Illustrated Rhodesia Life". "The Valiant Years" by Beryl Salt told the history of Rhodesia from 1890 to 1978 entirely through the medium of facsimile reproduction of articles and headlines from Rhodesian newspapers.

</doc>
<doc id="26220" url="http://en.wikipedia.org/wiki?curid=26220" title="Relational model">
Relational model

The relational model for database management is a database model based on first-order predicate logic, first formulated and proposed in 1969 by Edgar F. Codd. In the relational model of a database, all data is represented in terms of tuples, grouped into relations. A database organized in terms of the relational model is a relational database.
The purpose of the relational model is to provide a declarative method for specifying data and queries: users directly state what information the database contains and what information they want from it, and let the database management system software take care of describing data structures for storing the data and retrieval procedures for answering queries.
Most relational databases use the SQL data definition and query language; these systems implement what can be regarded as an engineering approximation to the relational model. A "table" in an SQL database schema corresponds to a predicate variable; the contents of a table to a relation; key constraints, other constraints, and SQL queries correspond to predicates. However, SQL databases deviate from the relational model in many details, and Codd fiercely argued against deviations that compromise the original principles.
Overview.
The relational model's central idea is to describe a database as a collection of predicates over a finite set of predicate variables, describing constraints on the possible values and combinations of values. The content of the database at any given time is a finite (logical) model of the database, i.e. a set of relations, one per predicate variable, such that all predicates are satisfied. A request for information from the database (a database query) is also a predicate.
Alternatives.
Other models are the hierarchical model and network model. Some systems using these older architectures are still in use today in data centers with high data volume needs, or where existing systems are so complex and abstract it would be cost-prohibitive to migrate to systems employing the relational model; also of note are newer object-oriented databases.
Implementation.
There have been several attempts to produce a true implementation of the relational database model as originally defined by Codd and explained by Date, Darwen and others, but none have been popular successes so far. Rel is one of the more recent attempts to do this.
The relational model was the first database model to be described in formal mathematical terms. Hierarchical and network databases existed before relational databases, but their specifications were relatively informal. After the relational model was defined, there were many attempts to compare and contrast the different models, and this led to the emergence of more rigorous descriptions of the earlier models; though the procedural nature of the data manipulation interfaces for hierarchical and network databases limited the scope for formalization.
History.
The relational model was invented by E.F. (Ted) Codd as a general model of data, and subsequently maintained and developed by Chris Date and Hugh Darwen among others. In The Third Manifesto (first published in 1995) Date and Darwen show how the relational model can accommodate certain desired object-oriented features.
Controversies.
Codd himself, some years after publication of his 1970 model, proposed a three-valued logic (True, False, Missing or NULL) version of it to deal with missing information, and in his "The Relational Model for Database Management Version 2" (1990) he went a step further with a four-valued logic (True, False, Missing but Applicable, Missing but Inapplicable) version. But these have never been implemented, presumably because of attending complexity. SQL's NULL construct was intended to be part of a three-valued logic system, but fell short of that due to logical errors in the standard and in its implementations.
Topics.
The fundamental assumption of the relational model is that all data is represented as mathematical "n"-ary relations, an "n"-ary relation being a subset of the Cartesian product of "n" domains. In the mathematical model, reasoning about such data is done in two-valued predicate logic, meaning there are two possible evaluations for each proposition: either "true" or "false" (and in particular no third value such as "unknown", or "not applicable", either of which are often associated with the concept of NULL). 
Data are operated upon by means of a relational calculus or relational algebra, these being equivalent in expressive power.
The relational model of data permits the database designer to create a consistent, logical representation of information. Consistency is achieved by including declared constraints in the database design, which is usually referred to as the logical schema. The theory includes a process of database normalization whereby a design with certain desirable properties can be selected from a set of logically equivalent alternatives. The access plans and other implementation and operation details are handled by the DBMS engine, and are not reflected in the logical model. This contrasts with common practice for SQL DBMSs in which performance tuning often requires changes to the logical model.
The basic relational building block is the domain or data type, usually abbreviated nowadays to type. A "tuple" is an ordered set of attribute values. An attribute is an ordered pair of attribute name and type name. An attribute value is a specific valid value for the type of the attribute. This can be either a scalar value or a more complex type.
A relation consists of a heading and a body. A heading is a set of attributes. A body (of an "n"-ary relation) is a set of "n"-tuples. The heading of the relation is also the heading of each of its tuples.
A relation is defined as a set of "n"-tuples. In both mathematics and the relational database model, a set is an "unordered" collection of unique, non-duplicated items, although some DBMSs impose an order to their data. In mathematics, a tuple has an order, and allows for duplication. E.F. Codd originally defined tuples using this mathematical definition. Later, it was one of E.F. Codd's great insights that using attribute names instead of an ordering would be so much more convenient (in general) in a computer language based on relations . This insight is still being used today. Though the concept has changed, the name "tuple" has not. An immediate and important consequence of this distinguishing feature is that in the relational model the Cartesian product becomes commutative.
A table is an accepted visual representation of a relation; a tuple is similar to the concept of a "row".
A "relvar" is a named variable of some specific relation type, to which at all times some relation of that type is assigned, though the relation may contain zero tuples.
The basic principle of the relational model is the Information Principle: all information is represented by data values in relations. In accordance with this Principle, a relational database is a set of relvars and the result of every query is presented as a relation.
The consistency of a relational database is enforced, not by rules built into the applications that use it, but rather by "constraints", declared as part of the logical schema and enforced by the DBMS for all applications. In general, constraints are expressed using relational comparison operators, of which just one, "is subset of" (⊆), is theoretically sufficient. In practice, several useful shorthands are expected to be available, of which the most important are candidate key (really, superkey) and foreign key constraints.
Interpretation.
To fully appreciate the relational model of data it is essential to understand the intended "interpretation" of a relation.
The body of a relation is sometimes called its extension. This is because it is to be interpreted as a representation of the extension of some predicate, this being the set of true propositions that can be formed by replacing each free variable in that predicate by a name (a term that designates something).
There is a one-to-one correspondence between the free variables of the predicate and the attribute names of the relation heading. Each tuple of the relation body provides attribute values to instantiate the predicate by substituting each of its free variables. The result is a proposition that is deemed, on account of the appearance of the tuple in the relation body, to be true. Contrariwise, every tuple whose heading conforms to that of the relation, but which does not appear in the body is deemed to be false. This assumption is known as the closed world assumption: it is often violated in practical databases, where the absence of a tuple might mean that the truth of the corresponding proposition is unknown. For example, the absence of the tuple ('John', 'Spanish') from a table of language skills cannot necessarily be taken as evidence that John does not speak Spanish.
For a formal exposition of these ideas, see the section Set-theoretic Formulation, below.
Application to databases.
A data type as used in a typical relational database might be the set of integers, the set of character strings, the set of dates, or the two boolean values "true" and "false", and so on. The corresponding type names for these types might be the strings "int", "char", "date", "boolean", etc. It is important to understand, though, that relational theory does not dictate what types are to be supported; indeed, nowadays provisions are expected to be available for "user-defined" types in addition to the "built-in" ones provided by the system.
Attribute is the term used in the theory for what is commonly referred to as a column. Similarly, table is commonly used in place of the theoretical term relation (though in SQL the term is by no means synonymous with relation). A table data structure is specified as a list of column definitions, each of which specifies a unique column name and the type of the values that are permitted for that column. An attribute "value" is the entry in a specific column and row, such as "John Doe" or "35".
A tuple is basically the same thing as a row, except in an SQL DBMS, where the column values in a row are ordered. (Tuples are not ordered; instead, each attribute value is identified solely by the attribute name and never by its ordinal position within the tuple.) An attribute name might be "name" or "age".
A relation is a table structure definition (a set of column definitions) along with the data appearing in that structure. The structure definition is the heading and the data appearing in it is the body, a set of rows. A database relvar (relation variable) is commonly known as a base table. The heading of its assigned value at any time is as specified in the table declaration and its body is that most recently assigned to it by invoking some update operator (typically, INSERT, UPDATE, or DELETE). The heading and body of the table resulting from evaluation of some query are determined by the definitions of the operators used in the expression of that query. (Note that in SQL the heading is not always a set of column definitions as described above, because it is possible for a column to have no name and also for two or more columns to have the same name. Also, the body is not always a set of rows because in SQL it is possible for the same row to appear more than once in the same body.)
SQL and the relational model.
SQL, initially pushed as the standard language for relational databases, deviates from the relational model in several places. The current ISO SQL standard doesn't mention the relational model or use relational terms or concepts. However, it is possible to create a database conforming to the relational model using SQL if one does not use certain SQL features.
The following deviations from the relational model have been noted in SQL. Note that few database servers implement the entire SQL standard and in particular do not allow some of these deviations. Whereas NULL is ubiquitous, for example, allowing duplicate column names within a table or anonymous columns is uncommon.
Relational operations.
Users (or programs) request data from a relational database by sending it a query that is written in a special language, usually a dialect of SQL. Although SQL was originally intended for end-users, it is much more common for SQL queries to be embedded into software that provides an easier user interface. Many Web sites, such as Wikipedia, perform SQL queries when generating pages.
In response to a query, the database returns a result set, which is just a list of rows containing the answers. The simplest query is just to return all the rows from a table, but more often, the rows are filtered in some way to return just the answer wanted.
Often, data from multiple tables are combined into one, by doing a join. Conceptually, this is done by taking all possible combinations of rows (the Cartesian product), and then filtering out everything except the answer. In practice, relational database management systems rewrite ("optimize") queries to perform faster, using a variety of techniques.
There are a number of relational operations in addition to join. These include project (the process of eliminating some of the columns), restrict (the process of eliminating some of the rows), union (a way of combining two tables with similar structures), difference (that lists the rows in one table that are not found in the other), intersect (that lists the rows found in both tables), and product (mentioned above, which combines each row of one table with each row of the other). Depending on which other sources you consult, there are a number of other operators – many of which can be defined in terms of those listed above. These include semi-join, outer operators such as outer join and outer union, and various forms of division. Then there are operators to rename columns, and summarizing or aggregating operators, and if you permit relation values as attributes (RVA – relation-valued attribute), then operators such as group and ungroup. The SELECT statement in SQL serves to handle all of these except for the group and ungroup operators.
The flexibility of relational databases allows programmers to write queries that were not anticipated by the database designers. As a result, relational databases can be used by multiple applications in ways the original designers did not foresee, which is especially important for databases that might be used for a long time (perhaps several decades). This has made the idea and implementation of relational databases very popular with businesses.
Database normalization.
Relations are classified based upon the types of anomalies to which they're vulnerable. A database that's in the first normal form is vulnerable to all types of anomalies, while a database that's in the domain/key normal form has no modification anomalies. Normal forms are hierarchical in nature. That is, the lowest level is the first normal form, and the database cannot meet the requirements for higher level normal forms without first having met all the requirements of the lesser normal forms.
Examples.
Database.
An idealized, very simple example of a description of some relvars (relation variables) and their attributes:
In this design we have six relvars: Customer, Order, Order Line, Invoice, Invoice Line and Product. The bold, underlined attributes are "candidate keys". The non-bold, underlined attributes are "foreign keys".
Usually one candidate key is arbitrarily chosen to be called the primary key and used in preference over the other candidate keys, which are then called alternate keys.
A "candidate key" is a unique identifier enforcing that no tuple will be duplicated; this would make the relation into something else, namely a bag, by violating the basic definition of a set. Both foreign keys and superkeys (that includes candidate keys) can be composite, that is, can be composed of several attributes. Below is a tabular depiction of a relation of our example Customer relvar; a relation can be thought of as a value that can be attributed to a relvar.
Customer relation.
If we attempted to "insert" a new customer with the ID "1234567890", this would violate the design of the relvar since Customer ID is a "primary key" and we already have a customer "1234567890". The DBMS must reject a transaction such as this that would render the database inconsistent by a violation of an integrity constraint.
"Foreign keys" are integrity constraints enforcing that the value of the attribute set is drawn from a "candidate key" in another relation. For example in the Order relation the attribute Customer ID is a foreign key. A "join" is the operation that draws on information from several relations at once. By joining relvars from the example above we could "query" the database for all of the Customers, Orders, and Invoices. If we only wanted the tuples for a specific customer, we would specify this using a restriction condition.
If we wanted to retrieve all of the Orders for Customer "1234567890", we could query the database to return every row in the Order table with Customer ID "1234567890" and join the Order table to the Order Line table based on Order No.
There is a flaw in our database design above. The Invoice relvar contains an Order No attribute. So, each tuple in the Invoice relvar will have one Order No, which implies that there is precisely one Order for each Invoice. But in reality an invoice can be created against many orders, or indeed for no particular order. Additionally the Order relvar contains an Invoice No attribute, implying that each Order has a corresponding Invoice. But again this is not always true in the real world. An order is sometimes paid through several invoices, and sometimes paid without an invoice. In other words there can be many Invoices per Order and many Orders per Invoice. This is a many-to-many relationship between Order and Invoice (also called a "non-specific relationship"). To represent this relationship in the database a new relvar should be introduced whose role is to specify the correspondence between Orders and Invoices:
OrderInvoice(Order No,Invoice No)
Now, the Order relvar has a "one-to-many relationship" to the OrderInvoice table, as does the Invoice relvar. If we want to retrieve every Invoice for a particular Order, we can query for all orders where Order No in the Order relation equals the Order No in OrderInvoice, and where Invoice No in OrderInvoice equals the Invoice No in Invoice.
Set-theoretic formulation.
Basic notions in the relational model are "relation names" and "attribute names". We will represent these as strings such as "Person" and "name" and we will usually use the variables formula_1 and formula_2 to range over them. Another basic notion is the set of "atomic values" that contains values such as numbers and strings.
Our first definition concerns the notion of "tuple", which formalizes the notion of row or record in a table:
The next definition defines "relation" that formalizes the contents of a table as it is defined in the relational model.
Such a relation closely corresponds to what is usually called the extension of a predicate in first-order logic except that here we identify the places in the predicate with attribute names. Usually in the relational model a database schema is said to consist of a set of relation names, the headers that are associated with these names and the constraints that should hold for every instance of the database schema.
Key constraints and functional dependencies.
One of the simplest and most important types of relation constraints is the "key constraint". It tells us that in every instance of a certain relational schema the tuples can be identified by their values for certain attributes.
Algorithm to derive candidate keys from functional dependencies.
 INPUT: a set "S" of FDs that contain only subsets of a header "H"
 OUTPUT: the set "C" of superkeys that hold as candidate keys in
 all relation universes over "H" in which all FDs in "S" hold
 begin
 "C" := ∅; // found candidate keys
 "Q" := { "H" }; // superkeys that contain candidate keys
 while "Q" <> ∅ do
 let "K" be some element from "Q";
 "Q" := "Q" – { "K" };
 "minimal" := true;
 for each "X->Y" in "S" do
 "K' ":= ("K" – "Y") ∪ "X"; // derive new superkey
 if "K' "⊂ "K" then
 "minimal" := false;
 "Q" := "Q" ∪ { "K' "};
 end if
 end for
 if "minimal" and there is not a subset of "K" in "C" then
 remove all supersets of "K" from "C";
 "C" := "C" ∪ { "K" };
 end if
 end while
 end

</doc>
<doc id="26221" url="http://en.wikipedia.org/wiki?curid=26221" title="Rathaus Schöneberg">
Rathaus Schöneberg

Rathaus Schöneberg is the city hall for the borough of Tempelhof-Schöneberg in Berlin. From 1949 until 1993 it served as the seat of the state senate of West Berlin and until 1991 also as the office of the Governing Mayor.
History.
The sandstone building was constructed between 1911–1914, when it replaced the old town hall of Schöneberg, at that time an independent city (German: "Stadtkreis") not yet incorporated into Greater Berlin, which took place in 1920. The Nazi authorities had a series of war murals by Franz Eichhorst added to the interior in 1938. In World War II the building was severely damaged by Allied bombing and during the final Battle of Berlin.
After the war the undestroyed "Neues Stadthaus", former head office of Berlin's municipal fire insurance "Feuersozietät", on Parochialstraße in Mitte, served as intermittent city hall, replacing the ruined Rotes Rathaus (Red City Hall, also in East Berlin), the traditional seat of the Berlin government. With the division of Berlin's city government and administration in September 1948 the "Neues Stadthaus" was in the Communist "Ostsektor" (eastern sector) and became off limits to West Berlin. As a "temporary" measure the barely repaired Rathaus Schöneberg on Rudolph-Wilde-Platz became the city hall for West Berlin. In 1950 the Liberty Bell ("Freiheitsglocke"), a gift by the United States, was installed in the rebuilt tower.
During the Berlin Blockade, the Uprising of 1953 and the Hungarian Revolution of 1956, Rudolph-Wilde-Platz in front of the building became a gathering place for protest rallies. After the construction of the Berlin Wall in 1961, the steps of Rathaus Schöneberg were the location where U.S. President John F. Kennedy spoke on 26 June 1963, proclaiming "Ich bin ein Berliner". On the night of his assassination, several thousand Berliners spontaneously gathered at the square, which was officially renamed John-F.-Kennedy-Platz three days later. A large memorial plaque, mounted on a column at the entrance of the building, and the room above the entrance overlooking the square are dedicated to Kennedy and his visit.
There was a large assembly in front of the Rathaus on 10 November 1989, the day after the fall of the Berlin Wall. Prominent people attending were chancellor Helmut Kohl, former chancellor Willy Brandt and foreign minister Hans-Dietrich Genscher.
After reunification, Rathaus Schöneberg reverted to its original purpose of being Schöneberg Borough Town Hall. Upon the 2001 Berlin administrative reform, Rathaus Schöneberg became the town hall for the newly constituted borough of Tempelhof-Schöneberg.
It was also the permanent home to an exhibition of the life of Willy Brandt (1913–1992), Mayor of West Berlin from 1957 to 1966, Chancellor of the Federal Republic of Germany 1969–1974. The exhibition was closed as from January 2010; it is planned to open again at another site in the city.
Since 2005, the exhibition called "Wir waren Nachbarn - Biografien jüdischer Zeitzeugen" (English title:"We were Neighbours once - Biographies of Jews in Schöneberg and Tempelhof under the Nazi Regime") takes place in the exhibition hall of the Rathaus Schöneberg.

</doc>
<doc id="26225" url="http://en.wikipedia.org/wiki?curid=26225" title="Rædwald of East Anglia">
Rædwald of East Anglia

Rædwald (Old English: "Rædwald", 'power in counsel'), also rendered as Raedwald or Redwald, was a 7th-century king of East Anglia, a long-lived Anglo-Saxon kingdom which today includes the English counties of Norfolk and Suffolk. He was the son of Tytila of East Anglia and a member of the Wuffingas dynasty (named after his grandfather, Wuffa), who were the first kings of the East Angles. Details about Rædwald's reign are scarce, primarily because the Viking invasions of the 9th century destroyed the monasteries in East Anglia where many documents would have been kept. Rædwald reigned from about 599 until his death around 624, initially under the overlordship of Æthelberht of Kent. In 616, as a result of fighting the Battle of the River Idle and defeating Æthelfrith of Northumbria, he was able to install Edwin, who was acquiescent to his authority, as the new king of Northumbria. During the battle, both Æthelfrith and Rædwald's son Rægenhere were killed.
From around 616, Rædwald was the most powerful of the English kings south of the River Humber. According to Bede he was the fourth ruler to hold "imperium" over other southern Anglo-Saxon kingdoms: he was referred to in the "Anglo-Saxon Chronicle", written centuries after his death, as a "bretwalda" (an Old English term meaning 'Britain-ruler' or 'wide-ruler'). He was the first king of the East Angles to become a Christian, converting at Æthelberht's court some time before 605, whilst at the same time maintaining a pagan temple. In receiving the faith he helped to ensure the survival of Christianity in East Anglia during the apostasy of the Anglo-Saxon kingdoms of Essex and Kent. He is generally considered by historians to be the most favoured candidate for the occupant of the Sutton Hoo ship-burial, although other theories have been advanced.
Sources.
The kingdom of East Anglia (Old English: "Ēast Engla Rīce") was a small independent Anglo-Saxon kingdom that comprised what are now the English counties of Norfolk and Suffolk and perhaps the eastern part of the Cambridgeshire Fens. Few sources have survived that were written by the Anglo-Saxons in England, and East Anglia has even less documentary evidence than most of the kingdoms existing at that time. The historian Barbara Yorke has suggested that the reason for the paucity of East Anglian sources was almost certainly the Viking expansion in the 9th century as the monks and scribes of East Anglia produced as much work as those living in other parts of England. The devastation caused by the Vikings is thought to have destroyed all the books and charters that may have been kept there.
Rædwald is the first king of the East Angles of whom more than a name is known, though no details of his life before his accession are known. The earliest and most substantial source for Rædwald is the "Historia ecclesiastica gentis Anglorum" ("Ecclesiastical History of the English People"), completed in 731 by Bede, a Northumbrian monk. Bede placed Rædwald's reign between the advent of the Gregorian mission to Kent in 597 and the marriage and conversion of Edwin of Northumbria during 625–26.
Later medieval chroniclers, such as Roger of Wendover, gave some information about East Anglian events, but Yorke suggests that the annalistic format used forced these writers to guess the dates of the key events they recorded. Such later sources are therefore treated with caution. The "Anglian collection", which dates from the late 8th century, contains an East Anglian genealogical tally, but Rædwald is not included. Rædwald is however referred to in the 8th century "Vita" of St Gregory the Great, written by a member of the religious community at Whitby. The Battle of the River Idle, in which Rædwald and his forces defeated the Northumbrians, is described in the 12th century "Historia Anglorum", written by Henry of Huntingdon.
The context of Rædwald's kingdom.
The Anglo-Saxons, who are known to have included Angles, Saxons, Jutes and Frisians, began to arrive in Britain in the 5th century. By 600, a number of kingdoms had begun to form in the conquered territories. By the beginning of the 7th century, southern England was almost entirely under their control.
During Rædwald's youth, the establishment of other ruling houses was accomplished. Sometime before 588, Æthelberht of Kent married Bercta, the Christian daughter of the Frankish ruler Charibert. As early as 568, Ceawlin of Wessex, the most powerful ruler south of the River Humber, repulsed Æthelberht. According to later sources, Mercia was founded by Creoda in 585, although a paucity of sources makes it difficult to know how the Mercian royal line became established.
North of the Humber, the kingdoms of Deira and Bernicia possessed rival royal dynasties. Ælla ruled Deira until his death in 588, leaving his daughter Acha, his son Edwin, and another unknown sibling. The Bernician dynasty, allied by kinship to the kingdom of Wessex, gained ascendancy over Deira, forcing Edwin to live in exile in the court of Cadfan ap Iago of Gwynedd. In various wars, Æthelfrith of Bernicia consolidated the Northumbrian state, and in around 604 he was able to bring Deira under his dominion.
Family.
Rædwald, which in Old English means 'power in counsel', was born around 560–580. The son of Tytila, whom he succeeded, he was the elder brother of Eni. According to Bede, he was descended from Wuffa, the founder of the Wuffingas dynasty: ' "filius Tytili, cuius pater fuit UUffa" ('the son of Tytil, whose father was Wuffa').
At some time during the 590s, Rædwald married a woman whose name is unknown, though it is known from Bede that she was pagan. By her he fathered at least two sons, Rægenhere and Eorpwald. He also had an older son, Sigeberht, whose name is unlike other Wuffingas names but which is typical of the East Saxon dynasty. It has been suggested that Rædwald's queen had previously been married to a member of the Essex royal family and that Sigeberht was Rædwald's stepson, as was stated by William of Malmesbury in the 12th century. Sigeberht earned the enmity of his step-father, who drove him into exile in Gaul, possibly to protect the Wuffingas bloodline.
Early reign and baptism.
Events that occurred during the early years of Rædwald's reign include the arrival of Augustine of Canterbury and his mission from Rome in 597, the conversions of Æthelberht of Kent and Saeberht of Essex, and the establishment of new bishoprics in their kingdoms. Bede, when relating the conversion of Rædwald's son Eorpwald in his "Historia ecclesiastica gentis Anglorum", mentioned that Rædwald received the Christian sacraments in Kent. This happened in perhaps 604 or later, presumably at the invitation of Æthelberht, who may have been his baptismal sponsor. The date of his conversion is unknown, but it would have occurred after the arrival of the Gregorian mission in 597. Since it is claimed that Saint Augustine, who died in about 605, dedicated a church near Ely, it may have followed Saebert's conversion fairly swiftly. Rædwald's marriage to a member of the royal dynasty of Essex helped form a diplomatic alliance between the neighbouring kingdoms of East Anglia and Essex. His conversion in Kent would have affiliated him with Æthelberht, bringing him directly into the sphere of Kent.
In East Anglia, Rædwald's conversion was not universally accepted by his household or his own queen. According to the historian Steven Plunkett, she and her pagan teachers persuaded him to default in part from his commitment to the Christian faith. As a result, he kept in the temple two altars, one pagan and another dedicated to Christ. Bede, writing decades later, described how Ealdwulf of East Anglia, a grandson of Rædwald's brother Eni, recalled seeing the temple when he was a boy. It was located at Rendlesham, the "regio" of the Wuffing dynasty, according to Plunkett. Barbara Yorke explains the dual nature of the temple by suggesting that Rædwald would have been unprepared to reject his old religion and fully embrace Christianity, as this act would have been a public acknowledgment of his inferiority to Æthelberht. Rædwald's lack of commitment towards Christianity earned him the enmity of Bede, who regarded him as a renouncer of the faith.
Rædwald and Edwin of Northumbria.
Edwin's exile.
Æthelfrith of Northumbria may have married Acha, who was the mother of his son Oswald (born in about 604), according to Bede. Æthelfrith pursued her exiled brother Edwin in an attempt to destroy him and ensure that the Bernician rulership of Northumbria would be unchallenged. Edwin found hospitality in the household of Cearl of Mercia and later married Cearl's daughter. Edwin's nephew Hereric, an exile in the British kingdom of Elmet, was slain there under treacherous circumstances. Edwin eventually sought the protection of Rædwald, where he was received willingly. Rædwald promised to protect him, and Edwin lived with the king amongst his royal companions. When news of Edwin reached Æthelfrith in Northumbria, he sent messengers to Rædwald offering money in return for Edwin's death, but Rædwald refused to comply. Æthelfrith sent messengers a second and a third time, offering even greater gifts of silver and promising war if these were not accepted. Rædwald then weakened and promised either to kill Edwin or to hand him over to ambassadors.
When a chance arose for him to escape to a safe country, Edwin chose to remain at Rædwald's court. He was then visited by a stranger who was aware of Rædwald's deliberations. The source for this story, written at Whitby, stated that the stranger was Paulinus of York, a member of the Canterbury mission, who offered Edwin the hope of Rædwald's support and held out the prospect that Edwin might someday attain greater royal power than any previous English king. Paulinus was assured by Edwin that he would accept his religious teaching. His vision of Paulinus was afterwards made the means of his decision to embrace Christianity, on the condition that he survived and achieved power. If, as is supposed by some, Paulinus appeared to him in the flesh, the bishop's presence at Rædwald's court would throw some light on the king's position regarding religion.
Rædwald's pagan queen admonished him for acting in a manner dishonourable for a king by betraying his trust for the sake of money and wanting to sell his imperiled friend in exchange for riches. As a result of her admonishment, once Æthelfrith's ambassadors had gone, Rædwald resolved on war.
The Battle of the River Idle.
In 616 or 617, Rædwald assembled an army and marched north, accompanied by his son Rægenhere, to confront Æthelfrith. They met on the western boundary of the kingdom of Lindsey, on the east bank of the River Idle. The battle was fierce and was long commemorated in the saying, ‘The river Idle was foul with the blood of Englishmen’. During the fighting, Æthelfrith and Rædwald's son Rægenhere were both slain. Edwin then succeeded Æthelfrith as the king of Northumbria, and Æthelfrith's sons were subsequently forced into exile.
A separate account of the battle, given by Henry of Huntingdon, stated that Rædwald's army was split into three formations, led by Rædwald, Rægenhere, and Edwin. With more experienced fighters, Æthelfrith attacked in loose formation. At the sight of Rægenhere, perhaps thinking he was Edwin, Æthelfrith's men cut their way through to him and slew him. After the death of his son, Rædwald furiously breached his lines, killing Æthelfrith amid a great slaughter of the Northumbrians.
D.P. Kirby has argued that the battle was more than a clash between two kings over the treatment of an exiled nobleman but was "part of a protracted struggle to determine the military and political leadership of the Anglian peoples" at that time.
Rædwald's "imperium".
On 24 February 616, the year of the Battle of the River Idle, Æthelberht of Kent died and was succeeded by his pagan son Eadbald. After the death of the Christian Saebert of Essex, his three sons shared the kingdom, returning it to pagan rule, and drove out the Gregorian missionaries led by Mellitus. The Canterbury mission had removed to Gaul before Eadbald was brought back into the fold. During this period the only royal Christian altar in England belonged to Rædwald. By the time of his death, the mission in Kent had been fully re-established.
Rædwald's power became great enough for Bede to recognise him as the successor to the "imperium" of Æthelberht. Bede also called him "Rex Anglorum", the 'King of the Angles', a term that Rædwald's contemporaries would have used for their overlord. It is unclear where his power was centred or even how he established his authority over the Angles of eastern England.
By Edwin's debt of allegiance to him, Rædwald became the first foreign king to hold direct influence in Northumbria. He would have been instrumental in Edwin's secure establishment as king of both Diera and Bernicia.
The development of Gipeswic.
During the first quarter of the 7th century, the quayside settlement at Gipeswic (Ipswich) became an important estuarine trading centre, receiving imported goods such as pottery from other trading markets situated around the coasts of the North Sea. Steven Plunkett suggests that the founding of Gipeswic took place under Wuffingas supervision. It took another hundred years for the settlement to develop into a town, but its beginnings can be seen as a reflection of the personal importance of Rædwald during the period of his supremacy.
The excavated grave-goods of the Anglo-Saxon cemetery at Gipeswic, including those found in burials under small barrows, were not particularly wealthy or elaborate. They lacked the strong characterization of a neighbouring late 6th century cemetery at a higher crossing of the river. One exception was a furnished grave that it has been suggested could have been that of a visitor from the Rhineland.
Death.
Rædwald is considered to have died in around 624: his death can be located only within a few years. He must have reigned for some time after Æthelberht died, in order for him to have been noted as a "bretwalda". Barbara Yorke suggests that he died before Edwin converted to Christianity in 627 and also before Paulinus became bishop of Northumbria in 625. His death is recorded twice by Roger of Wendover, in 599 and in 624, in a history that dates from the 13th century but appears to include earlier annals of unknown origin and reliability. Plunkett notes that the earlier date of 599 is now taken as a mistaken reference to the death of Rædwald’s father, Tytila, and the later date is commonly given for the death of Rædwald.
He was succeeded by his pagan son Eorpwald, who was later persuaded to adopt Christianity by Edwin of Northumbria.
Sutton Hoo.
Rædwald lived at a time when eminent individuals were buried in barrows at the cemetery at Sutton Hoo, near Woodbridge, Suffolk. There, large mounds—which were originally much higher and more visible—can still be seen, overlooking the upper estuary of the River Deben.
In 1939, a mound at Sutton Hoo, now known as Mound 1, was discovered to contain an Anglo-Saxon ship-burial of unparalleled richness. The mound enclosed a ship, 27 m long, which had seen use on the seas and had been repaired. In the centre of the ship was a chamber containing a collection of jewellery and other rich grave goods, including silver bowls, drinking vessels, clothing and weaponry. One unusual item was a large 'sceptre' in the form of a whetstone that showed no sign of previous use as a tool: it has been suggested that this was a symbol of the office of "bretwalda". The gold and garnet body-equipment found with the other goods was produced for a patron who employed a goldsmith the equal or better than any in Europe and was designed to project an image of imperial power. The Mediterranean silverware in the grave is a unique assemblage for its period in Europe.
The magnificence of the objects, both the personal possessions and those items designed to denote the authority of the dead individual, point to the death of a person connected with the royal court, according to Rupert Bruce-Mitford, who regards the burial as "very likely the monument of the High King or "bretwalda" Raedwald". Yorke suggests that the treasures buried with the ship reflect the size of the tribute paid to Rædwald by subject kings during his period as "bretwalda". Bruce-Mitford has suggested that the inclusion of bowls and spoons amongst the treasures fits with Bede's account of Raedwald's conversion: the spoons may have been a present for a convert from paganism and the bowls had Christian significance. Coins found in the burial have been dated to the approximate date of Rædwald's death. The controversy surrounding the identity of the person for whom the mound was built are reflected in the comments in the article on Rædwald in the "Oxford Dictionary of National Biography" ("It has been argued, more strongly than convincingly, that Rædwald must be the man buried in Mound 1 at Sutton Hoo") and by McClure and Collins, who note that the evidence for Raedwald is "almost non-existent".
Alternative suggestions as candidates include other East Anglian kings or a prestigious foreign visitor. There are alternative explanations: the person may have been a wealthy status-seeker, rather than a king, though Rendlesham, a known residence of the East Anglian kings, is only 4 miles away.
Swedish cultural influence has been detected at Sutton Hoo: there are strong similarities in both the armour and the burial with Vendel-age finds from Sweden. Bruce-Mitford suggested that the connection is close enough to imply that the Wuffing dynasty came from that part of Scandinavia. There are also significant differences, and exact parallels with the workmanship and style of the Sutton Hoo artefacts cannot be found elsewhere; as a result the connection is generally regarded as unproven.
It is also possible that the mound is actually a cenotaph rather than a grave, the only sign of body was a chemical stain which could have had other origins, indeed the site includes burials of both meat and companion animals. Further there is a lack of shroud ties, and no clear evidence of items which might have adorned a body being left in the expected places in relation to the stain. The cenotaph theory may be consistent with the transition from pagan burial to Christian burial, certainly as far as Rædwald is concerned, he could have received a Christian burial, and the mound, whether completed before or after his conversion being used as a memorial and as symbol of the status of the Kingship of East Anglia.
Sources.
Primary sources
Secondary sources

</doc>
<doc id="26226" url="http://en.wikipedia.org/wiki?curid=26226" title="Rhyme">
Rhyme

A rhyme is a repetition of similar sounds (or the same sound) in two or more words, most often in the final syllables of lines in poems and songs. The word "rhyme" may also be used as a "pars pro toto" ("a part (taken) for the whole") to refer a short poem, such as a rhyming couplet or other brief rhyming poem such as nursery rhymes.
Function of rhyming words.
Rhyme partly seems to be enjoyed simply as a repeating pattern that is pleasant to hear. It also serves as a powerful mnemonic device, facilitating memorization. The regular use of tail rhyme helps to mark off the ends of lines, thus clarifying the metrical structure for the listener. As with other poetic techniques, poets use it to suit their own purposes; for example William Shakespeare often used a rhyming couplet to mark off the end of a scene in a play.
Types of rhyme.
The word "rhyme" can be used in a specific and a general sense. In the specific sense, two words rhyme if their final stressed vowel and all following sounds are identical; two lines of poetry rhyme if their final strong positions are filled with rhyming words. A rhyme in the strict sense is also called a perfect rhyme. Examples are "sight" and "flight", "deign" and "gain", "madness" and "sadness".
Perfect rhymes.
Perfect rhymes can be classified according to the number of syllables included in the rhyme, which is dictated by the location of the final stressed syllable.
General rhymes.
In the general sense, "general rhyme" can refer to various kinds of phonetic similarity between words, and to the use of such similar-sounding words in organizing verse. Rhymes in this general sense are classified according to the degree and manner of the phonetic similarity:
Identical rhymes.
Identical rhymes are considered less than perfect in English poetry; but are valued more highly in other literatures such as, for example, "rime riche" in French poetry.
Though homophones and homonyms satisfy the first condition for rhyming—that is, that the stressed vowel sound is the same—they do not satisfy the second: that the preceding consonant be different. As stated above, in a perfect rhyme the last stressed vowel and all following sounds are identical in both words.
If the sound preceding the stressed vowel is also identical, the rhyme is sometimes considered to be inferior and not a perfect rhyme after all. An example of such a "super-rhyme" or "more than perfect rhyme" is the "identical rhyme", in which not only the vowels but also the onsets of the rhyming syllables are identical, as in "gun" and "begun". Punning rhymes such are "bare" and "bear" are also identical rhymes. The rhyme may extend even farther back than the last stressed vowel. If it extends all the way to the beginning of the line, so that there are two lines that sound identical, then it is called a "holorhyme" ("For I scream/For ice cream").
In poetics these would be considered "identity", rather than rhyme.
Eye rhyme.
Eye rhymes or sight rhymes or spelling rhymes refer to similarity in spelling but not in sound where the final sounds are spelled identically but pronounced differently. Examples in English are "cough", "bough", and "love", "move".
Some early written poetry appears to contain these, but in many cases the words used rhymed at the time of writing, and subsequent changes in pronunciation have meant that the rhyme is now lost.
Mind rhyme.
Mind rhyme is a kind of substitution rhyme similar to rhyming slang, but it is less generally codified and is “heard” only when generated by a specific verse context. For instance, “this sugar is neat / and tastes so sour.” If a reader or listener thinks of the word “sweet” instead of “sour”, then a mind rhyme has occurred.
Classification by position.
Rhymes may be classified according to their position in the verse:
A rhyme scheme is the pattern of rhyming lines in a poem.
History.
In many languages, including modern European languages and Arabic, poets use rhyme in set patterns as a structural element for specific poetic forms, such as ballads, sonnets and rhyming couplets. Some rhyming schemes have become associated with a specific language, culture or period, while other rhyming schemes have achieved use across languages, cultures or time periods. However, the use of structural rhyme is not universal even within the European tradition. Much modern poetry avoids traditional rhyme schemes.
The earliest surviving evidence of rhyming is the Chinese Shi Jing (ca. 10th century BC). Rhyme is also occasionally used in the Bible. Overwhelmingly, Classical Greek and Latin poetry did not use rhyme. But rhyme is used very occasionally. For instance, Catullus includes partial rhymes in the poem "Cui dono lepidum novum libellum". The ancient Greeks knew rhyme, and rhymes in "The Wasps" by Aristophanes are noted by a translator.
According to some archaic sources, Irish literature introduced the rhyme to Early Medieval Europe, though this is a disputed claim; in the 7th century we find the Irish had brought the art of rhyming verses to a high pitch of perfection. Also in the 7th Century, rhyme was used in the Qur'an. The leonine verse is notable for introducing rhyme into High Medieval literature in the 12th century.
Rhyme entered European poetry in the High Middle Ages, in part under the influence of the Arabic language in Al Andalus (modern Spain). Arabic language poets used rhyme extensively from the first development of literary Arabic in the sixth century, as in their long, rhyming qasidas.
Since languages change over time, lines which rhymed in the past may no longer rhyme in today's language and it may not be clear how one would pronounce the words so that they rhyme. For example:
"Should we really sing 'harmonious jine' [or 'songs divoin']?"
Etymology.
The word is derived from Old French "rime" or "ryme", which may be derived from Old Frankish "*rīm", a Germanic term meaning "series, sequence" attested in Old English (Old English "rīm" meaning "enumeration, series, numeral") and Old High German "rīm", ultimately cognate to Old Irish "rím", Greek "ἀριθμός" "arithmos" "number". Alternatively, the Old French words may derive from Latin "rhythmus", from Greek "ῥυθμός" ("rhythmos", rhythm).
The spelling "rhyme" (from original "rime") was introduced at the beginning of the Modern English period, due to a learned (but perhaps etymologically incorrect) association with Latin "rhythmus". The older spelling "rime" survives in Modern English as a rare alternative spelling. A distinction between the spellings is also sometimes made in the study of linguistics and phonology, where "rime/rhyme" is used to refer to the nucleus and coda of a syllable. In this context, some prefer to spell this "rime" to separate it from the poetic rhyme covered by this article (see syllable rime).
Rhyme in various languages.
Celtic languages.
Rhyming in the Celtic Languages takes a drastically different course from most other Western rhyming schemes despite strong contact with the Romance and English patterns. Even today, despite extensive interaction with English and French culture, Celtic rhyme continues to demonstrate native characteristics. Brian Ó Cuív sets out the rules of rhyme in Irish poetry of the classical period: the last stressed vowel and any subsequent long vowels must be identical in order for two words to rhyme. Consonants are grouped into six classes for the purpose of rhyme: they need not be identical, but must belong to the same class. Thus 'b' and 'd' can rhyme (both being 'voiced plosives'), as can 'bh' and 'l' (which are both 'voiced continuants') but 'l', a 'voiced continuant', cannot rhyme with 'ph', a 'voiceless continuant'. Furthermore, "for perfect rhyme a palatalized consonant may be balanced only by a palatalized consonant and a velarized consonant by a velarized one." In the post-Classical period, these rules fell into desuetude, and in popular verse simple assonance often suffices, as can be seen in an example of Irish Gaelic rhyme from the traditional song "Bríd Óg Ní Mháille":
Translation: "Oh young Bridget O'Malley / You have left my heart breaking"
Here the vowels are the same, but the consonants, although both palatalized, do not fall into the same class in the bardic rhyming scheme.
Chinese.
Besides the vowel/consonant aspect of rhyming, Chinese language rhymes often include tone quality (that is, tonal contour) as an integral linguistic factor in determining rhyme.
Use of rhyme in Classical Chinese poetry typically but not always appears in the form of paired couplets, with end-rhyming in the final syllable of each couplet.
Another important aspect of rhyme in regard to Chinese language studies is the study or reconstruction of past varieties of Chinese, such as Middle Chinese.
English.
Old English poetry is mostly alliterative verse. One of the earliest rhyming poems in English is The Rhyming Poem.
As English is a language in which stress is important, lexical stress is one of the factors affecting the similarity of sounds for the perception of rhyme. Perfect rhyme can be defined as the case when two words rhyme if their final stressed vowel and all following sounds are identical.
Some words in English, such as "orange" and "silver", are commonly regarded as having no rhyme. Although a clever writer can get around this (for example, by obliquely rhyming "orange" with combinations of words like "door hinge" or with lesser-known words like "Blorenge" – a hill in Wales – or the surname Gorringe), it is generally easier to move the word out of rhyming position or replace it with a synonym ("orange" could become "amber")("Silver" could become a combination of "bright and argent").
One view of rhyme in English is from John Milton's preface to "Paradise Lost":
A more tempered view is taken by W. H. Auden in The Dyer's Hand:
Forced or clumsy rhyme is often a key ingredient of doggerel.
French.
In French poetry, unlike in English, it is common to have "identical rhymes", in which not only the vowels of the final syllables of the lines rhyme, but their onset consonants ("consonnes d'appui") as well. To the ear of someone accustomed to English verse, this often sounds like a very weak rhyme. For example, an English perfect rhyme of homophones, "flour" and "flower", would seem weak, whereas a French rhyme of homophones "doigt" and "doit" is not only acceptable but quite common.
Rhymes are sometimes classified into the categories "rime pauvre" ("poor rhyme"), "rime suffisante" ("sufficient rhyme"), "rime riche" ("rich rhyme") and "rime richissime" ("very rich rhyme"), according to the number of rhyming sounds in the two words or in the parts of the two verses. For example to rhyme "parla" with "sauta" would be a poor rhyme (the words have only the vowel in common), to rhyme "pas" with "bras" a sufficient rhyme (with the vowel and the silent consonant in common), and "tante" with "attente" a rich rhyme (with the vowel, the onset consonant, and the coda consonant with its mute "e" in common). Authorities disagree, however, on exactly where to place the boundaries between the categories.
"Holorime" is an extreme example of "rime richissime" spanning an entire verse. Alphonse Allais was a notable exponent of holorime. Here is an example of a holorime couplet from Marc Monnier:
Classical French rhyme not only differs from English rhyme in its different treatment of onset consonants. It also treats coda consonants in a distinctive way.
French spelling includes several final letters that are no longer pronounced, and that in many cases have never been pronounced. Such final unpronounced letters continue to affect rhyme according to the rules of Classical French versification. They are encountered in almost all of the pre-20th-century French verse texts, but these rhyming rules are almost never taken into account from the 20th century.
The most important "silent" letter is the "mute e". In spoken French today, final "e" is, in some regional accents (in Paris for example), omitted after consonants; but in Classical French prosody, it was considered an integral part of the rhyme even when following the vowel. "Joue" could rhyme with "boue", but not with "trou". Rhyming words ending with this silent "e" were said to make up a "double rhyme", while words not ending with this silent "e" made up a "single rhyme". It was a principle of stanza-formation that single and double rhymes had to alternate in the stanza. All 17th-century French plays in verse alternate single and double alexandrine couplets.
The "silent" final consonants present a more complex case. They, too, were considered an integral part of the rhyme, so that "pont" could rhyme only with "vont" and not with "long"; but this cannot be reduced to a simple rule about the spelling, since "pont" would also rhyme with "rond" even though one word ends in "t" and the other in "d". This is because the correctness of the rhyme depends not on the spelling on the final consonant, but on how it would have been pronounced. There are a few simple rules that govern word-final consonants in French prosody:
In fact, only the "silent" final consonants which would be able to be pronounced the same way, if they were followed by a vowel, are able to rhyme together.
Hebrew.
Ancient Hebrew verse generally did not employ rhyme. However, many Jewish liturgical poems rhyme today, because they were written in medieval Europe, where rhymes were in vogue.
Latin.
In Latin rhetoric and poetry homeoteleuton and alliteration were frequently used devices.
Tail rhyme was occasionally used, as in this piece of poetry by Cicero:
But tail rhyme was not used as a prominent structural feature of Latin poetry until it was introduced under the influence of local vernacular traditions in the early Middle Ages. This is the Latin hymn "Dies Irae":
Medieval poetry may mix Latin and vernacular languages. Mixing languages in verse or rhyming words in different languages is termed macaronic.
Portuguese.
Portuguese classifies rhymes in the following manner:
Russian.
Rhyme was introduced into Russian poetry in the 18th century. Folk poetry had generally been unrhymed, relying more on dactylic line endings for effect. Rhyme depends on a vowel and adjacent consonant (which may include the semivowel "Short I"). Vowel pairs rhyme - even though non-Russian speakers may not perceive them as the same sound. Consonant pairs rhyme if both are devoiced. Early 18th century poetry demanded perfect rhymes which were also grammatical rhymes, namely that noun endings rhymed with noun endings, verb endings with verb endings, and so on. Such rhymes relying on morphological endings become much rarer in modern Russian poetry, and greater use is made of approximate rhymes.
Sanskrit.
Patterns of rich rhyme ("prāsa") play a role in modern Sanskrit poetry, but only to a minor extent in historical Sanskrit texts. They are classified according to their position within the "pada" (metrical foot): "ādiprāsa" (first syllable), "dvitīyākṣara prāsa" (second syllable), "antyaprāsa" (final syllable) etc.
Tamil.
There are some unique rhyming schemes in Dravidian languages like Tamil. Specifically, the rhyme called "etukai" (anaphora) occurs on the second consonant of each line.
The other rhyme and related patterns are called "mōnai" (alliteration), "toṭai" (epiphora) and "iraṭṭai kiḷavi" (parallelism).
Some classical Tamil poetry forms, such as "veṇpā", have rigid grammars for rhyme to the point that they could be expressed as a context-free grammar.

</doc>
<doc id="26227" url="http://en.wikipedia.org/wiki?curid=26227" title="Rhythm">
Rhythm

Rhythm (from Greek ῥυθμός, "rhythmos", "any regular recurring motion, symmetry" ) generally means a "movement marked by the regulated succession of strong and weak elements, or of opposite or different conditions" . This general meaning of regular recurrence or pattern in time can apply to a wide variety of cyclical natural phenomena having a periodicity or frequency of anything from microseconds to millions of years.
In the performance arts rhythm is the timing of events on a human scale; of musical sounds and silences, of the steps of a dance, or the meter of spoken language and poetry. Rhythm may also refer to visual presentation, as "timed movement through space" (, ) and a common language of pattern unites rhythm with geometry. In recent years, rhythm and meter have become an important area of research among music scholars. Recent work in these areas includes books by Maury Yeston , Fred Lerdahl and Ray Jackendoff, Jonathan Kramer, Christopher Hasty , Godfried Toussaint , William Rothstein, and Joel Lester .
In "Thinking and Destiny", Harold W. Percival defined rhythm as the character and meaning of thought expressed through the measure or movement in sound or form, or by written signs or words .
Anthropology.
In his television series "How Music Works", Howard Goodall presents theories that human rhythm recalls the regularity with which we walk and the heartbeat . Other research suggests that it does not relate to the heartbeat directly, but rather the speed of emotional affect, which also influences heartbeat. Yet other researchers suggest that since certain features of human music are widespread, it is reasonable to suspect that beat-based rhythmic processing has ancient evolutionary roots (, ). Justin London writes that musical metre "involves our initial perception as well as subsequent anticipation of a series of beats that we abstract from the rhythm surface of the music as it unfolds in time" . The "perception" and "abstraction" of rhythmic measure is the foundation of human instinctive musical participation, as when we divide a series of identical clock-ticks into "tick-tock-tick-tock" (; ).
Some types of parrots can know rhythm . Neurologist Oliver Sacks states that chimpanzees and other animals show no similar appreciation of rhythm yet posits that human affinity for rhythm is fundamental, so that a person's sense of rhythm cannot be lost (e.g. by stroke). "There is not a single report of an animal being trained to tap, peck, or move in synchrony with an auditory beat" (, cited in , who adds, "No doubt many pet lovers will dispute this notion, and indeed many animals, from the Lippizaner horses of the Spanish Riding School of Vienna to performing circus animals appear to 'dance' to music. It is not clear whether they are doing so or are responding to subtle visual or tactile cues from the humans around them.") Human rhythmic arts are possibly to some extent rooted in courtship ritual (, ).
The establishment of a basic beat requires the perception of a regular sequence of distinct short-duration pulses and, as subjective perception of loudness is relative to background noise levels, a pulse must decay to silence before the next occurs if it is to be really distinct. For this reason the fast-transient sounds of percussion instruments lend themselves to the definition of rhythm. Musical cultures that rely upon such instruments may develop multi-layered polyrhythm and simultaneous rhythms in more than one time signature, called polymeter. Such are the cross-rhythms of Sub-Saharan Africa and the interlocking "kotekan" rhythms of the "gamelan".
For information on rhythm in Indian music see Tala (music). For other Asian approaches to rhythm see Rhythm in Persian music, Rhythm in Arabian music and "Usul"—Rhythm in Turkish music and Dumbek rhythms.
Terminology.
Pulse, beat and measure.
"(See main articles; Pulse (music), Beat (music))"
Most music, dance and oral poetry establishes and maintains an underlying "metric level", a basic unit of time that may be audible or implied, the pulse or "tactus" of the mensural level (; ; ), or "beat level", sometimes simply called the beat. This consists of a (repeating) series of identical yet distinct periodic short-duration stimuli perceived as points in time . The "beat" pulse is not necessarily the fastest or the slowest component of the rhythm but the one that is perceived as basic: it has a tempo to which listeners entrain as they tap their foot or dance to a piece of music . It is currently most often designated as a crotchet or quarter note in western notation (see time signature). Faster levels are "division levels", and slower levels are "multiple levels" . "Rhythms of recurrence" arise from the interaction of two levels of motion, the faster providing the pulse and the slower organizing the beats into repetitive groups . "Once a metric hierarchy has been established, we, as listeners, will maintain that organization as long as minimal evidence is present" .
Unit and gesture.
A durational pattern that synchronises with a pulse or pulses on the underlying metric level may be called a "rhythmic unit". These may be classified as; "metric"—even patterns, such as steady eighth notes or pulses—"intrametric"—confirming patterns, such as dotted eighth-sixteenth note and swing patterns—"contrametric"—non-confirming, or syncopated patterns and "extrametric"—irregular patterns, such as tuplets.
A rhythmic gesture is any durational pattern that, in contrast to the rhythmic unit, does not occupy a period of time equivalent to a pulse or pulses on an underlying metric level. It may be described according to its beginning and ending or by the rhythmic units it contains. Beginnings on a strong pulse are "thetic", a weak pulse, "anacrustic" and those beginning after a rest or tied-over note are called "initial rest". Endings on a strong pulse are "strong", a weak pulse, "weak" and those that end on a strong or weak upbeat are "upbeat" .
Alternation and repetition.
Rhythm is marked by the regulated succession of opposite elements, the dynamics of the strong and weak beat, the played beat and the inaudible but implied rest beat, the long and short note. As well as perceiving rhythm we must be able to anticipate it. This depends upon repetition of a pattern that is short enough to memorize.
The alternation of the strong and weak beat is fundamental to the ancient language of poetry, dance and music. The common poetic term "foot" refers, as in dance, to the lifting and tapping of the foot in time. In a similar way musicians speak of an upbeat and a downbeat and of the "on" and "off" beat. These contrasts naturally facilitate a dual hierarchy of rhythm and depend upon repeating patterns of duration, accent and rest forming a "pulse-group" that corresponds to the poetic foot. Normally such pulse-groups are defined by taking the most accented beat as the first and counting the pulses until the next accent (; ). A rhythm that accents another beat and de-emphasises the down beat as established or assumed from the melody or from a preceding rhythm is called syncopated rhythm.
Normally, even the most complex of meters may be broken down into a chain of duple and triple pulses (; ) either by addition or division. According to Pierre Boulez, beat structures beyond four, in western music, are "simply not natural" . Western rhythms are usually arranged with respect to a time signature, partially signifying a meter usually corresponding to measure length and grouped into either two or three beats, which are called duple meter and triple meter, respectively. If the beats are in consistently even or odd groups of two, three, or four, it is simple meter, if by admixtures of two and three it is compound meter. In other systems of music such as Indian classical music rhythms may be grouped into various number of beats. In some music styles such as Yakshagana even group rhythms into fractional beats.
Tempo and duration.
"(See main articles; Duration (music), Tempo)"
The tempo of the piece is the speed or frequency of the "tactus", a measure of how quickly the beat flows. This is often measured in 'beats per minute' (bpm): 60 bpm means a speed of one beat per second, a frequency of 1 Hz. A rhythmic unit is a durational pattern that has a period equivalent to a pulse or several pulses . The duration of any such unit is inversely related to its tempo.
Musical sound may be analyzed on five different time scales, which Moravscik has arranged in order of increasing duration .
Curtis Roads takes a wider view by distinguishing nine time scales, this time in order of decreasing duration. The first two, the infinite and the supramusical, encompass natural periodicities of months, years, decades, centuries, and greater, while the last three, the sample and subsample, which take account of digital and electronic rates "too brief to be properly recorded or perceived", measured in millionths of seconds (microseconds), and finally the infinitesimal or infinitely brief, are again in the extra-musical domain. Roads' Macro level, encompassing "overall musical architecture or form" roughly corresponds to Moravcsik's "very long" division while his Meso level, the level of "divisions of form" including movements, sections, phrases taking seconds or minutes, is likewise similar to Moravcsik's "long" category. Roads' Sound object (; ): "a basic unit of musical structure" and a generalization of note (Xenakis' ministructural time scale); fraction of a second to several seconds, and his Microsound (see granular synthesis) down to the threshold of audible perception; thousands to millionths of seconds, are similarly comparable to Moravcsik's "short" and "supershort" levels of duration.
Metric structure.
"(See main articles; Metre (music), Bar (music), Metre (poetry))"
The study of rhythm, stress, and pitch in speech is called prosody: it is a topic in linguistics and poetics, where it means the number of lines in a verse, the number of syllables in each line and the arrangement of those syllables as long or short, accented or unaccented. Music inherited the term "meter or metre" from the terminology of poetry (; ; ).
The metric structure of music includes meter, tempo and all other rhythmic aspects that produce temporal regularity against which the foreground details or durational patterns of the music are projected (, ). The terminology of western music is notoriously imprecise in this area . preferred to speak of "time" and "rhythmic shape", Imogen Holst of "measured rhythm".
Dance music has instantly recognizable patterns of beats built upon a characteristic tempo and measure. The Imperial Society of Teachers of Dancing defines the tango, for example, as to be danced in 2/4 time at approximately 66 beats per minute. The basic slow step forwards or backwards, lasting for one beat, is called a "slow", so that a full "right-left" step is equal to one 2/4 measure (, ).
The general classifications of "metrical rhythm", "measured rhythm", and "free rhythm" may be distinguished . Metrical or divisive rhythm, by far the most common in Western music calculates each time value as a multiple or fraction of the beat. Normal accents re-occur regularly providing systematical grouping (measures). Measured rhythm (additive rhythm) also calculates each time value as a multiple or fraction of a specified time unit but the accents do not recur regularly within the cycle. Free rhythm is where there is neither , such as in Christian chant, which has a basic pulse but a freer rhythm, like the rhythm of prose compared to that of verse . "See Free time (music)".
Finally some music, such as some graphically scored works since the 1950s and non-European music such as Honkyoku repertoire for shakuhachi, may be considered "ametric" . "Senza misura" is an Italian musical term for "without meter", meaning to play without a beat, using time to measure how long it will take to play the bar ().
Composite rhythm.
A "composite rhythm" is the durations and patterns (rhythm) produced by amalgamating all sounding parts of a musical texture. In music of the common practice period, the composite rhythm usually confirms the meter, often in metric or even-note patterns identical to the pulse on a specific metric level. White defines "composite rhythm" as, "the resultant overall rhythmic articulation among all the voices of a contrapuntal texture" .
Rhythm notation.
Worldwide there are many different approaches to passing on rhythmic phrases and patterns, as they exist in traditional music, from generation to generation.
African music.
In the Griot tradition of Africa everything related to music has been passed on orally. Babatunde Olatunji (1927–2003), a Nigerian drummer who lived and worked in the United States, developed a simple series of spoken sounds for teaching the rhythms of the hand drum. He used six vocal sounds: Goon Doon Go Do Pa Ta. There are three basic sounds on the drum, but each can be played with either the left or the right hand. This simple system is now used worldwide, particularly by Djembe players.
It is noteworthy that the debate about the appropriateness of staff notation for African music is a subject of particular interest to outsiders, not insiders. African scholars from Kyagambiddwa to Kongo have, for the most part, accepted the conventions—and limitations—of staff notation, and gone on to produce transcriptions in order to inform and make possible a higher level of discussion and debate 
John Miller has argued that West African music is based on tension between rhythms. This tension between rhythms is called polyrhythms and is created by the simultaneous sounding of two or more different rhythms. Often there is a dominant rhythm interacting with an independent competing rhythm, or rhythms. These often oppose or complement each other, and combine freely with the dominant rhythm creating a rich rhythmic texture not limited to any one set meter or tempo.
A set of moral values underpins a full musical system based on repetition of relatively simple patterns that meet at distant cross-rhythmic intervals and call and answer schemes. Values also show up in collective utterances such as proverbs or lineages appear either in phrases that translate as drum talk or in the words of songs. People expect musicians to stimulate participation of all present, notably by reacting to people dancing the music. Appreciation of musicians is related to the effectiveness of their upholding community values .
Indian music.
Indian music has also been passed on orally. Tabla players would learn to speak complex rhythm patterns and phrases before attempting to play them. Sheila Chandra, an English pop singer of Indian descent, made performances based on her singing these patterns. In Indian Classical music, the Tala of a composition is the rhythmic pattern over which the whole piece is structured.
Western music.
In the 20th century, composers like Igor Stravinsky, Béla Bartók, Philip Glass, and Steve Reich wrote more rhythmically complex music using odd meters, and techniques such as phasing and additive rhythm. At the same time, modernists such as Olivier Messiaen and his pupils used increased complexity to disrupt the sense of a regular beat, leading eventually to the widespread use of irrational rhythms in New Complexity. This use may be explained by a comment of John Cage's where he notes that regular rhythms cause sounds to be heard as a group rather than individually; the irregular rhythms highlight the rapidly changing pitch relationships that would otherwise be subsumed into irrelevant rhythmic groupings . LaMonte Young also wrote music in which the sense of a regular beat is absent because the music consists only of long sustained tones (drones). In the 1930s, Henry Cowell wrote music involving multiple simultaneous periodic rhythms and collaborated with Léon Thérémin to invent the Rhythmicon, the first electronic rhythm machine, in order to perform them. Similarly, Conlon Nancarrow wrote for the player piano.
Use of polyrhythms in American music is generally traced to the influence of black culture through Dixieland and Jazz styles. The effect of multiple soloing in these forms, often utilizing cross-rhythms comes directly from the underlying aesthetics of sub-Saharan African music. These complex rhythmic structures have been widely adopted in many current forms of western popular music.
Linguistics.
In linguistics, rhythm or isochrony is one of the three aspects of prosody, along with stress and intonation. Languages can be categorized according to whether they are syllable-timed, mora-timed, or stress-timed. Speakers of syllable-timed languages such as Spanish and Cantonese put roughly equal time on each syllable; in contrast, speakers of stressed-timed languages such as English and Mandarin Chinese put roughly equal time lags between stressed syllables, with the timing of the unstressed syllables in between them being adjusted to accommodate the stress timing.
 (cited in , ) describes three categories of prosodic rules that create rhythmic successions that are additive (same duration repeated), cumulative (short-long), or countercumulative (long-short). Cumulation is associated with closure or relaxation, countercumulation with openness or tension, while additive rhythms are open-ended and repetitive. Richard Middleton points out this method cannot account for syncopation and suggests the concept of transformation (, ).

</doc>
<doc id="26228" url="http://en.wikipedia.org/wiki?curid=26228" title="Rondeau">
Rondeau

Rondeau may refer to:

</doc>
<doc id="26229" url="http://en.wikipedia.org/wiki?curid=26229" title="Riboflavin">
Riboflavin

Riboflavin (vitamin B2) is part of the vitamin B group. It is the central component of the cofactors FAD and FMN and as such required for a variety of flavoprotein enzyme reactions including activation of other vitamins. It was formerly known as vitamin G.
Riboflavin is a yellow-orange solid substance with poor solubility in water. It is best known visually as it imparts the color to vitamin supplements and the yellow color to the urine of persons taking it.
The name "riboflavin" comes from "ribose" (the sugar whose reduced form, ribitol, forms part of its structure) and "flavin", the ring-moiety which imparts the yellow color to the oxidized molecule (from Latin "flavus", "yellow"). The reduced form, which occurs in metabolism along with the oxidized form, is colorless.
Function.
The active forms Flavin mononucleotide (FMN) and flavin adenine dinucleotide (FAD) function as cofactors for a variety of flavoproteine enzyme reactions:
For the molecular mechanism of action see main articles Flavin mononucleotide (FMN) and flavin adenine dinucleotide (FAD)
Nutrition.
Food sources.
Sources of riboflavin are milk, cheese, leaf vegetables, liver, kidneys, legumes, yeast, mushrooms, and almonds.
Yeast extract is considered to be exceptionally rich in vitamin B2. Cereals contain relatively low concentrations of flavins, but are important sources in those parts of the world where cereals constitute the staple diet.
The milling of cereals results in considerable loss (up to 60%) of vitamin B2, so white flour is enriched in some countries such as USA by addition of the vitamin. The enrichment of bread and ready-to-eat breakfast cereals contributes significantly to the dietary supply of vitamin B2. Polished rice is not usually enriched, because the vitamin’s yellow color would make the rice visually unacceptable to the major rice-consumption populations. However, most of the flavin content of whole brown rice is retained if the rice is steamed (parboiled) prior to milling. This process drives the flavins in the germ and aleurone layers into the endosperm. Free riboflavin is naturally present in foods along with protein-bound FMN and FAD. Bovine milk contains mainly free riboflavin, with a minor contribution from FMN and FAD. In whole milk, 14% of the flavins are bound noncovalently to specific proteins. Egg white and egg yolk contain specialized riboflavin-binding proteins, which are required for storage of free riboflavin in the egg for use by the developing embryo.
It is used in baby foods, breakfast cereals, pastas, sauces, processed cheese, fruit drinks, vitamin-enriched milk products, and some energy drinks. It is difficult to incorporate riboflavin into many liquid products because it has poor solubility in water, hence the requirement for riboflavin-5'-phosphate (E101a), a more soluble form of riboflavin. Riboflavin is also used as a food coloring and as such is designated in Europe as the E number E101.
Riboflavin is generally stable during the heat processing and normal cooking of foods if light is excluded. The alkaline conditions in which riboflavin is unstable are rarely encountered in foodstuffs. Riboflavin degradation in milk can occur slowly in dark during storage in the refrigerator.
Dietary reference intakes.
The latest (1998) RDA recommendations for vitamin B2 are similar to the 1989 RDA, which for adults, suggested a minimum intake of 1.2 mg for persons whose caloric intake may be > 2,000 Kcal. The current RDAs for riboflavin for adult men and women are 1.3 mg/day and 1.1 mg/day, respectively; the estimated average requirement for adult men and women are 1.1 mg and 0.9 mg, respectively. Recommendations for daily riboflavin intake increase with pregnancy and lactation to 1.4 mg and 1.6 mg, respectively (1in advanced). For infants, the RDA is 0.3-0.4 mg/day and for children it is 0.6-0.9 mg/day.
Deficiency.
Signs and symptoms.
In humans.
Riboflavin deficiency (also called ariboflavinosis) results in stomatitis including painful red tongue with sore throat, chapped and fissured lips (cheilosis), and inflammation of the corners of the mouth (angular stomatitis). There can be oily scaly skin rashes on the scrotum, vulva, philtrum of the lip, or the nasolabial folds. The eyes can become itchy, watery, bloodshot and sensitive to light. Due to interference with iron absorption, riboflavin deficiency results in an anemia with normal cell size and normal hemoglobin content (i.e. normochromic normocytic anemia). This is distinct from anemia caused by deficiency of folic acid (B9) or cyanocobalamin (B12), which causes anemia with large blood cells (megaloblastic anemia). Deficiency of riboflavin during pregnancy can result in birth defects including congenital heart defects and limb deformities.
The stomatitis symptoms are similar to those seen in pellagra, which is caused by niacin (B3) deficiency. Therefore, riboflavin deficiency is sometimes called "pellagra sine pellagra" (pellagra without pellagra), because it causes stomatitis but not widespread peripheral skin lesions characteristic of niacin deficiency.
Riboflavin deficiency has been implicated in cancer, and has been noted to prolong recovery from malaria, despite preventing growth of plasmodium.
In other animals.
In other animals, riboflavin deficiency results in lack of growth, failure to thrive, and eventual death. Experimental riboflavin deficiency in dogs results in growth failure, weakness, ataxia, and inability to stand. The animals collapse, become comatose, and die. During the deficiency state, dermatitis develops together with hair loss. Other signs include corneal opacity, lenticular cataracts, hemorrhagic adrenals, fatty degeneration of the kidney and liver, and inflammation of the mucous membrane of the gastrointestinal tract. Post-mortem studies in rhesus monkeys fed a riboflavin-deficient diet revealed about one-third the normal amount of riboflavin was present in the liver, which is the main storage organ for riboflavin in mammals. Riboflavin deficiency in birds results in low egg hatch rates.
Diagnosis.
Overt clinical signs are rarely seen among inhabitants of the developed countries. The assessment of Riboflavin status is essential for confirming cases with unspecific symptoms where deficiency is suspected.
Causes.
Riboflavin is continuously excreted in the urine of healthy individuals, making deficiency relatively common when dietary intake is insufficient. Riboflavin deficiency is usually found together with other nutrient deficiencies, particularly of other water-soluble vitamins.
A deficiency of riboflavin can be primary - poor vitamin sources in one's daily diet - or secondary, which may be a result of conditions that affect absorption in the intestine, the body not being able to use the vitamin, or an increase in the excretion of the vitamin from the body.
Subclinical deficiency has also been observed in women taking oral contraceptives, in the elderly, in people with eating disorders, chronic alcoholism and in diseases such as HIV, inflammatory bowel disease, diabetes and chronic heart disease.
Phototherapy to treat jaundice in infants can cause increased degradation of riboflavin, leading to deficiency if not monitored closely.
Treatment.
Treatment involves a diet which includes an adequate amount of riboflavin usually in form of commercially available supplements.
Medical uses.
Riboflavin has been used in several clinical and therapeutic situations. For over 30 years, riboflavin supplements have been used as part of the phototherapy treatment of neonatal jaundice. The light used to irradiate the infants breaks down not only bilirubin, the toxin causing the jaundice, but also the naturally occurring riboflavin within the infant's blood, so extra supplementation is necessary.
One clinical trial found that high dose riboflavin appears to be useful alone or along with beta-blockers in the prevention of migraine. A dose of 400 mg daily has been used effectively in the prophylaxis of migraines, especially in combination with a daily supplement of magnesium citrate 500 mg and, in some cases, a supplement of coenzyme Q10. However, two other clinical studies have failed to find any significant results for the effectiveness of B2 as a treatment for migraine.
Riboflavin in combination with UV light has been shown to be effective in reducing the ability of harmful pathogens found in blood products to cause disease. When UV light is applied to blood products containing riboflavin, the nucleic acids in pathogens are damaged, rendering them unable to replicate and cause disease. Riboflavin and UV light treatment has been shown to be effective for inactivating pathogens in platelets and plasma, and is under development for application to whole blood. Because platelets and red blood cells do not contain a nucleus (i.e. they have no DNA to be damaged) the technique is well-suited for destroying nucleic acid containing pathogens (including viruses, bacteria, parasites, and white blood cells) in blood products.
Recently, riboflavin has been used in a new treatment to slow or stop the progression of the corneal disorder keratoconus. This is called corneal collagen cross-linking. In corneal crosslinking, riboflavin drops are applied to the patient’s corneal surface. Once the riboflavin has penetrated through the cornea, ultraviolet A light therapy is applied. This induces collagen crosslinking, which increases the tensile strength of the cornea. The treatment has been shown in several studies to stabilize keratoconus.
Treatment for Brown vialetto van laere, fazio londe, and the myopathic form of adult onset coenzyme q10 deficiency.
Industrial uses.
Because riboflavin is fluorescent under UV light, dilute solutions (0.015-0.025% w/w) are often used to detect leaks or to demonstrate coverage in an industrial system such a chemical blend tank or bioreactor. (See the ASME BPE section on Testing and Inspection for additional details.)
Toxicity.
In humans, there is no evidence for riboflavin toxicity produced by excessive intakes, as its low solubility keeps it from being absorbed in dangerous amounts within the digestive tract. Even when 400 mg of riboflavin per day was given orally to subjects in one study for three months to investigate the efficacy of riboflavin in the prevention of migraine headache, no short-term side effects were reported. Although toxic doses can be administered by injection, any excess at nutritionally relevant doses is excreted in the urine, imparting a bright yellow color when in large quantities.
Industrial synthesis.
Various biotechnological processes have been developed for industrial scale riboflavin biosynthesis using different microorganisms, including filamentous fungi such as "Ashbya gossypii", "Candida famata" and "Candida flaveri", as well as the bacteria "Corynebacterium ammoniagenes" and "Bacillus subtilis". The latter organism has been genetically modified to both increase the bacteria's production of riboflavin and to introduce an antibiotic (ampicillin) resistance marker, and is now successfully employed at a commercial scale to produce riboflavin for feed and food fortification purposes. The chemical company BASF has installed a plant in South Korea, which is specialized on riboflavin production using "Ashbya gossypii". The concentrations of riboflavin in their modified strain are so high, that the mycelium has a reddish/brownish color and accumulates riboflavin crystals in the vacuoles, which will eventually burst the mycelium. Riboflavin is sometimes overproduced, possibly as a protective mechanism, by certain bacteria in the presence of high concentrations of hydrocarbons or aromatic compounds. One such organism is "Micrococcus luteus" (American Type Culture Collection strain number ATCC 49442), which develops a yellow color due to production of riboflavin while growing on pyridine, but not when grown on other substrates, such as succinic acid.
Research.
An animal model of riboflavin kinase deficiency has been identified. Since riboflavin cannot be converted into the catalytically active cofactors without this enzyme, a vitamin deficiency syndrome is generated in the model.
History.
Vitamin B was originally considered to have two components, a heat-labile vitamin B1 and a heat-stable vitamin B2. In the 1920s, vitamin B2 was thought to be the factor necessary for preventing pellagra. In 1923, Paul Gyorgy in Heidelberg was investigating egg-white injury in rats; the curative factor for this condition was called vitamin H (which is now called biotin or vitamin B7). Since both pellagra and vitamin H deficiency were associated with dermatitis, Gyorgy decided to test the effect of vitamin B2 on vitamin H deficiency in rats. He enlisted the service of Wagner-Jauregg in Kuhn’s laboratory. In 1933, Kuhn, Gyorgy, and Wagner found that thiamin-free extracts of yeast, liver, or rice bran prevented the growth failure of rats fed a thiamin-supplemented diet.
Further, the researchers noted that a yellow-green fluorescence in each extract promoted rat growth, and that the intensity of fluorescence was proportional to the effect on growth. This observation enabled them to develop a rapid chemical and bioassay to isolate the factor from egg white in 1933, they called it Ovoflavin. The same group then isolated the same preparation (a growth-promoting compound with yellow-green fluorescence) from whey using the same procedure (lactoflavin). In 1934 Kuhn’s group identified the structure of so-called flavin and synthesized vitamin B2.

</doc>
<doc id="26230" url="http://en.wikipedia.org/wiki?curid=26230" title="Rijksmuseum">
Rijksmuseum

The Rijksmuseum (]; English: State Museum) is a Netherlands national museum dedicated to arts and history in Amsterdam. The museum is located at the Museum Square in the borough Amsterdam South, close to the Van Gogh Museum, the Stedelijk Museum Amsterdam, and the Concertgebouw.
The Rijksmuseum was founded in The Hague in 1800 and moved to Amsterdam in 1808, where it was first located in the Royal Palace and later in the Trippenhuis. The current main building was designed by Pierre Cuypers and first opened its doors in 1885. On 13 April 2013, after a ten-year renovation which cost € 375 million, the main building was reopened by Queen Beatrix. In 2013, it was the most visited museum in the Netherlands with a record number of 2.2 million visitors.
The museum has on display 8,000 objects of art and history, from their total collection of 1 million objects from the years 1200–2000, among which are some masterpieces by Rembrandt, Frans Hals, and Johannes Vermeer. The museum also has a small Asian collection which is on display in the Asian pavilion.
History.
18th century.
In 1795, the Batavian Republic was proclaimed. The Minister of Finance Isaac Gogel argued that a national museum, following the French example of The Louvre, would serve the national interest. On 19 November 1798, the government decided to found the museum.
On 31 May 1800, the National Art Gallery (Dutch: "Nationale Kunst-Galerij"), precursor of the Rijksmuseum, opened its doors in Huis ten Bosch in The Hague. The museum exhibited around 200 paintings and historic objects from the collections of the Dutch stadtholders.
19th century.
In 1805, the National Art Gallery moved within The Hague to the Buitenhof.
In 1806, the Kingdom of Holland was established by Napoleon Bonaparte. On the orders of king Louis Bonaparte, brother of Napoleon, the museum moved to Amsterdam in 1808. The paintings owned by that city, such as "The Night Watch" by Rembrandt, became part of the collection. In 1809, the museum opened its doors in the Royal Palace in Amsterdam.
In 1817, the museum moved to the Trippenhuis. The Trippenhuis turned out to be unsuitable as a museum. In 1820, the historical objects were moved to the Mauritshuis in The Hague, and in 1838 the 19th-century paintings were moved to Paviljoen Welgelegen in Haarlem.
In 1863, there was a design contest for a new building for the Rijksmuseum, but none of the submissions was considered to be of sufficient quality. Pierre Cuypers also participated in the contest and his submission reached the second place.
In 1876 a new contest was held and this time Pierre Cuypers won. The design was a combination of gothic and renaissance elements. The construction began on 1 October 1876. On both the inside and the outside, the building was richly decorated with references to Dutch art history. Another contest was held for these decorations. The winners were B. van Hove and J.F. Vermeylen for the sculptures, G. Sturm for the tile tableaus and painting and W.F. Dixon for the stained glass. The museum was opened at its new location on 13 July 1885.
In 1890 a new building was added a short distance to the south-west of the Rijksmuseum. As the building was made out of fragments of demolished buildings, that together give an overview of the history of Dutch architecture, it has come to be known informally as the 'fragment building'. It is also known as the 'south wing', and is currently (in 2013) branded the "Philips Wing".
20th century.
In 1906 the hall for the "Night Watch" was rebuilt. In the interior more changes were made, between the 1920s and 1950s most multi-coloured wall decorations were painted over. In the 1960s exposition rooms and several floors were built into the two courtyards. The building had some minor renovations and restorations in 1984, 1995–1996 and 2000.
A renovation of the south wing of the museum, also known as the 'fragment building' or 'Philips Wing', was completed in 1996, the same year that the museum held its first major photography exhibition featuring its extensive collection of 19th-century photos.
21st century.
In December 2003, the main building of the museum closed for a major renovation. During this renovation, about 400 objects from the collection were on display in the 'fragment building', including Rembrandt's "The Night Watch" and other 17th-century masterpieces.
The restoration and renovation of the Rijksmuseum are based on a design by Spanish architects Antonio Cruz and Antonio Ortiz. Many of the old interior decorations were restored and the floors in the courtyards were removed. The renovation would have initially taken five years, but was delayed and eventually took almost ten years to complete. The renovation cost € 375 million.
The reconstruction of the building was completed on 16 July 2012. In March 2013 the museum's main pieces of art were moved back from the 'fragment building' (Philips Wing) to the main building. "The Night Watch" returned to the Night Watch Room, at the end of the Hall of Fame. On 13 April 2013, the main building was reopened by Queen Beatrix. On 1 November 2014 the Philips Wing reopened with the exhibition .
Building.
The building of the Rijksmuseum was designed by Pierre Cuypers and opened in 1885. It consists of two squares with an atrium in each centre. In the central axis is a tunnel with the entrances at ground level and the Gallery of Honour at the first floor. The building also contains a library. The fragment building, branded Philips wing, contains building fragments that show the history of architecture in the Netherlands. The Rijksmuseum is a "rijksmonument" (national heritage site) since 1970 and was listed in the Top 100 Dutch heritage sites in 1990. The Asian pavilion was designed by Cruz y Ortiz and opened in 2013.
According to Muriel Huisman, Project Architect for the Rijksmuseum's renovation, "Cruz y Ortiz always like to look for a synergy between old and new, and we try not to explain things with our architecture.” With the Rijks, “there’s no cut between old and new; we’ve tried to merge it. We did this by looking for materials that were true to the original building, resulting in a kind of silent architecture."
Collection.
The collection of the Rijksmuseum consists of 1 million objects and is dedicated to arts, crafts, and history from the years 1200 to 2000. Around 8000 objects are currently on display in the museum.
The collection contains more than 2,000 paintings from the Dutch Golden Age by notable painters such as Jacob Isaakszoon van Ruisdael, Frans Hals, Johannes Vermeer, Jan Steen, Rembrandt, and Rembrandt's pupils.
The museum also has a small Asian collection which is on display in the Asian pavilion.
It also displays the stern of HMS Royal Charles which was captured in the Raid on the Medway, and the Hartog plate.
The museum has taken the unusual step of making some 125,000 high-resolution images available for download via its Rijksstudio webplatform, with plans to add another 40,000 images per year until the entire collection of one million works is available, according to Taco Dibbits, director of collections.
Visitors.
The 20th-century visitor record of 1,412,000 was reached in the year 1975.
In the 1990s and early 2000s the Rijksmuseum was annually visited by 0.9 to 1.3 million people. On 7 December 2003, the main building of the museum was closed for a renovation until 13 April 2013. In the following decade, the number of visitors slightly decreased to 0.8 to 1.1 million people. The museum says after the renovation, the museum's capacity is 1.5 to 2.0 million visitors annually. Within eight months since the reopening in 2013, the museum was visited by 2 million people.
The museum was had 2.2 million visitors in 2013 and an all time visitor record of 2.45 million in 2014. The museum was the most visited museum in the Netherlands and the 19th most visited art museum in the world in 2013.
Library.
The Rijksmuseum Research Library is part of the Rijksmuseum, and is the best and the largest public art history research library in The Netherlands.

</doc>
<doc id="26232" url="http://en.wikipedia.org/wiki?curid=26232" title="Ruhollah Khomeini">
Ruhollah Khomeini

Ruhollah Mostafavi Moosavi Khomeini (Persian: روح‌الله خمینی‎, ], 24 September 1902 – 3 June 1989), was an Iranian religious leader and politician, and leader of the 1979 Iranian Revolution which saw the overthrow of Mohammad Reza Pahlavi, the Shah of Iran. Following the revolution, Khomeini became the country's Supreme Leader, a position created in the constitution as the highest ranking political and religious authority of the nation, which he held until his death. After coming to power, Khomeini ordered the destruction of Reza Shah's mausoleum, and executed opposition members by the tens of thousands. Khomeini's killing spree included 1988 executions of Iranian political prisoners and Chain murders of Iran among others. Khomeini used religion to gain and justify his political power. Khomeini’s close circle created a myth around him and elevated him to god-like status.
Khomeini was a "marja" ("source of emulation") in Twelver Shi'a Islam, author of more than forty books, but is primarily known for his political activities. He spent more than 15 years in exile for his opposition to the last Shah. In his writings and preachings he expanded the Shi'a Usuli theory of "velayat-e faqih", the "guardianship of the jurisconsult (clerical authority)" to include theocratic . This principle (though not known to the wider public before the revolution)
was installed in the new Iranian constitution after being put to a referendum. The U.S. State Department ignored the call from the Iranian military and the Grand Ayatollah Kazem Shariatmadari only months after Khomeini's return to Iran to undertake a coup d'etat.
In 1979, the Ayatollah Khomeini created the "Basij Mostazafan", a voluntary mass movement mainly of young people. When the Iran–Iraq War started in 1980, Khomeini issued a fatwa and promise of paradise and they were incorporated into the Iranian military. During Iran–Iraq War, Khomeini organized his "human wave" attacks - mass frontal assaults by thousands of young men, advancing up to certain death. 
He was named Man of the Year in 1979 by American newsmagazine "TIME" for his international influence, and has been described as the "virtual face of Islam in Western popular culture" where he remains a controversial figure. He was known for his support of the hostage takers during the Iran hostage crisis and for calling the US Government the Great Satan. Khomeini was one of Moscow's five sources of intelligence at the heart of the Shiite hierarchy before the 1979 Iranian Revolution. However after the 1979 Iranian Revolution, he called the USSR the "Lesser Satan" and said that Iran should support neither side.
Khomeini held the title of Grand Ayatollah and is officially known as Imam Khomeini inside Iran and by his supporters internationally, and generally referred to as Ayatollah Khomeini by others. Since the beginning of his reign, Khomeini attempted to establish good relations between Sunnis and Shias.
Iran’s course of economic development foundered under Khomeini’s rule, and his pursuit of victory in the Iran–Iraq War ultimately proved futile. In 1982, there was an attempted military coup against Khomeini. This was followed by large purges of Iran's military again. Khomeini for a long time suffered from several kinds of cancers and had several heart attacks. Khomeini died of intestinal cancer and a heart attack in June 1989. Khomeini's gold-domed tomb in Tehrān’s Behesht-e Zahrāʾ cemetery has since become a shrine for his supporters. In 2009, a suicide bomber attacked the Mausoleum of Khomeini.
After the death of Ruhollah Khomeini, Ali Khamenei became the Supreme Leader of Iran in 1989. There have been rifts between Ali Khamenei and Ruhollah Khomeini's family. There were also reports that Ruhollah Khomeini's son Ahmad Khomeini was murdered by Iranian intelligence agents due to his criticisms about Iran's then supreme leader, Ali Khamenei. Media of Iran now refers to Ali Khamenei as the leader of the 1979 Iranian Revolution.
While Khomeini has often been described as a traditional cleric, he was a major innovator in Iran due to both his political theory and his religious-oriented populist strategy. Ayatollah Khomeini said, "Those intellectuals who say that the clergy should leave politics and go back to the mosque speak on behalf of Satan." Ruhollah Khomeini is legally considered "inviolable" in Iran, and people regularly get punished for insulting Khomeini.
Early life.
Background.
The ancestors of Ruhollah Khomeini migrated from their original home in Nishapur, North-Eastern Iran, to the kingdom of Awadh, whose rulers were Twelver Shia Muslims of Persian origin. During their rule they extensively invited, and received, a steady stream of Persian scholars, poets, jurists, architects, and painters. The Khomeini family eventually settled in the small town of Kintoor, just outside of Lucknow, the capital. Ayatollah Khomeini's paternal grandfather, Seyyed Ahmad Musavi Hindi, was born in Kintoor and was a contemporary and relative of the famous scholar Ayatollah Syed Mir Hamid Hussain Musavi. He left Lucknow in 1830 on a pilgrimage to the tomb of Imam Ali in Najaf, Mesopotamia (now Iraq) and never returned. According to Moin, this migration was to escape from the spread of British power in India. In 1834 Seyyed Ahmad Musavi Hindi visited Persia, and in 1839 he settled down at Khomein. Although he stayed and settled in Iran, he continued to be known as "Hindi", indicating his stay in India, and Ruhollah Khomeini even used "Hindi" as a pen name in some of his ghazals. There are also claims that Seyyed Ahmad Musavi Hindi was of Kashmiri origin.
Childhood.
Ruhollah Musavi Khomeini, whose name means "inspired of God", was born on 22 or 24 September 1902 in Khomeyn, Markazi Province. He was raised by his mother Hajieh Agha Khanum and aunt Sahebeth following the murder of his father Seyed Mostafa Hindi 5 months after his birth in 1903.
Ruhollah began to study the Qur'an and elementary Persian at the age of six. The following year, he began to attend a local school, where he learned religion, "noheh khani" and other traditional subjects. Throughout his childhood, he would continue his religious education with the assistance of his relatives, including his mother's cousin, Ja'far, and his elder brother, Morteza Pasandideh.
Education and lecturing.
After World War I arrangements were made for him to study at the Islamic seminary in Esfahan, but he was attracted instead to the seminary in Arak. He was placed under the leadership of Ayatollah Abdul Karim Haeri Yazdi. In 1920, Khomeini moved to Arak and commenced his studies. The following year, Ayatollah Haeri Yazdi transferred to the Islamic seminary at the holy city of Qom, southwest of Tehran, and invited his students to follow. Khomeini accepted the invitation, moved, and took up residence at the Dar al-Shafa school in Qom. Khomeini's studies included Islamic law ("sharia") and jurisprudence ("fiqh"), but by that time, Khomeini had also acquired an interest in poetry and philosophy ("irfan"). So, upon arriving in Qom, Khomeini sought the guidance of Mirza Ali Akbar Yazdi, a scholar of philosophy and mysticism. Yazdi died in 1924, but Khomeini would continue to pursue his interest in philosophy with two other teachers, Javad Aqa Maleki Tabrizi and Rafi'i Qazvini. However, perhaps Khomeini's biggest influences were yet another teacher, Mirza Muhammad 'Ali Shahabadi, and a variety of historic Sufi mystics, including Mulla Sadra and Ibn Arabi.
Khomeini studied Greek Philosophy and was influenced by both the philosophy of Aristotle, whom he regarded as the founder of logic, and Plato, whose views "in the field of divinity" he regarded as "grave and solid". Among Islamic philosophers, Khomeini was mainly influenced by Avicenna and Mulla Sadra.
Apart from philosophy, Khomeini was interested in literature and poetry. His poetry collection was released after his death. Beginning in his adolescent years, Khomeini composed mystic, political and social poetry. His poetry works were published in three collections "The Confidant," "The Decanter of Love and Turning Point", and "Divan".
Ruhollah Khomeini was a lecturer at Najaf and Qom seminaries for decades before he was known in the political scene. He soon became a leading scholar of Shia Islam. He taught political philosophy, Islamic history and ethics. Several of his students (e.g. Morteza Motahhari) later became leading Islamic philosophers and also "marja." As a scholar and teacher, Khomeini produced numerous writings on Islamic philosophy, law, and ethics. He showed an exceptional interest in subjects like philosophy and mysticism that not only were usually absent from the curriculum of seminaries but were often an object of hostility and suspicion.
Political aspects.
His seminary teaching often focused on the importance of religion to practical social and political issues of the day, and he worked against secularism in the 1940s. His first book, "Kashf al-Asrar" (Uncovering of Secrets) published in 1942, was a point-by-point refutation of "Asrar-e hazar salih" (Secrets of a Thousand Years), a tract written by a disciple of Iran's leading anti-clerical historian, Ahmad Kasravi. In addition, he went from Qom to Tehran to listen to Ayatullah Hasan Mudarris, the leader of the opposition majority in Iran's parliament during the 1920s. Khomeini became a marja in 1963, following the death of Grand Ayatollah Seyyed Husayn Borujerdi.
Early political activity.
Background.
Most Iranians had a deep respect for the Shi'a clergy or Ulama, and tended to be religious, traditional, and alienated from the process of Westernization pursued by the Shah. In the late 19th century the clergy had shown themselves to be a powerful political force in Iran initiating the Tobacco Protest against a concession to a foreign (British) interest.
At the age of 61, Khomeini found the arena of leadership open following the deaths of Ayatollah Sayyed Husayn Borujerdi (1961), the leading, although quiescent, Shi'ah religious leader; and Ayatollah Abol-Ghasem Kashani (1962), an activist cleric. The clerical class had been on the defensive ever since the 1920s when the secular, anti-clerical modernizer Reza Shah Pahlavi rose to power. Reza's son Mohammad Reza Shah, instituted a "White Revolution", which was a further challenge to the Ulama.
Opposition to the White Revolution.
In January 1963, the Shah announced the "White Revolution", a six-point programme of reform calling for land reform, nationalization of the forests, the sale of state-owned enterprises to private interests, electoral changes to enfranchise women and allow non-Muslims to hold office, profit-sharing in industry, and a literacy campaign in the nation's schools. Some of these initiatives were regarded as dangerous, especially by the powerful and privileged Shi'a ulama (religious scholars), and as Westernizing trends by traditionalists (Khomeini viewed them as "an attack on Islam"). Ayatollah Khomeini summoned a meeting of the other senior marjas of Qom and persuaded them to decree a boycott of the referendum on the White Revolution. On 22 January 1963 Khomeini issued a strongly worded declaration denouncing the Shah and his plans. Two days later the Shah took an armored column to Qom, and delivered a speech harshly attacking the ulama as a class.
Khomeini continued his denunciation of the Shah's programmes, issuing a manifesto that bore the signatures of eight other senior Iranian Shia religious scholars. In it he listed the various ways in which the Shah had allegedly violated the constitution, condemned the spread of moral corruption in the country, and accused the Shah of submission to the United States and Israel. He also decreed that the Nowruz celebrations for the Iranian year 1342 (which fell on 21 March 1963) be canceled as a sign of protest against government policies.
On the afternoon of 'Ashura (3 June 1963), Khomeini delivered a speech at the Feyziyeh madrasah drawing parallels between the Muslim caliph Yazid, who is perceived as a 'tyrant' by Shias, and the Shah, denouncing the Shah as a "wretched, miserable man," and warning him that if he did not change his ways the day would come when the people would offer up thanks for his departure from the country.
On 5 June 1963 (15 of Khordad) at 3:00 am, two days after this public denunciation of the Shah Mohammad Reza Pahlavi, Khomeini was detained in Qom and transferred to Tehran. This sparked three days of major riots throughout Iran and led to the deaths of some 400. That event is now referred to as the Movement of 15 Khordad. Khomeini was kept under house arrest and released in August.
Opposition to capitulation.
On 26 October 1964, Khomeini denounced both the Shah and the United States. This time it was in response to the "capitulations" or diplomatic immunity granted by the Shah to American military personnel in Iran. The famous "capitulation" law (or "status-of-forces agreement") would allow members of the U.S. armed forces in Iran to be tried in their own military courts. Khomeini was arrested in November 1964 and held for half a year. Upon his release, he was brought before Prime Minister Hasan Ali Mansur, who tried to convince Khomeini that he should apologize and drop his opposition to the government. Khomeini refused. In fury, Mansur slapped Khomeini's face. Two weeks later, Mansur was assassinated on his way to parliament. Four members of the Fadayan-e Islam were later executed for the murder.
Life in exile.
Khomeini spent more than 14 years in exile, mostly in the holy Shia city of Najaf, Iraq. Initially he was sent to Turkey on 4 November 1964 where he stayed in the city of Bursa hosted by a colonel in Turkish Military Intelligence named Ali Cetiner in his own residence. In October 1965, after less than a year, he was allowed to move to Najaf, Iraq, where he stayed until 1978, when he was encouraged to leave by then-Vice President Saddam Hussein. By this time discontent with the Shah was becoming intense and Khomeini went to Neauphle-le-Château, suburb of Paris, France on a tourist visa. During this last four months of his exile he was courted by press and politicians.
By the late 1960s, Khomeini was a marja-e taqlid (model for imitation) for "hundreds of thousands" of Shia, one of six or so models in the Shia world. While in the 1940s Khomeini accepted the idea of a limited monarchy under the Iranian Constitution of 1906–1907 — as evidenced by his book Kashf al-Asrar – by the 1970s he had rejected the idea. In early 1970, Khomeini gave a series of lectures in Najaf on Islamic government, later published as a book titled variously "Islamic Government" or "" ("Hokumat-e Islami : Velayat-e faqih").
This was his most famous and influential work, and laid out his ideas on governance (at that time):
A modified form of this wilayat al-faqih system was adopted after Khomeini and his followers took power, and Khomeini was the Islamic Republic's first "Guardian" or "Supreme Leader". In the meantime, however, Khomeini was careful not to publicize his ideas for clerical rule outside of his Islamic network of opposition to the Shah which he worked to build and strengthen over the next decade. In Iran, a number of actions of the Shah including his repression of opponents began to build opposition to his regime.
Cassette copies of his lectures fiercely denouncing the Shah as (for example) "... the Jewish agent, the American serpent whose head must be smashed with a stone", became common items in the markets of Iran, helped to demythologize the power and dignity of the Shah and his reign. Aware of the importance of broadening his base, Khomeini reached out to Islamic reformist and secular enemies of the Shah, despite his long-term ideological incompatibility with them.
After the 1977 death of Ali Shariati (an Islamic reformist and political revolutionary author/academic/philosopher who greatly popularized the Islamic revival among young educated Iranians), Khomeini became the most influential leader of the opposition to the Shah. Adding to his mystique was the circulation among Iranians in the 1970s of an old Shia saying attributed to the Imam Musa al-Kadhem. Prior to his death in 799, al-Kadhem was said to have prophesied that "A man will come out from Qom and he will summon people to the right path". In late 1978, a rumour swept the country that Khomeini's face could be seen in the full moon. Millions of people were said to have seen it and the event was celebrated in thousands of mosques. He was perceived by many Iranians as the spiritual, if not political, leader of revolt.
As protest grew so did his profile and importance. Although thousands of kilometers away from Iran in Paris, Khomeini set the course of the revolution, urging Iranians not to compromise and ordering work stoppages against the regime. During the last few months of his exile, Khomeini received a constant stream of reporters, supporters, and notables, eager to hear the spiritual leader of the revolution.
Supreme leader of the Islamic Republic of Iran.
Return to Iran.
Khomeini was not allowed to return to Iran during the Shah's reign (as he had been in exile). On 17 January 1979, the Shah left the country (ostensibly "on vacation"), never to return. Two weeks later, on Thursday, 1 February 1979, Khomeini returned in triumph to Iran, welcomed by a joyous crowd estimated (by BBC) to be of up to five million people.
On his chartered flight back to Tehran 120 journalists accompanied him, including three women. One of the journalists, Peter Jennings, asked: "Ayatollah, would you be so kind as to tell us how you feel about being back in Iran?" Khomeini answered via his aide Sadegh Ghotbzadeh: "Hichi" (Nothing). This statement—much discussed at the time and since—was considered by some reflective of his mystical beliefs and non-attachment to ego. Others considered it a warning to Iranians who hoped he would be a "mainstream nationalist leader" that they were in for disappointment. To others, it was a reflection of an unfeeling leader incapable or unconcerned with understanding the thoughts, beliefs, or the needs of the Iranian populace.
Khomeini adamantly opposed the provisional government of Shapour Bakhtiar, promising "I shall kick their teeth in. I appoint the government." On 11 February (Bahman 22), Khomeini appointed his own competing interim prime minister, Mehdi Bazargan, demanding, "since I have appointed him, he must be obeyed." It was "God's government," he warned, disobedience against him or Bazargan was considered a "revolt against God."
As Khomeini's movement gained momentum, soldiers began to defect to his side and Khomeini declared ill fortune on troops who did not surrender. On 11 February, as revolt spread and armories were taken over, the military declared neutrality and the Bakhtiar regime collapsed. On 30 and 31 March 1979, a referendum to replace the monarchy with an Islamic Republic passed with 98% voting in favour of the replacement, with the question: "should the monarchy be abolished in favour of an Islamic Government?"
Islamic constitution.
Although revolutionaries were now in charge and Khomeini was their leader, some opposition groups claim that several secular and religious groups were unaware of Khomeini's plan for Islamic government by "wilayat al-faqih," which involved rule by a marja' Islamic cleric. They claim that this provisional constitution for the Islamic Republic did not include the post of supreme Islamic clerical ruler. The Islamic government was clearly defined by Khomeini in his book "" (Islamic Government: Governance of the Jurist) which was published while Khomeini was in exile in 1970, smuggled into Iran, and distributed to Khomeini's supporters. This book included Khomeini's notion of "wilayat al-faqih" (Governance of the Jurist) as well as the reasoning and in his view, the necessity of it in running an Islamic state.
Khomeini and his supporters worked to suppress some former allies and rewrote the proposed constitution. Some newspapers were closed, and those protesting the closings were attacked. Opposition groups such as the National Democratic Front and Muslim People's Republican Party were attacked and finally banned. Through popular support, Khomeini supporters gained an overwhelming majority of the seats of the Assembly of Experts which revised the proposed constitution. The newly proposed constitution included an Islamic jurist Supreme Leader of the country, and a Council of Guardians to veto un-Islamic legislation and screen candidates for office, disqualifying those found un-Islamic.
In November 1979, the new constitution of the Islamic Republic was adopted by national referendum. Khomeini himself became instituted as the Supreme Leader (supreme jurist ruler), and officially became known as the "Leader of the Revolution." On 4 February 1980, Abolhassan Banisadr was elected as the first president of Iran. Critics complain that Khomeini had gone back on his word to advise, rather than rule the country.
Hostage crisis.
On 22 October 1979, the United States admitted the exiled and ailing Shah into the country for cancer treatment. In Iran, there was an immediate outcry, with both Khomeini and leftist groups demanding the Shah's return to Iran for trial and execution. Revolutionaries were reminded of Operation Ajax, 26 years earlier, when the Shah fled abroad while American CIA and British intelligence organized a coup d'état to overthrow his nationalist opponent.
On 4 November, Iranian students calling themselves Muslim Student Followers of the Imam's Line, took control of the American Embassy in Tehran, holding 52 embassy staff hostage for 444 days – an event known as the Iran hostage crisis. In 2005, when Mahmoud Ahmadinejad became president, several of the hostages identified him as one of their captors, however this claim has been denied by a CIA investigation on the matter. In the United States, the hostage-taking was seen as a flagrant violation of international law and aroused intense anger and anti-Iranian sentiments.
In Iran, the takeover was immensely popular and earned the support of Khomeini under the slogan "America can't do a damn thing against us." The seizure of the embassy of a country he called the "Great Satan" helped to advance the cause of theocratic government and outflank politicians and groups who emphasized stability and normalized relations with other countries. Khomeini is reported to have told his president: "This action has many benefits ... this has united our people. Our opponents do not dare act against us. We can put the constitution to the people's vote without difficulty, and carry out presidential and parliamentary elections." The new constitution was successfully passed by referendum a month after the hostage crisis began.
The crisis had the effect of splitting of the opposition into two groups – radicals supporting the hostage taking, and the moderates opposing it. On 23 February 1980, Khomeini proclaimed Iran's Majlis would decide the fate of the American embassy hostages, and demanded that the United States hand over the Shah for trial in Iran for crimes against the nation. Although the Shah died a few months later, during the summer, the crisis continued. In Iran, supporters of Khomeini named the embassy a "Den of Espionage", publicizing details regarding armaments, espionage equipment and many volumes of official and classified documents which they found there.
Relationship with Islamic and non-aligned countries.
Khomeini believed in Muslim unity and solidarity and the export of his revolution throughout the world. "Establishing the Islamic state world-wide belong to the great goals of the revolution." He declared the birth week of Muhammad (the week between 12th to 17th of Rabi' al-awwal) as the "Unity week". Then he declared the last Friday of Ramadan as International Day of Quds in 1981.
Iran-Iraq War.
Shortly after assuming power, Khomeini began calling for Islamic revolutions across the Muslim world, including Iran's Arab neighbor Iraq, the one large state besides Iran with a Shia majority population. At the same time Saddam Hussein, Iraq's secular Arab nationalist Ba'athist leader, was eager to take advantage of Iran's weakened military and (what he assumed was) revolutionary chaos, and in particular to occupy Iran's adjacent oil-rich province of Khuzestan, and to undermine Iranian Islamic revolutionary attempts to incite the Shi'a majority of his country.
In September 1980, Iraq launched a full scale invasion of Iran, starting what would become the eight-year-long Iran–Iraq War (September 1980 – August 1988). A combination of fierce resistance by Iranians and military incompetence by Iraqi forces soon stalled the Iraqi advance, and, by early 1982, Iran had regained almost all of the territory lost to the invasion. The invasion rallied Iranians behind the new regime, enhancing Khomeini's stature and allowing him to consolidate and stabilize his leadership. After this reversal, Khomeini refused an Iraqi offer of a truce, instead demanding reparations and the toppling of Saddam Hussein from power. The war ended in 1988, with 320,000–720,000 Iranian soldiers and militia killed.
Although Iran's population and economy were three times the size of Iraq's, the latter was aided by neighboring Persian Gulf Arab states, as well as the Soviet Bloc and Western countries. The Persian Gulf Arabs and the West wanted to be sure the Islamic revolution did not spread across the Persian Gulf, while the Soviet Union was concerned about the potential threat posed to its rule in central Asia to the north. However, Iran had large amounts of ammunition provided by the United States of America during the Shah's era and the United States illegally smuggled arms to Iran during the 1980s despite Khomeini's anti-Western policy (see Iran-Contra affair).
The war continued for over seven years with mounting costs. 1988 saw deadly month-long Iraqi missile attacks on Tehran, mounting economic problems, the demoralization of Iranian troops, attacks by the American Navy on Iranian ships, oil rigs, and a commercial airplane, and the recapture by Iraq of the Faw Peninsula.
In July of that year, Khomeini, in his words, "drank the cup of poison" and accepted a truce mediated by the United Nations. Despite the high cost of the war – 450,000 to 950,000 Iranian casualties and USD $300 billion – Khomeini insisted that extending the war into Iraq in an attempt to overthrow Saddam had not been a mistake. In a 'Letter to Clergy' he wrote: '... we do not repent, nor are we sorry for even a single moment for our performance during the war. Have we forgotten that we fought to fulfill our religious duty and that the result is a marginal issue?'
Rushdie fatwa.
In early 1989, Khomeini issued a fatwā calling for the assassination of Salman Rushdie, an India-born British author. Rushdie's book, "The Satanic Verses", published in 1988, was alleged to commit blasphemy against Islam and Khomeini's juristic ruling (fatwā) prescribed Rushdie's assassination by any Muslim. The fatwā required not only Rushdie's execution, but also the execution of "all those involved in the publication" of the book.
Khomeini's fatwā was condemned across the western world by governments on the grounds that it violated the universal human rights of free speech and freedom of religion. The fatwā has also been attacked for violating the rules of fiqh by not allowing the accused an opportunity to defend himself, and because "even the most rigorous and extreme of the classical jurist only require a Muslim to kill anyone who insults the Prophet in his hearing and in his presence."
Though Rushdie publicly regretted "the distress that publication has occasioned to sincere followers of Islam", the fatwa was not revoked. Khomeini explained,
Even if Salman Rushdie repents and becomes the most pious man of all time, it is incumbent on every Muslim to employ everything he has got, his life and wealth, to send him to Hell.
Rushdie himself was not killed but Hitoshi Igarashi, the Japanese translator of the book "The Satanic Verses", was murdered and two other translators of the book survived murder attempts.
Life under Khomeini.
In a speech given to a huge crowd after returning to Iran from exile 1 February 1979, Khomeini made a variety of promises to Iranians for his coming Islamic regime: A popularly elected government that would represent the people of Iran and with which the clergy would not interfere. He promised that "no one should remain homeless in this country," and that Iranians would have free telephone, heating, electricity, bus services and free oil at their doorstep. While these things did not come to pass, many other changes did.
Under Khomeini's rule, Sharia (Islamic law) was introduced, with the Islamic dress code enforced for both men and women by Islamic Revolutionary Guards and other Islamic groups Women were required to cover their hair, and men were not allowed to wear shorts. Alcoholic drinks, most Western movies, the practice of men and women swimming or sunbathing together were banned. The Iranian educational curriculum was Islamized at all levels with the Islamic Cultural Revolution; the "Committee for Islamization of Universities" carried this out thoroughly. The broadcasting of any music other than martial or religious on Iranian radio and television was banned by Khomeini in July 1979. The ban lasted 10 years (approximately the rest of his life).
Emigration and economy.
Khomeini is said to have stressed "the spiritual over the material". Six months after his first speech he expressed exasperation with complaints about the sharp drop in Iran's standard of living: 'I cannot believe that the purpose of all these sacrifices was to have less expensive melons' On another occasion emphasizing the importance of martyrdom over material prosperity: "Could anyone wish his child to be martyred to obtain a good house? This is not the issue. The issue is another world." He is also reportedly famous for answering a question about his economic policies by declaring that 'economics is for donkeys'. This low opinion of economics is said to be "one factor explaining the inchoate performance of the Iranian economy since the revolution." Another factor was the long war with Iraq, the cost of which led to government debt and inflation, eroding personal incomes, and unprecedented unemployment.
Due to the Iran–Iraq war, poverty is said to have risen by nearly 45% during the first 6 years of Khomeini's rule. Emigration from Iran also developed, reportedly for the first time in the country's history. Since the revolution and war with Iraq, an estimated "two to four million entrepreneurs, professionals, technicians, and skilled craftspeople (and their capital)" have emigrated to other countries.
Suppression of opposition.
In a talk at the Fayzieah School in Qom, 30 August 1979, Khomeini warned pro-imperialist opponents: "Those who are trying to bring corruption and destruction to our country in the name of democracy will be oppressed. They are worse than Bani-Ghorizeh Jews, and they must be hanged. We will oppress them by God's order and God's call to prayer."
The Shah Mohammad Reza Pahlavi and his family left Iran and escaped harm, but hundreds of former members of the overthrown monarchy and military met their end in firing squads, with exiled critics complaining of "secrecy, vagueness of the charges, the absence of defense lawyers or juries", or the opportunity of the accused "to defend themselves." In later years these were followed in larger numbers by the erstwhile revolutionary allies of Khomeini's movement—Marxists and socialists, mostly university students—who opposed the theocratic regime. Following the 1981 Hafte Tir bombing, Ayatollah Khomeini declared the Mojahedin and anyone violently opposed to the government, "enemies of God" and pursued a mass campaign against members of the Mojahedin, Fadaiyan, and Tudeh parties as well as their families, close friends, and even anyone who was accused of counterrevolutionary behavior.
In the 1988 executions of Iranian political prisoners, following the People's Mujahedin of Iran operation Forough-e Javidan against the Islamic Republic, Khomeini issued an order to judicial officials to judge every Iranian political prisoner and kill those who would not repent anti-regime activities. Estimates of the number executed vary from 1,400 to 30,000.
Minority religions.
Life for religious minorities was mixed under Khomeini. Senior government posts were reserved for Muslims. Schools set up by Jews, Christians and Zoroastrians had to be run by Muslim principals. Conversion to Islam was encouraged by entitling converts to inherit the entire share of their parents (or even uncle's) estate if their siblings (or cousins) remain non-Muslim. Iran's non-Muslim population has decreased. For example, the Jewish population in Iran dropped from 80,000 to 30,000.
Only four of the 270 seats in parliament were reserved for each three non-Muslim minority religions, under the Islamic constitution that Khomeini oversaw. Khomeini also called for unity between Sunni and Shi'a Muslims. Sunni Muslims are the largest religious minority in Iran. 4% belong to the Sunni branch.
Shortly after Khomeini's return from exile in 1979, he issued a fatwa ordering that Jews and other minorities (except Bahá'ís) be treated well. In power, Khomeini distinguished between Zionism as a secular political party that employs Jewish symbols and ideals and Judaism as the religion of Moses.
One non-Muslim group treated differently were the 300,000 members of the Bahá'í Faith. Starting in late 1979 the new government systematically targeted the leadership of the Bahá'í community by focusing on the Bahá'í National Spiritual Assembly (NSA) and Local Spiritual Assemblies (LSAs); prominent members of NSAs and LSAs were often detained and even executed by forces outside of Khomeini's direct control. "Some 200 of whom have been executed and the rest forced to convert or subjected to the most horrendous disabilities."
Like most conservative Muslims, Khomeini believed Bahá'í to be apostates. He claimed they were a political rather than a religious movement,
declaring:
the Baha'is are not a sect but a party, which was previously supported by Britain and now the United States. The Baha'is are also spies just like the Tudeh [Communist Party].
Ethnic minorities.
After the Shah left Iran in 1979, a Kurdish delegation traveled to Qom to present the Kurds' demands to Ayatollah Khomeini. Their demands included language rights and the provision for a degree of political autonomy. Khomeini responded that such demands were unacceptable, since it involved the division of the Iranian nation. The following months saw numerous clashes between Kurdish militia groups and the Revolutionary Guards. The referendum on the Islamic Republic was massively boycotted in Kurdistan, where it was thought 85 to 90% of voters abstained. Khomeini ordered additional attacks later on in the year, and by September most of Iranian Kurdistan was under direct martial law.
Death and funeral.
After eleven days in a hospital, Khomeini died painfully on 3 June 1989 after suffering five heart attacks in just ten days, at the age of 86 just before midnight. At the time of his death, Khomeini was said to be also suffering from intestinal cancer and prostate cancer. He was succeeded by Ali Khamenei. Iranians poured out into the cities and streets in enormous numbers to mourn Khomeini's death in a spontaneous outpouring of grief. In the scorching summer heat, fire trucks sprayed water on the crowds to cool them. Ten mourners were trampled to death, more than four hundred were badly hurt and several thousand more treated for injuries sustained in the ensuing pandemonium
Figures about Khomeini's funeral attendance which took place on 4 June range around 2.5–3.5 million people. Later that day, Khomeini's corpse was flown in by helicopter for burial at the Paradise of Zahra cemetery. Iranian officials postponed Khomeini's first funeral after a huge mob stormed the funeral procession, destroying Khomeini's wooden coffin in order to get a last glimpse of his body or touch of his coffin. In some cases, armed soldiers were compelled to fire warning shots in the air to restrain the crowds. At one point, Khomeini's body fell to the ground, as the crowd ripped off pieces of the death shroud, trying to keep them as if they were holy relics.
The second funeral was held under much tighter security five hours later. This time, Khomeini's casket was made of steel, and heavily armed security personnel surrounded it. In accordance with Islamic tradition, the casket was only to carry the body to the burial site. In 1995, his son Ahmad was buried next to him. Khomeini's grave is now housed within a larger mausoleum complex.
Succession.
Grand Ayatollah Hussein-Ali Montazeri, a former student of Khomeini and a major figure of the Revolution, was chosen by Khomeini to be his successor as Supreme Leader and approved as such by the Assembly of Experts in November 1985. The principle of "velayat-e faqih" and the Islamic constitution called for the Supreme Leader to be a "marja" (a grand ayatollah), and of the dozen or so grand ayatollahs living in 1981 only Montazeri qualified as a potential Leader (this was either because only he accepted totally Khomeini's concept of rule by Islamic jurists, or, as at least one other source stated, because only Montazeri had the "political credentials" Khomeini found suitable for his successor). In 1989 Montazeri began to call for liberalization, freedom for political parties. Following the execution of thousands of political prisoners by the Islamic government, Montazeri told Khomeini 'your prisons are far worse than those of the Shah and his SAVAK.' After a letter of his complaints was leaked to Europe and broadcast on the BBC, a furious Khomeini ousted him from his position as official successor.
To deal with the disqualification of the only suitable "marja", Khomeini called for an 'Assembly for Revising the Constitution' to be convened. An amendment was made to Iran's constitution removing the requirement that the Supreme Leader be a Marja and this allowed Ali Khamenei, the new favoured jurist who had suitable revolutionary credentials but lacked scholarly ones and who was not a Grand Ayatollah, to be designated as successor. Ayatollah Khamene'i was elected Supreme Leader by the Assembly of Experts on 4 June 1989. Grand Ayatollah Hossein Montazeri continued his criticism of the regime and in 1997 was put under house arrest for questioning what he regarded to be an unaccountable rule exercised by the supreme leader.
Political thought and legacy.
According to at least one scholar, politics in the Islamic Republic of Iran "are largely defined by attempts to claim Khomeini's legacy" and that "staying faithful to his ideology has been the litmus test for all political activity" there.
Throughout his many writings and speeches, Khomeini's views on governance evolved. Originally declaring rule by monarchs or others permissible so long as sharia law was followed Khomeini later adamantly opposed monarchy, arguing that only rule by a leading Islamic jurist (a marja'), would insure Sharia was properly followed (wilayat al-faqih), before finally insisting the ruling jurist need not be a leading one and Sharia rule could be overruled by that jurist if necessary to serve the interests of Islam and the "divine government" of the Islamic state.
Khomeini's concept of Guardianship of the Islamic Jurists (ولایت فقیه, "velayat-e faqih") did not win the support of the leading Iranian Shi'i clergy of the time. Towards the 1979 Revolution, many clerics gradually became disillusioned with the rule of the Shah, although none came around to supporting Khomeini's vision of a theocratic Islamic Republic.
There is much debate to as whether Khomeini's ideas are or are not compatible with democracy and whether he intended the Islamic Republic to be a democratic republic. According to the state-run "Aftab News", both ultraconservative (Mohammad Taghi Mesbah Yazdi) and reformist opponents of the regime (Akbar Ganji and Abdolkarim Soroush) believe he did not, while regime officials and supporters like Ali Khamenei, Mohammad Khatami and Mortaza Motahhari believe Khomeini intended the Islamic republic to be democratic and that it is so. Khomeini himself also made statements at different times indicating both support and opposition to democracy.
One scholar, Shaul Bakhash, explains this disagreement as coming from Khomeini's belief that the huge turnout of Iranians in anti-Shah demonstrations during the revolution constituted a 'referendum' in favor of an Islamic republic. Khomeini also wrote that since Muslims must support a government based on Islamic law, Sharia-based government will always have more popular support in Muslim countries than any government based on elected representatives.
Khomeini offered himself as a "champion of Islamic revival" and unity, emphasizing issues Muslims agreed upon – the fight against Zionism and imperialism – and downplaying Shia issues that would divide Shia from Sunni.
Khomeini strongly opposed close relations with either Eastern or Western Bloc nations, believing the Islamic world should be its own bloc, or rather converge into a single unified power. He viewed Western culture as being inherently decadent and a corrupting influence upon the youth. The Islamic Republic banned or discouraged popular Western fashions, music, cinema, and literature. In the Western world it is said "his glowering visage became the virtual face of Islam in Western popular culture" and "inculcated fear and distrust towards Islam," making the word 'Ayatollah' "a synonym for a dangerous madman ... in popular parlance." This has particularly been the case in the United States where some Iranians complained that even at universities they felt the need to hide their Iranian identity for fear of physical attack. There Khomeini and the Islamic Republic are remembered for the American embassy hostage taking and accused of sponsoring hostage-taking and terrorist attacks, and which continues to apply economic sanctions against Iran.
Before taking power Khomeini expressed support for the Universal Declaration of Human Rights. "We would like to act according to the Universal Declaration of Human Rights. We would like to be free. We would like independence." However once in power Khomeini took a firm line against dissent, warning opponents of theocracy for example: "I repeat for the last time: abstain from holding meetings, from blathering, from publishing protests. Otherwise I will break your teeth."
Many of Khomeini's political and religious ideas were considered to be progressive and reformist by leftist intellectuals and activists prior to the Revolution. However, once in power his ideas often clashed with those of modernist or secular Iranian intellectuals. This conflict came to a head during the writing of the Islamic constitution when many newspapers were closed by the government. Khomeini angrily told the intellectuals:
Yes, we are reactionaries, and you are enlightened intellectuals: You intellectuals do not want us to go back 1400 years. You, who want freedom, freedom for everything, the freedom of parties, you who want all the freedoms, you intellectuals: freedom that will corrupt our youth, freedom that will pave the way for the oppressor, freedom that will drag our nation to the bottom. 
In contrast to his alienation from Iranian intellectuals, and "in an utter departure from all other Islamist movements," Khomeini embraced international revolution and Third World solidarity, giving it "precedence over Muslim fraternity." From the time Khomeini's supporters gained control of the media until his death, the Iranian media "devoted extensive coverage to non-Muslim revolutionary movements (from the Sandinistas to the African National Congress and the Irish Republican Army) and downplayed the role of the Islamic movements considered conservative, such as the Afghan mujahidin."
Khomeini's legacy to the economy of the Islamic Republic has been expressions of concern for the "mustazafin" (a Quranic term for the oppressed or deprived), but not always results that aided them. During the 1990s the "mustazafin" and disabled war veterans rioted on several occasions, protesting the demolition of their shantytowns and rising food prices, etc. Khomeini's disdain for the science of economics ("economics is for donkeys") is said to have been "mirrored" by the populist redistribution policies of former president, Mahmoud Ahmadinejad, who allegedly wears "his contempt for economic orthodoxy as a badge of honour", and has overseen sluggish growth and rising inflation and unemployment.
In 1963, an anti-Shah revolutionary, Khomeini wrote a book in which he stated that there is no religious restriction on corrective surgery for transgendered individuals.
Appearance and habits.
Khomeini was described as "slim," but athletic and "heavily boned."
He was known for his punctuality:
He's so punctual that if he doesn't turn up for lunch at exactly ten past everyone will get worried, because his work is regulated in such a way that he turned up for lunch at exactly that time every day. He goes to bed exactly on time. He eats exactly on time. And he wakes up exactly on time. He changes his cloak every time he comes back from the mosque.
Sheikh Ahmad Deedat, a Muslim Scholar from South Africa has described Imam Khomeini in his speech which he delivered on 3 March 1982:
I saw the Imam. He delivered the Lecture to us for about half an hour, and it was nothing but the Quran, the man is like a computerized Quran. And the electric effect he had on everybody, his charisma, was amazing. You just look at the man and tears come down your cheek. You just look at him and you get tears. I never saw a more handsome old man in my life, no picture, no video, no TV could do justice to this man, the handsomest old man I ever saw in my life was this man.
Khomeini was also known for his aloofness and austere demeanor. He is said to have had "variously inspired admiration, awe, and fear from those around him." His practice of moving "through the halls of the madresehs never smiling at anybody or anything; his practice of ignoring his audience while he taught, contributed to his charisma."
Khomeini adhered to traditional beliefs of Islamic hygienical jurisprudence holding that things like urine, excrement, blood, wine etc. and also non-Muslims were some of eleven ritualistically "impure" things that physical contact with which while wet required ritual washing or Ghusl before prayer or salat. He is reported to have refused to eat or drink in a restaurant unless he knew for sure the waiter was a Muslim.
Mystique.
Khomeini was noted by many for his mystique. Before the revolution he benefited from the widespread circulation of a Hadith attributed to the Imam Musa al-Kazim who is said to have prophesied shortly before his death in 799 that
'A man will come out from Qom and he will summon people to the right path. There will rally to him people resembling pieces of iron, not to be shaken by violent winds, unsparing and relying on God.'
Khomeini was the first and only Iranian cleric to be addressed as "Imam", a title hitherto reserved in Iran for the twelve infallible leaders of the early Shi'a. He was also associated with the "Mahdi" or 12th Imam of Shia belief in a number of ways. One of his titles was "Na'eb-e Imam" (Deputy to the Twelfth Imam). His enemies were often attacked as "taghut" and "Mofsed-e-filarz" (corrupters of the earth), religious terms used for enemies of the Twelfth Imam. Many of the officials of the overthrown Shah's government executed by Revolutionary Courts were convicted of "fighting against the Twelfth Imam". When a deputy in the majlis asked Khomeini directly if he was the 'promised Mahdi', Khomeini did not answer, "astutely" neither confirming nor denying the title.
As the revolution gained momentum, even some non-supporters exhibited awe, called him "magnificently clear-minded, single-minded and unswerving." His image was as "absolute, wise, and indispensable leader of the nation"
The Imam, it was generally believed, had shown by his uncanny sweep to power, that he knew how to act in ways which others could not begin to understand. His timing was extraordinary, and his insight into the motivation of others, those around him as well as his enemies, could not be explained as ordinary knowledge. This emergent belief in Khomeini as a divinely guided figure was carefully fostered by the clerics who supported him and spoke up for him in front of the people.
Even many secularists who firmly disapproved of his policies were said to feel the power of his "messianic" appeal. Comparing him to a father figure who retains the enduring loyalty even of children he disapproves of, journalist Afshin Molavi writes of the defenses of Khomeini he is "heard in the most unlikely settings":
A whiskey-drinking professor told an American journalist that Khomeini brought pride back to Iranians. A women's rights activist told me that Khomeini was not the problem; it was his conservative allies who had directed him wrongly. A nationalist war veteran, who held Iran's ruling clerics in contempt, carried with him a picture of 'the Imam'. Another journalist tells the story of listening to bitter criticism of the regime by an Iranian who tells her of his wish for his son to leave the country and who "repeatedly" makes the point "that life had been better" under the Shah. When his complaint is interrupted by news that "the Imam" — over 85 years old at the time — might be dying, the critic becomes "ashen faced" and speechless, pronouncing 'this is terrible for my country.'
Family and descendants.
In 1929, Khomeini married Khadijeh Saqafi, the 16 year old daughter of a cleric in Tehran. By all accounts their marriage was harmonious and happy. She died in 2009. They had seven children, though only five survived infancy. His daughters all married into either merchant or clerical families, and both his sons entered into religious life. Mostafa, the elder son, died in 1977 while in exile in Najaf, Iraq with his father and was rumored by supporters of his father to have been murdered by SAVAK. Ahmad Khomeini, who died in 1995 at the age of 50, was also rumoured to be a victim of foul play, but at the hands of the regime. Perhaps his "most prominent daughter", Zahra Mostafavi, is a professor at the University of Tehran, and still alive.
Of Khomeini's fifteen grandchildren the most notable include:
Works.
Khomeini was a prolific writer (200 of his books are online) who authored commentaries on the Qur'an, on Islamic jurisprudence, the roots of Islamic law, and Islamic traditions. He also released books about philosophy, gnosticism, poetry, literature, government and politics. Some of his books:
Sources.
</dl>

</doc>
<doc id="26235" url="http://en.wikipedia.org/wiki?curid=26235" title="Rousseau (disambiguation)">
Rousseau (disambiguation)

Rousseau most commonly refers to 18th-century Swiss author and philosopher Jean-Jacques Rousseau, and it may also refer to:

</doc>
<doc id="26239" url="http://en.wikipedia.org/wiki?curid=26239" title="Rhineland-Palatinate">
Rhineland-Palatinate

Rhineland-Palatinate (German: "Rheinland-Pfalz", ]; French: "Rhénanie-Palatinat") is one of the 16 states (German: "Länder") of the Federal Republic of Germany. It has an area of 19846 km2 and about four million inhabitants. The city of Mainz functions as the state capital.
History.
The state of Rhineland-Palatinate was established on 30 August 1946. It was formed from the northern part of the French Occupation Zone, which included parts of Bavaria (the Rhenish Palatinate), the southern parts of the Prussian Rhine Province (including the District of Birkenfeld, which formerly belonged to Oldenburg), parts of the Prussian Province of Nassau (see Hesse-Nassau), and parts of Hesse-Darmstadt (Rhinehessen on the western bank of the Rhine). The state constitution was confirmed by referendum on 18 May 1947.
Geography.
Located in western Germany, the Rhineland-Palatinate borders (from the north and clockwise) the German "Bundesländer" North Rhine-Westphalia, Hesse, and Baden-Württemberg. It shares an international border with France, followed by another "Bundesland", Saarland, and international borders with Luxembourg and Belgium.
The largest river in the state is the Rhine River, which forms the border with Baden-Württemberg and Hesse in the southeast before flowing through the northern part of Rhineland-Palatinate. The Rhine Valley is bounded by mountain chains and it containins several of the historically significant places in Germany.
The Eifel and Hunsrück mountain chains are found on the west bank of the Rhine in northern Rhineland-Palatinate, while the Westerwald and Taunus mountains are found on the east bank. The hilly lands in the southernmost region of the state are covered by the Palatinate forest and the Palatinate. These mountain chains are separated from each other by several tributaries of the Rhine: the Mosel, the Lahn,and the Nahe.
The modern state consists of a conglomeration of the historic regions of southern Rhine Province, Rheinhessen, and the Palatinate.
See also List of places in Rhineland-Palatinate and List of landscapes in Rhineland-Palatinate.
Administration.
Rhineland-Palatinate is divided into 24 districts, formerly grouped into the three administrative regions: Koblenz, Trier and Rheinhessen-Pfalz.
Since 2000, the employees and assets of the former administrative regions have been organized into the "Aufsichts- und Dienstleistungsdirektion Trier" (Supervisory and Service Directorate Trier) and the "Struktur- und Genehmigungsdirektionen" (Structural and Approval Directorates) "Nord" in Koblenz and "Süd" in Neustadt (Weinstraße). These administrations execute their authority over the whole state, for example, the "ADD Trier" oversees all schools.
Map of the districts of Rhineland-Palatinate:
Every district is composed of numerous municipalities, which can consist of cities, villages, or groups of villages known as Verbandsgemeinden. There are also twelve urban districts that are identified on the map with letters:
Religion.
As of 2010 44.9% of the population of the state adhered to the Roman Catholic Church, 30.6% to the Evangelical Church in Germany. 22.0% of the population is irreligious or adheres to other religions. Muslims made up 2.5% of the total.
Jewish culture.
The league of ShUM-cities in the later Rhineland-Palatinate comprised the Jewish communities of Mainz, Speyer and Worms which became the center of Jewish life during Medieval times. The Takkanot Shum (Hebrew: תקנות שו"ם‎), or Enactments of ShU"M were a set of decrees formulated and agreed upon over a period of decades by their Jewish community leaders.
Politics.
State elections.
Elections for the Rhineland-Palatinate Landtag are held every five years, with residents over age 18 eligible to vote. This regional parliament or legislature then elects the premier and confirms the cabinet members. Rhineland-Palatinate is the only German state to have a cabinet minister for winegrowing ("Ministry of Economy, Traffic, Agriculture and Winegrowing")
Economy.
Industry.
Rhineland-Palatinate leads all German states with an export rate of approximately 50%. Important sectors are the winegrowing industry, chemical industry, pharmaceutical industry and auto parts industry. "Distinctive regional industries" includes gemstone industry, ceramic and glass industry and leather industry. Small and medium enterprises are considered the "backbone" of the economy in Rhineland-Palatinate. The principal employer is the chemical and plastics processing industry which is represented by BASF in Ludwigshafen. Boehringer, Joh. A. Benckiser, SGE Deutsche Holding, Schott Glassworks concludes the top 5 companies in the state.
Agriculture and viticulture.
Rhineland-Palatinate is Germany's leading producer of wine in terms of grape cultivation and wine export. Its capital, Mainz, may be called the capital of the German wine industry, being the home of the German Wine Institute, the German Wine Fund in the "Haus des Deutschen Weines" (House of German Wine), and the Verband Deutscher Prädikats- und Qualitätsweingüter Wine Bourse, which brings together the top winemakers of Germany and the wine merchants of the world.
Of thirteen wine regions producing quality wine in Germany, six (Rheinhessen, Pfalz, Mosel, Nahe, Mittelrhein and Ahr) are located in Rhineland-Palatinate, with 65% to 70% of the production of wine grapes in Germany having their origin within the state. 13,000 wine producers generate 80% to 90% of the German wine export, which was 2.6 million hectoliters in 2003.
Traditional grape varieties and a wide range of varieties developed during the last 125 years are characteristic for the region.
Classical white varieties are cultivated at 63683 ha. These comprise the famous Rieslings 14446 ha, Müller-Thurgau (8663 ha), Silvaner (3701 ha) and Kerner (3399 ha).
The share of red varieties grew constantly during the last decades and amounts to 20000 ha. Dornfelder, a new cultivar, is the leading red grape cultivated on 7626 ha, which is more than a third. Blauer Portugieser (4446 ha) and Spätburgunder (3867 ha) show also appreciable cultivated shares.
In addition, Pinot blanc, Pinot gris, Chardonnay as white varieties and Regent and St. Laurent as red varieties have been increasing their share, as the growing conditions improve in Rhineland-Palatinate.
The state supports the wine industry by providing a comprehensive consultancy and education program in the service supply centers ("German:" DLR) of the land. The Geilweilerhof Institute for Grape Breeding is fully financed by the state. Many well known new breeds, such as Morio-Muskat, Bacchus, Optima and Regent have been created in these institutes.
The world-wide leader in sparkling wine production, producing 245 million bottles in 2006, is the renowned Schloss Wachenheim Group. This company is headquartered in Trier, with operations in several locations in Rhineland-Palatinate.
Other renowned sparkling wine producers, such as Kupferberg, Deinhard and Henkell, also had their roots in the region, but now belong to companies outside the state as a result of business consolidation.
Emigration.
Rhineland-Palatinate has supplied immigrants to many parts of the world. The names of the villages of New Paltz, Palatine Bridge and German Flatts, New York, and Palatine, Illinois, attest to settlements of Palatine Germans. The Hunsrückischen dialect in Brazil also bears testimony to an immigrant community.
The Pennsylvania Dutch spoken by the Amish in the United States is (among other dialects) derived from the German dialect spoken in the Rhineland-Palatinate, which many Palatine refugees brought to the colony in the early decades of the 18th century. The only existing Pennsylvania German newspaper, Hiwwe wie Driwwe, is published bi-annually in the village Ober-Olm, which is located close to Mainz, the state capital. In the same village, one can find the headquarters of the German-Pennsylvanian Association.
Certain colonies in the United States were settled by major groups of poor Palatines—then refugees in England—passage paid for by Queen Anne to reduce the number of impoverished families who had taken refuge in London. In 1710 the English used ten ships to transport nearly 3,000 Germans to the colony of New York. Many died en route, as they had been weakened by disease. They were settled in work camps along the Hudson River, where they developed naval stores for the English to work off their passage. Churches set up in both the East and West Camps provided some of the earliest population records in New York. In 1723 the first hundred heads of families were allowed to acquire land west of Little Falls, New York, along the Mohawk River, in what was called the Burnetsfield Patent after the governor. This became Herkimer County. The Germans and their descendants were important in the defense of the Mohawk Valley during the American Revolutionary War.
New Bern is one of the earliest North Carolina colonies settled in 1710 by about 400 Palatines (650 left Germany, but about half died in passage) and 100 Swiss. This venture was orchestrated by the Swiss-born Christoph von Graffenried after purchasing more than 19000 acre from the British Proprietors of the Carolinas.
In the 19th century, there was a substantial number of emigrants from the area around Trier, many of whom settled in Wisconsin.

</doc>
<doc id="26240" url="http://en.wikipedia.org/wiki?curid=26240" title="House of Romanov">
House of Romanov

The House of Romanov (Russian: Рома́нов, ]) was the second imperial dynasty, after the Rurik dynasty, to rule over Russia, which reigned from 1613 until the abdication of Emperor Nicholas II on March 15, 1917, as a result of the February Revolution.
The direct male line of the Romanovs came to an end when Elizabeth of Russia died in 1762. After an era of dynastic crisis, the House of Holstein-Gottorp, a cadet branch of the House of Oldenburg, ascended the throne in 1762 with Peter III, a grandson of Peter I. All rulers from the middle of the 18th century to the revolution of 1917 were descended from that branch. Though officially known as the House of Romanov, these descendants of the Romanov and Oldenburg Houses are sometimes referred to as Holstein-Gottorp-Romanov.
"In early 1917 the Romanov Dynasty had 65 members, 18 of which were killed by the Bolsheviks. The remaining 47 members were exiled abroad." In 1924, Grand Duke Kirill Vladimirovich, the direct male-line patrilineal descendant of Alexander II of Russia, claimed the headship of the defunct Imperial House of Russia. His granddaughter, Grand Duchess Maria Vladimirovna of Russia, is the current pretender, her only child George Mikhailovich is her heir apparent.
Surname "Romanov".
Legally, it is not clear if a "ukase" was issued that abolished the surname of Michael Romanov upon his accession to the Russian throne or of his subsequent male-line descendants, although by tradition members of reigning dynasties seldom use surnames. Rather, they are known by their dynastic titles ("Tsarevich Ivan Alexeevich", "Grand Duke Nikolai Nikolaevich", etc.) In addition, since 1761 Russian rulers descend from the son of Grand Duchess Anna Petrovna of Russia and Charles Frederick, Duke of Holstein-Gottorp, and thus they were no longer Romanovs by patrilineage, but belonged to the Holstein-Gottorp cadet branch of the German House of Oldenburg. In such genealogical literature as the Almanach de Gotha, the name of Russia's ruling dynasty from the time of Peter III is "Holstein-Gottorp-Romanov". However, the name "Romanov" and "House of Romanov" were often used in official references to the Russian imperial house. The coat of arms of the Romanov boyars was included in legislation on the imperial dynasty, and in 1913 there was an official jubilee celebrating the "300th Anniversary of the Romanovs rule".
After the February Revolution all members of the imperial family were given the surname "Romanov" by special decree of the Provisional Government of Russia. The only exception were the morganatic descendants of the Grand Duke Dmitri Pavlovich who, in exile, took the surname 
Origins.
The Romanovs share their origin with two dozen other Russian noble families. Their earliest common ancestor is one Andrei Kobyla, attested around 1347 as a boyar in the service of Semyon I of Moscow. Later generations assigned to Kobyla the most illustrious pedigrees. An 18th-century genealogy claimed that he was the son of the Prussian prince Glanda Kambila, who came to Russia in the second half of the 13th century, fleeing the invading Germans. Indeed, one of the leaders of the Old Prussian rebellion of 1260-1274 against the Teutonic order was named Glande.
His actual origin may have been less spectacular. Not only is "Kobyla" Russian for "mare", some of his relatives also had as nicknames the terms for horses and other domestic animals, thus suggesting descent from one of the royal equerries. One of Kobyla's sons, Feodor, a boyar in the boyar duma of Dmitri Donskoi, was nicknamed Koshka (cat). His descendants took the surname Koshkin, then changed it to Zakharin, which family later split into two branches: Zakharin-Yakovlev and Zakharin-Yuriev. During the reign of Ivan the Terrible, the former family became known as Yakovlev (Alexander Herzen among them), whereas grandchildren of Roman Zakharin-Yuriev changed their name to Romanov.
Rise to power.
The family fortunes soared when Roman's daughter, Anastasia Zakharyina, married Ivan IV, the Rurikid Grand Prince of Moscow, on 3 (13) February 1547. Since her husband had assumed the title of tsar, which literally means "Caesar", on 16 January 1547, she was crowned the very first tsaritsa of Russia. Their marriage was a happy one, but her mysterious death in 1560 changed Ivan's character for the worse. Suspecting the boyars of having poisoned his beloved, the tsar started a reign of terror against them. Among his children by Anastasia, the elder (Ivan) was murdered by the tsar in a quarrel; the younger Feodor, a pious and lethargic prince, inherited the throne upon his father's death in 1584.
Throughout Feodor's reign (1584-1598), the Tsar's brother-in-law, Boris Godunov, and his Romanov cousins contested the "de facto" rule of Russia. Upon the death of childless Feodor, the 700-year-old line of Moscow Rurikids came to an end. After a long struggle, the party of Boris Godunov prevailed over the Romanovs, and the Zemsky sobor elected Godunov as tsar in 1599. Godunov's revenge on the Romanovs was terrible: all the family and its relatives were deported to remote corners of the Russian North and Ural, where most of them died of hunger or in chains. The family's leader, Feodor Nikitich Romanov, was exiled to the Antoniev Siysky Monastery and forced to take monastic vows with the name Filaret.
The Romanovs' fortunes again changed dramatically with the fall of the Godunov dynasty in June 1605. As a former leader of the anti-Godunov party and cousin of the last legitimate Tsar, Filaret Romanov's recognition was sought by several impostors who attempted to claim the Rurik legacy and throne during the Time of Troubles. False Dmitriy I made him a metropolitan, and False Dmitriy II raised him to the dignity of patriarch. Upon expulsion of Poles from Moscow in 1612, the "Zemsky Sobor" offered the Russian crown to several Rurik and Gedimin princes, but all of them declined the honour of it.
On being offered the Russian crown, Filaret's 16-year-old son Mikhail Romanov, then living at the Ipatiev Monastery of Kostroma, burst into tears of fear and despair. He was finally persuaded to accept the throne by his mother Kseniya Ivanovna Shestova, who blessed him with the holy image of Our Lady of St. Theodore. Feeling how insecure his throne was, Mikhail attempted to emphasize his ties with the last Rurik tsars and sought advice from the Assembly of the Land on every important issue. This strategy proved successful. The early Romanovs were generally loved by the population as in-laws of Ivan the Terrible and innocent martyrs of Godunov's wrath.
The era of dynastic crisis.
Mikhail was succeeded by his only son Alexei, who steered the country quietly through numerous troubles. Upon his death, there was a period of dynastic struggle between his children by his first wife Maria Ilyinichna Miloslavskaya (Fyodor III, Sofia Alexeyevna, Ivan V) and his son by his second wife Nataliya Kyrillovna Naryshkina, the future Peter the Great. Peter ruled from 1682 until his death in 1725. In numerous successful wars he expanded the Tsardom into a huge empire that became a major European power. He led a cultural revolution that replaced some of the traditionalist and medieval social and political system with a modern, scientific, Europe-oriented, and rationalist system.
New dynastic struggles followed the death of Peter. His only son to survive into adulthood, Alexei, did not support Peter's modernization of Russia. He had previously been arrested and died in prison shortly thereafter. Near the end of his life, Peter managed to alter the succession tradition of male heirs to allow him to name his own heir. Power then passed into the hands of his second wife, the Empress Catherine. Within five years, the Romanov male line ended with the death of Peter II in 1730. He was succeeded by Anna I. Ivanovna, daughter of Peter the Great's half-brother and co-ruler, Ivan V. Before she died in 1740 the empress declared her grandnephew, Ivan VI, should succeed her. This was an attempt to secure the line of her father and exclude descendants of Peter the Great from inheriting the throne. Ivan VI was only a one-year-old baby at the time of his succession to the throne and his parents, grand duchess Anna Leopoldovna and Duke Anthony Ulrich of Brunswick, the ruling regent, were detested for their German counselors and relations. As a consequence, shortly after Empress Anna Ivanovna's death, Elizabeth Petrovna, a legitimized daughter of Peter I, managed to gain the favor of the populace and dethroned Ivan VI with a coup d'état, supported by the Preobrazhensky Regiment and the ambassadors of France and Sweden. Ivan VI and his parents died in prison many years later.
The Holstein-Gottorp-Romanov Dynasty.
The Holstein-Gottorps of Russia retained the Romanov surname, emphasizing their matrilineal descent from Peter the Great, through Anna Petrovna (Peter I's elder daughter by his second wife). In 1742, Empress Elizabeth of Russia brought Anna's son, her nephew, Peter of Holstein-Gottorp, to St. Petersburg and proclaimed him as her heir and would, in time, marry him off the German princess Sophia of Anhalt-Zerbst. In 1762, shortly after the death of Empress Elizabeth, Sophia, who had taken the Russian name Catherine upon her marriage, overthrew her unpopular husband, with the aid of her lover, Grigory Orlov, and would reign as Catherine the Great. Catherine's son, Paul I, who succeeded his mother in 1796, was particularly proud to be great-grandson of the illustrious Russian monarch, although his mother insinuated in her memoirs that Paul's natural father had been her lover Serge Saltykov, as opposed to her husband, Peter. Painfully aware of the hazards resulting from battles of succession, Paul decreed house laws for the Romanovs—the so-called Pauline laws, among the strictest in Europe—which established semi-Salic primogeniture as the rule of succession to the throne, requiring Orthodox faith for the monarch and dynasts, and for the consorts of the monarchs and their nearest heirs in line. Later, Alexander I, responding to the morganatic marriage of his brother and heir, added the requirement that consorts of all Russian dynasts in the male line had to be of equal birth (i.e., born to a royal or sovereign house).
Paul I was murdered in his palace in Saint Petersburg in 1801. Alexander I succeeded him on the throne and later died without leaving a male heir. His brother, crowned Nicholas I, succeeded him on the throne. The succession was far from smooth, however, as hundreds of troops took the oath of allegiance to Nicholas's elder brother, Constantine, who, unbenknownst to them, had renounced his claim to the throne. The confusion, combined with opposition to Nicholas' accession, led to the Decembrist Revolt. Nicholas I fathered four sons, educating them for the prospect of ruling Russia and for successful military careers.
Alexander II, son of Nicholas I, became the next Russian emperor in 1855, in the midst of the Crimean War. Alexander considered that his task was to keep peace in Europe and Russia. However, he believed only a country with a strong army could keep the peace. By paying attention to the army, giving much freedom to Finland, and freeing the serfs in 1861, he gained much popular support.
Despite his popularity, however, his family life began to unravel by the mid 1860s. In 1864, his eldest son, and heir, Tsarevich Nicholas, died suddenly. Furthermore, his wife, Empress Maria Alexandrovna, who suffered from tuberculosis, spent much of her time abroad. Alexander eventually turned to a mistress, Princess Catherine Dolgoruki, with whom he had several children, an action which alienated him from most of his children. Following the death of his wife, in 1880 he contracted a morganatic marriage with Dolgoruki. His legitimization their children, and rumors that he was contemplating crowning his new wife as empress, caused tension with the entire Romanov family. In particular, the grand duchesses were scandalized at the prospect of subordination to a woman who had borne Alexander several illegitimate children during his wife's lifetime. Before Princess Catherine could be elevated in rank, however, on 13 March 1881, Alexander was assassinated by a hand-made bomb hurled by Ignacy Hryniewiecki. Slavic patriotism, cultural revival, and Panslavist ideas grew in importance in the latter half of this century, evoking expectations of a more Russian than cosmopolitan dynasty. Several marriages were contracted with princesses from other Slavic monarchies and Orthodox kingdoms (Greece, Montenegro, Serbia). In the early 20th century a couple of cadet-line princesses were allowed to marry Russian high noblemen - whereas until the 1850s, practically all marriages had been with German princelings.
Alexander II was succeeded by his son Alexander III. Alexander III, the second-to-last Romanov emperor, was responsible for conservative reforms in Russia. Never meant to be emperor, he was educated in matters of state only after the death of his older brother, Nicholas. This lack of extensive education may have influenced his politics as well as those of his son, Nicholas II. Alexander III cut an impressive figure. Not only was he tall (1.93 m or 6'4" according to some sources), but his physique was proportionately large. Rumors spread about his incredible strength – a strength that was the size of his temper. In addition, the beard he wore hearkened back to the likeness of tsars/emperors of old, contributing to the aura of authority with which he carried himself.
Alexander, fearful of the fate which had befallen his father, strengthened autocratic rule in Russia. Many of the reforms the more liberal Alexander II had pushed through were reversed. Alexander, at his brother's death, not only inherited his brother's position as Tsarevich, but, also his brother's Danish fiancee, Princess Dagmar, who took the name Maria Fyodorovna upon her conversion to Orthodoxy, daughter of King Christian IX and Queen Louise of Denmark, and among whose siblings were Kings Frederik VIII of Denmark and George I of Greece, and Queen Alexandra of the United Kingdom, consort of King Edward VII. Despite contrasting natures and background the marriage was considered harmonious, producing six children and acquiring for Alexander the reputation of being the first tsar not known to take mistresses.
His eldest son, Nicholas, became emperor upon Alexander III's death due to kidney disease at age 49 in November 1894. Nicholas reputedly said, "I am not ready to be tsar..." Just a week after the funeral, Nicholas married his fiancee, Alix of Hesse-Darmstadt, a favorite grandchild of Queen Victoria of the United Kingdom. Though an intelligent and kind-hearted man, he tended to leave intact his father's harsh polices, and for her part, the shy Alix, who had taken the name Alexandra, upon her conversion to Orthodoxy, while a devoted wife to Nicholas and mother to their five children, disliked many of the social duties of being tsarina, and was seen as cold and distant, drawing many unfavorable comparisons between Alexandra, and her popular mother-in-law, Maria Fyodorovna. In 1916, when Nicholas took control of the army at the front lines during World War I, Alexandra sought to influence government affairs even more than she had done during peace time. His well-known devotion to her injured both his and the dynasty's reputation during World War I, due both to her German origin and her unique relationship with Rasputin, whose role in the life of her only son was not widely known. Alexandra was a carrier of the gene for haemophilia, which she inherited from her maternal grandmother, Queen Victoria. Her son, Alexei, the long-awaited heir to the throne, inherited hemophilia and suffered agonizing bouts of protracted bleeding, the suffering of which was partially alleviated by Rasputin's ministrations. Nicholas and Alexandra also had four daughters (Olga, Tatiana, Maria, and Anastasia).
The six crowned representatives of the Holstein-Gottorp-Romanov line were: Paul (1796–1801), Alexander I (1801–1825), Nicholas I (1825–55), Alexander II (1855–81), Alexander III (1881–94), and Nicholas II (1894–1917).
Constantine Pavlovich and Michael Alexandrovich, both morganatically married, are occasionally counted among Russia's emperors by historians who observe that the Russian monarchy did not legally permit interregnums. But neither was crowned and both declined the throne.
Downfall.
The February Revolution of 1917 resulted in the abdication of Nicholas II in favor of his brother Grand Duke Michael Alexandrovich. The latter declined to accept imperial authority save to delegate it to the Provisional Government pending a future democratic referendum. But that action effectively terminated the Romanov dynasty's rule over Russia. Some historians contend that the crown did not lawfully pass to Michael, as Tsesarevich Alexei would have automatically succeeded his father, Nicholas II, if the latter had not illegally altered his act of abdication to include his son—in fear that the afflicted boy would be forcibly separated from his parents. By this theory, Alexei followed Nicholas II as Russia's rightful emperor, although he remained a prisoner of the Bolsheviks for the rest of his short life. His uncle Michael would then only have become emperor after Alexei's death, along with his parents and sisters, in June 1918. But by then Michael Aleksandrovich was also in captivity and would himself be executed by the Bolsheviks the following month.
After the February Revolution, Nicholas II and his family were placed under house arrest in the Alexander Palace. While several members of the imperial family managed to stay on good terms with the Provisional Government, and were eventually able to leave Russia, Nicholas II and his family, were sent into exile in the Siberian town of Tobolsk by Alexander Kerensky in August 1917. The October Revolution of 1917, saw the ousting of the Provisional government by the Bolsheviks, and in April 1918, the Romanovs were moved to the Russian town of Yekaterinburg, in the Urals, where they were placed in the Ipatiev House.
Shooting of Tsar and family.
On the night of July 17, 1918, Bolshevik authorities acting on Yakov Sverdlov's orders in Moscow and led locally by Filip Goloschekin and Yakov Yurovsky, shot Nicholas II, his immediate family, and four servants in the Ipatiev House's cellar.
The family was told that they were to be photographed to prove to the people that they were still alive. The family members were arranged appropriately and left alone for several minutes, the gunmen then walked in and started shooting. The girls did not die from the first shots, because bullets rebounded off diamonds that were sewn into their corsets. The gunmen tried to stab them with bayonets, which also failed because of the stones. The gunmen then allegedly shot each girl in the head at close range although no physical evidence of head wounds was found many decades later. The bodies of the Romanovs were then hidden and moved several times before being interred in an unmarked pit where they remained until the summer of 1991 when amateur enthusiasts disinterred them. DNA evidence was used to make tentative identification and the bodies were later given a state funeral under the nascent democracy of post-Soviet Russia.
The Ipatiev House has the same name as the Ipatiev Monastery in Kostroma, where Mikhail Romanov had been offered the Russian Crown in 1613. The large memorial church "on the blood" has been built on the spot where the Ipatiev House once stood.
Nicholas II and his family were proclaimed passion-bearers by the Russian Orthodox Church in 2000. (In orthodoxy, a passion-bearer is a saint who was not killed "because" of his faith like a martyr but died "in" faith at the hand of murderers.)
Remains of the Tsar.
In July 1991, the crushed bodies of Nicholas II and his wife, along with three of their five children and four of their servants, were exhumed (although some questioned the authenticity of these bones despite DNA testing). Because two bodies were not present, many people believed that two Romanov children escaped the killings. There was much debate as to which two children's bodies were missing. A Russian scientist made photographic superimpositions and determined that Maria and Alexei were not accounted for. Later, an American scientist concluded from dental, vertebral, and other remnants that it was Anastasia and Alexei who were missing. Much mystery surrounded Anastasia's fate. Several films have been produced suggesting that she lived on. This has since been completely disproved with the discovery of the final Romanov children's remains and extensive DNA testing that connected these remains with those of Nicholas II, his wife and three children.
After the bodies were exhumed in June 1991, they sat in laboratories until 1998, while there was a debate as to whether they should be reburied in Yekaterinburg or St. Petersburg. A commission eventually chose St. Petersburg. The remains were transferred with full military honor guard and accompanied by members of the Romanov family from Yekaterinburg to St. Petersburg. In St. Petersburg the remains of the imperial family were moved by a formal military honor guard cortege from the airport to the Sts. Peter and Paul Fortress where they (along with several loyal servants who were killed with them) were interred in a special chapel in the Peter and Paul Cathedral near the tombs of their ancestors. President Boris Yeltsin attended the interment service on behalf of the Russian people.
Alexei.
Late summer of 2007, a Russian archaeologist announced a discovery by one of his workers. The excavation uncovered the following items in the two pits which formed a "T": 
The area where the remains were found was near the old Koptyaki Road, under what appeared to be double bonfire sites about 70 m from the mass grave in Pigs Meadow near Yekaterinburg. The general directions were described in Yurovsky's memoirs, owned by his son, although no one is sure who wrote the notes on the page. The archaeologists said the bones are from a boy who was roughly between the ages of ten and thirteen years at the time of his death and of a young woman who was roughly between the ages of eighteen and twenty-three years old. Anastasia was seventeen years and one month old at the time of the murder, while Maria was nineteen years and one month old. Alexei would have been fourteen in two weeks' time. Alexei's elder sisters Olga and Tatiana were twenty-two and twenty-one years old at the time of the murder respectively. The bones were found using metal detectors and metal rods as probes. Also, striped material was found that appeared to have been from a blue-and-white striped cloth; Alexei commonly wore a blue-and-white striped undershirt.
DNA proof.
On April 30, 2008, Russian forensic scientists announced that DNA testing proves that the remains belong to the Tsarevich Alexei and his sister Anastasia. DNA information, made public in July 2008, that has been obtained from Yekaterinburg and repeatedly subject to independent testing by laboratories such as the University of Massachusetts Medical School, USA, and reveals that the final two missing Romanov remains are indeed authentic and that the entire Romanov family housed in the Ipatiev House, Yekaterinburg were executed in the early hours of 17 July 1918. In March 2009, results of the DNA testing were published, confirming that the two bodies discovered in 2007 were those of Tsarevich Alexei and Anastasia.
Research on mitochondrial DNA (mtDNA) was conducted in the American AFDIL and in European GMI laboratories. In comparison with the previous analyses mtDNA in the area of Aleksandra Fyodorovna, positions 16519C, 524.1A and 524.2C were added. The DNA of Prince Philip, Duke of Edinburgh, a great-nephew of the last Tsarina, was used by forensic scientists to identify her body and those of her children.
Verification of identification via a repeat analysis of Prince Philip's mtDNA, to resolve apparent discrepancies between his genotype and those of the remains of the empress and her children, has not been conducted. Thus writer Michael Kirk (Great Britain) in 1998 put forward the version that Prince Philip wasn't the son of his putative mother (Princess Andrew of Greece), so mtDNA couldn't be used in the comparative analysis for identification of the remains of Aleksandra Fiodorovna and her children. Some experts suggest analysis of other relatives in the female line, in particular of the Spanish Queen Sofia or her brother, the deposed King Constantine II of Greece. As the repeated analysis of blood from Prince Philip wasn't carried out, there is a difference, and, so while repeated or other analysis on Prince Philip won't be carried out, legal identification of the remains of the Tsarina and her children will not be complete.
Killing of extended family.
On July 18, 1918, the day after the killing at Yekaterinburg of the tsar and his family, members of the extended Russian imperial family met a brutal death by being killed near Alapayevsk by Bolsheviks. They included: Grand Duke Sergei Mikhailovich of Russia, Prince Ioann Konstantinovich of Russia, Prince Konstantin Konstantinovich of Russia, Prince Igor Konstantinovich of Russia and Prince Vladimir Pavlovich Paley, Grand Duke Sergei's secretary Varvara Yakovleva, and Grand Duchess Yelizaveta Fyodorovna, a granddaughter of Queen Victoria and elder sister of Tsarina Alexandra. Grand Duchess Yelizaveta had departed her family following the 1905 assassination of her husband, Grand Duke Sergei Alexandrovich, had donated all her wealth to the poor and became a nun, but was nonetheless killed. In January 1919 revolutionary authorities killed Grand Duke Dmitry Konstantinovich, Grand Duke Nikolai Mikhailovich, Grand Duke Pavel Alexandrovich, and Grand Duke Georgy Mikhailovich who had been held in the prison of the Saint Peter and Paul Fortress in Petrograd.
The bodies were recovered from the mine by the White Army in 1918, who arrived too late to rescue them. The bodies were placed in coffins and were moved around Russia during struggles between the White and the opposing Red Army. By 1920 the coffins were interred in a former Russian Mission in Beijing, now beneath a parking area. In 1981 Princess Yelizaveta was canonized by the Russian Orthodox Church Outside of Russia, and in 1992 by the Moscow Patriarchate. In 2006 representatives of the Romanov family were making plans to reinter the remains elsewhere. The town is a place of pilgrimage to the memory of Yelizaveta Romanova.
Exiles.
Dowager Empress Maria Fyodorovna.
In 1919, Maria Fyodorovna, widow of Alexander III, and mother of Nicholas II, managed to escape Russia aboard the HMS Marlborough, which her nephew, King George V of the United Kingdom, had sent, at the urging of his own mother, Queen Alexandra, Maria's elder sister, to rescue her. After a stay in England with Queen Alexandra, she returned to her native Denmark, first living at Amalienborg Palace, with her nephew, King Christian X, and later, at Villa Hvidøre. Upon her death in 1928 her coffin was placed in the crypt of Roskilde Cathedral, the burial site of members of the Danish Royal Family.
In 2006, the coffin with her remains was moved to the Sts. Peter and Paul Fortress, in order to be buried alongside her husband. The transfer of her remains was accompanied by elaborate ceremonies, including at St. Isaac's officiated by the Patriarch Alexis II. Princes Dmitri and Prince Nicholas Romanov were present at the ceremony, along with , daughter of Prince Ioann Konstantinovich of Russia. Other members of the Imperial Family present included the descendants of the Dowager Empress Maria Feodorovna including Prince Michael Andreevich of Russia the senior direct male descendant. Princess Catherine who was 90 years old at the time, and died in Montevideo Uruguay the following year, was the last member of the Imperial Family to be born before the fall of the dynasty, and was ultimately to become the last surviving uncontested dynast of the Imperial House of Russia.
Other exiles.
Among the other exiles who managed to leave Russia, were Maria Fyodorovna's two daughters, the Grand Duchesses Xenia, and Olga Alexandrovna, with their husbands, Grand Duke Alexander Mikhailovich and Nikolai Kulikovsky, respectively, and their children. Xenia remained in England, following her mother's return to Denmark, although Olga remained with her mother until her death, with both sisters dying in 1960. Grand Duchess Maria Pavlovna, widow of Nicholas II's uncle, Grand Duke Vladimir, and her children Kiril, Boris, Andrei, and Elena, also managed to flee Russia. Grand Duke Dmitri Pavlovich, a cousin of Nicholas II, had been exiled to the Caucusus in 1916, for his part in the murder of Grigori Rasputin, and managed to escape Russia, as did his sister, Maria. Grand Duke Nicholas Nikolaievich, who had commanded Russian troops in World War I, prior to Nicholas II taking command, and his brother, Grand Duke Peter, and their wives, Grand Duchesses Anastasia and Militza Nikolaevna of Russia, whom were sisters, also fled.
Romanov family jewelry.
The collection of jewels and jewelry collected by the Romanov family during their reign are commonly referred to as the "Russian Crown Jewels" and they include official state regalia as well as personal pieces of jewelry worn by Romanov rulers and their family. After the Czar was deposed and his family murdered, their jewels and jewelry became the property of the new Soviet government. A select number of pieces from the collection were sold at auction by Christie's in London in March 1927. The remaining collection is on view today in the Armory at the Moscow Kremlin
On August 28, 2009, a Swedish public news outlet reported that a collection of over 60 jewel-covered cigarette cases and cufflinks owned by the Romanov family, had been found in the archives of the Swedish Ministry for Foreign Affairs, and was returned. The jewelry was allegedly turned over to the Swedish embassy in St. Petersburg in November 1918 by Duchess Marie of Mecklenburg-Schwerin to keep it safe. The jewelry's worth was estimated to 20 million Swedish krona (about 2.6 million US dollars).
Contemporary Romanovs.
There have been numerous unsubstantiated reports of Romanov impostors claiming to be of members of the deposed Tsar Nicholas II's family, the best known Anna Anderson. Proven research however makes clear that all of the Romanovs held prisoners inside the Ipatiev House in Ekaterinburg were brutally killed. The descendants of Xenia and Olga, Nicholas II's two sisters, survive to this day.
Grand Duke Kirill Vladimirovich, the direct male-line patrilineal descendant of Alexander II of Russia, claimed the headship of the defunct Imperial House of Russia, and assumed the pretender title "Emperor and Autocrat of all the Russians" in 1924 when evidence that all earlier claimants had been killed was final. He was followed by his only son Vladimir Kirillovich. Vladimir's only child, Maria Vladimirovna, is the current pretender; her only son from her marriage with Prince Franz Wilhelm of Prussia, George Mikhailovich, is her heir apparent, dynastically about to found a new branch Hohenzollern-Romanov, while other descendants of junior side branches of the "Holstein-Gottorp-Romanovs" are still living. The Romanov Family Association, a private association of most of the remaining descendants of Emperor Paul I of Russia, makes no claim to the defunct throne, and disputes the current headship of the house. These two groups continue to have differences of opinion.
Heraldry.
For a detailed armorial, see here: .
There is a detailed article here on the arms of the Russian Empire.
Family tree.
A summary family tree is:
In popular culture.
In the real time strategy game , the leader of the Soviet Union is Premier Alexander Romanov (voiced by Nicholas Worth). In one part of the game, he claims to be "upholding the Romanov legacy". He is also shown to be a distant relative of Nicholas II.

</doc>
<doc id="26243" url="http://en.wikipedia.org/wiki?curid=26243" title="Robert Bloch">
Robert Bloch

Robert Albert Bloch (; April 5, 1917 – September 23, 1994) was a prolific American fiction writer, primarily of crime, horror, fantasy and science fiction, from Milwaukee, Wisconsin. He is best known as the writer of "Psycho", the basis for the film of the same name by Alfred Hitchcock. He wrote that "Despite my ghoulish reputation, I really have the heart of a small boy. I keep it in a jar on my desk," (a quote borrowed by Stephen King and often misattributed to him). His fondness for a pun is evident in the titles of his story collections such as "Tales in a Jugular Vein", "Such Stuff as Screams Are Made Of" and "Out of the Mouths of Graves".
Bloch wrote hundreds of short stories and over 30 novels. He was one of the youngest members of the Lovecraft Circle. H. P. Lovecraft was Bloch's mentor and one of the first to seriously encourage his talent. However, while Bloch started his career by emulating Lovecraft and his brand of "cosmic horror", he later specialized in crime and horror stories dealing with a more psychological approach.
Bloch was a contributor to pulp magazines such as "Weird Tales" in his early career, and was also a prolific screenwriter and a major contributor to science fiction fanzines and fandom in general.
He won the Hugo Award (for his story "That Hell-Bound Train"), the Bram Stoker Award, and the World Fantasy Award. He served a term as president of the Mystery Writers of America (1970) and was a member of that organisation and of Science Fiction Writers of America, the Writers Guild of America, the Academy of Motion Picture Arts and Sciences and the Count Dracula Society. In 2008, The Library of America selected Bloch's essay "The Shambles of Ed Gein" (1962) for inclusion in its two-century retrospective of American true crime.
His favorites among his own novels were "The Kidnapper", "The Star Stalker", "Psycho", "Night-World," and "Strange Eons". His work has been extensively adapted for the movies and television, comics and audio books.
Biography.
Youth and education.
Bloch was born in Chicago, the son of Raphael "Ray" Bloch (1884–1952), a bank cashier, and his wife Stella Loeb (1880–1944), a social worker, both of German Jewish descent. Bloch's family moved to Maywood, a Chicago suburb, when he was five. He attended the Methodist Church there, despite his parents' Jewish heritage. At ten years of age, living in Maywood, he attended a screening of Lon Chaney, Sr.'s film "The Phantom of the Opera" (1925) late at night on his own. The scene of Chaney removing his mask terrified the young Bloch and sparked his interest in horror.
In 1929 Ray Bloch lost his bank job, and the family moved to Milwaukee, where Stella worked at the Milwaukee Jewish Settlement settlement house. Robert attended Washington, then Lincoln High School, where he met lifelong friend Harold Gauer. Gauer was editor of "The Quill", Lincoln's literary magazine, and accepted Bloch's first published short story, a horror story titled "The Thing" (the "thing" of the title was Death). Both Bloch and Gauer graduated from Lincoln in 1934 during the height of the Great Depression. Bloch was involved in the drama department at Lincoln and wrote and performed in school vaudeville skits.
Weird Tales and the Influence of H.P. Lovecraft.
During the 1930s, Bloch was an avid reader of the pulp magazine "Weird Tales", which he had had discovered at the age of ten in 1927; he began his readings of the magazine with the first instalment of Otis Adelbert Kline's "The Bride of Osiris" which dealt with a secret Egyptian city called Karneter located beneath Bloch's birth city of Chicago. H. P. Lovecraft, a frequent contributor to "Weird Tales" became one of his favorite writers. As a teenager, Bloch wrote a fan letter to Lovecraft (1933), who gave him advice on his own fiction-writing efforts. Bloch's first publication was with the short story "Lilies" in the semi-professional magazine "Marvel Tales" (Winter 1934). Bloch began correspondence with August Derleth, Clark Ashton Smith and others of the 'Lovecraft Circle'. Bloch's first publication in "Weird Tales" was in its letter-column, with a letter criticising the Conan stories of Robert E. Howard. His first professional sales, at the age of 17 (July 1934), to "Weird Tales", were the short stories "The Feast in the Abbey" and "The Secret in the Tomb". "Feast..." appeared first, in the January 1935 issues which actually went on sale November 1, 1934; "Secret in the Tomb" appeared in the May 1935 "Weird Tales".
Bloch's early stories were strongly influenced by Lovecraft. Indeed, a number of his stories were set in, and extended, the world of Lovecraft's Cthulhu Mythos. These include "The Dark Demon", in which the character Gordon is a figuration of Lovecraft, and which features Nyarlathotep; "The Faceless God" (features Nyarlathotep); "The grinning Ghoul" (written after the manner of Lovecraft) and "The Unspeakable Betrothal" (vaguely attached to the Cthulhu Mythos). It was Bloch who invented, for example, the oft-cited Mythos texts "De Vermis Mysteriis" and "Cultes des Goules". Many other stories influenced by Lovecraft were later collected in Bloch's volume "Mysteries of the Worm" (now in its third, expanded edition).
The young Bloch appears, thinly disguised, as the character "Robert Blake" in Lovecraft's story "The Haunter of the Dark" (1936), which is dedicated to Bloch. Bloch was the only individual to whom Lovecraft ever dedicated a story. In this story, Lovecraft kills off the Bloch character, repaying a courtesy Bloch earlier paid Lovecraft with his 1935 tale "The Shambler from the Stars", in which the Lovecraft-inspired figure dies; the story goes so far as to use Bloch's then-current street address in Milwaukee. (Bloch even had a signed certificate from Lovecraft [and some of his creations] giving Bloch permission to kill Lovecraft off in a story.) Bloch later wrote a third tale, "The Shadow From the Steeple", picking up where "The Haunter of the Dark" finished ("Weird Tales" Sept 1950). After Lovecraft's death in 1937, Bloch continued writing for "Weird Tales", where he became one of its most popular authors. He also began contributing to other pulps, such as the science fiction magazine "Amazing Stories".
Bloch's late novel "Strange Eons" is a full-length tribute to the style and subject matter of Lovecraft.
After Lovecraft's death in 1937, which affected Bloch deeply, Bloch broadened the scope of his fiction. His horror themes included voodoo ("Mother of Serpents"), the conte cruel ("The Mandarin's Canaries"), demonic possession ("Fiddler's Fee"), and black magic ("Return to the Sabbat"). Bloch visited Henry Kuttner in California in 1937. Bloch's first science fiction story, "The Secret of the Observatory", was published in "Amazing Stories" (August 1938).
Milwaukee Fictioneers and the Depression.
In 1935 Bloch joined a writers' group, The Milwaukee Fictioneers, members of which included Stanley Weinbaum, Ralph Milne Farley and Raymond A. Palmer. Another member of the group was Gustav Marx, who offered Bloch a job writing copy in his advertising firm, also allowing Bloch to write stories in his spare time in the office. Bloch was close friends with C.L. Moore and her husband Henry Kuttner, who visited him in Milwaukee.
During the years of the Depression, Bloch appeared regularly in dramatic productions, writing and performing in his own sketches. Around 1936 he sold some gags to radio comedians Stoopnagle and Budd, and to Roy Atwell.
Campaign manager for Carl Zeidler.
In 1939, Bloch was contacted by James Doolittle, who was managing the campaign for a little-known assistant city attorney in Milwaukee, Wisconsin named Carl Zeidler. He was asked to work on his speechwriting, advertising, and photo ops, in collaboration with Harold Gauer. They created elaborate campaign shows; in Bloch's 1993 autobiography, "Once Around the Bloch", he gives an inside account of the campaign, and the innovations he and Gauer came up with – for instance, the original releasing-balloons-from-the-ceiling shtick. He comments bitterly on how, after Zeidler's victory, they were ignored and not even paid their promised salaries. He ends the story with a wryly philosophical point:
If Carl Zeidler had not asked Jim Doolittle to manage his campaign, Doolittle would never have contacted me about it. And the only reason Doolittle knew me to begin with was because he read my yarn ("The Cloak") in "Unknown". Rattling this chain of circumstances, one may stretch it a bit further. If I had not written a little vampire story called "The Cloak", Carl Zeidler might never have become mayor of Milwaukee.
The 1940s and '50s.
In the 1940s, Bloch created the humorous series character Lefty Feep in a story for "Fantastic Adventures". He published a total of 23 Lefty Feep stories, the last one published in 1950, but the bulk appeared during World War II. Feep's character name had actually been coined by Bloch's friend/collaborator Harold Gauer for their unpublished novel "In the Land of Sky-Blue Ointments", Bloch also worked for a time in local vaudeville and tried to break into writing for nationally known performers.
In 1944 Bloch was asked to write 39 15-minute episodes of a radio horror show called "Stay Tuned for Terror". Many of the programs were adaptations of his own pulp stories. None of the episodes, which were all broadcast, are extant.
A year later August Derleth's Arkham House, Lovecraft's publisher, published Bloch's first collection of short stories, "The Opener of the Way". At the same time, his best-known early tale, "Yours Truly, Jack the Ripper", received considerable attention through dramatization on radio and reprinting in anthologies. This story, as noted below, involving a Ripper who has found literal immortality through his crimes, has been widely imitated (or plagiarized); Bloch himself would return to the theme (see below).
Bloch gradually evolved away from Lovecraftian imitations towards a unique style of his own. One of the first distinctly "Blochian" stories was "Yours Truly, Jack the Ripper", which was published in "Weird Tales" in 1943. The story was Bloch's take on the Jack the Ripper legend, and was filled out with more genuine factual details of the case than many other fictional treatments. It cast the Ripper as an eternal being who must make human sacrifices to extend his immortality. It was adapted for both radio (in "Stay Tuned for Terror") and television (as an episode of "Thriller" in 1961 adapted by Barré Lyndon). Bloch followed up this story with a number of others in a similar vein dealing with half-historic, half-legendary figures such as the Man in the Iron Mask ("Iron Mask", 1944), the Marquis de Sade ("The Skull of the Marquis de Sade", 1945) and Lizzie Borden ("Lizzie Borden Took an Axe...", 1946).
Bloch's first novel was the thriller "The Scarf" (1947). (He later issued a revised edition in 1966). It tells the story of a writer, Daniel Morley, who uses real women as models for his characters. But as soon as he is done writing the story, he is compelled to murder them, and always the same way: with the maroon scarf he has had since childhood. The story begins in Minneapolis and follows him and his trail of dead bodies to Chicago, New York, and finally Hollywood, where his hit novel is going to be turned into a movie, and where his self-control may have reached its limit.
Bloch published three novels in 1954 – "Spiderweb", "The Kidnapper" and "The Will to Kill" as he endeavored to support his family. That same year he was a weekly guest panellist on the TV quiz show "It's a Draw". "Shooting Star" (1958), a mainstream novel, was published in a double volume with a collection of Bloch's stories titled "Terror in the Night". "This Crowded Earth" (1958) was science fiction.
With the demise of "Weird Tales", Bloch continued to have his fiction published in "Amazing", "Fantastic", "The Magazine of Fantasy and Science Fiction", and "Fantastic Universe"; he was a particularly frequent contributor to "Imagination" and "Imaginative Tales". His output of thrillers increased and he began to appear regularly in "The Saint", "Ellery Queen" and similar mystery magazines, and to such suspense and horror-fiction magazine projects as "Shock".
Jack the Ripper.
Bloch continued to revisit the Jack the Ripper theme. His contribution to Harlan Ellison's 1967 science fiction anthology "Dangerous Visions" was a story, "A Toy for Juliette", which evoked both Jack the Ripper and the Marquis de Sade in a time-travel story. The same anthology had Ellison's sequel to it titled "The Prowler in the City at the Edge of the World". His earlier idea of the Ripper as an immortal being resurfaced in Bloch's contribution to the original "Star Trek" series episode "Wolf in the Fold". His 1984 novel "Night of the Ripper" is set during the reign of Queen Victoria and follows the investigation of Inspector Frederick Abberline in attempting to apprehend the Ripper, and includes some famous Victorians such as Sir Arthur Conan Doyle within the storyline.
"Psycho".
Bloch won the Hugo Award for Best Short Story in 1959, the same year that "Psycho" was published. Bloch had written an earlier short story involving split personalities, "The Real Bad Friend", which appeared in the February 1957 "Mike Shayne Mystery Magazine", that foreshadowed the 1959 novel "Psycho". However, "Psycho" also has thematic links to the story "Lucy Comes to Stay".
Norman Bates, the main character in "Psycho", was very loosely based on two people. First was the real-life serial killer Ed Gein, about whom Bloch later wrote a fictionalized account, "The Shambles of Ed Gein". (The story can be found in "Crimes and Punishments: The Lost Bloch, Volume 3"). Second, it has been indicated by several people, including Noel Carter (wife of Lin Carter) and Chris Steinbrunner, as well as allegedly by Bloch himself, that Norman Bates was partly based on Calvin Beck, publisher of "Castle of Frankenstein". Bloch's basing of the character of Norman Bates on Ed Gein is discussed in the documentary "Ed Gein: The Ghoul of Plainfield", which can be found on Disc 2 of the DVD release of 2003's "The Texas Chainsaw Massacre". Bloch has also, however, commented that it was the situation itself - a mass murderer living undetected and unsuspected in a typical small town in middle America - rather than Gein himself who sparked Bloch's storyline. He writes: "Thus the real-life murderer was not the role model for my character Norman Bates. Ed Gein didn't own or operate a motel. Ed Gein didn't kill anyone in the shower. Ed Gein wasn't into taxidermy. Ed Gein didn't stuff his mother, keep her body in the house, dress in a drag outfit, or adopt an alternative personality. These were the functions and characteristics of Norman Bates, and Norman Bates didn't exist until I made him up. Out of my own imagination, I add, which is probably the reason so few offer to take showers with me." 
Though Bloch had little involvement with the film version of his novel, which was directed by Alfred Hitchcock from an adapted screenplay by Joseph Stefano, he was to become most famous as its author.
The novel is one of the first examples at full length of Bloch's use of modern urban horror relying on the horrors of interior psychology rather than the supernatural. "By the mid-1940s, I had pretty well mined the vein of ordinary supernatural themes until it had become varicose," Bloch explained to Douglas E. Winter in an interview. "I realized, as a result of what went on during World War II and of reading the more widely disseminated work in psychology, that the real horror is not in the shadows, but in that twisted little world inside our own skulls." While Bloch was not the first horror writer to utilise a psychological approach (that honor belongs to Edgar Allan Poe and J. Sheridan Le Fanu), Bloch's psychological approach in modern times was comparatively unique.
Bloch's agent, Harry Altshuler, received a "blind bid" for the novel – the buyer's name wasn't mentioned – of $7,500 for screen rights to the book. The bid eventually went to $9,500, which Bloch accepted. Bloch had never sold a book to Hollywood before. His contract with Simon and Schuster included no bonus for a film sale. The publisher took 15 percent according to contract, while the agent took his 10%; Bloch wound up with about $6,750 before taxes. Despite the enormous profits generated by Hitchcock's film, Bloch received no further direct compensation.
Only Hitchcock's film was based on Bloch's novel. The later films in the "Psycho" series bear no relation to either of Bloch's sequel novels. Indeed, Bloch's proposed script for the film "Psycho II" was rejected by the studio (as were many other submissions), and it was this that he subsequently adapted for his own sequel novel.
The 2012 film Hitchcock tells the story of Alfred Hitchcock's making of the film version of "Psycho" but although it mentions Bloch and his novel, no part for the character of Bloch was cast for the movie.
The 1960s: Hollywood and screenwriting.
Following his move to Hollywood, around 1960, Bloch had multiple assignments from various television companies. However, he was not allowed to write for five months when the Writers Guild had a strike. After the strike was over, he became a much used scriptwriter in television and film projects in the mystery, suspense, and horror genre. His first assignments were for the Macdonald Carey vehicle, "Lock-Up", (penning five episodes) as well as one for "Whispering Smith", and an original screenplay for the 1962 film "The Couch". Further TV work included an episode of "Bus Stop", 10 episodes of "Thriller" (1960–62, several based on his own stories), and 10 episodes of "Alfred Hitchcock Presents" (1960–62). In 1962, he wrote the screenplay for "The Cabinet of Caligari" (1962), an unhappy experience (see Films section below).
In 1962, Bloch penned the story and teleplay "The Sorcerer's Apprentice" for "Alfred Hitchcock Presents". The episode was shelved when the NBC Television Network and sponsor Revlon called its ending "too gruesome" (by 1960s standards) for airing. Bloch was pleased later when the episode was included in the program's syndication package to affiliate stations where not one complaint was registered. Today, due to public domain status, the episode is readily available in home media formats from numerous distributors and is even available on free video on demand. For details of Bloch's scripts for Hitchcock shows see
His TV work did not slow Bloch's fictional output. In the early 1960s he published several novels, including "The Dead Beat" (1960), and "Firebug " (1961) (for which Harlan Ellison, then an editor at Regency Books, contributed the first 1200 words). In 1962 his novels "The Couch" (1962) (the basis for the screenplay of his first movie, filmed the same year) and "Terror" (originally titled "Kill for Kali") were published. Stephen King has written that "What Bloch did with such novels as "The Deadbeat", "The Scarf", "Firebug", "Psycho", and "The Couch" was to re-discover the suspense novel and reinvent the antihero as first discovered by James Cain." 
Bloch wrote original screenplays for two movies produced and directed by William Castle, "Strait-Jacket" (1964) and "The Night Walker" (1964), along with "The Skull" (1965). The latter film was based on his short story "The Skull of the Marquis de Sade".
Marriages and family.
On October 2, 1940, Bloch married Marion Ruth Holcombe; it was reportedly a marriage of convenience designed to keep Bloch out of the army. During their marriage, she suffered (initially undiagnosed) tuberculosis of the bone, which affected her ability to walk.
After working for 11 years for the Gustav Marx Advertising Agency in Milwaukee, Bloch left in 1953 and moved to Weyauwega, Marion's home town, so she could be close to friends and family. Although she was eventually cured of tuberculosis, she and Bloch divorced in 1963. Bloch's daughter Sally (born 1943) elected to stay with him. On January 18, 1964, Bloch met recently widowed Eleanor ("Elly") Alexander (née Zalisko) — who had lost her first husband, writer/producer John Alexander, to a heart attack three months earlier — and made her his second wife in a civil ceremony on the following October 16. Elly was a fashion model and cosmetician. They honeymooned in Tahiti, and in 1965 visited London, then British Columbia. They remained happily married until Bloch's death. Elly remained in the Los Angeles area for several years after selling their Laurel Canyon Home to fans of Bloch's, eventually choosing to go home to Canada to be closer to her own family. She died March 7, 2007 at the Betel Home in Selkirk, Manitoba, Canada. Her ashes have been placed next to Bloch's in a similar book-shaped urn at Pierce Brothers in Westwood, California.
The 1960s: Screenwriting continued.
In 1964 Bloch wrote two movies for William Castle – "Strait-Jacket" and "The Night Walker".
Bloch's further TV writing in this period included "The Alfred Hitchcock Hour" (7 episodes, 1962–1965), "I Spy" (1 episode, 1966), "Run for Your Life" (1 episode, 1966), and "The Girl from U.N.C.L.E." (1 episode, 1967). He notably penned three original scripts for the original series of "" (1966–67): "What Are Little Girls Made Of?", "Wolf in the Fold" (a Jack the Ripper variant), and "".
His novels of the second half of the 1960s include "Ladies Day/This Crowded Earth" (1968)(sf), "The Star Stalker" (1968) and "The Todd Dossier" (1969) (the book publication of which bears the byline "Collier Young").
In 1968 Bloch returned to London to do two episodes for the Hammer Films series "Journey to the Unknown" for Twentieth Century Fox. One of the episodes, "The Indian Spirit Guide", was included in the TV movie "Journey to Midnight" (1968).
Following the 1965 movie "The Skull", which was based on a Bloch story but scripted by Milton Subotsky, between 1966 and 1972 Bloch wrote no less than five feature movies for Amicus Productions – "The Psychopath", "The Deadly Bees", "Torture Garden", "The House That Dripped Blood" and "Asylum". The last two films featured stories written by Bloch that were printed first in anthologies he wrote in the 1940s and early 1950s.
In 1969 he was invited to the Second International Film Festival in Rio de Janeiro, March 23–31, along with other science fiction writers from the US, Britain and Europe.
The 1970s and '80s.
During the 1970s Bloch wrote two TV movies for director Curtis Harrington – "The Cat Creature" and "The Dead Don't Die". "The Cat Creature" was an unhappy production experience for Bloch. Producer Doug Cramer wanted to do an update of Cat People, the Val Lewton classic. Bloch says: "Instead I suggested a blending of the elements of several well-remembered films, and came up with a story line which dealt with the Egyptian cat-goddess (Bast), reincarnation and the first bypass operation ever performed on an artichoke heart.". The rest of story of the troubled production in which he was firstly made to shorten his screenplay by twelve minutes, and then to lengthen it again at short notice, is described in Bloch's autobiography 
Bloch meanwhile (interspersed between his screenplays for Amicus Productions), penned single episodes for "Night Gallery" (1971), "Ghost Story" (1972), "The Manhunter" (1974), and "Gemini Man" (1976).
In 1975 Bloch was presented with a Lifetime Achievement Award at the First World Fantasy Convention held in Providence, Rhode Island. The award was a bust of H.P. Lovecraft. An audio recording was made of Robert Bloch during that 1975 convention, accessible at the following link. 
Bloch continued to published short story collections throughout this period. His "Selected Stories" (reprinted in paperback with the incorrect title "The Complete Stories") appeared in three volumes just prior to his death, although many previously uncollected tales have appeared in volumes published since 1997 (see below). Bloch contributed a story, "Heir Apparent" set in Andre Norton's Witch World to the volume "Tales of the Witch World"(Vol 1) NY: Tor, 1987.
His numerous novels of this two decade period range from science fiction - "Sneak Preview" (1971) - through horror novels such as the Lovecraftian "Strange Eons" (1978) and "Night of the Ripper" (1984); the non-supernatural mystery "There is a Serpent in Eden" (1979); his two sequels to the original "Psycho" ("Psycho II" and "Psycho House"), and late novels such as the thriller "Lori" (1989) and "The Jekyll Legacy" with Andre Norton (1991), a sequel to Robert Louis Stevenson's "Dr. Jekyll and Mr. Hyde". Omnibus editions of hard-to-acquire early novels appeared as "Unholy Trinity" (1986) and "Screams" (1989).
Bloch's screenplay-writing career continued active through the 1980s, with teleplays for "Tales of the Unexpected" (one episode, 1980), "Darkroom" (two episodes,1981), "Alfred Hitchcock Presents" (1 episode, 1986), "Tales from the Darkside" (three episodes, 1984–87) and "Monsters" (three episodes, 1988–1989 - "Beetles", "A Case of the Stubborns" and "Everybody needs a Little Love"). No further screen work appeared in the last five years before his death, although an adaptation of his "collaboration" with Edgar Allan Poe, "The Lighthouse", was filmed as an episode of "The Hunger" in 1998.
In February 1991 he was given the Honor of Master of Ceremonies at the first World Horror Convention held in Nashville, Tennessee.
Death and Legacy.
In 1994, Bloch died of cancer at the age of 77. in Los Angeles after a writing career lasting 60 years, including more than 30 years in television and film. He was cremated and interred in the Room of Prayer columbarium at Westwood Village Memorial Park Cemetery in Los Angeles. His wife Elly is also interred there.
The Robert Bloch Award is presented at the annual Necronomicon convention. Its recipient in 2013 was editor and scholar S.T. Joshi. The award is in the shape of the Shining Trapezohedron as described in H.P. Lovecraft's tale dedicated to Bloch, The Haunter of the Dark.
Writings on Bloch.
An early reference work by Australian writer Graeme Flanagan, "Robert Bloch: A Bio-Bibliography" (1979) includes interviews with Bloch and memoirs by fellow writers such as Harlan Ellison, Richard Matheson, Mary Elizabeth Counselman and Fritz Leiber.
An essay by Lee Prosser about Robert Bloch was published in "The Roswell Literary Review" at Roswell, New Mexico, 1996.
"The Existential Robert Bloch," an interview by Lee Prosser with Bloch in March 1983, was published at Michael G. Pfefferkorn's "The Bat is My Brother" website.
"A Conversation With Lee Prosser," an in-depth interview with Lee Prosser about Bloch by Michael G. Pfefferkorn on May 31, 2002 was published at Michael G. Pfefferkorn's The Unofficial Robert Bloch Website.
Randall D. Larson is the premier Bloch scholar. He published and edited an early tribute to Bloch, "The Robert Bloch Fanzine" (Fandom Unlimited, 1972). He later authored three reference books about Robert Bloch: "The Robert Bloch Reader's Guide" (1986, a literary analysis of Bloch's entire output through 1986), "The Complete Robert Bloch" (1986, an illustrated bibliography of Bloch's writing), and "The Robert Bloch Companion" (1986, collected interviews). In addition, an issue of "Paperback Parade" magazine (No. 39, August 1994) contains an article by Larson on collecting Bloch – "Paperblochs: Robert Bloch in Paperback."
"Crypt of Cthulhu" magazine No 40 (Vol. 5 No. 6 St. John's Eve, 1986). was a special Robert Bloch issue. It included some story reprints by Bloch, essays on his work and bibliography of his books by R. Dixon Smith.
In the anthology "My Favorite Horror Story" (DAW, 2000), edited by Mike Baker and Martin H. Greenberg, influential horror writers in the field picked their favourite stories. Out of 15 tales, only Bloch and H.P. Lovecraft are represented by two stories. Of Bloch's, Stephen King chose "Sweets to the Sweet" and Joe R. Lansdale chose "The Animal Fair". The selected Lovecraft stories are "The Colour Out of Space" and "The Rats in the Walls."
There is an essay on Bloch's work, with particular reference to the novels "Psycho" and "The Scarf", in S. T. Joshi's book "The Modern Weird Tale" (2001). Joshi examines Bloch's literary relationship with Lovecraft in a further essay in "The Evolution of the Weird Tale" (2004).
A more recent essay collection focusing on a range of Bloch's work is "Robert Bloch: the Man Who Collected Psychos", edited by Benjamin J. Szumskyj (McFarland, 2009).
Comic adaptations.
A number of Bloch's works have been adapted in graphic form for comics. These include:
The comic "Aardwolf" (No 2, Feb 1995) is a special tribute issue to Bloch. It contains brief tributes to Bloch from Harlan Ellison, Ray Bradbury, Richard Matheson, Julius Schwartz and Peter Straub incorporated within a piece called "Robert Bloch: A Retrospective" compiled by Clifford Lawrence. The first part of the text of Bloch's story 'The Past Master" is also reprinted in this issue.
Bloch also contributed a script as part of the one-shot comic "Heroes Against Hunger".
Audio adaptations.
A number of Bloch's works have been adapted for audio productions. For details on "Stay Tuned for Terror", see 1940s section above.
Other adaptations include:
Various recordings of Bloch speaking at fantasy and sf conventions are also extant. Many of these are available for download from Will Hart's CthulhuWho site: 
Bibliography.
Short-story collections.
Note: The following three entries represent paperback reprints of the Underwood Miller "Selected Stories" set. "Complete Stories" is a misnomer as these three volumes do not contain anywhere near the complete oeuvre of Bloch's short fiction. 
Awards.
See also 42nd World Science Fiction Convention
Movies.
The following is a list of films based on Bloch's work. For some of these he wrote the original screenplay; for others, he supplied the story or a novel (as in the case of "Psycho") on which the screenplay was based.
Unproduced screenplays.
Bloch wrote a number of screenplays which remain unproduced. These include "Merry-Go-Round" for MGM (based on Ray Bradbury's story "Black Ferris"); "Night-World" (from Bloch's novel, for MGM); and "Day of the Comet" (from the H.G. Wells story), and a television adaptation of "Out of the Aeons". See also "The Todd Dossier" under the Bibliography section above.

</doc>
<doc id="26244" url="http://en.wikipedia.org/wiki?curid=26244" title="Recorder (musical instrument)">
Recorder (musical instrument)

The recorder is a family of woodwind musical instruments of the group known as "fipple flutes" or "internal duct flutes"—whistle-like instruments that include the tin whistle. The recorder is end-blown, and the mouth of the instrument is constricted by a wooden plug, known as a "block" or "fipple". It is distinguished from other members of the family by having holes for seven fingers (the lower one or two often doubled to facilitate the production of semitones) and one for the thumb of the uppermost hand. The bore of the recorder can be tapered slightly, being widest at the mouthpiece end and narrowest towards the foot on Baroque recorders. Renaissance-era instruments also taper, but generally have more nearly cylindrical bores. Recorders can be made out of wood, plastic, or ivory.
The recorder was popular in medieval times through the baroque era, but declined in the 18th century in favour of orchestral woodwind instruments, such as the flute, oboe, and clarinet. During its heyday, the recorder was traditionally associated with pastoral scenes, miraculous events, funerals, marriages, and amorous scenes. Images of recorders can be found in literature and artwork associated with all of these. Purcell, Bach, Telemann, and Vivaldi used the recorder to suggest shepherds and imitate birds in their music, a theme that continued in 20th-century music.
The recorder was revived in the 20th century, partly in the pursuit of historically informed performance of early music, but also because of its suitability as a simple instrument for teaching music and its appeal to amateur players. Today, it is sometimes thought of as a child's instrument, but there are many professional players who demonstrate the instrument's full solo range. The sound of the recorder is remarkably clear and sweet, partly because of the lack of upper harmonics and predominance of odd harmonics in the sound.
Name of the instrument.
The instrument has been known by its modern name at least since the 14th century. David Lasocki reports the earliest use of the word "recorder" was in the household of the Earl of Derby (later to become King Henry IV) in 1388: "fistula nomine Recordour". By 1530 the word had transferred into English: "Of recorders fond fyrst the melodies". The name is apparently derived from the verb "to record" (senses 2 & 3) with the suffix -er. To record in this sense is uncommon in modern English; it originally meant to practice and learn, literally "by heart" from the Latin "corda". In 1413 the "Pilgr. Sowle" (Pilgrim Soul, printed by Caxton in 1483) includes: "When they hadde these instrumentes they recorded songes besyly tylle that they were ... parfyte ynowe in al maner musike".
Up to the 18th century, the instrument was called "flauto" in Italian, the language used in writing music, whereas the instrument we today call the flute was called "traverso". This has led to some pieces of music occasionally being mistakenly performed on the "flauto traverso" (transverse flute) rather than on the recorder.
Today, the recorder is known as "flauto dolce" in Italian (sweet flute), with equivalents in other languages, such as "flauta doce" in Brazilian Portuguese (or "flauta de Bisel" in European Portuguese) and "flauta dulce" in Spanish. In those two languages, the name "flauta" is ambiguous, as it can mean any kind of transverse flutes, a recorder, or different other types of wind blown instruments, like the pan flute and some instruments used by the descendants of native peoples of the Central and South Americas (with varied degrees of influence of European instruments). In French the word "flûte" is similarly ambiguous (the French translation is "flûte à bec", literally "beaked flute"), and it is also called "flauta de pico" in Spanish, meaning the same. In German the fipple block is called the "Block" and hence the German name is "Blockflöte", while the modern flute is called "Querflöte" (literally from "flauto traverso)", "Großflöte" (great flute) or simply "Flöte". Naming in Dutch and the Scandinavian languages follows the same convention as in German. In Dutch, for example, the recorder is called "blokfluit" and the flute is "dwarsfluit", and in Swedish the corresponding names are "blockflöjt" and "tvärflöjt".
Types of recorder.
Sizes.
Recorders are made in a variety of sizes and each has its own register. They are most often tuned in C or F, meaning their lowest note possible is a C or an F. However, instruments in D, B-flat, G, and E-flat were not uncommon historically and are still found today, especially the alto (treble) recorder in G, the standard Renaissance alto recorder, and the alto recorder in D, called a "voice flute". The table shows the recorders in common use, though the large ones are very rare.
The recorder most often used for solo music is the alto (America) or treble (UK) recorder. Sometimes and when the recorder is specified without further qualification, it is this size that is meant. The soprano (America) or descant (UK) also has an important repertoire of solo music (not just school music) and there is a little for tenor and bass recorders.
The larger recorders have great enough distances between the finger holes that most people's hands can not reach them all. So, instruments larger than the tenor have keys to enable the player to cover the holes or to provide better tonal response; this is also true of the tenor itself, over the last hole, and much more rarely the alto. In addition, the largest recorders are so long that the player cannot simultaneously reach the finger holes with the hands and reach the mouthpiece with the lips. So, instruments larger than the bass (and some bass recorders too) may use a bocal or crook, a thin metal tube, to conduct the player's breath to the windway, or they may be constructed in sections that fold the recorder into a shape that brings the mouthpiece closer to the player..
Construction.
Today, high-quality recorders are made from a range of hardwoods: maplewood ("Acer pseudoplatanus", specific gravity 0.63), pearwood ("Pyrus communis", specific gravity 0.65), plumwood ("Prunus domestica", specific gravity 0.79), Castello "boxwood" ("Calycophyllum multiflorum", specific gravity 0.8), Zapatero "boxwood" ("Gossypiospermum praecox", specific gravity 0.8), olivewood ("Olea europaea", specific gravity 0.85), European boxwood ("Buxus sempervirens", specific gravity 0.95), rosewood (including tulipwood ("Dalbergia decipularis", specific gravity 0.95), palisander ("Dalbergia retusa", specific gravity 1.05), kingwood ("Dalbergia cearensis", specific gravity 1.2), etc.), ebony ("Diospyros perrieri", specific gravity 1.1), or grenadilla ("Dalbergia melanoxylon", specific gravity 1.2), with a block of red cedar wood. 
Plastic recorders are produced in large quantities. Plastics are cheaper and require less maintenance and quality plastic recorders can be as good as lower-end wooden instruments. Beginners' instruments, the sort usually found in children's ensembles, are plastic and can be purchased quite cheaply.
Some newer designs of recorder are now being produced. Larger recorders built like organ pipes with square cross-sections are cheaper than the normal designs if, perhaps, not so elegant. Another area is the development of instruments with a greater dynamic range and more powerful bottom notes. These modern designs make it easier to be heard when playing concertos. Finally, recorders with a downward extension of a semitone are becoming available; such instruments can play a full three octaves in tune.
German fingering.
In the early part of the 20th century, Peter Harlan developed a recorder that allowed for apparently simpler fingering, called German fingering. A recorder designed for German fingering has a hole five which is smaller than hole four, whereas baroque and neo-baroque recorders have a hole four which is smaller than hole five. The immediate difference in fingering is for F and B♭ which on a neo-baroque instrument must be fingered 0 123 4–67. With German fingering, this becomes a simpler 0 123 4 – – –. Unfortunately, however, this causes many other chromatic notes to be too badly out of tune to be usable. German fingering became popular in Europe, especially Germany, in the 1930s, but rapidly became obsolete in the 1950s as the recorder began to be treated more seriously and the limitations of German fingering became more widely appreciated. Despite this, many recorder makers continue to produce German fingered instruments today, essentially for beginner use only.
Notation and pitch.
Recorders are most commonly pitched at concert pitch (A=440 Hz). Increasingly, they are pitched at A=442 Hz as are pianos and orchestral instruments. However, among serious amateurs and professionals, two other standard pitches are commonly found. For baroque instruments, A=415 Hz is the "de facto" standard, while Renaissance instruments are often pitched at A=466 Hz. Both tunings are a compromise between historical accuracy and practicality. For instance, the Stanesby Sr alto, copied by many contemporary makers is based on A=403 Hz; some makers indeed offer an instrument at that pitch. Some recorder makers offer 3-piece instruments with two middle sections, accommodating two tuning systems. At 415 Hz, a semitone lower than 440 Hz, recorders can play with those harpsichords that are also tuned to that pitch.
The recorder family is non-transposing, which means that sheet music for recorder is nearly always written in the key in which it is played. A written C in the score actually sounds as a C. (The higher members of the family (soprano and above) transpose at the octave, as do the bass instruments (bass and great bass)). Recorders are referred to as "D-fingered", "C-fingered", "G-fingered", "F-fingered", etc., depending on the lowest note, fingered with all holes closed ("C-fingered" and "F-fingered" instruments being the most common). This implies that the player must learn two different sets of similar fingerings, one for the C recorders and another for the F recorders. A player may go from one C-fingered instrument to another easily, and from one F-fingered instrument to another easily, but switching between the two requires knowing both sets of fingerings, or the ability to transpose the music at sight.
Most recorders do transpose at the octave. The garklein sounds two octaves above the written pitch; the sopranino and soprano/descant sound one octave above written pitch. Treble (alto) and tenor sizes do not transpose at all, while the bass and great bass sound one octave above written (bass clef) pitch. In modern scores, these transpositions are indicated by adding a small figure "8" above the treble or bass clef on sopranino, soprano or bass recorder parts, but in the past and still commonly today, the transpositions are not indicated and instead are assumed from context. Contrabass and sub contrabass are non-transposing while the octocontrabass sounds one octave below written pitch.
Sizes from garklein down through tenor are notated in the treble clef while the bass size and lower usually read the bass clef. Professionals can usually read C-clefs and often perform from original notation.
Alternative notations which are only occasionally used:
As a rule of thumb, recorders sound one octave above the human voice after which they are named (soprano recorder is an octave above soprano voice, alto an octave above alto voice, etc.) The recorder's mellow tone and limited harmonics allows for the seemingly deeper sound.
How the instrument is played.
The recorder is held outwards from the player's lips (rather than to the side, like the "transverse" flute). The player's breath is compressed into a linear airstream by a channel cut into the wooden "block" or fipple (A), in the mouthpiece of the instrument, so as to travel along this channeled duct (B) called the "windway". Exiting from the windway, the breath is directed against a hard edge (C), called the "labium" or "ramp", which causes the column of air within the resonator tube to oscillate with standing waves. Unlike simple whistles where holes are progressively unstopped, the recorder uses half-holing and forking (see below) to modify the position of the nodes.
It is easiest to consider the pressure nodes rather than the displacement nodes. A half hole or a fork fingering does not allow all the pressure to escape, like a leaky valve, and so the pressure under the (half) open hole is higher than expected, leading to the node being displaced down the tube. The analysis of the higher registers will produce a series of closely (but not identically) spaced nodes down the tube but which, since they are coupled, modify each other to produce a single pitch.
Basic fingering.
The range of a modern recorder is usually taken to be about two octaves except in virtuoso pieces. See the table above for fingerings of notes in the nominal recorder range of 2 octaves and 1 whole tone. Notes above this range are more difficult to play, and the exact fingerings vary from instrument to instrument, so it is impractical to put them into the table here. The numbers at the top correspond to the fingers and the holes on the recorder, according to the pictures.
The note two octaves and one semitone above the lowest note (C♯ for soprano, tenor and great bass instruments; F♯ for sopranino, alto and bass instruments) is difficult to play on most recorders. These notes are best played by covering the end of the instrument (the "bell"); players typically use their upper leg to accomplish this. Some recorder makers added a special bell key for this note – newer recorder designs with longer bores also solve this problem and extend the range even further. The note is only occasionally found in pre-20th-century music, but it has become standard in modern music.
Recorders with Ganassi fingering have, for a tenor C recorder, the bottom hole open for the lower f, no holes covered at all for the d above that f, and shaded fingering for the upper f. The upper a has holes covered below. The upper e flat is fingered like the f above it, with hole 5 covered.
Some fonts show miniature glyphs of complete recorder fingering charts in TrueType format. Because there are no Unicode values for complete recorder fingering charts, these fonts are custom encoded.
Half-holing, forking, and shading.
The lowest chromatic scale degrees – a semitone and a minor third above the lowest note – are played by covering only a part of a hole, a technique known as "half-holing." Most modern instruments are constructed with double holes or keys to facilitate the playing of these notes; such double holes are occasionally found on baroque instruments, where even the hole for the third finger of the left hand can be doubled. Other chromatic scale degrees are played by so-called "fork" fingerings, uncovering one hole and covering one or more of the ones below it. Fork fingerings have a different tonal character from the diatonic notes, giving the recorder a somewhat uneven sound. Budget tenor/bass recorders might have a single key for low C/F but not low C♯/F♯, making this note virtually impossible to play. Double low keys allowing both C/F and C♯/F♯ are more or less standard today.
Ganassi's (1535) fingering for the note an octave and fourth above the lowest note shades hole 6. These fingerings are thus both forked and half-holed. They are not chromatic notes as they are B♭ for a bass recorder, F for a tenor recorder, and C for a Renaissance G alto. The note one whole tone below is fingered the same, but with hole 5 covered.
Pinching.
Most of the notes in the second octave and above are produced by partially opening the thumbhole on the back of the recorder, a technique known as "pinching". There are two basic methods for achieving this: a) drawing the thumb away from the hole, and b) bending the thumb. The first method uses only the skin of the thumb to define the opening, while the second method uses also the nail edge. The latter technique enables better feel and thus control of the size of the opening. The placement of the thumb is crucial to the tone, intonation and stability of these notes, and varies as the notes increase in pitch, making the boring of a double hole for the thumb unviable. To play the notes in the second register and above, the player must generally blow more air into the instrument and/or tongue somewhat harder to excite the second or upper harmonics of the instrument. This is, however, not universally true; it is possible for example to slur "piano" between and in the second and third registers.
Notes in the third octave.
A skilled player can, with a good recorder, play chromatically over two octaves and a fifth. Use of notes in the 3rd octave is becoming more common in modern compositions; several of these notes require closure of the bell or shading of the window area (i.e. holding the palm of the hand above the window, partially restricting the air emerging from it). In the hands of a competent player, these upper notes are not especially loud or shrill.
The Renaissance recorder had a normal range of one octave and a perfect fifth, though exceptional players could extend the range upward by one or two notes, and Ganassi found fingerings that would work for five further notes—though they varied according to the maker of the instrument. In 1619, Michael Praetorius explained that the larger sizes of recorders are only normally able to manage 13 notes (an octave and a fifth), and the better-made smaller ones can reach one more note. However, especially skilled instrumentalists are able to add four more "falsetto notes", reaching two octaves and a third. Modern reproductions of Renaissance instruments, especially those from the middle of the last century, often have a range as little as one and a half octaves.
Dynamics.
Changes in dynamics are not easy to achieve on the recorder if the player is accustomed to other wind instruments. The general belief is that if the player blows harder to play louder, or more softly to play softer, the pitch changes and the note goes out of tune, and unlike the transverse flute, the player cannot change the position of the mouth in relation to the labium in order to compensate, and that therefore the recorder is not capable of dynamic changes. This is misleading. It is true that in the hands of a skilled player changes in dynamics by simply blowing harder or softer are possible provided the instrument is of a high quality and the player knows the instrument well. Subtle changes in wind pressure are possible if the player has a good ear for tuning and knows how hard the instrument can be pushed before pitch changes become noticeable. But this is not the correct approach to recorder dynamics. On the recorder it is better to think of the breath controlling pitch, and the fingers controlling dynamics; for example by resting the fingers lightly on the holes breath leaks around them, lifting the pitch; and the resulting instinctive change in breath pressure to bring the pitch back also drops the volume. Advanced players use alternative fingerings to enable changes in dynamics. The recorder is notable for its sensitivity to articulation; in addition to its obvious use for artistic effect skilled players can also use this sensitivity to suggest changes in volume.
History.
Earliest recorders.
Internal duct-flutes have a long history: an example of an Iron Age specimen, made from a sheep bone, exists in Leeds City Museum.
The true recorders are distinguished from other internal duct flutes by having eight finger holes; seven on the front of the instrument and one, for the upper hand thumb, on the back, and having a slightly tapered bore, with its widest end at the mouthpiece.
One of the earliest surviving instruments was discovered in a castle moat in Dordrecht, the Netherlands in 1940, and has been dated to the 14th century. It is largely intact, though not playable. A second more or less intact 14th-century recorder was found in a latrine in northern Germany (in Göttingen): other 14th-century examples survive from Esslingen (Germany), Tartu (Estonia), and Nysa (Poland). There is a fragment of a possible 14th- or 15th-century bone recorder in Rhodes (Greece); and there is an intact 15th-century example from Elblag (Poland).
The earliest recorders were designed to be played either right-handed (with the right hand lowermost) or left-handed (with the left hand lowermost). The holes were all in a line except for the lowest hole, for the lower hand little finger. This last hole was offset from the center line, and drilled twice, once on each side. The player would fill in the hole they didn't want to use with wax. It is this doubled hole (not to be confused with the later double holes for semitones) which accounts for the early French name "flute à neuf trous". In later years, the right-hand style of playing was settled on as standard and the second hole disappeared.
Renaissance.
The recorder achieved great popularity in the 16th and 17th centuries. This development was linked to the fact that art music (as opposed to folk music) was no longer the exclusive domain of nobility and clergy. The advent of the printing press made it available to the more affluent commoners as well. The popularity of the instrument also reached the courts however. For example, at Henry VIII's death in 1547, an inventory of his possessions included 76 recorders. There are also numerous references to the instrument in contemporary literature (e.g. Shakespeare and Milton).
During the Renaissance musical instruments were principally used in dance music and as accompaniment for voices. There are many vocal works with non-texted lines, which possibly were written for instruments. In addition, some vocal music was easily playable with instruments, chansons, for example. Nevertheless, composers also produced more and more works exclusively for instruments, often based on dance music. (e.g. the "Lachrimae Pavans" by John Dowland). Often they did not specify the instruments to use although some, such as Anthony Holborne, indicated that their music was suitable for the recorder. However, even when the composer specified, for instance, viols, the music could successfully be played on recorders. A taste for ensembles of like instruments developed in this era, and so arose "consorts" (groups of musicians playing the same type of instrument) and the families of instruments of various sizes. The diversity of sizes in an instrument family allowed the consort to play music with a very large pitch range. Some of the well known Renaissance composers who wrote instrumental music, or whose vocal music plays well on recorders, were:
Polyphony was the dominant music style of the Renaissance, but composers also began to write chordal pieces. The Medieval custom of juxtaposing 2 or 3 different melodies coexisted with "imitative polyphony". Imitative polyphony uses only one melodic line, but breaks it in pieces and divides it among the different parts. One part plays the melody, then the other parts play it in their turns. The music of this epoch was characterized by complex improvised ornamentation.
Similar to the Medieval recorders, and unlike the Baroque style recorders typically used today, Renaissance recorders have a wide, more or less cylindrical bore. They have powerful low notes (much more so than the Baroque recorders). The wide bore means that a greater quantity of air is required to play the instrument, but this makes them more responsive. Many reproduction instruments, especially from the middle of the last century, can only be played reliably over a range of an octave and a sixth; but Ganassi reported a larger range is possible. When modern music is written for 'Ganassi recorders' it is this type of recorder which is intended. Historically there was, in truth, no such thing as the 'Ganassi' recorder as a distinct type; it was simply the ordinary Renaissance recorder played by a good player who understood the instrument.
Baroque recorders.
Several changes in the construction of recorders took place in the 17th century, resulting in the type of instrument generally referred to as "Baroque" recorders, as opposed to the earlier "Renaissance" recorders. These innovations allowed baroque recorders to possess a tone which was regarded as "sweeter" than that of the earlier instruments, at the expense of a reduction in volume, particularly in the lowest notes.
In the 18th century, rather confusingly, the instrument was normally referred to simply as Flute ("Flauto") – the transverse form was separately referred to as Traverso. In the 4th Brandenburg Concerto in G major, J.S. Bach calls for two "flauti d'echo". The musicologist Thurston Dart mistakenly suggested that it was intended for flageolets at a higher pitch, and in a recording under Neville Marriner using Dart's editions it was played an octave higher than usual on sopranino recorders. An argument can be made that the instruments Bach identified as "flauti d'echo" were echo flutes, an example of which survives in Leipzig to this day. It consisted of two recorders in f' connected together by leather flanges: one instrument was voiced to play softly, the other loudly. Vivaldi wrote three concertos for the "flautino" and required the same instrument in his opera orchestra. In modern performance, the "flautino" was initially thought to be the piccolo. It is now generally accepted, however, that the instrument intended was some variant of the sopranino recorder.
Decline of the recorder.
The instrument went into decline after the 18th century, being used for about the last time as an otherworldly sound by Gluck in his opera "Orfeo ed Euridice", first performed in 1762. Many reasons have been put forward for this decline. One possible reason is that the main flute innovators of the time, Grenser, Tromlitz, and others, extended the transverse flute's range and evened out its tonal consistency, making it more appealing than the recorder. Also, the fixed relationship of the windway to the labium limits the range of dynamics and expression of the recorder, when compared with the transverse flute. Another possible reason was that music as an amateur pastime was declining in favour of the professional musician and that composers began writing exclusively for professional ensembles. Other possible reasons include an apparent lack of sufficient professional players; a change in musical tastes; a lack of appreciation of the true nature of the recorder by composers; the high pitch of the instrument; the problems (for makers and players) of utilising the full chromatic range; and a perceived bad reputation of the instrument based on all these factors.
The art of recorder making never completely died, though. Berchtesgaden Fleitl continue to be made to this day by Bernhard Oeggle, whose great-grandfather Georg learned his craft from Paul Walch (c. 1862–1873), the last of three generations of the Walch family of recorder makers. Similarly, the careers of the Schlosser family of woodwind makers from the town of Zwota can be traced over five generations. Their founder was Johan Gabriel Sr who was active in the early 19th century; Rüdiger, who seems to have been the last maker, died in 2005. Heinrich Oskar (1875–1947) made instruments sold by the firm of Moeck in Celle and helped to design their Tuju series of recorders.
Modern revival.
The recorder was revived around the turn of the 20th century by early music enthusiasts, but used almost exclusively for this purpose. It was considered a mainly historical instrument.
The eventual success of the recorder in the modern era is often attributed to Arnold Dolmetsch in the UK and various German scholar/performers. While he was responsible for broadening interest beyond that of the early music specialist in the UK, Dolmetsch was far from being solely responsible for the recorder's revival. On the Continent his efforts were preceded by those of musicians at the Brussels Conservatoire (where Dolmetsch received his training), and by the performances of the Bogenhauser Künstlerkapelle (Bogenhausen Artists' Band) based in Germany. Over the period from 1890–1939 the Bogenhausers played music of all ages, including arrangements of classical and romantic music. Also in Germany, the work of Willibald Gurlitt, Werner Danckerts and Gustav Scheck proceeded quite independently of the Dolmetsches. Thus the revival, far from being the work of one man, was the result of several strands coming and working together.
Among the influential virtuosos who figure in the revival of the recorder as a serious concert instrument in the latter part of the 20th century are Ferdinand Conrad, Kees Otten, Frans Brüggen, Roger Cotte, Hans-Martin Linde, Bernard Krainis, and David Munrow. Brüggen recorded most of the landmarks of the historical repertoire and commissioned a substantial number of new works for the recorder. Munrow's 1975 double album "The Art of the Recorder" remains as an important anthology of recorder music through the ages.
Another notable performer is Michala Petri, who has toured, recorded discs of old and new pieces, and had several works written for her, including concertos by Malcolm Arnold and Richard Harvey.
Carl Dolmetsch, the son of Arnold Dolmetsch, became one of the first virtuoso recorder players in the 1920s; but more importantly he began to commission recorder works from leading composers of his day, especially for performance at the Haslemere festival which his father ran. Initially as a result of this, and later as a result of the development of a Dutch school of recorder playing led by Kees Otten, the recorder was introduced to serious musicians as a virtuoso solo instrument both in Britain and in northern Europe, and consequently modern composers of great stature have written for the recorder, including Paul Hindemith, Luciano Berio, Jürg Baur, Josef Tal, John Tavener, Michael Tippett, Benjamin Britten, Leonard Bernstein, Gordon Jacob, Malcolm Arnold, Steven Stucky and Edmund Rubbra.
The recorder is often used in popular music, including that of groups such as The Beatles; the Rolling Stones (see, for example, "Ruby Tuesday"); Yes, for example, in the song "I've Seen All Good People"; Jefferson Airplane (see Personnel as well as Grace Slick); Led Zeppelin (Stairway to Heaven); Jimi Hendrix; Siouxsie and the Banshees; Judy Dyble of Fairport Convention; Dido (e.g. "Grafton Street"); and Mannheim Steamroller.
Some modern music calls for the recorder to produce unusual noises, rhythms and effects, by such techniques as flutter-tonguing and overblowing to produce multiphonics. David Murphy's 2002 composition "Bavardage" is an example, as is Hans Martin Linde's "Music for a Bird".
Among late 20th-century and early 21st-century recorder ensembles, the trio Sour Cream (led by Frans Brüggen), Flautando Köln, the Flanders Recorder Quartet, Amsterdam Loeki Stardust Quartet and Quartet New Generation have programmed remarkable mixtures of historical and contemporary repertoire. Piers Adams, Maurice Steger or Dorothee Oberlinger are recorder player who have toured and recorded widely, as well as being involved in education.
In the 2012 BBC Young Musician of the Year a recorder player, Charlotte Barbour-Condini, reached the final. This was the first time that a recorder player had got that far in the competition. In the 2014 competition another recorder player, Sophie Westbrooke, also reached the finals.
Makers.
The evolution of the Renaissance recorder into the Baroque instrument is generally attributed to the Hotteterre family, in France. They developed the ideas of a more tapered bore, bringing the finger-holes of the lowermost hand closer together, allowing greater range, and enabling the construction of instruments in several jointed sections. The last innovation allowed more accurate shaping of each section and also offered the player minor tuning adjustments, by slightly pulling out one of the sections to lengthen the instrument.
The French innovations were taken to London by Pierre Bressan, a set of whose instruments survive in the Grosvenor Museum, Chester, as do other examples in various American, European and Japanese museums and private collections. Bressan's contemporary, Thomas Stanesby, was born in Derbyshire but became an instrument maker in London. He and his son (Thomas Stanesby junior) were the other important British-based recorder-makers of the early 18th century.
In continental Europe, the Denner family of Nuremberg were the most celebrated makers of this period.
Many modern recorders are based on the dimensions and construction of surviving instruments produced by Bressan, the Stanesbys or the Denner family. Well-known larger contemporary makers of recorders include Angel (South Korea), Aulos (Japan), Dolmetsch (England), Fehr, Huber, Küng (Switzerland), Moeck (Germany), Mollenhauer (Germany), and Yamaha (Japan). Smaller workshops include names such as Ammann, Blezinger, Boudreau, Adrian Brown, Coomber, Cranmore, Ehlert, Grinter, Marvin, Meyer, Netsch, Prescott, Rohmer, Takeyama, and Von Huene.
Recorder ensembles.
The recorder is a very social instrument. Many recorder players participate in large groups or in one-to-a-part chamber groups, and there is a wide variety of music for such groupings including many modern works. Groups of different sized instruments help to compensate for the limited note range of the individual instruments. Four part arrangements with a soprano, alto, tenor and bass part played on the corresponding recorders are common, although more complex arrangements with multiple parts for each instrument and parts for lower and higher instruments may also be regularly encountered.

</doc>
<doc id="26247" url="http://en.wikipedia.org/wiki?curid=26247" title="Received Pronunciation">
Received Pronunciation

Received Pronunciation (RP) is regarded as the standard accent of Standard English in the United Kingdom, with a relationship to regional accents similar to the relationship in other European languages between their standard varieties and their regional forms. RP is defined in the "Concise Oxford English Dictionary" as "the standard accent of English as spoken in the south of England", although it can be heard from native speakers throughout England and Wales. Peter Trudgill estimated in 1974 that 3% of people in Britain were RP speakers.
Although nothing intrinsic about RP marks it as superior to any other variety, sociolinguistic factors have given RP particular prestige in parts of Britain. It has thus been seen as the accent of those with power, money, and influence, though it has in recent times been perceived negatively as associated with undeserved privilege. Since the 1960s, a greater permissiveness towards allowing regional English varieties has taken hold in education. 
The study of RP is concerned exclusively with pronunciation, whereas "Standard English", "the Queen's English", "Oxford English", or "BBC English" is also concerned with matters such as grammar, vocabulary and style. An individual using RP will typically speak Standard English, although the reverse is not necessarily true (e.g. the standard language may be pronounced with a regional accent, such as a Scottish or Yorkshire accent; but it is very unlikely that someone speaking RP would use it to speak the Scots or the Yorkshire dialect).
History.
The introduction of the term 'Received Pronunciation' is usually credited to Daniel Jones. In the first edition of the "English Pronouncing Dictionary" (1917), he named the accent "Public School Pronunciation", but for the second edition in 1926, he wrote, "In what follows I call it Received Pronunciation (abbreviation RP), for want of a better term." However, the term had actually been used much earlier by Alexander Ellis in 1869 and P. S. Du Ponceau in 1818 (the term used by Henry C. K. Wyld in 1927 was "received standard"). According to "Fowler's Modern English Usage" (1965), the correct term is "'the Received Pronunciation'. The word 'received' conveys its original meaning of 'accepted' or 'approved', as in 'received wisdom'."
RP is often believed to be based on the accents of southern England, but it actually has most in common with the Early Modern English dialects of the East Midlands. This was the most populated and most prosperous area of England during the 14th and 15th centuries. By the end of the 15th century, "Standard English" was established in the City of London. A mixture of London speech with elements from East Midlands, Middlesex, and Essex became what is now known as Received Pronunciation. By the 1970s, an estimated 3% of British people were RP speakers.
Alternative names.
Some linguists have used the term RP, but expressed reservations about its suitability. The Cambridge-published "English Pronouncing Dictionary" (aimed at those learning English as a foreign language) uses the term "BBC Pronunciation" on the basis that the name "Received Pronunciation" is "archaic" and that BBC news-presenters no longer suggest high social class and privilege to their listeners. The name "BBC Pronunciation" has been used by other writers. The phonetician Jack Windsor Lewis frequently criticises the name "Received Pronunciation" on his blog: he has called it "invidious", a "ridiculously archaic, parochial and question-begging term" and argued that American scholars find the term "quite curious". He used the term "General British" (to parallel "General American") in his 1970s publication of "A Concise Pronouncing Dictionary of American and British English" and subsequent publications. Beverley Collins and Inger Mees use the term "Non-Regional Pronunciation" for what is often otherwise called RP, and reserve the term "Received Pronunciation" for the "upper-class speech of the twentieth century". Received Pronunciation has sometimes been called "Oxford English", as it was traditionally the accent of most members of Oxford University. 
The "Handbook of the International Phonetic Association" uses the name "Standard Southern British". Page 4 reads:
Standard Southern British (where 'Standard' should not be taken as implying a value judgment of 'correctness') is the modern equivalent of what has been called 'Received Pronunciation' ('RP'). It is an accent of the south east of England which operates as a prestige norm there and (to varying degrees) in other parts of the British Isles and beyond.
Usage.
Faced with the difficulty of defining RP, many writers have tried to distinguish between different sub-varieties. proposed Conservative, General, and Advanced; Conservative RP refers to a traditional accent associated with older speakers with certain social backgrounds; General RP is often considered neutral regarding age, occupation or lifestyle of the speaker; and Advanced RP refers to speech of a younger generation of speakers. Later editions (e.g. Gimson 2008) use General, Refined and Regional. refers to "mainstream RP" and "U-RP"; he suggests that Gimson's categories of Conservative and Advanced RP referred to the U-RP of the old and young respectively. However, Wells stated, "It is difficult to separate stereotype from reality" with U-RP. Writing on his blog in February 2013, Wells wrote, "the percentage speaking U-RP is vanishingly small" and "If I were redoing it today, I think I’d drop all mention of “U-RP”".
The modern style of RP is an accent often taught to non-native speakers learning British English. Non-RP Britons abroad may modify their pronunciation to something closer to Received Pronunciation to be better understood by people unfamiliar with the diversity of British accents. They may also modify their vocabulary and grammar to be closer to those of Standard English for the same reason. RP is used as the standard for English in most books on general phonology and phonetics, and is represented in the pronunciation schemes of most dictionaries published in the United Kingdom.
In dictionaries.
Most English dictionaries published in Britain (including the "Oxford English Dictionary") now give phonetically transcribed RP pronunciations for all words. Pronunciation dictionaries are a special class of dictionary giving a wide range of possible pronunciations; British pronunciation dictionaries are all based on RP, though not necessarily using that name. Daniel Jones transcribed RP pronunciations of a large number of words and names in his "English Pronouncing Dictionary". This is still being published by Cambridge University Press, and is now edited by Peter Roach, the accent having been renamed "BBC Pronunciation". Two other pronunciation dictionaries are in common use: the "Longman Pronunciation Dictionary", compiled by John C Wells, using the name Received Pronunciation, and the "Oxford Dictionary of Pronunciation for Current English", compiled by Clive Upton. This represents an accent named BR which is based on RP, but is claimed to be representative of a wider group of speakers. An earlier pronunciation dictionary by J. Windsor Lewis gives both British and American pronunciations, using the term General British (GB) for the former and General American (GA) for the latter.
Status.
Traditionally, Received Pronunciation was the "everyday speech in the families of Southern English persons whose men-folk [had] been educated at the great public boarding-schools" and which conveyed no information about that speaker's region of origin before attending the school.
It is the business of educated people to speak so that no-one may be able to tell in what county their childhood was passed.—A. Burrell, "Recitation. A Handbook for Teachers in Public Elementary School", 1891
In the 19th century, some British prime ministers still spoke with some regional features, such as William Ewart Gladstone. From the 1970s onwards, attitudes towards Received Pronunciation have been changing slowly. The BBC's use of Yorkshire-born Wilfred Pickles during the Second World War (to distinguish BBC broadcasts from German propaganda) is an earlier example of the use of non-RP accents, but even then Pickles modified his speech towards RP when reading the news.
Although admired in some circles, RP is disliked in others. It is common in parts of Britain to regard it as a south-eastern English accent rather than a non-regional one and as a symbol of the south-east's political power in Britain. A 2007 survey found that residents of Scotland and Northern Ireland tend to dislike RP. It is shunned by some with left-wing political views, who may be proud of having an accent more typical of the working classes. The British band Chumbawamba recorded a song entitled "R.I.P. RP", which is part of their album "The Boy Bands Have Won".
Phonology.
Consonants.
Nasals and liquids (/m/, /n/, /ŋ/, /r/, /l/) may be syllabic in unstressed syllables. While the IPA symbol [ɹ] is phonetically correct for the consonant in 'row', 'arrow' in many accents of American and British English, most published work on Received Pronunciation represents this phoneme as /r/.
Voiceless plosives (/p/, /t/, /k/, /tʃ/) are aspirated at the beginning of a syllable, unless a completely unstressed vowel follows. (For example, the /p/ is aspirated in "impasse", with secondary stress on "-passe", but not "compass", where "-pass" has no stress.) Aspiration does not occur when /s/ precedes in the same syllable, as in "spot" or "stop". When a sonorant /l/, /r/, /w/, or /j/ follows, this aspiration is indicated by partial devoicing of the sonorant. /r/ is a fricative when devoiced.
Syllable final /p/, /t/, /tʃ/, and /k/ may be either preceded by a glottal stop (glottal reinforcement) or, in the case of /t/, fully replaced by a glottal stop, especially before a syllabic nasal ("bitten" [ˈbɪʔn̩]). The glottal stop may be realised as creaky voice; thus, an alternative phonetic transcription of "attempt" [əˈtʰemʔt] could be [əˈtʰemm̰t].
As in other varieties of English, voiced plosives (/b/, /d/, /ɡ/, /dʒ/) are partly or even fully devoiced at utterance boundaries or adjacent to voiceless consonants. The voicing distinction between voiced and voiceless sounds is reinforced by a number of other differences, with the result that the two of consonants can clearly be distinguished even in the presence of devoicing of voiced sounds:
As a result, some authors prefer to use the terms "fortis" and "lenis" in place of "voiceless" and "voiced". However, the latter are traditional and in more frequent usage.
The voiced dental fricative (/ð/) is more often a weak dental plosive; the sequence /nð/ is often realised as [n̪n̪] (a long dental nasal). /l/ has velarised allophone ([ɫ]) in the syllable rhyme. /h/ becomes voiced ([ɦ]) between voiced sounds.
Vowels.
Examples of short vowels: /ɪ/ in "kit", "mirror" and "rabbit", /ʊ/ in "put", /e/ in "dress" and "merry", /ʌ/ in "strut" and "curry", /æ/ in "trap" and "marry", /ɒ/ in "lot" and orange", /ə/ in ago" and "sofa".
Examples of long vowels: /iː/ in "fleece", /uː/ in "goose", /ɜː/ in "nurse" and "furry", /ɔː/ in "north", "force" and "thought", /ɑː/ in "father", "bath" and "start".
Long and short vowels.
RP's long vowels are slightly diphthongised, especially the high vowels /iː/ and /uː/, which are often narrowly transcribed in phonetic literature as diphthongs [ɪi] and [ʊu].
"Long" and "short" are relative to each other. Because of phonological process affecting vowel length, short vowels in one context can be longer than long vowels in another context. For example, the long vowel /iː/ in 'reach' /riːtʃ/ (which ends with a voiceless consonant) may be shorter than the short vowel /ɪ/ in the word 'ridge' /rɪdʒ/ (which ends with a voiced consonant). Wiik, cited in Gimson, published durations of English vowels with a mean value of 17.2 csec. for short vowels before voiced consonants but a mean value of 16.5 csec for long vowels preceding voiceless consonants.
Conversely, the short vowel /æ/ becomes longer if it is followed by a voiced consonant. Thus, "bat" is pronounced [bæʔt] and "bad" is [bæːd]. In natural speech, the plosives /t/ and /d/ may be unreleased utterance-finally, and voiced consonants partly or completely devoiced (as in [b̥æːd̥]); thus distinction between these words would rest mostly on vowel length and the presence or absence of glottal reinforcement.
In addition to such length distinctions, unstressed vowels are both shorter and more centralised than stressed ones. In unstressed syllables occurring before vowels and in final position, contrasts between long and short high vowels are neutralised and short [i] and [u] occur (e.g. "happy" [ˈhæpi], "throughout" [θɹuˈaʊʔt]). The neutralisation is common throughout many English dialects, though the phonetic realisation of e.g. [i] rather than [ɪ] (a phenomenon called "happy"-tensing) is not as universal.
Unstressed vowels vary in quality:
Diphthongs and triphthongs.
The centring diphthongs are gradually being eliminated in RP. The vowel /ɔə/ (as in "door", "boar") had largely merged with /ɔː/ by the Second World War, and the vowel /ʊə/ (as in "poor", "tour") has more recently merged with /ɔː/ as well among most speakers, although the sound /ʊə/ is still found in conservative speakers (and this is still the only pronunciation given in the OED). See poor–pour merger. The remaining two centring glides /ɪə/ /eə/ are increasingly pronounced as long monophthongs [ɪː] [ɛː], although without merging with any existing vowels.
The diphthong /əʊ/ is pronounced by some RP speakers in a noticeably different way when it occurs before /l/, if that consonant is syllable-final and not followed by a vowel (the context in which /l/ is pronounced as a "dark l"). The realization of /əʊ/ in this case begins with a more back, rounded and sometimes more open vowel quality; it may be transcribed as [ɔʊ] or [ɒʊ]. It is likely that the backness of the diphthong onset is the result of allophonic variation caused by the raising of the back of the tongue for the /l/. If the speaker has "l-vocalization" the /l/ is realized as a back rounded vowel, which again is likely to cause backing and rounding in a preceding vowel as coarticulation effects. This phenomenon has been discussed in several blogs by John C Wells. It is possible, according to Wells, that a speaker with the [ɒʊ] or [ɔʊ] pronunciation may pronounce the words 'holy' and 'wholly' with different realizations of /əʊ/ (the former having [əʊ] and the latter [ɒʊ] or [ɔʊ]), thus creating a phonological distinction (the wholly–holy split). In the recording included in this article the phrase 'fold his cloak' contains examples of the /əʊ/ diphthong in the two different contexts. The onset of the pre-/l/ diphthong in 'fold' is slightly more back and rounded than that in 'cloak', though the allophonic transcription does not at present indicate this.
RP also possesses the triphthongs /aɪə/ as in "tire", /aʊə/ as in "tower", /əʊə/ as in "lower", /eɪə/ as in "layer" and /ɔɪə/ as in "loyal". There are different possible realisations of these items: in slow, careful speech they may be pronounced as a two-syllable triphthong with three distinct vowel qualities in succession, or as a monosyllabic triphthong. In more casual speech the middle vowel may be considerably reduced, by a process known as smoothing, and in an extreme form of this process the triphthong may even be reduced to a single vowel, though this is rare, and almost never found in the case of /ɔɪə/. In such a case the difference between /aʊə/, /aɪə/, and /ɑː/ in "tower", "tire", and "tar" may be neutralised with all three units realised as [ɑː] or [äː]. This type of smoothing is known as the "tower"–"tire", "tower"–"tar" and "tire"–"tar" mergers.
BATH vowel.
There are differing opinions as regards whether /æ/ in the lexical set can be considered RP. The pronunciations with /ɑː/ are invariably accepted as RP. The English Pronouncing Dictionary does not admit /æ/ in words and the Longman Pronunciation Dictionary lists them with a § marker of non-RP status. John Wells wrote in a blog entry on 16 March 2012 that, when growing up in the north of England, he used /ɑː/ in "bath" and "glass", and considers this the only acceptable phoneme in RP. Others have argued that /æ/ is too categorical in the north of England to be excluded. Clive Upton believes that /æ/ in these words must be considered within RP and has called the opposing view "south-centric". Upton's "Oxford Dictionary of Pronunciation for Current English" gives both variants for words. A. F. Gupta's survey of mostly middle-class students found that /æ/ was used by almost everyone who was from clearly north of the isogloss for words. She wrote, "There is no justification for the claims by Wells and Mugglestone that this is a sociolinguistic variable in the north, though it is a sociolinguistic variable on the areas on the border [the isogloss between north and south]". In a study of speech in West Yorkshire, K. M. Petyt wrote that "the amount of /ɑː/ usage is too low to correlate meaningfully with the usual factors", having found only two speakers (both having attended boarding schools in the south) who consistently used /ɑː/.
Jack Windsor Lewis has noted that the Oxford Dictionary's position has changed several times on whether to include short /æ/ within its prescribed pronunciation. The "BBC Pronouncing Dictionary of British Names" uses only /ɑː/, but its author, Graham Pointon, has stated on his blog that he finds both variants to be acceptable in place names.
Some research has concluded that many people in the North of England have a dislike of the /ɑː/ vowel in words. A. F. Gupta wrote, "Many of the northerners were noticeably hostile to /ɡrɑːs/, describing it as 'comical', 'snobbish', 'pompous' or even 'for morons'." On the subject, K. M. Petyt wrote that several respondents "positively said that they did not prefer the long-vowel form or that they really detested it or even that it was incorrect". Mark Newbrook has assigned this phenomenon the name "conscious rejection", and has cited the BATH vowel as "the main instance of conscious rejection of RP" in his research in West Wirral.
Alternative notation.
Not all reference sources use the same system of transcription. In particular:
Most of these variants are used in the transcription devised by Clive Upton for the "Shorter Oxford English Dictionary" (1993) and now used in many other Oxford University Press dictionaries.
The linguist Geoff Lindsey has argued that the system of transcription for RP has become outdated and has proposed a new system as a replacement.
Historical variation.
Like all accents, RP has changed with time. For example, sound recordings and films from the first half of the 20th century demonstrate that it was usual for speakers of RP to pronounce the /æ/ sound, as in "land", with a vowel close to [ɛ], so that "land" would sound similar to a present-day pronunciation of "lend". RP is sometimes known as the Queen's English, but recordings show that even Queen Elizabeth II has changed her pronunciation over the past 50 years, no longer using an [ɛ]-like vowel in words like "land". 
Some changes in RP during the 20th century include:
The change in RP may be observed in the home of "BBC English". The BBC accent of the 1950s was distinctly different from today's: a news report from the 1950s is recognisable as such, and a mock-1950s BBC voice is used for comic effect in programmes wishing to satirise 1950s social attitudes such as the Harry Enfield Show and its "Mr. Cholmondley-Warner" sketches.
More recently, in speakers born between 1981 and 1993, the vowel /ɒ/ shifted up approaching [ɔ] in quality. The vowels /ʊ/ and /uː/ have undergone fronting and reduction in the amount of lip-rounding (phonetically, this can be transcribed [ʊ̜̈] and [ʉ̜ː], respectively), while /æ/ has become more open [a].
Spoken specimen.
The "Journal of the International Phonetic Association" regularly publishes "Illustrations of the IPA" which present an outline of the phonetics of a particular language or accent. It is usual to base the description on a recording of the traditional story of the North Wind and the Sun. There is an IPA illustration of British English (Received Pronunciation). The audio recording on which the transcriptions are based may be heard here: The speaker (female) is described as having been born in 1953, and educated at Oxford University. To accompany the recording there are three transcriptions: orthographic, phonemic and allophonic. 
Notable speakers.
John C. Wells, a notable British phonetician, has identified the following people as RP speakers:
Bibliography.
</dl>
External links.
Sources of regular comment on RP
Audio files

</doc>
<doc id="26252" url="http://en.wikipedia.org/wiki?curid=26252" title="Ryan Lackey">
Ryan Lackey

Ryan Donald Lackey (born March 17, 1979) is an entrepreneur and computer security professional. He was a co-founder of HavenCo, the world's first data haven. He also speaks at numerous conferences and trade shows, including DEF CON, RSA Data Security Conference, on various topics in the computer security field, and has appeared on the cover of Wired Magazine, in numerous television, radio, and print articles on HavenCo and Sealand. Lackey operated BlueIraq, a VSAT communications and IT company serving the DoD and domestic markets in Iraq and Afghanistan during the US conflicts.
Lackey was born in West Chester, Pennsylvania and has also lived throughout the US and Europe, Anguilla, Sealand, Dubai, and Iraq. As a teenager, he was briefly involved with the Globewide Network Academy. Lackey attended MIT and majored in Course 18 (mathematics), one of the most difficult courses at the institute. While a student at MIT (he later dropped out due to financial constraints) Lackey became interested in electronic cash and distributed systems, originally for massively multiplayer online gaming. This interest led to attending several conferences (Financial Cryptography 98, various MIT presentations), participating on mailing lists such as "cypherpunks" and "dbs", and eventually implementing patented Chaumian digital cash in an underground library, HINDE, with Ian Goldberg, named after Hinde ten Berge, a Dutch cypherpunk also present at FC98. In part, he contributed to the cypherpunks movement as one of the longest Anonymous remailer operators.
In 1999 Lackey lived in the San Francisco Bay Area after a period in Anguilla before moving to the unrecognized state of Sealand off the coast of the United Kingdom and establishing HavenCo. In December 2002, he left HavenCo following a dispute with other company directors and the Sealand "Royal Family".
Eventually, BlueIraq's business model became economically unfeasible due to an escalation in anti-western violence primarily in the form of Improvised Explosive Devices and troop draw downs. BlueIraq sought venture capital to transform itself into a large general consumer cellular telephone company. However, the 2008 financial crisis and the instability of Iraq and Afghanistan made fund raising impossible.
Lackey returned to the US and located in San Francisco where he worked for a number of start-up companies before applying to Y Combinator. He was accepted into Y Combinator's Summer 2011 round. Lackey founded CryptoSeal, a VPN as a service start-up with a small group of people well known in the computer security community, and secured funding from Ron Conway and a well known venture capital fund. In June 2014, CryptoSeal was acquired by CloudFlare.
External links.
<br>

</doc>
<doc id="26253" url="http://en.wikipedia.org/wiki?curid=26253" title="Revised Julian calendar">
Revised Julian calendar

The Revised Julian calendar, also known as the Milanković calendar, or, less formally, New calendar, is a calendar, developed and proposed by the Serbian scientist Milutin Milanković in 1923, which effectively discontinued the 340 years of divergence between the naming of dates sanctioned by those Eastern Orthodox churches adopting it and the Gregorian calendar that has come to predominate worldwide. This calendar was intended to replace the ecclesiastical calendar based on the Julian calendar hitherto in use by all of the Eastern Orthodox Church. The Revised Julian calendar temporarily aligned its dates with the Gregorian calendar proclaimed in 1582 by Pope Gregory XIII for adoption by the Christian world.
History.
In the nineteenth century German astronomer Johann Heinrich von Maedler measured the mean tropical year to a high degree of accuracy. Finding a significant difference to the mean length of the Gregorian year, he urged the Russian government, then using the Julian calendar, to introduce a new calendar under which twelve days would be dropped, 1900 would not be a leap year, and then leap years would be omitted every 128 years.
At the turn of the twentieth century there was support in Russia for a modified version of this proposal. Fourteen days would be omitted from the calendar and thereafter years which divided exactly by 128 would not be leap years. This moves the astronomical vernal equinox forward to 22 March, slightly earlier than the Gregorian date of the mean vernal equinox, 1.48 AM GMT on 23 March in 1983. The mean vernal equinox moves forward to 25 March, where it stays. This is the date on which the equinox was marked when the calendar was introduced in 45 BC.
In 1923 the Serbian astronomer Milutin Milankovic moved this proposal forward in a slightly different form. His proposal was to save up the 128 - year adjustments and implement them at the end of the century. Under his scheme, every year which divided exactly by four would be leap as before, but exceptionally years which divided exactly by 100 would be common, unless they gave remainder 200 or 600 when divided by 900, in which case they would still be leap.
Implementation.
The new calendar was proposed for adoption by the Orthodox churches at a synod in Constantinople in May 1923 and subsequently it was adopted by several of the autocephalous Orthodox churches. The synod was chaired by controversial Patriarch Meletius IV of Constantinople, and called Pan-Orthodox by its supporters. But only the Patriarch of Constantinople and the Serbian Patriarch were represented. There were no representatives of the other members of the original Orthodox Pentarchy (the Patriarchates of Jerusalem, Antioch, and Alexandria) or from the largest Orthodox Church, the Russian Orthodox Church.
This synod synchronized the new calendar with the Gregorian calendar by specifying that the next 1 October of the Julian calendar would be 14 October in the new calendar, thus dropping thirteen days. It then adopted the leap rule of Milanković, an astronomical delegate to the synod representing the Kingdom of Serbs, Croats and Slovenes. Milanković selected this rule because its mean year was within two seconds of the then current length of the "mean" tropical year. The present "vernal equinox" year, however, is about 12 seconds longer, in terms of mean solar days.
The synod also proposed the adoption of an astronomical rule for Easter: Easter was to be the Sunday after the midnight-to-midnight day at the meridian of the Church of the Holy Sepulchre in Jerusalem (35°13'47.2"E or UT+2h20m55s for the small dome) during which the first full moon after the vernal equinox occurs. Although the instant of the full moon must occur after the instant of the vernal equinox, it may occur on the same day. If the full moon occurs on a Sunday, Easter is the following Sunday. However, all Eastern Orthodox churches rejected this rule and continue to use the Julian calendar to determine the date of Easter (except for the Finnish Orthodox Church and the Estonian Orthodox Church which now use the Gregorian Easter).
Arithmetic.
The following are Gregorian minus Revised Julian date differences, calculated for the beginning of March in each century year, which is where differences arise or disappear, until 10000 AD. These are exact arithmetic calculations, not depending on any astronomy. A negative difference means that the proleptic Revised Julian calendar was behind the proleptic Gregorian calendar. The Revised Julian calendar is the same as the Gregorian calendar from 1 March 1600 to 28 February 2800. A positive difference means that the Revised Julian calendar will be ahead of the Gregorian calendar, which will first occur on 1 March 2800:
In 900 Julian years there are 900⁄4 = 225 leap days. The Revised Julian leap rule omits seven of nine century leap years, leaving 225−7 = 218 leap days per 900-year cycle. Thus the calendar mean year is 365+218⁄900 days, but this is actually a double-cycle that reduces to 365+109⁄450 = 365.242 days, or exactly 365 days 5 hours 48 minutes 48 seconds, which is exactly 24 seconds shorter than the Gregorian mean year of 365.2425 days, so in the long term on "average" the Revised Julian calendar pulls ahead of the Gregorian calendar by one day in 3600 years.
The number of days per Revised Julian cycle = 900 × 365 + 218 = 328,718 days. Taking mod 7 leaves a remainder of 5, so like the Julian calendar, but unlike the Gregorian calendar, the Revised Julian calendar cycle does not contain a whole number of weeks. Therefore, a full repetition of the Revised Julian leap cycle with respect to the seven-day weekly cycle is seven times the cycle length = 7 × 900 = 6300 years.
Epoch.
The epoch of the Julian calendar was on the Saturday before the Monday that was the epoch of the Gregorian calendar. In other words, Gregorian 1 January 1 AD = Julian 3 January 1 AD. The Revised Julian reform not only changed the leap rule but also made the epoch the same as that of the Gregorian calendar. This seems to have been carried out implicitly, and even scientific articles make no mention of it. Nevertheless, it is impossible to implement calendrical calculations and calendar date conversion software without appreciating this detail and taking the 2-day shift into account. If the original Julian calendar epoch is mistakenly used in such calculations then there is no way to reproduce the currently accepted dating of the Revised Julian calendar, which yields no difference between Gregorian and Revised Julian dates in the 21st century.
March equinox.
The following is a scatter plot of actual astronomical northward equinox moments as numerically integrated by SOLEX 11 using DE421 mode with extended (80-bit) floating point precision, high integration order (18th order), and forced solar mass loss ("forced" means taken into account at all times). SOLEX can automatically search for northern hemisphere spring equinox moments by finding when the solar declination crosses the celestial equator northward, and then it outputs that data as the Terrestrial Time day and fraction of day relative to 1 January 2000 at noon (J2000.0 epoch). The progressive tidal slowing of the Earth rotation rate was accounted for by subtracting ΔT as calculated by the Espenak-Meeus polynomial set recommended at the NASA Eclipses web site to obtain the J2000.0-relative Universal Time moments, which were then properly converted to Revised Julian dates and Jerusalem local apparent time, taking local apparent midnight as the beginning of each calendar day. The year range of the chart was limited to dates before the year 4400 AD—by then ΔT is expected to accumulate to about six hours, with an uncertainty of less than 21/2 hours.
The chart shows that the long-term equinox drift of the Revised Julian calendar is quite satisfactory, at least until 4400 AD. The medium-term wobble spans about two days because, like the Gregorian calendar, the leap years of the Revised Julian calendar are not smoothly spread: they occur mostly at intervals of four years but there are occasional eight-year gaps (at 7 out of 9 century years). Evidently each of the authorities responsible for the Gregorian and Revised Julian calendars, respectively, accepted a modest amount of medium-term equinox wobble for the sake of traditionally perceived leap rule mental arithmetic simplicity. Therefore the wobble is essentially a curiosity that is of no practical or ritual concern.
Adoption.
The new calendar has been adopted by the Orthodox churches of Constantinople, Alexandria, Antioch, Greece, Cyprus, Romania, Poland, and Bulgaria (the last in 1963), called the New calendarists. It has not been adopted by the Orthodox churches of Jerusalem, Russia, Serbia (including the uncanonical Macedonian Orthodox Church), Georgia, Mount Athos and the Greek Old Calendarists. Although Milanković stated that the Russian Orthodox Church adopted the new calendar in 1923, the present church continues to use the Julian calendar for both its fixed festivals and for Easter. A solution to this conundrum is to hypothesize that it was accepted only by the short-lived schismatic Renovationist Church, which had seized church buildings with the support of the Soviet government while Patriarch Tikhon was under house arrest. After his release, on 15 July 1923, he declared that all Renovationist decrees were without grace, presumably including its acceptance of the new calendar.
Defense.
The basic justification for the new calendar is the known errors of the Julian calendar, which will in the course of time lead to a situation in which those following the Julian calendar will be reckoning the month of December (and the feast of Christ's Nativity) during the heat of summer, August and its feasts during the deep cold of winter, Pascha during the autumn season, and the November feasts in the springtime. This would conflict with the Church's historic practice of celebrating Christ's birth on 25 December, a date chosen for a number of reasons. One of the reasons mentioned by Bennet is the time of the winter solstice, when the days begin to lengthen again as the physical sun makes its reappearance, along with the fact that Christ has traditionally been recognized by Christians as the metaphorical and spiritual sun who fulfills Malachi's prophetic words: "the sun of righteousness will shine with healing in its wings" (Malachi 4:2). The identification, based on this prophecy, of Jesus Christ as the "sun of righteousness" is found many times in writings of the early Church fathers and follows from many New Testament references linking Jesus with imagery of sun and light.
The defenders of the new calendar do not regard the Julian calendar as having any particular divine sanction (for more on this, see below); rather, they view the Julian calendar as a device of human technology, and thus subject to improvement or replacement just as many other devices of technology that were in use at the dawn of the Church have been replaced with newer forms of technology.
Supporters of the new calendar can also point to certain pastoral problems that are resolved by its adoption.
(1) Parishes observing the Julian calendar are faced with the problem that parishioners are supposed to continue fasting throughout western Christmas and New Year, seasons when their families and friends are likely to be feasting and celebrating New Year's, often with parties, use of liquor, etc. This situation presents obvious temptations, which are eliminated when the new calendar is adopted.
(2) Another pastoral problem is the tendency of some local American media to focus attention each year on the 7 January (N.S.) / 25 December (O.S.) celebration of Christmas, even in localities where most Orthodox parishes are following the new calendar. So too, in all likelihood, do certain non-Orthodox churches profit from the Orthodox remaining Old Style, since the 7 January observance of Christmas among the Orthodox tends to focus attention on ethnic identifications of the feast, rather than on its Christian, dogmatic significance; which, in turn, tends to foster the impression in the public mind that for the Orthodox, the feast of Christ's Nativity is centered on the observance of the Julian date of that feast, rather than on the commemoration of Christ's birth. Such a focus appears to the defenders of the Revised Julian calendar and to many non-Orthodox as well, as a practice that is charming and quaint, but also anachronistic, unscientific and hence ultimately unreasonable and even cultish.
(3) Some Orthodox themselves may unwittingly reinforce this impression by ignorance of their own faith and by a consequential exclusive, or excessive, focus on the calendar issue: it has been observed, anecdotally, that some Russians cannot cite "any" difference in belief or practice between their faith and the faith of western Christians, "except" for the 13-day calendar difference.
Against the new calendar, the argument is made that inasmuch as the use of the Julian calendar was implicit in the decision of the First Ecumenical Council at Nicaea (325), no authority less than an Ecumenical Council may change this decision. However, the fact is that that Council made no decision or decree at all concerning the Julian calendar. Its silence constituted an implicit acceptance not of the Julian calendar, but of the civil calendar, which happened to be, at that time, the Julian calendar (the explicit decision of Nicaea being concerned, rather, with the date of Easter). By virtue of this, defenders of the new calendar argue that no decision by an Ecumenical Council was or is necessary today in order to "revise" (not abandon) the Julian calendar; and further, that by making the revision, the Church stays with the spirit of Nicaea I by keeping with the civil calendar in all its essentials—while conversely, failure to keep with the civil calendar could be seen as a departure from the spirit of Nicaea I in this respect. Lastly, it is argued that since the adoption of the new calendar evidently involves no change in or departure from the theological or the ethical teachings of Orthodox Christianity, but rather amounts to a merely disciplinary or administrative change—a clock correction of sorts—the authority to enact that change falls within the competency of contemporary, local episcopal authority. Implicit acceptance of this line of reasoning, or something very close to it, underlies the decision to adopt the new calendar by those Orthodox churches which have done so.
It follows that, in general, the defenders of the new calendar hold the view that in localities where the Church's episcopal authority has elected to adopt the new calendar, but where some have broken communion with those implementing this change, it is those who have broken communion who have in fact introduced the disunity, rather than the new calendar itself or those who have adopted it — although most would agree that attempts at various times to mandate the use of the new calendar through compulsion, have magnified the disunity.
To the objection that the new calendar has created problems by adjusting only the fixed calendar, while leaving all of the commemorations in the moveable cycle on the original Julian calendar, the obvious answer, of course, is that the 1923 Synod, which adopted the new calendar, did in fact change the moveable calendar as well, and that calendar problems introduced as a result of the adoption of the (fixed) new calendar alone, would not have existed had the corrections to the moveable calendar also been implemented.
According to the defenders of the new calendar, the argument that the 25 December (N.S.) observance of Christmas is a purely "secular" observance and is therefore an unsuitable time for Orthodox Christians to celebrate Christ's Nativity, is plainly inaccurate, since the 25 December observances of Christ's birth among western Christians (and today, among many Orthodox Christians) obviously occur overwhelmingly in places of worship and involve hymns, prayers, scripture readings, religious dramas, liturgical concerts, and the like. Defenders of the new calendar further note that, to the extent that 25 December is a secular observance in the western world, 7 January (i.e. 25 December O.S.) appears to be becoming one as well, in Orthodox countries that continue to follow the old calendar. In Russia, for example, 7 January is no longer a spiritual holiday for Orthodox Christians alone, but has now become a national (hence secular) holiday for all Russians, including non-Orthodox Christians, people of other religions, and nonbelievers. Where this will lead in the end remains to be seen.
Among other arguments made by the defenders of the new calendar for their view, are those made on the basis of "truth" (notwithstanding that the detractors of that calendar make the claim that the Old Style date, 7 January / 25 December, is the "true" celebration of Christ's Nativity). Arguments from truth can take two forms: (1) If a calendar is a system for reckoning time based on the motions of astronomical bodies—specifically the movements of Sun and Moon, in the case of the church calendar—and if precision or accuracy is understood as one aspect of truth, then a calendar that is more accurate and precise with respect to the motions of those bodies must be regarded as truer than one which is less precise. In this regard, some of those who champion the old calendar as "truth" (rather than for pastoral reasons, as seems to be the case with the national churches that adhere to it) may appear, to those following the new calendar, as the defenders of a fiction. (2) Some defenders of the new calendar argue that the celebration, in any way or form, of two feasts of Christ's Nativity within the same liturgical year is not possible, since according to the faith there is only one celebration of that feast in a given year. On this basis, they argue that those who prefer to observe a "secular" feast of the Nativity on 25 December and a "religious" one on 7 January, err in respect of the truth that there is but one feast of the Nativity each year.
Criticism.
While the new calendar has been adopted by many of the smaller national churches, a majority of Orthodox Christians continue to adhere to the traditional Julian calendar, and there has been much acrimony between the two parties over the decades since the change, leading sometimes even to violence, especially in Greece.
Critics see the change in calendar as an unwarranted innovation, influenced by Western society. They say that no sound theological reason has been given for changing the calendar, that the only reasons advanced are social. The proposal for change was introduced by Meletios Metaxakis, a patriarch whose canonical status has been disputed and who is alleged to have been a Freemason, which is forbidden by the Orthodox Church.
The argument is also made that since the use of the Julian calendar was implicit in the decision of the First Ecumenical Council at Nicaea (325) which standardized the calculation of the date of Pascha (Easter), no authority less than an Ecumenical Council may change it. It is further argued that the adoption of the new calendar in some countries and not in others has broken the liturgical unity of the Eastern Orthodox churches, undoing the decision made by the council of bishops at Nicaea to decree that all local churches celebrate Easter on the same day. The emperor Constantine, writing to the bishops absent from the Council to notify them of the decision, argued, "Think, then, how unseemly it is, that on the same day some should be fasting whilst others are seated at a banquet".
Liturgical objections to the new calendar stem from the fact that it adjusts only those liturgical celebrations that occur on fixed calendar dates, leaving all of the commemorations on the moveable cycle on the original Julian calendar. This upsets the harmony and balance of the liturgical year. (This would not have been a problem if the recommendations of the 1923 synod to use an astronomical rule to reckon the date of Easter, as outlined above, had not been rejected.) This disruption is most noticeable during Great Lent. Certain feast days are designed to fall during Lent, such as the feast of the Forty Martyrs of Sebaste. The Feast of the Annunciation is also intended to fall either before Pascha or during Bright Week. Sometimes, Annunciation will fall on the day of Pascha itself, a very special concurrence known as "Kyrio-Pascha", with special liturgical practices appointed for such an occurrence. However, under the new calendar, "Kyrio-Pascha" becomes an impossibility. The Apostles' Fast displays the most difficult aspect of the new calendar. The fast begins on the moveable cycle and ends on the fixed date of 29 June; since the new calendar is 13 days ahead of the traditional Julian calendar, the Apostles' Fast is 13 days shorter for those who follow the new calendar, and some years it is completely abrogated. Furthermore, critics of the new calendar point out the advantage to celebrating Nativity separately from the secular observances of Christmas and New Year, which are associated with partying and alcohol consumption.
Critics also point out that proponents of the new calendar tend to use worldly rather than spiritual justification for changing the calendar: wanting to "party with everyone else" at Christmas; concern that the gradual shift in the Julian calendar will somehow negatively affect the celebration of feasts that are linked to the seasons of the year. However, opponents counter that the seasons are reversed in the southern hemisphere, where the liturgical celebrations are no less valid. The validity of this argument is questionable, since the feasts of the Orthodox Church were not changed no matter where they were celebrated, and Orthodox services were held in the southern hemisphere with little issue centuries before the introduction of the new calendar.
Proponents also argue that the new calendar is somehow more "scientific", but opponents argue that science is not the primary concern of the Church; rather, the Church is concerned with other-worldliness, with being "in the world, but not of it", fixing the attention of the faithful on eternity. Scientifically speaking, neither the Gregorian calendar nor the new calendar is absolutely precise. This is because the solar year cannot be evenly divided into 24-hour segments. So any public calendar is imprecise; it is simply an agreed-upon designation of days.
From a spiritual perspective, Old Calendarists also point to a number of miraculous occurrences which occur on the old calendar exclusively, such as the "descent of the cloud on the mount" on the feast of the Transfiguration. After the calendar change was instituted, the followers of the old calendar in Greece apparently witnessed the appearance of a cross in the sky, visible to thousands on the feast of the Exaltation of the Holy Cross, 1925, of which eyewitness accounts were recorded.
For such special events, if the original Julian date and year is known then the option always exists to calculate what was the proleptic Revised Julian date of that event and then observe its anniversary on that day, if that could be socially and ritually accepted.
Revised Julian calendrical calculations.
The calendrical arithmetic discussed here is adapted from Gregorian and Julian calendar arithmetic published by Dershowitz and Reingold, although those authors explicitly ignored the Revised Julian calendar. Their book, which will be referred to hereinafter as "CC3", should be consulted for methods to handle BC dates and the traditional omission of a year zero, both of which will be ignored here. They define the MOD operator as x MOD y = x − y × floor(x / y), because that expression is valid for negative and floating point operands, returning the remainder from dividing x by y while discarding the quotient. Expressions like floor(x / y) return the quotient from dividing x by y while discarding the remainder.
Leap rule.
"isLeapYear" = ("year" MOD 4 = 0)
IF "isLeapYear" THEN
END IF
Fixed days.
Calendrical calculations are made consistent and straightforward for arithmetic operations if dates are first converted to an ordinal number of days relative to an agreed-upon epoch, in this case the Revised Julian epoch, which was the same as the Gregorian epoch. To find the difference between any two Revised Julian dates, convert both to ordinal day counts and simply subtract. To find a past or future date, convert a given date to an ordinal day count, subtract or add the desired number of days, then convert the result to a Revised Julian date.
The arithmetic given here will not "crash" if an invalid date is given. To verify that a given date is a valid Revised Julian date, convert it to an ordinal day count and then back to a Revised Julian date—if the final date differs from the given date then the given date is invalid. This method should also be used to validate any implementation of calendrical arithmetic, by iteratively checking thousands of random and sequential dates for such errors.
To convert a Revised Julian date to any other calendar, first convert it to an ordinal day count, and then all that is needed is a function to convert the ordinal days count to that calendar.
To convert a date from any other calendar to a Revised Julian date, first convert that calendar date to an ordinal day count, then convert ordinal days to the Revised Julian date.
The following constant defined midnight at the start of Revised Julian date Monday, 1 January 1 AD as the beginning of the first ordinal day. This moment was Julian day number 1721425.5.
CC3 outlines functions for Gregorian and Julian calendar conversions, as well as many other calendars, always calculating in terms of the ordinal day number, which they call the "fixed date" or "rata die" (RD), assigning the number 1 to the Gregorian calendar epoch. The arithmetic herein, by using the same ordinal day numbering epoch, is fully compatible with all CC3 functions for calendrical calculations and date inter-conversions.
One can assign a different integer to the Revised Julian epoch, for the purpose of numbering ordinal days relative to some other epoch, but if you do so then one must take the epoch difference into account when using any CC3 calendar functions and when converting an ordinal day number to a weekday number.
Optionally the ordinal day number can include a fractional component to represent the time as the elapsed fraction of a day. The ordinal day number of the J2000 moment (1 January 2000 noon) was 730120.5.
Revised Julian to fixed days.
Convert a "year", "month", and "day" to the corresponding fixed day number:
If "month" is after February then subtract 1 day for a leap year or subtract 2 days for a common year:
Finally subtract a day for each prior century year (most of which are non-leap) and then add back in the number of prior century leap years:
Fixed days to Revised Julian.
Convert an ordinal day number to the corresponding Revised Julian "year", "month", and "day", starting by removing any fractional time-of-day portion:
Finally, calculate the day number within the month by subtracting the Fixed days count for the start of the month from the originally given Fixed days count, and then add one day:
Fixed days to weekday number.
Convert the ordinal number of days since the Revised Julian epoch to a weekday number (Sunday=1 through Saturday = 7):
Don't be tempted to omit subtracting the "RJepoch" just because it is offset by adding +1. As written, this expression is robust even if you assign a value other than one to the epoch.

</doc>
<doc id="26254" url="http://en.wikipedia.org/wiki?curid=26254" title="Reform of the date of Easter">
Reform of the date of Easter

Reform of the date of Easter has been proposed several times because the current system for determining the date of Easter is seen as presenting two significant problems:
Fixed date.
It has been proposed that the first problem could be resolved by making Easter occur on a fixed date every year, or alternatively on a Sunday within a fixed range of seven dates. While tying it to one fixed date would serve to underline the belief that Easter commemorates an actual historical event, without an accompanying calendar reform that changes the pattern of the days of the week (itself a subject of religious controversy) it would also break the tradition of Easter always being on a Sunday, established since the 2nd century AD and by now deeply embedded in the liturgical practice and theological understanding of almost all Christian denominations. 
The two most widespread proposals for fixing the date of Easter would set it on either the second Sunday in April (8 to 14), or the Sunday after the second Saturday in April (9 to 15). In both schemes, account has been taken of the fact that—in spite of the many difficulties in establishing the dates of the historical events involved—many scholars attribute a high degree of probability to Friday April 7, 30, as the date of the crucifixion of Jesus, which would make April 9 the date of the Resurrection. Another date which is supported by many scholars is April 3, 33, making April 5 the date of the Resurrection. Many churches, including the Roman Catholic Church, have stated that they have no objection in principle to fixing the date of Easter in this way, but no serious discussions have yet taken place on implementing such a change.
In the late 1920s and 1930s, this idea gained some momentum (along with other calendar reform proposals, such as the World Calendar), and in 1928 a law was passed in the United Kingdom authorising an Order in Council which would fix the date of Easter in that country as the first Sunday after the second Saturday in April. However, this was never implemented. In 1977, some Eastern Orthodox representatives objected to separating the date of Easter from lunar phases.
Unified date.
Proposals to resolve the second problem have made greater progress, but they are yet to be adopted.
1923 attempt.
An astronomical rule for Easter was proposed by the 1923 synod that also proposed the Revised Julian calendar: Easter was to be the Sunday after the midnight-to-midnight day at the meridian of the Church of the Holy Sepulchre in Jerusalem (35°13'47.2"E or UT+2h20m55s for the small dome) during which the first full moon after the vernal equinox occurs. Although the instant of the full moon must occur after the instant of the vernal equinox, it may occur on the same day. If the full moon occurs on a Sunday, Easter is the following Sunday. This proposed astronomical rule was rejected by all Orthodox churches and was never considered by any Western church.
1997 attempt.
The World Council of Churches (WCC) proposed a reform of the method of determining the date of Easter at a summit in Aleppo, Syria, in 1997: Easter would be defined as the first Sunday following the first astronomical full moon following the astronomical vernal equinox, as determined from the meridian of Jerusalem. The reform would have been implemented starting in 2001, since in that year the Eastern and Western dates of Easter would coincide.
This reform has not been implemented. It would have relied mainly on the co-operation of the Eastern Orthodox Church, since the date of Easter would change for them immediately; whereas for the Western churches, the new system would not differ from that currently in use until 2019. However, Eastern Orthodox support was not forthcoming, and the reform failed. The much greater impact that this reform would have had on the Eastern churches in comparison with those of the West led some Orthodox to suspect that the WCC's decision was an attempt by the West to impose its viewpoint unilaterally on the rest of the world under the guise of ecumenism.
2008–2009 attempt.
In 2008 and 2009, there was a new attempt to reach a consensus on a unified date on the part of Catholic, Orthodox and Protestant leaders. This effort largely relies on earlier work carried out during the 1997 Aleppo conference. It was organized by academics working at the Institute of Ecumenical Studies of Lviv University.
Part of this attempt was reportedly influenced by ecumenical efforts in Syria and Lebanon, where the Greek-Melkite Church has played an important role in improving ties with the Orthodox. There is also a series of apparition phenomena known as Our Lady of Soufanieh that has urged for a common date of Easter.

</doc>
<doc id="26255" url="http://en.wikipedia.org/wiki?curid=26255" title="Robert Lowth">
Robert Lowth

Robert Lowth FRS (; 27 November 1710 – 3 November 1787) was a Bishop of the Church of England, Oxford Professor of Poetry and the author of one of the most influential textbooks of English grammar.
Life.
Lowth was born in Hampshire, Great Britain, the son of Dr William Lowth. He was educated at Winchester College and became a scholar of New College, Oxford in 1729. Lowth obtained his BA in 1733 and his Master of Arts degree in 1737. In 1735, while still at Oxford, Lowth took orders in the Anglican Church and was appointed vicar of Ovington, Hampshire, a position he retained until 1741, when he was appointed Oxford Professor of Poetry.
Bishop Lowth made a translation of the Bible. EJ Waggoner said in 1899 that his translation included "without doubt, as a whole, the best English translation of the prophecy of Isaiah."
In 1750 he was appointed archdeacon of Winchester. In 1752 he resigned the professorship at Oxford and married Mary Jackson. Shortly afterwards, in 1753, Lowth was appointed rector of East Woodhay. In 1754 he was awarded a Doctorate in Divinity by Oxford University, for his treatise on Hebrew poetry entitled "Praelectiones Academicae de Sacra Poesi Hebraeorum" ("On the Sacred Poetry of the Hebrews"). This derives from a series of lectures and was originally published in Latin. An English translation was published by George Gregory in 1787 as "Lectures on the Sacred Poetry of the Hebrews". This, and subsequent editions include the life of Bishop Lowth as a preface There was a further edition issued in 1815. This was republished in North America in 1829 with some additional notes. However, apart from those notes, the 1829 edition is less useful to a modern reader. This is because the editor of that edition chose to revert to citing many of the scriptural passages that Lowth uses as examples, and some of the annotations by Michaelis (Johann David Michaelis) and others in Latin.
Lowth was appointed a fellow of the Royal Societies of London and Göttingen in 1765. He was consecrated bishop of St David's in 1766; however, before the end of the year he was transferred to the see of Oxford. He remained Bishop of Oxford until 1777 when he was appointed Bishop of London as well as dean of the chapel royal and privy councillor. In 1783 he was offered the chance to become Archbishop of Canterbury, but declined due to failing health.
Lowth wrote a Latin epitaph, "Cara, Vale" on the death of his daughter Maria. It was much admired in the late 18th and early 19th centuries. It was by the English composer John Wall Callcott.
Lowth died in 1787, and is buried in the churchyard of All Saints Church, Fulham.
Old Testament Scholarship.
Lowth seems to have been the first modern Bible scholar to notice or draw attention to the poetic structure of the Psalms and much of the prophetic literature of the Old Testament. In Lecture 19 he sets out the classic statement of parallelism, which remains the most fundamental category for understanding Hebrew poetry. He identifies three forms of parallelism, the synonymous, antithetic and synthetic (i.e., balance only in the manner of expression without either synonymy or antithesis). This idea has been influential in Old Testament Studies to the present day.
Work on English Grammar.
Lowth is also remembered for his publication in 1762 of "A Short Introduction to English Grammar". Prompted by the absence of simple and pedagogical grammar textbooks in his day, Lowth set out to remedy the situation. Lowth's grammar is the source of many of the prescriptive shibboleths that are studied in schools, and established him as the first of a long line of usage commentators who judge the English language in addition to describing it. An example of both is one of his footnotes: ""Whose" is by some authors made the possessive case of "which", and applied to things as well as persons; I think, improperly."
His most famous contribution to the study of grammar may have been his tentative suggestion that sentences ending with a preposition—such as "what did you ask for?"—are inappropriate in formal writing. (This is known as preposition stranding.) In what may have been intentional self-reference, Lowth used that very construction in discussing it. "This is an Idiom which our language is strongly inclined to; it prevails in common conversation, and suits very well with the familiar style in writing; but the placing of the Preposition before the Relative is more graceful, as well as more perspicuous; and agrees much better with the solemn and elevated Style."2 Others had previously expressed this opinion; the earliest known is John Dryden in 1672.
Lowth's method included criticising "false syntax"; his examples of false syntax were culled from Shakespeare, the King James Bible, John Donne, John Milton, Jonathan Swift, Alexander Pope, and other famous writers. His understanding of grammar, like that of all linguists of his period, was influenced by the study of Latin, though he was aware that this was problematic and condemned "forcing the English under the rules of a foreign Language"1. Thus Lowth condemns Addison's sentence "Who should I meet the other night, but my old friend?" on the grounds that the thing acted upon should be in the "Objective Case" (corresponding, as he says earlier, to an oblique case in Latin), rather than taking this example and others as evidence from noted writers that "who" can refer to direct objects.
Lowth's dogmatic assertions appealed to those who wished for certainty and authority in their language. Lowth's grammar was not written for children; however, within a decade after it appeared, versions of it adapted for the use of schools had appeared, and Lowth's stylistic opinions acquired the force of law in the schoolroom. The textbook remained in standard usage throughout educational institutions until the early 20th century.
Literary critic.
Lowth has been regarded as the first imagery critic of Shakespeare's plays and highlighted the importance of the imagery in the interpretation of motives and actions of characters and dramatic movement of the plot and narrative structure.3
Notes.
1"", p. 107, condemning Richard Bentley's "corrections" of some of Milton's constructions.
2"Ibid"., pp. 127–128.
3Notes & Queries (OUP) in 1983 Vol. 30, Page 55-58 by Sailendra Kumar Sen, "Robert Lowth :the first imagery critic of Shakespeare".

</doc>
<doc id="26259" url="http://en.wikipedia.org/wiki?curid=26259" title="Robert Askin">
Robert Askin

Sir Robert William Askin GCMG, (4 April 1907 – 9 September 1981) was an Australian politician and the 32nd Premier of New South Wales from 1965 to 1975, the first representing the Liberal Party of Australia. He was born in 1907 as Robin William Askin, but always disliked his first name and changed it by deed poll in 1971. Before being knighted in 1972, however, he was generally known as "Bob Askin". Born in Sydney in 1907, Askin was educated at Sydney Technical High School. After serving as a bank officer and as a Sergeant in the Second World War, Askin joined the Liberal Party and was elected to the seat of Collaroy at the 1950 election.
Askin quickly rose through party ranks, eventually becoming Deputy Leader following Walter Howarth's resignation in July 1954. When long-serving party leader Vernon Treatt announced his resignation in August 1954, Askin put his name forward to replace him. At the vote, he became deadlocked against Pat Morton and Askin asked his former commanding officer Murray Robson to take the leadership instead. Robson did not live up to expectations and was deposed in September 1955 by Morton, who then became Leader. Askin remained as Deputy until, after leading the party to a second electoral defeat in 1959, Morton was deposed and Askin was elected to succeed him. At the May 1965 election, Askin presented the Liberal Party as a viable alternative government. He won a narrow victory, ending a 24-year Labor hold on government.
Askin's time in office was marked by a significant increase in public works programs, strong opposition to an increase in Commonwealth powers, laissez-faire economic policies and wide-ranging reforms in laws and regulations such as the Law Reform Commission, the introduction of consumer laws, legal aid, breath-testing of drivers, the liberalisation of liquor laws and the restoration of Postal voting in NSW elections. More controversial changes included the 1967 abolition of Sydney City Council and increased rates of development in Sydney, often at the expense of architectural heritage and historic buildings. This culminated in the 'Green ban' movement of the 1970s led by the Union movement to conserve the heritage of Sydney.
At the end of his term, after winning another three elections, Askin was the longest-serving Premier of New South Wales; his record has since been overtaken by Neville Wran and Bob Carr. Askin remains the longest-serving Leader of the New South Wales Liberal Party and the only Liberal Premier to retire from office. Since his death in 1981, however, Askin's legacy has been tarnished by persistent unproven allegations that he was involved in organised crime and official corruption.
Early years.
Robin William Askin was born in Sydney, New South Wales on 4 April 1907 at the Crown Street Women's Hospital, the eldest of three sons of Ellen Laura Halliday (née Rowe) and William James Askin, an Adelaide-born sailor and worker for New South Wales Railways. His parents later married on 29 September 1916. Askin spent his early years in Stuart Town before his family moved to Glebe, a working-class inner-city suburb of Sydney. After primary education at Glebe Public School, Askin was awarded a bursary to study at Sydney Technical High School, where he sat in the same class as the future aviator Charles Kingsford Smith. At school he gained good marks, with a particular interest in Mathematics and History, and enjoyed swimming and Rugby League. He completed his Intermediate Certificate in 1921.
At the age of 15, after a short time in the electrical trade, in 1922 Askin joined the Government Savings Bank of New South Wales as a Clerk. However, when the Savings Bank closed due to the Great Depression in 1931, he joined the Rural Bank of New South Wales. Between 1925 and 1929 Askin served part-time as a Lieutenant in the 55th Battalion, Citizens Military Forces. On 5 February 1937 Askin married Mollie Isabelle Underhill, a typist at the bank, at Gilbert Park Methodist Church, Manly. They lived in Manly for the rest of their lives. He began his interest in politics by assisting in Percy Spender's successful campaign for Askin's local seat of Warringah as an Independent candidate at the 1937 Federal election. In 1940 Askin was appointed manager of the Bank service department, which focused on public relations. He served as Vice-President from 1939 to 1940 and President from 1940 to 1941 of the Rural Bank branch of the United Bank Officers’ Association.
Askin enlisted as a Private in the Second Australian Imperial Force on 30 March 1942. An instructor with the 14th Infantry Training Battalion at Dubbo, he was appointed Acting Corporal, then reverted to Private. In November 1942 he joined the 2/31st Infantry Battalion in New Guinea, where he served for two months. He was in New Guinea for another six months from July 1943. Landing at Balikpapan, Borneo, in July 1945, Askin was promoted to Sergeant under Lieutenant Colonel Murray Robson. When hostilities ceased, he unsuccessfully attempted to set up an import business in Bandjermasin. Returning to Australia in February 1946, he was demobilised on 22 March.
Early political career.
Upon demobilisation, Askin returned to work at the Rural Bank, managing its travel department. However, his interest in politics arose again when he assisted his former commanding officer, Lieutenant Colonel Robson, in retaining his seat of Vaucluse at the 1947 state election for the newly formed Liberal Party of Australia, which Askin then joined. Rapidly rising through the party ranks, Askin soon became President of the Liberals' Manly branch and supported Bill Wentworth's successful bid for the new seat of Mackellar at the 1949 election.
Askin gained preselection for and won the seat of Collaroy at the 17 June 1950 election, gaining 63.69% of the vote. The Leader of the Liberal Party since 1946, Vernon Treatt led the Liberal/Country Coalition at the election, which resulted in a hung parliament, with Treatt's Coalition gaining 12 seats and a swing of 6.7% for a total of 46 seats. With the Australian Labor Party also holding 46 seats, the balance of power lay with the two re-elected Independent Labor member, James Geraghty and John Seiffert, who had been expelled from the party for disloyalty during the previous parliament. Under a legalistic interpretation of the ALP rules, Seiffert was readmitted to the party and, together with the support of Geraghty, Premier James McGirr and Labor were able to stay in power. As the new local member for a constituency covering most of the Northern Beaches from North Manly to Pittwater, Askin protested against the lack of government development and services in the area, such as sewerage, education, and transport.
The near loss of the election by Labor weakened McGirr's position and he was replaced as premier by Joseph Cahill in April 1952. Cahill had won popular support as a vigorous and impressive minister who had resolved problems with New South Wales' electricity supply and in his first 10 months as premier had reinvigorated the party. He appeared decisive and brought order to the government's chaotic public works program. In addition, he attacked the increasingly unpopular federal Coalition government of Robert Menzies. All this contributed to Treatt's Coalition being defeated at the 14 February 1953 election, with a total loss of ten seats and a swing against them of 7.2%. Askin retained his seat with 63.35%.
Deputy Leader.
With confidence in his leadership demolished, Treatt's Liberal Party descended into factional in-fighting culminating in the resignation of Deputy Leader Walter Howarth on 22 July 1954, who publicly announced it on 4 July citing that he felt that Treatt doubted his loyalty. He was replaced by now-Party Whip Askin. The resignation split the party and sparked a leadership challenge from Pat Morton. At the party meeting on 6 July, Treatt narrowly defeated Morton with 12 votes to 10. With party support eroded, Treatt did not remain long as leader afterwards. On Friday 6 August 1954, Treatt announced that he would resign as leader. At the following party meeting, after a deadlocked vote between Askin and Morton, Askin asked his friend Murray Robson to nominate and subsequently he was elected to succeed Treatt.
Like other senior members of the party, after having no conservative government since Alexander Mair in 1941, Robson had no experience in government, had little interest in policy and alienated many party members by trying to forge a closer alliance with Michael Bruxner's Country Party. Over a year after Robson assumed the leadership, at a party meeting on 20 September 1955, senior party member Ken McCaw moved that the leadership be declared vacant, citing that Robson's leadership lacked the qualities necessary for winning the next election. The motion was carried 15 votes to 5. Morton was then elected unnopposed as leader, with Askin remaining as Deputy Leader.
Morton then led the party to defeat at the election on 3 March 1956. The Coalition gained six seats, reducing the government's majority from twenty to six. Askin retained Collaroy with 70.14%. Morton again led the opposition to the ballot at the 21 March 1959 election, which resulted in an overall gain of three seats but the loss of two seats to Labor. After counting was finalised the Cahill Government was left with an overall majority of four seats. Askin retained his seat with 71.09%.
Leader of the Opposition.
Morton's refusal to give up his many business interests while as leader led many to accuse him of being a 'part-time leader' and together with his second election loss, eroded confidence in his leadership. On 14 July 1959, three Liberal MLAs called on Morton to resign, stating that the party needed a full-time leader and that Morton no longer commanded the majority support of his colleagues. Morton refused and instead called an emergency meeting on 17 July to confirm his leadership. Soon after, the two main opponents to Morton, the Member for Earlwood, Eric Willis, and Askin, declared that they would only take the Leadership if they were given an absolute majority of 28 votes. At the party meeting, Morton was removed as Leader by two votes. Willis then surprised many by deciding not to put his name forward for nomination, leaving Askin as the only contender. Askin was then elected unanimously as leader, with Willis eventually becoming Deputy Leader. Upon election, Askin declared that "One of my main tasks will be to sell our [Liberal Party] ideas and principles to the working man."
When Premier Cahill died on 22 October 1959, he was replaced by Askin's friend and parliamentary contemporary, Robert "Bob" Heffron, which tended to calm his aggression and opposition towards the government. At the March 1962 election, Labor had been in power for 21 years and Heffron had since been Premier for 2 and a half years. Heffron was 72 at the time of the election and his age and the longevity of the government were made issues by the Askin's opposition which described it as being composed of "tired old men". The standing of Heffron's government suffered when the electors rejected its proposal to abolish the New South Wales Legislative Council at a referendum in April 1961, being the first time Labor had lost a state electoral poll in 20 years. Askin's successful opposition campaign centred on warning of a Labor-dominated single house subject to "Communist and Trades Hall influence".
Labor's policies for the election included the establishment of a Department of Industrial Development to reduce unemployment, free school travel, aid to home buyers and commencing the construction of the Sydney–Newcastle Freeway as a toll-road. By contrast, Askin put forward a wide-ranging program of reform and addressed contentious issues including the introduction of State Aid for private schools, making rent control fairer and the legalisation of off-course betting on horse races. Askin accused the state government of allowing the transport infrastructure of the state to decline and promised to build the Newcastle freeway without a toll, to construct the Eastern Suburbs Railway and to plan for a second crossing of Sydney Harbour. Askin also made promises for more resources in mental health and district hospitals.
Despite these promises, Askin and the new Country Party Leader, Charles Cutler, lost the election to Heffron, mainly due to the adverse reactions of voters towards the November 1960 "horror budget" and credit squeeze made by the federal Liberal government of Robert Menzies. The Coalition lost five seats, despite a small swing of 0.16% and the Coalition gaining the support of prominent media businessman, Frank Packer, who helped project the image of Askin and the Liberal party as a viable alternative government. Askin retained his seat with 72.53%.
The 1965 campaign against the Labor Government (lead since April 1964 by Jack Renshaw), a government widely perceived to be tired and devoid of ideas, was notable for being one of Australia's first "presidential-style" campaigns, with Askin being the major focus of campaigning and a main theme of "With Askin You'll Get Action". He received vigorous support from the newspapers and TV stations owned by Packer. At the May 1965 election, the Liberal/Country Coalition gained 49.8% of the vote to 43.3% to the ALP. While the Liberals took only two seats from Labor, Askin got the support of the two independent members, Douglas Darby (Manly) and Harold Coates (Hartley), giving him enough support to end Labor's 24-year run in power. He officially took office on 1 May, with Charles Cutler of the Country Party as Deputy Premier.
Premier of New South Wales.
The Askin Government was sworn in by the Governor of New South Wales, Sir Eric Woodward, on 13 May at Government House. It was the first to be headed by the Liberal Party since the main non-Labor party in the state adopted the Liberal banner. Askin, who served as his own Treasurer, heavily involved himself in the business of Government, while also maintaining a range of social agendas and regular outings to the racetrack or Rugby League games. One of the privileges of office was the access to a Ministerial car and personal driver, which became particularly important for Askin, who did not drive. On one occasion when Askin was supposed to drive a new Holden from the factory assembly line during a visit, Askin arranged for his driver, Russ Ferguson, to be hidden on the car floor working the controls while Askin held the wheel.
Askin's government was marked by strong opposition to an increase in Commonwealth powers, a tough stance on "law and order" issues, laissez-faire economic policies, and aggressive support for industrial and commercial development. At his first Cabinet meeting, Askin restored direct air services between Sydney and Dubbo, and required Joern Utzon, the Danish architect then working on the Sydney Opera House, to provide a final price and completion date for the Opera House, which had gone past the original estimates for both. His Public Works Minister Davis Hughes began to assert control over the project and demanded that costs be reined in. This brought him into direct conflict with Utzon and in February 1966, after a bitter standoff and the suspension of progress payments by Hughes, Utzon resigned, sparking a major public outcry. Two weeks after the first Government meeting, the Askin Government abolished the tow-away system for Sydney and Newcastle. In 1966 the University of New South Wales awarded him an honorary Doctor of Letters (D.Litt).
Law reform.
Despite a hostile Legislative Council, an extended drought and various industrial disputes, Askin and his Government passed several reforms. Among them were the removal of trading-hours restrictions on small businesses, abolishing juries for motor accident damage cases, extending the hours for liquor trading, thereby bringing an end to the "Six o'clock swill". The Government also moved into legal and local government reforms, attacking pollution and restoring the previously abolished postal voting rights in state elections. Askin also addressed the demands of the New England New State Movement by holding a referendum in 1967, which was defeated by a large margin.
Many of his government’s reforms were due to his Minister for Justice, John Maddison, and Attorney-General Sir Kenneth McCaw, who initiated the establishment of the Law Reform Commission of New South Wales, the introduction of consumer laws, an ombudsman, legal aid, health labels on cigarette packs, breath-testing of drivers, limits on vehicle emissions, the liberalisation of liquor laws, and compensation for victims of violent crime. There was also a new National Parks and Wildlife Service to assist environment conservation and protection. Despite these positive reforms, Askin's government maintained a brutal prison and corrective regime that was to culminate in the Bathurst Gaol riots in 1970 and 1974.
Local government and planning.
Askin, along with his Minister for Local Government, Pat Morton, oversaw the rapid escalation of building development in inner-city Sydney and the central business district, which followed in the wake of his controversial 1967 abolition of Sydney City Council and a redistribution of municipal electoral boundaries that was aimed at reducing the power of the rival Australian Labor Party. On its abolition, Morton commented that it was "essential for Sydney's progress" and replaced the City Council with a Commission, headed by another former Liberal leader, Vernon Treatt.
The Sydney metropolitan area at the time was marked by increasing strains on state infrastructure and Askin's Government's pro-development stance was largely attributed as an attempt to alleviate these problems. Despite this, the newly established State Planning Authority were continuously criticised for not being totally accountable to the public, particularly as the pro-business Sydney Commissioners worked side-by-side with the Planning authority to increase developments in the Sydney CBD to their highest levels ever, embodied by the construction of the MLC Centre, the demolition of the Theatre Royal, Sydney and the Australia Hotel. Other controversial schemes proposed by his government were a massive freeway system that was planned to be driven through the hearts of historic inner-city suburbs including Glebe and Newtown and an equally ambitious scheme of 'slum clearance' that would have brought about the wholescale destruction of the historic areas of Woolloomooloo and The Rocks. This eventually culminated in the 1970s Green ban movement led by Unions Leader Jack Mundey, to protect the architectural heritage of Sydney.
Second term.
At the 24 February 1968 election, Askin increased his previously tenuous majority, scoring a six-seat swing against Labor's Renshaw and an overall majority of 12 over the Labor Party and the two Independents. Askin retained his seat with 70.97%. It was the first time since the UAP/Country Coalition won three consecutive elections from 1932 to 1938 that a non-Labor government in New South Wales had been reelected.
In mid-1968 Askin famously became embroiled in a media controversy over the reporting of several words spoken to the United States Chamber of Commerce lunch in Sydney on 32 July 1968 (also the day Opposition Leader Renshaw resigned, to be replaced by Pat Hills), in which he spoke of the October 1966 state visit by United States President Lyndon B. Johnson. Askin had joined Prime Minister Harold Holt, President Johnson and the American Ambassador, Ed Clark, in a drive through the Sydney CBD. As Johnson's motorcade drove into Liverpool Street, several anti-Vietnam War protesters, including Graeme Dunstan, threw themselves in front of the car carrying them. As Askin later recalled, a police officer had informed him that some communists were obstructing the route. Askin claimed he had instructed the officer to drag them off. As the car moved on, he then said to Johnson "half-jocularly": "what I ought to have told him was to ride over them", to which Johnson replied "a man after my own heart". At the subsequent luncheon, Askin instead reported that he had said the remark to the police officer, which a journalist attending the event later reported it as "Run over the bastards."
Federal relations.
As Treasurer, Askin focused on the state budget and on Commonwealth-State financial relations. His attitude towards the Commonwealth and the Federal was shaped by his first premiers’ conference in 1965 when Prime Minister Menzies negotiated with the Victorian premier Henry Bolte to achieve an extra grant of funds for Victoria at the expense of the other states and closed the conference before the other Premiers could object. At subsequent premiers’ conferences he opposed the 'centralising' tendencies of Canberra and became a strong advocate of the rights of the states.
With John Gorton becoming Prime Minister after Holt's death, Askin came into conflict with the Commonwealth Government over Gorton's determination to maintain federal command over taxation and in June 1968 declared that he could veto any form of state taxation. In late 1969, Askin, with Bolte, organised an 'emergency' premiers' conference, without Gorton, to publicise the disadvantages of the States, a move that was partly responsible for the party deposition of Gorton in 1971.
Askin had a greater dislike for Gorton's successor, William McMahon and received financial support from McMahon only when Askin threatened to release a NSW "horror budget" that could damage Federal Liberal voting intentions. However, when McMahon lost the 1972 election to Labor Leader Gough Whitlam, relations between Sydney and Canberra got even worse. Whitlam's centralising economic policies and decision to end legal appeals to the Privy Council of the United Kingdom drew criticism from Askin.
Allegations of corruption.
Since his death, there have been persistent allegations that Askin, allegedly assisted by then Police Commissioner Norman Allan, oversaw the creation of a lucrative network of corruption and bribery that involved politicians, public servants and police and the nascent Sydney organised crime syndicates.
When questioned about his wealth, Askin always attributed it to the salary from his high public office, his frugal lifestyle, good investments and canny punting. After his death the Australian Taxation Office audited his estate, and although it made no finding of criminality, it determined that a substantial part of it came from undisclosed income derived from sources other than shares or gambling.
With Askin's death, investigative journalists were freed from the threat of legal action under Australia's defamation laws. Stories about his reputed corruption were published almost immediately. The most notable of these was the article appeared in the "National Times" co-written by David Marr and David Hickie, headlined "Askin: friend of organised crime", which was famously published on the day of Askin's funeral in 1981. This was followed by David Hickie's book "The Prince and The Premier", which detailed Askin's long involvement in illegal bookmaking and allegations that he had received substantial and long-running payoffs from organised crime figures.
The allegations of corruption against Askin were revived in 2008 when Alan Saffron, the son of the late Sydney crime boss Abe Saffron, published a biography of his father in which he alleged that Saffron had paid bribes to major public officials including Askin, former police commissioner Norman Allan, and other leading figures whom he claimed he could not name because they were still alive. Alan Saffron alleged that his father made payments of between A$5000 and $10,000 per week to both men over many years, that Askin and Allan both visited Saffron's office on several occasions, that Allan also visited the Saffron family home, and that Abe Saffron paid for an all-expenses overseas trip for Allan and a young female 'friend'. He also alleged that, later in Askin's premiership, Abe Saffron became the "bagman" for Sydney's illegal liquor and prostitution rackets and most illegal gambling activities, collecting payoffs that were then passed to Askin, Allan and others, in return for which his father was completely protected.
End of premiership and legacy.
Throughout his time as Premier, he was assisted by Charles Cutler as Deputy Premier and Leader of the Country Party. Cutler served as Acting Premier at times when Askin was suffering from illness, having suffered two heart attacks in 1969 and 1973. In 1972 the Orthodox Church of Antioch presented Askin with the Order of St Peter and St Paul for his services to ethnic minorities.
In 1971 Askin changed his name from "Robin" to "Robert" by a deed poll. On 1 January 1972, on his own recommendation, he was appointed a Knight Commander of the Order of St Michael and St George (KCMG). Later that year, taking advantage of unease at the increasingly erratic Labor government of Gough Whitlam and the increasing economic problems seen to be caused by it, Askin called an early election for 1973. However, a setback arose in the northern Sydney seat of Gordon, when the Liberal member and Education Minister, Harry Jago, failed to nominate his candidacy, thereby losing the seat to the Democratic Labor Party before the election took place. However, the Coalition went to a record fourth win against the ALP, led by Pat Hills, increasing the Liberal/Country majority by four seats and making Askin the only major party leader to win four consecutive terms as Premier until Neville Wran of the ALP. Askin contested the election in Pittwater, replacing his former seat of Collaroy. In 1973 he was appointed an Officer of the Lebanese National Order of the Cedar.
His last term in office was marked by tension between the NSW and Victorian Governments and a view that Askin was getting out of touch with the voters. Late in 1974, Askin announced his resignation, and his last intervention was to support his Minister for Lands, Thomas Lewis, in his bid to be Askin's successor instead of the Deputy Leader and Minister for Education, Sir Eric Willis. It was reported that Lewis had offered to upgrade Askin's knighthood from Knight Commander (KCMG) to Knight Grand Cross (GCMG) of the Order of St Michael and St George, while Willis was uncommitted. Askin retired from politics in January 1975 and was succeeded by Lewis as Premier. On 14 June 1975 he was elevated to Knight Grand Cross, for his service as Premier. His resignation began a turbulent year for the government. Lewis was ousted in a party room coup by Willis in 1976, but Willis only lasted four months before losing the 1976 election to Labor, ending the longest unbroken run for a non-Labor government since World War I.
His health declined still further after 1975, and he died of heart failure on 9 September 1981 in St Vincent's Hospital, Sydney. He left an estate valued at just under $2 million, a very substantial sum for the time, to his widow, Lady Askin.
The next day, the "Sydney Morning Herald" editorialised him as "one of the ablest, most industrious and colourful political leaders of Australia's post-war era". He was granted a state funeral on 14 September, which was attended by over 1,000 mourners including Prime Minister Malcolm Fraser, Premier Neville Wran, Mervyn Wood, Justice Lionel Murphy and former NSW Labor Premier and former Governor-General Sir William McKell.

</doc>
<doc id="26262" url="http://en.wikipedia.org/wiki?curid=26262" title="Redshift">
Redshift

In physics, redshift happens when light or other electromagnetic radiation from an object is increased in wavelength, or shifted to the red end of the spectrum. In general, whether or not the radiation is within the visible spectrum, "redder" means an increase in wavelength – equivalent to a lower frequency and a lower photon energy, in accordance with, respectively, the wave and quantum theories of light.
Some redshifts are an example of the Doppler effect, familiar in the change in the apparent pitches of sirens and frequency of the sound waves emitted by speeding vehicles. A redshift occurs whenever a light source moves away from an observer. Another kind of redshift is cosmological redshift, which is due to the expansion of the universe, and sufficiently distant light sources (generally more than a few million light years away) show redshift corresponding to the rate of increase in their distance from Earth. Finally, gravitational redshift is a relativistic effect observed in electromagnetic radiation moving out of gravitational fields. Conversely, a "decrease" in wavelength is called blueshift and is generally seen when a light-emitting object moves toward an observer or when electromagnetic radiation moves into a gravitational field. However, redshift is a more common term and sometimes blueshift is referred to as negative redshift.
Knowledge of redshifts and blueshifts has been applied to develop several terrestrial technologies such as Doppler radar and radar guns. Redshifts are also seen in the spectroscopic observations of astronomical objects. Its value is represented by the letter "z."
A special relativistic redshift formula (and its classical approximation) can be used to calculate the redshift of a nearby object when spacetime is flat. However, in many contexts, such as black holes and Big Bang cosmology, redshifts must be calculated using general relativity. Special relativistic, gravitational, and cosmological redshifts can be understood under the umbrella of frame transformation laws. There exist other physical processes that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from true redshift and are not generally referred to as such (see section on physical optics and radiative transfer).
History.
The history of the subject began with the development in the 19th century of wave mechanics and the exploration of phenomena associated with the Doppler effect. The effect is named after Christian Doppler, who offered the first known physical explanation for the phenomenon in 1842. The hypothesis was tested and confirmed for sound waves by the Dutch scientist Christophorus Buys Ballot in 1845. Doppler correctly predicted that the phenomenon should apply to all waves, and in particular suggested that the varying colors of stars could be attributed to their motion with respect to the Earth. Before this was verified, however, it was found that stellar colors were primarily due to a star's temperature, not motion. Only later was Doppler vindicated by verified redshift observations.
The first Doppler redshift was described by French physicist Hippolyte Fizeau in 1848, who pointed to the shift in spectral lines seen in stars as being due to the Doppler effect. The effect is sometimes called the "Doppler–Fizeau effect". In 1868, British astronomer William Huggins was the first to determine the velocity of a star moving away from the Earth by this method. In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines using solar rotation, about 0.1 Å in the red.
In 1887, Vogel and Scheiner discovered the "annual Doppler effect", the yearly change in the Doppler shift of stars located near the ecliptic due to the orbital velocity of the Earth. In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors.
The earliest occurrence of the term "red-shift" in print (in this hyphenated form) appears to be by American astronomer Walter S. Adams in 1908, in which he mentions "Two methods of investigating that nature of the nebular red-shift". The word does not appear unhyphenated until about 1934 by Willem de Sitter, perhaps indicating that up to that point its German equivalent, "Rotverschiebung", was more commonly used.
Beginning with observations in 1912, Vesto Slipher discovered that most spiral galaxies, then mostly thought to be spiral nebulae, had considerable redshifts. Slipher first reports on his measurement in the inaugural volume of the "Lowell Observatory Bulletin". Three years later, he wrote a review in the journal "Popular Astronomy". In it he states, "[...] the early discovery that the great Andromeda spiral had the quite exceptional velocity of –300 km(/s) showed the means then available, capable of investigating not only the spectra of the spirals but their velocities as well." Slipher reported the velocities for 15 spiral nebulae spread across the entire celestial sphere, all but three having observable "positive" (that is recessional) velocities. Subsequently, Edwin Hubble discovered an approximate relationship between the redshifts of such "nebulae" and the distances to them with the formulation of his eponymous Hubble's law. These observations corroborated Alexander Friedmann's 1922 work, in which he derived the famous Friedmann equations. They are today considered strong evidence for an expanding universe and the Big Bang theory.
Measurement, characterization, and interpretation.
The spectrum of light that comes from a single source (see idealized spectrum illustration top-right) can be measured. To determine the redshift, one searches for features in the spectrum such as absorption lines, emission lines, or other variations in light intensity. If found, these features can be compared with known features in the spectrum of various chemical compounds found in experiments where that compound is located on Earth. A very common atomic element in space is hydrogen. The spectrum of originally featureless light shone through hydrogen will show a signature spectrum specific to hydrogen that has features at regular intervals. If restricted to absorption lines it would look similar to the illustration (top right). If the same pattern of intervals is seen in an observed spectrum from a distant source but occurring at shifted wavelengths, it can be identified as hydrogen too. If the same spectral line is identified in both spectra—but at different wavelengths—then the redshift can be calculated using the table below. Determining the redshift of an object in this way requires a frequency- or wavelength-range. In order to calculate the redshift one has to know the wavelength of the emitted light in the rest frame of the source, in other words, the wavelength that would be measured by an observer located adjacent to and comoving with the source. Since in astronomical applications this measurement cannot be done directly, because that would require travelling to the distant star of interest, the method using spectral lines described here is used instead. Redshifts cannot be calculated by looking at unidentified features whose rest-frame frequency is unknown, or with a spectrum that is featureless or white noise (random fluctuations in a spectrum).
Redshift (and blueshift) may be characterized by the relative difference between the observed and emitted wavelengths (or frequency) of an object. In astronomy, it is customary to refer to this change using a dimensionless quantity called "z". If "λ" represents wavelength and "f" represents frequency (note, where "c" is the speed of light), then "z" is defined by the equations:
After "z" is measured, the distinction between redshift and blueshift is simply a matter of whether "z" is positive or negative. See the formula section below for some basic interpretations that follow when either a redshift or blueshift is observed. For example, Doppler effect blueshifts ("z" < 0) are associated with objects approaching (moving closer to) the observer with the light shifting to greater energies. Conversely, Doppler effect redshifts ("z" > 0) are associated with objects receding (moving away) from the observer with the light shifting to lower energies. Likewise, gravitational blueshifts are associated with light emitted from a source residing within a weaker gravitational field as observed from within a stronger gravitational field, while gravitational redshifting implies the opposite conditions.
Redshift formulae.
In general relativity one can derive several important special-case formulae for redshift in certain special spacetime geometries, as summarized in the following table. In all cases the magnitude of the shift (the value of "z") is independent of the wavelength.
Doppler effect.
If a source of the light is moving away from an observer, then redshift ("z" > 0) occurs; if the source moves towards the observer, then blueshift ("z" < 0) occurs. This is true for all electromagnetic waves and is explained by the Doppler effect. Consequently, this type of redshift is called the "Doppler redshift". If the source moves away from the observer with velocity "v", which is much less than the speed of light ("v" ≪ "c"), the redshift is given by
where "c" is the speed of light. In the classical Doppler effect, the frequency of the source is not modified, but the recessional motion causes the illusion of a lower frequency.
A more complete treatment of the Doppler redshift requires considering relativistic effects associated with motion of sources close to the speed of light. A complete derivation of the effect can be found in the article on the relativistic Doppler effect. In brief, objects moving close to the speed of light will experience deviations from the above formula due to the time dilation of special relativity which can be corrected for by introducing the Lorentz factor "γ" into the classical Doppler formula as follows (for motion solely in the line of sight):
This phenomenon was first observed in a 1938 experiment performed by Herbert E. Ives and G.R. Stilwell, called the Ives–Stilwell experiment.
Since the Lorentz factor is dependent only on the magnitude of the velocity, this causes the redshift associated with the relativistic correction to be independent of the orientation of the source movement. In contrast, the classical part of the formula is dependent on the projection of the movement of the source into the line-of-sight which yields different results for different orientations. If "θ" is the angle between the direction of relative motion and the direction of emission in the observer's frame (zero angle is directly away from the observer), the full form for the relativistic Doppler effect becomes:
and for motion solely in the line of sight (), this equation reduces to:
For the special case that the light is approaching at right angles () to the direction of relative motion in the observer's frame, the relativistic redshift is known as the transverse redshift, and a redshift:
is measured, even though the object is not moving away from the observer. Even when the source is moving towards the observer, if there is a transverse component to the motion then there is some speed at which the dilation just cancels the expected blueshift and at higher speed the approaching source will be redshifted.
Expansion of space.
In the early part of the twentieth century, Slipher, Hubble and others made the first measurements of the redshifts and blueshifts of galaxies beyond the Milky Way. They initially interpreted these redshifts and blueshifts as due solely to the Doppler effect, but later Hubble discovered a rough correlation between the increasing redshifts and the increasing distance of galaxies. Theorists almost immediately realized that these observations could be explained by a different mechanism for producing redshifts. Hubble's law of the correlation between redshifts and distances is required by models of cosmology derived from general relativity that have a metric expansion of space. As a result, photons propagating through the expanding space are stretched, creating the cosmological redshift.
There is a distinction between a redshift in cosmological context as compared to that witnessed when nearby objects exhibit a local Doppler-effect redshift. Rather than cosmological redshifts being a consequence of relative velocities, the photons instead increase in wavelength and redshift because of a feature of the spacetime through which they are traveling that causes space to expand. Due to the expansion increasing as distances increase, the distance between two remote galaxies can increase at more than 3×108 m/s, but this does not imply that the galaxies move faster than the speed of light at their present location (which is forbidden by Lorentz covariance).
Mathematical derivation.
The observational consequences of this effect can be derived using the equations from general relativity that describe a homogeneous and isotropic universe.
To derive the redshift effect, use the geodesic equation for a light wave, which is
where
For an observer observing the crest of a light wave at a position and time , the crest of the light wave was emitted at a time in the past and a distant position . Integrating over the path in both space and time that the light wave travels yields:
In general, the wavelength of light is not the same for the two positions and times considered due to the changing properties of the metric. When the wave was emitted, it had a wavelength "λ"then. The next crest of the light wave was emitted at a time
The observer sees the next crest of the observed light wave with a wavelength "λ"now to arrive at a time
Since the subsequent crest is again emitted from and is observed at , the following equation can be written:
The right-hand side of the two integral equations above are identical which means
Using the following manipulation:
we find that:
For very small variations in time (over the period of one cycle of a light wave) the scale factor is essentially a constant ( today and previously). This yields
which can be rewritten as
Using the definition of redshift provided above, the equation
is obtained. In an expanding universe such as the one we inhabit, the scale factor is monotonically increasing as time passes, thus, "z" is positive and distant galaxies appear redshifted.
Using a model of the expansion of the Universe, redshift can be related to the age of an observed object, the so-called "cosmic time–redshift relation". Denote a density ratio as Ω0:
with "ρ"crit the critical density demarcating a universe that eventually crunches from one that simply expands. This density is about three hydrogen atoms per thousand liters of space. At large redshifts one finds:
where "H"0 is the present-day Hubble constant, and "z" is the redshift.
Distinguishing between cosmological and local effects.
For cosmological redshifts of "z" < 0.01 additional Doppler redshifts and blueshifts due to the peculiar motions of the galaxies relative to one another cause a wide scatter from the standard Hubble Law. The resulting situation can be illustrated by the Expanding Rubber Sheet Universe, a common cosmological analogy used to describe the expansion of space. If two objects are represented by ball bearings and spacetime by a stretching rubber sheet, the Doppler effect is caused by rolling the balls across the sheet to create peculiar motion. The cosmological redshift occurs when the ball bearings are stuck to the sheet and the sheet is stretched.
The redshifts of galaxies include both a component related to recessional velocity from expansion of the Universe, and a component related to peculiar motion (Doppler shift). The redshift due to expansion of the Universe depends upon the recessional velocity in a fashion determined by the cosmological model chosen to describe the expansion of the Universe, which is very different from how Doppler redshift depends upon local velocity. Describing the cosmological expansion origin of redshift, cosmologist Edward Robert Harrison said, "Light leaves a galaxy, which is stationary in its local region of space, and is eventually received by observers who are stationary in their own local region of space. Between the galaxy and the observer, light travels through vast regions of expanding space. As a result, all wavelengths of the light are stretched by the expansion of space. It is as simple as that..." Steven Weinberg clarified, "The increase of wavelength from emission to absorption of light does not depend on the rate of change of "a"("t") [here "a"("t") is the Robertson-Walker scale factor] at the times of emission or absorption, but on the increase of "a"("t") in the whole period from emission to absorption."
Popular literature often uses the expression "Doppler redshift" instead of "cosmological redshift" to describe the redshift of galaxies dominated by the expansion of spacetime, but the cosmological redshift is not found using the relativistic Doppler equation which is instead characterized by special relativity; thus "v" > "c" is impossible while, in contrast, "v" > "c" is possible for cosmological redshifts because the space which separates the objects (for example, a quasar from the Earth) can expand faster than the speed of light. More mathematically, the viewpoint that "distant galaxies are receding" and the viewpoint that "the space between galaxies is expanding" are related by changing coordinate systems. Expressing this precisely requires working with the mathematics of the Friedmann-Robertson-Walker metric.
If the Universe were contracting instead of expanding, we would see distant galaxies blueshifted by an amount proportional to their distance instead of redshifted.
Gravitational redshift.
In the theory of general relativity, there is time dilation within a gravitational well. This is known as the gravitational redshift or "Einstein Shift". The theoretical derivation of this effect follows from the Schwarzschild solution of the Einstein equations which yields the following formula for redshift associated with a photon traveling in the gravitational field of an uncharged, nonrotating, spherically symmetric mass:
where
This gravitational redshift result can be derived from the assumptions of special relativity and the equivalence principle; the full theory of general relativity is not required.
The effect is very small but measurable on Earth using the Mössbauer effect and was first observed in the Pound–Rebka experiment. However, it is significant near a black hole, and as an object approaches the event horizon the red shift becomes infinite. It is also the dominant cause of large angular-scale temperature fluctuations in the cosmic microwave background radiation (see Sachs-Wolfe effect).
Observations in astronomy.
The redshift observed in astronomy can be measured because the emission and absorption spectra for atoms are distinctive and well known, calibrated from spectroscopic experiments in laboratories on Earth. When the redshift of various absorption and emission lines from a single astronomical object is measured, "z" is found to be remarkably constant. Although distant objects may be slightly blurred and lines broadened, it is by no more than can be explained by thermal or mechanical motion of the source. For these reasons and others, the consensus among astronomers is that the redshifts they observe are due to some combination of the three established forms of Doppler-like redshifts. Alternative hypotheses and explanations for redshift such as tired light are not generally considered plausible.
Spectroscopy, as a measurement, is considerably more difficult than simple photometry, which measures the brightness of astronomical objects through certain filters. When photometric data is all that is available (for example, the Hubble Deep Field and the Hubble Ultra Deep Field), astronomers rely on a technique for measuring photometric redshifts. Due to the broad wavelength ranges in photometric filters and the necessary assumptions about the nature of the spectrum at the light-source, errors for these sorts of measurements can range up to , and are much less reliable than spectroscopic determinations. However, photometry does at least allow a qualitative characterization of a redshift. For example, if a Sun-like spectrum had a redshift of , it would be brightest in the infrared rather than at the yellow-green color associated with the peak of its blackbody spectrum, and the light intensity will be reduced in the filter by a factor of four, (1 + "z")2. Both the photon count rate and the photon energy are redshifted. (See K correction for more details on the photometric consequences of redshift.)
Local observations.
In nearby objects (within our Milky Way galaxy) observed redshifts are almost always related to the line-of-sight velocities associated with the objects being observed. Observations of such redshifts and blueshifts have enabled astronomers to measure velocities and parametrize the masses of the orbiting stars in spectroscopic binaries, a method first employed in 1868 by British astronomer William Huggins. Similarly, small redshifts and blueshifts detected in the spectroscopic measurements of individual stars are one way astronomers have been able to diagnose and measure the presence and characteristics of planetary systems around other stars and have even made very detailed differential measurements of redshifts during planetary transits to determine precise orbital parameters. Finely detailed measurements of redshifts are used in helioseismology to determine the precise movements of the photosphere of the Sun. Redshifts have also been used to make the first measurements of the rotation rates of planets, velocities of interstellar clouds, the rotation of galaxies, and the dynamics of accretion onto neutron stars and black holes which exhibit both Doppler and gravitational redshifts. Additionally, the temperatures of various emitting and absorbing objects can be obtained by measuring Doppler broadening – effectively redshifts and blueshifts over a single emission or absorption line. By measuring the broadening and shifts of the 21-centimeter hydrogen line in different directions, astronomers have been able to measure the recessional velocities of interstellar gas, which in turn reveals the rotation curve of our Milky Way. Similar measurements have been performed on other galaxies, such as Andromeda. As a diagnostic tool, redshift measurements are one of the most important spectroscopic measurements made in astronomy.
Extragalactic observations.
The most distant objects exhibit larger redshifts corresponding to the Hubble flow of the Universe. The largest observed redshift, corresponding to the greatest distance and furthest back in time, is that of the cosmic microwave background radiation; the numerical value of its redshift is about ( corresponds to present time), and it shows the state of the Universe about 13.8 billion years ago, and 379,000 years after the initial moments of the Big Bang.
The luminous point-like cores of quasars were the first "high-redshift" ("z" > 0.1) objects discovered before the improvement of telescopes allowed for the discovery of other high-redshift galaxies.
For galaxies more distant than the Local Group and the nearby Virgo Cluster, but within a thousand megaparsecs or so, the redshift is approximately proportional to the galaxy's distance. This correlation was first observed by Edwin Hubble and has come to be known as Hubble's law. Vesto Slipher was the first to discover galactic redshifts, in about the year 1912, while Hubble correlated Slipher's measurements with distances he measured by other means to formulate his Law. In the widely accepted cosmological model based on general relativity, redshift is mainly a result of the expansion of space: this means that the farther away a galaxy is from us, the more the space has expanded in the time since the light left that galaxy, so the more the light has been stretched, the more redshifted the light is, and so the faster it appears to be moving away from us. Hubble's law follows in part from the Copernican principle. Because it is usually not known how luminous objects are, measuring the redshift is easier than more direct distance measurements, so redshift is sometimes in practice converted to a crude distance measurement using Hubble's law.
Gravitational interactions of galaxies with each other and clusters cause a significant scatter in the normal plot of the Hubble diagram. The peculiar velocities associated with galaxies superimpose a rough trace of the mass of virialized objects in the Universe. This effect leads to such phenomena as nearby galaxies (such as the Andromeda Galaxy) exhibiting blueshifts as we fall towards a common barycenter, and redshift maps of clusters showing a Fingers of God effect due to the scatter of peculiar velocities in a roughly spherical distribution. This added component gives cosmologists a chance to measure the masses of objects independent of the "mass to light ratio" (the ratio of a galaxy's mass in solar masses to its brightness in solar luminosities), an important tool for measuring dark matter.
The Hubble law's linear relationship between distance and redshift assumes that the rate of expansion of the Universe is constant. However, when the Universe was much younger, the expansion rate, and thus the Hubble "constant", was larger than it is today. For more distant galaxies, then, whose light has been travelling to us for much longer times, the approximation of constant expansion rate fails, and the Hubble law becomes a non-linear integral relationship and dependent on the history of the expansion rate since the emission of the light from the galaxy in question. Observations of the redshift-distance relationship can be used, then, to determine the expansion history of the Universe and thus the matter and energy content.
While it was long believed that the expansion rate has been continuously decreasing since the Big Bang, recent observations of the redshift-distance relationship using Type Ia supernovae have suggested that in comparatively recent times the expansion rate of the Universe has begun to accelerate.
Highest redshifts.
Currently, the objects with the highest known redshifts are galaxies and the objects producing gamma ray bursts. The most reliable redshifts are from spectroscopic data, and the highest confirmed spectroscopic redshift of a galaxy is that of
UDFy-38135539
at a redshift of , corresponding to just 600 million years after the Big Bang.
The previous record was held by
IOK-1, at a redshift , corresponding to just 750 million years after the Big Bang. Slightly less reliable are Lyman-break redshifts, the highest of which is the lensed galaxy A1689-zD1 at a redshift The most distant observed gamma ray burst with a spectroscopic redshift measurement was GRB 090423, which had a redshift of . The most distant known quasar, ULAS J1120+0641, is at . The highest known redshift radio galaxy (TN J0924-2201) is at a redshift and the highest known redshift molecular material is the detection of emission from the CO molecule from the quasar SDSS J1148+5251 at 
"Extremely red objects" (EROs) are astronomical sources of radiation that radiate energy in the red and near infrared part of the electromagnetic spectrum. These may be starburst galaxies that have a high redshift accompanied by reddening from intervening dust, or they could be highly redshifted elliptical galaxies with an older (and therefore redder) stellar population. Objects that are even redder than EROs are termed "hyper extremely red objects" (HEROs).
The cosmic microwave background has a redshift of , corresponding to an age of approximately 379,000 years after the Big Bang and a comoving distance of more than 46 billion light years. The yet-to-be-observed first light from the oldest Population III stars, not long after atoms first formed and the CMB ceased to be absorbed almost completely, may have redshifts in the range of 20 < "z" < 100. Other high-redshift events predicted by physics but not presently observable are the cosmic neutrino background from about two seconds after the Big Bang (and a redshift in excess of "z" > 1010) and the cosmic gravitational wave background emitted directly from inflation at a redshift in excess of "z" > 1025.
Redshift surveys.
With advent of automated telescopes and improvements in spectroscopes, a number of collaborations have been made to map the Universe in redshift space. By combining redshift with angular position data, a redshift survey maps the 3D distribution of matter within a field of the sky. These observations are used to measure properties of the large-scale structure of the Universe. The Great Wall, a vast supercluster of galaxies over 500 million light-years wide, provides a dramatic example of a large-scale structure that redshift surveys can detect.
The first redshift survey was the CfA Redshift Survey, started in 1977 with the initial data collection completed in 1982. More recently, the 2dF Galaxy Redshift Survey determined the large-scale structure of one section of the Universe, measuring redshifts for over 220,000 galaxies; data collection was completed in 2002, and the final data set was released 30 June 2003. The Sloan Digital Sky Survey (SDSS), is ongoing as of 2013 and aims to measure the redshifts of around 3 million objects. SDSS has recorded redshifts for galaxies as high as 0.8, and has been involved in the detection of quasars beyond . The DEEP2 Redshift Survey uses the Keck telescopes with the new "DEIMOS" spectrograph; a follow-up to the pilot program DEEP1, DEEP2 is designed to measure faint galaxies with redshifts 0.7 and above, and it is therefore planned to provide a high redshift complement to SDSS and 2dF.
Effects due to physical optics or radiative transfer.
The interactions and phenomena summarized in the subjects of radiative transfer and physical optics can result in shifts in the wavelength and frequency of electromagnetic radiation. In such cases the shifts correspond to a physical energy transfer to matter or other photons rather than being due to a transformation between reference frames. These shifts can be due to such physical phenomena as coherence effects or the scattering of electromagnetic radiation whether from charged elementary particles, from particulates, or from fluctuations of the index of refraction in a dielectric medium as occurs in the radio phenomenon of radio whistlers. While such phenomena are sometimes referred to as "redshifts" and "blueshifts", in astrophysics light-matter interactions that result in energy shifts in the radiation field are generally referred to as "reddening" rather than "redshifting" which, as a term, is normally reserved for the effects discussed above.
In many circumstances scattering causes radiation to redden because entropy results in the predominance of many low-energy photons over few high-energy ones (while conserving total energy). Except possibly under carefully controlled conditions, scattering does not produce the same relative change in wavelength across the whole spectrum; that is, any calculated "z" is generally a function of wavelength. Furthermore, scattering from random media generally occurs at many angles, and "z" is a function of the scattering angle. If multiple scattering occurs, or the scattering particles have relative motion, then there is generally distortion of spectral lines as well.
In interstellar astronomy, visible spectra can appear redder due to scattering processes in a phenomenon referred to as interstellar reddening – similarly Rayleigh scattering causes the atmospheric reddening of the Sun seen in the sunrise or sunset and causes the rest of the sky to have a blue color. This phenomenon is distinct from red"shift"ing because the spectroscopic lines are not shifted to other wavelengths in reddened objects and there is an additional dimming and distortion associated with the phenomenon due to photons being scattered in and out of the line-of-sight.
"For a list of scattering processes, see Scattering."

</doc>
<doc id="26264" url="http://en.wikipedia.org/wiki?curid=26264" title="Rob Reiner">
Rob Reiner

Robert "Rob" Reiner (born March 6, 1947) is an American actor, director, producer, and activist. As an actor, Reiner first came to national prominence with the role of Michael Stivic, son-in-law of Archie and Edith Bunker (played by Carroll O'Connor and Jean Stapleton), on "All in the Family" (1971–78). That role earned him two Emmy Awards during the 1970s. As a director, Reiner was recognized by the Directors Guild of America (DGA) with nominations for the coming of age drama comedy "Stand by Me" (1986), the romantic comedy film "When Harry Met Sally..." (1989), and the courtroom drama "A Few Good Men" (1992). He also directed the psychological horror thriller film "Misery" (1990), the romantic comedy fantasy adventure film "The Princess Bride" (1987) and the heavy metal comedy-mockumentary "This Is Spinal Tap" (1984).
Early life.
Reiner was born to a Jewish family in The Bronx, New York, and is the son of Estelle Reiner (née Lebost), an actress, and Carl Reiner, a renowned comedian, actor, writer, producer, and director. As a child, Reiner lived at 48 Bonnie Meadow Road in New Rochelle, New York; the home of the fictional Petrie family in "The Dick Van Dyke Show", created by Rob's father, was 148 Bonnie Meadow Lane. He studied at the UCLA Film School.
Career.
In the late 1960s, Reiner acted in bit roles in several television shows including "Batman", "The Andy Griffith Show", "Gomer Pyle, U.S.M.C." and "The Beverly Hillbillies". He began his career writing for the "Smothers Brothers Comedy Hour" in 1968 and 1969. A few years later, Reiner became famous playing Michael Stivic, Archie Bunker's liberal son-in-law, on Norman Lear's 1970s situation comedy "All in the Family", which was the most-watched television program in the United States for five seasons (1971–1976). The character's nickname became closely associated with him, even after he had left the role and went on to build a high-profile career as a director. Reiner has stated, "I could win the Nobel Prize and they'd write 'Meathead wins the Nobel Prize'." For his performance, Reiner won two Emmy Awards in addition to three other nominations and five Golden Globe nominations. After an extended absence, Reiner has recently returned to television acting with a recurring role on "New Girl" (2012–present).
In 1972, Reiner, Phil Mishkin, and Gerry Isenberg created the situation comedy "The Super" for ABC. Starring Richard S. Castellano, the show depicted the life of the harried Italian American superintendent of a New York City apartment building and ran for 10 episodes in the summer of 1972. Reiner and Mishkin co-wrote the premiere episode.
Beginning in the 1980s, Reiner became known as a director of several successful Hollywood films that spanned many different genres. Several of these film remains highly popular with fans and critics. Some of his earlier films include cult classics such as the rock-band mockumentary "This Is Spinal Tap" (1984) and the comedic fantasy film "The Princess Bride" (1987), as well as his period piece coming of age tale "Stand by Me" (1986), which was also highly acclaimed. He often collaborates with film editor Robert Leighton, whom he also shares with fellow director-actor Christopher Guest as their go-to editor.
Reiner has gone on to direct other critically and commercially successful films with his own company, Castle Rock Entertainment. These include several iconic films such as the romantic comedy "When Harry Met Sally..." (1989), which has been critically ranked among the all-time best of its genre, the tense thriller "Misery" (1990), for which Kathy Bates won the Academy Award for Best Actress, and his most commercially successful work, the military courtroom drama "A Few Good Men" (1992), which was nominated for the Academy Award for Best Picture. Subsequent films directed by Reiner include the political romance "The American President" (1995), the courtroom drama "Ghosts of Mississippi" (1996), and the uplifting comedy "The Bucket List" (2007).
Reiner has continued to act in supporting roles in a number of movies and television shows, including "Throw Momma from the Train" (1987), "Sleepless in Seattle" (1993), "Bullets Over Broadway" (1994), "The First Wives Club" (1996), "Primary Colors" (1998), "EDtv" (1999), "New Girl" (2012–present), and "The Wolf of Wall Street" (2013). He has also parodied himself with cameos in works such as "" (2003) and "30 Rock" (2010).
Politics and activism.
Reiner has devoted considerable time and energy to liberal activism in recent years. His lobbying as an anti-smoking advocate in particular, earned his likeness a satirical role in a "South Park" episode titled "Butt Out".
Reiner is a co-founder of the American Foundation for Equal Rights, which initiated the court challenge against California Proposition 8 which banned same-sex marriage in the state.
In 1998, Reiner chaired the campaign to pass Prop 10, the California Children and Families Initiative, which created First 5 California, a program of early childhood development services, funded by a tax on tobacco products. He served as the first chairman of First 5 California, from 1999 to 2006. Reiner came under criticism for campaigning for a ballot measure (Prop 82) to fund state-run preschools while still chair of the First Five Commission, causing him to resign from his position on March 29, 2006. An audit was conducted, and it concluded that the state commission did not violate state law and that it had clear legal authority to conduct its public advertising campaigns related to preschool. In the end, Prop 82 failed to win approval, garnering only 39.1% support.
Reiner was mentioned as a possible candidate to run against California Governor Arnold Schwarzenegger in 2006 but decided not to run for personal reasons. He campaigned extensively for Democratic presidential nominee Al Gore in the 2000 Presidential election, and he campaigned in Iowa for Democratic Presidential candidate Howard Dean just before the 2004 Iowa caucuses. He endorsed Hillary Clinton for president for the 2008 election.
Reiner is a member of the Social Responsibility Task Force, an organization advocating moderation where social issues (such as violence and tobacco use) and the entertainment industry meet.
Reiner is also active in environmental issues, and he successfully led the effort to establish California's Ahmanson Ranch as a state park and wildlife refuge rather than as a commercial real estate development. He introduced Spinal Tap at the London Live Earth concert in July 2007.
Personal life.
Rob Reiner married actress/director Penny Marshall in 1971 and adopted Marshall's daughter from a previous marriage, actress Tracy Reiner. Reiner and Marshall divorced in 1981.
Reiner was introduced to his future wife, photographer Michele Singer, while directing "When Harry Met Sally". That meeting not only resulted in his deciding to change the ending of that movie, but he also eventually married Singer in 1989. They have three children. In 1997, Reiner and Singer founded the "I Am Your Child Foundation," now "Parents' Action for Children," a non-profit organization promoting early childhood development by producing and distributing celebrity-hosted educational videos for parents.
Reiner has stated that his childhood home was not observantly Jewish, although he did have a Bar Mitzvah. He identified himself as having no religious affiliation on the January 13, 2012, episode of "Real Time with Bill Maher". Reiner later told Huffington Post contributor Debra Oliver that while he rejected organized religion, he was sympathetic to the ideas of Buddhism.
In addition to his four children, Reiner has five grandchildren.

</doc>
<doc id="26265" url="http://en.wikipedia.org/wiki?curid=26265" title="Robin Wright">
Robin Wright

Robin Gayle Wright (born April 8, 1966) is an American actress and television director. She was previously credited as Robin Wright Penn. Wright first gained attention when starring in the NBC soap opera "Santa Barbara" as Kelly Capwell from 1984 to 1988, for which she received three Daytime Emmy nominations. She made the transition to film, starring in the romantic comedy fantasy adventure film "The Princess Bride" (1987), the fantasy-comedy "Toys" (1992), the epic romantic-comedy-drama "Forrest Gump" (1994) (earning her a Golden Globe Award nomination), the romantic drama "Message in a Bottle" (1999), the superhero drama-thriller "Unbreakable" (2000), the historical drama "The Conspirator" (2010), the biographical sports drama "Moneyball" (2011), and the mystery thriller "The Girl with the Dragon Tattoo" (2011).
She also stars as Claire Underwood in the Netflix political drama "House of Cards", for which she has been nominated for a Primetime Emmy Award for Outstanding Lead Actress in a Drama Series twice, and for which she won the Golden Globe in 2014 for Best Actress – Television Series Drama, making her the first actress to win a Golden Globe for an online-only web television series. From 1996 to 2010, Wright was married to actor Sean Penn, with whom she has two children.
Early life.
Wright was born in Dallas, Texas, to Gayle Gaston, a cosmetics saleswoman, and Freddie Wright, a pharmaceutical-company employee. She was raised in San Diego, California. She attended La Jolla High School and Taft High School in Woodland Hills, Los Angeles.
Career.
She was listed as one of twelve "Promising New Actors of 1986" in John Willis' Screen World, Vol. 38. [1986]. Wright first became famous on television, playing Kelly Capwell on the NBC soap opera "Santa Barbara", which earned her three Daytime Emmy Award nominations.
It wasn't until early to mid-1990s that she rose to fame after her roles as Buttercup in "The Princess Bride" and Jenny Curran in "Forrest Gump", the latter role garnering her Golden Globe Award and Screen Actors Guild nominations for Best Supporting Actress. Wright was offered the role of Maid Marian in "", but turned it down because she was pregnant. She backed out of the role of Abby McDeere in "The Firm" (1993), with Tom Cruise, upon discovering that she was pregnant with her second child, son Hopper Penn.
In 1996, she married actor Sean Penn and changed her name to Robin Wright Penn. The same year, she starred in the film adaptation of Daniel Defoe's "Moll Flanders" for which she received a Satellite Nomination for Best Actress in a Drama. She went on to co-star with her husband in the 1997 film "She's So Lovely", for which she was nominated for a Screen Actors Guild Award for Best Actress. One of her most recent successes was a supporting role in the television film "Empire Falls" as Grace Roby, mother of Ed Harris's character Miles Roby. Wright received her third Screen Actors Guild Award nomination for this role.
In 2013, Wright began her role as Claire Underwood, the spouse of Kevin Spacey's character, Congressman Francis Underwood in Netflix's series "House of Cards". On January 12, 2014, she won a Golden Globe for the role, becoming the first actress to win the award for an online-only web television series. She was nominated for the same award the following year. She also received nominations for the Primetime Emmy Award in 2013 and 2014 for the same role.
Personal life.
From 1986 to 1988, Wright was married to actor Dane Witherspoon, whom she met in 1984 on the set of the soap-opera "Santa Barbara".
In 1989, Wright became involved with actor Sean Penn following his divorce from Madonna. Their daughter Dylan Frances was born in April 1991. Their son Hopper Jack Penn was born in August 1993. Wright and Penn married in 1996. Their on-and-off relationship seemingly ended in divorce plans, announced in December 2007, but the divorce petition was withdrawn upon the couple's request four months later. In February 2009, Wright and Penn attended the 81st Academy Awards together, at which Penn won Best Actor. Penn subsequently filed for legal separation in April 2009, but withdrew the petition in May. On August 12, 2009, Wright filed for divorce once more, declaring she had no plans to reconcile. She also dropped "Penn" from her professional name. The divorce was finalized on July 22, 2010.
In February 2012, it was reported that Wright had begun dating actor Ben Foster. Their engagement was announced in January 2014. The couple have split and they called off their engagement on November 12, 2014.
She is the Honorary Spokesperson for the Dallas, Texas-based non-profit The Gordie Foundation. Robin has also been an advocate for the Enough Project. In 2014, she co-partnered with two California based companies called Pour Les Femmes and The SunnyLion. The SunnyLion gives a portion of its profit back to the Raise Hope For Congo Movement.

</doc>
<doc id="26266" url="http://en.wikipedia.org/wiki?curid=26266" title="Rhea">
Rhea

Rhea may refer to:

</doc>
<doc id="26268" url="http://en.wikipedia.org/wiki?curid=26268" title="Radius (disambiguation)">
Radius (disambiguation)

A radius is a straight line or distance from the center to the edge of a curve.
Radius may also refer to:

</doc>
<doc id="26269" url="http://en.wikipedia.org/wiki?curid=26269" title="Richard Butler">
Richard Butler

Richard Butler may refer to:

</doc>
<doc id="26270" url="http://en.wikipedia.org/wiki?curid=26270" title="List of rulers of Japan">
List of rulers of Japan

The rulers of Japan have been its Emperors, whether effectively or nominally, for its entire recorded history. These include the ancient legendary emperors, the attested but undated emperors of the Yamato period (early fifth to early 6th centuries), and the clearly dated emperors of 539 to the present. Political power was held in various eras by regents and shoguns, and since 1946 has been exercised exclusively by the Prime Minister as leader of a representative government. 

</doc>
<doc id="26272" url="http://en.wikipedia.org/wiki?curid=26272" title="Ryuichi Sakamoto">
Ryuichi Sakamoto

Ryuichi Sakamoto (坂本 龍一, Sakamoto Ryūichi, born January 17, 1952) (]) is a Japanese musician, activist, composer, record producer, writer, singer, pianist, and actor based in Tokyo and New York. Gaining major success in 1978 as a member of the electronic music group Yellow Magic Orchestra (YMO), Sakamoto served on keyboards and sometimes vocals. The band had worldwide hits such as "Computer Game / Firecracker" (1978), "Behind the Mask" (1978) and "Rydeen" (1979), later playing a pioneering role in the techno and acid house movements of the 1990s.
He concurrently pursued a solo career, releasing the experimental electronic fusion album "Thousand Knives" (1978), and later released the pioneering album "B-2 Unit" (1980), which included the electro classic "Riot in Lagos". From thereon, he produced more solo records, collaborated with many international artists, and pursued a wide variety of projects, such as having composed music for the 1992 Barcelona Olympics opening ceremony. His composition "Energy Flow" (1999) was the first instrumental number-one single in Japan's Oricon charts history.
"Merry Christmas Mr. Lawrence" (1983) marked his debut as a film score composer and as an actor. The film's score received a BAFTA Award, and its main theme was adapted into a pop single entitled "Forbidden Colours" which became a worldwide hit. For his work as a film composer, he has won a Golden Globe Award for "The Sheltering Sky" (1990), plus another Golden Globe, Grammy, and Academy Award for "The Last Emperor" (1987). In 2009, he was awarded the Ordre des Arts et des Lettres from France's Ministry of Culture for his musical contributions. On occasion, Sakamoto has also worked on anime and video games as a composer as well as a scenario writer.
Career.
1970s.
Sakamoto entered the Tokyo National University of Fine Arts and Music in 1970, earning a B.A. in music composition and an M.A. with special emphasis on both electronic and ethnic music. He studied ethnomusicology there with the intention of becoming a researcher in the field, due to his interest in various world music traditions, particularly the Japanese (especially Okinawan), Indian and African musical traditions. He was also trained in classical music and began experimenting with the electronic music equipment available at the university, including synthesizers such as the Buchla, Moog, and ARP. One of Sakamoto's classical influences was Claude Debussy, who he described as his "hero" and stated that “Asian music heavily influenced Debussy, and Debussy heavily influenced me. So, the music goes around the world and comes full circle.”
After working as a session musician with Haruomi Hosono and Yukihiro Takahashi in 1977, the trio formed the internationally successful electronic music band Yellow Magic Orchestra (YMO) in 1978. Known for their seminal influence on electronic music, the group helped pioneer electronic genres such as electropop/technopop, synthpop, cyberpunk music, ambient house, and electronica. The group's work has had a lasting influence across genres, ranging from hip hop and techno to acid house and general melodic music. Sakamoto was the songwriter and composer for a number of the band's hit songs—including "Yellow Magic (Tong Poo)" (1978), "Technopolis" (1979), "Nice Age" (1980), "Ongaku" (1983) and "You've Got to Help Yourself" (1983)—while playing keyboards for many of their other songs, including international hits such as "Computer Game/Firecracker" (1978) and "Rydeen" (1979). He also sang on several songs, such as "Kimi ni Mune Kyun" (1983). Sakamoto's composition "Technopolis" (1979) was credited as a contribution to the development of techno music, while the internationally successful "Behind the Mask" (1978)—a synthpop song in which he sang vocals through a vocoder—was later covered by a number of international artists, including Michael Jackson and Eric Clapton.
Sakamoto released his first solo album "Thousand Knives of Ryūichi Sakamoto" in mid-1978 with the help of Hideki Matsutake—Hosono also contributed to the song "Thousand Knives". The album experimented with different styles, such as "Thousand Knives" and "The End of Asia"—in which electronic music was fused with traditional Japanese music—while "Grasshoppers" is a more minimalistic piano song. The album was recorded from April to July 1978 with a variety of electronic musical instruments, including various synthesizers, such as the KORG PS-3100, a polyphonic synthesizer; the Oberheim Eight-Voice; the Moog III-C; the Polymoog, the Minimoog; the Micromoog; the Korg VC-10, which is a vocoder; the KORG SQ-10, which is an analog sequencer; the Syn-Drums, an electronic drum kit; and the microprocessor-based Roland MC-8 Microcomposer, which is a music sequencer that was programmed by Matsutake and played by Sakamoto. A version of the song "Thousand Knives" was released on the Yellow Magic Orchestra's 1981 album "BGM".
1980s.
In 1980 Sakamoto released the solo album "B-2 Unit", which has been referred to as his "edgiest" record and is known for the electronic song "Riot in Lagos", which is considered an early example of electro music (electro-funk), as Sakamoto anticipated the beats and sounds of electro. Early electro and hip hop artists, such as Afrika Bambaata and Kurtis Mantronik were influenced by the album—especially "Riot in Lagos"—with Mantronik citing the work as a major influence on his electro hip hop group Mantronix. "Riot in Lagos" was later included in Playgroup's compilation album "Kings of Electro" (2007), alongside other significant electro compositions, such as Hashim's "Al-Nafyish" (1983).
According to "Dusted Magazine", Sakamoto's use of squelching bounce sounds and mechanical beats was later incorporated in early electro and hip hop music productions, such as “Message II (Survival)” (1982), by Melle Mel and Duke Bootee; “Magic’s Wand” (1982), by Whodini and Thomas Dolby; Twilight 22’s “Electric Kingdom” (1983); and Kurt Mantronik's "" (1985). The 1980 release of "Riot in Lagos" was listed by "The Guardian" in 2011 as one of the 50 key events in the history of dance music.
Also in 1980, Sakamoto released the single "War Head/Lexington Queen", an experimental synthpop and electro record, and began a long-standing collaboration with David Sylvian, when he co-wrote and performed on the Japan track "Taking Islands In Africa". In 1982, Sakamoto worked on another collaboration with Sylvian, a single entitled "Bamboo Houses/Bamboo Music". Sakamoto's 1980 collaboration with Kiyoshiro Imawano, "Ikenai Rouge Magic", topped the Oricon singles chart.
Sakamoto released a number of solo albums during the 1980s. While primarily focused on the piano and synthesizer, this series of albums included collaborations with artists such as Sylvian, David Byrne, Thomas Dolby, Nam June Paik and Iggy Pop. Sakamoto would alternate between exploring a variety of musical styles, ideas and genres—captured most notably in his 1983 album "Illustrated Musical Encyclopedia"—and focusing on a specific subject or theme, such as the Italian Futurism movement in "Futurista" (1986). For the song "Broadway Boogie Woogie", Sakamoto liberally used samples from Ridley Scott's film "Blade Runner" and blended them with raucous, sax-driven techno-pop.
As his solo career began to extend outside Japan in the late 1980s, Sakamoto's explorations, influences and collaborators also developed further. "Beauty" (1989) features a tracklist that combines pop with traditional Japanese and Okinawan songs, as well as guest appearances by Jill Jones, Robert Wyatt, Brian Wilson and Robbie Robertson. "Heartbeat" (1991) and "Sweet Revenge" (1994) features Sakamoto's collaborations with a global range of artists such as Roddy Frame, Dee Dee Brave, Marco Prince, Arto Lindsay, Youssou N'Dour, David Sylvian and Ingrid Chavez.
1990s.
In 1995 Sakamoto released "Smoochy", described by the "Sound On Sound" website as Sakamoto's "excursion into the land of easy-listening and Latin", followed by the "1996" album, which featured a number of previously released pieces arranged for solo piano, violin and cello. During the December of 1996 Sakamoto, composed the entirety of an hour-long orchestral work entitled "Untitled 01" and released as the album "Discord" (1998). The Sony Classical release of "Discord" was sold in a jewel case that was covered by a blue-colored slipcase made of foil, while the CD also contained a data video track. In 1998 the Ninja Tune record label released the "Prayer/Salvation Remixes", for which prominent electronica artists such as Ashley Beedle and Andrea Parker remixed sections from the "Prayer" and "Salvation" parts of "Discord". Sakamoto collaborated primarily with guitarist David Torn and DJ Spooky—artist Laurie Anderson provides spoken word on the composition—and the recording was condensed from nine live performances of the work, recorded during a Japanese tour. "Discord" was divided into four parts: "Grief", "Anger", "Prayer" and "Salvation"; Sakamoto explained in 1998 that he was "not religious, but maybe spiritual" and "The Prayer is to anybody or anything you want to name." Sakamoto further explained:
The themes of Prayer and Salvation came out of the feelings of sadness and frustration that I expressed in the first two movements, about the fact that people are starving in the world, and we are not able to help them. People are dying, and yet the political and economical and historical situations are too complicated and inert for us to do much about it. So I got really angry with myself. I asked myself what I could do, and since there's not a lot I can do on the practical level, all that's left for me is to pray. But it's not enough just to pray; I also had to think about actually saving those people, so the last movement is called Salvation. That's the journey of the piece.
In 1998 Italian ethnomusicologist Massimo Milano published "Ryuichi Sakamoto. Conversazioni" through the Padova, Arcana imprint. All three editions of the book were published in the Italian language. Sakamoto's next album, "BTTB" (1998)—an acronym for "Back to the Basics"—was a fairly opaque reaction to the prior year's multilayered, lushly orchestrated "Discord". The album comprised a series of original pieces on solo piano, including "Energy Flow" (a major hit in Japan) and a frenetic, four-hand arrangement of the Yellow Magic Orchestra classic "Tong Poo". On the "BTTB" U.S. tour, he opened the show performing a brief avant-garde DJ set under the stage name DJ Lovegroove.
1999 saw the long-awaited release of Sakamoto's "opera" "LIFE". It premiered with seven sold-out performances in Tokyo and Osaka. This ambitious multi-genre multi-media project featured contributions by over 100 performers, including Pina Bausch, Bernardo Bertolucci, Josep Carreras, His Holiness The Dalai Lama and Salman Rushdie.
2000s–10s.
Sakamoto teamed with cellist Jaques Morelenbaum (a member of his "1996" trio), and Morelenbaum's wife, Paula, on a pair of albums celebrating the work of bossa nova pioneer Antonio Carlos Jobim. They recorded their first album, "Casa" (2001), mostly in Jobim's home studio in Rio de Janeiro, with Sakamoto performing on the late Jobim's grand piano. The album was well received, having been included in the list of New York Times's top albums of 2002.
Sakamoto collaborated with Alva Noto (an alias of Carsten Nicolai) to release "Vrioon", an album of Sakamoto's piano clusters treated by Nicolai's unique style of digital manipulation, involving the creation of "micro-loops" and minimal percussion. The two produced this work by passing the pieces back and forth until both were satisfied with the result. This debut, released on German label Raster-Noton, was voted record of the year 2004 in the electronica category by British magazine The Wire. They then released "Insen" (2005) – while produced in a similar manner to Vrioon, this album is somewhat more restrained and minimalist.
In 2005, Finnish mobile phone manufacturer Nokia hired Sakamoto to compose ring and alert tones for their high-end phone, the Nokia 8800. A recent reunion with YMO pals Hosono and Takahashi also caused a stir in the Japanese press. They released a single "Rescue" in 2007 and a DVD "HAS/YMO" in 2008. In July 2009 Sakamoto was honored as Officier of Ordre des Arts et des Lettres at the French Embassy in Tokyo.
In 2013 Sakamoto was a jury member at the 70th Venice International Film Festival. The jury viewed 20 films and was chaired by filmmaker Bernardo Bertolucci.
Hiatus.
On July 10, 2014 Sakamoto released a statement indicating that he had been diagnosed with oropharyngeal cancer in late June of the same year. He announced a break from his work while he sought treatment and recovery.
Production work.
Sakamoto's production credits represent a prolific career in this role. In 1983, he produced Mari Iijima's debut album "Rosé", the same year that the Yellow Magic Orchestra was disbanded. Sakamoto subsequently worked with artists such as Thomas Dolby; Aztec Camera, on the "Dreamland" (1993) album; and Imai Miki, co-producing her 1994 album "A Place In The Sun".
Frame, who worked with Sakamoto under the Aztec Camera moniker, explained in a 1993 interview preceding the release of "Dreamland" that he needed to wait a lengthy period of time before he was able to work with Sakamoto, who wrote two soundtracks, a solo album and the music for the opening ceremony at the Barcelona Olympics, prior to working with Frame over four weeks in a New York, United States (US) studio. Frame explained that he was impressed by the work of YMO and the "Merry Christmas Mr Lawrence" soundtrack, explaining: "That's where you realise that the atmosphere around his compositions is actually in the writing - it's got nothing to do with synthesisers." Frame's decision to ask Sakamoto was finalized after he saw his performance at the Japan Festival that was held in London, United Kingdom. Of his experience recording with Sakamoto, Frame said:
He's got this reputation as a boffin, a professor of music who sits in front of a computer screen. But he's more intuitive than that, and he's always trying to corrupt what he knows. Halfway through the day in the studio, he will stop and play some hip hop or some house for 10 minutes, and then go back to what he was doing. He's always trying to trip himself up like that, and to discover new things. Just before we worked together he'd been out in Borneo, I think, with a DAT machine, looking for new sounds.
Film work.
Moviegoers may recognize Sakamoto primarily through his score work on two films: Nagisa Oshima's "Merry Christmas Mr. Lawrence" (1983), including the title theme and the duet "Forbidden Colours" with David Sylvian, and Bernardo Bertolucci's "The Last Emperor" (1987), the latter of which earned him the Academy Award with fellow composers David Byrne and Cong Su. In that same year he composed the score to the cult-classic anime film "".
Frequent collaborator David Sylvian contributed lead vocals to "Forbidden Colours" – the main theme to "Merry Christmas Mr. Lawrence" – which became a minor hit. Sixteen years later, the piece resurfaced as a popular dance track called "Heart of Asia" (by the group Watergate).
Other films scored by Sakamoto include Pedro Almodóvar's "Tacones lejanos (High Heels)" (1991), Bertolucci's "The Little Buddha" (1993), Oliver Stone's "Wild Palms" (1993), John Maybury's "" (1998), Brian De Palma's "Snake Eyes" (1998) and "Femme Fatale" (2002), Oshima's "Gohatto" (1999), and Kiran Rao's "Dhobi Ghat" (2011). He also composed the score of the opening ceremony for the 1992 Summer Olympics in Barcelona, Spain, telecast live to an audience of over a billion viewers.
Several tracks from Sakamoto's earlier solo albums have also appeared in film soundtracks. In particular, variations of "Chinsagu No Hana" (from "Beauty") and "Bibo No Aozora" (from "1996") provide the poignant closing pieces for Sue Brooks's "Japanese Story" (2003) and Alejandro González Iñárritu's "Babel" (2006), respectively.
Sakamoto has also acted in several films: perhaps his most notable performance was as the conflicted Captain Yonoi in "Merry Christmas Mr Lawrence", alongside Takeshi Kitano and British rock singer David Bowie. He also played roles in "The Last Emperor" (as Masahiko Amakasu) and Madonna's "Rain" music video.
Activism.
Sakamoto is a member of the anti-nuclear organization Stop Rokkasho and has demanded the closing of the Hamaoka Nuclear Power Plant. In 2012, he organized the "No Nukes 2012" concert, which featured performances by 18 groups, including Yellow Magic Orchestra and Kraftwerk. Sakamoto is also known as a critic of copyright law, arguing in 2009 that it is antiquated in the information age. He argued that in "the last 100 years, only a few organizations have dominated the music world and ripped off both fans and creators" and that "with the internet we are going back to having tribal attitudes towards music."
Commmons.
In 2006 Sakamoto, in collaboration with Japan's largest independent music company Avex Group, founded Commmons (コモンズ, Komonzu), a record label seeking to change the manner in which music is produced. Sakamoto has explained that Commmons is not his label, but is a platform for all aspiring artists to join as equal collaborators, to share the benefits of the music industry. On the initiative's "About" page, the label is described as a project that "aims to find new possibilities for music, while making meaningful contribution to culture and society." The name "Commmons" is spelt with three "m"s because the third "m" stands for music.
Personal life.
Sakamoto's first of two marriages occurred in 1972, but ended in divorce two years later—Sakamoto has a daughter from this relationship. Sakamoto then married popular Japanese pianist and singer Akiko Yano in 1982, following several musical collaborations with her, including touring work with the Yellow Magic Orchestra. Sakamoto's second marriage ended in August 2006, 14 years after a mutual decision to live separately—Yano and Sakamoto raised one daughter, J-pop singer Miu Sakamoto.
Awards.
Sakamoto has won a number of awards for his work as a film composer, beginning with his score for "Merry Christmas, Mr. Lawrence" (1983) winning him the BAFTA Award for Best Film Music. His greatest award success was for scoring "The Last Emperor" (1987), which won him the Academy Award for Best Original Score, Golden Globe Award for Best Original Score, and Grammy Award for Best Score Soundtrack Album for a Motion Picture, Television or Other Visual Media, as well as a BAFTA nomination.
His score for "The Sheltering Sky" (1990) later won him his second Golden Globe Award, and his score for "Little Buddha" (1993) received another Grammy Award nomination. In 1997, his collaboration with Toshio Iwai, "Music Plays Images X Images Play Music", was awarded the Golden Nica, the grand prize of the Prix Ars Electronica competition. He also contributed to the Academy Award winning soundtrack for "Babel" (2006) with several pieces of music, including the "Bibo no Aozora" closing theme. In 2009, he was awarded the Ordre des Arts et des Lettres from France's Ministry of Culture for his musical contributions.
The music video for "Risky", written and directed by Meiert Avis, also won the first ever MTV "Breakthrough Video Award". The ground breaking video explores transhumanist philosopher FM-2030's (Persian: فریدون اسفندیاری) ideas of "Nostalgia for the Future", in the form of an imagined love affair between a robot and one of Man Ray's models in Paris in the late 1930s. Additional inspiration was drawn from Jean Baudrillard, Edvard Munch's 1894 painting "Puberty", and Roland Barthes "Death of the Author". The surrealist black and white video uses stop motion, light painting, and other retro in-camera effects techniques. Meiert Avis shot Sakamoto while at work on the score for "The Last Emperor" in London. Sakamoto also appears in the video painting words and messages to an open shutter camera. Iggy Pop, who performs the vocals on "Risky", chose not to appear in the video, allowing his performance space to be occupied by the surrealist era robot.
Sakamoto won the Golden Pine Award (Lifetime Achievement) at the 2013 International Samobor Film Music Festival, along with Clint Eastwood and Gerald Fried.
Discography.
Studio albums.
Several albums exist in 2 versions: the original Japanese version; and the international version, which contains a different tracklist.

</doc>
<doc id="26273" url="http://en.wikipedia.org/wiki?curid=26273" title="Roger the Dodger">
Roger the Dodger

Roger the Dodger is a fictional character featured regularly in the UK comic "The Beano". His strip consists solely of Roger's basic remit to avoid doing chores and homework which usually involves him concocting complex and ultimately disastrous plans, the undoing of which results in him being punished (usually by his long-suffering father). To perform these tasks he enlists the help of his many 'dodge books'.
Character history.
He first appeared in issue 561, dated 18 April 1953. His appearance is vaguely similar to that of Dennis the Menace; he wears a black-and-red chequered jumper, black trousers and takes better care of his hair than his equally mischievous counterpart. He also used to have a white tie, but it has seems to have disappeared. Originally drawn by Ken Reid, Gordon Bell took over in 1959, but Roger dodged his way out of the Beano in 1960. He returned, drawn by Bob McGrath, in April 1961. Ken Reid was re-commissioned to draw the strip in 1962, and Robert Nixon when Reid left DC Thomson in 1964. When Nixon left in 1973, Tom Lavery began drawing the strip, who was then followed by Frank McDiarmid in 1976.
Ten years later, after Euan Kerr took over as Beano editor, Nixon returned, drawing in a noticeably different style than before. Roger's strip was given a second page in 1986. Between 1986 and 1992, a spin-off strip appeared at the end called Roger the Dodger's Dodge Clinic. Readers would write in with problems, and Roger would try and find a dodge for it (which would usually go wrong). Winning suggestions would win a transistor radio and special scroll. Roger is often shown in other Beano characters' stories offering "help", which he took to a new level in Beano issue 2648 from April 1993. This issue marked Roger's 40th birthday, and in order to celebrate he made appearances in every strip in the comic.
Nixon continued drawing it until his death in October 2002, though due to the strips being drawn months in advance, his strips continued appearing in the Beano until the end of January 2003, when artist Barrie Appleby took over. He drew the strip until 2011, when he stopped to concentrate on Dennis and Gnasher, though Trevor Metcalfe drew a few strips in 2003 and 2004, and there have also been some Robert Nixon reprints during 2005 and 2006. Since Appleby stopped drawing Roger, the comic has run reprints of Robert Nixon strips from the 1980s. Along with the Nixon reprints, Roger's Dodge Diary was introduced on the second half of Roger's pages, where Beano readers can send in their own dodges. In each one, Roger says a good thing, a bad thing and the results of the dodge. When the Beano was revamped on 8 August 2012, Appleby started drawing Roger again and Roger's parents were made younger. In the 75th birthday issue released on 24 July 2013, Jamie Smart took over as artist. On 9 April 2014, Wayne Thompson replaced Jamie Smart as Roger's artist, until Barrie Appleby returned to draw the strip temporarily, before Wayne Thompson surprisingly returned. 
Roger is currently the second longest-running character in the Beano, behind only Dennis the Menace. However if taken into account the strip's absence in 1960 then he would be the third longest-running behind Minnie the Minx.
Characteristics.
Roger is a crafty-looking ten-year-old boy who sports a red-and-black chequered jersey with a white shirt collar poking out. He is also usually attired in a white tie, however, the Barrie Appleby dropped this around 2005. Upon his debut, Roger sported the usual school boy shorts, however he began wearing trousers during the 1970s.
Roger, unlike most Beano characters, does not go out of his way to cause chaos and mayhem. Instead, he chooses to watch from the sidelines, dodging responsibilities and punishments.
Timeline.
18 April 1953: Roger The Dodger made his debut in issue 561, drawn by Ken Reid.
1959: Gordon Bell becomes the artist.
1960: Roger's first series ends.
April 1961: Roger returns to the Beano, drawn by Bob McGrath.
1962: Reid is artist again.
1964: Robert Nixon takes over.
1973: Tom Lavery takes over.
1976: Frank McDiarmid takes over.
1986: Nixon returns to draw Roger again, he moves to two pages and Roger The Dodger's Dodge Clinic is introduced.
1992: Roger The Dodger's Dodge Clinic ends.
April 1993: Roger's 40th anniversary is celebrated.
January 2003: Barrie Appleby takes over, after Nixon's death.
2011: Appleby stops drawing Roger to focus on Dennis and Gnasher, and Roger's Dodge Diary is introduced alongside Nixon reprints.
2012: Appleby resumes drawing Roger after Nigel Parkinson takes over Dennis and Gnasher.
July 2013: Jamie Smart takes over as artist.
April 2014: Wayne Thompson takes over as artist.
July 2014: Barrie Appleby returns as artist.
References.
<noinclude
This template is used in hundreds of articles. Please think carefully before changing it, but feel free to add relevant characters or people, as long as you explain your edits in your . Any unexplained changes may be reverted.
</noinclude>

</doc>
<doc id="26275" url="http://en.wikipedia.org/wiki?curid=26275" title="Robert Nozick">
Robert Nozick

Robert Nozick (; November 16, 1938 – January 23, 2002) was an American philosopher who was most prominent in the 1970s and 1980s. He was a professor at Harvard University. He is best known for his book "Anarchy, State, and Utopia" (1974), a libertarian answer to John Rawls' "A Theory of Justice" (1971). His other work involved decision theory and epistemology.
Personal life.
Nozick was born in Brooklyn, the son of a Jewish entrepreneur from the Russian shtetl who had been born with the name of Cohen. Nozick was married to the poet Gjertrud Schnackenberg. He died in 2002 after a prolonged struggle with stomach cancer. He is interred at Mount Auburn Cemetery in Cambridge, Massachusetts.
Career and works.
Nozick was educated at Columbia (A.B. 1959, "summa cum laude"), where he studied with Sidney Morgenbesser, and later at Princeton (Ph.D. 1963) under Carl Hempel, and at Oxford as a Fulbright Scholar (1963–1964).
Political philosophy.
For "Anarchy, State, and Utopia" (1974) Nozick received a National Book Award in category Philosophy and Religion.
There, Nozick argues that only a minimal state "limited to the narrow functions of protection against force, theft, fraud, enforcement of contracts, and so on" could be justified without violating people's rights. For Nozick, a distribution of goods is just if brought about by free exchange among consenting adults from a "just" starting position, even if large inequalities subsequently emerge from the process. Nozick appealed to the Kantian idea that people should be treated as ends (what he termed 'separateness of persons'), not merely as a means to some other end.
Nozick challenged the partial conclusion of John Rawls' Second Principle of Justice of his "A Theory of Justice", that "social and economic inequalities are to be arranged so that they are to be of greatest benefit to the least-advantaged members of society." "Anarchy, State and Utopia" claims a heritage from John Locke's "Second Treatise on Government" and seeks to ground itself upon a natural law doctrine, but reaches some importantly different conclusions from Locke himself in several ways.
Most controversially, Nozick argued that a consistent upholding of the non-aggression principle would allow and regard as valid consensual or non-coercive enslavement contracts between adults. He rejected the notion of inalienable rights advanced by Locke and most contemporary capitalist-oriented libertarian academics, writing in "Anarchy, State and Utopia" that the typical notion of a "free system" would allow adults to voluntarily enter into non-coercive slave contracts.
Epistemology.
In "Philosophical Explanations" (1981), which received the Phi Beta Kappa Society's Ralph Waldo Emerson Award, Nozick provided novel accounts of knowledge, free will, personal identity, the nature of value, and the meaning of life. He also put forward an epistemological system which attempted to deal with both the Gettier problem and those posed by skepticism. This highly influential argument eschewed justification as a necessary requirement for knowledge.:ch. 7
Nozick's Four Conditions for S's knowing that P were:
Nozick's third and fourth conditions are counterfactuals. He called this the "tracking theory" of knowledge. Nozick believed the counterfactual conditionals bring out an important aspect of our intuitive grasp of knowledge: For any given fact, the believer's method must reliably track the truth despite varying relevant conditions. In this way, Nozick's theory is similar to reliabilism. Due to certain counterexamples that could otherwise be raised against these counterfactual conditions, Nozick specified that:
Where M stands for the method by which S came to arrive at a belief whether or not P.
A major criticism of Nozick's theory of knowledge is his rejection of the principle of deductive closure. This principle states that if S knows X and S knows that X implies Y, then S knows Y. Nozick's truth tracking conditions do not allow for the principle of deductive closure. Nozick believes that the truth tracking conditions are more fundamental to human intuition than the principle of deductive closure.
Unlike many epistemologists, Nozick does not attempt to disprove the skeptic. Instead, he qualifies using his truth tracking theory of knowledge. For example, the skeptic may argue that one cannot know that they are not a brain in a vat. Nozick argues that one can know that they are not a brain in a vat. He simultaneously argues that in another sense, they can not know that they are not a brain in a vat. Specifically, he uses G. E. Moore's Here is one hand thought experiment. Assume one has a true belief that they have hands. Furthermore, in nearby possible worlds in which one continues to have hands, one continues to have hands. Moreover, in nearby possible worlds in which one no longer has hands, they stop believing that they have hands. Thus, they know that they have hands.
Nonetheless, Nozick admits that he cannot overcome the skeptic. Assuming one is not actually a brain in a vat, the third condition cannot be satisfied. In other words, one does not stop believing that they are not a brain in a vat in nearby possible worlds in which they are a vatted brain. If one does not know that they are not a brain in a vat, then they do not know that they have hands. Thus, Nozick argues that one can know that they have hands and also not know that they are not a handless brain in a vat.
Later books.
"The Examined Life" (1989), pitched to a broader public, explores love, death, faith, reality, and the meaning of life. According to Stephen Metcalf, Nozick expresses serious misgivings about capitalist libertarianism, going so far as to reject much of the foundations of the theory on the grounds that personal freedom can sometimes only be fully actualized via a collectivist politics and that wealth is at times justly redistributed via taxation to protect the freedom of the many from the potential tyranny of an overly selfish and powerful few. Nozick suggests that citizens opposed to wealth redistribution that funds programs they object to should be able to opt out by supporting alternative government approved charities with an added 5% surcharge. However, Jeff Riggenbach has noted that "...in an interview conducted in July 2001, he stated that he had never stopped self-identifying as a libertarian. And Roderick Long reports that in his last book, "Invariances", [Nozick] identified voluntary cooperation as the 'core principle' of ethics, maintaining that the duty not to interfere with another person's 'domain of choice' is '[a]ll that any society should (coercively) demand'; higher levels of ethics, involving positive benevolence, represent instead a 'personal ideal' that should be left to 'a person's own individual choice and development.' And that certainly sounds like an attempt to embrace libertarianism all over again. My own view is that Nozick's thinking about these matters evolved over time and that what he wrote at any given time was an accurate reflection of what he was thinking at that time."
"The Nature of Rationality" (1993) presents a theory of practical reason that attempts to embellish notoriously spartan classical decision theory. "Socratic Puzzles" (1997) is a collection of papers that range in topic from Ayn Rand and Austrian economics to animal rights, while his last production, "Invariances" (2001), applies insights from physics and biology to questions of objectivity in such areas as the nature of necessity and moral value.
Utilitarianism.
Nozick created the thought experiment of the "utility monster" to show that average utilitarianism could lead to a situation where the needs of the vast majority were sacrificed for one individual. He also wrote a version of what was essentially a previously-known thought experiment, The Experience Machine, in an attempt to show that ethical hedonism was false. Nozick asked us to imagine that "superduper neuropsychologists" have figured out a way to stimulate a person's brain to induce pleasurable experiences.:210–11 We would not be able to tell that these experiences were not real. He asks us, if we were given the choice, would we choose a machine-induced experience of a wonderful life over real life? Nozick says no, then asks whether we have reasons not to plug into the machine and concludes that since it does not seem to be rational to plug in, ethical hedonism must be false.
Unusual philosophical method.
Nozick was notable for the exploratory style of his philosophizing and for his methodological ecumenism. Often content to raise tantalizing philosophical possibilities and then leave judgment to the reader, Nozick was also notable for drawing from literature outside of philosophy (e.g., economics, physics, evolutionary biology).

</doc>
<doc id="26276" url="http://en.wikipedia.org/wiki?curid=26276" title="Redirect examination">
Redirect examination

 <ns>10</ns>
 <id>2198199</id>
 <revision>
 <id>646505875</id>
 <parentid>645922985</parentid>
 <timestamp>2015-02-10T15:06:52Z</timestamp>
 <contributor>
 <username>Edokter</username>
 <id>1624037</id>
 </contributor>
 <minor />
 <comment>Reverted edits by () to last version by 24.131.80.54</comment>
 <model>wikitext</model>
 <format>text/x-wiki</format>
Redirect examination is the trial process by which the party who offered the witness has a chance to explain or otherwise qualify any damaging or accusing testimony brought out by the opponent during cross-examination. Redirect examination may question only those areas brought out on cross-examination and may not stray beyond that boundary.
When a witness is presented for testimony in the U.S. judicial system, the order is "direct" testimony, then the opposing attorney does "cross" and then "redirect" from the attorney first offering the witness. "Recross" may be allowed, but usually the opposing attorney must ask for permission from the judge before proceeding with this additional round of questioning.
In Australia, Canada and South Africa the process is called re-examination.

</doc>
<doc id="26277" url="http://en.wikipedia.org/wiki?curid=26277" title="Real property">
Real property

In English common law, real property, real estate, realty, or immovable property is any subset of land that has been legally defined and the improvements to it have been made by human efforts: buildings, machinery, wells, dams, ponds, mines, canals, roads, etc. Real property and personal property are the two main subunits of property in English Common Law.
In countries with personal ownership of real property, civil law protects the status of real property in real-estate markets, where estate agents work in the market of buying and selling real estate. Scottish civil law calls real property "heritable property", and in French-based law, it is called "immobilier".
Historical background.
The word "real" ultimately derives from Latin "res" ("thing") and was used in Middle English to mean "relating to things, especially real property".
In common law, real property was property that could be protected by some form of real action, in contrast to personal property, where a plaintiff would have to resort to another form of action. As a result of this formalist approach, some things the common law deems to be land would not be classified as such by most modern legal systems, for example an advowson (the right to nominate a priest) was real property. By contrast the rights of a leaseholder originate in personal actions and so the common law originally treated a leasehold as part of personal property.
The law now broadly distinguishes between real property (land and anything affixed to it) and personal property (everything else, e.g., clothing, furniture, money). The conceptual difference was between immovable property, which would transfer title along with the land, and movable property, which a person would retain title to.
In modern legal systems derived from English common law, classification of property as real or personal may vary somewhat according to jurisdiction or, even within jurisdictions, according to purpose, as in defining whether and how the property may be taxed.
Bethell (1998) contains much historical information on the historical evolution of real property and property rights.
Identification of real property.
To be of any value a claim to any property must be accompanied by a verifiable and legal property description. Such a description usually makes use of natural or manmade boundaries such as seacoasts, rivers, streams, the crests of ridges, lakeshores, highways, roads, and railroad tracks, and/or purpose-built markers such as cairns, surveyor's posts, fences, official government surveying marks (such as ones affixed by the U.S. Geodetic Survey (USGS)), and so forth. In many cases, a description refers to one or more lots on a plat, a map of property boundaries kept in public records.
Estates and ownership interests defined.
The law recognizes different sorts of interests, called estates, in real property. The type of estate is generally determined by the language of the 5 deed, lease, bill of sale, will, land grant, etc., through which the estate was acquired. Estates are distinguished by the varying property rights that vest in each, and that determine the duration and transferability of the various estates. A party enjoying an estate is called a "tenant."
Some important types of estates in land include:
A tenant enjoying an undivided estate in some property after the termination of some estate of limited term, is said to have a "future interest." Two important types of future interests are:
Estates may be held jointly as joint tenants with rights of survivorship or as tenants in common. The difference in these two types of joint ownership of an estate in land is basically the inheritability of the estate and the shares of interest that each tenant owns.
In a joint tenancy with rights of survivorship deed, or JTWROS, the death of one tenant means that the surviving tenant(s) become the sole owner(s) of the estate. Nothing passes to the heirs of the deceased tenant. In some jurisdictions, the specific words "with right of survivorship" must be used, or the tenancy will assumed to be tenants in common without rights of survivorship. The co-owners always take a JTWROS deed in equal shares, so each tenant must own an equal share of the property regardless of his/her contribution to purchase price. If the property is someday sold or subdivided, the proceeds must be distributed equally with no credits given for any excess than any one co-owner may have contributed to purchase the property.
The death of a co-owner of a tenants in common (TIC) deed will have a heritable portion of the estate in proportion to his ownership interest which is presumed to be equal among all tenants unless otherwise stated in the transfer deed. However, if TIC property is sold or subdivided, in some States, Provinces, etc., a credit can be automatically made for unequal contributions to the purchase price (unlike a partition of a JTWROS deed).
Real property may be owned jointly with several tenants, through devices such as the condominium, housing cooperative, and building cooperative.
Bundle of Rights:
Real property is unique due to the fact that there are multiple "rights" associated with each piece of property. For example, most U.S. jurisdictions recognized the following rights: right to sell, right to lease, right to acquire minerals/gas/oil/etc. within the land, right to use, right to possess, right to develop, etc. These multiple rights are important because the owner of the real property can generally do what he/she chooses with each right. For example, the owner could choose to keep all the rights but lease the right to dig for oil to an oil company. Or the owner could choose to keep all the rights but lease the property to a tenant. In other words, the owner can elect to keep and/or lease and/or sell the rights to his/her land.
Other Ownership types:
Jurisdictional peculiarities.
In the law of almost every country, the state is the ultimate owner of all land under its jurisdiction, because it is the sovereign, or supreme lawmaking authority. Physical and corporate persons do not have allodial title; they do not own land but only enjoy estates in the land, also known as "equitable interests."
Australia and New Zealand.
In many countries the Torrens title system of real estate ownership is managed and guaranteed by the government and replaces cumbersome tracing of ownership. The Torrens title system operates on the principle of "title by registration" (i.e. the indefeasibility of a registered interest) rather than "registration of title." The system does away with the need for a chain of title (i.e. tracing title through a series of documents) and does away with the conveyancing costs of such searches. The State guarantees title and is usually supported by a compensation scheme for those who lose their title due to the State's operation. It has been in practice in all Australian states and in New Zealand since between 1858 and 1875, has more recently been extended to strata title, and has been adopted by many states, provinces and countries, and in modified form in 9 states of the USA.
United Kingdom.
In the United Kingdom, The Crown is held to be the ultimate owner of all real property in the realm. This fact is material when, for example, property has been disclaimed by its erstwhile owner, in which case the law of escheat applies. In some other jurisdictions (not including the United States), real property is held absolutely.
England and Wales.
English law has retained the common law distinction between real property and personal property, whereas the civil law distinguishes between "movable" and "immovable" property. In English law, real property is not confined to the ownership of property and the buildings sited thereon – often referred to as "land." Real property also includes many legal relationships between individuals or owners of land that are purely conceptual. One such relationship is the easement, where the owner of one property has the right to pass over a neighboring property. Another is the various "incorporeal hereditaments," such as "profits-à-prendre", where an individual may have the right to take crops from land that is part of another's estate.
English law retains a number of forms of property which are largely unknown in other common law jurisdictions such as the advowson, chancel repair liability and lordships of the manor. These are all classified as real property, as they would have been protected by real actions in the early common law.
USA.
Each U.S. State except Louisiana has its own laws governing real property and the estates therein, grounded in the common law. In Arizona, real property is generally defined as land and the things permanently attached to the land. Things that are permanently attached to the land, which also can be referred to as "improvements", include homes, garages, and buildings. Manufactured homes can obtain an affidavit of affixture.
Economic aspects of real property.
Land use, land valuation, and the determination of the incomes of landowners, are among the oldest questions in economic theory. Land is an essential input (factor of production) for agriculture, and agriculture is by far the most important economic activity in pre-industrial societies. With the advent of industrialization, important new uses for land emerge, as sites for factories, warehouses, offices, and urban agglomerations. Also, the value of real property taking the form of man-made structures and machinery increases relative to the value of land alone. The concept of real property eventually comes to encompass effectively all forms of tangible fixed capital. with the rise of extractive industries, real property comes to encompass natural capital. With the rise of tourism and leisure, real property comes to include scenic and other amenity values.
Starting in the 1960s, as part of the emerging field of law and economics, economists and legal scholars began to study the property rights enjoyed by tenants under the various estates, and the economic benefits and costs of the various estates. This resulted in a much improved understanding of the:
For an introduction to the economic analysis of property law, see Shavell (2004), and Cooter and Ulen (2003). For a collection of related scholarly articles, see Epstein (2007). Ellickson (1993) broadens the economic analysis of real property with a variety of facts drawn from history and ethnography.
References and further reading.
</dl>

</doc>
<doc id="26278" url="http://en.wikipedia.org/wiki?curid=26278" title="Robert Abbot (theologian)">
Robert Abbot (theologian)

Robert Abbot (1588? - 1662?) was an English theologian who promoted puritan doctrines. He is sometimes mistakenly described as the son of the Archbishop of Canterbury, George Abbot, but this is generally considered to be incorrect. The misunderstanding probably stems from a passage in Robert Abbot's work "A Hand of Fellowship to Helpe Keepe out Sinne and Antichrist", in which he thanks the Archbishop for "worldly maintenance," "best earthly countenance" and "fatherly incouragements."
Biography.
Robert Abbot received his education at Cambridge University, and later at
Oxford University. The details of Abbot's ecclesiastical career are somewhat unclear, and can only be pieced together from fragmentary evidence, but based on something he wrote in his work "Bee Thankfull London and her Sisters", it is probable that he began his church service with a posting as "assistant to a reverend divine". A note in the margin indicates that the reverend in question was "Master Haiward of Wool Church", in Dorset. In 1616, it is known that he was appointed by George Abbot to the vicarage of Cranbrook in Kent. His ministry at Cranbrook was regarded as successful, but he was noted for his lack of tolerance towards nonconformists. In 1643, Abbot left Cranbrook, becoming vicar of Southwick, Hampshire. Later, he became pastor at the "extruded" Udall of St Austin's, in London, where he apparently still served in 1657. Between 1657 and 1658, and in 1662, Abbot appears to vanish from record, and his activities are unknown.
Written works.
Robert Abbot's books are conspicuous amongst the works of his time by their terseness and variety. In addition to those mentioned above he wrote "Triall of our Church-Forsakers" (1639), "Milk for Babes, or a Mother's Catechism for her Children" (1646), and "A Christian Family builded by God, or Directions for Governors of Families" (1653).
References.
 Cited authorities:

</doc>
<doc id="26284" url="http://en.wikipedia.org/wiki?curid=26284" title="Richard III of England">
Richard III of England

Richard III (2 October 1452 – 22 August 1485) was King of England from 1483 until his death in 1485 in the Battle of Bosworth Field. He was the last king of the House of York and the last of the Plantagenet dynasty. His defeat at Bosworth Field, the last decisive battle of the Wars of the Roses, marked the end of the Middle Ages in England. He is the subject of the fictional historical play "Richard III" by William Shakespeare.
When his brother King Edward IV died in April 1483, Richard was named Lord Protector of the realm for Edward's son and successor, the 12-year-old Edward V. As the young king travelled to London from Ludlow, Richard met and escorted him to lodgings in the Tower of London where Edward V's own brother Richard of Shrewsbury joined him shortly afterwards. Arrangements were made for Edward's coronation on 22 June 1483, but before the young king could be crowned, his father's marriage to his mother Elizabeth Woodville was declared invalid, making their children illegitimate and ineligible for the throne. On 25 June, an assembly of lords and commoners endorsed the claims. The following day, Richard III began his reign, and he was crowned on 6 July 1483. The young princes were not seen in public after August, and accusations circulated that the boys had been murdered on Richard's orders, giving rise to the legend of the Princes in the Tower.
Of the two major rebellions against Richard, the first, in October 1483, was led by staunch allies of Edward IV and Richard's former ally, Henry Stafford, 2nd Duke of Buckingham; but the revolt collapsed. In August 1485, Henry Tudor and his uncle, Jasper Tudor, led a second rebellion against Richard. Henry Tudor landed in southern Wales with a small contingent of French troops and marched through his birthplace, Pembrokeshire, recruiting soldiers. Henry's force engaged Richard's army and defeated it at the Battle of Bosworth Field in Leicestershire. Richard was struck down in the conflict, making him the last English king to die in battle on home soil and the first since Harold II was killed at the Battle of Hastings in 1066.
After the battle Richard's corpse was taken to Leicester and buried without pomp. His original tomb is believed to have been destroyed during the Reformation, and his remains were lost for more than five centuries. In 2012, an archaeological excavation was conducted on a city council car park on the site once occupied by Greyfriars Priory Church. The University of Leicester identified the skeleton found in the excavation as that of Richard III as a result of radiocarbon dating, comparison with contemporary reports of his appearance, and comparison of his mitochondrial DNA with that of two matrilineal descendants of Richard III's eldest sister, Anne of York. Richard's remains were reburied in Leicester Cathedral on 26 March 2015.
Childhood.
Richard was born on 2 October 1452 at Fotheringhay Castle, the twelfth of thirteen children of Richard Plantagenet, 3rd Duke of York and Cecily Neville at the beginning of what has traditionally been labelled the "Wars of the Roses," a period of "three or four decades of political instability and periodic open civil war in the second half of the fifteenth century," between supporters of Richard's father (a potential claimant to the throne of King Henry VI from birth),—"Yorkists"—in opposition to the regime of Henry VI and his Queen Margaret of Anjou, and those loyal to the crown ("Lancastrians").
When his father and elder brother Edmund, Earl of Rutland were killed in the Battle of Wakefield on 30 December 1460, Richard, who was eight years old, and his older brother George (later Duke of Clarence) were sent by his mother, the Duchess of York, to the Low Countries. They returned to England following the defeat of the Lancastrians at the Battle of Towton and participated in the coronation of Richard's eldest brother as King Edward IV in June 1461. At this time Richard was named Duke of Gloucester and made a Knight of the Garter and Knight of the Bath; he was involved in the rough politics of the Wars of the Roses from an early age (for example, Edward appointed him the sole Commissioner of Array for the Western Counties in 1464, when he was eleven). By the age of seventeen, he had an independent command.
Richard spent several years during his childhood at Middleham Castle in Wensleydale, Yorkshire, under the tutelage of his cousin Richard Neville, 16th Earl of Warwick (later known as the "Kingmaker" because of his role in the Wars of the Roses) who took care of his knightly training: in autumn 1465 King Edward granted the earl £1000 for the expenses of his younger brother’s tutelage. With some interruptions, Richard stayed at Middleham either from late 1461 until early 1465, when he was twelve or from 1465 until his coming of age in 1468 when he turned sixteen. While at Warwick's estate, he probably met Francis Lovell, a strong supporter later in his life, and Warwick's younger daughter, his future wife Anne Neville,
It is possible that even at this early stage Warwick was considering the king’s brothers as strategic matches for his daughters, Isabel and Anne: young aristocrats were often sent to be raised in the households of their intended future partners, as had been the case for the young dukes’ father, Richard of York. As the relationship between the king and Warwick became strained, Edward IV opposed the match. During Warwick’s lifetime, George was the only royal brother to marry one of his daughters, the eldest, Isabel, on 12 July 1469, without the king's permission. George joined his father-in-law's revolt against the king, while Richard remained loyal to Edward, even though rumour coupled Richard’s name with Anne Neville until August 1469.
Richard and Edward were forced to flee to Burgundy in October 1470 after Warwick defected to the side of the former Lancastrian Queen Margaret of Anjou and for a second time, Richard was forced to seek refuge in the Low Countries, which were part of the realm of the Duchy of Burgundy. In 1468, Richard's sister Margaret had married Charles the Bold, the Duke of Burgundy, and the brothers could expect a welcome there. Although only eighteen years old, Richard played crucial roles in the battles of Barnet and Tewkesbury that resulted in Edward's restoration to the throne in spring 1471.
During his adolescence, Richard developed idiopathic scoliosis. In 2014 the osteoarchaeologist Dr Jo Appleby, of Leicester University's School of Archaeology and Ancient History, imaged the spinal column and reconstructed a model using 3D printing, and concluded that though the spinal scoliosis looked dramatic, it probably did not cause any major physical deformity that could not be disguised by clothing.
Marriage and family relationships.
Following a decisive Yorkist victory over the Lancastrians at the Battle of Tewkesbury, Richard married Anne Neville, the younger daughter of the Earl of Warwick, on 12 July 1472. By the end of 1470 Anne had previously been wedded to Edward of Westminster, only son of Henry VI, to seal her father's allegiance with the Lancastrian party. Edward died at the Battle of Tewkesbury on 4 May 1471, while Warwick had died at the Battle of Barnet on 14 April 1471. Richard's marriage plans brought him into conflict with his brother George: John Paston’s letter of 17 February 1472 makes it clear that George was not happy about the marriage but grudgingly accepted it on the basis that "he may well have my Lady his sister-in-law, but they shall part no livelihood". The reason was the inheritance Anne shared with her elder sister Isabel, whom George had married in 1469. It was not only the earldom that was at stake; Richard Neville had inherited it as a result of his marriage to Anne Beauchamp, who was still alive (and outlived both her daughters) and was technically the owner of the substantial Beauchamp estates, her own father having left no male heirs.
The Croyland Chronicle records that Richard agreed to a prenuptial contract in the following terms: "the marriage of the Duke of Gloucester with Anne before-named was to take place, and he was to have such and so much of the earl's lands as should be agreed upon between them through the mediation of arbitrators; while all the rest were to remain in the possession of the Duke of Clarence".
The date of Paston’s letter suggests the marriage was still being negotiated in February 1472. In order to win his brother George’s final consent to the marriage, Richard renounced most of Warwick’s land and property including the earldoms of Warwick (which the Kingmaker had held in his wife’s right) and Salisbury and surrendered to Clarence the office of Great Chamberlain of England, while he retained Neville’s forfeit estates he had already been granted in the summer of 1471: Penrith, Sheriff Hutton and Middleham, where he later established his marital household.
The requisite Papal dispensation was obtained dated 22 April 1472. Michael Hicks has suggested that the terms of the dispensation deliberately understated the degrees of consanguinity between the couple, and the marriage was therefore illegal on the ground of first degree consanguinity following George's marriage to Anne's sister Isabel. First degree consanguinity applied in the case of Henry VIII and his brother's widow Catherine of Aragon. In their case the papal dispensation was obtained after Catherine declared the first marriage had not been consummated. In Richard's case, there would have been first degree consanguinity if Richard had sought to marry Isabel (in case of widowhood) after she had married his brother George, but no such consanguinity applied for Anne and Richard. Richard's marriage to Anne was never declared null, and it was public to everyone including secular and canon lawyers for 13 years.
In June 1473, Richard persuaded his mother-in-law to leave sanctuary and come to live under his protection at Middleham. Later in the year, under the terms of the 1473 Act of Resumption, George lost some of the property he held under royal grant, and made no secret of his displeasure. John Paston's letter of November 1473 says that the king planned to put both his younger brothers in their place by acting as "a stifler atween them".
Early in 1474, Parliament assembled and King Edward attempted to reconcile his brothers by stating that both men, and their wives, would enjoy the Warwick inheritance just as if the Countess of Warwick "was naturally dead". The doubts cast by Clarence on the validity of Richard and Anne's marriage were addressed by a clause protecting their rights in the event they were divorced (i.e. of their marriage being declared null and void by the Church) and then legally remarried to each other, and also protected Richard's rights while waiting for such a valid second marriage with Anne. The following year, Richard was rewarded with all the Neville lands in the north of England, at the expense of Anne's cousin, George Neville. From this point, George seems to have fallen steadily out of King Edward's favour, his discontent coming to a head in 1477 when, following Isabel's death, he was denied the opportunity to marry Mary of Burgundy, the stepdaughter of his sister Margaret, even though Margaret approved the proposed match. There is no evidence of Richard's involvement in George's subsequent conviction and execution on a charge of treason.
Reign of Edward IV.
Estates and titles.
Richard was granted the dukedom of Gloucester on 1 November 1461, and on 12 August the next year was awarded large estates in northern England, including the lordships of Richmond in Yorkshire, and Pembroke in Wales. He gained the forfeited lands of the Lancastrian John de Vere, Earl of Oxford, in East Anglia. In 1462, on his birthday, he was made Constable of Gloucester and Corfe Castles and Admiral of England, Ireland and Aquitaine and appointed Governor of the North, becoming the richest and most powerful noble in England. On 17 October 1469, he was made Constable of England. In November, he replaced William Hastings, 1st Baron Hastings, as Chief Justice of North Wales. The following year, he was appointed Chief Steward and Chamberlain of Wales. On 18 May 1471, Richard was named Great Chamberlain and Lord High Admiral of England. Other positions followed: High Sheriff of Cumberland for life, Lieutenant of the North and Commander-in Chief against the Scots and hereditary Warden of the West March. Two months later, on 14 July, he gained the Lordships of the strongholds Sheriff Hutton and Middleham in Yorkshire and Penrith in Cumberland, which had belonged to Warwick the Kingmaker. It is possible that the grant of Middleham seconded Richard's personal wishes. However, any personal attachment he may have felt to Middleham was likely mitigated in his adulthood, as surviving records demonstrate he spent less time there than at Barnard Castle and Pontefract.
Exile and return.
During the latter part of the reign of Edward IV, Richard demonstrated his loyalty, in contrast to their brother George, who had allied himself with Warwick through the 1460s, and threw in his lot with the earl when the latter rebelled at the end of the decade. Following Warwick's 1470 rebellion, in which he made peace with Margaret of Anjou and promised the restoration of Henry VI to the English throne, Richard, William, Lord Hastings and Anthony Woodville, Earl Rivers escaped capture at Doncaster by Warwick's brother, Lord Montagu. On 2 October they sailed from King's Lynn in two ships; Edward landed at Marsdiep and Richard at Zeeland. It was said that, having left England in such haste as to possess almost nothing, Edward was forced to pay their passage with his fur cloak; certainly Richard borrowed three pounds from Zeeland's town-bailiff. They were attainted by Warwick's only Parliament on 26 November. They resided in Bruges with Louis de Gruthuse, who had been the Burgundian Ambassador to Edward's court, but it was not until Louis XI of France declared war on Burgundy that Charles, Duke of Burgundy, assisted their return, providing, along with the Hanseatic merchants, £20,000, 36 ships and 1200 men. They departed Flushing for England on 11 March 1471. Warwick's arrest of local sympathisers prevented them from landing in Yorkist East Anglia and on 14 March, after being separated in a storm, their ships ran ashore at Holderness. The town of Hull refused him entry, and Edward gained entry to York by using the same claim as Henry of Bolingbroke had before deposing Richard II in 1399; "viz", that he was merely reclaiming the Dukedom of York rather than the crown. It was in Edward's attempt to regain his throne that Gloucester began to demonstrate his skill as a military commander.
1471 military campaign.
Once Edward had regained the support of Clarence, he mounted a swift and decisive campaign to regain the Crown through combat; it is believed that Richard was his principal lieutenant as some of the king's earliest support came from members of Richard's affinity, including Sir James Harrington and Sir William Parr, who brought 600 men-at-arms to them at Doncaster. He may have led the vanguard at the Battle of Barnet, in his first command, on 14 April 1471, where he successfully outflanked the Duke of Exeter's wing, although the degree to which his command was fundamental may have been exaggerated. That his personal household sustained losses indicates he was in the thick of the fighting. A contemporary source is clear about his holding the vanguard for Edward at Tewkesbury, deployed against the Lancastrian vanguard under the Duke of Somerset on 4 May 1471, and his role two days later, as Constable of England, sitting alongside John Howard as Earl Marshal, in the trial and sentencing of leading Lancastrians captured after the battle.
1475 invasion of France.
In part at least resentful of the French king's previous support of his Lancastrian opponents, and possibly in support of his brother-in-law Charles the Bold, duke of Burgundy, Edward went to parliament in October 1472 for funding a military campaign, and eventually landed in Calais on 4 July 1475. Gloucester's was the largest private contingent of his army. Although well known to have publicly been against the eventual treaty signed with Louis XI at Picquigny (and absent from the negotiations, in which one of his rank would have been expected to take a leading role), he acted as Edward's witness when the king instructed his delegates to the French court, and received 'some very fine presents' from Louis on a visit to the French king at Amiens. In refusing other gifts, which included 'pensions' in the guise of 'tribute', he was joined only by Cardinal Bourchier. He supposedly disapproved of Edward's policy of personally benefitting—politically and financially—from a campaign paid for out of a parliamentary grant, and hence out of public funds. Any military prowess was therefore not be revealed further until the last years of Edward's reign.
Council of the North.
Richard controlled the north of England until Edward IV's death. There, and especially in the city of York, he was highly regarded; although it has been questioned whether this view was reciprocated by Richard. Edward IV set up the Council of the North as an administrative body in 1472 to improve government control and economic prosperity and benefit the whole of Northern England. Kendall and later historians have suggested that this was with the intention of making Richard the "Lord of the North"; Peter Booth, however, has argued that "instead of allowing his brother the Duke of Gloucester "carte blanche", [Edward] restricted his influence by using his own agent, Sir William Parr." Richard served as its first Lord President from 1472 until his accession to the throne. On his accession, he made his nephew John de la Pole, 1st Earl of Lincoln, president and formally institutionalised it as an offshoot of the royal Council; all its letters and judgements were issued on behalf of the king and in his name. The council had a budget of 2000 marks per annum (approximately £1320) and had issued "Regulations" by July of that year: councillors to act impartially and declare vested interests, and to meet at least every three months. Its main focus of operations was Yorkshire and the north-east, and its primary responsibilities were land disputes, keeping of the king's peace, and punishing lawbreakers.
War with Scotland.
Richard's increasing role in the north from the mid-1470s to some extent explains his withdrawal from the royal court. He had been Warden of the West March on the Scottish border since 10 September 1470, and again from May 1471; he used Penrith as a base while 'taking effectual measures' against the Scots, and 'enjoyed the revenues of the estates' of the Forest of Cumberland while doing so. It was at the same time that the duke was appointed sheriff of Cumberland five consecutive years, being described as 'of Penrith Castle' in 1478. By 1480, war with Scotland was looming; on 12 May that year he was appointed Lieutenant-General of the North (a position created for the occasion) as fears of a Scottish invasion grew. Louis XI of France had attempted to negotiate a military alliance with Scotland (in the tradition of the "Auld Alliance"), with the aim of attacking England, according to a contemporary French chronicler. Richard had the authority to summon the Border Levies and issue Commissions of Array to repel the Border raids. Together with the Earl of Northumberland he launched counter-raids, and when the king and council formally declared war in November 1480, he was granted £10,000 for wages. The king failed to arrive to lead the English army and the result was intermittent skirmishing until early 1482. Richard witnessed the treaty with Alexander, Duke of Albany, brother of the Scottish king James III. Northumberland, Stanley, Dorset, Sir Edward Woodville, and Richard with approximately 20,000 men took the town of Berwick almost immediately. The castle held until 24 August 1482, when Richard recaptured Berwick-upon-Tweed from the Kingdom of Scotland. Although it is debatable whether the English victory was due more to internal Scottish divisions rather than any outstanding military prowess by Richard, it was the last time that the Royal Burgh of Berwick changed hands between the two realms.
Accession.
On the death of Edward IV, on 9 April 1483, his twelve-year-old son, Edward V, succeeded him.
Richard was named Lord Protector of the young king and moved to keep the queen's family from exercising power. The Duke of Buckingham met him with an armed escort at Northampton. Elizabeth's brother Anthony Woodville, 2nd Earl Rivers, and others were accused of planning to assassinate Richard, arrested, and taken to Pontefract Castle, where they were later executed without trial after appearing before a tribunal led by Henry Percy, 4th Earl of Northumberland. Baron Hastings had advised Richard to take Edward and Edward's younger brother, nine-year-old Richard, Duke of York, to the Tower of London, and Richard did so.
At a council meeting on 13 June at the Tower of London, Richard accused Hastings and others of having conspired against him with the Woodvilles, with Jane Shore, lover to both Hastings and Thomas Grey, 1st Marquess of Dorset, acting as a go-between. Hastings was summarily executed, while others were arrested. Hastings was not attainted and Richard sealed an indenture that placed Hastings' widow Katherine directly under his own protection. John Morton, Bishop of Ely, one of those arrested, was released into the custody of Buckingham before the latter's rebellion.
A clergyman is said to have informed Richard that Edward IV's marriage to Elizabeth Woodville was invalid because of Edward's earlier union with Eleanor Butler, making Edward V and his siblings illegitimate. The identity of the informant is known only through the memoires of French diplomat Philippe de Commines as Robert Stillington, the Bishop of Bath and Wells. On 22 June 1483, a sermon was preached outside Old St. Paul's Cathedral declaring Edward's children bastards and Richard the rightful king. Shortly after, the citizens of London, both nobles and commons, convened and drew up a petition asking Richard to assume the throne. He accepted on 26 June and was crowned at Westminster Abbey on 6 July 1483. His title to the throne was confirmed by Parliament in January 1484 by the document "Titulus Regius".
The princes, presumably still lodged in the Tower of London, the Royal Residence, disappeared from sight. Although Richard III has been accused of having Edward and his brother killed, there is debate about their actual fate.
Richard and his wife Anne endowed King's College and Queens' College at Cambridge University, and made grants to the church. He planned the establishment of a large chantry chapel in York Minster, with over one hundred priests. Richard also founded the College of Arms.
Rebellion of 1483.
In 1483, a conspiracy arose among a number of disaffected gentry, many of whom had been supporters of Edward IV and the 'whole Yorkist establishment.' The conspiracy was nominally led by Richard's former ally and first cousin once removed Henry Stafford, 2nd Duke of Buckingham, although it had begun as a Woodville-Beaufort conspiracy (being 'well under way' by the time of the duke's involvement). Indeed, Davies has suggested that it was 'only the subsequent parliamentary attainder that placed Buckingham at the centre of events,' in order to blame a single disaffected magnate motivated by greed, rather than 'the embarrassing truth' that those opposing Richard were actually 'overwhelmingly Edwardian loyalists'. It is possible that they planned to depose Richard III and place Edward V back on the throne, and that when rumours arose that Edward and his brother were dead, Buckingham proposed that Henry Tudor should return from exile, take the throne and marry Elizabeth of York, elder sister of the Tower Princes. However, it has also been pointed out that as this narrative stems from Richard's own parliament of 1484, it should probably be treated 'with caution.' For his part, Buckingham raised a substantial force from his estates in Wales and the Marches. Henry, in exile in Brittany, enjoyed the support of the Breton treasurer Pierre Landais, who hoped Buckingham's victory would cement an alliance between Brittany and England.
Some of Henry Tudor's ships ran into a storm and were forced to return to Brittany or Normandy, while Henry himself anchored off Plymouth for a week before learning of Buckingham's failure. Buckingham's army was troubled by the same storm and deserted when Richard's forces came against them. Buckingham tried to escape in disguise, but was either turned in by a retainer for the bounty Richard had put on his head, or was discovered in hiding with him. He was convicted of treason and beheaded in Salisbury, near the Bull's Head Inn, on 2 November. His widow, Catherine Woodville, later married Jasper Tudor, the uncle of Henry Tudor, who was in the process of organising another rebellion.
Richard made overtures to Landais, offering military support for Landais's weak regime under Duke Francis II of Brittany in exchange for Henry. Henry fled to Paris, where he secured support from the French regent Anne of Beaujeu, who supplied troops for an invasion in 1485. The French Government, recalling Richard's effective disowning of the Treaty of Picquigny and refusal to accept the accompanying French pension, would not have welcomed the accession of one known to be unfriendly to France.
Death at the Battle of Bosworth Field.
On 22 August 1485, Richard met the outnumbered forces of Henry Tudor at the Battle of Bosworth Field. Richard rode a white courser. The size of Richard's army has been estimated at 8000, Henry's at 5000, but exact numbers are not known; all that can be said is that the Royal army 'substantially' outnumbered Tudor's. The traditional view of the king's famous cries of "Treason!" before falling was that during the battle Richard was abandoned by Lord Stanley (made Earl of Derby in October), Sir William Stanley, and Henry Percy, 4th Earl of Northumberland. However, the role of Northumberland is unclear; his position was with the reserve—behind the king's line—and he could not easily have moved forward without a general royal advance, which did not take place. Indeed, the physical confines behind the crest of Ambion Hill, combined with a difficulty of communications, probably physically hampered any attempt he made to join the fray. Despite appearing 'a pillar of the Ricardian regime,' and his previous loyalty to Edward IV, Lord Stanley's wife, Lady Margaret Beaufort, was Henry Tudor's mother, and Stanley's inaction, combined with his brother's entering the battle on Tudor's behalf was fundamental to Richard's defeat. The death of John Howard, Duke of Norfolk, his close companion, may have had a demoralising effect on Richard and his men. Either way, Richard led a cavalry charge deep into the enemy ranks in an attempt to end the battle quickly by striking at Henry Tudor himself.
Accounts note that King Richard fought bravely and ably during this manoeuvre, unhorsing Sir John Cheyne, a well-known jousting champion, killing Henry's standard bearer Sir William Brandon and coming within a sword's length of Henry Tudor before being surrounded by Sir William Stanley's men and killed. The Burgundian chronicler Jean Molinet says that a Welshman struck the death-blow with a halberd while Richard's horse was stuck in the marshy ground. It was said that the blows were so violent that the king's helmet was driven into his skull. The contemporary Welsh poet Guto'r Glyn implies a leading Welsh Lancastrian Rhys ap Thomas, or one of his men, killed the king, writing that he "killed the boar, shaved his head". The identification in 2013 of King Richard's body shows that the skeleton had 11 wounds, eight of them to the skull, clearly inflicted in battle and suggesting he had lost his helmet. Professor Guy Rutty, from the University of Leicester, said: "The most likely injuries to have caused the king's death are the two to the inferior aspect of the skull—a large sharp force trauma possibly from a sword or staff weapon, such as a halberd or bill, and a penetrating injury from the tip of an edged weapon." The skull showed that a blade had hacked away part of the rear of the skull. King Richard III was the last English king to be killed in battle.
Polydore Vergil, Henry Tudor's official historian, recorded that "King Richard, alone, was killed fighting manfully in the thickest press of his enemies". Richard's naked body was then exposed, possibly in the collegiate foundation of the Annunciation of Our Lady, before being buried at Greyfriars Church in Leicester. In 1495, Henry VII paid £50 for a marble and alabaster monument. According to a discredited tradition, during the Dissolution of the Monasteries, his body was thrown into the River Soar, although other evidence suggests that a memorial stone was visible in 1612, in a garden built on the site of Greyfriars. The exact location was then lost, owing to more than 400 years of subsequent development, until archaeological investigations in 2012 (see the Discovery of remains section) revealed the site of the garden and Greyfriars church. There is a memorial ledger stone in the choir of the cathedral and a stone plaque on the bridge where tradition had suggested his remains were thrown into the river.
According to another tradition, Richard consulted a seer in Leicester before the battle who foretold that "where your spur should strike on the ride into battle, your head shall be broken on the return". On the ride into battle, his spur struck the bridge stone of Bow Bridge in the city; legend states that as his corpse was carried from the battle over the back of a horse, his head struck the same stone and was broken open. Bow Bridge has become a notable landmark due to its association with Richard.
Henry Tudor succeeded Richard to become Henry VII and sought to cement the succession by marrying the Yorkist heiress Elizabeth of York, Edward IV's daughter and Richard III's niece.
Succession.
Richard and Anne had one son, born between 1474 and 1476, Edward of Middleham, who was created Earl of Salisbury on 15 February 1478. He died in April 1484, after being created Prince of Wales on 8 September the previous year, and only two months after formally being declared heir apparent. Richard also had two acknowledged illegitimate children: John of Gloucester (also known as "John of Pontefract"), who was appointed Captain of Calais in 1485, and Katherine Plantagenet who married William Herbert, 2nd Earl of Pembroke in 1484. Neither their birth date nor the name of their mothers are documented, but since Katherine was old enough to be wedded in 1484 (age of consent was 12) and John was old enough to be knighted in September 1483 in York Minster (when his half brother Edward, Richard's only legitimate heir, was invested Prince of Wales) and to be made Captain of Calais in March 1485, most historians agree that they were fathered during Richard's teen years. There is no trace of infidelity on Richard's part after his marriage to Anne Neville in 1472, when he was around 20.
Michael Hicks and Josephine Wilkinson have suggested that Katherine's mother may have been Katherine Haute, on the basis of the grant of an annual payment of 100 shillings made to her in 1477. The Haute family was related to the Woodvilles through the marriage of Elizabeth Woodville's aunt, Joan Woodville to Sir William Haute. One of their children was Richard Haute, Controller of the Prince's Household. Their daughter, Alice, married Sir John Fogge; they were ancestors to queen consort Catherine Parr, sixth wife of King Henry VIII. They also suggest that John's mother may have been Alice Burgh. Richard visited Pontefract from 1471, in April and October 1473, and in early March 1474, for a week. On 1 March 1474, he granted Alice Burgh £20 a year for life "for certain special causes and considerations". She later received another allowance, apparently for being engaged as nurse for Clarence's son, Edward of Warwick. Richard continued her annuity when he became king. John Ashdown-Hill has suggested that John was conceived during Richard's first solo expedition to the eastern counties in the summer of 1467 at the invitation of John Howard and that the boy was born in 1468 and named after his friend and supporter. Richard himself noted John was still a minor (not being yet 21) when he issued the royal patent appointing him Captain of Calais on 11 March 1485, possibly on his seventeenth birthday.
Both of Richard's illegitimate children survived him, but they seem to have died without issue and their fate after Richard's demise at Bosworth is not certain. John received a £20 annuity from Henry VII, but there are no mentions of him in contemporary records after 1487 (the year of the Battle of Stoke Field). He may have been executed in 1499, though no record of this exists beyond an assertion by George Buck over a century later. Katherine apparently died before her cousin Elizabeth of York's coronation on 25 November 1487, since her husband Sir William Herbert is described as a widower by that time. Katherine's burial place was located in the London parish church of St John Garlickhithe The mysterious Richard Plantagenet, who was first mentioned in Francis Peck's "Desiderata Curiosa" (a two-volume miscellany published 1732–1735) was said to be a possible illegitimate child of Richard III and is sometimes referred to as "Richard the Master-Builder", but it has also been suggested he could have been Richard, Duke of York, one of the missing Princes in the Tower. He died in 1550.
At the time of his last stand against the Lancastrians, Richard was a widower without a legitimate son. After his son's death, he had initially named his nephew Edward, Earl of Warwick, Clarence's young son and the nephew of Queen Anne Neville, as his heir. After Anne's death, however, Richard named another nephew, John de la Pole, Earl of Lincoln, the son of his elder sister Elizabeth. However, he was also negotiating with John II of Portugal to marry his sister, Joanna, a pious young woman who had already turned down several suitors because of her preference for the religious life.
Legacy.
Richard's Council of the North, described as his 'one major institutional innovation,' derived from his ducal council following his own viceregal appointment by Edward IV; when Richard himself became king, he maintained the same conciliar structure in his absence. It officially became part of the royal council machinery under the presidency of John de la Pole, Earl of Lincoln in April 1484, based at Sandal Castle in Wakefield. It is considered to have greatly improved conditions for northern England, as it was, in theory at least, intended to keep the peace and punish law breakers, as well as resolving land disputes. Bringing regional governance directly under the control of central government, it has been described as the king's 'most enduring monument,' surviving unchanged until 1641.
In December 1483, Richard instituted what later became known as the Court of Requests, a court to which poor people who could not afford legal representation could apply for their grievances to be heard. He also improved bail in January 1484, to protect suspected felons from imprisonment before trial and to protect their property from seizure during that time. He founded the College of Arms in 1484, he banned restrictions on the printing and sale of books, and he ordered the translation of the written Laws and Statutes from the traditional French into English.
Richard's death at Bosworth resulted in the end of the Plantagenet dynasty, which had ruled England since the succession of Henry II in 1154. The last legitimate male Plantagenet, Richard's nephew, Edward, Earl of Warwick (son of Richard III's brother Clarence), was executed by Henry VII in 1499. However, a direct but illegitimate male line still exists today, with the current Duke of Beaufort.
Reputation.
There are numerous contemporary, or near-contemporary, sources of information about the reign of Richard III. These include the "Croyland Chronicle", Commines' "Mémoires", the report of Dominic Mancini, the Paston Letters, the Chronicles of Robert Fabyan and numerous court and official records, including a few letters by Richard himself. However, the debate about Richard's true character and motives continues, both because of the subjectivity of many of the written sources, reflecting the generally partisan nature of writers of this period, and because of the fact that none was written by men with an intimate knowledge of Richard, even if they had met him in person.
During Richard's reign, the historian John Rous praised him as a "good lord" who punished "oppressors of the commons", adding that he had "a great heart". In relating to his master Angelo Catho, Archbishop of Vienne, on the events that led to Richard's accession to the throne of England, the Italian observer Mancini in 1483 reported that the Duke of Gloucester enjoyed good reputation for both "his private life and public activities" that "powerfully attracted the esteem of strangers". His bond to the City of York in particular was such that on hearing of Richard's demise at the battle of Bosworth the City Council officially deplored the King's death, at the risk of facing the victor's wrath. During his lifetime he was nonetheless the subject of attacks. Even in the North in 1482 a man was prosecuted for offences against the Duke of Gloucester, saying he did 'nothing but grin at' the city of York. In 1484 the discreditory actions took the form of hostile placards, the only surviving one being William Collingbourne's lampoon of July 1484 "The Cat, the Rat, and Lovell the Dog, all rule England under a Hog" which was pinned to the door of St. Paul's Cathedral and referred to the King himself (the Hog) and his most trusted councilors William Catesby, Richard Ratcliffe and Francis Viscount Lovell. On 30 March 1485 Richard felt forced to summon the Lords and London City Councillors to publicly deny the rumours that he had poisoned Queen Anne and that he had planned a marriage to his niece Elizabeth, at the same time ordering the Sheriff of London to imprison anyone spreading such slanders. The same orders were issued throughout the realm, including York where the royal pronouncement recorded in the City Records dates 5 April 1485 and carries specific instructions to suppress seditious talk and remove and destroy evidently hostile placards unread.
As for Richard's physical appearance, most contemporary descriptions bear out the evidence that Richard had no noticeable bodily deformity. John Stow talked to old men who, remembering him, said "that he was of bodily shape comely enough, only of low stature" and a German traveller, Nicolas von Poppelau, who spent ten days in Richard's household in May of 1484, describes him as "three fingers taller than himself...much more lean, with delicate arms and legs and also a great heart." Six years after Richard's death, in 1491, a schoolmaster named William Burton, on hearing a defence of Richard, launched into a diatribe, accusing the dead King of being 'a hypocrite and a crookback...who was deservedly buried in a ditch like a dog.'
Richard's death encouraged the furtherance of this later negative image by his Tudor successors due to the fact that it helped to legitimise Henry VII's seizure of the throne. The Richard III Society contends that this means that 'a lot of what people thought they knew about Richard III was pretty much propaganda and myth building.' The Tudor characterisation culminated in the famous fictional portrayal of him in Shakespeare's play "Richard III" as a physically deformed machiavellian villain, albeit courageous and witty, cheerfully committing numerous murders in order to claw his way to power; Shakespeare's intention perhaps being to use Richard III as a vehicle for creating his own Marlowesque protagonist. Rous himself, in his "History of the Kings of England", written during Henry VII's reign, initiated the process. He reversed his earlier position, and now portrayed Richard as a freakish individual who was born with teeth and shoulder-length hair after having been in his mother's womb for two years. His body was stunted and distorted, with one shoulder higher than the other, and he was "slight in body and weak in strength". Rous also attributes the murder of Henry VI to Richard, and claims that he poisoned his own wife. Jeremy Potter, a former Chair of the Richard III Society, claims that 'At the bar of history Richard III continues to be guilty because it is impossible to prove him innocent. The Tudors ride high in popular esteem.'
Polydore Vergil and Thomas More expanded on this portrayal, emphasising Richard's outward physical deformities as a sign of his inwardly twisted mind. More describes him as "little of stature, ill-featured of limbs, crook-backed ... hard-favoured of visage". Vergil also says he was "deformed of body ... one shoulder higher than the right". Both emphasise that Richard was devious and flattering, while planning the downfall of both his enemies and supposed friends. Richard's good qualities were his cleverness and bravery. All these characteristics are repeated by Shakespeare, who portrays him as having a hunch, a limp and a withered arm.
With regard to the "hunch", the second quarto edition of "Richard III" (1598) used the term "hunched-backed" but in the First Folio edition (1623) it became "bunch-backed".
Richard's reputation as a promoter of legal fairness persisted, however. William Camden in his "Remains Concerning Britain" (1605) states that Richard, "albeit he lived wickedly, made good laws". Francis Bacon also states that he was "a good lawmaker for the ease and solace of the common people". In 1525, Cardinal Wolsey upbraided the people of London for relying on a statute of Richard to avoid paying an extorted tax (benevolence) but received the reply 'although he did evil, yet in his time were many good acts made.'
Despite this, the image of Richard as a ruthless power-grabber remained dominant in the 18th and 19th centuries. David Hume described him as a man who used dissimulation to conceal "his fierce and savage nature" and who had "abandoned all principles of honour and humanity". Hume acknowledges that some historians have argued "that he was well qualified for government, had he legally obtained it; and that he committed no crimes but such as were necessary to procure him possession of the crown", but he dismisses this view on the grounds that Richard's exercise of arbitrary power encouraged instability. The most important late 19th-century biographer of the king was James Gairdner, who also wrote the entry on Richard in the "Dictionary of National Biography". Gairdner stated that he had begun to study Richard with a neutral viewpoint, but became convinced that Shakespeare and More were essentially correct in their view of the king, despite some exaggerations.
Richard was not without his defenders, the first of whom was George Buck, a descendant of one of the king's supporters, whose life of Richard was completed in 1619. Buck attacked the "improbable imputations and strange and spiteful scandals" related by Tudor writers, including the alleged deformities and murders. He located lost archival material, including the "Titulus Regius", but also claimed to have seen a letter written by Elizabeth of York, according to which Elizabeth sought to marry the king. The book was published in 1646, Elizabeth's supposed letter was never produced. Documents which later emerged from the Portuguese Royal archives show that after Queen Anne's death, Richard's ambassadors were sent on a formal errand to negotiate a double marriage between Richard and the Portuguese King's sister Joana, of Lancastrian descent, and Elizabeth of York and Joana's cousin Duke Manuel (later King of Portugal).
The most significant of Richard's defenders was Horace Walpole. In "Historic Doubts on the Life and Reign of King Richard the Third" (1768), Walpole disputed all the alleged murders and argued that Richard may have acted in good faith. He also argued that any physical abnormality was probably no more than a minor distortion of the shoulders. However, he retracted his views in 1793 after the Terror, stating he now believed that Richard could have committed the crimes he was charged with, although Pollard observes that this retraction is frequently overlooked by later admirers of Richard. Other defenders of Richard include the noted explorer Clements Markham, whose "Richard III: His Life and Character" (1905) replied to the work of Gairdner. He argued that Henry VII killed the princes and that evidence of other "crimes" was nothing more than rumour and propaganda. An intermediate view was provided by Alfred Legge in "The Unpopular King" (1885). Legge argued that Richard's "greatness of soul" was eventually "warped and dwarfed" by the ingratitude of others.
Twentieth-century historians were less inclined to moral judgement, seeing Richard's actions as a product of the unstable times. In the words of Charles Ross, "the later fifteenth century in England is now seen as a ruthless and violent age as concerns the upper ranks of society, full of private feuds, intimidation, land-hunger, and litigiousness, and consideration of Richard's life and career against this background has tended to remove him from the lonely pinnacle of Villainy Incarnate on which Shakespeare had placed him. Like most men, he was conditioned by the standards of his age". The Richard III Society, founded in 1924 as "The Fellowship of the White Boar", is the oldest of several groups dedicated to improving his reputation.
In culture.
Apart from Shakespeare, Richard appears in many other works of literature. Two other plays of the Elizabethan era predated Shakespeare's work. The Latin-language drama "Richardus Tertius" (first known performance in 1580) by Thomas Legge is believed to be the first history play written in England. The anonymous play "The True Tragedy of Richard III" (c.1590), performed in the same decade as Shakespeare's work, was probably an influence on Shakespeare. Neither of the two plays places any emphasis on Richard's physical appearance, though the "True Tragedy" briefly mentions that he is "A man ill shaped, crooked backed, lame armed" adding that he is "valiantly minded, but tyrannous in authority". Both portray him as a man motivated by personal ambition, who uses everyone around him to get his way. Ben Jonson is also known to have written a play "Richard Crookback" in 1602, but it was never published and nothing is known about its portrayal of the king.
Marjorie Bowen's 1929 novel "Dickon" set the trend for pro-Ricardian literature. Particularly influential was "The Daughter of Time" (1951) by Josephine Tey. in which a modern detective concludes that Richard III is innocent in the death of the Princes. Other novelists such as Valerie Anand in the novel "Crown of Roses" (1989) have also offered alternative versions to the theory that he murdered them. Sharon Kay Penman, in her historical novel "The Sunne in Splendour", attributes the death of the Princes to the Duke of Buckingham. In the mystery novel "The Murders of Richard III" by Elizabeth Peters (1974) the central plot revolves around the debate as to whether Richard III was guilty of these and other crimes. A sympathetic portrayal of Richard III is given in "The Founding", the first volume in "The Morland Dynasty" series by Cynthia Harrod-Eagles.
One film adaptation of Shakespeare's play "Richard III" is the 1955 version directed and produced by Laurence Olivier, who also played the lead role. Also notable are the 1995 film version starring Ian McKellen, set in a fictional 1930s fascist England, and "Looking for Richard", a 1996 documentary film directed by Al Pacino, who plays the title character as well as himself. The play has been adapted for television on several occasions.
Discovery of remains.
On 24 August 2012, the University of Leicester and Leicester City Council, in association with the Richard III Society, announced that they had joined forces to begin a search for the remains of King Richard. Originally instigated by Philippa Langley of the Society's "Looking For Richard" Project and led by University of Leicester Archaeological Services (ULAS), experts set out to locate the lost site of the former Greyfriars Church (demolished during Henry VIII's dissolution of the monasteries), and to discover whether his remains were still interred there. By comparing fixed points between maps in a historical sequence, the search located the Church of the Grey Friars, where Richard's body had been hastily buried without pomp in 1485, its foundations identifiable beneath a modern-day city centre car park.
On 5 September 2012, the excavators announced that they had identified Greyfriars church and two days later that they had identified the location of Robert Herrick's garden, where the memorial to Richard III stood in the early 17th century. A human skeleton was found beneath the Church's choir.
Improbably, the excavators found the remains in the first location in which they dug at the car park. It was subsequently noted from a photograph taken that there was a painted "R" in nearly the exact spot at which the remains were found. Further, sources have not been able to definitively identify the source of the painted "R" or exactly what it signified, although there are plausible explanations, such as an indication of a reserved space. However, local officials admitted that it would be a strange location for a reserved space. What is less in doubt is that the painted "R" had been present for quite some time prior to the dig.
On 12 September, it was announced that the skeleton discovered during the search might be that of Richard III. Several reasons were given: the body was of an adult male; it was buried beneath the choir of the church; and there was severe scoliosis of the spine, possibly making one shoulder higher than the other (to what extent depended on the severity of the condition). Additionally, there was an object that appeared to be an arrowhead embedded in the spine; and there were perimortem injuries to the skull. These included a relatively shallow orifice, which is most likely to have been caused by a rondel dagger and a scooping depression to the skull, inflicted by a bladed weapon, most probably a sword. Additionally, the bottom of the skull presented a gaping hole, where a halberd had cut away and entered it. Forensic pathologist, Dr Stuart Hamilton stated that this injury would have left the King's brain visible, and most certainly would have been the cause of death. Dr Jo Appleby, the osteo-archaeologist who excavated the skeleton, concurred and described the latter as "a mortal battlefield wound in the back of the skull". The base of the skull also presented another fatal wound in which a bladed weapon had been thrust through it, leaving behind a jagged hole. Closer examination of the interior of the skull revealed a mark opposite this wound, showing that the blade penetrated to a depth of 10.5 cm. In total, the skeleton presented 10 wounds: 4 minor injuries on the top of the skull, 1 dagger blow on the cheekbone, 1 cut on the lower jaw, 2 fatal injuries on the base of the skull, 1 cut on a rib bone, and 1 final wound on the King's pelvis, most probably inflicted after death. It is generally accepted that postmortem, Richard's naked body was tied to the back of a horse, with his arms slung over one side and his legs and buttocks over the other. This presented a very opportunistic target for onlookers, and the angle of the blow on the pelvis suggests that one of them stabbed Richard's right buttock with substantial force, as the cut extends from the back all the way to the front of the pelvic bone and was most probably an act of humiliation. It is also possible that Richard suffered other injuries which left no trace on the skeleton.
In 2004, the British historian John Ashdown-Hill had used genealogical research to trace matrilineal descendants of Anne of York, Richard's elder sister. A British-born woman who emigrated to Canada after the Second World War, Joy Ibsen (née Brown), was found to be a 16th-generation great-niece of the king in the same direct maternal line. Joy Ibsen's mitochondrial DNA was tested and belongs to mitochondrial DNA Haplogroup J, which by deduction, should also be the mitochondrial DNA haplogroup of Richard III. Joy Ibsen died in 2008. Her son Michael Ibsen gave a mouth-swab sample to the research team on 24 August 2012. His mitochondrial DNA passed down the direct maternal line was compared to samples from the human remains found at the excavation site and used to identify King Richard.
On 4 February 2013, the University of Leicester confirmed that the skeleton was beyond reasonable doubt that of King Richard III. This conclusion was based on mitochondrial DNA evidence, soil analysis, and dental tests (there were some molars missing as a result of caries), as well as physical characteristics of the skeleton which are highly consistent with contemporary accounts of Richard's appearance. The team announced that the "arrowhead" discovered with the body was a Roman-era nail, probably disturbed when the body was first interred. However, there were numerous perimortem wounds on the body, and part of the skull had been sliced off with a bladed weapon; this would have caused rapid death. The team concluded that it is unlikely that the king was wearing a helmet in his last moments. Soil taken from the Plantagenet King's remains was found to contain microscopic roundworm eggs. Several eggs were found in samples taken from the pelvis, where the king's intestines were, but not from the skull and only very small numbers were identified in soil surrounding the grave. The findings suggest that the higher concentration of eggs in the pelvic area probably arose from a roundworm infection the King suffered in his life, rather than from human waste dumped in the area at a later date, researchers said. The Mayor of Leicester announced that the king's skeleton would be re-interred at Leicester Cathedral in early 2014, but a judicial review on that decision delayed the reinterment for a year. A museum to Richard III was opened in July 2014 in the Victorian school buildings next to the Greyfriars grave site.
The proposal to have King Richard buried in Leicester attracted some controversy. Those who challenged the decision included fifteen 'collateral [non-direct] descendants' of Richard, represented by the Plantagenet Alliance, who believe that the body should be reburied in York, as they claim the king wished. In August 2013, they filed a court case in order to contest Leicester's claim to re-inter the body within its cathedral, and propose the body be buried in York instead. However, Michael Ibsen, who gave the DNA sample that identified the king, gave his support to Leicester's claim to re-inter the body in their cathedral. On 20 August, a judge ruled that the opponents had the legal standing to contest his burial in Leicester Cathedral, despite a clause in the contract which had authorized the excavations requiring his burial there. He urged the parties, though, to settle out of court in order to "avoid embarking on the Wars of the Roses, Part Two". The Plantagenet Alliance, and the supporting fifteen 'collateral [non-direct] descendants', also faced the challenge that 'Basic maths shows Richard, who had no surviving children but five siblings, could have millions of "collateral" descendants' and they don't represent 'the only people who can speak on behalf of him', as one member claimed. A ruling in May 2014 decreed that there are "no public law grounds for the Court interfering with the decisions in question". The remains were taken to Leicester Cathedral on 22 March 2015 and reinterred on 26 March.
On 5 February 2013 Professor Caroline Wilkinson of the University of Dundee conducted a forensic facial reconstruction of Richard III, commissioned by the Richard III Society, based on 3D mappings of his skull. The face is described as "warm, young, earnest and rather serious". On 11 February 2014 the University of Leicester announced the project to sequence the entire genome of Richard III and one of his living relatives, Michael Ibsen, whose mitochondrial DNA confirmed the identification of the excavated remains. Richard III was the first ancient person, with known historical identity, to have the genome sequenced.
In November 2014, the results of the testing were announced, confirming that the maternal side was as previously thought. The paternal side, however, demonstrated some variance from what had been expected, with the DNA showing no links to the purported descendants of Richard's great-great-grandfather Edward III of England through Henry Somerset, 5th Duke of Beaufort. This could be the result of misattributed paternity that does not reflect the accepted genealogies between Richard and Edward III or between Edward III and the 5th Duke of Beaufort.
Reburial and tomb.
In 1485, following his death in battle against Henry Tudor at Bosworth Field, Richard III's body was buried in Greyfriars Church in Leicester.
Following the discoveries of Richard's remains in 2012, it was decided that they should be reburied at Leicester Cathedral, despite feelings in some quarters that he should have been reburied in Yorkshire. His remains were carried in procession to the cathedral on 22 March 2015, and reburied on 26 March 2015 at a religious re-burial service at which both the Right Reverend Tim Stevens, the Bishop of Leicester and the Most Reverend Justin Welby, the Archbishop of Canterbury officiated. The British Royal Family was represented by the Duke and Duchess of Gloucester and the Countess of Wessex. The actor Benedict Cumberbatch, a distant relation of the king, read a poem by Poet Laureate Carol Ann Duffy. The last funeral for an English monarch prior to this was for Edward VIII, who died (as Duke of Windsor) in 1972, 43 years before Richard's reinterment.
His cathedral tomb was designed by the architects van Heningen and Haward. The tombstone is deeply incised with a cross, and consists of a rectangular block of white Swaledale fossil stone, quarried in North Yorkshire. It sits on a low plinth made of dark Kilkenny marble and incised with Richard’s name, dates and motto. The plinth also carries his coat of arms in pietra dura. 
The remains of Richard III are in a lead ossuary, inside an English oak coffin crafted by Michael Ibsen, a direct descendant of Richard's sister Anne of York, and laid in a brick-lined vault below the floor, and below the plinth and tombstone.
Titles, styles and honours.
On 1 November 1461, Richard gained the title of Duke of Gloucester; in late 1461, he was invested as a Knight of the Garter. Following the death of King Edward IV, he was made Lord Protector of England. Richard held this office from 30 April to 26 June 1483, when he made himself king of the realm. As King of England, Richard was styled "Dei Gratia Rex Angliae et Franciae et Dominus Hiberniae" ("by the Grace of God, King of England and France and Lord of Ireland").
Informally, he may have been known as "Dickon", according to a sixteenth-century legend of a note, warning of treachery, that was sent to the Duke of Norfolk on the eve of Bosworth:
Arms.
As Duke of Gloucester, Richard used the Royal Arms of England quartered with the Royal Arms of France, differenced by a label argent of three points ermine, on each point a canton gules. As sovereign, he used the arms of the kingdom undifferenced. His motto was "Loyaulte me lie", "Loyalty binds me"; and his personal device was a white boar.
Bibliography.
</dl>
Further reading.
</dl>

</doc>
<doc id="26285" url="http://en.wikipedia.org/wiki?curid=26285" title="Restriction fragment length polymorphism">
Restriction fragment length polymorphism

In molecular biology, restriction fragment length polymorphism, or RFLP (commonly pronounced “rif-lip”), is a technique that exploits variations in homologous DNA sequences. It refers to a difference between samples of homologous DNA molecules from differing locations of restriction enzyme sites, and to a related laboratory technique by which these segments can be illustrated. In RFLP analysis, the DNA sample is broken into pieces and (digested) by restriction enzymes and the resulting "restriction fragments" are separated according to their lengths by gel electrophoresis. Although now largely obsolete due to the rise of inexpensive DNA sequencing technologies, RFLP analysis was the first DNA profiling technique inexpensive enough to see widespread application. RFLP analysis was an important tool in genome mapping, localization of genes for genetic disorders, determination of risk for disease, and paternity testing.
Analysis technique.
The basic technique for the detecting RFLPs involves fragmenting a sample of DNA by a restriction enzyme, which can recognize and cut DNA wherever a specific short sequence occurs, in a process known as a restriction digest. The resulting DNA fragments are then separated by length through a process known as agarose gel electrophoresis, and transferred to a membrane via the Southern blot procedure. Hybridization of the membrane to a labeled DNA probe then determines the length of the fragments which are complementary to the probe. An RFLP occurs when the length of a detected fragment varies between individuals. Each fragment length is considered an allele, and can be used in genetic analysis.
RFLP analysis may be subdivided into single- (SLP) and multi-locus probe (MLP) paradigms. Usually, the SLP method is preferred over MLP because it is more sensitive, easier to interpret and capable of analyzing mixed-DNA samples. Moreover, data can be generated even when the DNA is degraded (e.g. when it is found in bone remains.)
Examples.
There are two common mechanisms by which the size of a particular restriction fragment can vary. In the first schematic, a small segment of the genome is being detected by a DNA probe (thicker line). In allele "A", the genome is cleaved by a restriction enzyme at three nearby sites (triangles), but only the rightmost fragment will be detected by the probe. In allele "a", restriction site 2 has been lost by a mutation, so the probe now detects the larger fused fragment running from sites 1 to 3. The second diagram shows how this fragment size variation would look on a Southern blot, and how each allele (two per individual) might be inherited in members of a family.
In the third schematic, the probe and restriction enzyme are chosen to detect a region of the genome that includes a variable VNTR segment (boxes). In allele "c" there are five repeats in the VNTR, and the probe detects a longer fragment between the two restriction sites. In allele "d" there are only two repeats in the VNTR, so the probe detects a shorter fragment between the same two restriction sites. Other genetic processes, such as insertions, deletions, translocations, and inversions, can also lead to RFLPs.
RFLP uses a much bigger sample of DNA than STR.
Applications.
Analysis of RFLP variation in genomes was a vital tool in genome mapping and genetic disease analysis. If researchers were trying to initially determine the chromosomal location of a particular disease gene, they would analyze the DNA of members of a family afflicted by the disease, and look for RFLP alleles that show a similar pattern of inheritance as that of the disease (see Genetic linkage). Once a disease gene was localized, RFLP analysis of other families could reveal who was at risk for the disease, or who was likely to be a carrier of the mutant genes.
RFLP analysis was also the basis for early methods of Genetic fingerprinting, useful in the identification of samples retrieved from crime scenes, in the determination of paternity, and in the characterization of genetic diversity or breeding patterns in animal populations.
Alternatives.
The technique for RFLP analysis is, however, slow and cumbersome. It requires a large amount of sample DNA, and the combined process of probe labeling, DNA fragmentation, electrophoresis, blotting, hybridization, washing, and autoradiography could take up to a month to complete. A limited version of the RFLP method that used oligonucleotide probes was reported in 1985. Fortunately, the results of the Human Genome Project have largely replaced the need for RFLP mapping, and the identification of many single-nucleotide polymorphisms (SNPs) in that project (as well as the direct identification of many disease genes and mutations) has replaced the need for RFLP disease linkage analysis (see SNP genotyping). The analysis of VNTR alleles continues, but is now usually performed by polymerase chain reaction (PCR) methods. For example, the standard protocols for DNA fingerprinting involve PCR analysis of panels of more than a dozen VNTRs.
RFLP is still a technique used in marker assisted selection. Terminal restriction fragment length polymorphism (TRFLP or sometimes T-RFLP) is a molecular biology technique initially developed for characterizing bacterial communities in mixed-species samples. The technique has also been applied to other groups including soil fungi.
TRFLP works by PCR amplification of DNA using primer pairs that have been labeled with fluorescent tags. The PCR products are then digested using RFLP enzymes and the resulting patterns visualized using a DNA sequencer. The results are analyzed either by simply counting and comparing bands or peaks in the TRFLP profile, or by matching bands from one or more TRFLP runs to a database of known species. The technique is similar in some aspects to DGGE or TGGE.
The sequence changes directly involved with an RFLP can also be analyzed more quickly by PCR. Amplification can be directed across the altered restriction site, and the products digested with the restriction enzyme. This method has been called Cleaved Amplified Polymorphic Sequence (CAPS). Alternatively, the amplified segment can be analyzed by Allele specific oligonucleotide (ASO) probes, a process that can often be done by a simple Dot blot.

</doc>
<doc id="26286" url="http://en.wikipedia.org/wiki?curid=26286" title="Rocket-propelled grenade">
Rocket-propelled grenade

A rocket-propelled grenade (often abbreviated RPG) is a shoulder-fired, anti-tank weapon system that fires rockets equipped with an explosive warhead. These warheads are affixed to a rocket motor and stabilized in flight with fins. Some types of RPG are reloadable, while others are single-use. RPGs, with the exception of self-contained versions, are loaded from the muzzle.
RPGs with high explosive anti-tank warheads (HEAT) are very effective against armored vehicles such as armored personnel carriers (APCs). However, heavily armored vehicles such as main battle tanks are generally too well armored to be penetrated by an RPG, unless weaker sections of the armor are exploited. Various warheads are also capable of causing secondary damage to vulnerable systems (especially sights, tracks, rear and roof of turrets) and other soft targets.
The term "rocket-propelled grenade" is strictly a backronym; it stems from the Russian language РПГ or ручной противотанковый гранатомёт (transliterated as "ruchnoy protivotankovy granatomyot"), meaning "hand-held anti-tank grenade launcher", the name given to early Russian designs.
History.
The RPG has its roots in the 20th century with the early development of the explosive shaped charge. Before the adoption of the shaped charge, anti-tank guns and tank guns relied primarily on kinetic energy to defeat armor, as tank armor increased in thickness and effectiveness, the guns needed to defeat them became increasingly heavy, cumbersome and expensive, meaning that infantry could find themselves defenseless against tanks. Armies found they needed to give infantry the means to defeat enemy armor when no anti-tank guns were available. Initial attempts to put such weapons in the hands of the infantry resulted in such weapons as the Soviet RPG-40 blast effect hand grenade (here RPG stood for "ruchnaya protivotankovaya granata", meaning hand-held anti-tank grenade). The later RPG-43 and RPG-6 used shaped charges, the chemical energy of their explosive being used more efficiently to enable the defeat of thicker armor, however being hand thrown weapons they still had to be deployed at suicidally close range to be effective. What was needed was a means to deliver the shaped charge warhead from a distance, different approaches to this goal would lead to the anti-tank spigot mortar, the recoilless rifle and, from the development of practical rocketry, the rocket propelled grenade.
Research occasioned by World War II produced such weapons as the American bazooka and German Panzerfaust, which combined portability with effectiveness against armored vehicles such as tanks.
The Soviet Union-developed RPG-7 is the most widely distributed, recognizable and used RPG in the world. The basic design of this RPG was developed by the Soviets shortly after World War II in the form of the RPG-2, which is similar in function to the bazooka and the Panzerfaust.
Design.
The RPG has its roots in the 20th century with the early development of the explosive shaped charge. A shaped charge is an explosive charge shaped to focus the effect of the explosive's energy. Various types are used to penetrate tank armour. A typical modern lined shaped charge can penetrate armor steel to a depth of 7 or more times the diameter of the charge (charge diameters, CD), though greater depths of 10 CD and above have been achieved.
The shaped charge does not depend in any way on heating or melting for its effectiveness; that is, the jet from a shaped charge does not melt its way through armor, as its effect is purely kinetic in nature.
An RPG comprises two main parts: the launcher and a rocket equipped with a warhead. The most common types of warheads are high explosive (HE) or high explosive anti-tank (HEAT) rounds. These warheads are affixed to a rocket motor and stabilized in flight with fins. Some types of RPGs are single-use disposable units similar to the RPG-22; others are re-loadable, such as the Soviet RPG-7.
The launcher is designed such that the rocket exits the launcher without discharging an exhaust that would be dangerous to the operator. In the case of the RPG-7, the rocket is launched by a gunpowder booster charge, and the rocket motor ignites only after 10 metres. In some other designs, the propellant charge burns completely within the tube.
An RPG is an inexpensive way to deliver an explosive payload over a distance with moderate accuracy. Substantially more expensive wire-guided missiles are used when accuracy is important; these rockets trail a thin wire behind them during firing and steering corrections can be sent by the operator while in flight, such as in the AT-3 Sagger.
Warheads.
The HEAT (high explosive anti-tank) round is a standard shaped charge warhead, similar in concept to those used in tank cannon rounds. In this type of warhead, the shape of the explosive material within the warhead focuses the explosive energy on a copper (or similar metal) lining. This heats the metal lining and propels some of it forward at a very high velocity in a highly plastic state. The resulting narrow jet of metal can defeat armor several hundred milimeters of RHA equivalent, such as that used in light and medium armored vehicles. However, heavily armored vehicles such as main battle tanks are generally too well armored to be penetrated by an RPG, unless weaker sections of the armor are exploited. Various warheads are also capable of causing secondary damage to vulnerable systems (especially sights, tracks, rear and roof of turrets) and other soft targets.
The warhead detonates on impact or when the fuse runs out; usually the fuse is set to the maximum burn of the rocket motor but it can be shortened for improvised anti aircraft purposes.
Specialized warheads are available for illumination, smoke, tear gas, and white phosphorus. Russia, China, and many former Warsaw Pact nations have also developed a fuel-air explosive (thermobaric) warhead. Another recent development is a tandem HEAT warhead capable of penetrating reactive armor.
So-called PRIGs (Propelled Recoilless Improvised Grenade) were improvised warheads used by the Provisional IRA.
Effectiveness.
The RPG-29 uses a tandem-charge high explosive anti-tank warhead to penetrate explosive reactive armor (ERA) as well as composite armor behind it. It is capable of penetrating MBTs such as the M1 Abrams, older model Mark II version of the Merkava, Challenger 2, or T-90.
In August 2006 in al-Amarah, a Soviet RPG-29 damaged the front underside of a Challenger 2, detonating ERA in the area of the driver's cabin. The driver lost part of his foot and two more of the crew were also injured but the driver was able to reverse 2.4 km to an aid post. The incident was not made public until May 2007, and in response to accusations, the MoD said "We have never claimed that the Challenger 2 is impenetrable." Since then, the ERA has been replaced with a Dorchester block and the steel underbelly lined with armour, as part of the 'Streetfighter' upgrade, which was a direct response to this incident. In May 2008, "The New York Times" disclosed that an American M1 tank had also been damaged by an RPG-29 in Iraq. The American army is ranking the RPG-29 threat to American armor as high; they have refused to allow the newly formed Iraqi army to buy it, fearing it will fall into the insurgent hands.
Various armies and manufacturers have developed add-on tank armor and other systems for urban combat, such as the Tank Urban Survival Kit (TUSK) for M1 Abrams, slat armor for the Stryker, ERA kit for the FV432, AZUR for Leclerc, and others. Similar solutions are active protection systems, engaging closing projectiles such as the Russian Drozd and Arena, as well as the recent Israeli Trophy system.
The RPG-30 was designed to address the threat of active protection systems on tanks by using a false target to trick the APS.
The RPG-30 shares a close resemblance with the RPG-27 in that it is a man-portable, disposable anti-tank rocket launcher with a single shot capacity. Unlike the RPG-27 however, there is a smaller diameter precursor round in a smaller side barrel tube, in addition to the main round in the main tube. This precursor round acts as a false target, tricking the target's active protection system (APS) into engaging it, allowing the main round a clear path into the target, while the APS is stuck in the 0.2-0.4 second delay it needs to start its next engagement.
The PG-30 is the main round of the RPG-30. The round is a 105-mm tandem shaped charge with a weight of 10.3-kg (22.7-lb) and has a range of 200 meters and a stated penetration capability in excess of 600-mm (24-in) rolled homogeneous armor (RHA) (after ERA), 1500-mm reinforced concrete, 2000-mm brick and 3700-mm of soil. Reactive armor, including explosive reactive armor (ERA), can be defeated with multiple hits into the same place, such as by tandem-charge weapons, which fire two or more shaped charges in rapid succession.
Protection.
An early method of disabling shaped charges developed during World War II was to apply thin skirt armor or meshwire at a distance around the hull and turret of the tank. The skirt or mesh armor (cage armor) triggers the RPG on contact and much of the molten jet that a shaped charge produces dissipates before coming into contact with the main armor of the vehicle. Well-sloped armor also gives some protection because the shaped charge is forced to penetrate a greater amount of armor due to the oblique angle. The benefits of cage armor are still considered great in modern battlefields in the Middle East, and although similar effects can be obtained using spaced armor, either as a part of the original design or as appliqué armor fitted later, cage armor is preferable due to its low weight and ease of repair.
Today, technologically advanced armies have implemented composite armors such as Chobham armor, which provide superior protection to steel. For added protection, vehicles may be retrofitted with reactive armor; on impact, reactive tiles explode or deform, disrupting the normal function of the shaped charge. Russian and Israeli vehicles also use active protection systems like Drozd, Arena APS or Trophy. Such a system detects and shoots down incoming projectiles before they reach the vehicle. As in all arms races, these developments in armor countermeasures have led to the development of RPG rounds designed specifically to defeat them, with methods such as a tandem-charge warhead, which has two shaped charges, of which the first is meant to activate any reactive armor, and the second to penetrate the vehicle.
Production by country.
Sweden.
Swedish arms makers began to design recoilless antitank rifles in the early 1940s. Their first example was a shoulder-fired, single shot weapon which fired a 20mm projectile, but this was not powerful enough to penetrate heavy tank armour. In the mid-1940s, Swedish arms makers began to try shaped-charge HEAT ammunition. After 1948, 84 mm Carl Gustav recoilless rifles were adopted by Australia, Austria, Belgium, Canada, Germany, USA and several other countries.
United States.
The United States Army developed a lightweight antitank weapon (LAW) in the middle 1950s. By 1961, the M72 LAW was in use. It is a shoulder-fired, disposable rocket launcher with HEAT warhead. It is a recoilless weapon, which was easy to use, and effective against armored vehicles. It was used in the Vietnam War. It uses a fin-stabilized rocket. In response to the threat of thicker armor, this weapon was replaced by the 'AT-4' recoilless rifle, a larger & non-collapsable - albeit still single-shot weapon.
The United States Marine Corps uses a different launcher, which is reloadable - the Shoulder-Launched Multipurpose Assault Weapon (SMAW). Unlike the RPG, it is reloaded from the breech-end rather than the muzzle.
Russia.
RPGs currently in service in the Russian Ground Forces:
Tactics.
RPGs were used extensively during the Vietnam War (by the Vietnam People's Army and Vietcong), Soviet invasion of Afghanistan by the Mujahideen and against South Africans in Angola and Namibia (formerly South West Africa) by SWAPO guerillas during what the South Africans called the South African Border War. Twenty years later, they are still being used widely in recent conflict areas such as Chechnya, Iraq, and Sri Lanka.
One of the first instances when it was used by militants was on 13 January 1975 at the Orly airport in France, when Carlos the Jackal, together with another member from the PFLP, used two Soviet RPG-7 grenades to attack an Israeli El Al airliner. Both missed, and one of them hit a DC-9 of Yugoslav Airlines instead.
In Afghanistan, Mujahideen guerrillas used RPG-7s to destroy Soviet vehicles. To assure a kill, two to four RPG shooters would be assigned to each vehicle. Each armor-vehicle hunter-killer teams can have as many as 15 RPG. In areas where vehicles were confined to a single path (a mountain road, swamps, snow, urban areas), RPG teams trapped convoys by destroying the first and last vehicles in line, preventing movement of the other vehicles. This tactic was especially effective in cities. Convoys learned to avoid approaches with overhangs and to send infantrymen forward in hazardous areas to detect the RPG teams.
Multiple shooters were also effective against heavy tanks with reactive armor: The first shot would be against the driver's viewing prisms. Following shots would be in pairs, one to set off the reactive armor, the second to penetrate the tank's armor. Favored weak spots were the top and rear of the turret.
Afghans sometimes used RPG-7s at extreme range, exploded by their 4.5-second self-destruct timer, which translates to roughly 950m flight distance, as a method of long distance approach denial for infantry and reconnaissance. The most noteworthy use of RPGs against aircraft in Afghanistan occurred on 6 August 2011 when Taliban fighters shot down a U.S. CH-47 Chinook helicopter killing all 38 personnel on board including SEAL Team 6 from a range of 220 meters. An earlier anti-aircraft kill by the Taliban occurred during Operation Red Wings, on 28 June 2005 when a Chinook helicopter was destroyed by unguided rocket propelled grenades. The threat to slow low flying aircraft by relatively simple shoulder-fired unguided RPGs is clearly demonstrated by these two incidents against which there is no counter measure.
In the period following the 2003 invasion of Iraq, the RPG became a favorite weapon of the insurgent forces fighting U.S. troops. Since most of the readily-available RPG-7 rounds cannot penetrate M1 Abrams tank armor from the front, it is primarily effective against soft-skinned or lightly armored vehicles, and infantry. Even if the RPG hit does not completely disable the tank or kill the crew, it can still damage external equipment, lowering the tank's effectiveness or forcing the crew to abandon and destroy it. Newer RPG-7 rounds are more capable, and in August 2006, an RPG-29 round penetrated the frontal ERA of a Challenger 2 tank during an engagement in al-Amarah, Iraq, and wounded several crew members.
RPGs were a main tool used by the FMLN's guerrilla forces in the Salvadoran Civil War. For example, during the June 19, 1986 overrun of the San Miguel Army base, FMLN sappers dressed only in black shorts, their faces blacked out with grease, sneaked through barbed wire at night, avoiding the searchlights, they made it to within firing range of the outer wall. Using RPGs to initiate the attack, they blew through the wall and killed a number of Salvadorean soldiers. They eliminated the outermost sentries and searchlights with the rockets, then made it into the inner wall, which they also punched through. They were then able to create mayhem as their comrades attacked from the outside.
During the First (1994–1996) and Second Chechen Wars (1999–2009), Chechen rebels used RPGs to attack Russian tanks from basements and high rooftops. This tactic was effective because tank main guns could not be depressed or raised far enough to return fire, in addition, armor on the very top and bottom of tanks is usually the weakest. Russian forces had to rely on artillery suppression, good crew gunners and infantry screens to prevent such attacks. Tank columns were eventually protected by attached self-propelled anti-aircraft guns (ZSU-23-4, Tunguska-M1) used in the ground role to suppress and destroy Chechen ambushes. Chechen fighters formed independent "cells" that worked together to destroy a specific Russian armored target. Each cell contained small arms and some form of RPG (RPG-7V or RPG-18, for example). The small arms were used to button the tank up and keep any infantry occupied, while the RPG gunner struck at the tank. While doing so, other teams would attempt to fire at the target in order to overwhelm the Russians' ability to effectively counter the attack. To further increase the chance of success, the teams took up positions at different elevations where possible. Firing from the third and higher floors allowed good shots at the weakest armor (the top). When the Russians began moving in tanks fitted with explosive reactive armor (ERA), the Chechens had to adapt their tactics, because the RPGs they had access to were unlikely to result in the destruction of the tank.
Using RPGs as improvised anti-aircraft batteries has proved successful in Somalia, Afghanistan and Chechnya. Helicopters are typically ambushed as they land, take off or hover. In Afghanistan, the Mujahideen often modified RPGs for use against Soviet helicopters by adding a curved pipe to the rear of the launcher tube, which diverted the backblast, allowing the RPG to be fired upward at aircraft from a prone position. This made the operator less visible prior to firing and decreased the risk of injury from hot exhaust gases. Mujahideen also utilised the 4.5-second timer on RPG rounds to make the weapon function as part of a flak battery, using multiple launchers to increase hit probabilities. At the time, Soviet helicopters countered the threat from RPGs at landing zones by first clearing them with anti-personnel saturation fire. The Soviets also varied the number of accompanying helicopters (two or three) in an effort to upset Afghan force estimations and preparation. In response, the Mujahideen prepared dug-in firing positions with top cover, and again, Soviet forces altered their tactics by using air-dropped fuel-air bombs on such landing zones. As the U.S.-supplied Stinger surface-to-air missiles became available to them, the Afghans abandoned RPG attacks as the smart missiles proved especially efficient in the destruction of unarmed Soviet transport helicopters, such as Mil Mi-17. In Somalia, both of the UH-60 Black Hawk helicopters lost by U.S. forces during the Battle of Mogadishu in 1993 were downed by RPG-7s.

</doc>
<doc id="26287" url="http://en.wikipedia.org/wiki?curid=26287" title="Roy Jenkins">
Roy Jenkins

Roy Harris Jenkins, Baron Jenkins of Hillhead, OM, PC (11 November 1920 – 5 January 2003) was a British politician and writer.
The son of a Welsh coal miner, Roy Jenkins later became a union official and Labour MP. He also served with distinction in World War II. Elected to Parliament as a Labour member in 1948, he served in several major posts in Harold Wilson's First Government. As Home Secretary from 1965–1967, he sought to build what he described as "a civilised society", with measures such as the effective abolition in Britain of capital punishment and theatre censorship, the decriminalisation of homosexuality, relaxing of divorce law, suspension of birching and the legalisation of abortion. As Chancellor of the Exchequer from 1967–1970, he pursued a tight fiscal policy. On 8 July 1970, he was elected Deputy Leader of the Labour Party, but resigned in 1972 because he supported entry to the Common Market, while the party opposed it.
When Wilson re-entered government in 1974 Jenkins returned to the Home Office, but increasingly disenchanted by the leftward swing of the Labour Party, he chose to leave British politics in 1976 and was appointed President of the European Commission in 1977, serving until 1981: he was the first and to date only British holder of this office. In 1981, dismayed with the Labour Party's continuing leftward drift, he was one of the "Gang of Four" – Labour moderates who formed the Social Democratic Party (SDP). In 1982 he won a famous by-election in a Conservative seat and returned to parliament; but after disappointment with the performance of the SDP in the 1983 election he resigned as SDP leader.
In 1987, Jenkins was elected to succeed Harold Macmillan as Chancellor of the University of Oxford following the latter's death; he held this position until his death. A few months after becoming Chancellor, Jenkins was defeated in his Hillhead constituency by then-Labour politician George Galloway. He accepted a life peerage and sat as a Liberal Democrat. In the late 1990s, he was an adviser to Tony Blair and chaired the Jenkins Commission on electoral reform. Roy Jenkins died in 2003, aged 82.
In addition to his political career, he was also a noted historian, biographer and writer. His "A Life at the Centre" (1991) is regarded as one of the best autobiographies of the later twentieth century, which 'will be read with pleasure long after most examples of the genre have been forgotten'.
Early life.
Born in Abersychan, Monmouthshire, in south-eastern Wales, as an only child, Roy Jenkins was the son of a National Union of Mineworkers official, Arthur Jenkins. His father was imprisoned during the 1926 General Strike for his alleged involvement in a disturbances. Jenkins later became President of the South Wales Miners' Federation and Member of Parliament for Pontypool, Parliamentary Private Secretary to Clement Attlee, and briefly a minister in the 1945 Labour government. Jenkins's mother, Hattie Harris, was the daughter of a steelworks manager.
Jenkins was educated at Abersychan County Grammar School, University College, Cardiff, and at Balliol College, Oxford, where he was twice defeated for the Presidency of the Oxford Union but took First-Class Honours in Politics, Philosophy and Economics (PPE). His university colleagues included Tony Crosland, Denis Healey, and Edward Heath, and he became friends with all three, although he was never particularly close to Healey.
During the Second World War, Jenkins served with the Royal Artillery and then as a Bletchley Park codebreaker, reaching the rank of captain.
Member of Parliament.
Having failed to win Solihull in 1945, he was elected to the House of Commons in a 1948 by-election as the Member of Parliament for Southwark Central, becoming the "Baby of the House." His constituency was abolished in boundary changes for the 1950 general election, when he stood instead in the new Birmingham Stechford constituency. He won the seat and represented the constituency until 1977.
Jenkins was principal sponsor, in 1959, of the bill which became the liberalising Obscene Publications Act, responsible for establishing the "liable to deprave and corrupt" criterion as a basis for a prosecution of suspect material and for specifying literary merit as a possible defence. Like Healey and Crosland, he had been a close friend of Hugh Gaitskell and for them Gaitskell's death and the elevation of Harold Wilson as Labour Party leader was a setback.
In Government.
After the 1964 general election Jenkins was appointed Minister of Aviation. While at Aviation he oversaw the high profile cancellations of the BAC TSR-2 and Concorde projects (although the latter was later reversed after strong opposition from the French Government). In January 1965 Patrick Gordon Walker resigned as Foreign Secretary and in the ensuing reshuffle Wilson offered Jenkins the Department for Education and Science; however. he declined it, preferring to stay at Aviation.
In the summer of 1965 Jenkins eagerly accepted an offer to replace Frank Soskice as Home Secretary. However Wilson, dismayed by a sudden bout of press speculation about the potential move, delayed Jenkins' appointment until December. Once Jenkins took office – the youngest Home Secretary since Churchill – he immediately set about reforming the operation and organisation of the Home Office. The Principal Private Secretary, Head of the Press and Publicity Department and Permanent Under-Secretary were all replaced. He also redesigned his office, famously replacing the board on which condemned prisoners were listed with a drinks cabinet. After the 1966 general election, in which Labour won a comfortable majority, Jenkins pushed through a series of police reforms which reduced the number of separate forces from 117 to 49.
Immigration was a divisive and provocative issue during the late 1960s and on 23 May 1966 Jenkins delivered a speech on race relations, which is widely considered to be one of his best. Addressing a London meeting of the National Committee for Commonwealth Immigrants he notably defined Integration:
... not as a flattening process of assimilation but as equal opportunity, accompanied by cultural diversity, in an atmosphere of mutual tolerance.
Before going onto ask:
Where in the world is there a university which could preserve its fame, or a cultural centre which could keep its eminence, or a metropolis which could hold its drawing power, if it were to turn inwards and serve only its own hinterland and its own racial group?
And concluding that:
To live apart, for a person, a city, a country, is to lead a life of declining intellectual stimulation.
Roy Jenkins is often seen as responsible for the most wide-ranging social reforms of the late 1960s, with popular historian Andrew Marr claiming 'the greatest changes of the Labour years' were thanks to Jenkins. He refused to authorise the birching of prisoners and was responsible for the relaxation of the laws relating to divorce, abolition of theatre censorship and gave government support to David Steel's Private Member's Bill for the legalisation of abortion and Leo Abse's bill for the decriminalisation of homosexuality. Wilson, with his puritan background, was not especially sympathetic to these developments, however. Jenkins replied to public criticism by asserting that the so-called permissive society was in reality the civilised society. For some conservatives, such as Peter Hitchens, Jenkins' reforms remain objectionable. In his book "The Abolition of Britain" Hitchens accuses him of being a "cultural revolutionary" who takes a large part of the responsibility for the decline of "traditional values" in Britain.
Chancellor of the Exchequer.
From 1967 to 1970 Jenkins served as Chancellor of the Exchequer, replacing James Callaghan following the devaluation crisis of November 1967. He quickly gained a reputation as a particularly tough Chancellor with his 1968 budget increasing taxes by £923 million, more than twice the increase of any previous budget to date. Despite Edward Heath claiming it was a 'hard, cold budget, without any glimmer of warmth' Jenkins' first budget broadly received a warm reception, with Harold Wilson remarking that 'it was widely acclaimed as a speech of surpassing quality and elegance' and Barbara Castle that it 'took everyone's breath away'. However, following a further sterling crisis in November 1968 Jenkins was forced to raise taxes by a further £250 million. After this the currency markets slowly began to settle and his 1969 budget represented more of the same with a £340 million increase in taxation to further limit consumption.
By May 1969 Britain's current account position was in surplus, thanks to a growth in exports, a drop in overall consumption and, in part, the Inland Revenue correcting a previous underestimation in export figures. In July Jenkins was also able to announce that the size of Britain's foreign currency reserves had been increased by almost $1 billion since the beginning of the year. It was at this time that he presided over Britain's only excess of government revenue over expenditure in the period 1936-7 to 1987–8. Thanks in part to these successes there was a high expectation that the 1970 budget would be a more generous one. Jenkins, however, was cautious about the stability of Britain's recovery and decided to present a more muted and fiscally neutral budget. It is often argued that this, combined with a series of bad trade figures, contributed to the Conservative victory at the 1970 general election. Historians and economists have often praised Jenkins for presiding over the transformation in Britain's fiscal and current account positions towards the end of the 1960s. Andrew Marr, for example, described him as one of the 20th century's 'most successful chancellors'.
Shadow Cabinet 1970–1974.
After Labour unexpectedly lost power in 1970 Jenkins was appointed Shadow Chancellor of the Exchequer by Harold Wilson. Jenkins was also subsequently elected to the deputy leadership of the Labour Party in July 1970, defeating future Labour Leader Michael Foot and former Leader of the Commons Fred Peart at the first ballot. At this time he appeared the natural successor to Harold Wilson, and it appeared to many only a matter of time before he inherited the leadership of the party, and the opportunity to become Prime Minister. 
This changed completely, however, as Jenkins refused to accept the tide of anti-European feeling that became prevalent in the Labour Party in the early 1970s. In 1972, he led sixty-nine Labour MPs through the division lobby in support of the Heath's government's motion to take Britain into the EEC. In so-doing they were defying a three-line whip and a five-to-one vote at the Labour Party annual conference. Jenkins's action gave the European cause a legitimacy that would have otherwise been absent had the issue been considered solely as a party political matter. At this stage, however, Jenkins would not fully abandon his position as a political insider, and chose to stand again for deputy leader, an act his colleague David Marquand claimed he later came to regret. Jenkins narrowly defeated Michael Foot on a second ballot.
Six months later, however, he resigned both the deputy leadership and his shadow cabinet position in April 1972, over the party's policy on favouring a referendum on British membership of the European Economic Community (EEC). This led to some former admirers, including Roy Hattersley, choosing to distance themselves from Jenkins. His lavish lifestyle — Wilson once described him as "more a socialite than a socialist" — had already alienated much of the Labour Party from him. Jenkins returned to the shadow cabinet in November 1973 as Shadow Home Secretary.
Return to Government.
When Labour returned to power in early 1974, Jenkins was appointed Home Secretary for the second time. Earlier, he had been promised the treasury; however, Wilson later decided to appoint Denis Healey as Chancellor instead. Upon hearing from Bernard Donoughue that Wilson had reneged on his promise, Jenkins reacted angrily. Despite being on a public staircase, he is reported to have shouted 'You tell Harold Wilson he must bloody well come to see me ... and if he doesn't watch out, I won't join his bloody government ... This is typical of the bloody awful way Harold Wilson does things!'
Jenkins served from 1974 to 1976. In this period he undermined his previous liberal credentials to some extent by pushing through the controversial Prevention of Terrorism Act, which, among other things, extended the length of time suspects could be held in custody and instituted exclusion orders. Although becoming increasingly disillusioned during this time by what he considered the party's drift to the left, he was the leading Labour figure in the referendum in September 1975 which saw the 'yes' campaign win a two-to-one victory in the referendum on continued membership of the European Community.
President of the European Commission.
When Harold Wilson suddenly resigned as Prime Minister, Jenkins was one of six candidates for the leadership of the Labour Party in March 1976, but came third out of the six candidates in the first ballot, behind Callaghan and Michael Foot. Realising that his vote was lower than expected, and sensing that the parliamentary party was in no mode to overlook his actions five years before, he immediately withdrew from the contest. Jenkins had wanted to become Foreign Secretary, but accepted an appointment as President of the European Commission (succeeding François-Xavier Ortoli) after Callaghan appointed Anthony Crosland to the Foreign Office.
The main development overseen by the Jenkins Commission was the development of the Economic and Monetary Union of the European Union from 1977, which began in 1979 as the European Monetary System, a forerunner of the Single Currency or Euro. President Jenkins was the first President to attend a G8 summit on behalf of the Community. Jenkins remained in Brussels until 1981, contemplating the political changes in the UK from there.
He received an Honorary Degree (Doctor of Laws) from the University of Bath in 1978.
The Social Democratic Party.
As one of the so-called "Gang of Four", Roy Jenkins was a founder of the Social Democratic Party (SDP) in January 1981 with David Owen, Bill Rodgers and Shirley Williams.
He attempted to re-enter Parliament at the Warrington by-election in 1981 but Labour retained the seat with a small majority. He was more successful in 1982, being elected in the Glasgow Hillhead by-election as the MP for a previously Conservative-held seat.
During the 1983 election campaign his position as the prime minister designate for the SDP-Liberal Alliance was questioned by his close colleagues, as his campaign style was now regarded as ineffective; the Liberal leader David Steel was considered to have a greater rapport with the electorate.
He led the new party from March 1982 until after the 1983 general election, when Owen succeeded him unopposed. Jenkins was disappointed with Owen's move to the right, and his acceptance and backing of some of Thatcher's policies. At heart, Jenkins remained a Keynesian.
He continued to serve as SDP Member of Parliament for Glasgow Hillhead until his defeat at the 1987 general election by the Labour candidate George Galloway.
Peerage, achievements, books and death.
From 1987, Jenkins remained in politics as a member of the House of Lords as a life peer with the title Baron Jenkins of Hillhead, of Pontypool in the County of Gwent. Also in 1987, Jenkins was elected Chancellor of the University of Oxford.
In 1988 he fought and won an amendment to the Education Reform Act of that year, guaranteeing academic freedom of speech in further and higher education establishments. This affords and protects the right of students and academics to "question and test received wisdom" and has been incorporated into the statutes or articles and instruments of governance of all universities and colleges in Britain.
In 1993, he was appointed to the Order of Merit. He was leader of the Liberal Democrats in the Lords until 1997. In December 1997, he was appointed chair of a Government-appointed Independent Commission on the Voting System, which became known as the "Jenkins Commission", to consider alternative voting systems for the UK. The Jenkins Commission reported in favour of a new uniquely British mixed-member proportional system called "Alternative vote top-up" or "limited AMS" in October 1998, although no action was taken on this recommendation.
Jenkins wrote 19 books, including a biography of Gladstone (1995), which won the 1995 Whitbread Award for Biography, and a much-acclaimed biography of Winston Churchill (2001). His official biographer, Andrew Adonis, Baron Adonis, was to have finished the Churchill biography had Jenkins not survived the heart surgery he underwent towards the end of its writing. Churchill's daughter Lady Mary Soames believed the Jenkins biography of her father to be the best available.
Jenkins underwent heart surgery in November 2000, and postponed his 80th birthday celebrations, by having a celebratory party on 7 March 2001. He died on 5 January 2003, aged 82, after suffering a heart attack at his home at East Hendred, in Oxfordshire. His last words, to his wife, were, "Two eggs, please, lightly poached". At the time of his death Jenkins was apparently starting work on a biography of US President Franklin D. Roosevelt.
Jenkins is seen by many as a key influence on "New Labour", as the Labour Party marketed itself after the election of Tony Blair (who served as prime minister from winning the first of three successive general elections in 1997) in 1994, when the party abandoned many of its long-established policies including nationalisation, nuclear disarmament and unconditional support for the trade unions. He was well regarded by other Labour statesmen including Tony Benn, but came under heavy criticism from others including Denis Healey, who condemned the SDP split as a "disaster" for the Labour Party which prolonged their time in opposition and allowed the Tories to have an unbroken run of 18 years in government.
Cardiff University honours the memory of Roy Jenkins by naming one of its halls of residence 'Roy Jenkins Hall'.
Marriage and personal life.
On 20 January 1945, in the final year of the War, he married Jennifer Morris. She was made a DBE for services to ancient and historical buildings. They had two sons, Charles and Edward, and a daughter, Cynthia.
A new biography, Roy Jenkins: A Well Rounded Life, by John Campbell, reveals how the former Labour chancellor and Home Secretary had a homosexual affair with his close friend Tony Crosland, who he met at Oxford University. http://www.telegraph.co.uk/culture/10687788/Roy-Jenkins-male-lover-Tony-Crosland-tried-to-halt-his-marriage.html
Selected bibliography.
Books by Roy Jenkins:
Books about Roy Jenkins:

</doc>
<doc id="26288" url="http://en.wikipedia.org/wiki?curid=26288" title="List of Polish monarchs">
List of Polish monarchs

Poland, or at least its nucleus, was ruled at various times either by dukes (the 10th–14th century) or by kings (the 11th-18th century). The longest-reigning dynasties were the Piasts (ca. 960–1370) and Jagiellonians (1386–1572). Intervening and subsequent monarchs were often rulers of foreign countries or princes recruited from foreign dynasties.
During the latter period, a tradition of free election of monarchs made it a uniquely electable position in Europe (16th–18th centuries). Polish independence ended with the Third Partition of Poland (1795) and was restored at the end of World War I (1918) on a republican basis.
Poland in the Early Middle Ages.
"See: Poland in the Early Middle Ages"
Kingdom of Poland, 960–1569.
Fragmentation of the Kingdom of Poland, 1138–1314.
"See also: Fragmentation of the realm and Testament of Bolesław III Wrymouth"
The First Republic of Poland, 1569–1795.
Elective Kings of Poland (Polish-Lithuanian Commonwealth).
"See also: History of the Polish–Lithuanian Commonwealth (1569–1648) and History of the Polish–Lithuanian Commonwealth (1648–1764) and History of the Polish–Lithuanian Commonwealth (1764–95)"

</doc>
<doc id="26289" url="http://en.wikipedia.org/wiki?curid=26289" title="Richard Henry Lee">
Richard Henry Lee

Richard Henry Lee (January 20, 1732 – June 19, 1794) was an American statesman from Virginia best known for the motion in the Second Continental Congress calling for the colonies' independence from Great Britain. He was a signatory to the Articles of Confederation and his famous resolution of June 1776 led to the United States Declaration of Independence, which Lee signed. He also served a one-year term as the President of the Continental Congress, and was a United States Senator from Virginia from 1789 to 1792, serving during part of that time as one of the first Presidents "pro tempore".
He was a member of the Lee family, a historically influential family in Virginia politics.
Early life and education.
Lee was born in Westmoreland County, Virginia to Thomas Lee and Hannah Harrison Ludwell Lee on January 20, 1732. He was raised Episcopalian and came from a line of military officers, diplomats, and legislators. His father, Thomas Lee, was the governor of Virginia before his death in 1750. Lee spent most of his early life in Stratford, Virginia with his family at Stratford Hall. Here he was tutored and taught in a variety of skills, and witnessed the very beginning of political career as his father sent him around to neighboring planters with the intention for Lee to become associated with neighboring men of like prominence. In 1748, 16, Lee left Virginia for Yorkshire, England, to complete his formal education at Queen Elizabeth Grammar School, Wakefield. Both of his parents died in 1750 and, in 1753, after touring Europe, he returned to Virginia to help his brothers settle the estate his parents had left behind.
Career.
In 1757, Lee was appointed justice of the peace in Westmoreland County. In 1758 he was elected to the Virginia House of Burgesses, where he met Patrick Henry. An early advocate of independence, Lee became one of the first to create Committees of Correspondence among the many independence-minded Americans in the various colonies. In 1766, almost ten years before the American Revolutionary War, Lee is credited with having authored the Westmoreland Resolution which was publicly signed by prominent landowners who met at Leedstown, Westmoreland County, Virginia on 27 February 1766. This resolution was signed by four brothers of George Washington as well as Gilbert Campbell.
American Revolution.
In August 1774, Lee was chosen as a delegate to the First Continental Congress in Philadelphia. In Lee's Resolution on the 7th of June 1776 during the Second Continental Congress, Lee put forth the motion to the Continental Congress to declare Independence from Great Britain, which read (in part):
Resolved: That these United Colonies are, and of right ought to be, free and independent States, that they are absolved from all allegiance to the British Crown, and that all political connection between them and the State of Great Britain is, and ought to be, totally dissolved.
Lee had returned to Virginia by the time Congress voted on and adopted the Declaration of Independence, but he signed the document when he returned to Congress.
President of Congress.
Richard Henry Lee was elected the sixth President of Congress under the Articles of Confederation on November 30, 1784, in the French Arms Tavern, Trenton, New Jersey. On January 11, 1785, Congress convened in the old New York City Hall and Lee presided over that Congress until November 23, 1785. Although, he was not paid a salary for his office as president, his household expenses were paid by Congress in the amount of $12,203.13.
Lee's Congress was most active in 1785, passing numerous legislation, including establishing a United States dollar tied to the Spanish dollar as the national currency. His most pressing issue, however, was to settle the states' territorial disputes over the Northwest Territory. Throughout his term, Lee remained steadfast that the release of states’ territorial claims on the Northwest Territory would enable the federal government to fund itself with land sales. He believed that the urgency of this measure was paramount because borrowing more foreign money was no longer prudent, and he abhorred the movement to establish federal taxes. The sale of these vast federal lands, he concluded, was the nation's only hope to pay off the war debt and adequately fund federal government. Debate began on the expansion of the Ordinance of 1784 and Thomas Jefferson’s survey method “hundreds of ten geographical miles square, each mile containing 6086 and 4-10ths of a foot” and “sub-divided into lots of one mile square each, or 850 and 4-10ths of an acre" on April 14. On May 3, 1785, William Grayson of Virginia made a motion seconded by James Monroe to change “seven miles square” to “six miles square”, and the current US Survey system was born. Lee wrote to his friend and colleague Samuel Adams:
I hope we shall shortly finish our plan for disposing of the western Lands to discharge the oppressive public debt created by the war & I think that if this source of revenue be rightly managed, that these republics may soon be discharged from that state of oppression and distress that an indebted people must invariably feel. 
The states relinquished their right to this "test tract" of land, and the Land Ordinance of 1785 was passed on May 20, 1785.
The federal government, however, lacked the resources to manage the newly surveyed lands because Native Americans refused to relinquish a large percentage of the platted land, and most of the territory remained too dangerous for settlement. This either required troops to eject the Native Americans or capital to purchase their land "fairly", insuring the peaceful sale and settlement. Additionally the small amount of federal land that was not in dispute by the Native Americans was enthusiastically being occupied by western settlers that had no faith in or respect for the Congress as a federal authority. The settlers claimed the land as squatters, and the Congress was unable to muster the capital to magistrates let alone troops to enforce the $1.00 per acre fee required for a clear federal land title. With the states no longer in control of the lands and no federal magistrates or troops to enforce the laws, a tide of western squatters flowed into the Northwest Territory. Lee's plan to fill the federal treasury with the proceeds of land sales failed, but the survey system developed under the Land Ordinance of 1785 is still used today.
Personal life.
Marriages and children.
Richard married first on December 5, 1757, Anne Aylett (1738–1768), daughter of William Aylett and Elizabeth Eskridge (1719). Anne died December 12, 1768 at Chantille, Westmoreland Co., Virginia. The couple had six children, four of whom survived infancy:
Richard remarried in June or July 1769 to Anne (Gaskins) Pinckard. The couple had seven children, five of whom survived infancy:
Richard honored his brother, Francis Lightfoot Lee (another signer of the Articles of Confederation and the Declaration of Independence), by naming his youngest son after him.
The younger Francis married Jane Fitzgerald on 9 Feb 1810. In 1811 he purchased the estate "Sully" in Fairfax County, Virginia from his second cousin Richard Bland Lee. Jane died on 25 Jul 1816, shortly after the birth on their fifth child.
Ancestry.
Richard was the son of Col. Thomas Lee, Hon. (1690–1750) of "Stratford Hall", Westmoreland Co., Virginia. Thomas married Hannah Harrison Ludwell (1701–1750).
Hannah was the daughter of Col. Philip Ludwell II (1672–1726) of "Greenspring", and Hannah Harrison (1679–1731).
Thomas was the son of Col. Richard Lee II, Esq., "the scholar" (1647–1715) and Laetitia Corbin (c. 1657 – 1706).
Laetitia was the daughter of Richard’s neighbor and, Councillor, Hon. Henry Corbin, Sr. (1629–1676) and Alice (Eltonhead) Burnham (c. 1627 – 1684).
Richard II, was the son of Col. Richard Lee I, Esq., "the immigrant" (1618–1664) and Anne Constable (c. 1621 – 1666).
Anne was the daughter of Thomas Constable and a ward of Sir John Thoroughgood.
Jason Barfield II (1626–1700)
Legacy.
Lee County, Georgia is named in his honor. "Richard Henry Lee Elementary School" in Rossmoor, California and "Richard Henry Lee School" in Chicago, Illinois are also named in his honor. Richard Henry Lee Elementary in Glen Burnie, Maryland is also named after him.
The Chantilly Archaeological Site was listed on the National Register of Historic Places in 1971.
In popular culture.
Lee is portrayed as a character in the musical "1776". He was portrayed by Ron Holgate in both the Broadway cast and in the 1972 film. In one scene, Lee performs a song called "The Lees of Old Virginia," in which he explains how he knows he will be able to convince the Virginia House of Burgesses to allow him to propose independence and celebrates his own status as a Lee, one of the First Families of Virginia. The character is presented as vain, but not very bright, serving the play as a comic device rather than a historically based portrayal of Lee.

</doc>
<doc id="26291" url="http://en.wikipedia.org/wiki?curid=26291" title="Rajasthan">
Rajasthan

Rajasthan ( ]; literally, "Land of Kings"), is India's largest state by area (342239 km2 or 10.4% of India's total area). It is located on the northern side of the country, where it comprises most of the wide and inhospitable Thar Desert (also known as the "Rajasthan Desert" and "Great Indian Desert") and shares a border with Pakistan along the Sutlej-Indus river valley. Elsewhere it is bordered by other Indian states: Gujarat to the southwest; Madhya Pradesh to the southeast; Uttar Pradesh and Haryana to the northeast; and Punjab to the north. Its features include the ruins of the Indus Valley Civilization at Kalibanga; the Dilwara Temples, a Jain pilgrimage site at Rajasthan's only hill station, Mount Abu, in the ancient Aravalli mountain range; and, in eastern Rajasthan, the Keoladeo National Park near Bharatpur, a World Heritage Site known for its bird life. Rajasthan is also home to two national tiger reserves, the Ranthambore National Park in Sawai Madhopur and Sariska Tiger Reserve in Alwar.
The state was formed on 30 March 1949 when Rajputana – the name adopted by British Raj for its dependencies in the region – was merged into the Dominion of India. Its capital and largest city is Jaipur, located on the state's eastern side.
Etymology.
The first mention of the name "Rajasthan" appears in James Tod's 1829 publication "Annals and Antiquities of Rajast'han or the Central and Western Rajpoot States of India", while the earliest known record of "Rajputana" as a name for the region is in George Thomas's 1800 memoir "Military Memories". John Keay, in his book "India: A History", stated that "Rajputana" was coined by the British and even given a previous history: in 1829, John Briggs, translating Ferishta's history of early Islamic India, used the phrase "Rajpoot (Rajput) princes" rather than "Indian princes". R. C. Majumdar explained that the region was long known as "Gurjaratra", meaning "country protected or ruled by the Gurjars".
History.
The Indus Valley Civilization, one of the world's first and oldest, was in parts of what is now Rajasthan. Kalibangan, in Hanumangarh district, was a major provincial capital of the Indus Valley Civilization. It is believed that Western Kshatrapas (405–35 BC) were Saka rulers of the western part of India (Saurashtra and Malwa: modern Gujarat, Southern Sindh, Maharashtra, Rajasthan). They were successors to the Indo-Scythians and were contemporaneous with the Kushans, who ruled the northern part of the Indian subcontinent. The Indo-Scythians invaded the area of Ujjain and established the Saka era (with their calendar), marking the beginning of the long-lived Saka Western Satraps state. Matsya, a state of the Vedic civilisation of India, is said to roughly corresponded to the former state of Jaipur in Rajasthan and included the whole of Alwar with portions of Bharatpur. The capital of Matsya was at Viratanagar (modern Bairat), which is said to have been named after its founder king Virata.
Traditionally the Meenas, Gurjars, Bhils, Rajputs, Rajpurohit, Charans, Jats, Yadavs, Bishnois, PhulMali (Saini) and other tribes made a great contribution in building the state of Rajasthan. All these tribes suffered great difficulties in protecting their culture and the land. Millions of them were killed trying to protect their land. A number of Gurjars had been exterminated in Bhinmal and Ajmer areas fighting with the invaders. Bhils once ruled Kota. Meenas were rulers of Bundi and the Dhundhar region.
Gurjars ruled many dynasties in this part of the country. In fact, this region was long known as "Gurjaratra". Up to the tenth century almost the whole of North India, excepting Bengal, acknowledged the supremacy of the Gurjars with their seat of power at Kannauj.
The Gurjar Pratihar Empire acted as a barrier for Arab invaders from the 8th to the 11th century. The chief accomplishment of the Gurjara Pratihara empire lies in its successful resistance to foreign invasions from the west, starting in the days of Junaid. Historian R. C. Majumdar says that this was openly acknowledged by the Arab writers themselves. He further notes that historians of India have wondered at the slow progress of Muslim invaders in India, as compared with their rapid advance in other parts of the world. Now there seems little doubt that it was the power of the Gurjara Pratihara army that effectively barred the progress of the Arabs beyond the confines of Sindh, their first conquest for nearly 300 years.
The earlier contributions of warriors and protectors of the land (the Meenas, Gurjars, Ahirs, Jats and Bhils) were ignored and lost in history due to the stories of great valour shown by certain specific clans in later years, which gained more prominence than the earlier acts of bravery.
Modern Rajasthan includes most of Rajputana, which comprises the erstwhile nineteen princely states, two chiefships, and the British district of Ajmer-Merwara. Marwar (Jodhpur), Bikaner, Mewar (Udaipur), Alwar and Dhundhar (Jaipur) were some of the main Rajput princely states. Bharatpur and Dholpur were Jat princely states whereas Tonk was a princely state under a Muslim Nawab. Rajput families rose to prominence in the 6th century CE. The Rajputs put up a valiant resistance to the Islamic invasions and protected this land with their warfare and chivalry for more than 500 years. They also resisted Mughal incursions into India and thus contributed to their slower-than-anticipated access to the Indian subcontinent. Later, the Mughals, through skilled warfare, were able to get a firm grip on northern India, including Rajasthan. Mewar led other kingdoms in its resistance to outside rule. Most notably, Rana Sanga fought the Battle of Khanua against Babur, the founder of the Mughal empire.
Samrat Hem Chandra Vikramaditya, the Hindu Emperor, also known as Hemu in the history of India, was born in the village of Machheri in Alwar District in 1501. He won 22 battles against Afghans, from Punjab to Bengal and defeated Akbar's forces twice at Agra and Delhi in 1556, before acceding to the throne of Delhi and establishing the "Hindu Raj" in North India, albeit for a short duration, from Purana Quila in Delhi. He was killed in the Second Battle of Panipat.)
Maharana Pratap of Mewar resisted Akbar in the famous Battle of Haldighati (1576) and later operated from hilly areas of his kingdom. The Bhils were Maharana's main allies during these wars. Most of these attacks were repulsed even though the Mughal forces outnumbered Mewar Rajputs in all the wars fought between them. The Haldighati war was fought between 10,000 Mewaris and a 100,000-strong Mughal force (including many Rajputs like Kachwahas from Dhundhar).
Over the years, the Mughals began to have internal disputes which greatly distracted them at times. The Mughal Empire continued to weaken, and with the decline of the Mughal Empire in the 18th century, Rajputana came under suzerainty of the Marathas.
The Marathas, who were Hindus from the state of what is now Maharashtra, ruled Rajputana for most of the eighteenth century. The Maratha Empire, which had replaced the Mughal Empire as the overlord of the subcontinent, was finally replaced by the British Empire in 1818.
Following their rapid defeat, the Rajput kings concluded treaties with the British in the early 19th century, accepting British suzerainty and control over their external affairs in return for internal autonomy.
Rajasthan's formerly independent kingdom created a rich architectural and cultural heritage, seen even today in their numerous forts and palaces (Mahals and Havelis), which are enriched by features of Islamic and Jain architecture.
The development of frescos in Rajasthan is linked with the history of the Marwaris, who played a crucial role in the economic development of the region. Many wealthy families throughout Indian history have links to Marwar. These include the legendary Birla, Bajaj and Mittal families.
Rajasthani language.
Rajasthani (Devanagari: राजस्थानी) is a language of the Indo-Aryan languages family. It is spoken by 20 million people in Rajasthan and neighbouring states of India and Pakistan, or 50 million if Marwari is counted as Rajasthani, as it often is. It is one of the languages descended from old western Rajasthani, AKA Maru-Gujar or Maruwani, the other being modern Gujarati.
Most of the Rajasthani dialects are chiefly spoken in the state of Rajasthan but are also spoken in Gujarat.
Geography.
The geographic features of Rajasthan are the Thar Desert and the Aravalli Range, which runs through the state from southwest to northeast, almost from one end to the other, for more than 850 km. Mount Abu lies at the southwestern end of the range, separated from the main ranges by the West Banas River, although a series of broken ridges continues into Haryana in the direction of Delhi where it can be seen as outcrops in the form of the Raisina Hill and the ridges farther north. About three-fifths of Rajasthan lies northwest of the Aravallis, leaving two-fifths on the east and south direction.
The northwestern portion of Rajasthan is generally sandy and dry. Most of this region is covered by the Thar Desert which extends into adjoining portions of Pakistan. The Aravalli Range does not intercept the moisture-giving southwest monsoon winds off the Arabian Sea, as it lies in a direction parallel to that of the coming monsoon winds, leaving the northwestern region in a rain shadow. The Thar Desert is thinly populated; the town of Bikaner is the largest city in the desert. The Northwestern thorn scrub forests lie in a band around the Thar Desert, between the desert and the Aravallis. This region receives less than 400 mm of rain in an average year. Temperatures can exceed 45 °C in the summer months and drop below freezing in the winter. The Godwar, Marwar, and Shekhawati regions lie in the thorn scrub forest zone, along with the city of Jodhpur. The Luni River and its tributaries are the major river system of Godwar and Marwar regions, draining the western slopes of the Aravallis and emptying southwest into the great Rann of Kutch wetland in neighbouring Gujarat. This river is saline in the lower reaches and remains potable only up to Balotara in Barmer district. The Ghaggar River, which originates in Haryana, is an intermittent stream that disappears into the sands of the Thar Desert in the northern corner of the state and is seen as a remnant of the primitive Saraswati river.
The Aravalli Range and the lands to the east and southeast of the range are generally more fertile and better watered. This region is home to the Kathiarbar-Gir dry deciduous forests ecoregion, with tropical dry broadleaf forests that include teak, "Acacia", and other trees. The hilly Vagad region lies in southernmost Rajasthan, on the border with Gujarat. With the exception of Mount Abu, Vagad is the wettest region in Rajasthan, and the most heavily forested. North of Vagad lies the Mewar region, home to the cities of Udaipur and Chittaurgarh. The Hadoti region lies to the southeast, on the border with Madhya Pradesh. North of Hadoti and Mewar lies the Dhundhar region, home to the state capital of Jaipur. Mewat, the easternmost region of Rajasthan, borders Haryana and Uttar Pradesh. Eastern and southeastern Rajasthan is drained by the Banas and Chambal rivers, tributaries of the Ganges.
The Aravalli Range runs across the state from the southwest peak Guru Shikhar (Mount Abu), which is 1,722 m in height, to Khetri in the northeast. This range divides the state into 60% in the northwest of the range and 40% in the southeast. The northwest tract is sandy and unproductive with little water but improves gradually from desert land in the far west and northwest to comparatively fertile and habitable land towards the east. The area includes the Thar Desert. The south-eastern area, higher in elevation (100 to 350 m above sea level) and more fertile, has a very diversified topography. in the south lies the hilly tract of Mewar. In the southeast, a large area within the districts of Kota and Bundi forms a tableland. To the northeast of these districts is a rugged region (badlands) following the line of the Chambal River. Farther north the country levels out; the flat plains of the northeastern Bharatpur district are part of an alluvial basin. Merta City lies in the geographical center of Rajasthan.
Flora and fauna.
Though a large percentage of the total area is desert with little forest cover, Rajasthan has a rich and varied flora and fauna. The natural vegetation is classed as Northern Desert Thorn Forest (Champion 1936). These occur in small clumps scattered in a more or less open forms. The density and size of patches increase from west to east following the increase in rainfall.
The Desert National Park in Jaisalmer is spread over an area of 3162 km2, is an excellent example of the ecosystem of the Thar Desert and its diverse fauna. Seashells and massive fossilised tree trunks in this park record the geological history of the desert. The region is a haven for migratory and resident birds of the desert. One can see many eagles, harriers, falcons, buzzards, kestrels and vultures. Short-toed eagles "(Circaetus gallicus)", tawny eagles "(Aquila rapax)", spotted eagles "(Aquila clanga)", laggar falcons "(Falco jugger)" and kestrels are the commonest of these.
The Ranthambore National Park located in Sawai Madhopur, one of the finest tiger reserves in the country, became a part of Project Tiger in 1973.
The Dhosi Hill located in district of Jhunjunu, known as 'Chayvan Rishi's Ashram', where 'Chayawanprash' was formulated for the first time, has unique and rare herbs growing.
The Sariska Tiger Reserve located in Alwar district, 200 km from Delhi and 107 km from Jaipur, covers an area of approximately 800 km2. The area was declared a national park in 1979.
Tal Chhapar Sanctuary is a very small sanctuary in Sujangarh, Churu District, 210 km from Jaipur in the Shekhawati region. This sanctuary is home to a large population of blackbuck. Desert foxes and the caracal, an apex predator, also known as the "desert lynx", can also be spotted, along with birds such as the partridge and sand grouse. The great Indian bustard, known locally as the "godavan", and which is a state bird, has been classed as critically endangered since 2011.
Wildlife protection.
Rajasthan is also noted for its national parks and wildlife sanctuaries. There are four national park and wildlife sanctuaries: Keoladeo National Park of Bharatpur, Sariska Tiger Reserve of Alwar, Ranthambore National Park of Sawai Madhopur, and Desert National Park of Jaisalmer.
Ranthambore National Park is known worldwide for its tiger population and is considered by both wilderness lovers and photographers as one of the best place in India to spot tigers. At one point, due to poaching and negligence, tigers became extinct at Sariska, but five tigers have been relocated there. Prominent among the wildlife sanctuaries are Mount Abu Sanctuary, Bhensrod Garh Sanctuary, Darrah Sanctuary, Jaisamand Sanctuary, Kumbhalgarh Wildlife Sanctuary, Jawahar Sagar sanctuary, and Sita Mata Wildlife Sanctuary.
Government and politics.
The politics of Rajasthan is dominated mainly by the Bharatiya Janata Party and the Indian National Congress. The current Chief Minister, serving a second term, is Vasundhara Raje.
Administrative divisions.
Rajasthan is divided into 33 districts within seven divisions:
Economy.
Rajasthan's economy is primarily agricultural and pastoral. Wheat and barley are cultivated over large areas, as are pulses, sugarcane, and oilseeds. Cotton and tobacco are the state's cash crops. Rajasthan is among the largest producers of edible oils in India and the second largest producer of oilseeds. Rajasthan is also the biggest wool-producing state in India and the main opium producer and consumer. There are mainly two crop seasons. The water for irrigation comes from wells and tanks. The Indira Gandhi Canal irrigates northwestern Rajasthan.
The main industries are mineral based, agriculture based, and textiles. Rajasthan is the second largest producer of polyester fibre in India. The Pali and Bhilwara District produces more cloth than Bhiwandi, Maharashtra and the bhilwara is the largest city in suitings production and export and Pali is largest city in cotton and polyster in blouse pieces and rubia production and export. Several prominent chemical and engineering companies are located in the city of Kota, in southern Rajasthan. Rajasthan is pre-eminent in quarrying and mining in India. The Taj Mahal was built from the white marble which was mined from a town called Makrana. The state is the second largest source of cement in India. It has rich salt deposits at Sambhar, copper mines at Khetri, Jhunjhunu, and zinc mines at Dariba, Zawar mines at Zawarmala for zinc, Rampura Aghucha (opencast) near Bhilwara. Dimensional stone mining is also undertaken in Rajasthan. Jodhpur sandstone is mostly used in monuments, important buildings and residential buildings. This stone is termed as "chittar patthar".
Rajasthan is also a part of the Mumbai-Delhi Industrial corridor is set to benefit economically. The State gets 39% of the DMIC, with major districts of Jaipur, Alwar, Kota and Bhilwara benefiting.
Rajasthan is earning ₹150 million (approx. US$2.5 million) per day as revenue from crude oil sector. This earning is expected to reach ₹250 million per day in 2013 (which is an increase of ₹100 million or more than 66 percent). The government of India has given permission to extract 300,000 barrels of crude per day from Barmer region which is now 175,000 barrels per day. Once this limit is achieved Rajasthan will become leader in Crude extraction in Country. Bombay High leads with a production of 250,000 barrels crude per day. Once the limit if 300,000 barrels per day is reached, the overall production of the country will increase by 15 percent. Cairn India is doing the work of exploration and extraction of crude oil in Rajasthan.
Transport.
Rajasthan is connected by many national highways. Most renowned being NH 8, which is India's first 4–8 lane highway. Rajasthan also has an inter-city surface transport system both in terms of railways and bus network. All chief cities are connected by air, rail and road.
There are three main airports at Rajasthan- Jaipur International Airport, Udaipur Airport, and Jodhpur Airport. These airports connect Rajasthan with the major cities of India such as Delhi and Mumbai. There are three other airports in Kota, Jaisalmer and NAL(Bikaner) but are not open for commercial/civilian flights yet. Jaisalmer airport is open for civilians but only during season time (from August to March). One more airport at Kishangarh, Ajmer is currently being constructed by the Airport Authority of India
Rajasthan is connected with the main cities of India by rail. Jaipur, Kota, Bharatpur, Bikaner, Ajmer, Alwar, Udaipur, Abu Road and Jodhpur are the principal railway stations in Rajasthan. Kota City is the only Electrified Section served by three Rajdhani Expresses and trains to all major cities of India. There is also an international railway, the Thar Express from Jodhpur to Karachi. However, this is not open to foreign nationals.
Rajasthan is well connected to the main cities of the country including Delhi, Ahmedabad and Indore by State and National Highways and served by Rajasthan State Road Transport Corporation (RSRTC) and Private operators.
Demographics.
Rajasthan has a mainly Rajasthani population of approximately 68,621,012. Rajasthan's population is made up mainly of Hindus, who account for 88.8% of the population. Muslims make up 8.5%, Sikhs 1.4% and Jains 1.2% of the population. The state of Rajasthan is also populated by Sindhis, who came to Rajasthan from Sindh province (now in Pakistan) during the India-Pakistan separation in 1947.
Hindi is the official and the most widely spoken language in the state (91% of the population as per the 2001 census), followed by Bhili (5%), Punjabi (2%), and Urdu (1%).
Culture.
Rajasthan is culturally rich and has artistic and cultural traditions which reflect the ancient Indian way of life. There is rich and varied folk culture from villages which is often depicted and is symbolic of the state. Highly cultivated classical music and dance with its own distinct style is part of the cultural tradition of Rajasthan. The music is uncomplicated and songs depict day-to-day relationships and chores, more often focused around fetching water from wells or ponds.
Rajasthani cooking was influenced by both the war-like lifestyles of its inhabitants and the availability of ingredients in this arid region. Food that could last for several days and could be eaten without heating was preferred. Scarcity of water and fresh green vegetables have all had their effect on the cooking. It is known for its snacks like Bikaneri Bhujia, Mirchi Bada, Pyaaj Kachori and ghevar. Other famous dishes include "bajre ki roti" (millet bread) and "lashun ki chutney" (hot garlic paste), "mawa kachori" from Jodhpur, Alwar ka Mawa(Milk Cake), "malpauas" from Pushkar and rassgollas from Bikaner. Originating for the Marwar region of the state is the concept Marwari Bhojnalaya, or vegetarian restaurants, today found in many part of India, which offer vegetarian food of the Marwari people.
Dal-Bati-Churma is very popular in Rajasthan. Traditional way to serve it is to first coarsely mash the Baati then pour pure Ghee on top of it. It is served with the daal (lentils) and spicy garlic chutney. Also served with Besan (gram flour) ki kadi . It is commonly served at all festivities, including religious occasions, wedding ceremonies, and birthday parties in Rajasthan. "Dal-Baati-Churma", is a combination of three different food items — Daal (lentils), Baati and Churma (Sweet). It is a typical Rajasthani dish.
The Ghoomar dance from Udaipur and Kalbeliya dance of Jaisalmer have gained international recognition. Folk music is a vital part of Rajasthani culture. Kathputli, Bhopa, Chang, Teratali, Ghindr, Kachchhighori, Tejaji, etc. are the examples of the traditional Rajasthani culture. Folk songs are commonly ballads which relate heroic deeds and love stories; and religious or devotional songs known as bhajans and banis (often accompanied by musical instruments like dholak, sitar, sarangi etc.) are also sung.
Rajasthan is known for its traditional, colourful art. The block prints, tie and dye prints, Bagaru prints, Sanganer prints, and Zari embroidery are major export products from Rajasthan. Handicraft items like wooden furniture and crafts, carpets, and blue pottery are commonly found here. Rajasthan is a shoppers' paradise, with beautiful goods at low prices. Reflecting the colourfulculture, Rajasthani clothes have a lot of mirror-work and embroidery. A Rajasthani traditional dress for females comprises an ankle-length skirt and a short top, also known as a "lehenga" or a "chaniya choli". A piece of cloth is used to cover the head, both for protection from heat and maintenance of modesty. Rajasthani dresses are usually designed in bright colours like blue, yellow and orange.
The main religious festivals are Deepawali, Holi, Gangaur, Teej, Gogaji, Shri Devnarayan Jayanti, Makar Sankranti and Janmashtami, as the main religion is Hinduism. Rajasthan's desert festival is held once a year during winter. Dressed in brilliantly hued costumes, the people of the desert dance and sing ballads. There are fairs with snake charmers, puppeteers, acrobats and folk performers. Camels play a role in this festival.
Spirit possession has been documented in modern Rajasthan. Some of the spirits possessing Rajasthanis are seen as good and beneficial, while others are seen as malevolent. The good spirits include murdered royalty, the underworld god Bhaironji, and Muslim saints. Bad spirits include perpetual debtors who die in debt, stillborn infants, deceased widows, and foreign tourists. The possessed individual is referred to as a "ghorala" ("mount"). Possession, even if it is by a benign spirit, is regarded as undesirable, as it entails loss of self-control and violent emotional outbursts.
Education.
During recent years, Rajasthan has made significant progress in the area of education. The state government has been making sustained efforts to improve the education standard. In 2014, IIT, IAS, Medical and CA all India toppers are from Rajasthan.
In recent decades, the literacy rate of Rajasthan has increased significantly. In 1991, the state's literacy rate was only 38.55% (54.99% male and 20.44% female). In 2001, the literacy rate increased to 60.41% (75.70% male and 43.85% female). This was the highest leap in the percentage of literacy recorded in India (the rise in female literacy being 23%). At the Census 2011, Rajasthan had a literacy rate of 67.06% (80.51% male and 52.66% female). Although Rajasthan's literacy rate is below the national average of 74.04% and although its female literacy rate is the lowest in the country (closely followed by Bihar at 53.33%), the state has been praised for its efforts and achievements in raising male and female literacy rates.
Rajasthan has three of India's finest educational institutions,Birla Institute of Technology and Science, Pilani IIT Jodhpur and IIM Udaipur. Kota, Rajasthan, is known for its excellent coaching for the engineering and medical college entrance examinations. Rajasthan has nine universities and more than 250 colleges, 55,000 primary and 7,400 secondary schools. There are 41 engineering colleges with an annual enrolment of about 11,500 students. The state has 23 polytechnic colleges and 152 Industrial Training Institutes (ITIs) that impart vocational training.
In rural areas of Rajasthan, the literacy rate is 76.16% for males and 45.8% for females. This has been debated across all the party level except BJP, when the governor of Rajasthan set a minimum educational qualification for the village panchayat elections.
Tourism.
Rajasthan attracted 14 percent of total foreign visitors during 2009–2010 which is the fourth highest among Indian states. It is fourth also in Domestic tourist visitors. Endowed with natural beauty and a great history, tourism is a flourishing industry in Rajasthan. The palaces of Jaipur and Ajmer-Pushkar, the lakes of Udaipur, the desert forts of Jodhpur, Taragarh Fort (Star Fort) in Bundi, and Bikaner and Jaisalmer rank among the most preferred destinations in India for many tourists both Indian and foreign. Tourism accounts for eight percent of the state's domestic product. Many old and neglected palaces and forts have been converted into heritage hotels. Tourism has increased employment in the hospitality sector.
Rajasthan is famous for its forts, intricately carved temples, and decorated havelis, which were built by Rajput kings in pre-Muslim era Rajasthan. Rajasthan's Jaipur Jantar Mantar, Dilwara Temples, Chittorgarh Fort, Lake Palace, miniature paintings in Bundi, and numerous city palaces and havelis are an important part of the architectural heritage of India. Jaipur, the Pink City, is noted for the ancient houses made of a type of sand stone dominated by a pink hue. In Bundi, maximum houses are painted blue. At Ajmer, the white marble Bara-dari on the Anasagar lake is exquisite. Jain Temples dot Rajasthan from north to south and east to west. Dilwara Temples of Mount Abu, Ranakpur Temple dedicated to Lord Adinath in Pali District, Jain temples in the fort complexes of Chittor, Jaisalmer and Kumbhalgarh, Lodurva Jain temples, Mirpur Jain Temple, Sarun Mata Temple kotputli, Bhandasar and Karni Mata Temple of Bikaner are some of the best examples.
"Big Temples In Karauli district"

</doc>
<doc id="26292" url="http://en.wikipedia.org/wiki?curid=26292" title="Raphael of Brooklyn">
Raphael of Brooklyn

 Saint Raphael of Brooklyn (November 08, 1860 – February 27, 1915), also known as Father Raphael, was born as Raphael Hawaweeny (Arabic: رفائيل هواويني‎) in Beirut, Lebanon, of Damascene Syrian parents of Greek Orthodox Antiochite-Byzantine origin. He was first educated at the Damascus Patriarchal School that had become the leading Greek Orthodox institution of higher learning in the Levant under the leadership of Saint Joseph of Damascus. He furthered his study of Christian theology at the Patriarchical Halki seminary in Turkey, and at the Theological Academy in Kiev, Russian Empire (now Ukraine).
Father Raphael was sent to New York City in 1895 by Tsar Nicholas II of Russia to administer the local Orthodox Christian community which then included mainly Russian and Levantine immigrants. In 1904 he became the first Eastern Orthodox bishop to be consecrated in North America; the consecration was performed by Archbishop (Saint) Tikhon of Moscow and Bishop Innocent in New York City. He served as Bishop of Brooklyn until his death.
During the course of his ministry as an auxiliary bishop of the Russian Orthodox Church in America, St. Raphael founded the present-day cathedral of the Antiochian Orthodox Archdiocese of North America, established twenty-nine parishes and assisted in the founding of St. Tikhon's Orthodox Monastery.
Saint Raphael was originally buried in New York until August 1989 when his relics were translated to the Antiochian Village Camp in Ligonier, Pennsylvania, on property of the Antiochian Archdiocese along with several other bishops and clergy.
Bishop Raphael was glorified by the Holy Synod of the Orthodox Church in America (OCA) in its March 2000 session. He is commemorated by the OCA on February 27, the anniversary of his death and by the Antiochian Orthodox Church on the first Saturday of November.

</doc>
<doc id="26294" url="http://en.wikipedia.org/wiki?curid=26294" title="Reflux suppressant">
Reflux suppressant

A reflux suppressant is any one of a number of drugs used to combat oesophageal reflux. Commonly, following ingestion a 'raft' of alginic acid is created, floating on the stomach contents by carbon dioxide released by the drug. This forms a mechanical barrier to further reflux. Some preparations also contain antacids to protect the oesophagus.
Reflux can also be coincidentally reduced by the and antidopaminergics.

</doc>
<doc id="26295" url="http://en.wikipedia.org/wiki?curid=26295" title="Russian Civil War">
Russian Civil War

The Russian Civil War (Russian: Гражданская война́ в Росси́и "Grazhdanskaya voyna v Rossiy") (November 1917 – October 1922) was a multi-party war in the former Russian Empire immediately after the Russian Revolutions of 1917, as many factions vied to determine Russia's political future. The two largest combatant groups were the Red Army, fighting for the Bolshevik form of socialism, and the loosely allied forces known as the White Army, which included diverse interests favoring monarchism, capitalism, and alternative forms of socialism, each with democratic and antidemocratic variants. In addition, rival militant socialists and nonideological Green armies fought against both the Bolsheviks and the Whites. Eight foreign nations intervened against the Red Army, notably the Allied Forces and the pro-German armies. The Red Army defeated the White Armed Forces of South Russia in Ukraine and the army led by Aleksandr Kolchak in Siberia in 1919. The remains of the White forces commanded by Pyotr Nikolayevich Wrangel were beaten in Crimea and evacuated in late 1920. Lesser battles of the war continued in the periphery for two more years, with minor skirmishes with the remnants of the White forces in the Far East continuing well into 1923. Armed national resistance in Central Asia was not completely crushed until 1934.
Many pro-independence movements emerged after the break-up of the Russian Empire and fought in the war. Several parts of the former Russian Empire – Finland, Estonia, Latvia, Lithuania, and Poland – were established as sovereign states, with their own civil wars and wars of independence. Some of them re-established their independence, previously lost to Russia. The rest of the former Russian Empire was consolidated into the Soviet Union shortly afterwards.
Background.
February Revolution.
After the abdication of Tsar Nicholas II of Russia, the Russian Provisional Government was established during the February Revolution of 1917.
Creation of the Red Army.
In the wake of the October Revolution, the old Russian Imperial Army had been demobilized; the volunteer-based Red Guard was the Bolsheviks' main military force, augmented by an armed military component of the Cheka, the Bolshevik state security apparatus. In January, after significant reverses in combat, War Commissar Leon Trotsky headed the reorganization of the Red Guard into a "Workers' and Peasants' Red Army," in order to create a more professional fighting force. Political commissars were appointed to each unit of the army to maintain morale and ensure loyalty.
In June 1918, when it became apparent that a revolutionary army composed solely of workers would be far too small, Trotsky instituted mandatory conscription of the rural peasantry into the Red Army. Opposition of rural Russians to Red Army conscription units was overcome by taking hostages and shooting them when necessary in order to force compliance, exactly the same practices used by the White Army officers. Former Tsarist officers were utilized as "military specialists" ("voenspetsy"), sometimes their families were taken hostage in order to ensure their loyalty. At the start of the war, three quarters of the Red Army officer corps was composed of former Tsarist officers. By its end, 83% of all Red Army divisional and corps commanders were ex-Tsarist soldiers.
Anti-Bolshevik movement.
While resistance to the Red Guard began on the very next day after the Bolshevik uprising, the Treaty of Brest-Litovsk and the political ban became a catalyst for the formation of anti-Bolshevik groups both inside and outside Russia, pushing them into action against the new regime.
A loose confederation of anti-Bolshevik forces aligned against the Communist government, including land-owners, republicans, conservatives, middle-class citizens, reactionaries, pro-monarchists, liberals, army generals, non-Bolshevik socialists who still had grievances and democratic reformists, voluntarily united only in their opposition to Bolshevik rule. Their military forces, bolstered by forced conscriptions and terror and by foreign influence and led by General Yudenich, Admiral Kolchak and General Denikin, became known as the White movement (sometimes referred to as the "White Army"), and they controlled significant parts of the former Russian Empire for most of the war.
A Ukrainian nationalist movement known as the Green Army was active in Ukraine in the early part of the war. More significant was the emergence of an anarchist political and military movement known as the Revolutionary Insurrectionary Army of Ukraine or the Anarchist Black Army led by Nestor Makhno. The Black Army, which counted numerous Jews and Ukrainian peasants in its ranks, played a key part in halting General Denikin's White Army offensive towards Moscow during 1919, later ejecting Cossack forces from Crimea.
The remoteness of the Volga Region, the Ural Region, Siberia, and the Far East was favourable for the anti-Bolshevik powers, and the Whites set up a number of organizations in the cities of these regions. Some of the military forces were set up on the basis of clandestine officers' organisations in the cities.
The Czechoslovak Legions had been part of the Russian army and numbered around 30,000 troops by October 1917. They had an agreement with the new Bolshevik government to be evacuated from the Eastern Front via the Port of Vladivostok to France. The transport from the Eastern Front to the Port of Vladivostok slowed down in the chaos, and the troops became dispersed all along the Trans-Siberian Railway. Under pressure from the Central Powers, Trotsky ordered the disarmament and arrest of the legionaries, which created tensions with the Bolsheviks.
The Western Allies also expressed their dismay at the Bolsheviks. They were worried about (1) a possible Russo-German alliance, (2) the prospect of the Bolsheviks making good their threats to assume no responsibility for, and so default on, Imperial Russia's massive foreign loans and (3) that the communist revolutionary ideas would spread (a concern shared by many Central Powers). Hence, many of these countries expressed their support for the Whites, including the provision of troops and supplies. Winston Churchill declared that Bolshevism must be "strangled in its cradle". The British and the French had supported Russia on a massive scale with war materials. After the treaty, it looked like much of that material would fall into the hands of the Germans. Under this pretext began allied intervention in the Russian Civil War with the United Kingdom and France sending troops into Russian ports. There were violent confrontations with troops loyal to the Bolsheviks.
The German Empire created several short-lived satellite buffer states within its sphere of influence after the Treaty of Brest-Litovsk: the "United Baltic Duchy", "Duchy of Courland and Semigallia", "Kingdom of Lithuania", "Kingdom of Poland", the "Belarusian People’s Republic", and the "Ukrainian State". Following the defeat of Germany in World War I in November 1918, these states were abolished.
Finland was the first republic that declared its independence from Russia in December 1917 and established itself in the ensuing Finnish Civil War from January to May 1918. The Second Polish Republic, Lithuania, Latvia, and Estonia formed their armies immediately after the abolition of the Brest-Litovsk Treaty and the start of the Soviet westward offensive in November 1918.
Geography and chronology.
In the European part of Russia, the war was fought across three main fronts: the eastern, the southern, and the northwestern. It can also be roughly split into the following periods.
The first period lasted from the Revolution until the Armistice. Already on the date of the Revolution, Cossack General Kaledin refused to recognize it and assumed full governmental authority in the Don region, where the Volunteer Army began amassing support. The signing of the Treaty of Brest-Litovsk also resulted in direct Allied intervention in Russia and the arming of military forces opposed to the Bolshevik government. There were also many German commanders who offered support against the Bolsheviks, fearing a confrontation with them was impending as well.
During this first period, the Bolsheviks took control of Central Asia out of the hands of the Provisional Government and White Army, setting up a base for the Communist Party in the Steppe and Turkestan, where nearly two million Russian settlers were located.
Most of the fighting in this first period was sporadic, involving only small groups amid a fluid and rapidly shifting strategic scene. Among the antagonists were the Czechoslovaks, known as the Czechoslovak Legion or "White Czechs", the Poles of the Polish 5th Rifle Division, and the pro-Bolshevik Red Latvian riflemen.
The second period of the war lasted from January to November 1919. At first the White armies' advances from the south (under General Denikin), the east (under Admiral Kolchak), and the northwest (under General Yudenich) were successful, forcing the Red Army and its leftist allies back on all three fronts. In July 1919, the Red Army suffered another reverse after a mass defection of Red Army units in the Crimea to the anarchist Black Army under Nestor Makhno, enabling anarchist forces to consolidate power in Ukraine.
Leon Trotsky soon reformed the Red Army, concluding the first of two military alliances with the anarchists. In June, the Red Army first checked Kolchak's advance. After a series of engagements, assisted by a Black Army offensive against White supply lines, the Red Army defeated Denikin's and Yudenich's armies in October and November.
The third period of the war was the extended siege of the last White forces in the Crimea. Wrangel had gathered the remnants of Denikin's armies, occupying much of the Crimea. An attempted invasion of southern Ukraine was rebuffed by the anarchist Black Army under the command of Nestor Makhno. Pursued into the Crimea by Makhno's troops, Wrangel went over to the defensive in the Crimea. After an abortive move north against the Red Army, Wrangel's troops were forced south by Red Army and Black Army forces; Wrangel and the remains of his army were evacuated to Constantinople in November 1920.
Warfare.
October Revolution.
In the October Revolution, the Bolshevik Party directed the Red Guard (armed groups of workers and Imperial army deserters) to seize control of Petrograd (Saint Petersburg), and immediately began the armed takeover of cities and villages throughout the former Russian Empire. In January 1918, the Bolsheviks dissolved the Russian Constituent Assembly, and proclaimed the Soviets (workers’ councils) as the new government of Russia.
Initial anti-Bolshevik uprisings.
The first attempt to regain power from the Bolsheviks was made by the Kerensky-Krasnov uprising in October 1917. It was supported by the Junker Mutiny in Petrograd but was quickly put down by the Red Guard, notably the Latvian rifle division.
The initial groups that fought against the Communists were local Cossack armies that had declared their loyalty to the Provisional Government. Kaledin of the Don Cossacks and Semenov of the Siberian Cossacks were prominent among them. The leading Tsarist officers of the old regime also started to resist. In November, General Alekseev, the Tsar's Chief-of-Staff during the First World War, began to organise the Volunteer Army in Novocherkassk. Volunteers of this small army were mostly officers of the old Russian army, military cadets and students. In December 1917, Alekseev was joined by Kornilov, Denikin, and other Tsarist officers who had escaped from the jail where they had been imprisoned following the abortive Kornilov affair just before the Revolution. At the beginning of December 1917, groups of volunteers and Cossacks captured Rostov.
Having stated in the November 1917 “Declaration of Rights of Nations of Russia” that any nation under imperial Russian rule should be immediately given the power of self-determination, the Bolsheviks had begun to usurp the power of the Provisional Government in the territories of Central Asia soon after the establishment of the Turkestan Committee in Tashkent. In April 1917, the Provisional Government set up this committee, which was mostly made up of former tsarist officials. The Bolsheviks attempted to take control of the Committee in Tashkent on 12 September 1917, but their mission was unsuccessful, and many Bolshevik leaders were arrested. However, because the Committee lacked representation of the native population and poor Russian settlers, they had to release the Bolshevik prisoners almost immediately due to public outcry, and a successful takeover of this government body took place two months later in November. The success of the Bolshevik party over the Provisional Government during 1917 was mostly due to the support they received from the working class of Central Asia. The Leagues of Mohammedam Working People, which Russian settlers and natives who had been sent to work behind the lines for the Tsarist government in 1916 formed in March 1917, had led numerous strikes in the industrial centers throughout September 1917.
However, after the Bolshevik destruction of the Provisional Government in Tashkent, Muslim elites formed an autonomous government in Turkestan, commonly called the "Kokand autonomy" (or simply Kokand). The White Russians supported this government body, which lasted several months because of Bolshevik troop isolation from Moscow.
In January 1918 the Soviet forces under Lieutenant Colonel Muravyov invaded Ukraine and invested Kiev, where the Central Council of the Ukrainian People's Republic held power. With the help of the Kiev Arsenal Uprising, the Bolsheviks captured the city on 26 January.
Peace with the Central Powers.
The Bolsheviks decided to immediately make peace with the German Empire and the Central Powers, as they had promised the Russian people before the Revolution. Vladimir Lenin's political enemies attributed that decision to his sponsorship by the foreign office of Wilhelm II, German Emperor, offered to Lenin in hope that, with a revolution, Russia would withdraw from World War I. That suspicion was bolstered by the German Foreign Ministry's sponsorship of Lenin's return to Petrograd. However, after the military fiasco of the summer offensive (June 1917) by the Russian Provisional Government, and in particular after the failed summer offensive of the Provisional Government had devastated the structure of the Russian Army, it became crucial that Lenin realize the promised peace. Even before the failed summer offensive the Russian population was very skeptical about the continuation of the war. Western socialists had promptly arrived from France and from the UK to convince the Russians to continue the fight but could not change the new pacifist mood of Russia.
On 16 December 1917, an armistice was signed between Russia and the Central Powers in Brest-Litovsk and peace talks began. As a condition for peace, the proposed treaty by the Central Powers conceded huge portions of the former Russian Empire to the German Empire and the Ottoman Empire, greatly upsetting nationalists and conservatives. Leon Trotsky, representing the Bolsheviks, refused at first to sign the treaty while continuing to observe a unilateral cease fire, following the policy of "No war, no peace".
In view of this, on 18 February 1918, the Germans began Operation Faustschlag on the Eastern Front, encountering virtually no resistance in a campaign that lasted eleven days. Signing a formal peace treaty was the only option in the eyes of the Bolsheviks because the Russian army was demobilized, and the newly formed Red Guard was incapable of stopping the advance. They also understood that the impending counterrevolutionary resistance was more dangerous than the concessions of the treaty, which Lenin viewed as temporary in the light of aspirations for a world revolution.
The Soviets acceded to a peace treaty, and the formal agreement, the Treaty of Brest-Litovsk, was ratified on 6 March. The Soviets viewed the treaty as merely a necessary and expedient means to end the war. Therefore, they ceded large amounts of territory to the German Empire.
Ukraine, South Russia, and Caucasus 1918.
Under Soviet pressure, the Volunteer Army embarked on the epic Ice March from Yekaterinodar to Kuban on 22 February 1918, where they joined with the Kuban Cossacks to mount an abortive assault on Yekaterinodar. The Soviets recaptured Rostov on the next day. General Kornilov was killed in the fighting on 13 April, and General Denikin took over the command. Fighting off its pursuers without respite, the army succeeded in breaking its way through back towards the Don, where the Cossack uprising against Bolsheviks had started.
The Baku Soviet Commune was established on 13 April. Germany landed its Caucasus Expedition troops in Poti on 8 June. The Ottoman Army of Islam (in coalition with Azerbaijan) drove them out of Baku on 26 July 1918. Subsequently, the Dashanaks, Right SRs and Mensheviks started negotiations with General Dunsterville, the commander of the British troops in Persia. The Bolsheviks and their Left SR allies were opposed to it, but on 25 July the majority of the Soviet voted to call in the British, and the Bolsheviks resigned. The Baku Soviet Commune ended its existence and was replaced by the Central Caspian Dictatorship.
In June 1918, the Volunteer Army, numbering some 9,000 men, started its second Kuban campaign. Yekaterinodar was encircled on 1 August and fell on the 3rd. In September–October, heavy fighting took place at Armavir and Stavropol. On 13 October, General Kazanovich's division took Armavir, and on 1 November, general Pyotr Wrangel secured Stavropol. This time Red forces had no escape, and by the beginning of 1919, the whole Northern Caucasus was free from Bolsheviks.
In October, General Alekseev, the leader for the White armies in southern Russia, died of a heart attack. An agreement was reached between Denikin, head of the Volunteer Army, and PN Krasnov, Ataman of the Don Cossacks, which united their forces under the sole command of Denikin. The Armed Forces of South Russia were thus created.
Eastern Russia and Siberia, 1918.
The Revolt of the Czechoslovak Legion broke out in May 1918, and the legionaries took control of Chelyabinsk in June. Simultaneously, Russian officers' organisations overthrew the Bolsheviks in Petropavlovsk and in Omsk. Within a month the Whites controlled most of the Trans-Siberian Railroad from Lake Baikal to the Ural regions. During the summer, Bolshevik power in Siberia was eliminated. The Provisional Government of Autonomous Siberia formed in Omsk.
By the end of July, the Whites had extended their gains westwards, capturing Ekaterinburg on 26 July 1918. Shortly before the fall of Yekaterinburg on 17 July 1918, the former Tsar and his family were executed by the Ural Soviet to prevent them falling into the hands of the Whites.
The Mensheviks and Socialist-Revolutionaries supported peasants fighting against Soviet control of food supplies. In May 1918, with the support of the Czechoslovak Legion, they took Samara and Saratov, establishing the Committee of Members of the Constituent Assembly – known as the "Komuch". By July, the authority of the Komuch extended over much of the area controlled by the Czechoslovak Legion. The Komuch pursued an ambivalent social policy, combining democratic and even socialist measures, such as the institution of an eight-hour working day, with "restorative" actions, such as returning both factories and land to their former owners. After the fall of Kazan, Vladimir Lenin called for the dispatch of Petrograd workers to the Kazan Front: "We must send down the "maximum" number of Petrograd workers: (1) a few dozen 'leaders' like Kayurov; (2) a few thousand militants 'from the ranks'".
After a series of reverses at the front, War Commissar Trotsky instituted increasingly harsh measures in order to prevent unauthorized withdrawals, desertions, or mutinies in the Red Army. In the field, the Cheka special investigations forces, termed the "Special Punitive Department of the All-Russian Extraordinary Commission for Combat of Counter-Revolution and Sabotage", or "Special Punitive Brigades", followed the Red Army, conducting field tribunals and summary executions of soldiers and officers who deserted, retreated from their positions, or failed to display sufficient offensive zeal. Trotsky extended the use of the death penalty to the occasional political commissar whose detachment retreated or broke in the face of the enemy. In August, frustrated at continued reports of Red Army troops breaking under fire, Trotsky authorized the formation of barrier troops stationed behind unreliable Red Army units, with orders to shoot anyone withdrawing from the battle-line without authorisation.
In September 1918, Komuch, the Siberian Provisional Government, and other local anti-Soviet governments met in Ufa and agreed to form a new Provisional All-Russian Government in Omsk, headed by a Directory of five: three Socialist-Revolutionaries (Nikolai Avksentiev, Boldyrev, and Vladimir Zenzinov) and two Kadets, (VA Vinogradov and PV Vologodskii).
By the fall of 1918, Anti-Bolshevik White Forces in the east included the People's Army (Komuch), the Siberian Army (of the Siberian Provisional Government) and insurgent Cossack units of Orenburg, Ural, Siberia, Semirechye, Baikal, Amur, and Ussuri Cossacks, nominally under the orders of general VG Boldyrev, Commander-in-Chief, appointed by the Ufa Directorate.
On the Volga, Colonel Kappel's White detachment captured Kazan 7 August, but the Reds re-captured the city on 8 September 1918 following the Red counter-offensive. On the 11th, Simbirsk fell, and on 8 October, Samara. The Whites fell back eastwards to Ufa and Orenburg.
In Omsk, the Russian Provisional Government quickly came under the influence then dominance of its new War Minister, Rear-Admiral Kolchak. On 18 November, a coup d'état established Kolchak as dictator. The members of the Directory were arrested and Kolchak proclaimed the "Supreme Ruler of Russia".
By mid-December 1918, White armies in the east had to leave Ufa, but they balanced this failure with a successful drive towards Perm. Perm was taken on 24 December.
Central Asia 1918.
In February 1918 the Red Army overthrew the White Russian-supported Kokand autonomy of Turkestan. Although this move seemed to solidify Bolshevik power in Central Asia, more troubles soon arose for the Red Army as the Allied Forces began to intervene. British support of the White Army provided the greatest threat to the Red Army in Central Asia during 1918. Great Britain sent three prominent military leaders to the area. One was Lieutenant-Colonel Bailey, who recorded a mission to Tashkent, from where the Bolsheviks forced him to flee. Another was General Malleson, leading the Malleson Mission, who assisted the Mensheviks in Ashkhabad (now the capital of Turkmenistan) with a small Anglo-Indian force. However, he failed to gain control of Tashkent, Bukhara, and Khiva. The third was Major-General Dunsterville, who the Bolsheviks drove out of Central Asia only a month after his arrival in August 1918. Despite setbacks due to British invasions during 1918, the Bolsheviks continued to make progress in bringing the Central Asian population under their influence. The first regional congress of the Russian Communist Party convened in the city of Tashkent in June 1918 in order to build support for a local Bolshevik Party.
Left SR uprising.
In July, two Left SR and Cheka employees, Blyumkin and Andreyev, assassinated the German ambassador, Count Mirbach. In Moscow Left SR uprising was put down by the Bolsheviks, using the Cheka military detachments. Lenin personally apologised to the Germans for the assassination. Mass arrests of Socialist-Revolutionaries followed.
Estonia, Latvia, and Petrograd.
Estonia cleared its territory from the Red Army by January 1919. Baltic German volunteers captured Riga from the Red Latvian Riflemen on 22 May, but the Estonian 3rd Division defeated the Baltic Germans a month later, aiding the establishment of the Republic of Latvia.
This rendered possible another threat to the Red Army – one from General Yudenich, who had spent the summer organizing the Northwestern Army in Estonia with local and British support. In October 1919, he tried to capture Petrograd in a sudden assault with a force of around 20,000 men. The attack was well-executed, using night attacks and lightning cavalry maneuvers to turn the flanks of the defending Red Army. Yudenich also had six British tanks, which caused panic whenever they appeared. The Allies gave large quantities of aid to Yudenich, who, however, complained that he was receiving insufficient support.
By 19 October, Yudenich's troops had reached the outskirts of the city. Some members of the Bolshevik central committee in Moscow were willing to give up Petrograd, but Trotsky refused to accept the loss of the city and personally organized its defenses. He declared, "It is impossible for a little army of 15,000 ex-officers to master a working class capital of 700,000 inhabitants." He settled on a strategy of urban defense, proclaiming that the city would "defend itself on its own ground" and that the White Army would be lost in a labyrinth of fortified streets and there "meet its grave".
Trotsky armed all available workers, men and women, ordering the transfer of military forces from Moscow. Within a few weeks the Red Army defending Petrograd had tripled in size and outnumbered Yudenich three to one. At this point Yudenich, short of supplies, decided to call off the siege of the city and withdrew, repeatedly asking permission to withdraw his army across the border to Estonia. However, units retreating across the border were disarmed and interned by order of the Estonian government, which had entered into peace negotiations with the Soviet Government on 16 September and had been informed by the Soviet authorities of their 6 November decision that, should the White Army be allowed to retreat into Estonia, it would be pursued across the border by the Reds. In fact the Reds attacked Estonian army positions, and fighting continued until a ceasefire came into effect on 3 January 1920. Following the Treaty of Tartu most of Yudenich's soldiers went into exile.
The Finnish general Mannerheim planned a Finnish intervention to help the Whites in Russia capture Petrograd. He did not, however, gain the necessary support for the endeavor. Lenin considered it "completely certain, that the slightest aid from Finland would have determined the fate of Petrograd".
Northern Russia 1919.
The British occupied Murmansk and, alongside the Americans, seized Arkhangelsk. With the retreat of Kolchak in Siberia, they pulled their troops out of the cities before the winter trapped their forces in the port.
Siberia 1919.
At the beginning of March 1919, the general offensive of the Whites on the eastern front began. Ufa was retaken on 13 March; by mid-April, the White Army stopped at the Glazov-Chistopol-Bugulma-Buguruslan-Sharlyk line. Reds started their counter-offensive against Kolchak's forces at the end of April. The Red Army, led by the capable commander Tukhachevsky, captured Elabuga on 26 May, Sarapul on 2 June, and Izevsk on the 7th and continued to push forward. Both sides had victories and losses, but by the middle of summer the Red Army was larger than the White Army and had managed to recapture territory previously lost.
Following the abortive offensive at Chelyabinsk, the White armies withdrew beyond the Tobol. In September 1919, White offensive was launched against the Tobol front, the last attempt to change the course of events. But on 14 October, the Reds counterattacked and then began the uninterrupted retreat of the Whites to the east.
On 14 November 1919, the Red Army captured Omsk. Admiral Kolchak lost control of his government shortly after this defeat; White Army forces in Siberia essentially ceased to exist by December. Retreat of the eastern front by White armies lasted three months, until mid-February 1920, when the survivors, after crossing Lake Baikal, reached Chita area and joined Ataman Semenov's forces.
South Russia 1919.
The Cossacks had been unable to organize and capitalize on their successes at the end of 1918. By 1919 they had begun to run short of supplies. Consequently, when the Soviet counter-offensive began in January 1919 under the Bolshevik leader Antonov-Ovseenko, the Cossack forces rapidly fell apart. The Red Army captured Kiev on 3 February 1919.
Denikin's military strength continued to grow in the spring of 1919. During the several months in winter and spring of 1919, hard fighting with doubtful outcomes took place in the Donbass, where the attacking Bolsheviks met White forces. At the same time, Denikin's Armed Forces of South Russia (AFSR) completed the elimination of Red forces in the northern Caucasus and advanced towards Tsaritsyn. At the end of April and beginning of May, the AFSR attacked on all fronts from the Dnepr to the Volga, and by the beginning of the summer they had won numerous battles. French forces landed in Odessa but after having done almost no fighting, withdrew their troops on 8 April 1919. By mid-June the Reds were chased from the Crimea and from the Odessa area. Denikin's troops took the cities of Kharkov and Belgorod. At the same time White troops under Wrangel's command took Tsaritsyn on 17 June 1919. On 20 June, Denikin issued his famous "Moscow directive", ordering all AFSR units to get ready for a decisive offensive to take Moscow.
Although Great Britain had withdrawn its own troops from the theater, it continued to give significant military aid (money, weapons, food, ammunition, and some military advisors) to the White armies during 1919.
After the capture of Tsaritsyn, Wrangel pushed towards Saratov, but Trotsky, seeing the danger of the union with Kolchak, against whom the Red command was concentrating large masses of troops, repulsed his attempts with heavy losses. When Kolchak's army in the east began to retreat in June and July, the bulk of the Red Army, free now from any serious danger from Siberia, was directed against Denikin.
Denikin's forces constituted a real threat and for a time threatened to reach Moscow. The Red Army, stretched thin by fighting on all fronts, was forced out of Kiev on 30 August. Kursk and Orel were taken. The Cossack Don Army under the command of General Konstantin Mamontov continued north towards Voronezh, but there Tukhachevsky's army defeated them on 24 October. Tukhachevsky's army then turned towards yet another threat, the rebuilt Volunteer Army of General Denikin.
The high tide of the White movement against the Soviets had been reached in September 1919. By this time Denikin's forces were dangerously overextended. The White front had no depth or stability: it had become a series of patrols with occasional columns of slowly advancing troops without reserves. Lacking ammunition, artillery, and fresh reinforcements, Denikin's army was decisively defeated in a series of battles in October and November 1919. The Red Army recaptured Kiev on 17 December, and the defeated Cossacks fled back towards the Black Sea.
While the White armies were being routed in the center and the east, they had succeeded in driving Nestor Makhno's anarchist Black Army (formally known as the Revolutionary Insurrectionary Army of Ukraine) out of part of southern Ukraine and the Crimea. Despite this setback, Moscow was loath to aid Makhno and the Black Army and refused to provide arms to anarchist forces in Ukraine.
The main body of White forces, the Volunteers and the Don Army, pulled back towards the Don, to Rostov. The smaller body (Kiev and Odessa troops) withdrew to Odessa and the Crimea, which it had managed to protect from the Bolsheviks during the winter of 1919–1920.
Central Asia 1919.
By February 1919 the British government had pulled their military forces out of Central Asia. Despite this success for the Red Army, the White Army’s assaults in European Russia and other areas broke communication between Moscow and Tashkent. For a time, Central Asia was completely cut off from the Red Army forces in Siberia. Although this communication failure weakened the Red Army, the Bolsheviks continued their efforts to gain support for the Bolshevik Party in Central Asia by holding a second regional conference in March. During this conference a regional bureau of Muslim organizations of the Russian Bolshevik Party was formed. The Bolshevik Party continued to try and gain support among the native population by giving them the impression of better representation for the Central Asian population and throughout the end of the year were able to maintain harmony with the Central Asian people.
Communication difficulties with the Red Army forces in Siberia and European Russia ceased to be a problem by mid-November 1919. Due to Red Army success north of Central Asia, communication with Moscow was re-established, and the Bolsheviks were able to claim victory over the White Army in Turkestan.
South Russia, Ukraine, and Kronstadt 1920–21.
By the beginning of 1920, the main body of the Armed Forces of South Russia was rapidly retreating towards the Don, to Rostov. Denikin hoped to hold the crossings of the Don, rest, and reform his troops, but the White Army was not able to hold the Don area and at the end of February 1920, started a retreat across Kuban towards Novorossiysk. Slipshod evacuation of Novorossiysk proved to be a dark event for the White Army. About 40,000 men were evacuated by Russian and Allied ships from Novorossiysk to the Crimea, without horses or any heavy equipment, while about 20,000 men were left behind and either dispersed or captured by the Red Army.
Following the disastrous Novorossiysk evacuation, Denikin stepped down, and the military council elected Wrangel as the new Commander-in-Chief of the White Army. He was able to restore order with dispirited troops and reshape an army that could fight as a regular force again. This remained an organised force in the Crimea throughout 1920.
After Moscow's Bolshevik government signed a military and political alliance with Nestor Makhno and the Ukrainian anarchists, the Black Army attacked and defeated several regiments of Wrangel's troops in southern Ukraine, forcing him to retreat before he could capture that year's grain harvest. Stymied in his efforts to consolidate his hold, Wrangel then attacked north in an attempt to take advantage of recent Red Army defeats at the close of the Polish-Soviet War of 1919–1920. This offensive was eventually halted by the Red Army, and Wrangel's troops were forced to retreat to the Crimea in November 1920, pursued by both the Red and Black cavalry and infantry. Wrangel and the remains of his army were evacuated from the Crimea to Constantinople on 14 November 1920. Thus ended the struggle of Reds and Whites in Southern Russia.
After the defeat of Wrangel, the Red Army immediately repudiated its 1920 treaty of alliance with Nestor Makhno and attacked the anarchist Black Army; the campaign to liquidate Makhno and the Ukrainian anarchists began with an attempted assassination of Makhno by the Cheka agents. Angered by continued repression by the Bolshevik Communist government and its liberal use of the Cheka to put down anarchist elements, a naval mutiny erupted at Kronstadt, followed by peasant revolts. Red Army attacks on the anarchist forces and their sympathizers increased in ferocity throughout 1921.
Siberia and the Far East 1920–22.
In Siberia, Admiral Kolchak's army had disintegrated. He himself gave up command after the loss of Omsk and designated Grigory Semyonov as the new leader of the White Army in Siberia. Not long after this, Kolchak was arrested by the disaffected Czechoslovak Corps as he traveled towards Irkutsk without the protection of the army and turned over to the socialist Political Centre in Irkutsk. Six days later, this regime was replaced by a Bolshevik-dominated Military-Revolutionary Committee. On 6–7 February, Kolchak and his prime minister Victor Pepelyaev were shot and their bodies thrown through the ice of the frozen Angara River, just before the arrival of the White Army in the area.
Remnants of Kolchak's army reached Transbaikalia and joined Grigory Semyonov's troops, forming the Far Eastern army. With the support of the Japanese Army, it was able to hold Chita, but after withdrawal of Japanese soldiers from Transbaikalia, Semenov's position become untenable, and in November 1920 he was repulsed by the Red Army from Transbaikalia and took refuge in China.
The Japanese, who had plans to annex the Amur Krai, finally pulled their troops out as the Bolshevik forces gradually asserted control over the Russian Far East. On 25 October 1922, Vladivostok fell to the Red Army, and the Provisional Priamur Government was extinguished.
Aftermath.
Ensuing rebellion.
In central Asia, Red Army troops continued to face resistance into 1923, where "basmachi" (armed bands of Islamic guerrillas) had formed to fight the Bolshevik takeover. The Soviets engaged non-Russian peoples in Central Asia, like Magaza Masanchi, commander of the Dungan Cavalry Regiment, to fight against the Basmachis. The Communist Party did not completely dismantle this group until 1934.
General Anatoly Pepelyayev continued armed resistance in the Ayano-Maysky District until June 1923. The regions of Kamchatka and Northern Sakhalin remained under Japanese occupation until their treaty with the Soviet Union in 1925, when their forces were finally withdrawn.
Casualties.
The results of the civil war were momentous. Soviet demographer Boris Urlanis estimated total number of men killed in action in the Civil War and Polish-Soviet war as 300,000 (125,000 in the Red Army, 175,500 White armies and Poles) and the total number of military personnel dead from disease (on both sides) as 450,000.
During the Red Terror, the Cheka carried out at least 250,000 summary executions of "enemies of the people" with estimates reaching above a million.
Some 300,000–500,000 Cossacks were killed or deported during decossackization, out of a population of around three million. An estimated 100,000 Jews were killed in Ukraine, mostly by the White Army. Punitive organs of the All Great Don Cossack Host sentenced 25,000 people to death between May 1918 and January 1919. Kolchak's government shot 25,000 people in Ekaterinburg province alone.
At the end of the Civil War, the Russian SFSR was exhausted and near ruin. The droughts of 1920 and 1921, as well as the 1921 famine, worsened the disaster still further. Disease had reached pandemic proportions, with 3,000,000 dying of typhus alone in 1920. Millions more were also killed by widespread starvation, wholesale massacres by both sides, and pogroms against Jews in Ukraine and southern Russia. By 1922, there were at least 7,000,000 street children in Russia as a result of nearly 10 years of devastation from the Great War and the civil war.
Another one to two million people, known as the White émigrés, fled Russia – many with General Wrangel, some through the Far East, others west into the newly independent Baltic countries. These émigrés included a large part of the educated and skilled population of Russia.
The Russian economy was devastated by the war, with factories and bridges destroyed, cattle and raw materials pillaged, mines flooded, and machines damaged. The industrial production value descended to one seventh of the value of 1913, and agriculture to one third. According to "Pravda", "The workers of the towns and some of the villages choke in the throes of hunger. The railways barely crawl. The houses are crumbling. The towns are full of refuse. Epidemics spread and death strikes—industry is ruined."
It is estimated that the total output of mines and factories in 1921 had fallen to 20% of the pre-World War level, and many crucial items experienced an even more drastic decline. For example, cotton production fell to 5%, and iron to 2% of pre-war levels.
War Communism saved the Soviet government during the Civil War, but much of the Russian economy had ground to a standstill. The peasants responded to requisitions by refusing to till the land. By 1921, cultivated land had shrunk to 62% of the pre-war area, and the harvest yield was only about 37% of normal. The number of horses declined from 35 million in 1916 to 24 million in 1920, and cattle from 58 to 37 million. The exchange rate with the U.S. dollar declined from two rubles in 1914 to 1,200 in 1920.
With the end of the war, the Communist Party no longer faced an acute military threat to its existence and power. However, the perceived threat of another intervention, combined with the failure of socialist revolutions in other countries, most notably the German Revolution, contributed to the continued militarization of Soviet society. Although Russia experienced extremely rapid economic growth in the 1930s, the combined effect of World War I and the Civil War left a lasting scar in Russian society, and had permanent effects on the development of the Soviet Union.
British historian Orlando Figes has contended that the root of the Whites' defeat was their inability to dispel the popular image that they were dually associated with Tsarist Russia and supportive of a Tsarist restoration.
Brief Timeline.
October 1917 - Kerensky and his supporters flee Petrograd.
5 January 1918 - The Red Guard break up a meeting of the Constituent Assembly on Lenin's orders.
28 January 1918 - Trotsky sets up the Red army.
March 1918 - Bolsheviks move the Russian capital to Moscow from Petrograd for protection and better communications as it is in the centre of their territory.
14 October 1919 - Denikins army reaches Orel 300 km from Moscow.
22 October 1919 - White forces reach the outskirts of Petrograd. Trotsky organises a counterattack.
Early November 1919 - Western allies pull the plug on support for the whites. Troops begin to desert.
7 February 1920 - Kolchak is executed by the Bolsheviks after being handed over by the Czech Legion.
April 1920 - Poles are driven back into Poland by the Bolsheviks
1921 - Some groups continue to fight but the Whites are beaten.

</doc>
<doc id="26296" url="http://en.wikipedia.org/wiki?curid=26296" title="Ralph Abercromby">
Ralph Abercromby

Sir Ralph Abercromby KB (sometimes spelt Abercrombie) (7 October 1734 – 28 March 1801) was a Scottish soldier and politician. He rose to the rank of lieutenant-general in the British Army, was noted for his services during the Napoleonic Wars, and served as Commander-in-Chief, Ireland.
He twice served as MP for Clackmannanshire, and he was appointed Governor of Trinidad.
Biography.
He was the eldest son of George Abercromby of Tullibody, Clackmannanshire, and a brother of the advocate Alexander Abercromby, Lord Abercromby and General Sir Robert Abercromby. He was born at Menstrie Castle, Clackmannanshire. Educated at Rugby and at the University of Edinburgh, he was sent to Leipzig University in 1754 to study civil law with a view to career as an advocate.
Abercromby was a Freemason. He was a member of Canongate Kilwinning Lodge No 2, Edinburgh, Scotland
Seven Years War.
On returning from the continent, Abercromby expressed a strong preference for the military profession, and a cornet's commission was accordingly obtained for him (March 1756) in the 3rd Dragoon Guards. He served with his regiment in the Seven Years' War, and thus, the opportunity afforded him of studying the methods of Frederick the Great, who moulded his military character and formed his tactical ideas.
He rose through the intermediate grades to the rank of lieutenant-colonel of the regiment (1773) and brevet colonel in 1780, and in 1781, he became colonel of the King's Irish infantry. When that regiment was disbanded in 1783, he retired upon half pay.
Up to this time, he had scarcely been engaged in active service due to his disapproval of the government's policies and especially to his sympathies for the American colonists in their struggles for independence. His retirement is no doubt to be ascribed to similar feelings. On leaving the army, he took up political life as Member of Parliament for Clackmannanshire. After some time, however, this role proved uncongenial, so he settled at Edinburgh and devoted himself to the education of his children.
War service.
When France declared war against Great Britain in 1793, he resumed his duties. He was appointed command of a brigade under the Duke of York for service in the Netherlands, where he commanded the advanced guard in the action at Le Cateau. During the 1794 withdrawal to Holland, he commanded the relief column at the fateful action at Boxtel and was wounded at Nijmegen. In 1795, he was appointed a Knight of the Bath for his services.
That same year, he was appointed to succeed Sir Charles Grey as commander-in-chief of the British forces in the West Indies. In 1796, Grenada was suddenly attacked and taken by a detachment of the army under his orders. Afterwards, Abercromby secured possession of the settlements of Demerara and Essequibo in South America and of the islands of Saint Lucia, Saint Vincent and Trinidad.
In this part of his career, Abercromby was involved in crushing the revolt of the Garifuna (Carib) people on Saint Vincent, bringing to an end their centuries-long resistance to European colonization. One of Abercromby's officers killed the Garifuna chief, Joseph Chatoyer, on 14 March 1795. While this was a minor campaign on the scale of Abercromby's overall career, it is well remembered up to the present on Saint Vincent, where Chatoyer is revered as a national hero.
On 17 April 1797, Abercromby invaded the island of Puerto Rico with a force of 7,000-13,000 men, which included German mercenary soldiers, Royal Marines, and a 60 to 64 ship armada. Island Governor and Captain General Don Ramón de Castro and his forces, consisting of the mostly Puerto Rican born Regimiento Fijo de Puerto Rico and the Milicias Disciplinadas, repelled the attack.
After two weeks of fierce combat, which included prolonged artillery exchanges and even hand-to-hand combat, Abercromby was unable to overcome San Juan's first line of defence and withdrew. This was to be one of the largest invasions to Spanish territories in the Americas.
Abercromby returned to Europe, and in reward for his important services, was appointed colonel of the regiment of Scots Greys; entrusted with the governments of the Isle of Wight, Fort-George, and Fort-Augustus; and raised to the rank of lieutenant-general. From 1797 to 1798, he was Commander-in-Chief of the forces in Ireland.
To quote the biographic entry in the 1888 Encyclopædia Britannica, "There he laboured to maintain the discipline of the army, to suppress the rising rebellion, and to protect the people from military oppression, with the care worthy of a great general and an enlightened and beneficent statesman. When he was appointed to the command in Ireland, an invasion of that country by the French was confidently anticipated by the British government. He used his utmost efforts to restore the discipline of an army that was utterly disorganized; and, as a first step, he anxiously endeavoured to protect the people by re-establishing the supremacy of the civil power, and not allowing the military to be called out, except when it was indispensably necessary for the enforcement of the law and the maintenance of order.
Finding that he received no adequate support from the head of the Irish government, and that all his efforts were opposed and thwarted by those who presided in the councils of Ireland, he resigned the command. His departure from Ireland was deeply lamented by the reflecting portion of the people, and was speedily followed by those disastrous results which he had anticipated, and which he so ardently desired and had so wisely endeavoured to prevent."
After holding for a short period the office of commander-in-chief in Scotland, Sir Ralph, when the enterprise against the Dutch Batavian Republic was resolved upon in 1799, was again called to command under the Duke of York. The campaign of 1799 ended in disaster, but friend and foe alike confessed that the most decisive victory could not have more conspicuously proved the talents of this distinguished officer.
In 1801, he was sent with an army to recover the Egypt from France. His experience in the Netherlands and the West Indies particularly fitted him for this new command, as was proved when he carried his army in health, in spirits, and with the requisite supplies to the destined scene of action despite great difficulties. The debarkation of the troops at Abukir, in the face of strenuous opposition, is justly ranked among the most daring and brilliant exploits of the British army.
Death.
Abercromby was injured at the Battle of Alexandria on 21 March 1801 and died seven days after the battle in HMS "Foudroyant", which was moored in the harbour.
His old friend, the commander of the Duke of York, paid a tribute to the soldier's memory in general orders: "His steady observance of discipline, his ever-watchful attention to the health and wants of his troops, the persevering and unconquerable spirit which marked his military career, the splendour of his actions in the field and the heroism of his death, are worthy the imitation of all who desire, like him, a life of heroism and a death of glory." He was buried in the Commandery of the Grand Master, the Knights of St John, Malta.
By a vote of the House of Commons, a monument was erected in his honour in St Paul's Cathedral in Abercromby Square, Liverpool. His widow was created Baroness Abercromby of Tullibody and Aboukir Bay, and a pension of £2,000 a year was settled on her and her two successors in the title.
Family.
On 17 November 1767, Abercromby married Mary Anne, daughter of John Menzies and Ann, daughter of Patrick Campbell. They had seven children. Of four sons, all four entered Parliament, and two saw military service.
Popular culture.
A public house in central Manchester, the 'Sir Ralph Abercromby', is named after him. There is also a "General Abercrombie" pub with his portrait by Hoppner as the sign off of the Blackfriars Bridge Road in London.

</doc>
<doc id="26297" url="http://en.wikipedia.org/wiki?curid=26297" title="Ripe">
Ripe

Ripe may refer to:
Persons with the name Ripe:

</doc>
<doc id="26298" url="http://en.wikipedia.org/wiki?curid=26298" title="Radiometric dating">
Radiometric dating

Radiometric dating (often called radioactive dating) is a technique used to date materials such as rocks or carbon, usually based on a comparison between the observed abundance of a naturally occurring radioactive isotope and its decay products, using known decay rates. The use of radiometric dating was first published in 1907 by Bertram Boltwood and is now the principal source of information about the absolute age of rocks and other geological features, including the age of the Earth itself, and can be used to date a wide range of natural and man-made materials. 
Together with stratigraphic principles, radiometric dating methods are used in geochronology to establish the geological time scale. Among the best-known techniques are radiocarbon dating, potassium-argon dating and uranium-lead dating. By allowing the establishment of geological timescales, it provides a significant source of information about the ages of fossils and the deduced rates of evolutionary change. Radiometric dating is also used to date archaeological materials, including ancient artifacts. 
Different methods of radiometric dating vary in the timescale over which they are accurate and the materials to which they can be applied.
Fundamentals of radiometric dating.
Radioactive decay.
All ordinary matter is made up of combinations of chemical elements, each with its own atomic number, indicating the number of protons in the atomic nucleus. Additionally, elements may exist in different isotopes, with each isotope of an element differing in the number of neutrons in the nucleus. A particular isotope of a particular element is called a nuclide. Some nuclides are inherently unstable. That is, at some point in time, an atom of such a nuclide will undergo radioactive decay and spontaneously transform into a different nuclide. This transformation may be accomplished in a number of different ways, including alpha decay (emission of alpha particles) and beta decay (electron emission, positron emission, or electron capture). Another possibility is spontaneous fission into two or more nuclides.
While the moment in time at which a particular nucleus decays is unpredictable, a collection of atoms of a radioactive nuclide decays exponentially at a rate described by a parameter known as the half-life, usually given in units of years when discussing dating techniques. After one half-life has elapsed, one half of the atoms of the nuclide in question will have decayed into a "daughter" nuclide or decay product. In many cases, the daughter nuclide itself is radioactive, resulting in a decay chain, eventually ending with the formation of a stable (nonradioactive) daughter nuclide; each step in such a chain is characterized by a distinct half-life. In these cases, usually the half-life of interest in radiometric dating is the longest one in the chain, which is the rate-limiting factor in the ultimate transformation of the radioactive nuclide into its stable daughter. Isotopic systems that have been exploited for radiometric dating have half-lives ranging from only about 10 years (e.g., tritium) to over 100 billion years (e.g., Samarium-147).
For most radioactive nuclides, the half-life depends solely on nuclear properties and is essentially a constant. It is not affected by external factors such as temperature, pressure, chemical environment, or presence of a magnetic or electric field. The only exceptions are nuclides that decay by the process of electron capture, such as beryllium-7, strontium-85, and zirconium-89, whose decay rate may be affected by local electron density. For all other nuclides, the proportion of the original nuclide to its decay products changes in a predictable way as the original nuclide decays over time. This predictability allows the relative abundances of related nuclides to be used as a clock to measure the time from the incorporation of the original nuclides into a material to the present.
Accuracy of radiometric dating.
The basic equation of radiometric dating requires that neither the parent nuclide nor the daughter product can enter or leave the material after its formation. The possible confounding effects of contamination of parent and daughter isotopes have to be considered, as do the effects of any loss or gain of such isotopes since the sample was created. It is therefore essential to have as much information as possible about the material being dated and to check for possible signs of alteration. Precision is enhanced if measurements are taken on multiple samples from different locations of the rock body. Alternatively, if several different minerals can be dated from the same sample and are assumed to be formed by the same event and were in equilibrium with the reservoir when they formed, they should form an isochron. This can reduce the problem of contamination. In uranium-lead dating, the concordia diagram is used which also decreases the problem of nuclide loss. Finally, correlation between different isotopic dating methods may be required to confirm the age of a sample. For example, a study of the Amitsoq gneisses from western Greenland used five different radiometric dating methods to examine twelve samples and achieved agreement to within 30 Ma (million years) on an age of 3,640 Ma.
Accurate radiometric dating generally requires that the parent has a long enough half-life that it will be present in significant amounts at the time of measurement (except as described below under "Dating with short-lived extinct radionuclides"), the half-life of the parent is accurately known, and enough of the daughter product is produced to be accurately measured and distinguished from the initial amount of the daughter present in the material. The procedures used to isolate and analyze the parent and daughter nuclides must be precise and accurate. This normally involves isotope ratio mass spectrometry.
The precision of a dating method depends in part on the half-life of the radioactive isotope involved. For instance, carbon-14 has a half-life of 5,730 years. After an organism has been dead for 60,000 years, so little carbon-14 is left that accurate dating can not be established. On the other hand, the concentration of carbon-14 falls off so steeply that the age of relatively young remains can be determined precisely to within a few decades.
Closure temperature.
If a material that selectively rejects the daughter nuclide is heated, any daughter nuclides that have been accumulated over time will be lost through diffusion, setting the isotopic "clock" to zero. The temperature at which this happens is known as the closure temperature or blocking temperature and is specific to a particular material and isotopic system. These temperatures are experimentally determined in the lab by artificially resetting sample minerals using a high-temperature furnace. As the mineral cools, the crystal structure begins to form and diffusion of isotopes is less easy. At a certain temperature, the crystal structure has formed sufficiently to prevent diffusion of isotopes. This temperature is what is known as closure temperature and represents the temperature below which the mineral is a closed system to isotopes. Thus an igneous or metamorphic rock or melt, which is slowly cooling, does not begin to exhibit measurable radioactive decay until it cools below the closure temperature. The age that can be calculated by radiometric dating is thus the time at which the rock or mineral cooled to closure temperature. Dating of different minerals and/or isotope systems (with differing closure temperatures) within the same rock can therefore enable the tracking of the thermal history of the rock in question with time, and thus the history of metamorphic events may become known in detail. This field is known as thermochronology or thermochronometry.
The age equation.
The mathematical expression that relates radioactive decay to geologic time is
where
The equation is most conveniently expressed in terms of the measured quantity "N"(t) rather than the constant initial value "No".
The above equation makes use of information on the composition of parent and daughter isotopes at the time the material being tested cooled below its closure temperature. This is well-established for most isotopic systems. However, construction of an isochron does not require information on the original compositions, using merely the present ratios of the parent and daughter isotopes to a standard isotope. Plotting an isochron is used to solve the age equation graphically and calculate the age of the sample and the original composition.
Modern dating methods.
Radiometric dating has been carried out since 1905 when it was invented by Ernest Rutherford as a method by which one might determine the age of the Earth. In the century since then the techniques have been greatly improved and expanded. Dating can now be performed on samples as small as a nanogram using a mass spectrometer. The mass spectrometer was invented in the 1940s and began to be used in radiometric dating in the 1950s. It operates by generating a beam of ionized atoms from the sample under test. The ions then travel through a magnetic field, which diverts them into different sampling sensors, known as "Faraday cups", depending on their mass and level of ionization. On impact in the cups, the ions set up a very weak current that can be measured to determine the rate of impacts and the relative concentrations of different atoms in the beams.
Uranium-lead dating method.
The uranium-lead radiometric dating scheme has been refined to the point that the error margin in dates of rocks can be as low as less than two million years in two-and-a-half billion years. An error margin of 2–5% has been achieved on younger Mesozoic rocks.
Uranium-lead dating is often performed on the mineral zircon (ZrSiO4), though it can be used on other materials, such as baddeleyite. Zircon and baddeleyite incorporate uranium atoms into their crystalline structure as substitutes for zirconium, but strongly reject lead. Zircon has a very high closure temperature, is resistant to mechanical weathering and is very chemically inert. Zircon also forms multiple crystal layers during metamorphic events, which each may record an isotopic age of the event. "In situ" micro-beam analysis can be achieved via laser ICP-MS or SIMS techniques.
One of its great advantages is that any sample provides two clocks, one based on uranium-235's decay to lead-207 with a half-life of about 700 million years, and one based on uranium-238's decay to lead-206 with a half-life of about 4.5 billion years, providing a built-in crosscheck that allows accurate determination of the age of the sample even if some of the lead has been lost. This can be seen in the concordia diagram, where the samples plot along an errorchron (straight line) which intersects the concordia curve at the age of the sample.
Samarium-neodymium dating method.
This involves the alpha-decay of 147Sm to 143Nd with a half-life of 1.06 x 1011 years. Accuracy levels of within twenty million years in two-and-a-half billion years are achievable.
Potassium-argon dating method.
This involves electron capture or positron decay of potassium-40 to argon-40. Potassium-40 has a half-life of 1.3 billion years, and so this method is applicable to the oldest rocks. Radioactive potassium-40 is common in micas, feldspars, and hornblendes, though the closure temperature is fairly low in these materials, about 125°C (mica) to 450°C (hornblende).
Rubidium-strontium dating method.
This is based on the beta decay of rubidium-87 to strontium-87, with a half-life of 50 billion years. This scheme is used to date old igneous and metamorphic rocks, and has also been used to date lunar samples. Closure temperatures are so high that they are not a concern. Rubidium-strontium dating is not as precise as the uranium-lead method, with errors of 30 to 50 million years for a 3-billion-year-old sample.
Uranium-thorium dating method.
A relatively short-range dating technique is based on the decay of uranium-234 into thorium-230, a substance with a half-life of about 80,000 years. It is accompanied by a sister process, in which uranium-235 decays into protactinium-231, which has a half-life of 34,300 years.
While uranium is water-soluble, thorium and protactinium are not, and so they are selectively precipitated into ocean-floor sediments, from which their ratios are measured. The scheme has a range of several hundred thousand years. A related method is ionium-thorium dating, which measures the ratio of ionium (thorium-230) to thorium-232 in ocean sediment.
Radiocarbon dating method.
 Carbon-14 is a radioactive isotope of carbon, with a half-life of 5,730 years, which is very short compared with the above isotopes. In other radiometric dating methods, the heavy parent isotopes were produced by nucleosynthesis in supernovas, meaning that any parent isotope with a short half-life should be extinct by now. Carbon-14, though, is continuously created through collisions of neutrons generated by cosmic rays with nitrogen in the upper atmosphere and thus remains at a near-constant level on Earth. The carbon-14 ends up as a trace component in atmospheric carbon dioxide (CO2).
An organism acquires carbon during its lifetime. Plants acquire it through photosynthesis, and animals acquire it from consumption of plants and other animals. When an organism dies, it ceases to take in new carbon-14, and the existing isotope decays with a characteristic half-life (5730 years). The proportion of carbon-14 left when the remains of the organism are examined provides an indication of the time elapsed since its death. The carbon-14 dating limit lies around 58,000 to 62,000 years.
The rate of creation of carbon-14 appears to be roughly constant, as cross-checks of carbon-14 dating with other dating methods show it gives consistent results. However, local eruptions of volcanoes or other events that give off large amounts of carbon dioxide can reduce local concentrations of carbon-14 and give inaccurate dates. The releases of carbon dioxide into the biosphere as a consequence of industrialization have also depressed the proportion of carbon-14 by a few percent; conversely, the amount of carbon-14 was increased by above-ground nuclear bomb tests that were conducted into the early 1960s. Also, an increase in the solar wind or the Earth's magnetic field above the current value would depress the amount of carbon-14 created in the atmosphere. These effects are corrected for by the calibration of the radiocarbon dating scale.
Fission track dating method.
This involves inspection of a polished slice of a material to determine the density of "track" markings left in it by the spontaneous fission of uranium-238 impurities. The uranium content of the sample has to be known, but that can be determined by placing a plastic film over the polished slice of the material, and bombarding it with slow neutrons. This causes induced fission of 235U, as opposed to the spontaneous fission of 238U. The fission tracks produced by this process are recorded in the plastic film. The uranium content of the material can then be calculated from the number of tracks and the neutron flux.
This scheme has application over a wide range of geologic dates. For dates up to a few million years micas, tektites (glass fragments from volcanic eruptions), and meteorites are best used. Older materials can be dated using zircon, apatite, titanite, epidote and garnet which have a variable amount of uranium content. Because the fission tracks are healed by temperatures over about 200°C the technique has limitations as well as benefits. The technique has potential applications for detailing the thermal history of a deposit.
Chlorine-36 dating method.
Large amounts of otherwise rare 36Cl were produced by irradiation of seawater during atmospheric detonations of nuclear weapons between 1952 and 1958. The residence time of 36Cl in the atmosphere is about 1 week. Thus, as an event marker of 1950s water in soil and ground water, 36Cl is also useful for dating waters less than 50 years before the present. 36Cl has seen use in other areas of the geological sciences, including dating ice and sediments.
Luminescence dating methods.
Natural sources of radiation in the environment knock loose electrons in, say, a piece of pottery, and these electrons accumulate in defects in the material's crystal lattice structure. Heating or illuminating the object will release the captured electrons, producing a luminescence. When the sample is heated, at a certain temperature it will glow from the emission of electrons released from the defects, and this glow can be used to estimate the age of the sample to a threshold of approximately 15 percent of its true age. The date of a rock is reset when volcanic activity remelts it. The date of a piece of pottery is reset by the heat of the kiln. Typically temperatures greater than 400 degrees Celsius will reset the "clock". This is termed thermoluminescence.
Other methods.
Other methods include:
Dating with short-lived extinct radionuclides.
Absolute radiometric dating requires a measurable fraction of parent nucleus to remain in the sample rock. For rocks dating back to the beginning of the solar system, this requires extremely long-lived parent isotopes, making measurement of such rocks' exact ages imprecise. To be able to distinguish the relative ages of rocks from such old material, and to get a better time resolution than that available from long-lived isotopes, short-lived isotopes that are no longer present in the rock can be used.
At the beginning of the solar system, there were several relatively short-lived radionuclides like 26Al, 60Fe, 53Mn, and 129I present within the solar nebula. These radionuclides—possibly produced by the explosion of a supernova—are extinct today, but their decay products can be detected in very old material, such as that which constitutes meteorites. By measuring the decay products of extinct radionuclides with a mass spectrometer and using isochronplots, it is possible to determine relative ages of different events in the early history of the solar system. Dating methods based on extinct radionuclides can also be calibrated with the U-Pb method to give absolute ages. Thus both the approximate age and a high time resolution can be obtained. Generally a shorter half-life leads to a higher time resolution at the expense of timescale.
The 129I – 129Xe chronometer.
129I beta-decays to 129Xe with a half-life of 16 million years. The iodine-xenon chronometer is an isochron technique. Samples are exposed to neutrons in a nuclear reactor. This converts the only stable isotope of iodine (127I) into 128Xe via neutron capture followed by beta decay (of 128I). After irradiation, samples are heated in a series of steps and the xenon isotopic signature of the gas evolved in each step is analysed. When a consistent 129Xe/128Xe ratio is observed across several consecutive temperature steps, it can be interpreted as corresponding to a time at which the sample stopped losing xenon.
Samples of a meteorite called Shallowater are usually included in the irradiation to monitor the conversion efficiency from 127I to 128Xe. The difference between the measured 129Xe/128Xe ratios of the sample and Shallowater then corresponds to the different ratios of 129I/127I when they each stopped losing xenon. This in turn corresponds to a difference in age of closure in the early solar system.
The 26Al – 26Mg chronometer.
Another example of short-lived extinct radionuclide dating is the 26Al – 26Mg chronometer, which can be used to estimate the relative ages of chondrules. 26Al decays to 26Mg with a half-life of 720 000 years. The dating is simply a question of finding the deviation from the natural abundance of 26Mg (the product of 26Al decay) in comparison with the ratio of the stable isotopes 27Al/24Mg.
The excess of 26Mg (often designated 26Mg* ) is found by comparing the 26Mg/27Mg ratio to that of other Solar System materials.
The 26Al – 26Mg chronometer gives an estimate of the time period for formation of primitive meteorites of only a few million years (1.4 million years for Chondrule formation).

</doc>
<doc id="26301" url="http://en.wikipedia.org/wiki?curid=26301" title="Rocket">
Rocket

A rocket (Italian "rocchetta"‚ "little fuse") is a missile, spacecraft, aircraft or other vehicle that obtains thrust from a rocket engine. Rocket engine exhaust is formed entirely from propellant carried within the rocket before use. Rocket engines work by action and reaction. Rocket engines push rockets forward by expelling their exhaust in the opposite direction at high speed. Rockets rely on momentum, airfoils, auxiliary reaction engines, gimballed thrust, momentum wheels, deflection of the exhaust stream, propellant flow, spin, and/or gravity to help control flight.
Rockets are relatively lightweight and powerful, capable of generating large accelerations and of attaining extremely high speeds with reasonable efficiency. Rockets are not reliant on the atmosphere and work very well in space.
Rockets for military and recreational uses date back to at least 13th century China. Significant scientific, interplanetary and industrial use did not occur until the 20th century, when rocketry was the enabling technology for the Space Age, including setting foot on the moon. Rockets are now used for fireworks, weaponry, ejection seats, launch vehicles for artificial satellites, human spaceflight, and space exploration.
Chemical rockets are the most common type of high power rocket, typically creating a high speed exhaust by the combustion of fuel with an oxidizer. The stored propellant can be a simple pressurized gas or a single liquid fuel that disassociates in the presence of a catalyst (monopropellants), two liquids that spontaneously react on contact (hypergolic propellants), two liquids that must be ignited to react, a solid combination of one or more fuels with one or more oxidizers (solid fuel), or solid fuel with liquid oxidant (hybrid propellant system). Chemical rockets store a large amount of energy in an easily released form, and can be very dangerous. However, careful design, testing, construction and use minimizes risks.
History of rockets.
In antiquity.
The availability of black powder (gunpowder) to propel projectiles was a precursor to experiments as weapons such as bombs, cannon, incendiary fire arrows and rocket-propelled fire arrows. The discovery of gunpowder was probably the product of centuries of alchemical experimentation in which Taoist alchemists were trying to create an elixir of immortality that would allow the person ingesting it to become physically immortal. However, anyone with a wood fire might have observed the acceleration of combustion that accidentally-chosen saltpetre-containing rocks would have produced.
Exactly when the first flights of rockets occurred is contested.
Merely lighting a centimeter-sized solid lump of gunpowder on one side can cause it to move via reaction (even without a nozzle for efficiency), so confinement in a tube and other design refinements may easily have followed for the experimentally-minded with ready access to saltpetre.
A problem for dating the first rocket flight is that Chinese "fire arrows" can be either arrows with explosives attached, or arrows propelled by gunpowder. There were reports of fire arrows and 'iron pots' that could be heard for 5 leagues (25 km, or 15 miles) when they exploded, causing devastation for a radius of 600 meters (2,000 feet), apparently due to shrapnel. A common claim is that the first recorded use of a rocket in battle was by the Chinese in 1232 against the Mongol hordes at Kai Feng Fu. However, the lowering of iron pots there may have been a way for a besieged army to blow up invaders. A scholarly reference occurs in the Ko Chieh Ching Yuan (The Mirror of Research), states that in 998 AD a man named Tang Fu invented a fire arrow of a new kind having an iron head.
Less controversially, one of the earliest devices recorded that used internal-combustion rocket propulsion, was the 'ground-rat,' a type of firework recorded in 1264 as having frightened the Empress-Mother Kung Sheng at a feast held in her honor by her son the Emperor Lizong.
Subsequently, one of the earliest texts to mention the use of rockets was the "Huolongjing", written by the Chinese artillery officer Jiao Yu in the mid-14th century. This text also mentioned the use of the first known multistage rocket, the 'fire-dragon issuing from the water' (huo long chu shui), used mostly by the Chinese navy.
Spread of rocket technology.
Rocket technology was first known to Europeans following its use by the Mongols Genghis Khan and Ögedei Khan when they conquered parts of Russia, Eastern, and Central Europe. The Mongolians had acquired the Chinese technology by conquest of the northern part of China and by the subsequent employment of Chinese rocketry experts as mercenaries for the Mongol military. Reports of the Battle of Mohi in the year 1241 describe the use of rocket-like weapons by the Mongols against the Magyars. Rocket technology also spread to Korea, where the 15th century hwacha wheeled cart was used as a platform to launch singijeon fire arrows.
Additionally, the spread of rockets into Europe was also influenced by the Ottomans at the siege of Constantinople in 1453, although it is very likely that the Ottomans themselves were influenced by the Mongol invasions of the previous few centuries. In their history of rockets published on the Internet, NASA says "Rockets appear in Arab literature in 1258 A.D., describing Mongol invaders' use of them on February 15 to capture the city of Baghdad."
Between 1270 and 1280, Hasan al-Rammah wrote "al-furusiyyah wa al-manasib al-harbiyya" ("The Book of Military Horsemanship and Ingenious War Devices"), which included 107 gunpowder recipes, 22 of which are for rockets. According to Ahmad Y Hassan, al-Rammah's recipes were more explosive than rockets used in China at the time. The terminology used by al-Rammah indicated a Chinese origin for the gunpowder weapons he wrote about, such as rockets and fire lances. Ibn al-Baytar, an Arab from Spain who had immigrated to Egypt, gave the name "snow of China" (Arabic: ثلج الصين‎ thalj al-ṣīn) to describe saltpetre. Al-Baytar died in 1248. The earlier Arab historians call saltpeter "Chinese snow" and " Chinese salt;" The Arabs also used the name "Chinese arrows" to refer to rockets. The Arabs attached "Chinese" to various names for gunpowder related objects. "Chinese flowers" was the name for fireworks, while "Chinese Snow" was given to saltpeter and "Chinese arrows" to rockets. While saltpeter was called "Chinese Snow" by Arabs, it was called "Chinese salt" (Persian: نمک چینی‎ "namak-i čīnī") by the Iranians, or "salt from the Chinese marshes" (namak shūra chīnī Persian: نمک شوره چيني‎).
The name "Rocket" comes from the Italian "Rocchetta" (i.e. "little fuse"), a name of a small firecracker created by the Italian artificer Muratori in 1379. Rockets for military use were first mentioned in the West (Europe) at around this time in connection with the Battle of Chioggia between the Genoese and Venetians in 1380.
Konrad Kyeser described rockets in his famous military treatise Bellifortis around 1405.
Between 1529 and 1556 Conrad Haas wrote a book that described rocket technology that combined fireworks and weapons technologies. This manuscript was discovered in 1961, in the Sibiu public records (Sibiu public records "Varia II 374"). His work dealt with the theory of motion of multi-stage rockets, different fuel mixtures using liquid fuel, and introduced delta-shape fins and bell-shaped nozzles.
"Lagari Hasan Çelebi" was a legendary Ottoman aviator who, according to an account written by Evliya Çelebi, made a successful manned rocket flight. Evliya Çelebi purported that in 1633 Lagari Hasan Çelebi launched in a 7-winged rocket using 50 okka (140 lbs) of gunpowder from Sarayburnu, the point below Topkapı Palace in Istanbul.
For over two centuries, the work of Polish-Lithuanian Commonwealth nobleman Kazimierz Siemienowicz "Artis Magnae Artilleriae pars prima" ("Great Art of Artillery, the First Part", also known as "The Complete Art of Artillery"), was used in Europe as a basic artillery manual. First printed in Amsterdam in 1650 it was translated to French in 1651, German in 1676, English and Dutch in 1729 and Polish in 1963. The book provided the standard designs for creating rockets, fireballs, and other pyrotechnic devices. It contained a large chapter on caliber, construction, production and properties of rockets (for both military and civil purposes), including multi-stage rockets, batteries of rockets, and rockets with delta wing stabilizers (instead of the common guiding rods ("bottle rockets"), which are also aerodynamic stabilizers but less efficient than fins).
Metal-cylinder rocket artillery.
In 1792, the first iron-cased rockets were successfully developed and used by Hyder Ali and his son Tipu Sultan, rulers of the Kingdom of Mysore in India against the larger British East India Company forces during the Anglo-Mysore Wars. The British then took an active interest in the technology and developed it further during the 19th century. The Mysore rockets of this period were much more advanced than the British had previously seen, chiefly because of the use of iron tubes for holding the propellant; this enabled higher thrust and longer range for the missile (up to 2 km range). After Tipu's eventual defeat in the Fourth Anglo-Mysore War and the capture of the Mysore iron rockets, they were influential in British rocket development, inspiring the Congreve rocket, which was soon put into use in the Napoleonic Wars.
Accuracy of early rockets.
William Congreve, son of the Comptroller of the Royal Arsenal, Woolwich, London, became a major figure in the field. From 1801, Congreve researched on the original design of Mysore rockets and set on a vigorous development program at the Arsenal's laboratory. Congreve prepared a new propellant mixture, and developed a rocket motor with a strong iron tube with conical nose. This early Congreve rocket weighed about 32 pounds (14.5 kilograms). The Royal Arsenal's first demonstration of solid fuel rockets was in 1805. The rockets were effectively used during the Napoleonic Wars and the War of 1812. Congreve published three books on rocketry.
From there, the use of military rockets spread throughout the western world. At the Battle of Baltimore in 1814, the rockets fired on Fort McHenry by the rocket vessel HMS "Erebus" were the source of the "rockets' red glare" described by Francis Scott Key in The Star-Spangled Banner. Rockets were also used in the Battle of Waterloo.
Early rockets were very inaccurate. Without the use of spinning or any gimballing of the thrust, they had a strong tendency to veer sharply off of their intended course. The early Mysorean rockets and their successor British Congreve rockets reduced this somewhat by attaching a long stick to the end of a rocket (similar to modern bottle rockets) to make it harder for the rocket to change course. The largest of the Congreve rockets was the 32-pound (14.5 kg) Carcass, which had a 15-foot (4.6 m) stick. Originally, sticks were mounted on the side, but this was later changed to mounting in the center of the rocket, reducing drag and enabling the rocket to be more accurately fired from a segment of pipe.
The accuracy problem was greatly improved in 1844 when William Hale modified the rocket design so that thrust was slightly vectored, causing the rocket to spin along its axis of travel like a bullet. The Hale rocket removed the need for a rocket stick, travelled further due to reduced air resistance, and was far more accurate.
In 1865 the British Colonel Edward Mounier Boxer built an improved versione of the Congreve rocket placing two rockets in one tube, one behind the other.
Theories of interplanetary rocketry.
At the beginning of the 20th Century, there was a burst of scientific investigation into interplanetary travel, largely driven by the inspiration of fiction by writers such as Jules Verne and H.G.Wells. Scientists seized on the rocket as a technology that was able to achieve this in real life.
In 1903, high school mathematics teacher Konstantin Tsiolkovsky (1857–1935), published "Исследование мировых пространств реактивными приборами" ("The Exploration of Cosmic Space by Means of Reaction Devices"), the first serious scientific work on space travel. The Tsiolkovsky rocket equation—the principle that governs rocket propulsion—is named in his honor (although it had been discovered previously). He also advocated the use of liquid hydrogen and oxygen for propellant, calculating their maximum exhaust velocity. His work was essentially unknown outside the Soviet Union, but inside the country it inspired further research, experimentation and the formation of the Society for Studies of Interplanetary Travel in 1924.
In 1912, Robert Esnault-Pelterie published a lecture on rocket theory and interplanetary travel. He independently derived Tsiolkovsky's rocket equation, did basic calculations about the energy required to make round trips to the Moon and planets, and he proposed the use of atomic power (i.e. radium) to power a jet drive.
In 1912 Robert Goddard, inspired from an early age by H.G. Wells, began a serious analysis of rockets, concluding that conventional solid-fuel rockets needed to be improved in three ways.
First, fuel should be burned in a small combustion chamber, instead of building the entire propellant container to withstand the high pressures. Second, rockets could be arranged in stages. Finally, the exhaust speed (and thus the efficiency) could be greatly increased to beyond the speed of sound by using a De Laval nozzle. He patented these concepts in 1914. He also independently developed the mathematics of rocket flight.
In 1920, Goddard published these ideas and experimental results in "A Method of Reaching Extreme Altitudes". The work included remarks about sending a solid-fuel rocket to the Moon, which attracted worldwide attention and was both praised and ridiculed. A "New York Times" editorial suggested:
In 1923, Hermann Oberth (1894–1989) published "Die Rakete zu den Planetenräumen" ("The Rocket into Planetary Space"), a version of his doctoral thesis, after the University of Munich had rejected it.
In 1924, Tsiolkovsky also wrote about multi-stage rockets, in 'Cosmic Rocket Trains'.
Modern rocketry.
Pre-World War II.
Modern rockets originated when Goddard attached a supersonic (de Laval) nozzle to the combustion chamber of a liquid-fueled rocket engine. These nozzles turn the hot gas from the combustion chamber into a cooler, hypersonic, highly directed jet of gas, more than doubling the thrust and raising the engine efficiency from 2% to 64%. On 16 March 1926 Robert Goddard launched the world's first liquid-fueled rocket in Auburn, Massachusetts.
During the 1920s, a number of rocket research organizations appeared worldwide. In 1927 the German car manufacturer Opel began to research rocket vehicles together with Mark Valier and the solid-fuel rocket builder Friedrich Wilhelm Sander. In 1928, Fritz von Opel drove a rocket car, the Opel-RAK.1 on the Opel raceway in Rüsselsheim, Germany. In 1928 the Lippisch Ente flew: rocket power launched the manned glider, although it was destroyed on its second flight. In 1929 von Opel started at the Frankfurt-Rebstock airport with the Opel-Sander RAK 1-airplane, which was damaged beyond repair during a hard landing after its first flight.
In the mid-1920s, German scientists had begun experimenting with rockets that used liquid propellants capable of reaching relatively high altitudes and distances. In 1927 and also in Germany, a team of amateur rocket engineers had formed the "Verein für Raumschiffahrt" (German Rocket Society, or VfR), and in 1931 launched a liquid propellant rocket (using oxygen and gasoline).
From 1931 to 1937 in the Soviet Union, extensive scientific work on rocket engine design occurred at the Gas Dynamics Laboratory in Leningrad. The well-funded and -staffed laboratory built over 100 experimental engines under the direction of Valentin Glushko. The work included regenerative cooling, hypergolic propellant ignition, and fuel injector designs that included swirling and bi-propellant mixing injectors. However, Glushko's arrest during Stalinist purges in 1938 curtailed the development.
Similar work was also done from 1932 onwards by the Austrian professor Eugen Sänger, who migrated from Austria to Germany in 1936. He worked there on rocket-powered spaceplanes such as Silbervogel (sometimes called the "antipodal" bomber.)
On November 12, 1932 at a farm in Stockton NJ, the American Interplanetary Society's attempt to static-fire their first rocket (based on German Rocket Society designs) failed in a fire.
In 1936, a British research programme based at Fort Halstead under the direction of Dr Alwyn Crow started work on a series of unguided solid-fuel rockets that could be used as anti-aircraft weapons. In 1939, a number of test firings were carried out in the British colony of Jamaica, on a purpose built range.
In 1930s, the German "Reichswehr" (which in 1935 became the "Wehrmacht") began to take an interest in rocketry. Artillery restrictions imposed by the 1919 Treaty of Versailles limited Germany's access to long-distance weaponry. Seeing the possibility of using rockets as long-range artillery fire, the Wehrmacht initially funded the VfR team, but because their focus was strictly scientific, created its own research team. At the behest of military leaders, Wernher von Braun, at the time a young aspiring rocket scientist, joined the military (followed by two former VfR members) and developed long-range weapons for use in World War II by Nazi Germany.
World War II.
In 1943, production of the V-2 rocket began in Germany. It had an operational range of 300 km and carried a 1000 kg warhead, with an amatol explosive charge. It normally achieved an operational maximum altitude of around 90 km, but could achieve 206 km if launched vertically. The vehicle was similar to most modern rockets, with turbopumps, inertial guidance and many other features. Thousands were fired at various Allied nations, mainly Belgium, as well as England and France. While they could not be intercepted, their guidance system design and single conventional warhead meant that it was insufficiently accurate against military targets. A total of 2,754 people in England were killed, and 6,523 were wounded before the launch campaign was ended. There were also 20,000 deaths of slave labour during the construction of V-2s. While it did not significantly affect the course of the war, the V-2 provided a lethal demonstration of the potential for guided rockets as weapons.
In parallel with the guided missile programme in Nazi Germany, rockets were also used on aircraft, either for assisting horizontal take-off (RATO), vertical take-off (Bachem Ba 349 "Natter") or for powering them (Me 163, etc.). During the war Germany also developed several guided and unguided air-to-air, ground-to-air and ground-to-ground missiles (see list of World War II guided missiles of Germany).
The Allies rocket programs were much less sophisticated, relying mostly on unguided missiles like the Soviet Katyusha rocket.
Post World War II.
At the end of World War II, competing Russian, British, and US military and scientific crews raced to capture technology and trained personnel from the German rocket program at Peenemünde. Russia and Britain had some success, but the United States benefited the most. The US captured a large number of German rocket scientists, including von Braun, and brought them to the United States as part of Operation Paperclip. In America, the same rockets that were designed to rain down on Britain were used instead by scientists as research vehicles for developing the new technology further. The V-2 evolved into the American Redstone rocket, used in the early space program.
After the war, rockets were used to study high-altitude conditions, by radio telemetry of temperature and pressure of the atmosphere, detection of cosmic rays, and further research; notably the Bell X-1, the first manned vehicle to break the sound barrier. This continued in the US under von Braun and the others, who were destined to become part of the US scientific community.
Independently, in the Soviet Union's space program research continued under the leadership of the chief designer Sergei Korolev. With the help of German technicians, the V-2 was duplicated and improved as the R-1, R-2 and R-5 missiles. German designs were abandoned in the late 1940s, and the foreign workers were sent home. A new series of engines built by Glushko and based on inventions of Aleksei Mihailovich Isaev formed the basis of the first ICBM, the R-7. The R-7 launched the first satellite- Sputnik 1, and later Yuri Gagarin-the first man into space, and the first lunar and planetary probes. This rocket is still in use today. These prestigious events attracted the attention of top politicians, along with additional funds for further research.
One problem that had not been solved was atmospheric reentry. It had been shown that an orbital vehicle easily had enough kinetic energy to vaporize itself, and yet it was known that meteorites can make it down to the ground. The mystery was solved in the US in 1951 when H. Julian Allen and A. J. Eggers, Jr. of the National Advisory Committee for Aeronautics (NACA) made the counterintuitive discovery that a blunt shape (high drag) permitted the most effective heat shield. With this type of shape, around 99% of the energy goes into the air rather than the vehicle, and this permitted safe recovery of orbital vehicles.
The Allen and Eggers discovery, initially treated as a military secret, was eventually published in 1958. Blunt body theory made possible the heat shield designs that were embodied in the Mercury, Gemini, Apollo, and Soyuz space capsules, enabling astronauts and cosmonauts to survive the fiery re-entry into Earth's atmosphere. Some spaceplanes such as the Space Shuttle made use of the same theory. At the time the STS was being conceived, Maxime Faget, the Director of Engineering and Development at the Manned Spacecraft Center, was not satisfied with the purely "lifting re-entry" method (as proposed for the cancelled X-20 "Dyna-Soar"). He designed a space shuttle which operated as a blunt body by entering the atmosphere at an extremely high angle of attack of 40° with the underside facing the direction of flight, creating a large shock wave that would deflect most of the heat around the vehicle instead of into it. The Space Shuttle essentially uses a combination of a "ballistic entry" (Blunt body theory) and then at an altitude of about 122,000 m, the re-entry interface takes place. Here the atmosphere is dense enough for the Space Shuttle to begin its "lifting re-entry" by reducing the angle-of-attack, pointing the nose down and using the lift its wings generate to "start flying" (gliding) towards the landing site. 
Cold War.
Rockets became extremely important militarily as modern intercontinental ballistic missiles (ICBMs) when it was realized that nuclear weapons carried on a rocket vehicle were essentially impossible for existing defense systems to stop once launched, and ICBM/Launch vehicles such as the R-7, Atlas and Titan became the delivery platform of choice for these weapons.
Fueled partly by the Cold War, the 1960s became the decade of rapid development of rocket technology particularly in the Soviet Union (Vostok, Soyuz, Proton) and in the United States (e.g. the X-15 and X-20 Dyna-Soar aircraft). There was also significant research in other countries, such as Britain, Japan, Australia, etc., and a growing use of rockets for Space exploration, with pictures returned from the far side of the Moon and unmanned flights for Mars exploration.
In America the manned programmes, Project Mercury, Project Gemini and later the Apollo programme culminated in 1969 with the first manned landing on the moon via the Saturn V, causing the New York Times to retract their earlier editorial implying that spaceflight couldn't work:
In the 1970s America made further lunar landings, before cancelling the Apollo program in 1975. The replacement vehicle, the partially reusable 'Space Shuttle' was intended to be cheaper, but this large reduction in costs was largely not achieved. Meanwhile in 1973, the expendable Ariane programme was begun, a launcher that by the year 2000 would capture much of the geosat market.
Current day.
Rockets remain a popular military weapon. The use of large battlefield rockets of the V-2 type has given way to guided missiles. However rockets are often used by helicopters and light aircraft for ground attack, being more powerful than machine guns, but without the recoil of a heavy cannon and by the early 1960s air-to-air missiles became favored. Shoulder-launched rocket weapons are widespread in the anti-tank role due to their simplicity, low cost, light weight, accuracy and high level of damage. Current artillery systems such as the MLRS or BM-30 Smerch launch multiple rockets to saturate battlefield targets with munitions.
Commercially, rocketry is the enabler of all space technologies particularly satellites, many of which impact people's everyday lives in almost countless ways.
Scientifically, rocketry has opened a window on the universe, allowing the launch of space probes to explore the solar system and space-based telescopes to obtain a clearer view of the rest of the universe.
However, it is probably manned spaceflight that has predominantly caught the imagination of the public. Vehicles such as the Space Shuttle for scientific research, the Soyuz increasingly for orbital tourism and SpaceShipOne for suborbital tourism may show a trend towards greater commercialisation of manned rocketry.
Types.
Rocket vehicles are often constructed in the archetypal tall thin "rocket" shape that takes off vertically, but there are actually many different types of rockets including:
Design.
A rocket design can be as simple as a cardboard tube filled with black powder, but to make an efficient, accurate rocket or missile involves overcoming a number of difficult problems. The main difficulties include cooling the combustion chamber, pumping the fuel (in the case of a liquid fuel), and controlling and correcting the direction of motion.
Components.
Rockets consist of a propellant, a place to put propellant (such as a propellant tank), and a nozzle. They may also have one or more rocket engines, directional stabilization device(s) (such as fins, vernier engines or engine gimbals for thrust vectoring, gyroscopes) and a structure (typically monocoque) to hold these components together. Rockets intended for high speed atmospheric use also have an aerodynamic fairing such as a nose cone, which usually holds the payload.
As well as these components, rockets can have any number of other components, such as wings (rocketplanes), parachutes, wheels (rocket cars), even, in a sense, a person (rocket belt). Vehicles frequently possess navigation systems and guidance systems that typically use satellite navigation and inertial navigation systems.
Engines.
Rocket engines employ the principle of jet propulsion. The rocket engines powering rockets come in a great variety of different types, a comprehensive list can be found in rocket engine. Most current rockets are chemically powered rockets (usually internal combustion engines, but some employ a decomposing monopropellant) that emit a hot exhaust gas. A rocket engine can use gas propellants, solid propellant, liquid propellant, or a hybrid mixture of both solid and liquid. Some rockets use heat or pressure that is supplied from a source other than the chemical reaction of propellant(s), such as steam rockets, solar thermal rockets, nuclear thermal rocket engines or simple pressurized rockets such as water rocket or cold gas thrusters. With combustive propellants a chemical reaction is initiated between the fuel and the oxidizer in the combustion chamber, and the resultant hot gases accelerate out of a rocket engine nozzle (or nozzles) at the rearward-facing end of the rocket. The acceleration of these gases through the engine exerts force ("thrust") on the combustion chamber and nozzle, propelling the vehicle (according to Newton's Third Law). This actually happens because the force (pressure times area) on the combustion chamber wall is unbalanced by the nozzle opening; this is not the case in any other direction. The shape of the nozzle also generates force by directing the exhaust gas along the axis of the rocket.
Propellant.
Rocket propellant is mass that is stored, usually in some form of propellant tank or casing, prior to being used as the propulsive mass that is ejected from a rocket engine in the form of a fluid jet to produce thrust. For chemical rockets often the propellants are a fuel such as liquid hydrogen or kerosene burned with an oxidizer such as liquid oxygen or nitric acid to produce large volumes of very hot gas. The oxidiser is either kept separate and mixed in the combustion chamber, or comes premixed, as with solid rockets.
Sometimes the propellant is not burned but still undergoes a chemical reaction, and can be a 'monopropellant' such as hydrazine, nitrous oxide or hydrogen peroxide that can be catalytically decomposed to hot gas.
Alternatively, an inert propellant can be used that can be externally heated, such as in steam rocket, solar thermal rocket or nuclear thermal rockets.
For smaller, low performance rockets such as attitude control thrusters where high performance is less necessary, a pressurised fluid is used as propellant that simply escapes the spacecraft through a propelling nozzle.
Uses.
Rockets or other similar reaction devices carrying their own propellant must be used when there is no other substance (land, water, or air) or force (gravity, magnetism, light) that a vehicle may usefully employ for propulsion, such as in space. In these circumstances, it is necessary to carry all the propellant to be used.
However, they are also useful in other situations:
Military.
Some military weapons use rockets to propel warheads to their targets. A rocket and its payload together are generally referred to as a "missile" when the weapon has a guidance system (not all missiles use rocket engines, some use other engines such as jets) or as a "rocket" if it is unguided. Anti-tank and anti-aircraft missiles use rocket engines to engage targets at high speed at a range of several miles, while intercontinental ballistic missiles can be used to deliver multiple nuclear warheads from thousands of miles, and anti-ballistic missiles try to stop them. Rockets have also been tested for reconnaissance, such as the Ping-Pong rocket, which was launched to surveil enemy targets, however, recon rockets have never come into wide use in the military.
Science and research.
Sounding rockets are commonly used to carry instruments that take readings from 50 km to 1500 km above the surface of the Earth, the altitudes between those reachable by weather balloons and satellites.
Rocket engines are also used to propel rocket sleds along a rail at extremely high speed. The world record for this is Mach 8.5.
Spaceflight.
Larger rockets are normally launched from a launch pad that provides stable support until a few seconds after ignition. Due to their high exhaust velocity—2500 to (Mach ~10+)—rockets are particularly useful when very high speeds are required, such as orbital speed (Mach 24+). Spacecraft delivered into orbital trajectories become artificial satellites, which are used for many commercial purposes. Indeed, rockets remain the only way to launch spacecraft into orbit and beyond. They are also used to rapidly accelerate spacecraft when they change orbits or de-orbit for landing. Also, a rocket may be used to soften a hard parachute landing immediately before touchdown (see retrorocket).
Rescue.
Rockets were used to propel a line to a stricken ship so that a Breeches buoy can be used to rescue those on board. Rockets are also used to launch emergency flares.
Some crewed rockets, notably the Saturn V and Soyuz have launch escape systems. This is a small, usually solid rocket that is capable of pulling the crewed capsule away from the main vehicle towards safety at a moments notice. These types of systems have been operated several times, both in testing and in flight, and operated correctly each time.
This was the case when the Safety Assurance System (Soviet nomenclature) successfully pulled away the L3 capsule during three of the four failed launches of the Soviet moon rocket, N1 vehicles 3L, 5L and 7L. In all three cases the capsule, albeit unmanned, was saved from destruction. It should be noted that only the three aforementioned N1 rockets had functional Safety Assurance Systems. The outstanding vehicle, 6L, had dummy upper stages and therefore no escape system giving the N1 booster a 100% success rate for egress from a failed launch.
A successful escape of a manned capsule occurred when Soyuz T-10, on a mission to the Salyut 7 space station, exploded on the pad.
Solid rocket propelled ejection seats are used in many military aircraft to propel crew away to safety from a vehicle when flight control is lost.
Hobby, sport, and entertainment.
Hobbyists build and fly a wide variety of model rockets. Many companies produce model rocket kits and parts but due to their inherent simplicity some hobbyists have been known to make rockets out of almost anything. Rockets are also used in some types of consumer and professional fireworks. A Water Powered Rocket is a type of model rocket using water as its reaction mass. The pressure vessel 
(the engine of the rocket) is usually a used plastic soft drink bottle. The water is forced out by a pressurized gas, typically compressed air. It is an example of Newton's third law of motion.
Hydrogen peroxide rockets are used to power jet packs, and have been used to power cars and a rocket car holds the all time (albeit unofficial) drag racing record.
Noise.
For all but the very smallest sizes, rocket exhaust compared to other engines is generally very noisy. As the hypersonic exhaust mixes with the ambient air, shock waves are formed. The sound intensity from these shock waves depends on the size of the rocket as well as the exhaust speed. The sound intensity of large, high performance rockets could potentially kill at close range.
The Space Shuttle generates over 200 dB(A) of noise around its base. A Saturn V launch was detectable on seismometers a considerable distance from the launch site.
Noise is generally most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the plume, as well as reflecting off the ground. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the plume and by deflecting the plume at an angle.
For crewed rockets various methods are used to reduce the sound intensity for the passengers, and typically the placement of the astronauts far away from the rocket engines helps significantly. For the passengers and crew, when a vehicle goes supersonic the sound cuts off as the sound waves are no longer able to keep up with the vehicle.
Physics.
Operation.
The action of the rocket engine's combustion chambers and expansion nozzles on a high pressure fluid is able to accelerate the fluid to extremely high speed, and conversely this exerts a large reactive thrust on the rocket (an equal and opposite reaction according to Newton's third law), which propels the rocket forwards. Care is needed in determining where exactly the reactive thrust acts and it is often mislocated.
In a closed chamber, the pressures are equal in each direction and no acceleration occurs. If an opening is provided in the bottom of the chamber then the pressure is no longer acting on the missing section. This opening permits the exhaust to escape. The remaining pressures give a resultant thrust on the side opposite the opening, and these pressures are what push the rocket along.
The shape of the nozzle is important. Consider a balloon propelled by air coming out of a tapering nozzle. In such a case the combination of air pressure and viscous friction is such that the nozzle does not push the balloon but is "pulled" by it. Using a convergent/divergent nozzle gives more force since the exhaust also presses on it as it expands outwards, roughly doubling the total force. If propellant gas is continuously added to the chamber then these pressures can be maintained for as long as propellant remains. Note that the pumps moving the propellant into the combustion chamber must maintain a pressure larger than the combustion chamber -typically on the order of 100 atmospheres.
As a side effect, these pressures on the rocket also act on the exhaust in the opposite direction and accelerate this exhaust to very high speeds (according to Newton's Third Law). From the principle of conservation of momentum the speed of the exhaust of a rocket determines how much momentum increase is created for a given amount of propellant. This is called the rocket's "specific impulse". Because a rocket, propellant and exhaust in flight, without any external perturbations, may be considered as a closed system, the total momentum is always constant. Therefore, the faster the net speed of the exhaust in one direction, the greater the speed of the rocket can achieve in the opposite direction. This is especially true since the rocket body's mass is typically far lower than the final total exhaust mass.
Forces on a rocket in flight.
The general study of the forces on a rocket is part of ballistics. The behaviour of spacecraft is studied in the subfield of astrodynamics.
Flying rockets are primarily affected by the following:
In addition, the inertia and centrifugal pseudo-force can be significant due to the path of the rocket around the center of a celestial body; when high enough speeds in the right direction and altitude are achieved a stable orbit or escape velocity is obtained.
These forces, with a stabilizing tail (the "empennage") present will, unless deliberate control efforts are made, naturally cause the vehicle to follow a roughly parabolic trajectory termed a gravity turn, and this trajectory is often used at least during the initial part of a launch. (This is true even if the rocket engine is mounted at the nose.) Vehicles can thus maintain low or even zero angle of attack, which minimizes transverse stress on the launch vehicle, permitting a weaker, and hence lighter, launch vehicle.
Net thrust.
A typical rocket engine can handle a significant fraction of its own mass in propellant each second, with the propellant leaving the nozzle at several kilometres per second. This means that the thrust-to-weight ratio of a rocket engine, and often the entire vehicle can be very high, in extreme cases over 100. This compares with other jet propulsion engines that can exceed 5 for some of the better engines.
It can be shown that the net thrust of a rocket is:
where:
The effective exhaust velocity formula_4 is more or less the speed the exhaust leaves the vehicle, and in the vacuum of space, the effective exhaust velocity is often equal to the actual average exhaust speed along the thrust axis. However, the effective exhaust velocity allows for various losses, and notably, is reduced when operated within an atmosphere.
The rate of propellant flow through a rocket engine is often deliberately varied over a flight, to provide a way to control the thrust and thus the airspeed of the vehicle. This, for example, allows minimization of aerodynamic losses and can limit the increase of "g"-forces due to the reduction in propellant load.
Total impulse.
Impulse is defined as a force acting on an object over time, which in the absence of opposing forces (gravity and aerodynamic drag), changes the momentum (integral of mass and velocity) of the object. As such, it is the best performance class (payload mass and terminal velocity capability) indicator of a rocket, rather than takeoff thrust, mass, or "power". The total impulse of a rocket (stage) burning its propellant is:
When there is fixed thrust, this is simply:
The total impulse of a multi-stage rocket is the sum of the impulses of the individual stages.
Specific impulse.
As can be seen from the thrust equation, the effective speed of the exhaust controls the amount of thrust produced from a particular quantity of fuel burnt per second.
An equivalent measure, the net impulse per weight unit of propellant expelled, is called specific Impulse, formula_7, and this is one of the most important figures that describes a rocket's performance. It is defined such that it is related to the effective exhaust velocity by:
where:
Thus, the greater the specific impulse, the greater the net thrust and performance of the engine. formula_7 is determined by measurement while testing the engine. In practice the effective exhaust velocities of rockets varies but can be extremely high, ~4500 m/s, about 15 times the sea level speed of sound in air.
Delta-v (rocket equation).
The delta-v capacity of a rocket is the theoretical total change in velocity that a rocket can achieve without any external interference (without air drag or gravity or other forces).
When formula_12 is constant, the delta-v that a rocket vehicle can provide can be calculated from the Tsiolkovsky rocket equation:
where:
When launched from the Earth practical delta-v's for a single rockets carrying payloads can be a few km/s. Some theoretical designs have rockets with delta-v's over 9 km/s.
The required delta-v can also be calculated for a particular manoeuvre; for example the delta-v to launch from the surface of the Earth to Low earth orbit is about 9.7 km/s, which leaves the vehicle with a sideways speed of about 7.8 km/s at an altitude of around 200 km. In this manoeuvre about 1.9 km/s is lost in air drag, gravity drag and gaining altitude.
The ratio formula_18 is sometimes called the "mass ratio".
Mass ratios.
Almost all of a launch vehicle's mass consists of propellant. Mass ratio is, for any 'burn', the ratio between the rocket's initial mass and the mass after. Everything else being equal, a high mass ratio is desirable for good performance, since it indicates that the rocket is lightweight and hence performs better, for essentially the same reasons that low weight is desirable in sports cars.
Rockets as a group have the highest thrust-to-weight ratio of any type of engine; and this helps vehicles achieve high mass ratios, which improves the performance of flights. The higher the ratio, the less engine mass is needed to be carried. This permits the carrying of even more propellant, enormously improving the delta-v. Alternatively, some rockets such as for rescue scenarios or racing carry relatively little propellant and payload and thus need only a lightweight structure and instead achieve high accelerations. For example, the Soyuz escape system can produce 20g.
Achievable mass ratios are highly dependent on many factors such as propellant type, the design of engine the vehicle uses, structural safety margins and construction techniques.
The highest mass ratios are generally achieved with liquid rockets, and these types are usually used for orbital launch vehicles, a situation which calls for a high delta-v. Liquid propellants generally have densities similar to water (with the notable exceptions of liquid hydrogen and liquid methane), and these types are able to use lightweight, low pressure tanks and typically run high-performance turbopumps to force the propellant into the combustion chamber.
Some notable mass fractions are found in the following table (some aircraft are included for comparison purposes):
Staging.
Often, the required velocity (delta-v) for a mission is unattainable by any single rocket because the propellant, tankage, structure, guidance, valves and engines and so on, take a particular minimum percentage of take-off mass that is too great for the propellant it carries to achieve that delta-v.
For example the first stage of the Saturn V, carrying the weight of the upper stages, was able to achieve a mass ratio of about 10, and achieved a specific impulse of 263 seconds. This gives a delta-v of around 5.9 km/s whereas around 9.4 km/s delta-v is needed to achieve orbit with all losses allowed for.
This problem is frequently solved by staging — the rocket sheds excess weight (usually empty tankage and associated engines) during launch. Staging is either "serial" where the rockets light after the previous stage has fallen away, or "parallel", where rockets are burning together and then detach when they burn out.
The maximum speeds that can be achieved with staging is theoretically limited only by the speed of light. However the payload that can be carried goes down geometrically with each extra stage needed, while the additional delta-v for each stage is simply additive.
Acceleration and thrust-to-weight ratio.
From Newton's second law, the acceleration, formula_19, of a vehicle is simply:
Where m is the instantaneous mass of the vehicle and formula_21 is the net force acting on the rocket (mostly thrust but air drag and other forces can play a part.)
As the remaining propellant decreases, rocket vehicles become lighter and their acceleration tends to increase until the propellant is exhausted. This means that much of the speed change occurs towards the end of the burn when the vehicle is much lighter. However, the thrust can be throttled to offset or vary this if needed. Discontinuities in acceleration also occur when stages burn out, often starting at a lower acceleration with each new stage firing.
Peak accelerations can be increased by designing the vehicle with a reduced mass, usually achieved by a reduction in the fuel load and tankage and associated structures, but obviously this reduces range, delta-v and burn time. Still, for some applications that rockets are used for, a high peak acceleration applied for just a short time is highly desirable.
The minimal mass of vehicle consists of a rocket engine with minimal fuel and structure to carry it. In that case the thrust-to-weight ratio of the rocket engine limits the maximum acceleration that can be designed. It turns out that rocket engines generally have truly excellent thrust to weight ratios (137 for the NK-33 engine, some solid rockets are over 1000), and nearly all really high-g vehicles employ or have employed rockets.
The high accelerations that rockets naturally possess means that rocket vehicles are often capable of vertical takeoff; this can be done provided a vehicle's engines provide more than the local gravitational acceleration away from the Earth or gravity source.
Drag.
Drag is a force opposite to the direction of the rocket's motion. This decreases acceleration of the vehicle and produces structural loads. Deceleration force for fast-moving rockets are calculated using the drag equation.
Drag can be minimised by an aerodynamic nose cone and by using a shape with a high ballistic coefficient (the "classic" rocket shape—long and thin), and by keeping the rocket's angle of attack as low as possible.
During a rocket launch, as the vehicle speed increases, and the atmosphere thins, there is a point of maximum aerodynamic drag called Max Q. This determines the minimum aerodynamic strength of the vehicle, as the rocket must avoid buckling under these forces.
Energy.
Energy efficiency.
Rocket launch vehicles take-off with a great deal of flames, noise and drama, and it might seem obvious that they are grievously inefficient. However, while they are far from perfect, their energy efficiency is not as bad as might be supposed.
The energy density of a typical rocket propellant is often around one-third that of conventional hydrocarbon fuels; the bulk of the mass is (often relatively inexpensive) oxidizer. Nevertheless, at take-off the rocket has a great deal of energy in the fuel and oxidizer stored within the vehicle. It is of course desirable that as much of the energy of the propellant end up as kinetic or potential energy of the body of the rocket as possible.
Energy from the fuel is lost in air drag and gravity drag and is used for the rocket to gain altitude and speed. However, much of the lost energy ends up in the exhaust.
In a chemical propulsion device, the engine efficiency is simply the ratio of the kinetic power of the exhaust gases and the power available from the chemical reaction:
100% efficiency within the engine (engine efficiency formula_23) would mean that all the heat energy of the combustion products is converted into kinetic energy of the jet. This is not possible, but the near-adiabatic high expansion ratio nozzles that can be used with rockets come surprisingly close: when the nozzle expands the gas, the gas is cooled and accelerated, and an energy efficiency of up to 70% can be achieved. Most of the rest is heat energy in the exhaust that is not recovered. The high efficiency is a consequence of the fact that rocket combustion can be performed at very high temperatures and the gas is finally released at much lower temperatures, and so giving good Carnot efficiency.
However, engine efficiency is not the whole story. In common with the other jet-based engines, but particularly in rockets due to their high and typically fixed exhaust speeds, rocket vehicles are extremely inefficient at low speeds irrespective of the engine efficiency. The problem is that at low speeds, the exhaust carries away a huge amount of kinetic energy rearward. This phenomenon is termed propulsive efficiency (formula_24).
However, as speeds rise, the resultant exhaust speed goes down, and the overall vehicle energetic efficiency rises, reaching a peak of around 100% of the engine efficiency when the vehicle is travelling exactly at the same speed that the exhaust is emitted. In this case the exhaust would ideally stop dead in space behind the moving vehicle, taking away zero energy, and from conservation of energy, all the energy would end up in the vehicle. The efficiency then drops off again at even higher speeds as the exhaust ends up travelling forwards- trailing behind the vehicle.
From these principles it can be shown that the propulsive efficiency formula_24 for a rocket moving at speed formula_26 with an exhaust velocity formula_27 is:
And the overall (instantaneous) energy efficiency formula_29 is:
For example, from the equation, with an formula_31 of 0.7, a rocket flying at Mach 0.85 (which most aircraft cruise at) with an exhaust velocity of Mach 10, would have a predicted overall energy efficiency of 5.9%, whereas a conventional, modern, air-breathing jet engine achieves closer to 35% efficiency. Thus a rocket would need about 6x more energy; and allowing for the specific energy of rocket propellant being around one third that of conventional air fuel, roughly 18x more mass of propellant would need to be carried for the same journey. This is why rockets are rarely if ever used for general aviation.
Since the energy ultimately comes from fuel, these considerations mean that rockets are mainly useful when a very high speed is required, such as ICBMs or orbital launch. For example NASA's space shuttle fires its engines for around 8.5 minutes, consuming 1,000 tonnes of solid propellant (containing 16% aluminium) and an additional 2,000,000 litres of liquid propellant (106,261 kg of liquid hydrogen fuel) to lift the 100,000 kg vehicle (including the 25,000 kg payload) to an altitude of 111 km and an orbital velocity of 30,000 km/h. At this altitude and velocity, the vehicle has a kinetic energy of about 3 TJ and a potential energy of roughly 200 GJ. Given the initial energy of 20 TJ, the Space Shuttle is about 16% energy efficient at launching the orbiter.
Thus jet engines, with a better match between speed and jet exhaust speed (such as turbofans—in spite of their worse formula_31)—dominate for subsonic and supersonic atmospheric use, while rockets work best at hypersonic speeds. On the other hand, rockets serve in many short-range "relatively" low speed military applications where their low-speed inefficiency is outweighed by their extremely high thrust and hence high accelerations.
Oberth effect.
One subtle feature of rockets relates to energy. A rocket stage, while carrying a given load, is capable of giving a particular delta-v. This delta-v means that the speed increases (or decreases) by a particular amount, independent of the initial speed. However, because kinetic energy is a square law on speed, this means that the faster the rocket is travelling before the burn the more orbital energy it gains or loses.
This fact is used in interplanetary travel. It means that the amount of delta-v to reach other planets, over and above that to reach escape velocity can be much less if the delta-v is applied when the rocket is travelling at high speeds, close to the Earth or other planetary surface; whereas waiting until the rocket has slowed at altitude multiplies up the effort required to achieve the desired trajectory.
Safety, reliability and accidents.
The reliability of rockets, as for all physical systems, is dependent on the quality of engineering design and construction.
Because of the enormous chemical energy in rocket propellants (greater energy by weight than explosives, but lower than gasoline), consequences of accidents can be severe. Most space missions have some issues. In 1986, following the Space Shuttle Challenger Disaster, American Physicist Richard Feynman, having served on the Rogers Commission estimated that the chance of an unsafe condition for a launch of the Shuttle was very roughly 1%; more recently the historical per person-flight risk in orbital spaceflight has been calculated to be around 2% or 4%.
Costs and economics.
The costs of rockets can be roughly divided into propellant costs, the costs of obtaining and/or producing the 'dry mass' of the rocket, and the costs of any required support equipment and facilities.
Most of the takeoff mass of a rocket is normally propellant. However propellant is seldom more than a few times more expensive than gasoline per kilogram (as of 2009 gasoline was about 1 $/kg or less), and although substantial amounts are needed, for all but the very cheapest rockets, it turns out that the propellant costs are usually comparatively small, although not completely negligible. With liquid oxygen costing 0.15 $/kg and liquid hydrogen 2.20 $/kg, the Space Shuttle in 2009 had a liquid propellant expense of approximately $1.4 million for each launch that cost $450 million from other expenses (with 40% of the mass of propellants used by it being liquids in the external fuel tank, 60% solids in the SRBs).
Even though a rocket's non-propellant, dry mass is often only between 5-20% of total mass, nevertheless this cost dominates. For hardware with the performance used in orbital launch vehicles, expenses of $2000–$10,000+ per kilogram of dry weight are common, primarily from engineering, fabrication, and testing; raw materials amount to typically around 2% of total expense. For most rockets except reusable ones (shuttle engines) the engines need not function more than a few minutes, which simplifies design.
Extreme performance requirements for rockets reaching orbit correlate with high cost, including intensive quality control to ensure reliability despite the limited safety factors allowable for weight reasons. Components produced in small numbers if not individually machined can prevent
amortization of R&D and facility costs over mass production to the degree seen in more pedestrian manufacturing. Amongst liquid-fueled rockets, complexity can be influenced by how much hardware must be lightweight, like pressure-fed engines can have two orders of magnitude lesser part count than pump-fed engines but lead to more weight by needing greater tank pressure, most often used in just small maneuvering thrusters as a consequence.
To change the preceding factors for orbital launch vehicles, proposed methods have included mass-producing simple rockets in large quantities or on large scale, or developing reusable rockets meant to fly very frequently to amortize their up-front expense over many payloads, or
reducing rocket performance requirements by constructing a hypothetical non-rocket spacelaunch system for part of the velocity to orbit (or all of it but with
most methods involving some rocket use).
The costs of support equipment, range costs and launch pads generally scale up with the size of the rocket, but vary less with launch rate, and so may be considered to be approximately a fixed cost.
Rockets in applications other than launch to orbit (such as military rockets and rocket-assisted take off), commonly not needing comparable performance and sometimes mass-produced, are often relatively inexpensive.
See also.
Lists
General rocketry
Recreational rocketry
Recreational pyrotechnic rocketry
Weaponry
Rockets for Research
Misc
Notes.
Footnotes
Citations
References.
</dl>
External links.
Governing agencies
Information sites

</doc>
<doc id="26304" url="http://en.wikipedia.org/wiki?curid=26304" title="Royal Botanic Gardens, Kew">
Royal Botanic Gardens, Kew

Royal Botanic Gardens, Kew (brand name Kew) is a non-departmental public body in the United Kingdom sponsored by the Department for Environment, Food and Rural Affairs. An internationally important botanical research and education institution, it employs 750 staff. Its chief executive is the current Director, Richard Deverell. Its board of trustees is chaired by Marcus Agius, a former chairman of Barclays PLC.
The organisation manages botanic gardens at Kew in Richmond upon Thames in southwest London, and at Wakehurst Place, a National Trust property in Sussex which is home to an internationally important Millennium Seed Bank. The Seed Bank is also the site of multiple research projects and international partnerships with at least 80 countries. Seed stored at the bank fulfils two functions: it provides an "ex situ" conservation resource and also facilitates research around the globe by acting as a repository for seed scientists. Kew also operates, jointly with the Forestry Commission, Bedgebury Pinetum in Kent, which specialising in growing conifers.
Governance.
Kew is governed by a Board of Trustees which comprises a chairman and 11 members. Ten members and the chair are appointed by the Secretary of State. Her Majesty the Queen appoints her own trustee on the recommendation of the Secretary of State. s of 2015[ [update]] the Board members are:
International Plant Names Index.
The Harvard University Herbaria and the Australian National Herbarium co-operate with Kew in the IPNI database, a project which was launched in 1999 to produce an authoritative source of information on botanical nomenclature including publication details. The IPNI includes information from the Index Kewensis, a project which began in the nineteenth century to provide an "Index to the Names and Authorities of all known flowering plants and their countries".
The Plant List.
Kew also cooperates with the Missouri Botanical Garden in a related project called The Plant List, which, unlike the IPNI, provides information on which names are currently accepted. The Plant List is an Internet encyclopedia project which was launched in 2010 to compile a comprehensive list of botanical nomenclature. The Plant List has 1,040,426 scientific plant names of species rank of which 298,900 are accepted species names. In addition, the list has 620 plant families and 16,167 plant genera.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="26306" url="http://en.wikipedia.org/wiki?curid=26306" title="Radon difluoride">
Radon difluoride

Radon difluoride (RnF2) is a compound of radon, a noble gas. Radon reacts readily with fluorine to form a solid compound, but this decomposes on attempted vaporization and its exact composition is uncertain. Calculations suggest that it may be ionic, unlike all other known binary noble gas compounds. The usefulness of radon compounds is limited because of the radioactivity of radon. The longest-lived isotope, radon-222, has a half-life of only 3.82 days.

</doc>
<doc id="26307" url="http://en.wikipedia.org/wiki?curid=26307" title="Robert Penn Warren">
Robert Penn Warren

Robert Penn Warren (April 24, 1905 – September 15, 1989) was an American poet, novelist, and literary critic and was one of the founders of New Criticism. He was also a charter member of the Fellowship of Southern Writers. He founded the influential literary journal "The Southern Review" with Cleanth Brooks in 1935. He received the 1947 Pulitzer Prize for the Novel for his novel "All the King's Men" (1946) and the Pulitzer Prize for Poetry in 1958 and 1979. He is the only person to have won Pulitzer Prizes for both fiction and poetry.
Biography.
Early years.
Warren was born in Guthrie, Kentucky, which is very near the Tennessee-Kentucky border, to Robert Warren and Anna Penn. Warren's mother's family had roots in Virginia, having given their name to the community of Penn's Store in Patrick County, Virginia, and was a descendant of Revolutionary War soldier Colonel Abram Penn. Robert Penn Warren graduated from Clarksville High School in Clarksville, Tennessee, Vanderbilt University (summa cum laude, Phi Beta Kappa) in 1925 and the University of California, Berkeley (M.A.) in 1926. Warren pursued further graduate study at Yale University from 1927 to 1928 and obtained his B. Litt. (a two-year graduate research degree) as a Rhodes Scholar from New College, Oxford, in England in 1930. He also received a Guggenheim Fellowship to study in Italy during the rule of Benito Mussolini. That same year he began his teaching career at Southwestern College (now Rhodes College) in Memphis, Tennessee.
Career.
While still an undergraduate at Vanderbilt University, Warren became associated with the group of poets there known as the Fugitives, and somewhat later, during the early 1930s, Warren and some of the same writers formed a group known as the Southern Agrarians. He contributed "The Briar Patch" to the Agrarian manifesto "I'll Take My Stand" along with 11 other Southern writers and poets (including fellow Vanderbilt poet/critics John Crowe Ransom, Allen Tate, and Donald Davidson). In "The Briar Patch" the young Warren defends racial segregation, in line with the traditionalist conservative political leanings of the Agrarian group, although Davidson deemed Warren's stances in the essay so progressive that he argued for excluding it from the collection. However, Warren recanted these views in an article on the Civil Rights Movement, "Divided South Searches Its Soul", which appeared in the July 9, 1956 issue of "Life" magazine. A month later, Warren published an expanded version of the article as a small book titled "Segregation: The Inner Conflict in the South". He subsequently adopted a high profile as a supporter of racial integration. In 1965, he published "Who Speaks for the Negro?", a collection of interviews with black civil rights leaders including Malcolm X and Martin Luther King, thus further distinguishing his political leanings from the more conservative philosophies associated with fellow Agrarians such as Tate, Cleanth Brooks, and particularly Davidson. Warren's interviews with civil rights leaders are at the Louie B. Nunn Center for Oral History at the University of Kentucky.
Warren's best-known work is "All the King's Men", a novel that won the Pulitzer Prize in 1947. Main character Willie Stark resembles Huey Pierce Long (1893–1935), the radical populist governor of Louisiana whom Warren was able to observe closely while teaching at Louisiana State University in Baton Rouge from 1933 to 1942. "All the King's Men" became a highly successful film, starring Broderick Crawford and winning the Academy Award for Best Picture in 1949. A 2006 film adaptation by writer/director Steven Zaillian featured Sean Penn as Willie Stark and Jude Law as Jack Burden. The opera "Willie Stark" by Carlisle Floyd to his own libretto based on the novel was premiered in 1981.
Warren served as the Consultant in Poetry to the Library of Congress, 1944–1945 (later termed Poet Laureate), and won two Pulitzer Prizes in poetry, in 1958 for "Promises: Poems 1954–1956" and in 1979 for "Now and Then". "Promises" also won the annual National Book Award for Poetry.
In 1974, the National Endowment for the Humanities selected him for the Jefferson Lecture, the U.S. federal government's highest honor for achievement in the humanities. Warren's lecture was entitled "Poetry and Democracy" (subsequently published under the title "Democracy and Poetry"). In 1980, Warren was presented with the Presidential Medal of Freedom by President Jimmy Carter. In 1981, Warren was selected as a MacArthur Fellow and later was named as the first U.S. Poet Laureate Consultant in Poetry on February 26, 1986. In 1987, he was awarded the National Medal of Arts.
Warren was co-author, with Cleanth Brooks, of "Understanding Poetry", an influential literature textbook. It was followed by other similarly co-authored textbooks, including "Understanding Fiction", which was praised by Southern Gothic and Roman Catholic writer Flannery O'Connor, and "Modern Rhetoric", which adopted what can be called a New Critical perspective.
Personal life.
His first marriage was to Emma Brescia Warren. His second marriage was in 1952 to Eleanor Clark, with whom he had two children, Rosanna Phelps Warren (born 1953) and Gabriel Penn Warren (born 1955). During his tenure at Louisiana State University he resided at Twin Oaks (Otherwise known as the Robert Penn Warren House) in Prairieville, Louisiana. He lived the latter part of his life in Fairfield, Connecticut, and Stratton, Vermont where he died of complications from bone cancer. He is buried at Stratton, Vermont, and, at his request, a memorial marker is situated in the Warren family gravesite in Guthrie, Kentucky.
Legacy.
In April 2005, the United States Postal Service issued a commemorative stamp to mark the 100th anniversary of Warren's birth. Introduced at the post office in his native Guthrie, it depicts the author as he appeared in a 1948 photograph, with a background scene of a political rally designed to evoke the setting of "All the King's Men". His son and daughter, Gabriel and Rosanna Warren, were in attendance.
Vanderbilt University houses the Robert Penn Warren Center for the Humanities, which is sponsored by the College of Arts and Science. It began its programs in January 1988, and in 1989 received a $480,000 Challenge Grant from the National Endowment for the Humanities. The center promotes "interdisciplinary research and study in the humanities, social sciences, and natural sciences."
The high school that Robert Penn Warren attended, Clarksville High School (Tennessee), was renovated into an apartment complex in 1982. The original name of the apartments was changed to The Penn Warren in 2010.

</doc>
