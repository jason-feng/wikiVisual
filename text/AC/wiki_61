<doc id="33971" url="http://en.wikipedia.org/wiki?curid=33971" title="William Herschel">
William Herschel

Sir Frederick William Herschel, KH, FRS ( "Friedrich Wilhelm Herschel"; 15 November 1738 – 25 August 1822) was a German-born British astronomer, composer, and brother of Caroline Herschel. Born in the Electorate of Hanover, Herschel followed his father into the Military Band of Hanover, before migrating to Great Britain at the age of nineteen. 
Herschel became interested in astronomy in 1773, and after constructing his first large telescope in 1774, he spent nine years carrying out thorough sky surveys, where his purpose was the investigation of double stars. The resolving power of the Herschel telescopes revealed that the nebulae in the Messier catalogue were clusters of stars: catalogues of nebulae were published in 1802 (2,500 objects) and 1820 (5,000 objects). In the course of an observation on 13 March 1781 he realized that one celestial body he had observed was not a star, but a planet, Uranus. This was the first planet to be discovered since antiquity and Herschel became famous overnight. As a result of this discovery George III appointed him 'Court Astronomer'. He was elected as a Fellow of the Royal Society and grants were provided for the construction of new telescopes.
Herschel pioneered the use of astronomical spectrophotometry as a diagnostic tool, using prisms and temperature measuring equipment to measure the wavelength distribution of stellar spectra. Other work included an improved determination of the rotation period of Mars, the discovery that the Martian polar caps vary seasonally, the discovery of Titania and Oberon (moons of Uranus) and Enceladus and Mimas (moons of Saturn). In addition, he was the first person to discover the existence of infrared radiation. Herschel was knighted in 1816. He died in August 1822, and his work was continued by his only son, John Herschel.
Early life and musical activities.
Herschel was born in the Electorate of Hanover in Germany, part of the Holy Roman Empire, one of ten children of Isaac Herschel by his marriage to Anna Ilse Moritzen. His family were Lutheran Christians. His father was an oboist in the Hanover Military Band. In 1755 the Hanoverian Guards regiment, in whose band Wilhelm and his brother Jakob were engaged as oboists, was ordered to England. At the time the crowns of Great Britain and Hanover were united under King George II. As the threat of war with France loomed, the Hanoverian Guards were recalled from England to defend Hanover. After they were defeated at the Battle of Hastenbeck, Herschel's father Isaak sent his two sons to seek refuge in England in late 1757. Although his older brother Jakob had received his dismissal from the Hanoverian Guards, Wilhelm was accused of desertion (for which he was pardoned by George III in 1782). Wilhelm, nineteen years old at this time, was a quick student of the English language. In England he went by the English rendition of his name, Frederick William Herschel.
In addition to the oboe, he played the violin and harpsichord and later the organ. He composed numerous musical works, including 24 symphonies and many concertos, as well as some church music. Six of his symphonies were recorded in April 2002 by the London Mozart Players, conducted by Matthias Bamert (Chandos 10048).
Herschel moved to Sunderland in 1761 when Charles Avison immediately engaged him as first violin and soloist for his Newcastle orchestra, where he played for one season. In ‘Sunderland in the County of Durh: apprill [sic] 20th 1761’ he wrote his symphony No. 8 in c minor. He was head of the Durham Militia band 1760–61 and visited the home of Sir Ralph Milbanke at Halnaby Hall in 1760, where he wrote two symphonies, as well as giving performances himself.
After Newcastle he moved to Leeds and Halifax where he was the first organist at St John the Baptist church (now Halifax Minster). He became organist of the Octagon Chapel, Bath, a fashionable chapel in a well-known spa, in which city he was also Director of Public Concerts. He was appointed as the organist in 1766 and gave his introductory concert on 1 January 1767. As the organ was still incomplete he showed off his versatility by performing his own compositions including a violin concerto, an oboe concerto and a harpsichord sonata. The organ was completed in October 1767. His sister Caroline came to England in 1772 and lived with him there in New King Street, Bath. The house they shared is now the location of the Herschel Museum of Astronomy. His brothers Dietrich, Alexander and Jakob (1734–1792) also appeared as musicians of Bath. In 1780, Herschel was appointed director of the Bath orchestra, with his sister often appearing as soprano soloist.
Astronomy.
Herschel's music led him to an interest in mathematics and lenses. His interest in astronomy grew stronger after he made the acquaintance of the English Astronomer Royal Nevil Maskelyne. He started building his own reflecting telescopes and would spend up to 16 hours a day grinding and polishing the speculum metal primary mirrors. He "began to look at the planets and the stars" in May 1773 and on 1 March 1774 began an astronomical journal by noting his observations of Saturn's rings and the Great Orion Nebula (M 42).
Double stars.
Herschel's early observational work soon focused on the search for pairs of stars that were very close together visually. Astronomers of the era expected that changes over time in the apparent separation and relative location of these stars would provide evidence for both the proper motion of stars and, by means of parallax shifts in their separation, for the distance of stars from the Earth (a method first suggested by Galileo Galilei). From the back garden of his house in New King Street, Bath, and using a 6.2 in, 7 ft (f/13) Newtonian telescope "with a most capital speculum" of his own manufacture, in October 1779, Herschel began a systematic search for such stars among "every star in the Heavens", with new discoveries listed through 1792. He soon discovered many more binary and multiple stars than expected, and compiled them with careful measurements of their relative positions in two catalogues presented to the Royal Society in London in 1782 (269 double or multiple systems) and 1784 (434 systems). A third catalogue of discoveries made after 1783 was published in 1821 (145 systems).
In 1797 Herschel measured many of the systems again, and discovered changes in their relative positions that could not be attributed to the parallax caused by the Earth's orbit. He waited until 1802 (in "Catalogue of 500 new Nebulae, nebulous Stars, planetary Nebulae, and Clusters of Stars; with Remarks on the Construction of the Heavens") to announce the hypothesis that the two stars might be "binary sidereal systems" orbiting under mutual gravitational attraction, a hypothesis he confirmed in 1803 in his "Account of the Changes that have happened, during the last Twenty-five Years, in the relative Situation of Double-stars; with an Investigation of the Cause to which they are owing". In all, Herschel discovered over 800 confirmed double or multiple star systems, almost all of them physical rather than virtual pairs. His theoretical and observational work provided the foundation for modern binary star astronomy; new catalogues adding to his work were not published until after 1820 by Friedrich Wilhelm Struve, James South and John Herschel.
Uranus.
In March 1781, during his search for double stars, Herschel noticed an object appearing as a nonstellar disk. Herschel originally thought it was a comet or a star. He made many more observations of it, and afterwards Russian Academician Anders Lexell computed the orbit and found it to be probably planetary. Herschel determined in agreement that it must be a planet beyond the orbit of Saturn. He called the new planet the 'Georgian star' (Georgium sidus) after King George III, which also brought him favour; the name did not stick. In France, where reference to the British king was to be avoided if possible, the planet was known as 'Herschel' until the name 'Uranus' was universally adopted. The same year, Herschel was awarded the Copley Medal and elected a Fellow of the Royal Society. In 1782, he was appointed "The King’s Astronomer" (not to be confused with the Astronomer Royal). He and his sister subsequently moved to Datchet (then in Buckinghamshire but now in Berkshire) on 1 August 1782. He continued his work as a telescope maker and achieved an international reputation for their manufacture, profitably selling over 60 completed reflectors to British and Continental astronomers.
Deep sky surveys.
From 1782 to 1802, and most intensively from 1783 to 1790, Herschel conducted systematic surveys in search of "deep sky" or nonstellar objects with two 20 foot, 12 inch and 18.7 inch telescopes (in combination with his favoured 6-inch aperture instrument). Excluding duplicated and "lost" entries, Herschel ultimately discovered over 2400 objects defined by him as nebulae. (At that time, nebula was the generic term for any visually extended or diffuse astronomical object, including galaxies beyond the Milky Way, until galaxies were confirmed as extragalactic systems by Edwin Hubble in 1924.)
Herschel published his discoveries as three catalogues: "Catalogue of One Thousand New Nebulae and Clusters of Stars" (1786), "Catalogue of a Second Thousand New Nebulae and Clusters of Stars" (1789) and the previously cited "Catalogue of 500 New Nebulae ..." (1802). He arranged his discoveries under eight "classes": (I) bright nebulae, (II) faint nebulae, (III) very faint nebulae, (IV) planetary nebulae, (V) very large nebulae, (VI) very compressed and rich clusters of stars, (VII) compressed clusters of small and large [faint and bright] stars, and (VIII) coarsely scattered clusters of stars. Herschel's discoveries were supplemented by those of Caroline Herschel (11 objects) and his son John Herschel (1754 objects) and published by him as "General Catalogue of Nebulae and Clusters" in 1864. This catalogue was later edited by John Dreyer, supplemented with discoveries by many other 19th century astronomers, and published in 1888 as the "New General Catalogue" (abbreviated NGC) of 7840 deep sky objects. The NGC numbering is still the most commonly used identifying label for these celestial landmarks.
Work with his sister Caroline.
In 1783 he gave Caroline a telescope, and she began to make astronomical discoveries in her own right, particularly comets. She discovered or observed eight comets, eleven nebulae and, at her brother's suggestion, updated and corrected Flamsteed's work detailing the position of stars. This was published as the British Catalogue of Stars. She was honoured by the Royal Astronomical Society for this work. Caroline also continued to serve as his assistant, often taking notes while he observed at the telescope.
In June 1785, owing to damp conditions, he and Caroline moved to Clay Hall in Old Windsor. In 1786, the Herschels moved to a new residence on Windsor Road in Slough. He lived the rest of his life in this residence, which came to be known as Observatory House. It is no longer standing.
On 7 May 1788, he married the widow Mary Pitt (née Baldwin) at St Laurence's Church, Upton in Slough. His sister Caroline then moved to separate lodgings, but continued to work as his assistant.
Herschel's telescopes.
During his career, he constructed more than four hundred telescopes. The largest and most famous of these was a reflecting telescope with a 49+1/2 in primary mirror and a 40-foot (12 m) focal length. Because of the poor reflectivity of the speculum mirrors of that day, Herschel eliminated the small diagonal mirror of a standard newtonian reflector from his design and tilted his primary mirror so he could view the formed image directly. This design has come to be called the Herschelian telescope. On 28 August 1789, his first night of observation using this instrument, he discovered a new moon of Saturn. A second moon followed within the first month of observation. The "40-foot telescope" proved very cumbersome, and most of his observations were done with a smaller 18.5 in 20 ft reflector. Herschel discovered that unfilled telescope apertures can be used to obtain high angular resolution, something which became the essential basis for interferometric imaging in astronomy (in particular Aperture Masking Interferometry and hypertelescopes).
Reconstruction of the 20ft telescope.
In 2012, the BBC "Stargazing Live" television programme built a replica of the 20-foot telescope using Herschel's original plans but modern materials. It is to be considered a close modern approximation rather than an exact replica. A modern glass mirror was used, the frame uses metal scaffolding and the tube is a sewer pipe. The telescope was shown on the programme in January 2013 and stands on the art, design and technology campus of the University of Derby where it will be used for educational purposes.
Life on other celestial bodies.
Herschel was sure that he had found ample evidence of life on the Moon and compared it to the English countryside. He did not refrain himself from theorizing that the other planets were populated, with an special interest in Mars, which was competely in line with most of his contemporary scientists. At Herschel's time, scientists tended to believe in a plurality of civilized worlds, while most religious thinkers referred to unique properties of the earth.
 Herschel went so far to speculate that the interior of the sun was populated.
Sunspots, climate, and wheat yields.
Herschel started to examine the correlation of solar variation and solar cycle and climate. Over a period of 40 years (1779–1818), Herschel had regularly observed sunspots and their variations in number, form and size. Most of his observations took place in a period of low solar activity, the Dalton minimum. Therefore solar activity behaved very unusually. This was one of the reasons why Herschel was not able to identify the standard 11-year period in solar activity. Herschel compared his observations with the series of wheat prices published by Adam Smith in The Wealth of Nations.
1801 Herschel reported his findings to the Royal Society and indicated five prolonged periods of few sunspots correlated with costly wheat. The result of this review of the foregoing five periods is, that, from the price of wheat, it seems probable that some temporary scarcity or defect of vegetation has generally taken place, when the sun has been without those appearances which we surmise to be symptoms of a copious emission of light and heat.
 Herschel's study was ridiculed by some of his contemporaries but did initiate further attempts to find a correlation. Later in the 19th century, William Stanley Jevons proposed the 11-year- cycle and Herschels basic idea of a correlation between low amount of sunspots and lower yields to explain for recurring booms and slumps in the economy. Herschels speculation on a connection between sunspots and regional climate, using the market price of wheat as a proxy continues to be cited regularly till today.
According a study of the Israel Cosmic Ray Center about the influence of solar activity on the historical wheat market in England, all ten solar cycles between 1600 and 1700 show high wheat prices coinciding with low activity, and vice versa. The topic is still subject of controversies and the significance of the correlation is doubted by some scientists.
Further discoveries.
In his later career, Herschel discovered two moons of Saturn, Mimas and Enceladus; as well as two moons of Uranus, Titania and Oberon. He did not give these moons their names; they were named by his son John in 1847 and 1852, respectively, after his death.
In 2007 evidence was cited by Dr. Stuart Eves that Herschel might have discovered rings around Uranus.
Herschel measured the axial tilt of Mars and discovered that the martian ice caps, first observed by Giovanni Domenico Cassini (1666) and Christiaan Huygens (1672), changed size with the planet's seasons.
From studying the proper motion of stars, he was the first to realise that the solar system is moving through space, and he determined the approximate direction of that movement. He also studied the structure of the Milky Way and concluded that it was in the shape of a disk. He incorrectly assumed the sun was in the centre of the disc, a theory known as Galactocentrism, which was eventually corrected by the findings of Harlow Shapley in 1918.
He also coined the word "asteroid", meaning "star-like" (from the Greek "asteroeides", "aster" "star" + "-eidos" "form, shape"), in 1802 (shortly after Olbers discovered the second minor planet, 2 Pallas, in late March), to describe the star-like appearance of the small moons of the giant planets and of the minor planets; the planets all show discs, by comparison. By the 1850s 'asteroid' became a standard term for describing certain minor planets.
Discovery of infrared radiation in sunlight.
On 11 February 1800, Herschel was testing filters for the sun so he could observe sun spots. When using a red filter he found there was a lot of heat produced. Herschel discovered infrared radiation in sunlight by passing it through a prism and holding a thermometer just beyond the red end of the visible spectrum. This thermometer was meant to be a control to measure the ambient air temperature in the room. He was shocked when it showed a higher temperature than the visible spectrum. Further experimentation led to Herschel's conclusion that there must be an invisible form of light beyond the visible spectrum.
Biology.
Herschel used a microscope to establish that coral was not a plant, as many believed at the time, since it lacked the cell walls characteristic of plants.
Family and death.
William Herschel and Mary had one child, John, born at Observatory House on 7 March 1792. William's personal background and rise as man of science had a profound impact on the upbringing of his son and grandchildren. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1788. In 1816, William was made a Knight of the Royal Guelphic Order by the Prince Regent and was accorded the honorary title 'Sir' although this was not the equivalent of an official British knighthood. He helped to found the Astronomical Society of London in 1820, which in 1831 received a royal charter and became the Royal Astronomical Society. In 1813, he was elected a foreign member of the Royal Swedish Academy of Sciences.
On 25 August 1822, Herschel died at Observatory House, Windsor Road, Slough, and is buried at nearby St Laurence's Church, Upton, Slough. Herschel's epitaph is
 Coelorum perrupit claustra.<br>(He broke through the barriers of the heavens.)
Herschel's son John Herschel also became a famous astronomer. One of William's brothers, Alexander Herschel, moved permanently to England, near his sister Caroline and nephew John. Caroline returned to Hanover after the death of her brother. She died on 9 January 1848.
His house at 19 New King Street in Bath, Somerset where he made many telescopes and first observed Uranus, is now home to the Herschel Museum of Astronomy.
Memorial.
William Herschel lived most of his life in Slough, a town then in Buckinghamshire. He died in the town and was buried under the tower of the Church of St Laurence, Upton-cum-Chalvey, near Slough. Herschel is very much respected in the town and there are several memorials to him and his discoveries.
In 2011 a new bus station, the design of which was inspired by the infrared experiment of William Herschel, was built in the centre of Slough. This claim of inspiration is not universally accepted.
Musical works.
Herschel's complete musical works were as follows:
Various vocal works including a "Te Deum", psalms, motets and sacred chants along with some catches.
Keyboard works for organ and harpsichord: 
Sources.
 #if: 
 #if:  
 |, 
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: Sir William Herschel, his life and works
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |: {{
 #if: Sir William Herschel, his life and works
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 
 #if:  
 |{{
 #if: Holden
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2="  
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: Holden
 |{{
 #if: 1881
 |, 1881{{
 #if:
}}{{
 #if: 
 #ifeq: | 1881
 |{{
 #if: 
 #if: Holden
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: Sir William Herschel, his life and works
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |, {{
 #if: Sir William Herschel, his life and works
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if:  
 |, {{
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |journal&rft.genre=article&rft.atitle={{urlencode:  
 |book{{
 #if: 
 |&rft.genre=bookitem&rft.btitle={{urlencode:}}&rft.atitle={{urlencode:  
 |&rft.genre=book&rft.btitle={{urlencode:  
 #if: Holden |&rft.aulast={{urlencode:Holden}}{{
 }}{{
 #if: Holden |&rft.au={{urlencode:Holden}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: Sir William Herschel, his life and works
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |&rft.pages={{urlencode: {{
 #if: Sir William Herschel, his life and works
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle = 
 |IncludedWorkURL = 
 |Other = 
 |Edition = 
 |Place = New York
 |PublicationPlace = New York
 |Publisher = Charles Scribner's Sons. Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=Holden
 |Sep = .
 |PS = .
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 

</doc>
<doc id="33972" url="http://en.wikipedia.org/wiki?curid=33972" title="Wolfgang Pauli">
Wolfgang Pauli

Wolfgang Ernst Pauli (25 April 1900 – 15 December 1958) was an Austrian-born Swiss theoretical physicist and one of the pioneers of quantum physics.
In 1945, after having been nominated by Albert Einstein, Pauli received the Nobel Prize in Physics for his "decisive contribution through his discovery of a new law of Nature, the exclusion principle or Pauli principle." The discovery involved spin theory, which is the basis of a theory of the structure of matter.
Biography.
Early years.
Pauli was born in Vienna to a chemist Wolfgang Joseph Pauli ("né" Wolf Pascheles, 1869–1955) and his wife Bertha Camilla Schütz. His middle name was given in honor of his godfather, physicist Ernst Mach. Pauli's paternal grandparents were from prominent Jewish families of Prague; his great-grandfather was the Jewish publisher Wolf Pascheles. Pauli's father converted from Judaism to Roman Catholicism shortly before his marriage in 1899. Pauli's mother, Bertha Schütz, was raised in her own mother's Roman Catholic religion; her father was Jewish writer Friedrich Schütz. Pauli was raised as a Roman Catholic, although eventually he and his parents left the Church. He is considered to have been a deist and a mystic.
Pauli attended the Döblinger-Gymnasium in Vienna, graduating with distinction in 1918. Only two months after graduation, he published his first paper, on Albert Einstein's theory of general relativity. He attended the Ludwig-Maximilians University in Munich, working under Arnold Sommerfeld, where he received his PhD in July 1921 for his thesis on the quantum theory of ionized hydrogen.
Sommerfeld asked Pauli to review the theory of relativity for the "Encyklopädie der mathematischen Wissenschaften" ("Encyclopedia of Mathematical Sciences"). Two months after receiving his doctorate, Pauli completed the article, which came to 237 pages. It was praised by Einstein; published as a monograph, it remains a standard reference on the subject to this day.
Pauli spent a year at the University of Göttingen as the assistant to Max Born, and the following year at the Institute for Theoretical Physics in Copenhagen, which later became the Niels Bohr Institute in 1965. From 1923 to 1928, he was a lecturer at the University of Hamburg. During this period, Pauli was instrumental in the development of the modern theory of quantum mechanics. In particular, he formulated the exclusion principle and the theory of nonrelativistic spin.
In 1928, he was appointed Professor of Theoretical Physics at ETH Zurich in Switzerland where he made significant scientific progress. He held visiting professorships at the University of Michigan in 1931, and the Institute for Advanced Study in Princeton in 1935. He was awarded the Lorentz Medal in 1931.
At the end of 1930, shortly after his postulation of the neutrino and immediately following his divorce in November, Pauli had a severe breakdown. He consulted psychiatrist and psychotherapist Carl Jung who, like Pauli, lived near Zurich. Jung immediately began interpreting Pauli's deeply archetypal dreams, and Pauli became one of the depth psychologist's best students. He soon began to criticize the epistemology of Jung's theory scientifically, and this contributed to a certain clarification of the latter's thoughts, especially about the concept of synchronicity. A great many of these discussions are documented in the Pauli/Jung letters, today published as "Atom and Archetype". Jung's elaborate analysis of more than 400 of Pauli's dreams is documented in "Psychology and Alchemy".
The German annexation of Austria in 1938 made him a German citizen, which became a problem for him in 1939 after the outbreak of World War II. In 1940, he tried in vain to obtain Swiss citizenship, which would have allowed him to remain at the ETH.
Pauli moved to the United States in 1940, where he was employed as a professor of theoretical physics at the Institute for Advanced Study. In 1946, after the war, he became a naturalized citizen of the United States and subsequently returned to Zurich, where he mostly remained for the rest of his life. In 1949, he was granted Swiss citizenship.
In 1958, Pauli was awarded the Max Planck medal. In that same year, he fell ill with pancreatic cancer. When his last assistant, Charles Enz, visited him at the Rotkreuz hospital in Zurich, Pauli asked him: "Did you see the room number?" It was number 137. Throughout his life, Pauli had been preoccupied with the question of why the fine structure constant, a dimensionless fundamental constant, has a value nearly equal to 1/137. Pauli died in that room on 15 December 1958.
Scientific research.
Pauli made many important contributions in his career as a physicist, primarily in the field of quantum mechanics. He seldom published papers, preferring lengthy correspondences with colleagues such as Niels Bohr and Werner Heisenberg, with whom he had close friendships. Many of his ideas and results were never published and appeared only in his letters, which were often copied and circulated by their recipients.
Pauli proposed in 1924 a new quantum degree of freedom (or quantum number) with two possible values, in order to resolve inconsistencies between observed molecular spectra and the developing theory of quantum mechanics. He formulated the Pauli exclusion principle, perhaps his most important work, which stated that no two electrons could exist in the same quantum state, identified by four quantum numbers including his new two-valued degree of freedom. The idea of spin originated with Ralph Kronig. George Uhlenbeck and Samuel Goudsmit one year later identified Pauli's new degree of freedom as electron spin.
In 1926, shortly after Heisenberg published the matrix theory of modern quantum mechanics, Pauli used it to derive the observed spectrum of the hydrogen atom. This result was important in securing credibility for Heisenberg's theory.
Pauli introduced the 2 × 2 Pauli matrices as a basis of spin operators, thus solving the nonrelativistic theory of spin. This work is sometimes said to have influenced Paul Dirac in his creation of the Dirac equation for the relativistic electron, though Dirac stated that he invented these same matrices himself independently at the time, without Pauli's influence. Dirac invented similar but larger (4x4) spin matrices for use in his relativistic treatment of fermionic spin.
In 1930, Pauli considered the problem of beta decay. In a letter of 4 December to Lise Meitner "et al.", beginning, "Dear radioactive ladies and gentlemen", he proposed the existence of a hitherto unobserved neutral particle with a small mass, no greater than 1% the mass of a proton, in order to explain the continuous spectrum of beta decay. In 1934, Enrico Fermi incorporated the particle, which he called a neutrino, into his theory of beta decay. The neutrino was first confirmed experimentally in 1956 by Frederick Reines and Clyde Cowan, two and a half years before Pauli's death. On receiving the news, he replied by telegram: "Thanks for message. Everything comes to him who knows how to wait. Pauli."
In 1940, he re-derived the spin-statistics theorem, a critical result of quantum field theory which states that particles with half-integer spin are fermions, while particles with integer spin are bosons.
In 1949, he published a paper on Pauli–Villars regularization: regularization is the term for techniques which modify infinite mathematical integrals to make them finite during calculations, so that one can identify whether the intrinsically infinite quantities in the theory (mass, charge, wavefunction) form a finite and hence calculable set which can be redefined in terms of their experimental values, which criterion is termed renormalization, and which removes infinities from quantum field theories, but also importantly allows the calculation of higher order corrections in perturbation theory.
Pauli made repeated criticisms of the modern synthesis of evolutionary biology, and his contemporary admirers point to modes of epigenetic inheritance as supportive of his arguments.
Personality and reputation.
The Pauli effect was named after the anecdotal bizarre ability of his to break experimental equipment simply by being in the vicinity. Pauli was aware of his reputation and was delighted whenever the Pauli effect manifested. These strange occurrences were in line with his investigations into the legitimacy of parapsychology, particularly his collaboration with C. G. Jung on the concept of synchronicity.
Regarding physics, Pauli was famously a perfectionist. This extended not just to his own work, but also to the work of his colleagues. As a result, he became known in the physics community as the "conscience of physics," the critic to whom his colleagues were accountable. He could be scathing in his dismissal of any theory he found lacking, often labelling it "ganz falsch", utterly wrong.
However, this was not his most severe criticism, which he reserved for theories or theses so unclearly presented as to be untestable or unevaluatable and, thus, not properly belonging within the realm of science, even though posing as such. They were worse than wrong because they could not be proven wrong. Famously, he once said of such an unclear paper: "It is not even wrong!"
His supposed remark when meeting another leading physicist, Paul Ehrenfest, illustrates this notion of an arrogant Pauli. The two met at a conference for the first time. Ehrenfest was familiar with Pauli's papers and was quite impressed with them. After a few minutes of conversation, Ehrenfest remarked, "I think I like your Encyclopedia article [on relativity theory] better than I like you," to which Pauli shot back, "That's strange. With me, regarding you, it is just the opposite." The two became very good friends from then on.
A somewhat warmer picture emerges from this story which appears in the article on Dirac:
"Werner Heisenberg [in "Physics and Beyond", 1971] recollects a friendly conversation among young participants at the 1927 Solvay Conference, about Einstein and Planck's views on religion. Wolfgang Pauli, Heisenberg, and Dirac took part in it. Dirac's contribution was a poignant and clear criticism of the political manipulation of religion, that was much appreciated for its lucidity by Bohr, when Heisenberg reported it to him later. Among other things, Dirac said: "I cannot understand why we idle discussing religion. If we are honest – and as scientists honesty is our precise duty – we cannot help but admit that any religion is a pack of false statements, deprived of any real foundation. The very idea of God is a product of human imagination. [...] I do not recognize any religious myth, at least because they contradict one another. [...]" Heisenberg's view was tolerant. Pauli had kept silent, after some initial remarks. But when finally he was asked for his opinion, jokingly he said: "Well, I'd say that also our friend Dirac has got a religion and the first commandment of this religion is 'God does not exist and Paul Dirac is his prophet'". Everybody burst into laughter, including Dirac.
Many of Pauli's ideas and results were never published and appeared only in his letters, which were often copied and circulated by their recipients. Pauli may have been unconcerned that much of his work thus went uncredited, but when it came to Heisenberg's world-renowned 1958 lecture at Göttingen on their joint work on a unified field theory, and the press release calling Pauli a mere "assistant to Professor Heisenberg", Pauli became offended, shooting back several times at CERN and elsewhere by denouncing Heisenberg's physics prowess. The deterioration between them resulted in Heisenberg ignoring Pauli's funeral, and writing in his autobiography that Pauli's criticisms were overwrought.
Personal life.
In May 1929, Pauli left the Roman Catholic Church. In December of that year, he married Käthe Margarethe Deppner. The marriage was an unhappy one, ending in divorce in 1930 after less than a year. He married again in 1934 to Franziska Bertram (1901-1987). They had no children.

</doc>
<doc id="33974" url="http://en.wikipedia.org/wiki?curid=33974" title="Will Eisner">
Will Eisner

William Erwin "Will" Eisner (March 6, 1917 – January 3, 2005) was an American cartoonist, writer, and entrepreneur. He was one of the earliest cartoonists to work in the American comic book industry, and his series "The Spirit" (1940–1952) was noted for its experiments in content and form. In 1978, he popularized the term "graphic novel" with the publication of his book "A Contract with God". He was an early contributor to formal comics studies with his book "Comics and Sequential Art" (1985). The Eisner Award was named in his honor, and is given to recognize achievements each year in the comics medium; he was one of the three inaugural inductees to the Will Eisner Comic Book Hall of Fame.
Biography.
Family background.
Eisner's father Shmuel "Samuel" Eisner was born March 6, 1886, in Kolomyia, Austria-Hungary, and was one of eleven children. He aspired to be an artist, and as a teenager painted murals for rich patrons and Catholic churches in Vienna. To avoid conscription in the army, he moved to New York before the outbreak of World War I. There he found getting work difficult as his English skills were poor. He made what living he could painting backdrops for vaudeville and the Jewish theater.
Eisner's mother, Fannie Ingber, was born to Jewish parents from Romania April 25, 1891, on a ship bound for the US. Her mother died on her tenth birthday, and was quickly followed by her father. An older stepsister thereafter raised her and kept her so busy with chores that she had little time for socializing or schooling; she did what she could later in life to keep knowledge of her illiteracy from her children.
Family introduced Shmuel and Fannie, who were distant relatives. They had three children: son Will Erwin, born on his father's birthday in 1917; son Julian, born February 3, 1921; and daughter Rhoda, born November 2, 1929.
Early life.
Eisner was born in Brooklyn, New York City. He grew up poor, and the family moved frequently. Young Eisner often got into physical confrontations when subject to antisemitism from his schoolmates. His family were not orthodox followers of Judaism; Eisner himself, while he prided his cultural background, turned against the religion when his family was denied entry to a synagogue over lack of money for admission.
Young Eisner was tall and of sturdy build, but lacked athletic skills. He was a voracious consumer of pulp magazines and film, including avant-garde films such as those by Man Ray. To his mother's disappointment, Eisner had his father's interest in art, and his father encouraged him by buying him art supplies.
Eisner's mother frequently berated his father for not providing the family a better income, as he went from one job to another. Without success he also tried his hand at such ventures as a furniture retailer and a coat factory. The family situation was especially dire following the Wall Street Crash of 1929 that marked the beginning of the Great Depression. In 1930, the situation was so desperate that Eisner's mother demanded that he, at thirteen, find some way to contribute to the family's income. He entered working life selling newspapers on street corners, a competitive job where the toughest boys fought for the best locations.
Eisner attended DeWitt Clinton High School. With influences that included the early 20th-century commercial artist J. C. Leyendecker, he drew for the school newspaper ("The Clintonian"), the literary magazine ("The Magpie") and the yearbook, and did stage design, leading him to consider doing that kind of work for theater. Upon graduation, he studied under Canadian artist George Brandt Bridgman (1864–1943) for a year at the Art Students League of New York. Contacts made there led to a position as an advertising writer-cartoonist for the "New York American" newspaper. Eisner also drew $10-a-page illustrations for pulp magazines, including "Western Sheriffs and Outlaws".
In 1936, high-school friend and fellow cartoonist Bob Kane, of future Batman fame, suggested that the 19-year-old Eisner try selling cartoons to the new comic book "Wow, What A Magazine!" "Comic books" at the time were tabloid-sized collections of comic strip reprints in color. In 1935, they had begun to include occasional new comic strip-like material. "Wow" editor Jerry Iger bought an Eisner adventure strip called "Captain Scott Dalton", an H. Rider Haggard-styled hero who traveled the world after rare artifacts. Eisner subsequently wrote and drew the pirate strip "The Flame" and the secret agent strip "Harry Karry" for "Wow" as well.
Eisner said that on one occasion a man who Eisner described as "a Mob type straight out of Damon Runyon, complete with pinkie ring, broken nose, black shirt, and white tie, who claimed to have 'exclusive distribution rights for all Brooklyn" asked Eisner to draw Tijuana bibles for $3 a page. Eisner said that he declined the offer; he described the decision as "one of the most difficult moral decisions of my life".
Eisner & Iger.
"Wow" lasted four issues (cover-dated July–September and November 1936). After it ended, Eisner and Iger worked together producing and selling original comics material, anticipating that the well of available reprints would soon run dry, though their accounts of how their partnership was founded differ. One of the first such comic-book "packagers", their partnership was an immediate success, and the two soon had a stable of comics creators supplying work to Fox Comics, Fiction House, Quality Comics (for whom Eisner co-created such characters as Doll Man and Blackhawk), and others.
Turning a profit of $1.50 a page, Eisner claimed that he "got very rich before I was 22," later detailing that in Depression-era 1939 alone, he and Iger "had split $25,000 between us", a considerable amount for the time.
Among the studio's products was a self-syndicated Sunday comic strip, "Hawks of the Sea", that initially reprinted Eisner's old strip "Wow, What A Magazine!" feature "The Flame" and then continued it with new material. Eisner's original work even crossed the Atlantic, with Eisner drawing the new cover of the October 16, 1937 issue of Boardman Books' comic-strip reprint tabloid "Okay Comics Weekly."
In 1939, Eisner was commissioned to create Wonder Man for Victor Fox, an accountant who had previously worked at DC Comics and was becoming a comic book publisher himself. Following Fox's instructions to create a Superman-type character, and using the pen name Willis, Eisner wrote and drew the first issue of "Wonder Comics." Eisner said in interviews throughout his later life that he had protested the derivative nature of the character and story, and that when subpoenaed after National Periodical Publications, the company that would evolve into DC Comics, sued Fox, alleging Wonder Man was an illegal copy of Superman, Eisner testified that this was so, undermining Fox's case; Eisner even depicts himself doing so in his semi-autobiographical graphic novel "The Dreamer". However, a transcript of the proceeding, uncovered by comics historian Ken Quattro in 2010, indicates Eisner in fact supported Fox and claimed Wonder Man as an original Eisner creation.
"The Spirit".
In "late '39, just before Christmas time," Eisner recalled in 1979, Quality Comics publisher Everett M. "Busy" Arnold "came to me and said that the Sunday newspapers were looking for a way of getting into this comic book boom," In a 2004 interview, he elaborated on that meeting:
"Busy" invited me up for lunch one day and introduced me to Henry Martin [sales manager of The Des Moines Register and Tribune Syndicate, who] said, "The newspapers in this country, particularly the Sunday papers, are looking to compete with comics books, and they would like to get a comic-book insert into the newspapers." ... Martin asked if I could do it. ... It meant that I'd have to leave Eisner & Iger [which] was making money; we were very profitable at that time and things were going very well. A hard decision. Anyway, I agreed to do the Sunday comic book and we started discussing the deal [which] was that we'd be partners in the 'Comic Book Section,' as they called it at that time. And also, I would produce two other magazines in partnership with Arnold.
Eisner negotiated an agreement with the syndicate in which Arnold would copyright "The Spirit," but, "Written down in the contract I had with 'Busy' Arnold — and this contract exists today as the basis for my copyright ownership — Arnold agreed that it was my property. They agreed that if we had a split-up in any way, the property would revert to me on that day that happened. My attorney went to 'Busy' Arnold and his family, and they all signed a release agreeing that they would not pursue the question of ownership" This would include the eventual backup features "Mr. Mystic" and "Lady Luck".
Selling his share of their firm to Iger, who would continue to package comics as the S. M. Iger Studio and as Phoenix Features through 1955, for $20,000, Eisner left to create "The Spirit." "They gave me an adult audience", Eisner said in 1997, "and I wanted to write better things than superheroes. Comic books were a ghetto. I sold my part of the enterprise to my associate and then began The Spirit. They wanted an heroic character, a costumed character. They asked me if he'd have a costume. And I put a mask on him and said, 'Yes, he has a costume!'"
"The Spirit", an initially eight- and later seven-page urban-crimefighter series, ran with the initial backup features "Mr. Mystic" and "Lady Luck" in a 16-page Sunday supplement (colloquially called "The Spirit Section") that was eventually distributed in 20 newspapers with a combined circulation of as many as five million copies. It premiered June 2, 1940, and continued through 1952. Eisner has cited the Spirit story "Gerhard Shnobble" as a particular favorite, as it was one of his first attempts at injecting his personal point of view into the series.
World War II and "Joe Dope".
Eisner was drafted into the U.S. Army in "late '41, early '42" and then "had about another half-year which the government gave me to clean up my affairs before going off" to fight in World War II. He was assigned to the camp newspaper in Aberdeen, where "there was also a big training program there, so I got involved in the use of comics for training. ... I finally became a warrant officer, which involved taking a test – that way you didn't have to go through Officer Candidate School."
En route to Washington, D.C., he stopped at the Holabird Depot in Baltimore, Maryland, where a mimeographed publication titled "Army Motors" was put together. "Together with the people there ... I helped develop its format. I began doing cartoons – and we began fashioning a magazine that had the ability to talk to the G.I.s in their language. So I began to use comics as a teaching tool, and when I got to Washington, they assigned me to the business of teaching – or selling – preventive maintenance."
Eisner then created the educational comic strip and titular character "Joe Dope" for "Army Motors", and spent four years working in The Pentagon editing the ordnance magazine "Firepower" and doing "all the general illustrations – that is, cartoons" for "Army Motors". He continued to work on that and its 1950 successor magazine, "PS, The Preventive Maintenance Monthly" until 1971. Eisner also illustrated an official Army pamphlet in 1968 and 1969 called "The M16A1 Rifle" specifically for troops in Vietnam. Eisner's style helped to popularize these officially-distributed works in order to better educate soldiers on equipment maintenance.
While Eisner's later graphic novels were entirely his own work, he had a studio working under his supervision on "The Spirit". In particular, letterer Abe Kanegson came up with the distinctive lettering style which Eisner himself would later imitate in his book-length works, and Kanegson would often rewrite Eisner's dialogue.
Eisner's most trusted assistant on "The Spirit," however, was Jules Feiffer, later a renowned cartoonist, playwright and screenwriter in his own right. Eisner later said of their working methods "You should hear me and Jules Feiffer going at it in a room. 'No, you designed the splash page for this one, then you wrote the ending — I came up with the idea for the story, and you did it up to this point, then I did the next page and this sequence here and...' And I'll be swearing up and down that 'he' wrote the ending on that one. We never agree".
So trusted were Eisner's assistants that Eisner allowed them to "ghost" "The Spirit" from the time that he was drafted into the U.S. Army in 1942 until his return to civilian life in 1945. The primary wartime artists were the uncredited Lou Fine and Jack Cole, with future "Kid Colt, Outlaw" artist Jack Keller drawing backgrounds. Ghost writers included Manly Wade Wellman and William Woolfolk. The wartime ghosted stories have been reprinted in DC Comics' hardcover collections "The Spirit Archives" Vols. 5 to 11 (2001–2003), spanning July 1942 – December 1944.
On Eisner's return from service and resumption of his role in the studio, he created the bulk of the "Spirit" stories on which his reputation was solidified. The post-war years also saw him attempt to launch the comic-strip/comic-book series "Baseball," "John Law," "Kewpies," and "Nubbin the Shoeshine Boy;" none succeeded, but some material was recycled into "The Spirit".
American Visuals Corporation.
During his World War II military service, Eisner had introduced the use of comics for training personnel in the publication "Army Motors", for which he created the cautionary bumbling soldier Joe Dope, who illustrated various methods of preventive maintenance of various military equipment and weapons. In 1948, while continuing to do "The Spirit" and seeing television and other post-war trends eat at the readership base of newspapers, he formed the American Visuals Corporation in order to produce instructional materials for the government, related agencies, and businesses.
One of his longest-running jobs was "PS, The Preventive Maintenance Monthly," a digest sized magazine with comic book elements that he started for the Army in 1951 and continued to work on until the 1970s with Klaus Nordling, Mike Ploog, and other artists. In addition, Eisner produced other military publications such as the graphic manual in 1969, "", which was distributed along with cleaning kits to address serious reliability concerns with the M16 Rifle during the Vietnam War.
Other clients of his Connecticut-based company included RCA Records, the Baltimore Colts NFL football team, and New York Telephone.
Graphic novels.
In the late 1970s, Eisner turned his attention to longer storytelling forms. "A Contract with God, and Other Tenement Stories" (Baronet Books, October 1978) is an early example of an American graphic novel, combining thematically linked short stories into a single square-bound volume. Eisner continued with a string of graphic novels that tell the history of New York's immigrant communities, particularly Jews, including "The Building", "A Life Force", "Dropsie Avenue" and "To the Heart of the Storm". He continued producing new books into his seventies and eighties, at an average rate of nearly one a year. Each of these books was done twice — once as a rough version to show editor Dave Schreiner, then as a second, finished version incorporating suggested changes.
Some of his last work was the retelling in sequential art of novels and myths, including "Moby-Dick". In 2002, at the age of 85, he published "Sundiata", based on the part-historical, part-mythical stories of a West African king, "The Lion of Mali". "Fagin the Jew" is an account of the life of Dickens' character Fagin, in which Eisner tries to get past the stereotyped portrait of Fagin in "Oliver Twist".
His last graphic novel, "The Plot: The Secret Story of The Protocols of the Elders of Zion", an account of the making of the anti-semitic hoax "The Protocols of the Learned Elders of Zion", was completed shortly before his death and published in 2005.
Teaching.
In his later years especially, Eisner was a frequent lecturer about the craft and uses of sequential art. He taught at the School of Visual Arts in New York City, where he published "Will Eisner's Gallery", a collection of work by his students and wrote two books based on these lectures, "Comics and Sequential Art" and "Graphic Storytelling and Visual Narrative", which are widely used by students of cartooning. In 2002, Eisner participated in the Will Eisner Symposium of the 2002 University of Florida Conference on Comics and Graphic Novels.
Death.
Eisner died January 3, 2005, in Lauderdale Lakes, Florida, of complications from a quadruple bypass surgery performed December 22, 2004. DC Comics held a memorial service in Manhattan's Lower East Side, a neighborhood Eisner often visited in his work, at the Angel Orensanz Foundation on Norfolk Street.
Eisner was survived by his wife, Ann Weingarten Eisner, and their son, John. In the introduction to the 2001 reissue of "A Contract with God", Eisner revealed that the inspiration for the title story grew out of the 1970 death of his leukemia-stricken teenaged daughter, Alice, next to whom he is buried. Until then, only Eisner's closest friends were aware of his daughter's life and death.
Awards and honors.
Eisner has been recognized for his work with the National Cartoonists Society Comic Book Award for 1967, 1968, 1969, 1987 and 1988, as well as its Story Comic Book Award in 1979, and its Reuben Award in 1998.
He was inducted into the Academy of Comic Book Arts Hall of Fame in 1971, and the Jack Kirby Hall of Fame in 1987. The following year, the Will Eisner Comic Industry Awards were established in his honor.
He received in 1975 the second Grand Prix de la ville d'Angoulême.
With Jack Kirby, Robert Crumb, Harvey Kurtzman, Gary Panter, and Chris Ware, Eisner was among the artists honored in the exhibition "Masters of American Comics" at the Jewish Museum in New York City, from September 16, 2006 to January 28, 2007.
On the 94th anniversary of Eisner's birth, in 2011, Google used an image featuring the Spirit as its logo.
References.
Works cited.
</dl>

</doc>
<doc id="33978" url="http://en.wikipedia.org/wiki?curid=33978" title="Weather">
Weather

Weather is the state of the atmosphere, to the degree that it is hot or cold, wet or dry, calm or stormy, clear or cloudy. Weather, seen from an anthropological perspective, is something all humans in the world constantly experience through their senses, at least while being outside. There are socially and scientifically constructed understandings of what weather is, what makes it change, the effect it has on humans in different situations, etc. Therefore weather is something people often communicate about. 
Most weather phenomena occur in the troposphere, just below the stratosphere. Weather generally refers to day-to-day temperature and precipitation activity, whereas climate is the term for the statistics of atmospheric conditions over longer periods of time. When used without qualification, "weather" is generally understood to mean the weather of Earth.
Weather is driven by air pressure (temperature and moisture) differences between one place and another. These pressure and temperature differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On Earth's surface, temperatures usually range ±40 °C (−40 °F to 100 °F) annually. Over thousands of years, changes in Earth's orbit can affect the amount and distribution of solar energy received by the Earth, thus influencing long-term climate and global climate change.
Surface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The system is a chaotic system; so small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that human activities such as agriculture and industry have modified weather patterns.
Studying how the weather works on other planets has been helpful in understanding how weather works on Earth. A famous landmark in the Solar System, Jupiter's "Great Red Spot", is an anticyclonic storm known to have existed for at least 300 years. However, weather is not limited to planetary bodies. A star's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System. The movement of mass ejected from the Sun is known as the solar wind.
Causes.
On Earth, the common weather phenomena include wind, cloud, rain, snow, fog and dust storms. Less common events include natural disasters such as tornadoes, hurricanes, typhoons and ice storms. Almost all familiar weather phenomena occur in the troposphere (the lower part of the atmosphere). Weather does occur in the stratosphere and can affect weather lower down in the troposphere, but the exact mechanisms are poorly understood.
Weather occurs primarily due to air pressure (temperature and moisture) differences between one place to another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. In other words, the farther from the tropics one lies, the lower the sun angle is, which causes those locations to be cooler due to the indirect sunlight. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow (see baroclinity). Weather systems in the tropics, such as monsoons or organized thunderstorm systems, are caused by different processes.
Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. In June the Northern Hemisphere is tilted towards the sun, so at any given Northern Hemisphere latitude sunlight falls more directly on that spot than in December (see Effect of sun angle on climate). This effect causes seasons. Over thousands to hundreds of thousands of years, changes in Earth's orbital parameters affect the amount and distribution of solar energy received by the Earth and influence long-term climate. (See Milankovitch cycles).
The uneven solar heating (the formation of zones of temperature and moisture gradients, or frontogenesis) can also be due to the weather itself in the form of cloudiness and precipitation. Higher altitudes are typically cooler than lower altitudes, which is explained by the lapse rate. In some situations, the temperature actually increases with height. This phenomenon is known as an inversion and can cause mountaintops to be warmer than the valleys below. Inversions can lead to the formation of fog and often act as a cap that suppresses thunderstorm development. On local scales, temperature differences can occur because different surfaces (such as oceans, forests, ice sheets, or man-made objects) have differing physical characteristics such as reflectivity, roughness, or moisture content.
Surface temperature differences in turn cause pressure differences. A hot surface warms the air above it causing it to expand and lower the density and the resulting surface air pressure. The resulting horizontal pressure gradient moves the air from higher to lower pressure regions, creating a wind, and the Earth's rotation then causes deflection of this air flow due to the Coriolis effect. The simple systems thus formed can then display emergent behaviour to produce more complex systems and thus other weather phenomena. Large scale examples include the Hadley cell while a smaller scale example would be coastal breezes.
The atmosphere is a chaotic system, so small changes to one part of the system can grow to have large effects on the system as a whole. This makes it difficult to accurately predict weather more than a few days in advance, though weather forecasters are continually working to extend this limit through the scientific study of weather, meteorology. It is theoretically impossible to make useful day-to-day predictions more than about two weeks ahead, imposing an upper limit to potential for improved prediction skill.
Shaping the planet Earth.
Weather is one of the fundamental processes that shape the Earth. The process of weathering breaks down the rocks and soils into smaller fragments and then into their constituent substances. During rains precipitation, the water droplets absorb and dissolve carbon dioxide from the surrounding air. This causes the rainwater to be slightly acidic, which aids the erosive properties of water. The released sediment and chemicals are then free to take part in chemical reactions that can affect the surface further (such as acid rain), and sodium and chloride ions (salt) deposited in the seas/oceans. The sediment may reform in time and by geological forces into other rocks and soils. In this way, weather plays a major role in erosion of the surface.
Effect on humans.
Effects on populations.
Weather has played a large and sometimes direct part in human history. Aside from climatic changes that have caused the gradual drift of populations (for example the desertification of the Middle East, and the formation of land bridges during glacial periods), extreme weather events have caused smaller scale population movements and intruded directly in historical events. One such event is the saving of Japan from invasion by the Mongol fleet of Kublai Khan by the Kamikaze winds in 1281. French claims to Florida came to an end in 1565 when a hurricane destroyed the French fleet, allowing Spain to conquer Fort Caroline. More recently, Hurricane Katrina redistributed over one million people from the central Gulf coast elsewhere across the United States, becoming the largest diaspora in the history of the United States.
The Little Ice Age caused crop failures and famines in Europe. The 1690s saw the worst famine in France since the Middle Ages. Finland suffered a severe famine in 1696–1697, during which about one-third of the Finnish population died.
Forecasting.
Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. Human beings have attempted to predict the weather informally for millennia, and formally since at least the nineteenth century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere and using scientific understanding of atmospheric processes to project how the atmosphere will evolve.
Once an all-human endeavor based mainly upon changes in barometric pressure, current weather conditions, and sky condition, forecast models are now used to determine future conditions. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes mean that forecasts become less accurate as the difference in current time and the time for which the forecast is being made (the "range" of the forecast) increases. The use of ensembles and model consensus helps to narrow the error and pick the most likely outcome.
There are a variety of end users to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to commodity traders within stock markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and the wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them.
Modification.
The aspiration to control the weather is evident throughout human history: from ancient rituals intended to bring rain for crops to the U.S. Military Operation Popeye, an attempt to disrupt supply lines by lengthening the North Vietnamese monsoon. The most successful attempts at influencing weather involve cloud seeding; they include the fog- and low stratus dispersion techniques employed by major airports, techniques used to increase winter precipitation over mountains, and techniques to suppress hail. A recent example of weather control was China's preparation for the 2008 Summer Olympic Games. China shot 1,104 rain dispersal rockets from 21 sites in the city of Beijing in an effort to keep rain away from the opening ceremony of the games on 8 August 2008. Guo Hu, head of the Beijing Municipal Meteorological Bureau (BMB), confirmed the success of the operation with 100 millimeters falling in Baoding City of Hebei Province, to the southwest and Beijing's Fangshan District recording a rainfall of 25 millimeters.
Whereas there is inconclusive evidence for these techniques' efficacy, there is extensive evidence that human activity such as agriculture and industry results in inadvertent weather modification:
The effects of inadvertent weather modification may pose serious threats to many aspects of civilization, including ecosystems, natural resources, food and fiber production, economic development, and human health.
Microscale meteorology.
Microscale meteorology is the study of short-lived atmospheric phenomena smaller than mesoscale, about 1 km or less. These two branches of meteorology are sometimes grouped together as "mesoscale and microscale meteorology" (MMM) and together study all phenomena smaller than synoptic scale; that is they study features generally too small to be depicted on a weather map. These include small and generally fleeting cloud "puffs" and other small cloud features.
Extremes on Earth.
On Earth, temperatures usually range ±40 °C (100 °F to −40 °F) annually. The range of climates and latitudes across the planet can offer extremes of temperature outside this range. The coldest air temperature ever recorded on Earth is -89.2 C, at Vostok Station, Antarctica on 21 July 1983. The hottest air temperature ever recorded was 57.7 C at 'Aziziya, Libya, on 13 September 1922, but that reading is queried. The highest recorded average annual temperature was 34.4 C at Dallol, Ethiopia. The coldest recorded average annual temperature was -55.1 C at Vostok Station, Antarctica.
The coldest average annual temperature in a permanently inhabited location is at Eureka, Nunavut, in Canada, where the annual average temperature is -19.7 C.
Extraterrestrial within the Solar System.
Studying how the weather works on other planets has been seen as helpful in understanding how it works on Earth. Weather on other planets follows many of the same physical principles as weather on Earth, but occurs on different scales and in atmospheres having different chemical composition. The Cassini–Huygens mission to Titan discovered clouds formed from methane or ethane which deposit rain composed of liquid methane and other organic compounds. Earth's atmosphere includes six latitudinal circulation zones, three in each hemisphere. In contrast, Jupiter's banded appearance shows many such zones, Titan has a single jet stream near the 50th parallel north latitude, and Venus has a single jet near the equator.
One of the most famous landmarks in the Solar System, Jupiter's "Great Red Spot", is an anticyclonic storm known to have existed for at least 300 years. On other gas giants, the lack of a surface allows the wind to reach enormous speeds: gusts of up to 600 metres per second (about 2100 km/h) have been measured on the planet Neptune. This has created a puzzle for planetary scientists. The weather is ultimately created by solar energy and the amount of energy received by Neptune is only about 1⁄900 of that received by Earth, yet the intensity of weather phenomena on Neptune is far greater than on Earth. The strongest planetary winds discovered so far are on the extrasolar planet HD 189733 b, which is thought to have easterly winds moving at more than 9600 km/h.
Space weather.
Weather is not limited to planetary bodies. Like all stars, the sun's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System. The movement of mass ejected from the Sun is known as the solar wind. Inconsistencies in this wind and larger events on the surface of the star, such as coronal mass ejections, form a system that has features analogous to conventional weather systems (such as pressure and wind) and is generally known as space weather. Coronal mass ejections have been tracked as far out in the solar system as Saturn. The activity of this system can affect planetary atmospheres and occasionally surfaces. The interaction of the solar wind with the terrestrial atmosphere can produce spectacular aurorae, and can play havoc with electrically sensitive systems such as electricity grids and radio signals.

</doc>
<doc id="33979" url="http://en.wikipedia.org/wiki?curid=33979" title="Wemic">
Wemic

In the "Dungeons & Dragons" fantasy roleplaying game, the wemic is a fantastical hybrid creature with the upper body of a human and the lower body of a lion. Like centaurs, they are considered "tauric" creatures.
Publication history.
David C. Sutherland III created the modern-day wemic for a game product called "Monster Cards Set 3", a first edition "Advanced Dungeons and Dragons" supplement released in 1982; with the artwork for the original depiction being undertaken by Jim Roslof. The wemic then appeared in first edition in the original "Monster Manual II" (1983).
The wemic appeared in second edition for the Forgotten Realms setting in the "Monstrous Compendium Forgotten Realms Appendix" (1989), and reprinted in the "Monstrous Manual" (1993). The wemic was further detailed in "Dragon" #157 (May 1990). The wemic is presented as a player character race in "The Complete Book of Humanoids" (1993), and is later presented as a playable character race again in ' (1995).
The wemic appeared in third edition for the Forgotten Realms setting in "Monsters of Faerûn" (2001), and as a player character race in "Races of Faerûn" (2003).
Description.
In the game, the wemic is described as larger and stronger than humans; a wemic can leap up to 30 feet with a running start. Their front claws are sharp, and they can fight with both claws and weapons at the same time. The leonine body is described as covered with dusky golden fur, while the underbelly fur is short and white. Males are generally depicted with long mane-like hair.
Society.
In the Dungeons and Dragons game, Wemics are excellent hunters and fighters. Descriptions in game books state that they do not make settled homes, but generally follow the herds they hunt for food. Some have compared them with the aboriginal people of the central plains of North America. A fictional, nomadic, stone-age folk, Wemics are often represented as barbaric, illiterate, and uncivilized; they are described as famous for being highly superstitious. Others would describe Wemics as nature-oriented people with a rich tradition of oral history; they live close to the earth and are in tune with its magical forces.
In the context of the game, when Wemic must be still for a time telling stories around a fire, pausing for a meal, waiting for a friend, or just to take a brief rest, the Wemic commonly assumes a posture in which his hindquarters rest on the ground as his front legs remain straight and his forepaws stay flat on the earth. The game refers to this as sitting, distinct from sprawling (both hind and forequarters on the ground, but with torso upright) or laying down.
Subspecies.
Wemics entered the 3rd edition of "Dungeons & Dragons" in the accessory "Monsters of Faerûn", which also introduced the mountain wemic: essentially the same build, but the lion portions replaced by those of a large cougar. Mountain wemics are slightly smaller than common wemics and generally solitary.
Wemics in the Forgotten Realms.
Wemics appear in the Forgotten Realms campaign setting. In the game, they inhabit the vast, imaginary, african-savanna-like grassland known as the shaar, waging war with the Loxos, Thri-kreen and Centaurs which also dwell there.
Wemics in other systems.
Wemics are a player race in Maelstrom, a UK based live action roleplaying game. However, in this system they appear as highly cultured, bipedal felines, rather than quadrupeds.
Wemics in the Maelstom game-world are divided into two main cultural groups - those from Amun-Sa and those from Tritoni, although there are also populations in the Free Islands, Gerosos and Lyzanium. This said, individuals or small prides of Wemics are still found all over the Known World.

</doc>
<doc id="33980" url="http://en.wikipedia.org/wiki?curid=33980" title="Waterfall model">
Waterfall model

The waterfall model is a sequential design process, used in software development processes, in which progress is seen as flowing steadily downwards (like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, production/implementation and maintenance.
The waterfall development model originates in the manufacturing and construction industries: highly structured physical environments in which after-the-fact changes are prohibitively costly, if not impossible. Since no formal software development methodologies existed at the time, this hardware-oriented model was simply adapted for software development.
History.
The first known presentation describing use of similar phases in software engineering was held by Herbert D. Benington at Symposium on advanced programming methods for digital computers on 29 June 1956. This presentation was about the development of software for SAGE. In 1983 the paper was republished with a foreword by Benington pointing out that the process was not in fact performed in a strict top-down fashion, but depended on a prototype.
The first formal description of the waterfall model is often cited as a 1970 article by Winston W. Royce, although Royce did not use the term "waterfall" in that article. Royce presented this model as an example of a flawed, non-working model; which is how the term is generally used in writing about software development—to describe a critical view of a commonly used software development practice.
The earliest use of the term "waterfall" may have been a 1976 paper by Bell and Thayer.
In 1985, the United States Department of Defense captured this approach in DOD-STD-2167A, their standards for working with software development contractors, which stated that "the contractor shall implement a software development cycle that includes the following six phases: Preliminary Design, Detailed Design, Coding and Unit Testing, Integration, and Testing".
Model.
In Royce's original waterfall model, the following phases are followed in order:
Thus the waterfall model maintains that one should move to a phase only when its preceding phase is reviewed and verified. 
Various modified waterfall models (including Royce's final model), however, can include slight or major variations on this process. These variations included returning to the previous cycle after flaws were found downstream, or returning all the way to the design phase if downstream phases deemed insufficient.
Supporting arguments.
Time spent early in the software production cycle can avoid costs at later stages, for example a problem found in the early stages (such as requirements specification) is cheaper to fix than the same bug found later on in the process (by a factor of 50 to 200).
In common practice waterfall methodologies result in a project schedule with 20–40% of the time invested for the first two phases, 30–40% of the time to coding, and the rest dedicated to testing and implementation. The actual project organization needs to be highly structured. Most medium and large projects will include a detailed set of procedures and controls, which regulate every process on the project.
A further argument for the waterfall model is that it places emphasis on documentation (such as requirements documents and design documents) as well as source code. In less thoroughly designed and documented methodologies, knowledge is lost if team members leave before the project is completed, and it may be difficult for a project to recover from the loss. If a fully working design document is present (as is the intent of Big Design Up Front and the waterfall model), new team members or even entirely new teams should be able to familiarize themselves by reading the documents.
The waterfall model provides a structured approach; the model itself progresses linearly through discrete, easily understandable and explainable phases and thus is easy to understand; it also provides easily identifiable milestones in the development process. It is perhaps for this reason that the waterfall model is used as a beginning example of a development model in many software engineering texts and courses.
It is argued that the waterfall model can be suited to projects where requirements and scope are fixed, the product itself is firm and stable, and the technology is clearly understood.
Criticism.
Clients may not know exactly what their requirements are before they see working software and so change their requirements, leading to redesign, redevelopment, and retesting, and increased costs.
Designers may not be aware of future difficulties when designing a new software product or feature; in which case, it is better to revise the design than persist in a design that does not account for any newly discovered constraints, requirements, or problems.
In response to the perceived problems with the "pure" waterfall model, modified waterfall models were introduced, such as "Sashimi (Waterfall with Overlapping Phases), Waterfall with Subprojects, and Waterfall with Risk Reduction".
Some organizations, such as the United States Department of Defense, now have a stated preference against waterfall type methodologies, starting with MIL-STD-498, which encourages "evolutionary acquisition" and "Iterative and Incremental Development".
While advocates of agile software development argue the waterfall model is an ineffective process for developing software, some sceptics suggest that the waterfall model is a false argument used purely to market "alternative" development methodologies.

</doc>
<doc id="33983" url="http://en.wikipedia.org/wiki?curid=33983" title="Wau Holland">
Wau Holland

Herwart Holland-Moritz, known as Wau Holland, (20 December 1951 – 29 July 2001) cofounded the Chaos Computer Club (CCC) in 1981, one of the world's oldest hacking clubs. The CCC became world famous when its members exposed security flaws in Germany's "Bildschirmtext" (Btx) online television service by getting a bank to send them DM 134,000 for accessing its Btx page many times. They returned the money the following day.
Holland also co-founded the CCC's hacker magazine "Datenschleuder" in 1984, which praised the possibilities of global information networks and powerful computers, and included detailed wiring diagrams for building your own modems cheaply. The then-monopolist phone company of Germany's Deutsche Bundespost had to approve modems and sold expensive, slow modems of their own. The telecommunications branch of Deutsche Bundespost was privatized and is now Deutsche Telekom.
Because of Holland's continuing participation in the club, the CCC gained popularity and credibility. He gave speeches on information control for the government and the private sector. Holland fought against copy protection and all forms of censorship and for an open information infrastructure. He compared the censorship demands by some governments to those of the Christian church in the Middle Ages and regarded copy protection as a product defect. In his last years, he spent a lot of his time in a youth center teaching children both the ethics and the science of hacking, with unique style and intelligent humor.
Holland was an amateur radio operator and held the callsign DB4FA.
Holland died in Bielefeld on 29 July 2001 of complications caused by a brain stem stroke from which he suffered in May.

</doc>
<doc id="33984" url="http://en.wikipedia.org/wiki?curid=33984" title="Weather Underground">
Weather Underground

The Weather Underground Organization (WUO), commonly known as the Weather Underground, was an American radical left-wing organization founded on the Ann Arbor campus of the University of Michigan. Originally called Weatherman, the group became known colloquially as the Weathermen. Weatherman organized in 1969 as a faction of Students for a Democratic Society (SDS) composed for the most part of the national office leadership of SDS and their supporters. Their goal was to create a clandestine revolutionary party for the overthrow of the U.S. government.
With revolutionary positions characterized by black power and opposition to the Vietnam War, the group conducted a campaign of bombings through the mid-1970s and took part in actions such as the jailbreak of Dr. Timothy Leary. The "Days of Rage", their first public demonstration on October 8, 1969, was a riot in Chicago timed to coincide with the trial of the Chicago Seven. In 1970 the group issued a "Declaration of a State of War" against the United States government, under the name "Weather Underground Organization".
The bombing campaign mostly targeted government buildings, along with several banks. Most were preceded by evacuation warnings, along with communiqués identifying the particular matter that the attack was intended to protest. No people were killed in any of their acts of property destruction, although three members of the group were killed in the Greenwich Village townhouse explosion. Former members of the group robbed a Brink's armored car in 1981, which resulted in the deaths of three people including Waverly Brown, the first black police officer on the Nyack police force.
For the bombing of the United States Capitol on March 1, 1971, they issued a communiqué saying that it was "in protest of the U.S. invasion of Laos". For the bombing of the Pentagon on May 19, 1972, they stated that it was "in retaliation for the U.S. bombing raid in Hanoi". For the January 29, 1975 bombing of the United States Department of State building, they stated that it was "in response to the escalation in Vietnam".
The Weathermen grew out of the Revolutionary Youth Movement (RYM) faction of SDS. It took its name from Bob Dylan's lyric, "You don't need a weatherman to know which way the wind blows", from the song "Subterranean Homesick Blues" (1965). "You don't need a weatherman to know which way the wind blows", was the title of a position paper that they distributed at an SDS convention in Chicago on June 18, 1969. This founding document called for a "white fighting force" to be allied with the "Black Liberation Movement" and other radical movements to achieve "the destruction of U.S. imperialism and achieve a classless world: world communism".
The Weathermen began to disintegrate after the United States reached a peace accord in Vietnam in 1973, after which the New Left declined in influence. By 1977, the organization was defunct.
Background and formation.
The Weathermen emerged from the campus-based opposition to the Vietnam War, and from the Civil Rights Movements of the late 1960s. During this time, United States military action in Southeast Asia, especially in Vietnam, escalated. In the U.S., the anti-war sentiment was particularly pronounced during the 1968 U.S. presidential election.
The origins of the Weathermen can be traced to the collapse and fragmentation of the Students for a Democratic Society following a split between office holders of SDS, or "National Office", and their supporters and the Progressive Labor Party (PLP). During the factional struggle National Office leaders such as Bernardine Dohrn and Mike Klonsky began announcing their emerging perspectives, and Klonsky published a document titled "Toward a Revolutionary Youth Movement" (RYM).
RYM promoted the philosophy that young workers possessed the potential to be a revolutionary force to overthrow capitalism, if not by themselves then by transmitting radical ideas to the working class. Klonsky's document reflected the philosophy of the National Office and was eventually adopted as official SDS doctrine. During the summer of 1969, the National Office began to split. A group led by Klonsky became known as RYM II, and the other side, RYM I, was led by Dohrn and endorsed more aggressive tactics such as direct action, as some members felt that years of non-violent resistance had done little or nothing to stop the Vietnam War. The Weathermen strongly sympathized with the radical Black Panthers. The police killing of Panther Fred Hampton prompted the Weatherman to issue a declaration of war upon the United States government.
We petitioned, we demonstrated, we sat in. I was willing to get hit over the head, I did; I was willing to go to prison, I did. To me, it was a question of what had to be done to stop the much greater violence that was going on.—David Gilbert
SDS Convention, June 1969.
At an SDS convention in Chicago on June 18, 1969, the National Office attempted to persuade unaffiliated delegates not to endorse a takeover of SDS by Progressive Labor who had packed the convention with their supporters. At the beginning of the convention, two position papers were passed out by the National Office leadership, one a revised statement of Klonksy's RYM manifesto, the other called "You Don't Need a Weatherman to Know Which Way the Wind Blows".
The latter document outlined the position of the group that would become the Weathermen. It had been signed by Karen Ashley, Bill Ayers, Bernardine Dohrn, John Jacobs, Jeff Jones, Gerry Long, Howie Machtinger, Jim Mellen, Terry Robbins, Mark Rudd, and Steve Tappis. The document called for creating a clandestine revolutionary party.
The most important task for us toward making the revolution, and the work our collectives should engage in, is the creation of a mass revolutionary movement, without which a clandestine revolutionary party will be impossible. A revolutionary mass movement is different from the traditional revisionist mass base of "sympathizers". Rather it is akin to the Red Guard in China, based on the full participation and involvement of masses of people in the practice of making revolution; a movement with a full willingness to participate in the violent and illegal struggle.
At this convention the Weatherman faction of the Students for a Democratic Society, planned for October 8–11, as a "National Action" built around John Jacobs' slogan, "bring the war home." The National Action grew out of a resolution drafted by Jacobs and introduced at the October 1968 SDS National Council meeting in Boulder, Colorado. The resolution, titled "The Elections Don't Mean Shit—Vote Where the Power Is—Our Power Is In The Street" and adopted by the council, was prompted by the success of the Democratic National Convention protests in August 1968 and reflected Jacobs' strong advocacy of direct action.
As part of the "National Action Staff," Jacobs was an integral part of the planning for what quickly came to be called "Four Days of Rage." For Jacobs, the goal of the "Days of Rage" was clear:
Weatherman would shove the war down their dumb, fascist throats and show them, while we were at it, how much better we were than them, both tactically and strategically, as a people. In an all-out civil war over Vietnam and other fascist U.S. imperialism, we were going to bring the war home. 'Turn the imperialists' war into a civil war', in Lenin's words. And we were going to kick ass.
In July 1969, 30 members of Weatherman leadership traveled to Cuba and met with North Vietnamese representatives to gain from their revolutionary experience. The North Vietnamese requested armed political action in order to stop the U.S. government's war in Vietnam. Subsequently, they accepted funding, training, recommendations on tactics and slogans from Cuba, and perhaps explosives as well.
SDS Convention, December 1969.
After the Days of Rage riots the Weatherman held the last of its National Council meetings from December 26 to December 31, 1969 in Flint, Michigan. The meeting, dubbed the "War Council" by the 300 people who attended, adopted Jacobs' call for violent revolution. Dohrn opened the conference by telling the delegates they needed to stop being afraid and begin the "armed struggle." Over the next five days, the participants met in informal groups to discuss what "going underground" meant, how best to organize collectives, and justifications for violence. In the evening, the groups reconvened for a mass "wargasm"—practicing karate, engaging in physical exercise, singing songs, and listening to speeches.
The War Council ended with a major speech by John Jacobs. Jacobs condemned the "pacifism" of white middle-class American youth, a belief which he claimed they held because they were insulated from the violence which afflicted blacks and the poor. He predicted a successful revolution, and declared that youth were moving away from passivity and apathy and toward a new high-energy culture of "repersonalization" brought about by drugs, sex, and armed revolution. "We're against everything that's 'good and decent' in honky America," Jacobs said in his most commonly quoted statement. "We will burn and loot and destroy. We are the incubation of your mother's nightmare."
Two major decisions came out of the War Council. The first was to go underground, and to begin a violent, armed struggle against the state without attempting to organize or mobilize a broad swath of the public. The Weather Underground hoped to create underground collectives in major cities throughout the country. In fact, the Weathermen eventually created only three significant, active collectives; one in California, one in the Midwest, and one in New York City. The New York City collective was led by Jacobs and Terry Robbins, and included Ted Gold, Kathy Boudin, Cathy Wilkerson (Robbins' girlfriend), and Diana Oughton. Jacobs was one of Robbins' biggest supporters, and pushed Weatherman to let Robbins be as violent as he wanted to be. The Weatherman national leadership agreed, as did the New York City collective. The collective's first target was Judge John Murtagh, who was overseeing the trial of the "Panther 21".
The second major decision was the dissolution of SDS. After the summer of 1969 fragmentation of SDS, Weatherman's adherents explicitly claimed themselves the "real leaders" of SDS and retained control of the SDS National Office. Thereafter, any leaflet, label, or logo bearing the name "Students for a Democratic Society" (SDS) was in fact the views and politics of Weatherman, not of the slate elected by Progressive Labor. Weatherman contained the vast majority of former SDS National Committee members, including Mark Rudd, David Gilbert and Bernardine Dohrn. The group, while small, was able to commandeer the mantle of SDS and all of its membership lists, but with Weatherman in charge there was little or no support from local branches or members of the organization, and local chapters soon disbanded. At the War Council, the Weathermen had decided to close the SDS National Office, ending the major campus-based organization of the 1960s which at its peak was a mass organization with 100,000 members.
Ideology.
The thesis of Weatherman theory, as expounded in its founding document, "You Don't Need a Weatherman to Know Which Way the Wind Blows", was that "the main struggle going on in the world today is between U.S. imperialism and the national liberation struggles against it", based on Lenin's theory of imperialism, first expounded in 1916 in "Imperialism, the Highest Stage of Capitalism". In Weatherman theory "oppressed peoples" are the creators of the wealth of empire, "and it is to them that it belongs." "The goal of revolutionary struggle must be the control and use of this wealth in the interest of the oppressed peoples of the world." "The goal is the destruction of US imperialism and the achievement of a classless world: world communism"
The Vietnamese and other third world countries, as well as third world people within the United States play a vanguard role. They "set the terms for class struggle in America..." The role of the "Revolutionary Youth Movement" is to build a centralized organization of revolutionaries, a "Marxist-Leninist Party" supported by a mass revolutionary movement to support international liberation movements and "open another battlefield of the revolution."
The theoretical basis of the Revolutionary Youth Movement was an insight that most of the American population, including both students and the supposed "middle class," comprised, due to their relationship to the instruments of production, the working class, thus the organizational basis of the SDS, which had begun in the elite colleges and had been extended to public institutions as the organization grew could be extended to youth as a whole including students, those serving in the military, and the unemployed. Students could be viewed as workers gaining skills prior to employment. This contrasted to the Progressive Labor view which viewed students and workers as being in separate categories which could ally, but should not jointly organize.
FBI analysis of the travel history of the founders and initial followers of the organization emphasized contacts with foreign governments, particularly the Cuban and North Vietnamese and their influence on the ideology of the organization. Participation in the Venceremos Brigade, a program which involved US students volunteering to work in the sugar harvest in Cuba, is highlighted as a common factor in the background of the founders of the Weather Underground, with China a secondary influence. This experience was cited by both Kathy Boudin and Bernardine Dohrn as a major influence on their political development.
Terry Robbins took the organization's name from the lyrics of the Bob Dylan song, “Subterranean Homesick Blues,” which featured the lyrics “You don’t need a weatherman to know which way the wind blows.” The lyrics had been quoted at the bottom of an influential essay in the SDS newspaper, "New Left Notes". By using this title the Weathermen meant, partially, to appeal to the segment of US youth inspired to action for social justice by Dylan’s songs.
The Weatherman group had long held that militancy was becoming more important than nonviolent forms of anti-war action, and that university-campus-based demonstrations needed to be punctuated with more dramatic actions, which had the potential to interfere with the US military and internal security apparatus. The belief was that these types of urban guerrilla actions would act as a catalyst for the coming revolution. Many international events indeed seemed to support the Weathermen’s overall assertion that worldwide revolution was imminent, such as the tumultuous Cultural Revolution in China; the 1968 student revolts in France, Mexico City and elsewhere; the Prague Spring; the Northern Ireland Civil Rights Association; the emergence of the Tupamaros organization in Uruguay; the emergence of the Guinea-Bissauan Revolution and similar Marxist-led independence movements throughout Africa; and within the United States, the prominence of the Black Panther Party together with a series of “ghetto rebellions” throughout poor black neighborhoods across the country.
We felt that doing nothing in a period of repressive violence is itself a form of violence. That's really the part that I think is the hardest for people to understand. If you sit in your house, live your white life and go to your white job, and allow the country that you live in to murder people and to commit genocide, and you sit there and you don't do anything about it, that's violence.—Naomi Jaffe
The Weathermen were outspoken advocates of the critical concepts that later came to be known as “white privilege” (described as white-skin privilege) and identity politics. As the unrest in poor black neighborhoods intensified in the early 1970s, Bernardine Dohrn said, “White youth must choose sides "now." They must either fight on the side of the oppressed, or be on the side of the oppressor.”
Anti-imperialism, anti-racism, and white-skin privilege.
Weather maintained that their stance differed from the rest of the movement at the time in the sense that they predicated their critiques on the notion that they were engaged in "an anti-imperialist, anti-racist struggle". Weather put the "international" proletariat at the center of their political theory. Weather warned that other political theories, including those organizing around class interests or youth interests, were "bound to lead in a racist and chauvinist direction". Weather denounced other political theories of the time as "objectively racist" if they did not side with the international proletariat; such political theories, they argued, needed to be "smashed".
Members of Weather further contended that efforts at "organizing whites against their own perceived oppression" were "attempts by whites to carve out even more privilege than they already derive from the imperialist nexus". Weather's political theory sought to make every struggle an anti-imperialist, anti-racist struggle; out of this premise came their interrogation of critical concepts that would later be known as "white privilege". As historian Dan Berger writes, Weather raised the question "what does it means to be a white person opposing racism and imperialism?"
Practice.
Shortly after its formation as an independent group, Weatherman created a central committee, the Weather Bureau, which assigned its cadres to a series of collectives in major cities. These cities included New York, Boston, Seattle, Philadelphia, Cincinnati, Buffalo, and Chicago, the home of the SDS' head office. The collectives set up under the Weather Bureau drew their design from Che Guevara's "foco" theory, which focused on the building of small, semi-autonomous cells guided by a central leadership.
Members of collectives engaged in intensive criticism sessions which attempted to reconcile their prior and current activities and political positions to Weatherman doctrine. Monogamy and other exclusive sexual relationships came under attack, bisexuality was encouraged. Martial arts were practiced and occasional direct actions were engaged in. This formation continued during 1969 and 1970 until the group went underground and a more relaxed lifestyle was adopted as the group blended into the counterculture.
Recruitment.
Weather used various means by which to recruit new members and set into motion a nation-wide revolt against the government. Weather members aimed to mobilize people into action against the established leaders of the nation and the patterns of injustice which existed in America and abroad due to America's presence overseas. They also aimed to convince people to resist reliance upon their given privilege and to rebel and take arms if necessary. According to Weatherman, if people tolerated the unjust actions of the state, they became complicit in those actions. In the manifesto compiled by Bill Ayers, Bernardine Dohrn, Jeff Jones, and Celia Sojourn, entitled "Prairie Fire: The Politics of Revolutionary Anti-Imperialism," Weatherman explained that their intention was to encourage the people and provoke leaps in confidence and consciousness in an attempt to stir the imagination, organize the masses, and join in the people's day-to-day struggles in every way possible.
In the year 1960, over a third of America’s population was under 18 years of age. The number of young citizens set the stage for a widespread revolt against perceived structures of racism, sexism, and classism, the violence of the Vietnam War and America’s interventions abroad. At college campuses throughout the country, anger against “the Establishment’s” practices prompted both peaceful and violent protest.
The members of Weatherman targeted high school and college students, assuming they would be willing to rebel against the authoritative figures who had oppressed them, including cops, principals, and bosses. Weather aimed to develop roots within the class struggle, targeting white working-class youths. The younger members of the working class became the focus of the organizing effort because they felt the oppression strongly in regards to the military draft, low-wage jobs, and schooling.
Schools became a common place of recruitment for the movement. In direct actions, dubbed Jailbreaks, Weather members invaded educational institutions as a means by which to recruit high school and college students. The motivation of these jailbreaks was the organization's belief that school was where the youth were oppressed by the system and where they learned to tolerate society’s faults instead of rise against them. According to “Prairie Fire”, young people are channeled, coerced, misled, miseducated, misused in the school setting. It is in schools that the youth of the nation become alienated from the authentic processes of learning about the world 
Factions of the Weatherman organization began recruiting members by applying their own strategies. Women's groups such as The Motor City Nine and Cell 16 took the lead in various recruitment efforts. Roxanne Dunbar-Ortiz, a member of the radical women's liberation group, Cell 16, spoke about her personal recruitment agenda saying that she wanted their group to go out in every corner of the country and tell women the truth, recruit the local people, poor and working-class people, in order to build a new society 
Berger explains the controversy surrounding recruitment strategies saying, “As an organizing strategy it was less than successful: white working class youths were more alienated than organized by Weather's spectacles, and even some of those interested in the group were turned off by its early hi-jinks”
The methods of recruitment applied by the Weathermen met controversy as their call to arms became intensely radical and their organization's leadership increasingly exclusive.
Armed propaganda.
In 2006 Dan Berger (writer, activist, and longtime anti-racism organizer) states that following their initial set of bombings, which resulted in the Greenwich Village townhouse explosion, the organization adopted a new paradigm of direct action set forth in the communiqué "New Morning, Changing Weather", which abjured attacks on people. The shift in the organization's outlook was in good part due to the 1970 death of Weatherman Terry Robbins, Diana Oughton and Ted Gold, all graduate students, in the Greenwich Village townhouse explosion. Terry Robbins was renowned among the organization members for his radicalism and belief in violence as effective action.
According to Dan Berger a relatively sophisticated program of armed propaganda was adopted. This consisted of a series of bombings of government and corporate targets in retaliation for specific imperialist and oppressive acts. Small, well-constructed time bombs were used, generally in vents in restrooms, which exploded at times the spaces were empty. Timely warnings were made and communiqués issued explaining the reason for the actions.
Major and suspected activities.
Haymarket Police Memorial bombing.
Shortly before the Days of Rage demonstrations on October 6, 1969, the Weatherman planted a bomb that blew up a statue in Chicago built to commemorate police casualties incurred in the 1886 Haymarket Riot. The blast broke nearly 100 windows and scattered pieces of the statue onto the Kennedy Expressway below. The statue was rebuilt and unveiled on May 4, 1970 (coincidentally, the same day as the Kent State massacre), only to be blown up by the Weathermen a second time on October 6, 1970. The statue was rebuilt once again and Mayor Richard J. Daley posted a 24-hour police guard to protect it, however the statue was later destroyed again a third time. The monument was rebuilt and is located at Chicago Police Headquarters.
"Days of Rage".
One of the first acts of the Weathermen after splitting from SDS was to announce they would hold the "Days of Rage" that autumn. This was advertised to "Bring the war home!" Hoping to cause sufficient chaos to "wake" the American public out of what they saw as complacency toward the role of the US in the Vietnam War, the Weathermen meant it to be the largest protest of the decade. They had been told by their regional cadre to expect thousands to attend; however, when they arrived they found only a few hundred people.
According to Bill Ayers in 2003, "The Days of Rage was an attempt to break from the norms of kind of acceptable theatre of 'here are the anti-war people: containable, marginal, predictable, and here's the little path they're going to march down, and here's where they can make their little statement.' We wanted to say, "No, what we're going to do is whatever we had to do to stop the violence in Vietnam.'" The protests did not meet Ayers' stated expectations.
Though the October 8, 1969 rally in Chicago had failed to draw as many as the Weathermen had anticipated, the two or three hundred who did attend shocked police by rioting through the affluent Gold Coast neighborhood. They smashed the windows of a bank and those of many cars. The crowd ran four blocks before encountering police barricades. They charged the police but broke into small groups; more than 1,000 police counter-attacked. Many protesters were wearing motorcycle or football helmets, but the police were well trained and armed. Large amounts of tear gas were used, and at least twice police ran squad cars into the mob. The rioting lasted approximately half an hour, during which 28 policemen were injured. Six Weathermen were shot by the police and an unknown number injured; 68 rioters were arrested.
For the next two days, the Weathermen held no rallies or protests. Supporters of the RYM II movement, led by Klonsky and Noel Ignatin, held peaceful rallies in front of the federal courthouse, an International Harvester factory, and Cook County Hospital. The largest event of the Days of Rage took place on Friday, October 9, when RYM II led an interracial march of 2,000 people through a Spanish-speaking part of Chicago.
On October 10, the Weatherman attempted to regroup and resume their demonstrations. About 300 protesters marched through The Loop, Chicago's main business district, watched by a double-line of heavily armed police. The protesters suddenly broke through the police lines and rampaged through the Loop, smashing the windows of cars and stores. The police were prepared, and quickly isolated the rioters. Within 15 minutes, more than half the crowd had been arrested.
The Days of Rage cost Chicago and the state of Illinois approximately $183,000 ($100,000 for National Guard expenses, $35,000 in damages, and $20,000 for one injured citizen's medical expenses). Most of the Weathermen and SDS leaders were now in jail, and the Weathermen would have to pay over $243,000 for their bail.
Flint War Council.
The Flint War Council was a series of meetings of the Weather Underground Organization and associates in Flint, Michigan, that took place from 27–31 December 1969. During these meetings, the decisions were made for the Weather Underground Organization to go underground and to "engage in guerilla warfare against the U.S. government." This decision was made in response to increased pressure from law enforcement, and a belief that underground guerilla warfare was the best way to combat the U.S. government.
During a closed-door meeting of the Weather Underground's leadership, the decision was also taken to abolish Students for a Democratic Society. This decision reflected the splintering of SDS into hostile rival factions.
Park Precinct Police Station bombing.
On February 16, 1970 a nail bomb placed on a window ledge of the Park Police substation in the Upper Haight neighborhood of San Francisco exploded at 10:45 p.m. The blast killed police Sergeant Brian McDonnell. Law enforcement suspected the Weather Underground but was unable to prove conclusively that the organization was involved. A second officer, Robert Fogarty, was partially blinded by the bomb’s shrapnel. Secret federal grand juries were convened in 2001 and again in 2009 to re-open the Park Precinct cold case in an attempt to again tie WUO members Billy Ayers, Bernadine Dohrn, Howie Machtinger and others to the deadly bombing. Ultimately, it was concluded that members of the Black Liberation Army, whom WUO members affiliated with while underground, were responsible for not only this action but also the bombing of another police precinct in San Francisco as well as bombing the Catholic Church funeral services of the police officer killed in the Park Precinct bombing in the early summer of 1970.
New York City arson attacks.
On February 21, 1970, at around 4:30 a.m., three gasoline-filled Molotov cocktails exploded in front of the home of New York Supreme Court Justice John M. Murtagh, who was presiding over the pretrial hearings of the so-called "Panther 21" members of the Black Panther Party over a plot to bomb New York landmarks and department stores. Justice Murtagh and his family were unharmed, but two panes of a front window were shattered, an overhanging wooden eave was scorched, and the paint on a car in the garage was charred. "Free the Panther 21" and "Viet Cong have won" were written in large red letters on the sidewalk in front of the judge's house at 529 W. 217th Street in the Inwood neighborhood of Manhattan. The judge's house had been under hourly police surveillance and an unidentified woman called the police a few minutes before the explosions to report several prowlers there, which resulted in a police car being sent immediately to the scene.
In the preceding hours, Molotov cocktails had been thrown at the second floor of Columbia University's International Law Library at 434 W. 116th Street and at a police car parked across the street from the Charles Street police station in the West Village in Manhattan, and at Army and Navy recruiting booths on Norstrand Avenue on the eastern fringe of the Brooklyn College campus in Brooklyn, causing no or minimal damage in incidents of unknown relation to that at Judge Murtagh's home.
According to the December 6, 1970 "New Morning—Changing Weather" Weather Underground communiqué signed by Bernardine Dohrn, and Cathy Wilkerson's 2007 memoir, the fire-bombing of Judge Murtagh's home was carried out by four members of the New York cell that was decimated two weeks later by the March 6, 1970 townhouse explosion.
Greenwich Village townhouse explosion.
On March 6, 1970, during preparations for the bombing of a Non-Commissioned Officers’ (NCO) dance at the Fort Dix U.S. Army base and for Butler Library at Columbia University, there was an explosion in a Greenwich Village safe house when the nail bomb being constructed prematurely detonated for unknown reasons. WUO members Diana Oughton, Ted Gold, and Terry Robbins died in the explosion. Cathy Wilkerson and Kathy Boudin escaped unharmed. The site of the Village explosion was the former residence of Merrill Lynch brokerage firm co-founder Charles Merrill and the childhood home of his son, poet James Merrill; the younger Merrill subsequently memorialized the event in his poem "18 West 11th Street", the title being the address of the brownstone townhouse. Another writer, Matthew Landy Steen, composed a widely reprinted poem "How Does It Feel To Be Inside An Explosion?"
An FBI report later stated that the group had possessed enough explosives to "level ... both sides of the street". Dustin Hoffman was seen wandering the street afterwards; he lived in the townhouse next door.
The bomb preparations have been pointed out by critics of the claim that the Weatherman group did not try to take lives with its bombings. Harvey Klehr, the Andrew W. Mellon professor of politics and history at Emory University in Atlanta, said in 2003, "The only reason they were not guilty of mass murder is mere incompetence. I don't know what sort of defense that is."
Underground strategy change.
After the Greenwich Village townhouse explosion, per the December 1969 Flint war council decisions the group was now well underground, and began to refer to themselves as the Weather Underground Organization. At this juncture, WUO shrank considerably, becoming even fewer than they had been when first formed. The group was devastated by the loss of their friends, and in late April 1970, members of the Weathermen met in California to discuss what had happened in New York and the future of the organization. The group decided to reevaluate their strategy, particularly regarding their initial belief in the acceptability of human casualties, and rejected such tactics as kidnapping and assassinations.
In 2003, Weather Underground members stated in interviews that they wanted to convince the American public that the United States was truly responsible for the calamity in Vietnam. The group began striking at night, bombing empty offices, with warnings always issued in advance to ensure a safe evacuation. According to David Gilbert, who took part in the 1981 Brink's robbery that killed 2 police officers and a Brinks' guard, and was jailed for murder, "[their] goal was to not hurt any people, and a lot of work went into that. But we wanted to pick targets that showed to the public who was responsible for what was really going on." After the Greenwich Village explosion, in a review of the documentary film "The Weather Underground" (2002), a "Guardian" journalist restated the film's contention that no one was killed by WUO bombs.
We were very careful from the moment of the townhouse on to be sure we weren't going to hurt anybody, and we never did hurt anybody. Whenever we put a bomb in a public space, we had figured out all kinds of ways to put checks and balances on the thing and also to get people away from it, and we were remarkably successful.—Bill Ayers, 2003
Declaration of war.
In response to the death of Black Panther members Fred Hampton and Mark Clark in December, 1969 during a police raid, on May 21, 1970 the Weather Underground issued a "" against the United States government, using for the first time its new name, the "Weather Underground Organization" (WUO), adopting fake identities, and pursuing covert activities only. These initially included preparations for a bombing of a U.S. military non-commissioned officers' dance at Fort Dix, New Jersey in what Brian Flanagan said had been intended to be "the most horrific hit the United States government had ever suffered on its territory".
We've known that our job is to lead white kids into armed revolution. We never intended to spend the next five to twenty-five years of our lives in jail. Ever since SDS became revolutionary, we've been trying to show how it is possible to overcome frustration and impotence that comes from trying to reform this system. Kids know the lines are drawn: revolution is touching all of our lives. Tens of thousands have learned that protest and marches don't do it. Revolutionary violence is the only way.—Bernardine Dohrn
Bernardine Dohrn subsequently stated that it was Fred Hampton's death that prompted the Weather Underground to declare war on the US government.
We felt that the murder of Fred required us to be more grave, more serious, more determined to raise the stakes and not just be the white people who wrung their hands when black people were being murdered.—Bernardine Dohrn
In December 1969, the Chicago Police Department, in conjunction with the FBI, conducted a raid on the home of Black Panther Fred Hampton, in which he and Mark Clark were killed, with four of the seven other people in the apartment wounded. The survivors of the raid were all charged with assault and attempted murder. The police claimed they shot in self-defense, although a controversy arose when the Panthers, other activists and a Chicago newspaper reporter presented visual evidence, as well as the testimony of an FBI ballistics expert, showing that the sleeping Panthers were not resisting arrest and only fired one shot, as opposed to the more than one hundred the police fired into the apartment. The charges were later dropped, and the families of the dead won a $1.8 million settlement from the government. It was discovered in 1971 that Hampton had been targeted by the FBI's COINTELPRO. True to Dohrn's words, this single event, in the continuing string of public killings of black leaders of any political stripe, was the trigger that pushed a large number of Weatherman and other students who had just attended the last SDS national convention months earlier to go underground and develop its logistical support network nationally.
On May 21, 1970, a communiqué from the Weather Underground was issued promising to attack a "symbol or institution of American injustice" within two weeks. The communiqué included taunts towards the FBI, daring them to try and find the group, whose members were spread throughout the United States. Many leftist organizations showed curiosity in the communiqué, and waited to see if the act would in fact occur. However, two weeks would pass without any occurrence. Then on June 9, 1970, their first publicly acknowledged bombing occurred at a New York City police station, saying it was "in outraged response to the assassination of the Soledad Brother George Jackson," who had recently been killed by prison guards in an escape attempt. The FBI placed the Weather Underground organization on the ten most-wanted list by the end of 1970.
Activity in 1970.
On June 9, 1970, a bomb made with ten sticks of dynamite exploded in the 240 Centre Street, the headquarters of the New York City Police Department. The explosion was preceded by a warning about six minutes prior to the detonation and subsequently by a WUO claim of responsibility.
On July 23, 1970, a Detroit federal grand jury indicted 13 Weathermen members in a national bombing conspiracy, along with several unnamed co-conspirators. Ten of the thirteen already had outstanding federal warrants.
In September 1970, the group accepted a $20,000 payment from the largest international psychedelics distribution organization, called The Brotherhood of Eternal Love, to break LSD advocate Timothy Leary out of a California prison in San Luis Obispo, north of Santa Barbara, California, and transport him and his wife to Algeria, where Leary joined Eldridge Cleaver. Rumors also circulated that the funds were donated by an internationally known female folk singer in Los Angeles or by Elephant's Memory, which was John Lennon's backup band in New York City and was a factor with the attempted deportation of Lennon, who had donated bail money for radical groups.
In October 1970, Bernardine Dohrn was put on the FBI's Ten Most Wanted List.
Pentagon bombing.
On May 19, 1972, Ho Chi Minh’s birthday, the Weather Underground placed a bomb in the women’s bathroom in the Air Force wing of the Pentagon. The damage caused flooding that destroyed computer tapes holding classified information. Other radical groups worldwide applauded the bombing, illustrated by German youths protesting against American military systems in Frankfurt. This was "in retaliation for the U.S. bombing raid in Hanoi." 
Withdrawal of charges.
In 1973, the government requested dropping charges against most of the WUO members. The requests cited a recent decision by the Supreme Court of the United States that barred electronic surveillance without a court order. This Supreme Court decision would hamper any prosecution of the WUO cases. In addition, the government did not want to reveal foreign intelligence secrets that a trial would require. Bernardine Dohrn was removed from the FBI's Ten Most Wanted List. The initial federal district court cases, and federal appellate decisions, upholding the decisions of the lower courts, originated in federal grand jury proceedings in Seattle and Portland investigating Matthew Steen, who had been arrested on federal bank robbery and conspiracy charges in Seattle in 1971. As with the earlier federal grand juries that subpoenaed Leslie Bacon and Stew Albert in the U.S. Capitol bombing case, these investigations were known as "fishing expeditions", with the evidence gathered through "black bag" jobs including illegal mail openings that involved the FBI and U.S. Postal Service, burglaries by FBI field offices, and electronic surveillance by the Central Intelligence Agency against the support network, friends, and family members of the Weather Underground as part of Nixon's COINTELPRO apparatus. In the case of Steen, a special federal grand jury was convened and directed by Guy Goodwin and Robert Mardian, Nixon-appointed heads of the Internal Security Division of the Justice Department. Mardian was later convicted in the COINTELPRO scandal but had his conviction reversed. During the time Steen was in a federal prison, the Senate Select Committee on Intelligence Church Committee convened to investigate these domestic spying abuses while the Weather Underground remained fugitives.
These grand juries caused Sylvia Jane Brown, Robert Gelbhard, and future members of the Seattle Weather Collective to be subpoenaed in Seattle and Portland for the investigation of one of the first (and last) captured WUO members. Steen was also suspected in the U.S. Capitol bombing, several attempted embassy bombings in Washington, D.C. in 1971, and participation in the break-in at the FBI office in Media, PA, that resulted in the release of numerous secret FBI files to the print media and which outlined the broad array of illegal domestic spying activities conducted by numerous federal agencies under the rubric of Cointelpro. Refusing to testify, all were incarcerated for the 18 month life of the special tribunal and later freed on appeal due to suspected wiretappings and mail openings. Four months afterwards the cases were dismissed The decisions in these cases led directly to the subsequent resignation of FBI Director, L. Patrick Gray, and the federal indictments of W. Mark Felt or "Deep Throat" and Edwin Miller and which, earlier, was the factor leading to the dismissal of federal "most-wanted" charges against members of the Weather Underground leadership in 1973. Steen was sentenced to two consecutive five-year terms in federal prison, his conviction later commuted by President Jimmy Carter under his amnesty program 
"Prairie Fire".
With the help from Clayton Van Lydegraf, the Weather Underground sought a more Marxist-Leninist ideological approach to the post-Vietnam reality. The leading members of the Weather Underground (Bill Ayers, Bernardine Dohrn, Jeff Jones, and Celia Sojourn) collaborated on ideas and published their manifesto: "Prairie Fire: The Politics of Revolutionary Anti-Imperialism." The name came from a quote by Mao Zedong, "a single spark can set a prairie fire." By the summer of 1974, 5,000 copies had surfaced in coffee houses, bookstores and public libraries across America. Leftist newspapers praised the manifesto.
Abbie Hoffman publicly praised "Prairie Fire" and believed every American should be given a copy. The manifesto’s influence initiated the formation of the Prairie Fire Organizing Committee in several American cities. Hundreds of above-ground activists helped further the new political vision of the Weather Underground. Among other things, the manifesto called for the violent overthrow of the U.S. government and the establishment of a Dictatorship of the Proletariat as a means to achieving its social goals:
The only path to the final defeat of imperialism and the building of socialism is revolutionary war... Socialism is the violent overthrow of the bourgeoisie, the establishment of the dictatorship of the proletariat, and the eradication of the social system based on profit... Revolutionary war will be complicated and protracted... It includes mass struggle and clandestine struggle, peaceful and violent, political and economic, cultural and military, where all forms are developed in harmony with the armed struggle. Without mass struggle there can be no revolution. Without armed struggle there can be no victory.
Essentially, after the 1969 failure of the Days of Rage to involve thousands of youth in massive street fighting, Weather renounced most of the Left and decided to operate as an isolated underground group. Prairie Fire urged people to never "dissociate mass struggle from revolutionary violence." To do so, claimed Weather, was to do the state's work. Just as in 1969-70, Weather still refused to renounce revolutionary violence for "to leave people unprepared to fight the state is to seriously mislead them about the inevitable nature of what lies ahead." However, the decision to build only an underground group caused the Weather Underground to lose sight of its commitment to mass struggle and made future alliances with the mass movement difficult and tenuous.:76–77
By 1974, Weather had recognized this shortcoming and in "Prairie Fire" detailed a different strategy for the 1970s which demanded both mass and clandestine organizations. The role of the clandestine organization would be to build the "consciousness of action" and prepare the way for the development of a people's militia. Concurrently, the role of the mass movement (i.e., above ground Prairie Fire collective) would include support for, and encouragement of, armed action. Such an alliance would, according to Weather, "help create the 'sea' for the guerrillas to swim in." :76–77
According to Bill Ayers in the late 1970s, the Weatherman group further split into two factions — the May 19th Communist Organization and the "Prairie Fire Collective" — with Bernardine Dohrn and Bill Ayers in the latter. The Prairie Fire Collective favored coming out of hiding and establishing an above ground revolutionary mass movement. With most WUO members facing the limited criminal charges (most charges had been dropped by the government in 1973) against them creating an above ground organization was more feasible. The May 19 Communist Organization continued in hiding as the clandestine organization. A decisive factor in Dohrn's coming out of hiding were her concerns about her children (Bill Ayers, "Fugitive Days: Memoirs of an Antiwar Activist", Beacon Press, 2001, 978-0-8070-3277-0). The Prairie Fire Collective faction started to surrender to the authorities from the late 1970s to the early 1980s. The remaining Weather Underground members continued to attack US institutions.
Cointelpro.
Event.
In April 1971, the "Citizens' Commission to Investigate the FBI" broke into an FBI office in Media, Pennsylvania. The group stole files with several hundred pages. The files detailed the targeting of civil rights leaders, labor rights organizations, and left wing groups in general, and included documentation of acts of intimidation and disinformation by the FBI, and attempts to erode public support for those popular movements. By the end of April, the FBI offices were to terminate all files dealing with leftist groups. The files were a part of an FBI program called Cointelpro.
After Cointelpro was dissolved in 1971 by J. Edgar Hoover, the FBI continued its counterintelligence on groups like the Weather Underground. In 1973, the FBI established the "Special Target Information Development" program, where agents were sent undercover to penetrate the Weather Underground. Due to the illegal tactics of FBI agents involved with the program, government attorneys requested all weapons- and bomb-related charges be dropped against the Weather Underground. The most well-publicized of these tactics were the "black-bag jobs," referring to searches conducted in the homes of relatives and acquaintances of Weatherman. The Weather Underground was no longer a fugitive organization and could turn themselves in with minimal charges against them. Additionally, the illegal domestic spying conducted by the CIA in collaboration with the FBI also lessened the legal repercussions for Weatherman turning themselves in.
Investigation and trial.
After the Church Committee revealed the FBI's illegal activities, many agents were investigated. In 1976, former FBI Associate Director W. Mark Felt publicly stated he had ordered break-ins and that individual agents were merely obeying orders and should not be punished for it. Felt also stated that acting Director L. Patrick Gray had also authorized the break-ins, but Gray denied this. Felt said on the CBS television program "Face the Nation" that he would probably be a "scapegoat" for the Bureau's work. "I think this is justified and I'd do it again tomorrow," he said on the program. While admitting the break-ins were "extralegal," he justified it as protecting the "greater good." Felt said, "To not take action against these people and know of a bombing in advance would simply be to stick your fingers in your ears and protect your eardrums when the explosion went off and then start the investigation."
The Attorney General in the new Carter administration, Griffin B. Bell, investigated, and on April 10, 1978, a federal grand jury charged Felt, Edward S. Miller, and Gray with conspiracy to violate the constitutional rights of American citizens by searching their homes without warrants. The case did not go to trial and was dropped by the government for lack of evidence on December 11, 1980.
The indictment charged violations of Title 18, Section 241 of the United States Code. The indictment charged Felt and the others "did unlawfully, willfully, and knowingly combine, conspire, confederate, and agree together and with each other to injure and oppress citizens of the United States who were relatives and acquaintances of the Weatherman fugitives, in the free exercise and enjoyments of certain rights and privileges secured to them by the Constitution and the laws of the United States of America.?
Felt and Miller attempted to plea bargain with the government, willing to agree to a misdemeanor guilty plea to conducting searches without warrants—a violation of 18 U.S.C. sec. 2236—but the government rejected the offer in 1979. After eight postponements, the case against Felt and Miller went to trial in the United States District Court for the District of Columbia on September 18, 1980. On October 29, former President Richard Nixon appeared as a rebuttal witness for the defense, and testified that presidents since Franklin D. Roosevelt had authorized the bureau to engage in break-ins while conducting foreign intelligence and counterespionage investigations.
It was Nixon's first courtroom appearance since his resignation in 1974. Nixon also contributed money to Felt's legal defense fund, with Felt's legal expenses running over $600,000. Also testifying were former Attorneys General Herbert Brownell, Jr., Nicholas Katzenbach, Ramsey Clark, John N. Mitchell, and Richard G. Kleindienst, all of whom said warrantless searches in national security matters were commonplace and not understood to be illegal, but Mitchell and Kleindienst denied they had authorized any of the break-ins at issue in the trial.
The jury returned guilty verdicts on November 6, 1980. Although the charge carried a maximum sentence of 10 years in prison, Felt was fined $5,000. (Miller was fined $3,500). Writing in "The New York Times" a week after the conviction, Roy Cohn claimed that Felt and Miller were being used as scapegoats by the Carter administration and that it was an unfair prosecution. Cohn wrote it was the "final dirty trick" and that there had been no "personal motive" to their actions.
"The Times" saluted the convictions, saying that it showed "the case has established that zeal is no excuse for violating the Constitution". Felt and Miller appealed the verdict, and they were later pardoned by Ronald Reagan.
Dissolution.
Despite the change in their legal status, the Weather Underground remained underground for a few more years. However, by 1976 the organization was disintegrating. The Weather Underground held a conference in Chicago called Hard Times. The idea was to create an umbrella organization for all radical groups. However, the event turned sour when Hispanic and Black groups accused the Weather Underground and the Prairie Fire Committee of limiting their roles in racial issues. The Weather Underground faced accusations of abandonment of the revolution by reversing their original ideology.
The conference increased divisions within the Weather Underground. East coast members favored a commitment to violence and challenged commitments of old leaders, Bernardine Dohrn, Bill Ayers, and Jeff Jones. These older members found they were no longer liable for federal prosecution because of illegal wire taps and the government's unwillingness to reveal sources and methods favored a strategy of inversion where they would be above ground "revolutionary leaders". Jeremy Varon argues that by 1977 the WUO had disbanded.
Matthew Steen appeared on the lead segment of CBS' "60 Minutes" in 1976 and was interviewed by Mike Wallace about the ease at creating fake identification, the first ex-Weatherman interview on national television. (The House document has the date wrong, it aired February 1, 1976 and the title was Fake ID)
The federal government estimated that only 38 Weathermen had gone underground in 1970, though the estimates varied widely, according to a variety of official and unofficial sources, as between 50 to 600 members. Most modern sources lean towards a much larger number than the FBI reference. An FBI estimate in 1976, or slightly later, of then current membership was down to 30 or fewer.
Plot to bomb office of California Senator.
In November 1977 five WUO members were arrested on conspiracy to bomb the office of California State Senator John Briggs. It was later revealed that the Revolutionary Committee and PFOC had been infiltrated by the FBI for almost six years. FBI agents Richard J. Gianotti and William D. Reagan lost their cover in November when federal judges needed their testimony to issue warrants for the arrest of Clayton Van Lydegraf and four Weather people. The arrests were the results of the infiltration.
WUO members Judith Bissell, Thomas Justesen, Leslie Mullin, and Marc Curtis pled guilty while Clayton Van Lydegraf, who helped write the 1974 Prairie Fire Manifesto, went to trial.
Within two years, many members turned themselves in after taking advantage of President Jimmy Carter's amnesty for draft dodgers. Mark Rudd turned himself in to authorities on January 20, 1978. Rudd was fined $4,000 and received two years probation. Bernardine Dohrn and Bill Ayers turned themselves in on December 3, 1980, in New York, with substantial media coverage. Charges were dropped for Ayers. Dohrn received three years probation and a $15,000 fine.
Brinks robbery.
Some members remained underground and joined splinter radical groups. The US government states that three members of the Weather Underground, Kathy Boudin, Judith Alice Clark, and David Gilbert, joined the May 19 Communist Organization. On October 20, 1981 in Nanuet, New York, the group robbed a Brinks armored truck containing $1.6 million. The robbery was violent, resulting in the murders of two police officers and a security guard.
Boudin, Clark, and Gilbert were found guilty and sentenced to lengthy terms in prison. Media reports listed them as former Weatherman Underground members considered the “last gasps” of the Weather Underground. The documentary "The Weather Underground" described the Brinks Robbery as the "unofficial end" of the Weather Underground.
May 19th Communist Organization.
The Weather Underground members involved in the May 19th Communist Organization alliance with the Black Liberation Army continued in a series of jail breaks, armed robberies and bombings until most members were finally arrested in 1985 and sentenced as part of the Brinks robbery and the Resistance Conspiracy case.
Coalitions with non-WUO members.
Throughout the underground years, the Weather Underground members worked closely with their counterparts in other organizations, including Jane Alpert, to bring attention their further actions to the press. She helped Weatherman pursue their main goal of overthrowing the U.S. government through her writings. However, there were tensions within the organization, brought about by her famous manifesto, "Mother Right", that specifically called on the Weatherwomen to focus on their own cause other than anti-imperialism. Weather members then wrote in response to her manifesto.
Legacy.
Widely known members of the Weather Underground include Kathy Boudin, Linda Sue Evans, Brian Flanagan, David Gilbert, Ted Gold, Naomi Jaffe, Jeff Jones, Joe Kelly, Diana Oughton, Eleanor Raskin, Terry Robbins, Mark Rudd, Matthew Steen, Susan Stern, Laura Whitehorn, Cathy Wilkerson, and the still-married couple Bernardine Dohrn and Bill Ayers. Most former Weathermen have successfully re-integrated into mainstream society, without necessarily repudiating their original intent.
The Weather Underground was referred to in its own time and afterwards as a terrorist group. A few police officers were allegedly killed by the group, one killed by a pipe bomb and another 2, plus a guard, by a robbery gone wrong, though the investigation of the first is still open and the latter was a robbery conducted by former members of the group after the group had dissolved.
The group also fell under the auspices of the FBI-New York City Police Anti Terrorist Task Force, a forerunner of the FBI's Joint Terrorism Task Forces. The FBI, on its website, describes the organization as having been a "domestic terrorist group", but no longer an active concern. Others either dispute or clarify the categorization, or justify the group's violence as an appropriate response to the Vietnam war, domestic racism and the assassinations of black leaders.
In his 2001 book about his Weatherman experiences, Bill Ayers stated his objection to describing the WUO as terrorist. Ayers wrote: "Terrorists terrorize, they kill innocent civilians, while we organized and agitated. Terrorists destroy randomly, while our actions bore, we hoped, the precise stamp of a cut diamond. Terrorists intimidate, while we aimed only to educate. No, we're not terrorists." Dan Berger, in his book about the Weatherman, "Outlaws in America", asserts that the group "purposefully and successfully avoided injuring anyone... Its war against property by definition means that the WUO was not a terrorist organization."
However, their claims that they never attacked or intended to attack people are disputed by others. Weather Underground member Mark Rudd, reminiscing in 2005 about the Greenwich Village townhouse explosion, said: "On the morning of March 6, 1970, three of my comrades were building pipe bombs packed with dynamite and nails, destined for a dance of non-commissioned officers and their dates at Fort Dix, N.J., that night." Harvey Klehr, the Andrew W. Mellon professor of politics and history at Emory University in Atlanta, said in 2003, "The only reason they were not guilty of mass murder is mere incompetence. I don't know what sort of defense that is."
Bill Ayers, a retired professor of education at the University of Illinois at Chicago, was quoted in an interview to say "I don't regret setting bombs" but has since claimed he was misquoted. During the presidential election campaign of 2008, several candidates questioned Barack Obama's contacts with Ayers, including Hillary Clinton, John McCain and Sarah Palin. Ayers responded in December 2008, after Obama's election victory, in an op-ed piece in "The New York Times":
We did carry out symbolic acts of extreme vandalism directed at monuments to war and racism, and the attacks on property, never on people, were meant to respect human life and convey outrage and determination to end the Vietnam war... The responsibility for the risks we posed to others in some of our most extreme actions in those underground years never leaves my thoughts for long. The antiwar movement in all its commitment, all its sacrifice and determination, could not stop the violence unleashed against Vietnam. And therein lies cause for real regret.
Brian Flanagan has expressed regret for his actions during the Weatherman years, and compared the group's activities to terrorism. Flanagan said: "When you feel that you have right on your side, you can do some pretty horrific things." Mark Rudd, now a teacher of mathematics at Central New Mexico Community College, has said he has "mixed feelings" and feelings of "guilt and shame".
These are things I am not proud of, and I find it hard to speak publicly about them and to tease out what was right from what was wrong. I think that part of the Weatherman phenomenon that was right was our understanding of what the position of the United States is in the world. It was this knowledge that we just couldn't handle; it was too big. We didn't know what to do. In a way I still don't know what to do with this knowledge. I don't know what needs to be done now, and it's still eating away at me just as it did 30 years ago.—Mark Rudd
A faction of the Weather Underground continues today as the Prairie Fire Organizing Committee. Their official site reads:
We oppose oppression in all its forms including racism, sexism, homophobia, classism and imperialism. We demand liberation and justice for all peoples. We recognize that we live in a capitalist system that favors a select few and oppresses the majority. This system cannot be reformed or voted out of office because reforms and elections do not challenge the fundamental causes of injustice.
The site further supports armed violence:
We also respect the right of people to take up armed struggle against colonialism for the liberation of oppressed peoples
See also.
General:

</doc>
<doc id="33986" url="http://en.wikipedia.org/wiki?curid=33986" title="War hammer">
War hammer

A war hammer is a late medieval weapon of war intended for close combat action, whose design resembles the hammer. Its appearance is similar to that of an ice axe.
Design.
The war hammer consists of a handle and a head. The handle may be of different lengths, the longest being roughly equivalent to the halberd, and the shortest about the same as a mace. Long war hammers were pole weapons (polearms) meant for use against riders, whereas short ones were used in closer quarters and from horseback.
War hammers were developed as a consequence of the prevalence of surface-hardened steel surfacing of wrought iron armours of the late medieval battlefields during the fourteenth and fifteenth centuries. The surface of the armour was now as hard as the edge of a blade, so a blade tended to ricochet. Swords and battleaxes were likely to only give a glancing blow, losing much of the impact, especially on the high curvature of the helmet. The war hammer could deliver the full force to the target.
War hammers, especially when mounted on a pole, could damage without penetrating the armour. In particular, they transmitted the impact through even the thickest helmet and caused concussions. Later war hammers often had a spike on one side of the head, making them more versatile weapons. A blade or spike tended to be used not against helmets but against other parts of the body where the armour was thinner and penetration was easier. The spike end could be used for grappling the target's armour, reins, or shield, or could be turned in the direction of the blow to pierce even heavy armour. Against mounted opponents, the weapon could also be directed at the legs of the horse, toppling the armoured foe to the ground where he could be more easily attacked.
Maul.
The maul is a long-handled hammer with a heavy metal head, either of lead or iron. Similar in appearance and function to a modern sledgehammer, it is sometimes shown as having a spear-like spike on the fore-end of the haft. 
The use of the maul as a weapon seems to date from the later 14th century. In 1382, rebellious citizens of Paris seized 3,000 mauls (French: "maillet") from the city armoury, leading to the rebels being dubbed Maillotins. Later in the same year, Froissart records French men-at-arms using mauls at the Battle of Roosebeke, demonstrating they were not simply weapons of the lower classes.
A particular use of the maul was by archers in the 15th and 16th centuries. At the Battle of Agincourt, English longbowmen are recorded as using lead mauls, initially as a tool to drive in stakes but later as an improvised weapon. Other references during the century (for example, in Charles the Bold's 1472 Ordinance) suggest continued use. They are recorded as a weapon of Tudor archers as late as 1562.

</doc>
<doc id="33989" url="http://en.wikipedia.org/wiki?curid=33989" title="Washington Treaty">
Washington Treaty

The Treaty of Washington may refer to:

</doc>
<doc id="33992" url="http://en.wikipedia.org/wiki?curid=33992" title="Wulfstan of Hedeby">
Wulfstan of Hedeby

Wulfstan of Hedeby (Latin "Haithabu") was a late ninth century traveller and trader. His travel accounts, as well as those of another trader, Ohthere, were included in Alfred the Great's translation of Orosius' "Histories". It is unclear if Wulfstan was English or indeed if he was from Hedeby, in today's northern Germany near the city of Schleswig.
According to this account, Wulfstan undertook a journey by sea from Hedeby to the trading centre of Truso around the year 880. He names the lands the coasts he passes.
This may be the earliest recorded use of the word "Denmark" (or "Danemearcan").

</doc>
<doc id="33994" url="http://en.wikipedia.org/wiki?curid=33994" title="Warren Farrell">
Warren Farrell

Warren Thomas Farrell (born June 26, 1943) is an American educator, activist and author of seven books on men's and women's issues.
He came to prominence in the 1970s, championing the cause of second wave feminism, and serving on the New York City Board of the National Organization for Women (NOW). However, he left NOW and is now recognized as an important figure in the modern men's movement.
His books cover ten fields: History, Law, Sociology and Politics ("The Myth of Male Power"); couples' communication ("Women Can't Hear What Men Don't Say, and Father and Child Reunion"); Economic and Career issues ("Why Men Earn More"); child psychology and child custody ("Father and Child Reunion"); and teenage to adult psychology and socialization ("Why Men Are The Way They Are" and "The Liberated Man"). All of his books are related to men's and women studies; consistent to his books since the early 90's has been a call for a gender transition movement.
Early life and education.
Farrell was born in 1943. He is the eldest of 3, of an accountant father and housewife mother. He grew up in New Jersey. Farrell attended high school at the American School of The Hague in his Freshman and Sophomore years, then graduated from Midland Park High School in New Jersey in 1961, where he was student body president. He was chosen by the American Legion as his town's (Waldwick's) selection for New Jersey Boys' State.
Warren Farrell received a B.A. from Montclair State University in social sciences in 1965. As a college student, Farrell was a national vice-president of the Student-National Education Association, leading President Lyndon B. Johnson to invite him to the White House Conference on Education.
In 1966 he received an M.A. from UCLA in political science and in 1974 a Ph.D. in the same discipline from New York University. While completing his Ph.D. at NYU, he served as an assistant to the president of New York University.
University teaching.
Farrell has taught university level courses in five disciplines (psychology; women's studies; sociology; political science; gender and parenting issues). These were at the School of Medicine at the University of California, San Diego; the California School of Professional Psychology; in the Department of Women's Studies at San Diego State; at Brooklyn College; Georgetown University; American University, and Rutgers.
Feminist Foundation.
When the second wave of the women's movement evolved in the late 1960s, Farrell's support of it led the National Organization for Women's New York City chapter to ask him to form a men's group. The response to that group led to his ultimately forming some 300 additional men and women's groups and becoming the only man to be elected three times to the Board of Directors of the National Organization for Women in N.Y.C. (1971–74). In 1974, Farrell left N.O.W. in N.Y.C. and his teaching at Rutgers when his wife became a White House Fellow and he moved with her to D.C. They subsequently divorced.
During his feminist period, Farrell wrote op-eds for the "New York Times" and appeared frequently on the "Today" show and "Phil Donahue" show, and was featured in "People", "Parade" and the international media. This, and his women and men's groups, one of which had been joined by John Lennon, inspired "The Liberated Man". "The Liberated Man" was written from a feminist perspective, introducing alternative family and work arrangements that could better accommodate working women and encourage care-giving men. "The Liberated Man" was the beginning of Farrell's development of parallels for men to the female experience: for example, to women's experience as "sex objects," Farrell labeled men's parallel experience as "success objects."
As a speaker, Farrell was known for creating audience participation role-reversal experiences to get both sexes "to walk a mile in the other's moccasins." The most publicized were his "men's beauty contest" and "role-reversal date." In the men's beauty contest, all the men are invited to experience "the beauty contest of everyday life that no woman can escape." In the "role-reversal date" every woman was encouraged to "risk a few of the 150 risks of rejection men typically experience between eye contact and intercourse."
Integrating men's issues into gender issues.
In a 1997 interview, Farrell stated:
"Why Men Are The Way They Are".
Farrell's books each contain personal introductions that describe his perspective on how aspects of public consciousness and his own personal development led to the book. By the mid-1980s, Farrell was writing that both the role-reversal exercises and the women and men's groups allowed him to hear women's increasing anger toward men, and also learn about men's feelings of being misrepresented. He wrote "Why Men Are The Way They Are" to answer women's questions about men in a way he hoped rang true for the men.
He distinguished between what he believed to be each sex's primary fantasies and primary needs, stating that "both sexes fell in love with members of the other sex who are the least capable of loving: women with men who are successful; men with women who are young and beautiful." He asserts that women feel disappointed because, "the qualities it takes to be successful at work are often in tension with the qualities it takes to be successful in love." Similarly he asserts that men feel disappointed because, "a young and beautiful woman ('genetic celebrity') often learns more about receiving, not giving, while older and less-attractive women often learn more about giving and doing for others, which is more compatible with love." Due partially to Oprah Winfrey's support, "Why Men Are the Way They Are" became his best-selling book.
"The Myth of Male Power".
By the early 1990s, Farrell was writing that he felt the misunderstandings about men had deepened and become dangerous to the survival of families and love. The result was "The Myth of Male Power".
In "The Myth of Male Power", Farrell offered his first in-depth outline of the thesis he would weave through his subsequent books: that for men and women to make an evolutionary shift from a focus on survival to a focus on a balance between survival and fulfillment, what was ultimately necessary was neither a women's movement nor a men's movement, but a "gender transition movement." He defined a gender transition movement as one that fosters a transition from the rigid roles of our past to more flexible roles for the future.
As the book's title implied, "The Myth of Male Power" challenged the belief that men had the power—in part by challenging the definition of power. Farrell defined power as "control over one's life." He wrote that, "In the past, neither sex had power; both sexes had roles: women's role was to raise children; men's role was to raise money."
Farrell documented how, cross-culturally, men's feelings of powerlessness involved how they felt they were socialized, even as boys, to become "the disposable sex." He argued that virtually every society that survived did so by training a cadre of its sons to be disposable—in war, and in work. The paradox of masculinity, he proposed, is that the very training for traditional masculinity that created a healthy society created unhealthy boys and men.
"The Myth of Male Power" was ardently challenged by some academic feminists, whose critique is that men earn more money, and that money is power. Farrell concurs that men earn more money, and that money is one form of power. However, Farrell also adds that "men often feel obligated to earn money someone else spends while they die sooner--and feeling "obligated" is not power." This perspective was to be more fully developed in Farrell's "Why Men Earn More".
Farrell says heterosexual men are conditioned to believe that they can obtain love and affection from women only by earning money. This belief in turn leads to psychological problems for both sexes; according to Farrell, "men's weakness is their facade of strength; women's strength is their facade of weakness."
Perhaps Farrell's most controversial contribution to gender politics is "The Myth of Male Power's" directly challenging the belief that patriarchal societies make rules to benefit men at the expense of women. Farrell argues that this oversimplification misses many realities—such as the facts that only American men, and not women are required to register for military conscription, or the fact that men constitute 93% of workplace deaths.
Analyses such as these led "The Myth of Male Power" to become both his most-praised and most-controversial book. In the discipline of men's studies, it is considered to be a classic. In the new academic journal, "New Male Studies: An International Journal", in-depth interviews with Dr. Farrell focusing on The Myth of Male Power are the main features of two issues in 2012. In the men's rights movement, "The Myth of Male Power" is sometimes referred to as "The Bible" and the "red pill". This book, however, led to Susan Faludi writing in Backlash that Farrell had recanted his original feminist position.
"Women Can't Hear What Men Don't Say" and "Father and Child Reunion".
The increase in divorces in the 1980s and 1990s turned Farrell's writing toward two issues: the poverty of couples' communication; and children's loss of their father in child custody cases.
In "Women Can't Hear What Men Don't Say", Farrell asserts that couples often fail to use couples' communication outside of counseling if the person receiving criticism does not know how to make her or himself feel safe. Farrell develops a method called "Cinematic Immersion" to create that safety and overcome what he posits is humans' biological propensity to respond defensively to personal criticism.
To address children's loss of their father in child custody cases, Farrell wrote "Father and Child Reunion", a meta-analysis of research about what is the optimal family arrangement for children of divorce. "Father and Child Reunion's" findings include some 26 ways in which children of divorce do better when three conditions prevail: equally-shared parenting (or joint custody); close parental proximity; and no bad-mouthing. His research for "Father and Child Reunion" provided the basis for his frequently appearing in the first decade of the 21st Century as an expert witness in child custody cases on the balance between mothers' and fathers' rights needed to create the optimal family arrangement for children of divorce.
"Why Men Earn More".
By the start of the 21st century, Farrell felt he had re-examined every substantial adult male-female issue except the pay gap (i.e., that men as a group tend to earn more money than women as a group). In "Why Men Earn More: The Startling Truth Behind the Pay Gap—and What Women Can Do About It," he documents 25 differences in men and women's work-life choices which, he argues, account for most or all of the pay gap more accurately than did claims of widespread discrimination against women. Common to each of men's choices was earning more money, while each of women's choices prioritized having a more-balanced life. These 25 differences allowed Farrell to offer women 25 ways to higher pay—and accompany each with their possible trade-offs. The trade-offs include working more hours and for more years; taking technical or more-hazardous jobs; relocating overseas or traveling overnight. This led to considerable praise for "Why Men Earn More" as a career book for women.
Some of Farrell's findings in "Why Men Earn More", included his analysis of census bureau data that never-married women without children earn 13% more than their male counterparts; or that the gender pay gap is largely about married men with children who earn more due to their assuming more workplace obligations.
Themes woven throughout "Why Men Earn More" are the importance of assessing trade-offs; that "the road to high pay is a toll road;" the "Pay Paradox" (that "pay is about the power we forfeit to get the power of pay"); and, since men earn more, and women have more balanced lives, that men have more to learn from women than women do from men.
"Does Feminism Discriminate Against Men?".
Farrell's most recent book, "Does Feminism Discriminate Against Men?" (ISBN 978-0195312836), published by Oxford University Press in 2008, is a debate book with feminist co-author James Sterba. Farrell felt gender studies in universities rarely incorporated the masculine gender except to demonize it. This book was Farrell's attempt to test whether a positive perspective about men would be allowed to be incorporated into universities' gender studies curriculum even if there were a feminist rebuttal. Farrell and Sterba debated 13 topics, from children's and fathers' rights, to the "Boy Crisis."
Critical reception.
Warren Farrell's books, published in sixteen languages, tend to make both international news and be the subject of both praise and criticism from the political right and left, and from feminists and anti-feminists. "The Myth of Male Power" is both the most-praised and criticized. For example, in 2012, when speaking on his forthcoming The Boy Crisis at the University of Toronto, Farrell encountered a protest organized by Socialist Worker Party feminists calling him a "rape apologist". This was said to be based on his chapter on "The Politics of Rape" in The Myth of Male Power. Farrell maintains the chapter is a blueprint for preventing rape.
Early critiques in the "New York Times Book Review" by Larry McMurtry and John Leonard included disdain for Farrell's use of gender neutral language in "The Liberated Man". More recently, conservative and antifeminist Phyllis Schlafly labels Farrell a "feminist apologist," though praises his research for "Father and Child Reunion".
Praise of Warren Farrell include Kate Zernike of "The Boston Globe" ranking him as "the sage of the men's movement," and the description of him as the "Gloria Steinem of men's liberation" by Carol Kleiman of the "Chicago Tribune". "Esquire" ranked Farrell, Thomas Aquinas, and John Stuart Mill as three of history's leading male feminists.
In 2012, a new international academic journal, "New Male Studies", highlighted "The Myth of Male Power" by making it the lead feature in two of its first issues, positioning it as a classic to the field of men's studies.
Farrell's recent collaborations with Ken Wilber, John Gray, and Richard Bolles, have introduced his messages to more diverse and receptive audiences. Farrell's claims on gender relations have attracted the interest of English academic Rory Ridley-Duff, who has integrated Farrell's perspectives into curriculum materials, academic papers and a book and developed "Attraction Theory" to capture the gendering dynamics implicit in Farrell's work.
Personal life.
Farrell married Ursula, a mathematician and IBM executive, in the sixties. After 10 years of marriage, in 1976, they separated and subsequently divorced. After what Farrell described as "twenty years of adventuresome single-hood", he married Liz Dowling in August 2002. He has two step-daughters. They live in Mill Valley, California.
Other activities.
During the California gubernatorial recall election, Farrell ran as a Democratic candidate, on a platform of father's rights, and received 626 votes.
Farrell's current foci are conducting communication workshops; being an expert witness in child custody cases; and researching a forthcoming book (working title "The Boy Crisis"), to be co-authored with John Gray. In 2010-11, he keynoted, along with Deepak Chopra, a world conference on spirituality (the Integral Spiritual Experience), addressing the evolution of love. He was then invited by the Center on World Spirituality to be one of their world leaders. Dr. Farrell is also now a semi-regular contributor to controversial web publication A Voice for Men, and at their international conference in June 2014, gave a keynote titled, "The Ten Major Boys and Men's Issues".
In 2009, a call from the White House requesting Dr. Farrell to be an advisor to the White House Council on Women and Girls led to Dr. Farrell creating and chairing a commission to create a White House Council on Boys and Men. The multi-partisan commission consists of thirty-five authors and practitioners (e.g., John Gray, Gov. Jennifer Granholm, Michael Gurian, Michael Thompson, Bill Pollack, Leonard Sax) of boys' and men's issues. They have completed a study that defines five components to a "boys' crisis," which has been submitted as a proposal for President Obama to create a White House Council on Boys and Men. In April 2015 the coalition went to Iowa to discuss their position with 2016 U.S. presidential condidates.

</doc>
<doc id="33999" url="http://en.wikipedia.org/wiki?curid=33999" title="List of cocktails">
List of cocktails

This is a list of cocktails. A cocktail is a mixed drink typically made with a distilled beverage (such as, gin, brandy, vodka, whiskey, tequila, or rum) that is mixed with other ingredients. If beer is one of the ingredients, the drink is called a beer cocktail.
Cocktails contain one or more types of liqueur, juice, fruit, sauce, honey, milk or cream, spices, or other flavorings. Cocktails may vary in their ingredients from bartender to bartender, and from region to region. Two creations may have the same name but taste very different because of differences in how the drinks are prepared.
This article is organized by the primary type of alcohol (by volume) contained in the beverage. Further organization details about the article are as follows:
Beer.
Cocktails made with beer are classified as beer cocktails.
Wines.
Cocktails with fortified wines.
The following drinks are technically cocktails because fortified wines are distilled.
Wine cocktails.
Wine variation cocktails.
The following drinks are not technically cocktails unless wine is secondary by volume to a distilled beverage, since wine is a fermented beverage not a distilled one.
Cocktails with a liqueur as the "primary" ingredient.
Chocolate liqueur.
Chocolate Martini Duo and trio cocktails#List of Duos and Trios
Coffee liqueurs.
"Coffee-flavored drinks"
Cream liqueurs.
"A liqueur containing cream, imparting a milkshake-like flavor"
"An intensely green, mint-flavored liqueur"
"A colorless mint-flavored liqueur"
Fruit liqueurs.
Orange-flavored.
"One of several orange-flavored liqueurs, like Grand Marnier or Triple Sec"
"A clear, blue-colored, orange-flavored liqueur"
Apple-flavored.
"A clear apple-flavored liqueur"
Other fruit flavors.
"A clear, bright-green, melon-flavored liqueur"
Herbal liqueurs.
Anise-flavored liqueurs.
"Licorice-flavored liqueurs"
Sambuca 

</doc>
<doc id="34001" url="http://en.wikipedia.org/wiki?curid=34001" title="Warmia">
Warmia

Warmia (Polish: "Warmia", Latin: "Varmia") or Erm(e)land (German:   ) is a historical region in northern Poland. 
It is nowadays the core of the Warmian-Masurian Voivodeship. It has about 4,500 km2 and 350,000 inhabitants. Its biggest city is Olsztyn, while the historical capital was Lidzbark Warmiński; another large town is Braniewo. Important landmarks include the cathedral in Frombork, where Mikołaj Kopernik elaborated the heliocentric theory, and sanctuary in Gietrzwałd, a site of Marian apparitions and miracles. It is an area of many lakes; it lies at the upper Łyna river and on the right bank of Pasłęka, stretching in the northwest to the Vistula Bay. Warmia is part of the historical province of Prussia and has traditionally strong connections with Masuria (the southern part was Polish-speaking too, while the rest has been German), but unlike it, remained Catholic and belonged to Poland before 1772. Warmia has been under the dominion of various states over the course of its history, most notably the Old Prussians, the Teutonic Knights, the Kingdom of Poland, and the Kingdom of Prussia. The history of the region is closely connected to that of the Archbishopric of Warmia (formerly, Duchy of Warmia). The region is associated with the Prussian tribe, the Warmians, who settled in an approximate area. According to folk etymology, Warmia is named after the legendary Prussian chief Warmo, whereas the name Ermland derives from his widow Erma.
History.
Early times.
The first traces of human settlement in the region come from roughly 14 to 15 thousand years ago: traces of settlements made by the Lusatian culture (thirteenth—fifth century BC), including above-ground water housings and artificial islands. By the early Middle Ages the Warmians, an Old Prussian tribe, inhabited the area.
The beginning of the Northern Crusades.
In the 13th century the area became a battleground in the Northern Crusades. Having failed to gather an expedition against Palestine, Pope Innocent III resolved in 1207 to organize a new crusade; beginning in 1209, he called for crusades against the Albigenses, against the Almohad dynasty of Spain (1213), and, also around that time, against the pagans of Prussia. The first Bishop of Prussia, Christian of Oliva, was commissioned in 1209 to convert the Prussians, at the request of Konrad I of Masovia (duke from 1194 to 1247).
The Teutonic Order.
In 1226 Duke Konrad I of Masovia invited the Teutonic Knights to Christianize the pagan Prussians. He supplied the Teutonic Order and allowed the usage of Chełmno Land ("Culmerland") as a base for the knights. They had the task of establishing secure borders between Masovia and the Prussians, with the assumption that conquered territories would become part of Masovia. The Order waited until they received official authorisation from the Empire, which Emperor Frederick II granted by issuing the Golden Bull of Rimini (March 1226). The papal Golden Bull of Rieti from Pope Gregory IX in 1234 confirmed the grant, although Konrad of Masovia never recognized the rights of the Order to rule Prussia. Later, the Knights were accused of forging these land grants. 
By the end of the 13th century the Teutonic Order had conquered and Christianized most of the Prussian region, including Warmia. The new régime reduced many of the native Prussians to the status of serfs and gradually Germanized them. Over several centuries the colonists, native Prussians and immigrants gradually intermingled.
In 1242 the papal legate William of Modena set up four dioceses, including the Archbishopric of Warmia. From the 13th century new colonists, mainly Germans, settled in the Monastic State of the Teutonic Knights (with Warmia) (the Duchy of Prussia, Lutheran from 1525 onwards, granted refuge to Protestant Poles, Lithuanians, Scots and Salzburgers). The bishopric was exempt and was governed by a prince-bishop, confirmed by Emperor Charles IV. The Bishops of Warmia were usually Germans or Poles, although Enea Silvio Piccolomini, the later Pope Pius II, served as an Italian bishop of the diocese. 
After the 1410 Battle of Grunwald, Bishop Heinrich Vogelsang of Warmia surrendered to King Władysław II Jagiełło of Poland, and later with Bishop Henry of Sambia gave homage to the Polish king at the Polish camp during the siege of Marienburg Castle (Malbork). After the Polish army moved out of Warmia, the new Grand Master of the Teutonic Knights, Heinrich von Plauen the Elder, accused the bishop of treachery and reconquered the region.
Polish Crown.
The Second Peace of Thorn (1466) removed Warmia from the control of the Teutonic Knights and placed it under the sovereignty of the Crown of Poland as part of the province of Royal Prussia, although with several privileges.
Soon after, in 1467, the Cathedral Chapter elected Nicolas von Tüngen against the wish of the Polish king. The Estates of Royal Prussia did not take the side of the Cathedral Chapter. Nicholas von Tüngen allied himself with the Teutonic Order and with King Matthias Corvinus of Hungary. The feud, known as the War of the Priests, was a low scale affair, affecting mainly Warmia. In 1478 Braniewo (Braunsberg) withstood a Polish siege which was ended in an agreement in which the Polish king recognized von Tüngen as bishop and the right of the Cathedral Chapter to elect future bishops, which however would have to be accepted by the king, and the bishop as well as Cathedral Chapter swore an oath to the Polish king. Later in the Treaty of Piotrków Trybunalski (December 7, 1512), conceded to the king of Poland a limited right to determine the election of bishops by choosing four candidates from Royal Prussia.
After the Union of Lublin in 1569 Duchy of Warmia was officially directly included as part of the Polish crown within the Polish-Lithuanian Commonwealth. At the same time the territory continued to enjoy substantial autonomy, with many legal differences from neighbouring lands. For example, the bishops were by law members of Polish Senat and the land elected MP's to the Sejmik resp. Landtag of Royal Prussia as well as MP's to the Sejm of Poland.
Warmia was under the Church jurisdiction of the Archbishopric of Riga until 1512, when Prince-Bishop Lucas Watzenrode received exempt status, placing Warmia directly under the authority of the Pope (in terms of church jurisdiction), which remained until the resolution of the Holy Roman Empire in 1806.
Prussia.
By the First Partition of Poland in 1772, Warmia was merged with the surrounding parts of East Prussia and annexed by the Kingdom of Prussia; the properties of the Archbishopric of Warmia were secularized by the Prussian state. Ignacy Krasicki, the last prince-bishop of Warmia as well as Enlightenment Polish poet, friend of Frederick the Great (whom he did not give homage as his new king), was nominated to the Archbishopric of Gnesen (Gniezno) in 1795. After the last partition of Poland and during his tenure as Archbishop of Poland and Prussian subject he was ordered by Pope Pius VI to teach his Catholic Poles to 'stay obedient, faithful, and loving to their new kings', Papal brief of 1795. The Prussian census in 1772 showed a total population of 96,547, including an urban population of 24,612 in 12 towns. 17,749 houses were listed and the biggest city was Braunsberg (Braniewo).
From 1772-1945 Warmia was part of Lutheran East Prussia, with the exception that the people of Warmia remained largely Catholic. Most of the German population of Warmia spoke High Prussian, while a small area in the north spoke Low Prussian; southern Warmia was mostly populated by Polish-speaking Warmiaks. Warmia became part of the German Empire in 1871.
In 1873 according to a regulation of the Imperial German government, school lessons at public schools inside Germany had to be held in German, as a result the Polish language was forbidden in all schools in Warmia, including Polish schools already founded in the sixteenth century. In 1900 Warmia's population was 240,000. In the jingoistic climate after World War I, Warmian Poles were subject to persecution by the German government. Polish children speaking their language were punished in schools and often had to wear signs with insulting names, such as "Pollack".
After the First World War in the aftermath of the East Prussian plebiscite, carried when Red Army was marching on Warsaw - Polish–Soviet War in 1920, the region remained in Germany, as in the Warmian district of Allenstein (Olsztyn) 86,53% and in the district of Rössel (Reszel) 97,9% voted for Germany. The persecutions of the German governments and militias worsened in the late 1930s when Hitler was in power, and the Poles in Warmia were subject to harsh persecution by German authorities and militias, such as attacks on schools and centers. During World War II Germany sought to suppress all elements of social and political life of the Polish minority in Germany by interning and murdering Polish activists and leaders, including the ones in Warmia. Unlike the rest of Protestant East Prussia, Warmia retained its Catholicism and Catholic-related folk customs, all the way through 1945.
Poland.
At the Yalta Conference and Potsdam Conference of 1945, the victorious Allies divided East Prussia into the two parts now known as Oblast Kaliningrad (in Russia) and the Warmian-Masurian Voivodeship (in Poland). The population was evacuated or fled the advancing Red Army in 1945. Prior to the Potsdam Conference, during the Soviet winter 1945 offensive (Vistula–Oder Offensive), the Red Army overran Warmia, killing and raping the local residents, without any regards towards age or ethnic origin. Following the Potsdam Conference agreements the Soviets and then the Polish administration expelled most of the remaining Warmian Germans to the Western part of Germany under the Allied rule, unless in the Polish voivodeship if they declared themselves Polish speakers. 
As a result of World War II, only a small minority of Germans of Poland remained in Warmia. The majority of present inhabitants of Warmia are descendants of Poles who either were Warmiaks or migrated there from other parts of Poland, including the pre-1939 Polish Borderlands, after 1945. Olsztyn is the largest city in Warmia and the capital of the Warmian-Masurian Voivedeship. During 1945-46, Warmia was part of the "Okreg Mazurski" (Masurian District). In 1946 a new voivodeship was created and named the Olsztyn Voivodeship, which encompassed both Warmia and Masurian counties. In 1975 this voivodeship was redistricted and survived in this form until the new redistricting and renaming in 1999 as Warmian-Masurian Voivodeship. The Catholic character of Warmia has been preserved in the architecture of its villages and towns, as well as in folk customs.

</doc>
<doc id="34002" url="http://en.wikipedia.org/wiki?curid=34002" title="William O'Dwyer">
William O'Dwyer

William O'Dwyer (July 11, 1890 – November 24, 1964) was the 100th Mayor of New York City, holding that office from 1946 to 1950.
Life and career.
O'Dwyer was born in Bohola, County Mayo, Ireland and studied at St. Nathys College, Ballaghaderreen, County Roscommon. He emigrated to the United States in 1910, after abandoning studies for the priesthood. He worked as a laborer, then as a New York City police officer, while studying law at night at Fordham University Law School. He received his degree in 1923 and then built up a successful practice before serving as a Kings County (Brooklyn) Court judge. He won election as the Kings County District Attorney in 1939 and his prosecution of the organized crime syndicate known as Murder, Inc. made him a national celebrity.
After losing the mayoral election to Fiorello La Guardia in 1941, O'Dwyer joined the United States Army for World War II, achieving the rank of brigadier general as a member of the Allied Commission for Italy and executive director of the War Refugee Board, for which he received the Legion of Merit.
In 1945, O'Dwyer received the nomination of Tammany Hall Leader Edward V. Loughlin and easily won the mayoral election. At his inauguration, O'Dwyer celebrated to the song, "It's a Great Day for the Irish," and addressed the 700 people gathered in Council Chambers at City Hall: "It is our high purpose to devote our whole time, our whole energy to do good work..." He established the Office of City Construction Coordinator, appointing Robert Moses to the post, worked to have the permanent home of the United Nations located in Manhattan, presided over the first billion-dollar New York City budget, created a traffic department and raised the subway fare from five cents to ten cents. In 1948, O'Dwyer received The Hundred Year Association of New York's Gold Medal Award "in recognition of outstanding contributions to the City of New York."
Shortly after his re-election to the mayoralty in 1949, O'Dwyer was confronted with a police corruption scandal uncovered by the Kings County District Attorney, Miles McDonald. O'Dwyer resigned from office on August 31, 1950. Upon his resignation, he was given a ticker tape parade up Broadway's Canyon of Heroes in the borough of Manhattan. President Harry Truman appointed him U.S. Ambassador to Mexico. He returned to New York City in 1951 to answer questions concerning his association with organized crime figures and the accusations followed him for the rest of his life. He resigned as Ambassador on December 6, 1952, but remained in Mexico until 1960.
He visited Israel for 34 days in 1951 on behalf of his Jewish constituents. He helped organize the first Israel Day Parade, along with New York's Jewish community.
Other.
His youngest sibling, his brother Paul (1907-1998), was also a New York politician, serving as the then-city-wide elected position of City Council President (1974–77).
Death.
William O'Dwyer died in New York City on November 24, 1964, in Beth Israel Hospital, aged 74, from heart failure and was interred at Arlington National Cemetery.

</doc>
<doc id="34004" url="http://en.wikipedia.org/wiki?curid=34004" title="Worms (series)">
Worms (series)

Worms is a series of artillery strategy computer games developed by British company Team17. Players control a small platoon of worms across a deformable landscape, battling other computer- or player-controlled teams. The games feature bright and humorous cartoon-style animation and a varied arsenal of bizarre weapons.
The game, whose concept was devised by Andy Davidson, was described by the Amiga gaming press as a cross between "Cannon Fodder" and "Lemmings". It is part of a wider genre of turn-based artillery games in which each player controls characters who duel with projectile weapons; similar games include "Scorched Earth" (1991), "Gorillas" (1991) and "Artillery Duel" (1983).
Games.
Spin-offs.
A number of "Worms"-themed spin-offs have also been released, including "Addiction Pinball" (1999), "Onlineworms" (2001), "Worms Blast" (2002), "Worms Golf" (2004) and "Worms Crazy Golf" (2011). "Worms Breakout" and "Worms Breakout 2", fangames based on the popular arcade game "Breakout", have been made available for download through the .
Clones and similar games.
Games that borrow from the "Worms" concept include Warmux and Hedgewars (open source, for Linux, Macintosh, Windows), "Hogs of War" (3D variation featuring pigs, for PlayStation and PC) and Snails for Pocket PC. Other games based on the "Worms" concept include "Liero", "Wurmz!" and "Gusanos", which make use of real-time rather than turn-based gameplay. Another game that follows the same format is Arcanists; focused around a wizard theme, was created by Jagex Games Studio, best known for their MMORPG "RuneScape". Very recently, Neuchatel Ltd. of London, UK released a futuristic version of Worms called Blast Angle. It was developed for the iPhone and iPad.
Collections.
A compilation, entitled "Worms United", was released in 1996 for DOS and included "Worms" and its expansion "Worms Reinforcements". A compilation, entitled "The Full Wormage", was later released in 1998 for DOS and Windows and included "Worms United", "Worms 2" and "Worms Pinball". On August 31, 2012, "Worms Collection" was released for PlayStation 3 and Xbox 360.
Gameplay.
Each player controls a team of several worms. During the course of the game, players take turns selecting one of their worms. They then use whatever tools and weapons are available to attack and kill the opponents' worms, thereby winning the game. Worms may move around the terrain in a variety of ways, normally by walking and jumping but also by using particular tools such as the "Bungee" and "Ninja Rope", to move to otherwise inaccessible areas. Each turn is time-limited to ensure that players do not hold up the game with excessive thinking or moving. The time limit can be modified in some of the games in the Worms series.
Over fifty weapons and tools may be available each time a game is played, and differing selections of weapons and tools can be saved into a "scheme" for easy selection in future games. Other scheme settings allow options such as deployment of reinforcement crates, from which additional weapons can be obtained, and sudden death where the game is rushed to a conclusion after a time limit expires. Some settings provide for the inclusion of objects such as land mines and explosive barrels.
When most weapons are used, they cause explosions that deform the terrain, creating circular cavities. The types of playable terrains include "island" (terrain floating on a body of water), or "cave" (cave with water at the bottom and terrain at both top and bottom of the screen that certain weapons such as "Air Strike" cannot go through; this type is not available in 3-D versions due to camera restrictions). If a worm is hit with a weapon, the amount of damage dealt to the worm will be removed from the worm's initial amount of health. The damage dealt to the attacked worm or worms after any player's turn is shown when all movement on the battlefield has ceased.
Worms die when one of the following situations occur:
Weapons and tools.
The "Worms" series is particularly notable for its extensive variety of weapons. With each new game that is released, new weapons are added, though many were removed in the 3D versions for gameplay reasons. As a result, the 2D series has accumulated 60 weapons, and the 3D series 40 weapons.
The weapons available in the game range from a standard timed grenade and homing missiles to exploding sheep and the highly destructive Banana Bomb (possible reference to the weapons in Gorillas game), both of which have appeared in every "Worms" game so far. More recently, the "Worms" series has seen weapons such as the iconic Holy Hand Grenade, the Priceless Ming Vase and the Inflatable Scouser.
Some of the bizarre weapons in a particular game are based on topical subjects at the time of the game's release. The Mail Strike, for example, which consists of a flying postbox dropping explosive envelopes, is a reference to the postal strikes of the time, while the Mad Cow refers to Britain's BSE epidemic of the 1990s. The French Nuclear Test, introduced in "Worms 2", was even updated to the Indian Nuclear Test in "Worms Armageddon" to keep with the times.
Other weapons are distinctly inside jokes. The MB Bomb, for example, which floats down from the sky and explodes on impact, is a cartoon caricature of Martyn Brown, Team17's studio director. Other such weapons include the "Concrete Donkey", one of the most powerful weapons in the game, which is based on a garden ornament in Andy Davidson's home garden, and an airstrike known in the game as Mike's Carpet Bomb was actually inspired by a store near the Team17 headquarters called "Mike's Carpets".
Since "Worms Armageddon", weapons that were intended to aid as utilities rather than damage-dealers were classified as tools. This classification mainly differs in the fact that they do not fall in ordinary weapon crates, and instead appear in toolboxes. Many tools were left in the wrong class for the sake of keyboard-shortcut conveniences. This was resolved in "Worms 3D".
Some weapons were inspired from popular Movies and TV programs, including the Holy Hand Grenade (from "Monty Python and the Holy Grail") and Ninja Rope (named the Bat Rope in early demos of the original game)
Audio.
One of the defining features of the "Worms" series is its light-hearted audio. Although the first few "Worms" games used darker, more authentic battlefield sounds for its ambient music, all of the games included a large number of high-pitched catchphrases shouted by the worms during the course of battle, such as "I'll get you!", "Revenge!", "Stupid!" and "Bombs away!".
"Worms & Reinforcements United" and its sequels gave players the ability to pick between a variety of speech sets (called "sound banks") for each platoon of worms. Many were based on regional accents, such as "The Raj" and "Angry Scots", while others, like "Drill Sergeant", made use of stereotypes. Players could even record their own speech sets and use those instead.
The ambient and theme music for "Worms 2", "Worms Armageddon", "Worms World Party" and, in part, "Worms 3D", was entirely provided by Bjørn Lynne.
The whole Worms series has a light-hearted themesong, known as the "Wormsong".
History.
The game was originally created by Andy Davidson as an entry for a Blitz BASIC programming competition run by the "Amiga Format" magazine, a cut-down version of the programming language having been covermounted previously. The game at this stage was called "Total Wormage" (possibly in reference to "Total Carnage") and it did not win the competition. Davidson sent the game to several publishers with no success. He then took the game to the European Computer Trade Show, where Team17 had a stand. Team17 made an offer on-the-spot to develop and publish the game.
It subsequently evolved into a full commercial game, renamed "Worms", available initially only for the Commodore Amiga computer. As the game was extremely popular, it was regularly released for other platforms including Windows- and Macintosh-based computers, Atari Jaguar, Mega Drive/Genesis, Dreamcast, Nintendo 64, Nintendo Game Boy, Game Boy Color and Game Boy Advance, Nintendo GameCube, Nokia N-Gage, SNES, PlayStation and PlayStation 2, Sega Saturn, Microsoft Pocket PC, and Xbox.
During the development of "Worms 2", Andy Davidson wrote "", a special edition produced exclusively for the Amiga. This was, in his eyes, the pinnacle of the series. Featuring weapons not seen in any "Worms" game before or since, it looks like an enhanced version of the original game. Only 5000 copies were ever sold. It was also the last version released for the Commodore Amiga platform from which the game originated.
New engine.
The engine was completely redesigned using Microsoft's DirectX for "Worms 2", dropping the darker tones of the first generation and adopting a more cartoonish look along the way-made possible by newer technology. "Worms 2" marked the first true step in the widespread Worm craze and characterised the direction which the series would take from then on. The second Worms version is by far the most customisable of the "Worms" games, with an extensive set of detailed settings and toggles. "Worms 2" also introduced internet play, which has since become a staple in the series. Worms 2 saw the return and enhancements of its predecessor's arsenal (e.g., the Banana Bomb -> Super Banana Bomb), as well as the addition of new weapons and tools. The game's interface is very dated by today's standards, resembling more of a generic Windows application than the colourful screens in later releases.
"Worms Armageddon" was initially intended to be released as an expansion pack for "Worms 2", but was released as a stand-alone game when it exceeded all expectations. "Worms Armageddon" included 33 in-depth missions in an extensive and elaborate campaign, along with training missions, a "deathmatch" feature, some new graphics and sounds, and a few new weapons and utilities. Much of the customization of "Worms 2" was removed, as Team17 thought that the interface would become cluttered and overwhelming.
"Worms Armageddon" also included a much more organized and functional internet play service, known as "WormNET", which required registration and provided leagues and ranks. Problems with cheating led to the removal of the leagues, but their re-introduction is planned in a series of updates that have provided the game with more customization. Other more subtle changes to the game include new physics to the ninja rope, and the removal of an in game glitch that allowed players to inflict huge damage to another worm, by aiming the mortar (a common weapon with high ammo) vertically above another player. The mortar shell would then return to earth and create a small but incredibly powerful explosion. In Worms Armageddon, the mortar shell would fall slightly to either side of the target worm if the same glitch was tried. Also, the booby-trapped crates were removed as Team 17 deemed them "unfair".
An official "Worms Armageddon" screensaver was included with a release bundling the title with "Addiction Pinball". The compilation, "The Armageddon Collection", is now out of print.
"Worms World Party" was originally designed for Dreamcast console to make use of its online capabilities, but was also released for the PlayStation and PC with new missions, a mission editor, and some extra customization. This was also released later in 2005 for the N-Gage Game Deck. A new feature, the WormPot, was added in all versions of the game except for the Dreamcast release, where it was omitted. With no new weapons, graphics or sounds.
The extensive customization of the 2D series, along with good online play support, has led to enduring popularity. A variety of unusual "schemes" have been developed by the WormNET community that are often played instead of the official schemes created by Team17. Some schemes have "rules" agreed to by the players but not enforced by the game itself.
Transition to 3D.
In 2003, Worms 3D was released for PlayStation 2, Nintendo GameCube, Macintosh, Windows, and Xbox. This was the first game in the series to bring the characters into a three-dimensional environment. It features a 'poxel' engine, described as a hybrid of polygons and voxels (the 3D analogues of pixels). This allows for pseudo-realistic terrain deformation similar in style to the 2D games, in which the terrain was represented by a bitmap.
The second 3D game in the series was , for PlayStation 2, Xbox and PC. It was released in November 2004 and features the biggest deviation from the traditional gameplay that the series has so far seen. Players' worms are able to build forts, and the objective of the game has shifted from simply killing the enemy worms, as players can now win a game by destroying the opponent's fort. Due to the change in strategy, this game could be seen more as a spin-off — though some aspects like the customizable costumes were carried into .
 was released in 2005. It was a revamp of the original Worms 3D engine, featuring smoother terrain deformation and improved graphics, resulting in a more polished feel closer to the second generation "Worms" games. The gameplay is much the same as it was in "Worms 3D", but new gameplay modes and weapons have been introduced, and the user interface has been improved and simplified. New features include the ability to select customized costumes for teams, and the ability to create custom weapons.
"", for the PlayStation Portable and the Nintendo DS, is specifically designed for the handheld systems and was released in March 2006. The game is considered to be a remake of the first "Worms" game, featuring enhanced graphics but no new weapons. The game has received mixed reviews.
"" for the PSP and DS is the sequel to "Worms: Open Warfare". It was released on August 31, 2007 in Europe, and it was released in the U.S. later on September 6, 2007.
"Worms" was developed by Team17 for release on Xbox Live Arcade. "Worms" was released on March 7, 2007. It was released on PSN on March 26, 2009 in the US and April 2009 in the UK.
" was developed by Team17 exclusively for Nintendo's Wii system, using a heavily modified version of the Worms: Open Warfare 2 engine. The game was released in March 2008, with a Sci-fi theme.
" was developed by Team17 for Xbox Live Arcade and PlayStation Network. The game is heavily inspired by Worms Armageddon's success, and tries to mimic the game's physics and several other aspects, such as the variety of available weapons. Several new weapons are also available, such as a "gas pump" which fills underground tunnels with poison gas.
' was developed and published by Team17 for the PC. It was released on 26 August 2010. It is an extended port version of the game ' which was released for the Xbox 360 and PlayStation 3, and so it has returned to the original 2D format, unlike the last three PC games.
" was developed by Team 17 and published by THQ and it was released on the Wii and PlayStation Portable.
" was released in 2011. It was a revamp of the original Worms 3D and engine, and it was released on Microsoft Windows, Xbox Live Arcade and PlayStation Network.
" was released in 2012. It was released on Microsoft Windows, Xbox Live Arcade and PlayStation Network. It was made as a 2.5D game, enabling players to play in a 3D world viewed from a 2D perspective. 
" was released in 2013 as a PC exclusive. 
"Worms Battlegrounds" was released in 2014 for the PlayStation 4 and Xbox One
Cancelled games.
In 2003, the company cancelled "Worms Battle Rally", a karting game that allowed for fragging opponents.
Worm Appearance.
The games in the Worms series had an almost realistic look for the worms in the cover, although in game-play, it is hard to point out the details due to the limitations of screen resolution. The appearance changed for Worms 2, which brought in a more cartoon look to the worms in the cover. The later "Worms" games retain this appearance with few changes. "" was the first game of the series to have team costumes.
Franchise awards.
Titles in the franchise have received a variety of awards.
Reception.
While the first versions of the game were generally praised, the series has since been criticized for the lack of meaningful additions. In 2001, Metacritic quoted "Worms World Party" reviews with comments like "it's virtually nothing more than an expansion pack for Worms Armageddon" and, as ActionTrip's Dejan Grbavcic put it, "And I thought that only Eidos was impertinent enough to keep selling the same game with a slightly different name...". In 2007, IGN included the "Worms" series in its list of game franchises that have jumped the shark.

</doc>
<doc id="34033" url="http://en.wikipedia.org/wiki?curid=34033" title="Wildebeest">
Wildebeest

The wildebeests, also called gnus, are a genus of antelopes, Connochaetes. They belong to the family Bovidae, which includes antelopes, cattle, goats, sheep and other even-toed horned ungulates. "Connochaetes" includes two species, both native to Africa: the black wildebeest, or white-tailed gnu ("C. gnou"); and the blue wildebeest, or brindled gnu ("C. taurinus"). Fossil records suggest these two species diverged about one million years ago, resulting in a northern and a southern species. The blue wildebeest remained in its original range and changed very little from the ancestral species, while the black wildebeest changed more in order to adapt to its open grassland habitat in the south. The most obvious way of telling the two species apart are the differences in their colouring and in the way their horns are orientated. Their main predators are lions, spotted hyenas, crocodiles, and African wild dogs.
In East Africa, the blue wildebeest is the most abundant big game species and some populations perform an annual migration to new grazing grounds but the black wildebeest is merely nomadic. Breeding in both takes place over a short period of time at the end of the rainy season and the calves are soon active and are able to move with the herd. Nevertheless, some fall prey to large carnivores. Wildebeest often graze in mixed herds with zebra which gives heightened awareness of potential predators. They are also alert to the warning signals emitted by other animals such as baboons. Wildebeest are a tourist attraction but compete with domesticated livestock for pasture and are sometimes blamed by farmers for transferring diseases and parasites to their cattle. Some illegal hunting goes on but the population trend is fairly stable and some populations are in national parks or on private land. The IUCN lists both species as being of "least concern".
Etymology.
The wildebeest ( or , plural wildebeest or wildebeests, wildebeesties (juv)), also called the gnu ( or ) is an antelope of the genus "Connochaetes". "Wildebeest" is Dutch for "wild beast" or "wild cattle" in Afrikaans ("bees" = cattle), while "Connochaetes" derives from the Greek words κόννος, "kónnos", "beard", and χαίτη, "khaítē", "flowing hair", "mane". Some sources claim the name "gnu" originates from the Khoikhoi name for these animals, "t'gnu". Others contend the name and its pronunciation in English go back to the word "!nu:" used for the black wildebeest among the Southern Bushmen, now generally referred to as the San people.
Classification.
Taxonomy and evolution.
The wildebeest, or the genus "Connochaetes", is placed under family Bovidae and subfamily Alcelaphinae, where its closest relatives are the hartebeest ("Alcelaphus" spp.), the hirola ("Beatragus hunteri") and species in the genus "Damaliscus", such as the topi, the tsessebe, the blesbok and the bontebok. The name "Connochaetes" was given by German zoologist Martin Hinrich Carl Lichtenstein in 1814. Wildebeest were first discovered about 1700 by Dutch settlers on their way to the interior of South Africa. Due to their resemblance to wild cattle, these people called them "wild ox" or "wildebeest". The black wildebeest was first known to westerners in the northern part of South Africa a century later, in the 1800s.
In the early twentieth century, one species of the wildebeest, "Connochaetes albojubatus", was identified in eastern Africa. In 1914, two separate races of the wildebeest were introduced, namely "Gorgon a. albojubatus" (Athi white-bearded wildebeest) and "G. a. mearnsi" (Loita white-bearded wildebeest). However, in 1939, the two were once again merged into a single race, "Connochaetes taurinus albojubatus". In the mid-twentieth century, two separate forms were recognised, "Gorgon taurinus hecki" and "G. t. albojubatus". Finally two distinct types of wildebeest - the blue and black wildebeest - were identified. The blue wildebeest was at first placed under a separate genus, "Gorgon", while the black wildebeest belonged to the genus "Connochaetes". Today they are united in the single genus "Connochaetes": the black wildebeest being named ("C. gnou") and the blue wildebeest, ("C. taurinus").
According to an mtDNA analysis, the black wildebeest seem to have diverged from the main lineage during the Middle Pleistocene and became a distinct species around a million years ago. A divergence rate of approximately 2% has been calculated. The split does not seem to have been driven by competition for resources but instead by the fact that each species adopted a different feeding niche and occupied a different trophic level.
Blue wildebeest fossils dating back some two and a half million years ago are common and widespread. They have been found in the fossil bearing caves at the Cradle of Humankind north of Johannesburg. Elsewhere in South Africa they are plentiful at such sites as Elandsfontein, Cornelia and Florisbad. The earliest fossils of the black wildebeest were found in sedimentary rock in Cornelia in the Orange Free State and dated back about eight hundred thousand years. Today, five subspecies of the blue wildebeest are recognised while the black wildebeest has no named subspecies.
Genetics and hybrids.
The diploid number of chromosomes in the wildebeest is 58. Chromosomes were studied in a male and a female wildebeest. In the female, all except a pair of very large submetacentric chromosomes were found to be acrocentric. Metaphases were studied in the male's chromosomes, and very large submetacentric chromosomes were found there as well, similar to those in the female both in size and morphology. Other chromosomes were acrocentric. The X chromosome is a large acrocentric and the Y chromosome a minute one.
The two species of the wildebeest are known to hybridise. Male black wildebeest have been reported to mate with female blue wildebeest and vice versa. The differences in social behaviour and habitats have historically prevented interspecific hybridisation between the species, however hybridisation may occur when they are both confined within the same area. The resulting offspring is usually fertile. A study of these hybrid animals at Spioenkop Dam Nature Reserve in South Africa revealed that many had disadvantageous abnormalities relating to their teeth, horns and the wormian bones in the skull. Another study reported an increase in the size of the hybrid as compared to either of its parents. In some animals the auditory bullae are highly deformed and in others the radius and ulna are fused.
Differences between species.
Both species of wildebeest are even-toed, horned, greyish-brown ungulates resembling cattle. Males are larger than females and both have heavy forequarters compared to their hindquarters. They have broad muzzles, Roman noses, shaggy manes and tails. The most striking morphological differences between the black and blue wildebeest are the orientation and curvature of their horns and the color of their coats. The blue wildebeest is the bigger of the two species. In males, blue wildebeest stand 150 cm tall at the shoulder and weigh around 250 kg, while the black wildebeest stands 111 to 120 cm tall and weighs about 180 kg. In females, blue wildebeest have a shoulder height of 135 cm and weigh 180 kg while black wildebeest females stand 108 cm at the shoulder and weigh 155 kg. The horns of blue wildebeest protrude to the side then curve downwards before curving up back towards the skull, while the horns of the black wildebeest curve forward then downward before curving upwards at the tips. Blue wildebeest tend to be a dark grey color with stripes, but may have a bluish sheen. The black wildebeest has brown-coloured hair, with a mane that ranges in color from cream to black, and a cream-coloured tail. The blue wildebeest lives in a wide variety of habitats, including woodlands and grasslands, while the black wildebeest tends to reside exclusively in open grassland areas. In some areas the blue wildebeest migrates over long distances in the winter, whereas the black wildebeest does not. The milk of the female black wildebeest contains a higher protein, lower fat, and lower lactose content than the milk of the blue wildebeest. Wildebeest can live more than forty years, though their average lifespan is around twenty years.
Distribution and habitat.
Wildebeest inhabit the plains and open woodlands of parts of Africa south of the Sahara. The black wildebeest is native to the southernmost parts of the continent. Its historical range included South Africa, Swaziland and Lesotho, but in the latter two countries it was hunted to extinction in the 19th century. It has now been reintroduced to them and also introduced to Namibia where it has become well established. It inhabits open plains, grasslands and Karoo shrublands in both steep mountainous regions and lower undulating hills at altitudes varying between 1350 and. In the past, it inhabited the highveld temperate grasslands during the dry winter season and the arid Karoo region during the rains. However, as a result of widespread hunting, it no longer occupies its historical range or makes migrations, and is now largely limited to game farms and protected reserves.
The blue wildebeest is native to eastern and southern Africa. Its range includes Kenya, Tanzania, Botswana, Zambia, Zimbabwe, Mozambique, South Africa, Swaziland and Angola. It is no longer found in Malawi but has been successfully reintroduced into Namibia. Blue wildebeest are mainly found in short grass plains bordering bush-covered acacia savannas, thriving in areas that are neither too wet nor too dry. They can be found in habitats that vary from overgrazed areas with dense bush to open woodland floodplains. In East Africa, the blue wildebeest is the most abundant big game species, both in population and biomass. It is a notable feature of the Serengeti National Park in Tanzania, the Masai Mara Game Reserve in Kenya and the Liuwa Plain National Park in Zambia.
Migration.
Not all wildebeest are migratory. Black wildebeest herds are often nomadic or may have a regular home range of 1 km2. Bulls may occupy territories, usually about 100 to apart, but this spacing varies according to the quality of the habitat. In favourable conditions they may be as close as 9 m or as far apart as 1600 m in poor habitat. Female herds have home ranges of about 250 acre in size. Herds of non-territorial batchelor males roam at will and do not seem to have a home range.
In the Masai Mara game reserve, there is a non-migratory population of blue wildebeest which had dwindled from about 119,000 animals in 1977 to about 22,000 in 1997. The reason for the decline is thought to be the increasing competition between cattle and wildebeest for a diminishing area of grazing land as a result of changes in agricultural practices, and possibly fluctuations in rainfall.
Each year, some East African populations of blue wildebeest have a long-distance migration, seemingly timed to coincide with the annual pattern of rainfall and grass growth. The timing of their migrations in both the rainy and dry seasons can vary considerably (by months) from year to year. At the end of the rainy season (May or June in East Africa), wildebeest migrate to dry-season areas in response to a lack of surface (drinking) water. When the rainy season begins again (months later), animals quickly move back to their wet-season ranges. Factors suspected to affect migration include food abundance, surface water availability, predators, and phosphorus content in grasses. Phosphorus is a crucial element for all life forms, particularly for lactating female bovids. As a result during the rainy season, wildebeest select grazing areas that contain particularly high phosphorus levels. One study found, in addition to phosphorus, wildebeest select ranges containing grass with relatively high nitrogen content.
Numerous documentaries feature wildebeest crossing rivers, with many being eaten by crocodiles or drowning in the attempt. While having the appearance of a frenzy, recent research has shown a herd of wildebeest possesses what is known as a "swarm intelligence", whereby the animals systematically explore and overcome the obstacle as one. Major predators that feed on wildebeest include the lion, hyena, cheetah, leopard, and crocodile, which seem to favour the wildebeest. Wildebeest, however, are very strong, and can inflict considerable injury even to a lion. Wildebeest have a maximum running speed of around 80 km/h. The primary defensive tactic is herding, where the young animals are protected by the older, larger ones, while the herd runs as a group. Typically, the predators attempt to cut out a young or ill animal and attack without having to worry about the herd. Wildebeest have developed additional sophisticated cooperative behaviours, such as animals taking turns sleeping while others stand guard against a night attack by invading predators. Wildebeest migrations are closely followed by vultures, as wildebeest carcasses are an important source of food for these scavengers. The vultures consume about 70% of the wildebeest carcasses available. Decreases in the number of migrating wildebeest have also had a negative effect on the vultures. In the Serengeti ecosystem, Tanzania, wildebeest may help facilitate the migration of other, smaller-bodied grazers, such as Thomson's gazelles ("Eudorcas thomsonii"), which eat the new-growth grasses stimulated by wildebeest foraging.
Ecology.
Interactions with nonpredators.
Zebras and wildebeest group together in open savannah environments with high chances of predation. This grouping strategy reduces predation risk because larger groups decrease each individual’s chance of being hunted, and predators are more easily seen in open areas.
Wildebeest can also listen in on the alarm calls of other species, and by doing so can reduce their risk of predation. One study showed, along with other ungulates, wildebeests responded more strongly to the baboon alarm calls compared to the baboon contest calls, though both types of calls had similar patterns, amplitudes, and durations. The alarm calls were a response of the baboons to lions, and the contest calls were recorded when a dispute between two males occurred.
Breeding and reproduction.
Wildebeest do not form permanent pair bonds and during the mating season, or rut, the males establish temporary territories and try to attract females into them. These small territories are about 3000 m2, with up to 300 territories per km2. The males defend these territories from other males while competing for females that are coming into season. The males use grunts and distinctive behaviour to entice females into their territories. Wildebeest usually breed at the end of the rainy season when the animals are well fed and at their peak of fitness. This usually occurs between May and July, and birthing usually takes place between January and March, at the start of the wet season. Wildebeest females breed seasonally and ovulate spontaneously. The estrous cycle is about 23 days and the gestation period lasts 250 to 260 days. The calves weigh about 21 kg at birth and scramble to their feet within minutes, being able to move with the herd in a matter of days. Groups of wildebeest females and young live in the small areas established by the male. When groups of wildebeest join together, the female to male ratio is higher because the females choose to move to the areas held by a smaller number of males. This female-dominated sex ratio may be due to illegal hunting and human disturbance as higher male mortality has been attributed to hunting.
Threats and conservation.
Today many wildebeest populations are experiencing rapid declines. Overland migration as a biological process requires large connected landscapes, which are increasingly difficult to maintain, particularly over the long term, when human demands on the landscape compete, as well. The most acute threat comes from migration barriers, such as fences and roads. In one of the more striking examples of the consequences of fence-building on terrestrial migrations, Botswanan authorities placed thousands of kilometres of fences across the Kalahari that prevented wildebeests from reaching watering holes and grazing grounds, resulting in the deaths of tens of thousands of individuals, reducing the wildebeest population to less than 10% of its previous size. Illegal hunting is a major conservation concern in many areas, along with natural threats posed by main predators (such as lions, leopards, hunting dogs and hyenas). Where the black and blue wildebeest share a common range, the two can hybridise, and this is regarded as a potential threat to the black wildebeest.
The black wildebeest has been classified as of "Least Concern" by the International Union for Conservation of Nature (IUCN), in its Red List of Threatened Species. The populations of this species are on an increase. There are now believed to be more than 18,000 individuals, 7,000 of which are in Namibia, outside its natural range, and where it is farmed. Around 80% of the wildebeest occur in private areas, while the other 20% are confined in protected areas. Its introduction into Namibia has been a success and numbers have increased substantially there from 150 in 1982 to 7,000 in 1992.
The blue wildebeest has also been rated as being of "Least Concern". The population trend is stable, and their numbers are estimated to be around 1,500,000 - mainly due to the increase of the populations in Serengeti National Park (Tanzania) to 1,300,000. However, the numbers of one of the subspecies, the Eastern white-bearded wildebeest ("C. t. albojubatus") have seen a steep decline. Population density ranges from 0.15/km2. in Hwange and Etosha National Parks to 35/km2. in Ngorongoro Crater and Serengeti National Park.
Uses and interaction with humans.
Wildebeest provide several useful animal products. The hide makes good quality leather and the flesh is coarse, dry and rather hard. Wildebeest are killed for food, especially to make biltong in Southern Africa. This dried game meat is a delicacy and an important food item in Africa. The meat of females is more tender than that of males, and is the most tender during the autumn season. Wildebeest are a regular target for illegal meat hunters because their numbers make them easy to find. Cooks preparing the wildebeest carcass usually cut it into 11 pieces. The estimated price for wildebeest meat was about US$0.47 per kilogram around 2008. The silky, flowing tail of the black wildebeest is used to make fly-whisks or "chowries".
The wildebeest benefit the ecosystem by increasing soil fertility with their excreta. Now they are economically important for human beings as they are a major tourist attraction. They also provide important products like leather to humans. The wildebeest, however, can also have a negative impact on humans. Wild individuals can be competitors of commercial livestock, and can transmit fatal diseases like rinderpest and cause epidemics among animals, particularly domestic cattle. They can also spread ticks, lungworms, tapeworms, flies and paramphistome flukes.
The black wildebeest is depicted on the coat of arms of the Province of Natal in South Africa. Over the years the South African authorities have issued several stamps displaying the animal and the South African Mint has struck a two cent piece with a prancing black wildebeest.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="34035" url="http://en.wikipedia.org/wiki?curid=34035" title="Web design">
Web design

Web design encompasses many different skills and disciplines in the production and maintenance of websites. The different areas of web design include web graphic design; interface design; authoring, including standardised code and proprietary software; user experience design; and search engine optimization. Often many individuals will work in teams covering different aspects of the design process, although some designers will cover them all. The term web design is normally used to describe the design process relating to the front-end (client side) design of a website including writing mark up. Web design partially overlaps web engineering in the broader scope of web development. Web designers are expected to have an awareness of usability and if their role involves creating mark up then they are also expected to be up to date with web accessibility guidelines.
History.
1988—2001.
Although web design has a fairly recent history, it can be linked to other areas such as graphic design. However web design can also be seen from a technological standpoint. It has become a large part of people’s everyday lives. It is hard to imagine the Internet without animated graphics, different styles of typography, background and music.
The start of the web and web design.
In 1989, whilst working at CERN Tim Berners-Lee proposed to create a global hypertext project, which later became known as the World Wide Web. During 1991 to 1993 the World Wide Web was born. Text-only pages could be viewed using a simple line-mode browser. In 1993 Marc Andreessen and Eric Bina, created the Mosaic browser. At the time there were multiple browsers, however the majority of them were Unix-based and naturally text heavy. There had been no integrated approach to graphic design elements such as images or sounds. The Mosaic browser broke this mould. The W3C was created in October 1994 to "lead the World Wide Web to its full potential by developing common protocols that promote its evolution and ensure its interoperability." This discouraged any one company from monopolizing a propriety browser and programming language, which could have altered the effect of the World Wide Web as a whole. The W3C continues to set standards, which can today be seen with JavaScript. In 1994 Andreessen formed Communications Corp. that later became known as Netscape Communications, the Netscape 0.9 browser. Netscape created its own HTML tags without regard to the traditional standards process. For example, Netscape 1.1 included tags for changing background colours and formatting text with tables on web pages. Throughout 1996 to 1999 the browser wars began, as Microsoft and Netscape fought for ultimate browser dominance. During this time there were many new technologies in the field, notably Cascading Style Sheets, JavaScript, and Dynamic HTML. On the whole, the browser competition did lead to many positive creations and helped web design evolve at a rapid pace.
Evolution of web design.
In 1996, Microsoft released its first competitive browser, which was complete with its own features and tags. It was also the first browser to support style sheets, which at the time was seen as an obscure authoring technique. The HTML markup for tables was originally intended for displaying tabular data. However designers quickly realized the potential of using HTML tables for creating the complex, multi-column layouts that were otherwise not possible. At this time, as design and good aesthetics seemed to take precedence over good mark-up structure, and little attention was paid to semantics and web accessibility. HTML sites were limited in their design options, even more so with earlier versions of HTML. To create complex designs, many web designers had to use complicated table structures or even use blank spacer .GIF images to stop empty table cells from collapsing. CSS was introduced in December 1996 by the W3C to support presentation and layout. This allowed HTML code to be semantic rather than both semantic and presentational, and improved web accessibility, see tableless web design.
In 1996, Flash (originally known as FutureSplash) was developed. At the time, the Flash content development tool was relatively simple compared to now, using basic layout and drawing tools, a limited precursor to ActionScript, and a timeline, but it enabled web designers to go beyond the point of HTML, animated GIFs and JavaScript. However, because Flash required a plug-in, many web developers avoided using it for fear of limiting their market share due to lack of compatibility. Instead, designers reverted to gif animations (if they didn't forego using motion graphics altogether) and JavaScript for widgets. But the benefits of Flash made it popular enough among specific target markets to eventually work its way to the vast majority of browsers, and powerful enough to be used to develop entire sites.
End of the first browser wars.
During 1998 Netscape released Netscape Communicator code under an open source licence, enabling thousands of developers to participate in improving the software. However, they decided to start from the beginning, which guided the development of the open source browser and soon expanded to a complete application platform. The Web Standards Project was formed and promoted browser compliance with HTML and CSS standards by creating Acid1, Acid2, and Acid3 tests. 2000 was a big year for Microsoft. Internet Explorer was released for Mac; this was significant as it was the first browser that fully supported HTML 4.01 and CSS 1, raising the bar in terms of standards compliance. It was also the first browser to fully support the PNG image format. During this time Netscape was sold to AOL and this was seen as Netscape’s official loss to Microsoft in the browser wars.
2001—2012.
Since the start of the 21st century the web has become more and more integrated into peoples lives. As this has happened the technology of the web has also moved on. There have also been significant changes in the way people use and access the web, and this has changed how sites are designed.
Modern browsers.
Since the end of "the browsers wars" there have been new browsers coming onto the scene. Many of these are open source meaning that they tend to have faster development and are more supportive of new standards. The new options are considered by many to be better than Microsoft's Internet Explorer.
New standards.
The W3C has released new standards for HTML (HTML5) and CSS (CSS3), as well as new JavaScript API's, each as a new but individual standard. However, while the term HTML5 is only used to refer to the new version of HTML and "some" of the JavaScript API's, it has become common to use it to refer to the entire suite of new standards (HTML5, CSS3 and JavaScript).
Tools and technologies.
Web designers use a variety of different tools depending on what part of the production process they are involved in. These tools are updated over time by newer standards and software but the principles behind them remain the same. Web graphic designers use vector and raster graphics packages to create web-formatted imagery or design prototypes. Technologies used to create websites include standardised mark-up, which can be hand-coded or generated by WYSIWYG editing software. There is also proprietary software based on plug-ins that bypasses the client’s browser versions. These are often WYSIWYG but with the option of using the software’s scripting language. Search engine optimisation tools may be used to check search engine ranking and suggest improvements.
Other tools web designers might use include mark up validators and other testing tools for usability and accessibility to ensure their web sites meet web accessibility guidelines.
Skills and techniques.
Marketing and communication design.
Marketing and communication design on a website may identify what works for its target market. This can be an age group or particular strand of culture; thus the designer may understand the trends of its audience. Designers may also understand the type of website they are designing, meaning, for example, that (B2B) business-to-business website design considerations might differ greatly from a consumer targeted website such as a retail or entertainment website. Careful consideration might be made to ensure that the aesthetics or overall design of a site do not clash with the clarity and accuracy of the content or the ease of web navigation, especially on a B2B website. Designers may also consider the reputation of the owner or business the site is representing to make sure they are portrayed favourably.
User experience design and interactive design.
User understanding of the content of a website often depends on user understanding of how the website works. This is part of the user experience design. User experience is related to layout, clear instructions and labeling on a website. How well a user understands how they can interact on a site may also depend on the interactive design of the site. If a user perceives the usefulness of the website, they are more likely to continue using it. Users who are skilled and well versed with website use may find a more unique, yet less intuitive or less user-friendly website interface useful nonetheless. However, users with less experience are less likely to see the advantages or usefulness of a less intuitive website interface. This drives the trend for a more universal user experience and ease of access to accommodate as many users as possible regardless of user skill. Much of the user experience design and interactive design are considered in the user interface design.
Advanced interactive functions may require plug-ins if not advanced coding language skills. Choosing whether or not to use interactivity that requires plug-ins is a critical decision in user experience design. If the plug-in doesn't come pre-installed with most browsers, there's a risk that the user will have neither the know how or the patience to install a plug-in just to access the content. If the function requires advanced coding language skills, it may be too costly in either time or money to code compared to the amount of enhancement the function will add to the user experience. There's also a risk that advanced interactivity may be incompatible with older browsers or hardware configurations. Publishing a function that doesn't work reliably is potentially worse for the user experience than making no attempt. It depends on the target audience if it's likely to be needed or worth any risks.
Page layout.
Part of the user interface design is affected by the quality of the page layout. For example, a designer may consider whether the site's page layout should remain consistent on different pages when designing the layout. Page pixel width may also be considered vital for aligning objects in the layout design. The most popular fixed-width websites generally have the same set width to match the current most popular browser window, at the current most popular screen resolution, on the current most popular monitor size. Most pages are also center-aligned for concerns of aesthetics on larger screens.
Fluid layouts increased in popularity around 2000 as an alternative to HTML-table-based layouts and grid-based design in both page layout design principle and in coding technique, but were very slow to be adopted. This was due to considerations of screen reading devices and varying windows sizes which designers have no control over. Accordingly, a design may be broken down into units (sidebars, content blocks, embedded advertising areas, navigation areas) that are sent to the browser and which will be fitted into the display window by the browser, as best it can. As the browser does recognize the details of the reader's screen (window size, font size relative to window etc.) the browser can make user-specific layout adjustments to fluid layouts, but not fixed-width layouts. Although such a display may often change the relative position of major content units, sidebars may be displaced below body text rather than to the side of it. This is a more flexible display than a hard-coded grid-based layout that doesn't fit the device window. In particular, the relative position of content blocks may change while leaving the content within the block unaffected. This also minimizes the user's need to horizontally scroll the page.
Responsive Web Design is a newer approach, based on CSS3, and a deeper level of per-device specification within the page's stylesheet through an enhanced use of the CSS codice_1 rule.
Typography.
Web designers may choose to limit the variety of website typefaces to only a few which are of a similar style, instead of using a wide range of typefaces or type styles. Most browsers recognize a specific number of safe fonts, which designers mainly use in order to avoid complications.
Font downloading was later included in the CSS3 fonts module and has since been implemented in Safari 3.1, Opera 10 and Mozilla Firefox 3.5. This has subsequently increased interest in web typography, as well as the usage of font downloading.
Most site layouts incorporate negative space to break the text up into paragraphs and also avoid center-aligned text.
Motion graphics.
The page layout and user interface may also be affected by the use of motion graphics. The choice of whether or not to use motion graphics may depend on the target market for the website. Motion graphics may be expected or at least better received with an entertainment-oriented website. However, a website target audience with a more serious or formal interest (such as business, community, or government) might find animations unnecessary and distracting if only for entertainment or decoration purposes. This doesn't mean that more serious content couldn't be enhanced with animated or video presentations that is relevant to the content. In either case, motion graphic design may make the difference between more effective visuals or distracting visuals.
Quality of code.
Website designers may consider it to be good practice to conform to standards. This is usually done via a description specifying what the element is doing. Failure to conform to standards may not make a website unusable or error prone, but standards can relate to the correct layout of pages for readability as well making sure coded elements are closed appropriately. This includes errors in code, more organized layout for code, and making sure IDs and classes are identified properly. Poorly-coded pages are sometimes colloquially called tag soup. Validating via W3C can only be done when a correct DOCTYPE declaration is made, which is used to highlight errors in code. The system identifies the errors and areas that do not conform to web design standards. This information can then be corrected by the user.
Homepage design.
Usability experts, including Jakob Nielsen and Kyle Soucy, have often emphasised homepage design for website success and asserted that the homepage is the most important page on a website. However practitioners into the 2000s were starting to find that a growing number of website traffic was bypassing the homepage, going directly to internal content pages through search engines, e-newsletters and RSS feeds. Leading many practitioners to argue that homepages are less important than most people think. Jared Spool argued in 2007 that a site's homepage was actually the least important page on a website.
In 2012 and 2013, carousels (also called 'sliders' and 'rotating banners') have become an extremely popular design element on homepages, often used to showcase featured or recent content in a confined space. Many practitioners argue that carousels are an ineffective design element and hurt a website's search engine optimisation and usability.
Occupations.
There are two primary jobs involved in creating a website: the web designer and web developer, who often work closely together on a website. The web designers are responsible for the visual aspect, which includes the layout, coloring and typography of a web page. Web designers will also have a working knowledge of using a variety of languages such as HTML, CSS, JavaScript, PHP and Flash to create a site, although the extent of their knowledge will differ from one web designer to another. Particularly in smaller organizations one person will need the necessary skills for designing and programming the full web page, while larger organizations may have a web designer responsible for the visual aspect alone.
Further jobs which may become involved in the creation of a website include: 
References and further reading.
</dl>

</doc>
<doc id="34036" url="http://en.wikipedia.org/wiki?curid=34036" title="Area rule">
Area rule

The Whitcomb area rule, also called the transonic area rule, is a design technique used to reduce an aircraft's drag at transonic and supersonic speeds, particularly between Mach 0.75 and 1.2. 
This is one of the most important operating speed ranges for commercial and military fixed-wing aircraft today, with transonic acceleration being considered an important performance metric for combat aircraft and necessarily dependent upon transonic drag.
Description.
At high-subsonic flight speeds, the local speed of the airflow can reach the speed of sound where the flow accelerates around the aircraft body and wings. The speed at which this development occurs varies from aircraft to aircraft and is known as the critical Mach number. The resulting shock waves formed at these points of sonic flow can greatly reduce power, which is experienced by the aircraft as a sudden and very powerful drag, called wave drag. To reduce the number and power of these shock waves, an aerodynamic shape should change in cross sectional area as smoothly as possible.
The area rule says that two airplanes with the same longitudinal cross-sectional area distribution have the same wave drag, independent of how the area is distributed laterally (i.e. in the fuselage or in the wing). Furthermore, to avoid the formation of strong shock waves, this total area distribution must be smooth. As a result, aircraft have to be carefully arranged so that at the location of the wing, the fuselage is narrowed or "waisted", so that the total area doesn't change much. Similar but less pronounced fuselage waisting is used at the location of a bubble canopy and perhaps the tail surfaces.
The area rule also holds true at speeds exceeding the speed of sound, but in this case the body arrangement is in respect to the Mach line for the design speed. For example, consider that at Mach 1.3 the angle of the Mach cone formed off the body of the aircraft will be at about μ = arcsin(1/M) = 50.3° (μ is the angle of the Mach cone, or simply Mach angle). In this case the "perfect shape" is biased rearward; therefore, aircraft designed for high speed cruise usually have wings towards the rear. A classic example of such a design is the Concorde. When applying the transonic area rule, the condition that the plane defining the cross-section meets the longitudinal axis at the Mach angle μ no longer prescribes a unique plane for μ other than the 90° given by M = 1. The correct procedure is to average over all possible orientations of the intersecting plane.
Sears-Haack body.
A superficially related concept is the Sears–Haack body, which is the shape with the minimum wave drag for a given length and a given volume. However, the Sears-Haack body shape is derived starting with the Prandtl-Glauert equation which governs small-disturbance supersonic flows. But this equation is not valid for transonic flows where the Area rule applies. So although the Sears-Haack body shape, being smooth, will have favorable wave drag properties according to the Area rule, it is not theoretically optimum.
History.
Germany.
The area rule was discovered by Otto Frenzl when comparing a swept wing with a w-wing with extreme high wave drag while working on a transonic wind tunnel at Junkers works in Germany between 1943 and 1945. He wrote a description on 17 December 1943, with the title "Arrangement of Displacement Bodies in High-Speed Flight"; this was used in a patent filed in 1944. The results of this research were presented to a wide circle in March 1944 by Theodor Zobel at the "Deutsche Akademie der Luftfahrtforschung" (German Academy of Aeronautics Research) in the lecture "Fundamentally new ways to increase performance of high speed aircraft." 
Subsequent German wartime aircraft design took account of the discovery, evident in the slim mid-fuselage of aircraft such as the Messerschmitt P.1112, P.1106, and the indisputably wasp-waisted Focke-Wulf 1000x1000x1000 type A long range bomber, but also apparent in delta wing designs like the Henschel Hs P.135. Several other researchers came close to developing a similar theory, notably Dietrich Küchemann who designed a tapered fighter that was dubbed the "Küchemann Coke Bottle" when it was discovered by US forces in 1946. In this case Küchemann arrived at the solution by studying airflow, notably spanwise flow, over a swept wing. The swept wing is already an indirect application of the area rule.
United States.
Wallace D. Hayes, a pioneer of supersonic flight, developed the transonic area rule in publications beginning in 1947 with his Ph.D. thesis at the California Institute of Technology.
Richard T. Whitcomb, after whom the rule is named, independently discovered this rule in 1952, while working at the NACA. While using the new Eight-Foot High-Speed Tunnel, a wind tunnel with performance up to Mach 0.95 at NACA's Langley Research Center, he was surprised by the increase in drag due to shock wave formation. Whitcomb realized that, for analytical purposes, an airplane could be reduced to a streamlined body of revolution, elongated as much as possible to mitigate abrupt discontinuities and, hence, equally abrupt drag rise. The shocks could be seen using Schlieren photography, but the reason they were being created at speeds far below the speed of sound, sometimes as low as Mach 0.70, remained a mystery. 
In late 1951, the lab hosted a talk by Adolf Busemann, a famous German aerodynamicist who had moved to Langley after World War II. He talked about the difference in the behavior of airflow as speeds approached the critical Mach number, where the air no longer behaved as an incompressible fluid. Whereas engineers were used to thinking of air flowing smoothly around the body of the aircraft, at high speeds it simply did not have time to "get out of the way", and instead started to flow as if it were rigid pipes of flow, a concept Busemann referred to as "streampipes", as opposed to streamlines, and jokingly suggested that engineers had to consider themselves "pipefitters".
Several days later Whitcomb had a "Eureka" moment. The reason for the high drag was that the "pipes" of air were interfering with each other in three dimensions. One does not simply consider the air flowing over a 2D cross-section of the aircraft as others could in the past; now they also had to consider the air to the "sides" of the aircraft which would also interact with these streampipes. Whitcomb realized that the shaping had to apply to the aircraft "as a whole", rather than just to the fuselage. That meant that the extra cross-sectional area of the wings and tail had to be accounted for in the overall shaping, and that the fuselage should actually be narrowed where they meet to more closely match the ideal.
Applications.
The area rule was immediately applied to a number of development efforts. One of the most famous was Whitcomb's personal work on the re-design of the Convair F-102 Delta Dagger, a U.S. Air Force jet fighter that was demonstrating performance considerably worse than expected. By indenting the fuselage beside the wings, and (paradoxically) adding more volume to the rear of the plane, transonic drag was considerably reduced and the intended Mach 1.2 design speed was reached. The culminating design of this research was the Convair F-106 Delta Dart, an aircraft which for many years was the USAF's primary all-weather interceptor.
Numerous designs of the era were likewise modified in this fashion, either by adding new fuel tanks or tail extensions to smooth out the profile. The Tupolev Tu-95 'Bear', a Soviet-era bomber, has large bulged landing gear nacelles behind the two inner engines, increasing the aircraft's overall cross section aft of the wing root. Its airliner version has been the fastest propeller-driven aircraft in the world since 1960. The Convair 990 used a similar solution, adding bumps called antishock bodies to the trailing edge of the upper wing. The 990 remains the fastest U.S. airliner in history, cruising at up to Mach 0.89. Designers at Armstrong-Whitworth took the concept a step further in their proposed M-Wing, in which the wing was first swept forward and then to the rear. This allowed the fuselage to be narrowed on either side of the root instead of just behind it, leading to a smoother fuselage that remained wider on average than one using a classic swept wing.
One interesting outcome of the area rule is the shaping of the Boeing 747's upper deck. The aircraft was designed to carry standard intermodal containers in a two-wide, two-high stack on the main deck, which was considered a serious accident risk for the pilots if they were located in a cockpit at the front of the aircraft. They were instead moved above the deck in a small "hump", which was designed to be as small as possible given normal streamlining principles. It was later realized that the drag could be reduced much more by lengthening the hump, using it to reduce wave drag offsetting the tail surface's contribution. The new design was introduced on the 747-300, improving its cruise speed and lowering drag, with the side effect of slightly increasing capacity on passenger flights.
Aircraft designed according to Whitcomb's area rule (such as the Blackburn Buccaneer) looked odd at the time they were first tested and were dubbed "flying Coke bottles," but the area rule is effective and came to be an expected part of the appearance of any transonic aircraft. Later designs started with the area rule in mind, and came to look much more pleasing. Although the rule still applies, the visible fuselage "waisting" can only be seen on a few aircraft, such as the B-1B Lancer, Learjet 60, and the Tupolev Tu-160 'Blackjack'. The same effect is now achieved by careful positioning of aircraft components, like the boosters and cargo bay on rockets; the jet engines in front of (and not directly below) the wings of the Airbus A380; the jet engines behind (and not purely at the side of) the fuselage of a Cessna Citation X; the shape and location of the canopy on the F-22 Raptor; and the image of the Airbus A380 above showing obvious area rule shaping at the wing root, which is practically invisible from any other angle.

</doc>
<doc id="34038" url="http://en.wikipedia.org/wiki?curid=34038" title="Word grammar">
Word grammar

Word grammar has been developed by Richard Hudson since the 1980s. It started as a model of syntax, whose most distinctive characteristic is its use of dependency grammar, an approach to syntax in which the sentence's structure is almost entirely contained in the information about individual words, and syntax is seen as consisting primarily of principles for combining words. The central syntactic relation is that of dependency between words; constituent structure is not recognized except in the special case of coordinate structures. 
However an even more important claim of Word Grammar is that statements about words and their properties form a complex network of propositions. More recent work on Word Grammar cites neurocognitive linguistics as a source of inspiration for the idea that language is nothing but a network. One of the attractions of the network view is the possibility of analysing language in the same way as other kinds of knowledge, given that knowledge, or long-term memory, is widely considered to be a network. 
Word grammar is an example of cognitive linguistics, which models language as part of general knowledge and not as a specialised mental faculty. This is in contrast to the nativism of Noam Chomsky and his students. 
External links.
<br>

</doc>
<doc id="34039" url="http://en.wikipedia.org/wiki?curid=34039" title="West Pakistan">
West Pakistan

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1955–1957
 |- class="mergedrow"
 |style="width:1.0em; padding:0 0 0 0.6em;"| - ||style="padding-left:0;text-align:left;"|1955–1957|| 
 | +92
 |  Pakistan
West Pakistan (Urdu: مغربی پاکستان‎, "Mag̱ẖribī Pākistān" ]; Bengali: পশ্চিম পাকিস্তান, "Pôścim Pākistān") was one of the two exclaves created at the formation of the modern State of Pakistan following the 1947 Partition of India.
After gaining independence from the British in 1947, the State of Pakistan was physically separated into two exclaves, with the western and eastern wings separated from each other by the Republic of India. The western wing of Pakistan comprised three Governor's provinces (North-West Frontier, West-Punjab and Sindh Province), one Chief Commissioner's province (Baluchistan Province), and the Baluchistan States Union along with several other independent princely states (notably Bahawalpur, Chitral, Dir, Hunza, Khairpur and Swat), the Federal Capital Territory around Karachi, and the tribal areas. The eastern wing of the new country – East Pakistan – formed the single province of East Bengal (including the former Assam district of Sylhet).
West Pakistan adopted the stance that West Pakistan was the true Pakistan, with East Pakistan as a provincial dominion. The western wing was politically dominant despite East Pakistan having over half of the population and a disproportionately small number of seats in the Constituent Assembly. This inequality of the two wings and the geographical distance between them were believed to be delaying the adoption of a new constitution. To diminish the differences between the two regions, the government decided to reorganise the country into two distinct provinces under the One Unit policy announced by Prime Minister Chaudhry Muhammad Ali on 22 November 1954.
During most of the Cold War, Pakistan was a close ally of the United States, having an influential membership in the Southeast Asia Treaty Organization (SEATO) and the Central Treaty Organization (CENTO). Geographically divided into two wings, the western contingent, claiming the exclusive mandate for all of Pakistan, considered itself to be the reorganised continuation of the country in the United Nations. President Field Marshal Ayub Khan, who remained in office from 1958 until 1969, worked for a full alignment with the West rather than neutrality. He not only secured membership in SEATO but was also a proponent of agreements that developed CENTO.
West Pakistan emerged as one of South Asia's largest economies and military powers. West Pakistan's economy boomed and at its highest peak it was called the "West Germany of East." Its economic progress was only limited to the western side, and the majority of promised funds for East Pakistan were never issued.
In 1970, President General Yahya Khan enacted a series of territorial, constitutional and military reforms. These established the , state parliament, and the current provisional borders of Pakistan's four provinces. On 1July 1970, West Pakistan was devolved and renamed "Pakistan" under Legal Framework Order No. 1970, which dissolved the "One Unit" and removed the term "West", simply establishing the country as Pakistan. The order had no effect on East Pakistan, which retained the geographical position established in 1955. The next year's civil war, however, resulted in the secession of East Pakistan as the new country of Bangladesh.
Political history.
Independence after British colonial period.
At the time of the state establishment in 1947, the of Pakistan participated in the Boundary Commission conference. Headed by Cyril Radcliffe, the Commission was tasked with negotiating the arrangement, area division, and future political set up of Pakistan and India.
Pakistan was formed from two distinct areas, separated by a thousand miles and India. The western state was composed of three Governor's provinces (North-West Frontier, West-Punjab and Sindh Province), one Chief Commissioner's province (Baluchistan Province), the Baluchistan States Union, several other princely states (notably Bahawalpur, Chitral, Dir, Hunza, Khairpur and Swat), the Federal Capital Territory (around Karachi) and the tribal areas. The eastern wing of the new country – East Pakistan – formed the single province of East Bengal, including the former Assam district of Sylhet.
West Pakistan experienced great problems related to the divisions, including ethnic and racial friction, lack of knowledge, and uncertainty of where to demarcate the permanent borders. East Pakistan, Balochistan, and the North-West Frontier Province experienced little difficulty, but Southern Pakistani Punjab faced considerable problems that had to be fixed. Former East Punjab was integrated with the Indian administration, and millions of Punjabi Muslims were expelled to be replaced by a Sikh and Hindu population and vice versa. The communal violence spread to all over the Indian subcontinent. Economic rehabilitation efforts needing the attention of Pakistan's founding fathers further escalated the problems.
The division also divided the natural resources, industries, economic infrastructure, manpower, and military might, with India as the larger share owner. India retained 345 million in population (91%) to Pakistan's 35 million (9%). Land area was divided as 78% to India and 23% to Pakistan. Military forces were divided up with a ratio of 64% for India and 36% for Pakistan. Most of the military assets – such as weapons depots and military bases – were located inside India; facilities in Pakistan were mostly obsolete, and they had a dangerously low ammunition reserve of only one week. Four divisions were raised in West Pakistan, whilst one division was raised in East Pakistan.
Parliamentary democracy.
From the time of its establishment, the State of Pakistan had the vision of a federal parliamentary democratic republic form of government. With the founding fathers remaining in West Pakistan, Liaquat Ali Khan was appointed the country's first prime minister, with Mohammad Ali Jinnah as Governor-General. West Pakistan claimed the exclusive mandate over all of Pakistan, with the majority of the Pakistan Movement's leading figures in West Pakistan. In 1949, the Constituent Assembly passed the Objectives Resolution and the Annex to the Constitution of Pakistan, paving the road to a Westernized federal parliamentary republic. The work on parliamentary reforms was constituted by the constituent assembly the year after, in 1950.
The western section of Pakistan dominated the politics of the new country. Although East Pakistan had over half of the population, it had a disproportionately small number of seats in the Constituent Assembly. This inequality of the two wings and the geographical distance between them was believed to be holding up the adoption of a new constitution. To diminish the differences between the two regions, the government decided to reorganise the country into two distinct provinces.
Under the One Unit policy announced by Prime Minister Chaudhry Muhammad Ali on 22 November 1954, the four provinces and territories of western Pakistan were integrated into one unit to mirror the single province in the east. The state of West Pakistan was established by the merger of the provinces, states, and tribal areas of West Pakistan. The province was composed of twelve divisions and the provincial capital was established at Karachi. Later the state capital moved to Lahore, and it was finally established in Islamabad in 1965. The province of East Bengal was renamed East Pakistan with the provincial state capital at Dhaka (Dacca).
Clashes between East Pakistan and West Pakistan soon erupted, further destabilising the entire country. The two states had different political ideologies and different lingual cultural aspect. West Pakistan had been founded on the main basis of a parliamentary democracy (and had a parliamentary republic form of government since 1947), with Islam as its state religion. In contrast, East Pakistan had been a socialist state since the 1954 elections, with state secularism proclaimed. West Pakistan sided with the United States and her NATO allies, whilst East Pakistan remained sympathetic to the Soviet Union and her Eastern Bloc. Pakistan's 1956 constitution validated the parliamentary form of government, with Islam as state religion and Urdu, English and Bengali as state languages. The 1956 constitution also established the Parliament of Pakistan as well as the Supreme Court of Pakistan.
Ethnic and religious violence in Lahore, which began in 1953, spread all over the country. Muhammad Ali Bogra, prime minister of Pakistan, declared martial law in Lahore to curb the violence. This inter-communal violence soon spread to India, and a regional conflicts put West Pakistan and India in a war-threatening situation. The prime ministers of Pakistan and India held an emergency meeting in Lahore.
Military dictatorships.
From 1947 to 1959, the government was only partially stable. Seven prime ministers, four governors-general, and one president were forcefully removed either by constitutional coup or by military coup. The One Unit program was met with harsh opposition, civil unrest, and political disturbance. Support for the Muslim League and Pakistan Socialist Party in the upcoming elections threatened Pakistan's technocracy. The Muslim League and Socialist Party gained momentum after the League's defeat in the 1954 elections, and the Socialist Party were challenging for the constituencies of the President Iskandar Mirza's Republican Party. Relations with the United States deteriorated, with the US assessing that democracy in both states was failing.
A US-backed military coup d'état was launched in 1958 by the Pakistan Army command. The Urdu-speaking class and the Bengali nation were forcefully removed from the affairs of West Pakistan. With the imposition of martial law led by then-Army Commander-in-Chief General Ayub Khan, the state capital was moved from Karachi to Army Generals Combatant Headquarters (The GHQ) at Rawalpindi in 1959, whilst the federal legislature was moved to Dacca. In 1963, Rawalpindi had become ineffective as a federal capital; a new city was planned and constructed, finally completing in 1965. In 1965, the state capital was finally re-located in Islamabad.
Dissolution in 1970.
On contrary perception, the provinces did not benefit from economic progress, but the One Unit program strengthened the central government. In West Pakistan, the four provinces also struggled hard for the abolition of One Unit which caused injustices to them as it was imposed on them.
The provisional powerful committees pressured the central government through the means of civil disobedience, violence on street, raising slogans against the martial law, and attacks on government machines such as police forces. For several weeks, the four provinces worked together and guided the "One Unit Dissolution Committee", towards resolving all outstanding issues in time set by the Yahya government. Finally, the committee’s plan went into effect on 1 July 1970, when West Pakistan's "One Unit" was dissolved, and all power was transferred to the provinces of Balochistan, the North West Frontier Province, Punjab and Sindh. President General Yahya Khan issued the decree, simply removing the "West", and adding the word "Pakistan" on 1 July 1970.
In the 1970 general elections (held in December 1970 and ), the far left Awami League under Mujibur Rahman won an overall majority of seats in Parliament and all but 2 of the 162 seats allocated to East Pakistan. The Awami League advocated greater autonomy for East Pakistan but the military government did not permit Mujibur Rahman to form a government.
On 25 March 1971, West Pakistan began a civil war to subdue the democratic victory of East Pakistanis. This began the war between the Pakistani military and the Mukhti Bahini resistance fighters. In November 1971, General Yahya Khan ordered Pakistan Army Corps of Military Police to arrest both Bhutto and Rehman, and he ordered action against East Pakistan's military government. The resulting refugee crisis led to the intervention by India, eventually leading to the surrender of the Pakistani Army. East Pakistan suffered a genocide of its Bengali population.
East Pakistan became the independent state of Bangladesh on 16 December 1971. The term West Pakistan became redundant.
Government.
West Pakistan went through many political changes, and had a multiple political party system. West Pakistan's political system consisted of the popular influential Left-wing sphere against elite Right-wing circles.
Parliamentary republic.
Since independence, West Pakistan had been a parliamentary republic (even as of today, the parliamentary system is the official form of government of Pakistan) with a Prime minister as the head of the government and a President as a head of state in a ceremonial office.
The 1956 Constitution provided the country with Semi-presidential system and the office of President was inaugurated the same year. The career civil service officer Major-General (retired) Iskander Mirza became the country's first President, but the system did not evolved for more than the three years, when Mirza imposed the martial law in 1958. Mirza appointed army commander General Ayub Khan as Chief Martial Law Administrator; he later turned his back on the President and exiled him to Great Britain after the military government was installed.
The Supreme Court of Pakistan was a judicial authority, a power broker in country's politics that played a major role in minimising the role of parliament. The Supreme Court was moved to Islamabad in 1965 and Chief Justice Alvin Robert Cornelius re-located the entire judicial arbiter, personnel and high-profile cases in Islamabad. The Supreme Court building is one of the most attractive places in Islamabad, yet the most largely beautiful building in the state capital.
This provisional parliament had no lasting effects of West Pakistan's affairs but it was a ceremonial legislature where the law makers would gather around to discuss non-political matters. In 1965, the legislative parliament was moved to Islamabad after Ayub Khan built a massive capitol. The assembly was renamed as the Parliament of Pakistan and staffed only with technocrats.
Governor and chief minister.
The office of Governor of West Pakistan was a largely ceremonial position but later Governors wielded some executive powers as well. The first Governor was Mushtaq Ahmed Gurmani, who was also the last Governor of West Punjab. Ayub Khan abolished the Governor's office and instead established the Martial Law Administrator of West Pakistan (MLA West).
The office Chief Minister of West Pakistan was the chief executive of the state and the leader of the largest party in the provincial assembly. The first Chief Minister was Abdul Jabbar Khan who had served twice as Chief Minister of the Khyber Pakhtunkhwa Province prior to independence. The office of Chief Minister was abolished in 1958 when Ayub Khan took over the administration of West Pakistan.
Local government.
The twelve divisions of West Pakistan province were Bahawalpur, Dera Ismail Khan, Hyderabad, Kalat, Khairpur, Lahore, Malakand, Multan, Peshawar, Quetta, Rawalpindi, and Sargodha; all named after their capitals except the capital of Malakand was Saidu, and Rawalpindi was administered from Islamabad. The province also incorporated the former Omani enclave of Gwadar following its purchase in 1958, and the former Federal Capital Territory (Karachi) in 1961; the latter forming a new division in its own right.
In 1970, the Martial law office was dissolved by General Yahya Khan who disestablished the state of West Pakistan. On 1 July 1970, the of Balochistan, Punjab, Sindh, and Khyber Pakhtunkhwa, Office of Prime minister, and much of the civil institutions were revived and re-established by the decree signed by General Yahya Khan. The four provinces and four administrative units retained their current status and local governments were constitutionally established in 1970 to manage and administer the provisional autonomy given to the provinces in 1970.
Domestic affairs.
Position toward East Pakistan.
During West Pakistan's conflict with India, East Pakistan's military government remained silent and did not send any troops to exert pressure on Eastern India. West Pakistan accused East Pakistan of not taking any action, and their inaction caused West Pakistani resentment against East Pakistan's government. In fact, the Indian Air Force Eastern Air Command attacked East Pakistan's Air Force. However, East Pakistan was defended only by the under-strength 14th Infantry Division and sixteen fighter jets; no tanks and no navy were established in East Pakistan.
Days of disintegration.
The One Unit policy was regarded as a rational administrative reform that would reduce expenditure and eliminate provincial prejudices. West Pakistan formed a seemingly homogeneous block, but in reality it comprised marked linguistic and ethnic distinctions. The four provinces did not quite fit official definitions of a single nation.
The Sindhi and Urdu-speaking class in Sindh Province revolted against the One Unit policy. The violence spread to Balochistan Province, Khyber-Pakhtunkhwa and Punjab Province. The One Unit policy was a failure in West Pakistan, and its survival was seen as improbable. However, with the military coup of 1958, trouble loomed for the province when the office of Chief Minister was abolished and the President took over executive powers for West Pakistan.
Influence of socialism.
Due to West Pakistan's close relations with the United States and the capitalist states, the influence of socialism had far more deeper roots in the West Pakistan population. The population favoured socialism but never allied with communism. The Pakistan Socialist Party had previously lost support due to its anti-Pakistan clauses during the time of the pre-independence movement. However, despite initiatives to improve the population during the Ayub Khan's government, the poor masses did not enjoy the benefits and reforms that were enjoyed by the middle and gentry classes of Pakistan.
After the Indo-Pakistani war of 1965, the cultural revolution, resentment, hostility towards the government began to arise when the population felt that "Kashmir cause" was betrayed by President Ayub Khan. Problems further mounted after Foreign minister Zulfikar Ali Bhutto was sacked and vowed to take a revenge. After gathering and uniting the scattered democratic socialist and Marxist masses, Bhutto founded the Pakistan Peoples Party in 1967. The socialists tapped a wave of antipathy against the United States-allied president. The socialists integrated in poor and urban provinces of West Pakistan, educating people to cast their vote for their better future, and the importance of democracy was widely sensed in the entire country. The socialists, under Bhutto's guidance and leadership, played a vital role in managing labour strikes and civil disobedience to challenge Khan's authority. The military government responded fiercely after arresting the senior socialists' leadership, notably Bhutto, Mubashir Hassan, and Malick Mirage. This sparked gruesome violence in West Pakistan, thereby increasing pressure on Khan that he was unable to endure. Khan called for a Round Table Conference in Rawalpindi, but socialists led by Bhutto refused to accept Ayub's continuation in office and rejected the 6 Point Movement for regional autonomy put forth in 1966 by East Pakistani politician Sheikh Mujibur Rahman.
In 1969, Khan handed over power to Army Chief of Staff General Yahya Khan, who promised to hold elections within two years. Meantime, Bhutto extensively worked to gather and unite the country's left-wing organisations, which, under Bhutto's leadership, participated with full force and became vital players in the country's politics.
Foreign relations.
Afghanistan.
The long border between Afghanistan and West Pakistan was uneasy. This is due in part to the independent Pashtun tribes that inhabit the area. In addition, the physical boundary is uncertain: the 1893 Durand Line was used by West Pakistan to mark the border between the two countries, but Afghanistan has never recognised that frontier.
In 1955, diplomatic relations were severed with the ransacking of Pakistan's embassy. In 1961, Pakistan Armed Forces suppressed an Afghan invasion in the Bajaur region of Pakistan.
India.
West Pakistan had hostile relations with India, primarily due to aftermaths of the 1947 independence and the issue of Kashmir.
In 1947, the Pakistan army and air force attempted to annexe Kashmir, but were pushed back by the Indian army. Although the operation was a failure, it did occupy 40% of Kashmir, which was later integrated into Northern Pakistan.
In 1965, "Operation Gibraltar" had long-ranging negative effects, outside and inside the country. Foreign minister Zulfikar Ali Bhutto and Defence minister Vice-Admiral Afzal Rahman Khan approached President Ayub Khan for approval of a covert operation to infiltrate Indian-held Kashmir using airborne troops from the Pakistan army (Special Service Group) and Pakistan air force (Special Service Wing). During nights in August 1965, airborne troops parachuted into Indian Kashmir whilst ground assault began by Pakistan Army's troops. The airborne troops managed to occupy much of Indian-held Kashmir and were only 6 miles (10 km) from Srinagar, but this was the closest Pakistani troops ever got to capturing the city. In September 1965, India launched a counter-attack and the airborne troops were pushed back to Azad Kashmir Province. The operation failed brutally, and Indian Armed Forces attacked West Pakistan with full force. The Soviet Union intervened in the conflict in September 1965 (for fear of escalation), and the month–long war ended with no permanent territorial changes. West Pakistan and India signed the Tashkent Declaration in January 1966, but the ceasefire was criticised both in India and Pakistan, and public resentment against each other grew. In West Pakistan, Ayub Khan deposed Bhutto as his Foreign minister, and Vice-Admiral Khan blamed Bhutto for the operation's failure. As an aftermath, Bhutto tapped into an anti-Ayub Khan movement and kicked off a storm of civil disobedience. Protests and spontaneous demonstrations broke out around the country, and Ayub Khan lost the control. In 1967, another martial law was imposed by another Army Commander-in-Chief, General Yahya Khan, who designated himself as the Chief Martial Law Administrator.
People's Republic of China.
West Pakistan had positive relations with the People's Republic of China, with whom it shared a small northern border.
In 1950, Pakistan was among the first countries to end official diplomatic relations with the Taiwanese Republic of China and recognise the PRC. After that, both countries maintained an extremely close and supportive relationship. The PRC provided economic, military and technical assistance to Pakistan during the Cold War, and the two countries considered each other to be close strategic allies.
Soviet Union.
Relations varied from cool to extremely strained between West Pakistan and the Soviet Union. This was during the Cold War, and Pakistan's close ties with the United States came at the expense of relations with the Soviets.
Soviet-Pakistan relations were further eroded during the 1960 U-2 incident, when the Soviets shot down a US spyplane; Army Chief-of-Staff Ayub Khan had given the US permission to fly out of Peshawar Air Station on reconnaissance and covert surveillance missions over the Soviet Union.
The USSR backed India during the Indo-Pakistani War of 1971. The Soviets were the biggest supplier of military hardware to India at that time.
United States.
The United States was one of the first nations[who?] to establish relations with Pakistan upon its independence.
Pakistan was allied with the US during the Cold war against the USSR. Pakistan was an integral member of the Southeast Asia Treaty Organization (SEATO) and the Central Treaty Organization (CENTO), both alliances opposed to the Soviet Union and communism.
A major factor in Pakistan's decision to ally with the West was their urgent need for aid. In the years that followed, the US supplied extensive economic, scientific, and military assistance to Pakistan.
This close relationship continued through Pakistan's years of democracy and military rule. Relations only soured after West Pakistan had dissolved into Pakistan, when the left-oriented Pakistan Peoples Party came to power in 1971.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="34040" url="http://en.wikipedia.org/wiki?curid=34040" title="West Bengal">
West Bengal

West Bengal (; ]) is a state in eastern India and is the nation's fourth-most populous state, with over 91 million inhabitants. Spread over 34267 sqmi, it is bordered by the countries of Bangladesh, Nepal and Bhutan, and the Indian states of Odisha, Jharkhand, Bihar, Sikkim, and Assam. The state capital is Kolkata. Together with the neighboring nation of Bangladesh and parts of the state Tripura, it makes up the ethno-linguistic region of Bengal.
Ancient Bengal was the site of several major janapadas (kingdoms). It was also part of large empires such as the Maurya Empire (second century BC) and Gupta Empire (fourth century AD); and part of the regional Pala Empire (eighth to 11th century) and Sena dynasty (11th–12th century). From the 13th century onward, the region was controlled by the Bengal Sultanate, Hindu kings and Baro-Bhuyan landlords until the beginning of British rule in the 18th century. The East India Company cemented their hold on the region following the Battle of Plassey in 1757, and Calcutta served for many years as the capital of British India. The early and prolonged exposure to British administration resulted in expansion of Western education, culminating in development in science, institutional education, and social reforms of the region, including what became known as the Bengali renaissance. A hotbed of the Indian independence movement through the early 20th century, Bengal was divided during India's independence in 1947 along religious lines into two separate entities: West Bengal—a state of India—and East Bengal—a part of the newly created Dominion of Pakistan—later becoming the independent nation of Bangladesh in 1971.
A major agricultural producer, West Bengal is the sixth-largest contributor to India's net domestic product. Noted for its political activism, the state was ruled by democratically elected communist government for 34 years, starting from 1977. It is noted for its cultural activities and presence of cultural and educational institutions; the state capital Kolkata is known as the "cultural capital of India". The state's cultural heritage, besides varied folk traditions, ranges from stalwarts in literature including Nobel-laureate Rabindranath Tagore to scores of musicians, film-makers and artists. West Bengal is also distinct from most other Indian states in its appreciation and practice of playing football besides the national favourite sport cricket.
Etymology.
The origin of the name Bengal (known as "Bangla" and "Bongo" in Bengali language) is unknown. One theory suggests that the word derives from "Bang", a Dravidian tribe that settled the region around 1000 BC. The word might have been derived from the ancient kingdom of "Vanga" (or "Banga"). Although some early Sanskrit literature mentions the name, the region's early history is obscure.
At times of British Rule over the Indian subcontinent, the Bengal region was partitioned in 1947 along religious lines into east and west. The east came to be known as East Bengal and the west came to known as West Bengal which remained as an Indian state. In 2011, the Government of West Bengal proposed a change in the official name for the state to "Poschimbongo" (Bengali: পশ্চিমবঙ্গ "Pôshchimbônggô") which reflects the native name of the state, literally meaning western Bengal in the native Bengali language.
History.
Stone Age tools dating back 20,000 years have been excavated in the state. The region was a part of the Vanga Kingdom according to the "Mahabharata". Several Vedic realms were present in Bengal region, including Vanga, Rarh, Pundravardhana and the Suhma Kingdom.
Era of the janapadas.
The kingdom of Magadha was formed in 7th century BCE, consisting of the regions now comprising Bihar and Bengal. It was one of the four main kingdoms of India at the time of the lives of Mahavira, founder of Jainism, and Gautama Buddha, founder of Buddhism, and consisted of several janapadas or kingdoms.
Under Ashoka, the Maurya Empire extended over nearly all of South Asia, including Afghanistan and parts of Balochistan in the 3rd century BCE.
Contact outside South Asia.
One of the earliest foreign references to Bengal is a mention by the Ancient Greeks around 100 BC of a land named Gangaridai, which was located at the mouths of the Ganges. Bengal had overseas trade relations with Suvarnabhumi (Burma, Lower Thailand, Lower Malay Peninsula, and the Sumatra). According to the "Mahavamsa", Prince Vijaya, a Vanga Kingdom prince, conquered Lanka (modern day Sri Lanka) and gave the name Sinhala Kingdom to the country.
Later rulers.
After a period of anarchy, the Buddhist Pala dynasty ruled the region for four hundred years, followed by a shorter reign of the Hindu Sena dynasty. Some areas of Bengal were invaded by Rajendra Chola I of the Chola dynasty between 1021 and 1023. Islam made its first appearance in Bengal during the 12th century when Sufi missionaries arrived. Later, occasional Muslim raiders reinforced the process of conversion by building mosques, madrasas and khanqahs. Between 1202 and 1206, Muhammad bin Bakhtiyar Khilji, a military commander from the Delhi Sultanate, overran Bihar and Bengal as far east as Rangpur City, Bogra and the Brahmaputra River. Although he failed to bring Bengal under his control, the expedition managed to defeat Lakshman Sen and his two sons moved to a place then called Vikramapur (present-day Munshiganj District), where their diminished dominion lasted until the late 13th century.
During the 14th century, the former kingdom became known as the Bengal Sultanate, ruled intermittently with the Delhi Sultanate as well as powerful Hindu states, landlords and Baro-Bhuyans. The Hindu Deva dynasty ruled over eastern Bengal after the collapse of the Sena. The Bengal Sultanate was interrupted by an uprising by the Hindus under Raja Ganesha. The Ganesha dynasty began in 1414, but his successors converted to Islam. Bengal came once more under the control of Delhi as the Mughal Empire conquered it in 1576. There were several independent Hindu states established in Bengal during the Mughal period like those of Pratapaditya of Jessore District and Raja Sitaram Ray of Bardhaman. These kingdoms contributed greatly to the economic and cultural landscape of Bengal. Extensive land reclamations in forested and marshy areas were carried out and trade as well as commerce were highly encouraged. These kingdoms also helped introduce new music, painting, dancing and sculpture into Bengali art-forms as well as many temples were constructed during this period. Militarily, they served as bulwarks against Portuguese and Burmese attacks.
The Koch dynasty in northern Bengal flourished during the period of 16th and the 17th centuries as well as weathered the Mughals and survived till the advent of the British.
Colonialism.
European traders arrived late in the fifteenth century. Their influence grew until the British East India Company gained taxation rights in Bengal subah, or province, following the Battle of Plassey in 1757, when Siraj ud-Daulah, the last independent Nawab, was defeated by the British.
The Bengal Presidency was established by 1765, eventually including all British territories north of the Central Provinces (now Madhya Pradesh), from the mouths of the Ganges and the Brahmaputra to the Himalayas and the Punjab. The Bengal famine of 1770 claimed millions of lives.
Calcutta was named the capital of British India in 1772.
The Bengal Renaissance and Brahmo Samaj socio-cultural reform movements had great impact on the cultural and economic life of Bengal. The failed Indian rebellion of 1857 started near Calcutta and resulted in transfer of authority to the British Crown, administered by the Viceroy of India. Between 1905 and 1911, an abortive attempt was made to divide the province of Bengal into two zones. Bengal suffered from the Great Bengal famine in 1943 that claimed 3 million lives.
The Indian independence movement.
Bengal played a major role in the Indian independence movement, in which revolutionary groups such as "Anushilan Samiti" and "Jugantar" were dominant. Armed attempts against the British Raj from Bengal reached a climax when Subhas Chandra Bose led the Indian National Army from Southeast Asia against the British.
When India gained independence in 1947, Bengal was partitioned along religious lines. The western part went to India (and was named West Bengal) while the eastern part joined Pakistan as a province called East Bengal (later renamed East Pakistan, giving rise to independent Bangladesh in 1971). In 1950, the Princely State of Cooch Behar merged with West Bengal. In 1955, the former French enclave of Chandannagar, which had passed into Indian control after 1950, was integrated into West Bengal; portions of Bihar were subsequently merged with West Bengal. Both West and East Bengal suffered from large refugee influx during and after the partition in 1947. Refugee settlement and related issues continued to play significant role in the politics and socio-economic condition of the state.
During the 1970s and 1980s, severe power shortages, strikes and a violent Naxalite movement damaged much of the state's infrastructure, leading to a period of economic stagnation. The Bangladesh Liberation War of 1971 resulted in the influx of millions of refugees to West Bengal, causing significant strains on its infrastructure. The 1974 smallpox epidemic killed thousands. West Bengal politics underwent a major change when the Left Front won the 1977 assembly election, defeating the incumbent Indian National Congress. The Left Front, led by Communist Party of India (Marxist), governed for the state for the subsequent three decades.
Recent history.
The state's economic recovery gathered momentum after economic liberalisations were introduced in the mid-1990s by the central government, aided by the advent of information technology and IT-enabled services. Since mid-2000s, armed activists conducted minor terrorist attacks in some parts of the state, while clashes with the administration took place at several sensitive places over the issue of industrial land acquisition, which became a crucial reason behind the defeat of ruling Left Front government in 2011 assembly election. Although the state's GDP has risen significantly since the 1990s, West Bengal has remained affected by political instability and bad governance. The state continues to suffer from regular bandhs (strikes), a low Human Development Index level, substandard healthcare services, a lack of socio-economic development, poor infrastructure, political corruption and civil violence.
Geography and climate.
West Bengal is on the eastern bottleneck of India, stretching from the Himalayas in the north, to the Bay of Bengal in the south. The state has a total area of 88752 km2. The Darjeeling Himalayan hill region in the northern extreme of the state belongs to the eastern Himalaya. This region contains Sandakfu (3636 m)—the highest peak of the state. The narrow Terai region separates this region from the plains, which in turn transitions into the Ganges delta towards the south. The Rarh region intervenes between the Ganges delta in the east and the western plateau and high lands. A small coastal region is on the extreme south, while the Sundarbans mangrove forests form a geographical landmark at the Ganges delta.
The Ganges is the main river, which divides in West Bengal. One branch enters Bangladesh as the "Padma" or "Pôdda", while the other flows through West Bengal as the Bhagirathi River and Hooghly River. The Farakka barrage over Ganges feeds the Hooghly branch of the river by a feeder canal, and its water flow management has been a source of lingering dispute between India and Bangladesh. The Teesta, Torsa, Jaldhaka and Mahananda rivers are in the northern hilly region. The western plateau region has rivers such as the Damodar, Ajay and Kangsabati. The Ganges delta and the Sundarbans area have numerous rivers and creeks. Pollution of the Ganges from indiscriminate waste dumped into the river is a major problem. Damodar, another tributary of the Ganges and once known as the "Sorrow of Bengal" (due to its frequent floods), has several dams under the Damodar Valley Project. At least nine districts in the state suffer from arsenic contamination of groundwater, and an estimated 8.7 million people drink water containing arsenic above the World Health Organisation recommended limit of 10 µg/L.
West Bengal's climate varies from tropical savanna in the southern portions to humid subtropical in the north. The main seasons are summer, rainy season, a short autumn, and winter. While the summer in the delta region is noted for excessive humidity, the western highlands experience a dry summer like northern India, with the highest day temperature ranging from 38 °C to 45 °C. At nights, a cool southerly breeze carries moisture from the Bay of Bengal. In early summer brief squalls and thunderstorms known as "Kalbaisakhi", or Nor'westers, often occur. West Bengal receives the Bay of Bengal branch of the Indian ocean monsoon that moves in a northwest direction. Monsoons bring rain to the whole state from June to September. Heavy rainfall of above 250 cm is observed in the Darjeeling, Jalpaiguri and Cooch Behar district. During the arrival of the monsoons, low pressure in the Bay of Bengal region often leads to the occurrence of storms in the coastal areas. Winter (December–January) is mild over the plains with average minimum temperatures of 15 °C. A cold and dry northern wind blows in the winter, substantially lowering the humidity level. The Darjeeling Himalayan Hill region experiences a harsh winter, with occasional snowfall at places.
Flora and fauna.
As of 2013, recorded forest area in the state is 16805 sqkm which is 18.93% of the state's geographical area, compared to the national average of 21.23%. Reserves, protected and unclassed forests constitute 59.4%, 31.8% and 8.9%, respectively, of the forest area, as of 2009. Part of the world's largest mangrove forest, the Sundarbans, is located in southern West Bengal.
From a phytogeographic viewpoint, the southern part of West Bengal can be divided into two regions: the Gangetic plain and the littoral mangrove forests of the Sundarbans. The alluvial soil of the Gangetic plain, compounded with favourable rainfall, make this region especially fertile. Much of the vegetation of the western part of the state shares floristic similarities with the plants of the Chota Nagpur plateau in the adjoining state of Jharkhand. The predominant commercial tree species is "Shorea robusta", commonly known as the Sal tree. The coastal region of Purba Medinipur exhibits coastal vegetation; the predominant tree is the "Casuarina". A notable tree from the Sundarbans is the ubiquitous "sundari" ("Heritiera fomes"), from which the forest gets its name.
The distribution of vegetation in northern West Bengal is dictated by elevation and precipitation. For example, the foothills of the Himalayas, the "Dooars", are densely wooded with Sal and other tropical evergreen trees. However, above an elevation of 1000 m, the forest becomes predominantly subtropical. In Darjeeling, which is above 1500 m, temperate-forest trees such as oaks, conifers, and rhododendrons predominate.
West Bengal has 3.26% of its geographical area under protected areas comprising 15 wildlife sanctuaries and 5 national parks — Sundarbans National Park, Buxa Tiger Reserve, Gorumara National Park, Neora Valley National Park and Singalila National Park. Extant wildlife include Indian rhinoceros, Indian elephant, deer, leopard, gaur, tiger, and crocodiles, as well as many bird species. Migratory birds come to the state during the winter. The high-altitude forests of Singalila National Park shelter barking deer, red panda, chinkara, takin, serow, pangolin, minivet and Kalij pheasants. The Sundarbans are noted for a reserve project conserving the endangered Bengal tiger, although the forest hosts many other endangered species, such as the Gangetic dolphin, river terrapin and estuarine crocodile. The mangrove forest also acts as a natural fish nursery, supporting coastal fishes along the Bay of Bengal. Recognizing its special conservation value, Sundarban area has been declared as a Biosphere Reserve.
Government and politics.
West Bengal is governed through a parliamentary system of representative democracy, a feature the state shares with other Indian states. Universal suffrage is granted to residents. There are two branches of government. The legislature, the West Bengal Legislative Assembly, consists of elected members and special office bearers such as the Speaker and Deputy Speaker, that are elected by the members. Assembly meetings are presided over by the Speaker or the Deputy Speaker in the Speaker's absence. The judiciary is composed of the Calcutta High Court and a system of lower courts. Executive authority is vested in the Council of Ministers headed by the Chief Minister, although the titular head of government is the Governor. The Governor is the head of state appointed by the President of India. The leader of the party or coalition with a majority in the Legislative Assembly is appointed as the Chief Minister by the Governor, and the Council of Ministers are appointed by the Governor on the advice of the Chief Minister. The Council of Ministers reports to the Legislative Assembly. The Assembly is unicameral with 295 Members of the Legislative Assembly, or MLAs, including one nominated from the Anglo-Indian community. Terms of office run for 5 years, unless the Assembly is dissolved prior to the completion of the term. Auxiliary authorities known as "panchayats", for which local body elections are regularly held, govern local affairs. The state contributes 42 seats to the Lok Sabha and 16 seats to the Rajya Sabha of the Indian Parliament.
The main players in the regional politics are the All India Trinamool Congress, the Indian National Congress, and the Left Front alliance (led by the Communist Party of India (Marxist) or CPI(M)). Following the West Bengal State Assembly Election in 2011, the All India Trinamool Congress and Indian National Congress coalition under Mamata Banerjee of the All India Trinamool Congress was elected to power (getting 225 seats in the legislature). Prior to this, West Bengal was ruled by the Left Front for 34 years (1977–2011), making it the world's longest-running democratically elected communist government.
Subdivisions.
The following is a list of nineteen districts of West Bengal by rank in India.
Each district is governed by a district collector or district magistrate, appointed either by the Indian Administrative Service or the West Bengal Civil Service. Each district is subdivided into Sub-Divisions, governed by a sub-divisional magistrate, and again into Blocks. Blocks consists of panchayats (village councils) and town municipalities.
The capital and largest city of the state is Kolkata – the third-largest urban agglomeration and the seventh-largest city in India. Asansol is the second largest city & urban agglomeration in West Bengal after Kolkata. Siliguri is an economically important city, strategically located in the northeastern Siliguri Corridor (Chicken's Neck) of India. Other major cities and towns in West Bengal are Howrah, Durgapur, Raniganj, Haldia, Jalpaiguri, Kharagpur, Burdwan, Darjeeling, Midnapore, and Malda.
Economy.
In 2009–10, the tertiary sector of the economy (service industries) was the largest contributor to the gross domestic product of the state, contributing 57.8% of the state domestic product compared to 24% from primary sector (agriculture, forestry, mining) and 18.2% from secondary sector (industrial and manufacturing).:12 Agriculture is the leading occupation in West Bengal. Rice is the state's principal food crop. Rice, potato, jute, sugarcane and wheat are the top five crops of the state.:14 Tea is produced commercially in northern districts; the region is well known for Darjeeling and other high quality teas.:14 State industries are localised in the Kolkata region, the mineral-rich western highlands, and Haldia port region. The Durgapur–Asansol colliery belt is home to a number of major steel plants. Manufacturing industries playing an important economic role are engineering products, electronics, electrical equipment, cables, steel, leather, textiles, jewellery, frigates, automobiles, railway coaches, and wagons. The Durgapur centre has established a number of industries in the areas of tea, sugar, chemicals and fertilisers. Natural resources like tea and jute in and nearby parts has made West Bengal a major centre for the jute and tea industries.
Years after independence, West Bengal was still dependent on the central government for meeting its demands for food; food production remained stagnant and the Indian green revolution bypassed the state. However, there has been a significant spurt in food production since the 1980s, and the state now has a surplus of grains. The state's share of total industrial output in India was 9.8% in 1980–81, declining to 5% by 1997–98. However, the service sector has grown at a rate higher than the national rate.
In terms net state domestic product (NSDP), West Bengal has the sixth largest economy (2009–2010) in India, with an NSDP of 3663 billion Indian rupees, behind Maharashtra (8179 billion), Uttar Pradesh (4530 billion), Andhra Pradesh (4268 billion), Tamil Nadu (4177 billion), and Gujarat (3704 billion). In the period 2004–2005 to 2009–2010, the average gross state domestic product (GSDP) growth rate was 13.9% (calculated in Indian rupee term), lower than 15.5%, the average for all states of the country.:4 The state’s per capita GSDP at current prices in 2009–10 was US$956.4, improved from US$553.7 in 2004–05,:10but lower than the national per capita GSDP of US$1,302.:4 The state's total financial debt stood at ₹1918350 million () as of 2011.
The state has promoted foreign direct investment, which has mostly come in the software and electronics fields; Kolkata is becoming a major hub for the Information technology (IT) industry. Rapid industrialisation process has given rise to debate over land acquisition for industry in this agrarian state. NASSCOM–Gartner ranks West Bengal power infrastructure the best in the country. Notably, many corporate companies are now headquartered in Kolkata include ITC Limited, India Government Mint, Kolkata, Haldia Petrochemicals, Exide Industries, Hindustan Motors, Britannia Industries, Bata India, Birla Corporation, CESC Limited, Coal India Limited, Damodar Valley Corporation, PwC India, Peerless Group, United Bank of India, UCO Bank and Allahabad Bank. In 2010s, events such as adoption of "Look East" policy by the government of India, opening of the Nathu La Pass in Sikkim as a border trade-route with China and immense interest in the South East Asian countries to enter the Indian market and invest have put Kolkata in an advantageous position for development in future, particularly with likes of Myanmar, where India needs oil from military regime.
Transport.
As of 2011, the total length of surface road in West Bengal is over 92023 km;:18 national highways comprise 2578 km and state highways 2393 km.:18 As of 2006, the road density of the state is 103.69 km per 100 km² (166.92 mi per 100 sq mi), higher than the national average of 74.7 km per 100 km² (120 mi per 100 sq mi). Average speed on state highways varies between 40–50 km/h (25–31 mi/h); in villages and towns, speeds are as low as 20–25 km/h (12–16 mi/h) due to the substandard quality of road constructions and low maintenance.
As of 2011, the total railway route length is around 4481 km.:20 Kolkata is the headquarters of three Zones of the Indian Railways—Eastern Railway and South Eastern Railway and the Kolkata Metro which is the newly formed 17th Zone of the Indian Railways. The Northeast Frontier Railway (NFR) plies in the northern parts of the state. The Kolkata metro is the country's first underground railway. The Darjeeling Himalayan Railway, part of NFR, is a UNESCO World Heritage Site.
Netaji Subhas Chandra Bose International Airport at Dum Dum, Kolkata, is the state's biggest airport. Bagdogra Airport near Siliguri is a customs airport that has international services to Bhutan and Thailand besides regular domestic services. Kazi Nazrul Islam Airport, serving the twin cities of Asansol-Durgapur is located at Andal, Bardhaman, 25 km from Asansol and 15 km from Durgapur. This was inaugurated on 10 May 2015, when Prime Minister Narendra Modi became the first passenger to use the new airport before commercial airlines started their regular services.
Kolkata is a major river-port in eastern India. The Kolkata Port Trust manages both the Kolkata docks and the Haldia docks. There is passenger service to Port Blair on the Andaman and Nicobar Islands and cargo ship service to ports in India and abroad, operated by the Shipping Corporation of India. Ferry is a principal mode of transport in the southern part of the state, especially in the Sundarbans area. Kolkata is the only city in India to have trams as a mode of transport and these are operated by the Calcutta Tramways Company.
Several government-owned organisations operate bus services in the state, including the Calcutta State Transport Corporation, the North Bengal State Transport Corporation, the South Bengal State Transport Corporation, the West Bengal Surface Transport Corporation, and the Calcutta Tramways Company. There are also private bus companies. The railway system is a nationalised service without any private investment. Hired forms of transport include metered taxis and auto rickshaws which often ply specific routes in cities. In most of the state, cycle rickshaws, and in Kolkata, hand-pulled rickshaws, are also used for short-distance travel. Large-scale transport accidents in West Bengal are common, particularly the sinking of transport boats and train crashes.
Demographics.
According to the provisional results of the 2011 national census, West Bengal is the fourth most populous state in India with a population of 91,347,736 (7.55% of India's population). Bengalis comprise the majority of the population. The Marwaris, Bihari and Oriya minorities are scattered throughout the state; communities of Sherpas and ethnic Tibetans can be found in the Darjeeling Himalayan hill region. The Darjeeling district has a large number of Nepalese immigrants. West Bengal is home to indigenous tribal "Adivasis" such as Santhal, Kol, and Toto tribe. There are a small number of ethnic minorities primarily in the state capital, including Chinese, Tamils, Gujaratis, Anglo-Indians, Armenians, Punjabis, and Parsis. India's sole Chinatown is in eastern Kolkata.
The official language is Bengali and English. Nepali is the official language in three subdivisions of Darjeeling district. As of 2001, in decreasing order of number of speakers, the languages of the state are: Bengali, Hindi, Santali, Urdu, Nepali, and Oriya. Languages such as Ho is used in some parts of the state.
As of 2001, Hinduism is the religion followed by 72.5% of the total population, while Muslims comprise 25.2% of the total population, being the second-largest community as also the largest minority group; Sikhism, Christianity and other religions make up the remainder. The state contributes 7.8% of India's population. The state's 2001–2011 decennial growth rate was 13.93%, lower than 1991–2001 growth rate of 17.8%, and also lower than the national rate of 17.64%. The gender ratio is 947 females per 1000 males. As of 2011, West Bengal has a population density of 1029 PD/km2 making it the second-most densely populated state in India, after Bihar.
The literacy rate is 77.08%, higher than the national rate of 74.04%. Data of 1995–1999 showed the life expectancy in the state was 63.4 years, higher than the national value of 61.7 years. About 72% of people live in rural areas. The proportion of people living below the poverty line in 1999–2000 was 31.9%. Scheduled Castes and Tribes form 28.6% and 5.8% of the population respectively in rural areas, and 19.9% and 1.5% respectively in urban areas. A study conducted in three districts of West Bengal found that accessing private health services to treat illness had a catastrophic impact on households. This indicates the value of public provision of health services to mitigate against poverty and the impact of illness on poor households.
In 2011, the police in West Bengal recorded 143,197 cognizable offences under the Indian Penal Code; the all-India statistic for the year was 2,325,575. The crime rate in the state was 158.1 per 100,000 people, less than the all-India average of 192.2. This is the twelfth-lowest crime rate among the 32 states and union territories of India. In 2011, in reported crimes against women, the state showed a crime rate of 29 compared to the national rate of 18. West Bengal accounted for about 12.2% of total crime against women (26,125 cases out of India's 213,585 cases). Some estimates state that there are more than 60,000 brothel-based women and girls in prostitution in Kolkata. The population of prostitutes in Sonagachi constitutes mainly of Nepalese, Indians and Bangladeshis. Some sources estimate there are 60,000 women in the brothels of Kolkata. The largest prostitution area in city is Sonagachi. West Bengal was the first Indian state to constitute a Human Rights Commission of its own.
Culture.
Literature.
The Bengali language boasts a rich literary heritage, shared with neighbouring Bangladesh. West Bengal has a long tradition in folk literature, evidenced by the "Charyapada", "Mangalkavya", "Shreekrishna Kirtana", "Thakurmar Jhuli", and stories related to Gopal Bhar. In the nineteenth and twentieth
century, Bengali literature was modernised in the works of authors such as Bankim Chandra Chattopadhyay, Michael Madhusudan Dutt, Rabindranath Tagore, Kazi Nazrul Islam, Sharat Chandra Chattopadhyay, Jibananda Das and Manik Bandyopadhyay. In modern times Jibanananda Das, Bibhutibhushan Bandopadhyay, Tarashankar Bandopadhyay, Manik Bandopadhyay, Ashapurna Devi, Shirshendu Mukhopadhyay, Buddhadeb Guha, Mahashweta Devi, Samaresh Majumdar, Sanjeev Chattopadhyay and Sunil Gangopadhyay among others are well known.
Music and dance.
The Baul tradition is a unique heritage of Bengali folk music, which has also been influenced by regional music traditions. Other folk music forms include Gombhira and Bhawaiya. Folk music in West Bengal is often accompanied by the ektara, a one-stringed instrument. West Bengal also has a heritage in North Indian classical music. "Rabindrasangeet", songs composed and set into tune by Rabindranath Tagore and "Nazrul geeti" (by Kazi Nazrul Islam) are popular. Also prominent are other musical forms like Dwijendralal, Atulprasad and Rajanikanta's songs, and "adhunik" or modern music from films and other composers. 
 Bengali dance forms draw from folk traditions, especially those of the tribal groups, as well as the broader Indian dance traditions. Chau dance of Purulia is a rare form of mask dance.
Films.
Mainstream Hindi films are popular in Bengal, and the state is home to a Tollywood. Tollygunj in Kolkata is the location of numerous Bengali movie studios, and the name "Tollywood" (similar to Hollywood and Bollywood) is derived from that name. The Bengali film industry is well known for its art films, and has produced acclaimed directors like Satyajit Ray, Mrinal Sen, Tapan Sinha and Ritwik Ghatak. Prominent contemporary directors include veterans like Buddhadev Dasgupta, Tarun Majumdar, Goutam Ghose, Aparna Sen, Rituparno Ghosh and a newer pool of directors like Kaushik Ganguly and Srijit Mukherji.
Fine arts.
Bengal had been the harbinger of modernism in fine arts. Abanindranath Tagore, called the father of Modern Indian Art had started the Bengal School of Art which was to create styles of art outside the European realist tradition which was taught in art colleges under the colonial administration of the British Government. The movement had many adherents like Gaganendranath Tagore, Ramkinkar Baij, Jamini Roy and Rabindranath Tagore. After Indian Independence, important groups like the Calcutta Group and the Society of Contemporary Artists were formed in Bengal which dominated the art scene in India.
Reformist heritage.
The capital, Kolkata, was the workplace of several social reformers, like Raja Ram Mohan Roy, Iswar Chandra Vidyasagar, and Swami Vivekananda. These social reforms have eventually led to a cultural atmosphere where practices like sati, dowry, and caste-based discrimination or untouchability, the evils that crept into the Hindu society, were abolished.The region was also home to several religious teachers, such as Chaitanya, Ramakrishna, Prabhupada and Paramahansa Yogananda.
Cuisine.
Rice and fish are traditional favourite foods, leading to a saying in Bengali, "machhe bhate bangali", that translates as "fish and rice make a Bengali". Bengal's vast repertoire of fish-based dishes includes hilsa preparations, a favourite among Bengalis. There are numerous ways of cooking fish depending on the texture, size, fat content and the bones. Sweets occupy an important place in the diet of Bengalis and at their social ceremonies. It is an ancient custom among both Hindu and Muslim Bengalis to distribute sweets during festivities. The confectionery industry has flourished because of its close association with social and religious ceremonies. Competition and changing tastes have helped to create many new sweets. Bengalis make distinctive sweetmeats from milk products, including "Rôshogolla", "Chômchôm", "Kalojam" and several kinds of "sondesh". Pitha, a kind of sweet cake, bread or dimsum are specialties of winter season. Sweets like coconut-naru, til-naru, moa, payesh, etc. are prepared during the festival of Lakshmi puja. Popular street food includes "Aloor Chop", Beguni, Kati roll, and phuchka.
The variety of fruits and vegetables that Bengal has to offer is incredible. A host of gourds, roots and tubers, leafy greens, succulent stalks, lemons and limes, green and purple eggplants, red onions, plantain, broad beans, okra, banana tree stems and flowers, green jackfruit and red pumpkins are to be found in the markets or anaj bazaar as popularly called. "Panta bhat" (rice soaked overnight in water)with onion & green chili is a traditional dish consumed in rural areas. Common spices found in a Bengali kitchen are cumin, ajmoda (radhuni), bay leaf, mustard, ginger, green chillies, turmeric, etc. People of erstwhile East Bengal use a lot of ajmoda, coriander leaves, tamarind, coconut and mustard in their cooking; while those aboriginally from West Bengal use a lot of sugar, garam masala and red chilli powder. Vegetarian dishes are mostly without onion and garlic.
Costumes.
Bengali women commonly wear the "shaŗi", often distinctly designed according to local cultural customs. In urban areas, many women and men wear Western attire. Among men, western dressing has greater acceptance. Men also wear traditional costumes such as the "panjabi" with "dhuti", often on cultural occasions.
Festivals.
Durga Puja in October is the most popular festival in the West Bengal. Poila Baishakh (the Bengali New Year), Rathayatra, Dolyatra or Basanta-Utsab, Nobanno, "Poush Parbon" (festival of Poush), Kali Puja, SaraswatiPuja, LaxmiPuja, Christmas, Eid ul-Fitr, Eid ul-Adha and Muharram are other major festivals. Buddha Purnima, which marks the birth of Gautama Buddha, is one of the most important Hindu/Buddhist festivals while Christmas, called "Bôŗodin" (Great day) in Bengali is celebrated by the minority Christian population. Poush mela is a popular festival of Shantiniketan in winter.
Education.
West Bengal schools are run by the state government or by private organisations, including religious institutions. Instruction is mainly in English or Bengali, though Urdu is also used, especially in Central Kolkata. The secondary schools are affiliated with the Council for the Indian School Certificate Examinations (CISCE), the Central Board for Secondary Education (CBSE), the National Institute of Open School (NIOS) or the West Bengal Board of Secondary Education. Under the 10+2+3 plan, after completing secondary school, students typically enroll for 2 years in a junior college, also known as pre-university, or in schools with a higher secondary facility affiliated with the West Bengal Council of Higher Secondary Education or any central board. Students choose from one of three streams, namely liberal arts, commerce or science. Upon completing the required coursework, students may enroll in general or professional degree programs.
West Bengal has eighteen universities. The University of Calcutta, the oldest public university in India, has 136 affiliated colleges. Kolkata has played a pioneering role in the development of the modern education system in India. It is the gateway to the revolution of European education. Sir William Jones (philologist) established the Asiatic Society in 1794 for promoting oriental studies. People like Ram Mohan Roy, David Hare, Ishwar Chandra Vidyasagar, Alexander Duff and William Carey played a leading role in the setting up of modern schools and colleges in the city. The Fort William College was established in 1810. The Hindu College was established in 1817. The Scottish Church College, which is the oldest Christian liberal arts college in South Asia, started its journey in 1830. In 1855 the Hindu College was renamed as the Presidency College. The
and Jadavpur University are prestigious technical universities. Visva-Bharati University at Santiniketan is a central university and an institution of national importance. The state has several higher education institutes of national importance including Indian Institute of Foreign Trade, Indian Institute of Management Calcutta (the first IIM), Indian Institute of Science Education and Research, Kolkata, Indian Statistical Institute, Indian Institute of Technology Kharagpur (the first IIT), Indian Institute of Engineering Science and Technology, Shibpur (the first IIEST), National Institute of Technology, Durgapur and West Bengal National University of Juridical Sciences. After 2003 the state govt supported the creation of West Bengal University of Technology, West Bengal State University and Gour Banga University.
Besides these, the state also has Kalyani University, The University of Burdwan, Vidyasagar University and North Bengal University-all well established and nationally renowned, to cover the educational needs at the district levels of the state and also an Indian Institute of Science Education and Research, Kolkata. Also recently Presidency College, Kolkata became a University named Presidency University. Apart from this there is another private university run by Ramakrishna mission named Ramakrishna Mission Vivekananda University at Belur Math.
There are a number of research institutes in Kolkata. The Indian Association for the Cultivation of Science is the first research institute in Asia. C. V. Raman got Nobel Prize for his discovery (Raman Effect) done in IACS. Also Bose Institute, Saha Institute of Nuclear Physics, S.N. Bose National Centre for Basic Sciences, Indian Institute of Chemical Biology, Central Glass and Ceramic Research Institute, Variable Energy Cyclotron Center are most prominent. Notable scholars who were born, worked or studied in the geographic area of the state include physicists Satyendra Nath Bose, Meghnad Saha, and Jagadish Chandra Bose; chemist Prafulla Chandra Roy; statistician Prasanta Chandra Mahalanobis; physician Upendranath Brahmachari; educator Ashutosh Mukherjee; and Nobel laureates Rabindranath Tagore, C. V. Raman, and Amartya Sen.
Media.
West Bengal had 505 published newspapers in 2005, of which 389 were in Bengali. "Ananda Bazar Patrika", published from Kolkata with 1,277,801 daily copies, has the largest circulation for a single-edition, regional language newspaper in India. Other major Bengali newspapers are "Bartaman", "Sangbad Pratidin", "Aajkaal", "Jago Bangla", "Uttarbanga Sambad" and "Ganashakti". Major English language newspapers which are published and sold in large numbers are "The Telegraph", "The Times of India", "Hindustan Times", "The Hindu", "The Statesman", "The Indian Express" and "Asian Age". Some prominent financial dailies like "The Economic Times", "Financial Express", "Business Line" and "Business Standard" are widely circulated. Vernacular newspapers such as those in Hindi, Nepali Gujarati, Oriya, Urdu and Punjabi are also read by a select readership.
Doordarshan is the state-owned television broadcaster. Multi system operators provide a mix of Bengali, Nepali, Hindi, English and international channels via cable. include ABP Ananda, Tara Newz, Kolkata TV, News Time, 24 Ghanta, Mahuaa Khobor, Ne Bangla, CTVN Plus, Channel 10 and R Plus. All India Radio is a public radio station. Private FM stations are available only in cities like Kolkata, Siliguri and Asansol. Vodafone, Airtel, BSNL, Reliance Communications, Uninor, Aircel, MTS India, Tata Indicom, Idea Cellular and Tata DoCoMo are available cellular phone operators. Broadband internet is available in select towns and cities and is provided by the state-run BSNL and by other private companies. Dial-up access is provided throughout the state by BSNL and other providers.
Sports.
Cricket and soccer are popular sports in the state. West Bengal, unlike most other states of India, is noted for its passion and patronage of football. Kolkata is one of the major centres for football in India and houses top national clubs such as East Bengal, Mohun Bagan and Mohammedan Sporting Club. Indian sports such as Kho Kho and Kabaddi are also played. Calcutta Polo Club is considered as the oldest polo club of the world, and the Royal Calcutta Golf Club is the oldest of its kind outside Great Britain. Salt Lake Stadium is the world's second largest stadium, and Eden Gardens stadium is the largest cricket stadium in India.
West Bengal has several large stadiums—The Eden Gardens is one of only two 100,000-seat cricket amphitheaters in the world, although renovations will reduce this figure. Kolkata Knight Riders, East Zone and Bengal play there, and the 1987 World Cup final was there although in 2011 World Cup, Eden Gardens was stripped due to construction incompleteness. Salt Lake Stadium—a multi-use stadium—is the world's second highest-capacity football stadium. Calcutta Cricket and Football Club is the second-oldest cricket club in the world. National and international sports events are also held in Durgapur, Siliguri and Kharagpur. Notable sports persons from West Bengal include former Indian national cricket captain Sourav Ganguly, Pankaj Roy Olympic tennis bronze medallist Leander Paes, and chess grand master Dibyendu Barua. Other major sporting icons over the years include famous football players such as Chuni Goswami, PK Banerjee and Sailen Manna as well as swimmer Mihir Sen and athlete Jyotirmoyee Sikdar (winner of gold medals at the Asian Games).
Panoramic View of the Eden Gardens Stadium during IPL 2008
References.
</dl>

</doc>
<doc id="34043" url="http://en.wikipedia.org/wiki?curid=34043" title="Wormhole">
Wormhole

A wormhole, also known as "Einstein-Rosen Bridge", is a hypothetical topological feature that would fundamentally be a shortcut through spacetime. A wormhole is much like a tunnel with two ends, each in separate points in spacetime.
For a simplified notion of a wormhole, visualize space as a two-dimensional (2D) surface. In this case, a wormhole can be pictured as a hole in that surface that leads into a 3D tube (the inside surface of a cylinder). This tube then re-emerges at another location on the 2D surface with a similar hole as the entrance. An actual wormhole would be analogous to this, but with the spatial dimensions raised by one. For example, instead of circular holes on a 2D plane, the entry and exit points could be visualized as spheres in 3D space.
Researchers have no observational evidence for wormholes, but the equations of the theory of general relativity have valid solutions that contain wormholes. Because of its robust theoretical strength, a wormhole is one of the great physics metaphors for teaching general relativity. The first type of wormhole solution discovered was the Schwarzschild wormhole, which would be present in the Schwarzschild metric describing an eternal black hole, but it was found that it would collapse too quickly for anything to cross from one end to the other. Wormholes that could be crossed in both directions, known as traversable wormholes, would only be possible if exotic matter with negative energy density could be used to stabilize them.
The Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where energy can be "arbitrarily" negative at a given point. Many physicists, such as Stephen Hawking, Kip Thorne and others, therefore argue that such effects might make it possible to stabilize a traversable wormhole. Physicists have not found any natural process that would be predicted to form a wormhole naturally in the context of general relativity, although the quantum foam hypothesis is sometimes used to suggest that tiny wormholes might appear and disappear spontaneously at the Planck scale, and stable versions of such wormholes have been suggested as dark matter candidates. It has also been proposed that, if a tiny wormhole held open by a negative-mass cosmic string had appeared around the time of the Big Bang, it could have been inflated to macroscopic size by cosmic inflation.
The American theoretical physicist John Archibald Wheeler coined the term "wormhole" in 1957; the German mathematician Hermann Weyl, however, had proposed the wormhole theory in 1921, in connection with mass analysis of electromagnetic field energy.
This analysis forces one to consider situations... where there is a net flux of lines of force, through what topologists would call "a handle" of the multiply-connected space, and what physicists might perhaps be excused for more vividly terming a "wormhole".—John Wheeler in "Annals of Physics"
Definition.
The basic notion of an intra-universe wormhole is that it is a compact region of spacetime whose boundary is topologically trivial, but whose interior is not simply connected. Formalizing this idea leads to definitions such as the following, taken from Matt Visser's "Lorentzian Wormholes".
If a Minkowski spacetime contains a compact region Ω, and if the topology of Ω is of the form Ω ~ R x Σ, where Σ is a three-manifold of the nontrivial topology, whose boundary has topology of the form ∂Σ ~ S2, and if, furthermore, the hypersurfaces Σ are all spacelike, then the region Ω contains a quasipermanent intra-universe wormhole.
Characterizing inter-universe wormholes is more difficult, with little consideration being given to available technology. For example, one can imagine a "baby" universe connected to its "parent" by a narrow "umbilicus". One might like to regard the umbilicus as the throat of a wormhole, but the spacetime is simply connected. For this reason, wormholes have been defined "geometrically", as opposed to "topologically", as regions of spacetime that constrain the incremental deformation of closed surfaces. For example, in Enrico Rodrigo’s "The Physics of Stargates, "a wormhole is defined informally as: a region of spacetime containing a "world tube" (the time evolution of a closed surface) that cannot be continuously deformed (shrunk) to a world line (the time evolution of a point).
Schwarzschild wormholes.
Lorentzian wormholes known as "Schwarzschild wormholes" or "Einstein–Rosen bridges" are connections between areas of space that can be modeled as vacuum solutions to the Einstein field equations, and that are now understood to be intrinsic parts of the maximally extended version of the Schwarzschild metric describing an eternal black hole with no charge and no rotation. Here, "maximally extended" refers to the idea that the spacetime should not have any "edges": for any possible trajectory of a free-falling particle (following a Geodesic in the spacetime, it should be possible to continue this path arbitrarily far into the particle's future or past, unless the trajectory hits a gravitational singularity like the one at the center of the black hole's interior. In order to satisfy this requirement, it turns out that in addition to the black hole interior region that particles enter when they fall through the event horizon from the outside, there must be a separate white hole interior region that allows us to extrapolate the trajectories of particles that an outside observer sees rising up "away" from the event horizon. And just as there are two separate interior regions of the maximally extended spacetime, there are also two separate exterior regions, sometimes called two different "universes", with the second universe allowing us to extrapolate some possible particle trajectories in the two interior regions. This means that the interior black hole region can contain a mix of particles that fell in from either universe (and thus an observer who fell in from one universe might be able to see light that fell in from the other one), and likewise particles from the interior white hole region can escape into either universe. All four regions can be seen in a spacetime diagram that uses Kruskal–Szekeres coordinates.
In this spacetime, it is possible to come up with coordinate systems such that if you pick a hypersurface of constant time (a set of points that all have the same time coordinate, such that every point on the surface has a space-like separation, giving what is called a 'space-like surface') and draw an "embedding diagram" depicting the curvature of space at that time, the embedding diagram will look like a tube connecting the two exterior regions, known as an "Einstein–Rosen bridge". Note that the Schwarzschild metric describes an idealized black hole that exists eternally from the perspective of external observers; a more realistic black hole that forms at some particular time from a collapsing star would require a different metric. When the infalling stellar matter is added to a diagram of a black hole's history, it removes the part of the diagram corresponding to the white hole interior region, along with the part of the diagram corresponding to the other universe.
The Einstein–Rosen bridge was discovered by Ludwig Flamm in 1916 a few months after Schwarzschild published his solution, and was rediscovered (although it is hard to imagine that Einstein had not seen Flamm's paper when it came out) by Albert Einstein and his colleague Nathan Rosen, who published their result in 1935. However, in 1962 John A. Wheeler and Robert W. Fuller published a paper showing that this type of wormhole is unstable if it connects two parts of the same universe, and that it will pinch off too quickly for light (or any particle moving slower than light) that falls in from one exterior region to make it to the other exterior region.
According to general relativity, the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole. In the Einstein–Cartan–Sciama–Kibble theory of gravity, however, it forms a regular Einstein–Rosen bridge. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. Torsion naturally accounts for the quantum-mechanical, intrinsic angular momentum (spin) of matter. The minimal coupling between torsion and Dirac spinors generates a repulsive spin–spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction prevents the formation of a gravitational singularity. Instead, the collapsing matter reaches an enormous but finite density and rebounds, forming the other side of the bridge.
Before the stability problems of Schwarzschild wormholes were apparent, it was proposed that quasars were white holes forming the ends of wormholes of this type.
Although Schwarzschild wormholes are not traversable in both directions, their existence inspired Kip Thorne to imagine traversable wormholes created by holding the "throat" of a Schwarzschild wormhole open with exotic matter (material that has negative mass/energy).
Traversable wormholes.
Lorentzian traversable wormholes would allow travel in both directions from one part of the universe to another part of that same universe very quickly or would allow travel from one universe to another. The possibility of traversable wormholes in general relativity was first demonstrated by Kip Thorne and his graduate student Mike Morris in a 1988 paper. For this reason, the type of traversable wormhole they proposed, held open by a spherical shell of exotic matter, is referred to as a "Morris–Thorne wormhole". Later, other types of traversable wormholes were discovered as allowable solutions to the equations of general relativity, including a variety analyzed in a 1989 paper by Matt Visser, in which a path through the wormhole can be made where the traversing path does not pass through a region of exotic matter. However, in the pure Gauss–Bonnet gravity (a modification to general relativity involving extra spatial dimensions which is sometimes studied in the context of brane cosmology) exotic matter is not needed in order for wormholes to exist—they can exist even with no matter. A type held open by negative mass cosmic strings was put forth by Visser in collaboration with Cramer "et al.", in which it was proposed that such wormholes could have been naturally created in the early universe.
Wormholes connect two points in spacetime, which means that they would in principle allow travel in time, as well as in space. In 1988, Morris, Thorne and Yurtsever worked out explicitly how to convert a wormhole traversing space into one traversing time. However, according to general relativity, it would not be possible to use a wormhole to travel back to a time earlier than when the wormhole was first converted into a time machine by accelerating one of its two mouths.
Raychaudhuri's theorem and exotic matter.
To see why exotic matter is required, consider an incoming light front traveling along geodesics, which then crosses the wormhole and re-expands on the other side. The expansion goes from negative to positive. As the wormhole neck is of finite size, we would not expect caustics to develop, at least within the vicinity of the neck. According to the optical Raychaudhuri's theorem, this requires a violation of the averaged null energy condition. Quantum effects such as the Casimir effect cannot violate the averaged null energy condition in any neighborhood of space with zero curvature, but calculations in semiclassical gravity suggest that quantum effects may be able to violate this condition in curved spacetime. Although it was hoped recently that quantum effects could not violate an achronal version of the averaged null energy condition, violations have nevertheless been found, so it remains an open possibility that quantum effects might be used to support a wormhole.
Faster-than-light travel.
The impossibility of faster-than-light relative speed only applies locally. Wormholes might allow superluminal (faster-than-light) travel by ensuring that the speed of light is not exceeded locally at any time. While traveling through a wormhole, subluminal (slower-than-light) speeds are used. If two points are connected by a wormhole whose length is shorter than the distance between them "outside" the wormhole, the time taken to traverse it could be less than the time it would take a light beam to make the journey if it took a path through the space "outside" the wormhole. However, a light beam traveling through the wormhole would of course beat the traveler.
Time travel.
The theory of general relativity predicts that if traversable wormholes exist, they could allow time travel. This would be accomplished by accelerating one end of the wormhole to a high velocity relative to the other, and then sometime later bringing it back; relativistic time dilation would result in the accelerated wormhole mouth aging less than the stationary one as seen by an external observer, similar to what is seen in the twin paradox. However, time connects differently through the wormhole than outside it, so that synchronized clocks at each mouth will remain synchronized to someone traveling through the wormhole itself, no matter how the mouths move around. This means that anything which entered the accelerated wormhole mouth would exit the stationary one at a point in time prior to its entry.
For example, consider two clocks at both mouths both showing the date as 2000. After being taken on a trip at relativistic velocities, the accelerated mouth is brought back to the same region as the stationary mouth with the accelerated mouth's clock reading 2004 while the stationary mouth's clock read 2012. A traveler who entered the accelerated mouth at this moment would exit the stationary mouth when its clock also read 2004, in the same region but now eight years in the past. Such a configuration of wormholes would allow for a particle's world line to form a closed loop in spacetime, known as a closed timelike curve. An object traveling through a wormhole could carry energy or charge from one time to another, but this would not violate conservation of energy or charge in each time, because the energy/charge of the wormhole mouth itself would change to compensate for the object that fell into it or emerged from it.
It is thought that it may not be possible to convert a wormhole into a time machine in this manner; the predictions are made in the context of general relativity, but general relativity does not include quantum effects. Analyses using the semiclassical approach to incorporating quantum effects into general relativity have sometimes indicated that a feedback loop of virtual particles would circulate through the wormhole and pile up on themselves, driving the energy density in the region very high and possibly destroying it before any information could be passed through it, in keeping with the chronology protection conjecture. The debate on this matter is described by Kip S. Thorne in the book "Black Holes and Time Warps", and a more technical discussion can be found in "The quantum physics of chronology protection" by Matt Visser. There is also the Roman ring, which is a configuration of more than one wormhole. This ring seems to allow a closed time loop with stable wormholes when analyzed using semiclassical gravity, although without a full theory of quantum gravity it is uncertain whether the semiclassical approach is reliable in this case.
Interuniversal travel.
A possible resolution to the paradoxes resulting from wormhole-enabled time travel rests on the many-worlds interpretation of quantum mechanics. In 1991 David Deutsch showed that quantum theory is fully consistent (in the sense that the so-called density matrix can be made free of discontinuities) in spacetimes with closed timelike curves. However, later it was shown that such model of closed timelike curve can have internal inconsistencies as it will lead to strange phenomena like distinguishing non orthogonal quantum states and distinguishing proper and improper mixture. Accordingly, the destructive positive feedback loop of virtual particles circulating through a wormhole time machine, a result indicated by semi-classical calculations, is averted. A particle returning from the future does not return to its universe of origination but to a parallel universe. This suggests that a wormhole time machine with an exceedingly short time jump is a theoretical bridge between contemporaneous parallel universes. Because a wormhole time-machine introduces a type of nonlinearity into quantum theory, this sort of communication between parallel universes is consistent with Joseph Polchinski’s discovery of an "Everett phone" in Steven Weinberg’s formulation of nonlinear quantum mechanics. Such a possibility is depicted in the science-fiction 2014 movie "Interstellar (film)".
Metrics.
Theories of "wormhole metrics" describe the spacetime geometry of a wormhole and serve as theoretical models for time travel. An example of a (traversable) wormhole metric is the following:
One type of non-traversable wormhole metric is the Schwarzschild solution (see the first diagram):
In fiction.
Wormholes are a common element in science fiction as they allow interstellar, intergalactic, and sometimes interuniversal travel within human timescales. They have also served as a method for time travel.
References.
</dl>

</doc>
<doc id="34044" url="http://en.wikipedia.org/wiki?curid=34044" title="Web banner">
Web banner

A web banner or banner ad is a form of advertising on the World Wide Web delivered by an ad server. This form of online advertising entails embedding an advertisement into a web page. It is intended to attract traffic to a website by linking to the website of the advertiser. In many cases, banners are delivered by a central ad server.
When the advertiser scans their logfiles and detects that a web user has visited the advertiser's site from the content site by clicking on the banner ad, the advertiser sends the content provider some small amount of money (usually around five to ten US cents). This payback system is often how the content provider is able to pay for the Internet access to supply the content in the first place. Usually though, advertisers use ad networks to serve their advertisements, resulting in a revshare system and higher quality ad placement.
Web banners function the same way as traditional advertisements are intended to function: notifying consumers of the product or service and presenting reasons why the consumer should choose the product in question, a fact first documented on HotWired in 1996 by researchers Rex Briggs and Nigel Hollis. Web banners differ in that the results for advertisement campaigns may be monitored real-time and may be targeted to the viewer's interests. Behavior is often tracked through the use of a click tag.
Many web surfers regard these advertisements as highly annoying because they distract from a web page's actual content or waste bandwidth. Newer web browsers often include options to disable pop-ups or block images from selected websites. Another way of avoiding banners is to use a proxy server that blocks them, such as Privoxy. Web browsers may also have extensions available that block banners, for example Adblock Plus for Mozilla Firefox, or AdThwart for Google Chrome and ie7pro for Internet Explorer.
History.
The pioneer of online advertising was Prodigy, a company owned by IBM and Sears at the time. Prodigy used online advertising first to promote Sears products in the 1980s, and then other advertisers, including AOL, one of Prodigy's direct competitors. Prodigy was unable to capitalize on any of its first mover advantage in online advertising.
The first clickable web ad (which later came to be known by the term "banner ad") was sold by Global Network Navigator (GNN) in 1993 to Heller, Ehrman, White, & McAuliffe, a now defunct law firm with a Silicon Valley office. GNN was the first commercially supported web publication and one of the very first commercial web sites ever.
HotWired was the first web site to sell banner ads in large quantities to a wide range of major corporate advertisers. Andrew Anker was HotWired's first CEO. Rick Boyce, a former media buyer with San Francisco advertising agency Hal Riney & Partners, spearheaded the sales effort for the company. HotWired coined the term "banner ad" and was the first company to provide click through rate reports to its customers. The first web banner sold by HotWired was paid for by AT&T Corp. and was put online on October 27, 1994. Another source also credits Hotwired and October 1994, but has Coors' "Zima" campaign as the first web banner.
In May 1994, Ken McCarthy mentored Boyce in his transition from traditional to online advertising and first introduced the concept of a clickable/trackable ad. He stated that he believed that only a direct response model—in which the return on investment of individual ads was measured—would prove sustainable over the long run for online advertising. In spite of this prediction, banner ads were valued and sold based on the number of impressions they generated.
The first central ad server was released in July 1995 by Focalink Communications, which enabled the management, targeting, and tracking of online ads. A local ad server quickly followed from NetGravity in January 1996. The technology innovation of the ad server, together with the sale of online ads on an impression basis, fueled a dramatic rise in the proliferation of web advertising and provided the economic foundation for the web industry from the period of 1994 to 2000.
The new online advertising model that emerged in the early years of the 21st century, introduced by GoTo.com (later Overture, then Yahoo! and mass marketed by Google's AdWords program), relies heavily on tracking ad response rather than impressions.
Standard sizes.
Ad sizes have been standardized to some extent by the IAB. Prior to the IAB standardization, banner ads appeared in over 250 different sizes. However, some websites and advertising networks (outside the Eurosphere or North America) may not use any or all of the IAB base ad sizes. The IAB ad sizes are :

</doc>
<doc id="34046" url="http://en.wikipedia.org/wiki?curid=34046" title="Vedda people">
Vedda people

Veddas (Sinhalese: වැද්දා ], Tamil: வேடுவர் "Vēṭuvar") are an indigenous people of Sri Lanka. They, amongst other self-identified native communities such as Coast Veddas and Anuradhapura Veddas, are accorded indigenous status. 
According to the genesis chronicle of the Sinhala people, the Mahavamsa ("Great Chronicle"), written in the 5th century CE, the Pulindas believed to refer to Veddas are descended from Prince Vijaya (6th–5th century BCE), the founding father of the Tamil nation, through Kuveni, a woman of the indigenous "Yakkha" he married. The "Mahavansa" relates that following the repudiation of Kuveni by Vijaya, in favour of a Kshatriya-caste princess from Pandya, their two children, a boy and a girl, departed to the region of "Sumanakuta" (Adam's Peak in the Ratnapura District), where they multiplied, giving rise to the Veddas. Anthropologists such as the Seligmanns ("The Veddhas" 1911) believed the Veddas to be identical with the "Yakkha".
Veddas are also mentioned in Robert Knox's history of his captivity by the King of Kandy in the 17th century. Knox described them as "wild men", but also said there was a "tamer sort", and that the latter sometimes served in the king's army.
The Ratnapura District, which is part of the Sabaragamuwa Province, is known to have been inhabited by the Veddas in the distant past. This has been shown by scholars like Nandadeva Wijesekera (Veddhas in transition 1964). The very name "Sabaragamuwa" is believed to have meant the village of the "Sabaras" or "forest barbarians". Such place-names as "Vedda-gala" (Vedda Rock), "Vedda-ela" (Vedda Canal) and "Vedi-kanda" (Vedda Mountain) in the Ratnapura District also bear testimony to this. As "Wijesekera" observes, a strong Vedda element is discernible in the population of "Vedda-gala" and its environs.
Language.
The original language of the Veddas is the Vedda language. Today it is used primarily by the interior Veddas of Dambana. Communities, such as Coast Veddas and Anuradhapura Veddas, that do not identify themselves strictly as Veddas also use Vedda language in part for communication during hunting and or for religious chants. When a systematic field study was conducted in 1959 it was determined that the language was confined to the older generation of Veddas from Dambana. In 1990s self-identifying Veddas knew few words and phrases in the Vedda language, but there were individuals who knew the language comprehensively. Initially there was considerable debate amongst linguists as to whether Vedda is a dialect of Sinhalese or an independent language. Later studies indicate that it diverged from its parent stock in the 10th century and became a Creole and a stable independent language by the 13th century, under the influence of Sinhalese. 
The parent Vedda language(s) is of unknown genetic origins, while Sinhalese is of the Indo-Aryan branch of Indo-European languages. Phonologically it is distinguished from Sinhalese by the higher frequency of palatal sounds C and J. The effect is also heightened by the addition of inanimate suffixes. Morphologically Vedda language word class is divided into nouns, verbs and invariables with unique gender distinctions in animate nouns. Per its Creole tradition, it has reduced and simplified many forms of Sinhalese such as second person pronouns and denotations of negative meanings. Instead borrowing new words from Sinhalese Vedda created combinations of words from a limited lexical stock. Vedda also maintains many archaic Sinhalese terms prior to the 10th to 12th centuries, as a relict of its close contact with Sinhalese. Vedda also retains a number of unique words that cannot be derived from Sinhalese. Conversely, Sinhalese has also borrowed from the original Vedda language, words and grammatical structures, differentiating it from its related Indo-Aryan languages. Vedda has exerted a substratum influence in the formation of Sinhalese.
Veddas that have adopted Sinhala are found primarily in the southeastern part of the country,
especially in the vicinity of Bintenne in Uva District. There are also Veddas that have adopted Sinhala who live in Anuradhapura District in the North Central Province.
Another group, often termed East Coast Veddas, is found in coastal areas of the Eastern Province, between Batticaloa and Trincomalee. These Veddas have adopted Tamil as their mother tongue.
Cultural aspects.
Religion.
The original religion of Veddas is animism. The Sinhalized interior Veddahs follow a mix of animism and nominal Buddhism, whereas the Tamilized east coast Veddahs follow a mix of animism and nominal Hinduism, which is known as folk Hinduism among anthropologists.
One of the most distinctive features of Vedda religion is the worship of dead ancestors, who are called "nae yaku" among the Sinhala-speaking Veddas and are invoked for game and yams. There are also peculiar deities unique to Veddas, such as "Kande Yakka".
Veddas, along with the Island's Buddhist, Hindu and Muslim communities, venerate the temple complex situated at Kataragama, showing the syncretism that has evolved over 2,000 years of coexistence and assimilation. Kataragama is supposed to be the site where the Hindu god Skanda or Murugan in Tamil met and married a local tribal girl, Valli, who in Sri Lanka is believed to have been a Vedda.
There are a number of less famous shrines across the island, which are as sacred to the Veddas and to other communities.
Rituals.
Vedda marriage is a simple ceremony. It consists of the bride tying a bark rope ("diya lanuva") that she has twisted, around the waist of the groom. This symbolizes the bride's acceptance of the man as her mate and life partner. Although marriage between cross-cousins was the norm until recently, this has changed significantly, with Vedda women even contracting marriages with their Sinhalese and Moor neighbours.
In Vedda society, women are in many respects men's equals. They are entitled to similar inheritance. Monogamy is the general rule, though a widow would frequently marry her husband's brother as a means of support and consolation (widow inheritance). They also do not practice a caste system.
Death, too, is a simple affair without ostentatious funeral ceremonies where the corpse of the deceased is promptly buried.
Although the medical knowledge of the Vedda is limited, it nevertheless appears to be sufficient. For example, pythonesa oil ("pimburu tel"), a local remedy used for healing wounds, has proven to be very successful in the treatment of fractures and deep cuts.
Burial.
Since the opening of colonisation schemes, Vedda burials changed when they dug graves of 4–5 feet deep and wrapped the body wrapped cloth and covered it with leaves and earth. The Veddas also laid the body between the scooped out trunks of the "gadumba" tree before they buried it. At the head of the grave were kept three open coconuts and a small bundle of wood, while at its foot were kept an opened coconut and an untouched coconut. Certain cactus species "(pathok)" were planted at the head, the middle and the foot. Personal possessions like the bow and arrow, betel pouch, were also buried. This practice varied by community. The contents of the betel pouch of the deceased were eaten after his death.
The dead body was scented or smeared with juice from the leaves of jungle trees or lime trees. The foot or the head of the grave was never lit either with fire or wax, and water was not kept in a vessel by the grave side.
Cult of the Dead.
The Veddas believe in the cult of the dead. They worshipped and made incantations to their "Nae Yakka" (Relative Spirit) followed by other customary ritual (called the "Kiri Koraha") which is still in vogue among the surviving Gam Veddas of Rathugala, Pollebedda Dambana and the Henanigala Vedda re-settlement (in Mahaweli systems off Mahiyangane).
They believed that the spirit of their dead would haunt them bringing forth diseases and calamity. To appease the dead spirit they invoke the blessings of the Nae Yakka and other spirits, like "Bilinda Yakka, Kande Yakka" followed by the dance ritual of the "Kiri Koraha".
"According to Sarasin Cousins (in 1886) and Seligmann's book - 'The Veddas' (1910)."
"When man or woman dies from sickness, the body is left in the cave or rock shelter where the death took place, the body is not washed or dressed or ornamented in any way, but is generally allowed to be in the natural supine position and is covered with leaves and branches. This was formerly the universal custom and still persists among the less sophisticated Veddas who sometimes in addition place a large stone upon the chest for which no reason could be given, this is observed at Sitala Wanniya (off Polle-bedda close to Maha Oya), where the body is still covered with branches and left where the death occurred."
Clothing.
Until fairly recent times, the raiment of the Veddas was remarkably scanty. In the case of men, it consisted only of a loincloth suspended with a string at the waist, while in the case of women, it was a piece of cloth that extended from the navel to the knees. Today, however, Vedda attire is more covering, men wear a short sarong extending from the waist to the knees, while the womenfolk clad themselves in a garment similar to the Sinhalese "diya-redda" which extends from the breastline to the knees.
Music.
"Bori Bori Sellam-Sellam Bedo Wannita,"
"Palletalawa Navinna-Pita Gosin Vetenne,"
"Malpivili genagene-Hele Kado Navinne,"
"Diyapivili Genagene-Thige Bo Haliskote Peni,"
"Ka tho ipal denne"
Meaning of this song - The bees from yonder hills of Palle Talawa and Kade suck nectar from the flowers and made the honeycomb. So why should you give them undue pain when there is no honey by cutting the honeycomb.
Art.
Vedda cave drawings such as those found at "Hamangala" provide graphic evidence of the sublime spiritual and artistic vision achieved by the ancestors of today's "Wanniyala-Aetto" people. Most researchers today agree that the artists most likely were the "Wanniyala-Aetto" women who spent long hours in these caves waiting for their menfolk's return from the hunt.
Understood from this perspective, these cave drawings depict brilliant feats of "Wanniyala-Aetto" culture as seen through the eyes of its womenfolk. The simple yet graceful abstract figures are portrayed engaging in feats of vision and daring that place them firmly above even the greatest beasts of their jungle habitat.
The nimbus or halo about the human figures' heads represents the sun's disc and, equally, the sacred power bordering upon divinity that accrues not only to great hunters but to all those endowed with the vision to behold and apprehend the marvel of divinity in humble guise. Even up to modern times, the Wanniyala-Aetto used to swear oaths of truth by the divinity of the sun, saying 'upon Maha Suriyo Deviyo'.
Such cave drawings have long served as visual memory aids and as teaching tools for the transmission of ancestral wisdom traditions to succeeding generations. To this day, they provide silent testimony to the profound heights attained by Lanka's indigenous culture expressed with elegant simplicity that people of all communities may appreciate.
Livelihood.
Veddas were originally hunter-gatherers. They used bows and arrows to hunt game, and also gathered wild plants and honey. Many Veddas also farm, frequently using slash and burn or swidden cultivation, which is called "chena" in Sri Lanka. East Coast Veddas also practice fishing. Veddas are famously known for their rich meat diet. Venison and the flesh of rabbit, turtle, tortoise, monitor lizard, wild boar and the common brown monkey are consumed with much relish. The Veddas kill only for food and do not harm young or pregnant animals. Game is commonly shared amongst the family and clan. Fish are caught by employing fish poisons such as the juice of the "pus-vel" (Entada scandens) and "daluk-kiri" (Cactus milk).
Vedda culinary fare is also deserving of mention. Amongst the best known are "gona perume", which is a sort of sausage containing alternate layers of meat and fat, and "goya-tel-perume", which is the tail of the monitor lizard (talagoya), stuffed with fat obtained from its sides and roasted in embers. Another Vedda delicacy is dried meat preserve soaked in honey. In the olden days, the Veddas used to preserve such meat in the hollow of a tree, enclosing it with clay.
Such succulent meat served as a ready food supply in times of scarcity. The early part of the year (January–February) is considered to be the season of yams and mid-year (June–July) that of fruit and honey, while hunting is availed of throughout the year. Nowadays, more and more Vedda folk have taken to "Chena" (slash and burn) cultivation. "Kurakkan" (Eleusine coracana) is cultivated very often. Maize, yams, gourds and melons are also cultivated. In the olden days, the dwellings of the Veddas consisted of caves and rock shelters. Today, they live in unpretentious huts of wattle, daub and thatch.
In the reign of King Datusena (6th century CE) the Mahaweli ganga was diverted at Minipe in the Minipe canal nearly 47 miles long said to be constructed with help from the Yakkas. The Mahawamsa refers to the canal as Yaka-bendi-ela. When the Ruwanweli Seya was built in King Dutugemunu's time (2nd century BCE) the Veddas procured the necessary minerals from the jungles.
King Parakrama Bahu the great (12th century) in his war against the rebels employed these Veddas as scouts.
In the reign of King Rajasinghe II (17th century) in his battle with the Dutch he had a Vedda regiment. In the abortive Uva-Welessa revolt of 1817-1818 of the British times, led by Keppetipola Disawe, the Veddas too fought with the rebels against the British forces.
Current status.
Some observers have said Veddas are disappearing and have lamented the decline of their distinct culture. Development, government forest reserve restrictions, and the civil war have disrupted traditional Vedda ways of life. Dr. Wiveca Stegeborn, an anthropologist, has been studying the Vedda since 1977 and alleges that their young women are being tricked into accepting contracts to the Middle East as domestic workers when in fact they will be trafficked into prostitution or sold as sex slaves.
However, cultural assimilation of Veddas with other local populations has been going on for a long time. "Vedda" has been used in Sri Lanka to mean not only hunter-gatherers, but also to refer to any people who adopt an unsettled and rural way of life and thus can be a derogatory term not based on ethnic group. Thus, over time, it is possible for non-Vedda groups to become Veddas, in this broad cultural sense. Vedda populations of this kind are increasing in some districts.
A recent genetic study on Veddas has supported them as an outlier in comparison with other Sri Lankan populations 
Today many Sinhalese people and some east coast Tamils claim that they have some trace of Veddah blood. Intermarriage between Veddas and Sinhalese is very frequent. They are not considered outcasts in Sri Lankan society, unlike the low caste Rodiyas (see Caste in Sri Lanka).
The current leader of the Wanniyala-Aetto community is Uru Warige Wanniya.
Further reading.
"The Cambridge Encyclopedia of Hunters and Gatherers", editor Richard B. Lee.
External links.
A great deal of information on them can be found at 

</doc>
<doc id="34049" url="http://en.wikipedia.org/wiki?curid=34049" title="Wart">
Wart

A wart is a small, rough growth resembling a cauliflower or a solid blister. It typically occurs on humans' hands or feet but often in other locations. Warts are caused by a viral infection, specifically by one of the many types of human papillomavirus (HPV). There are as many as 10 varieties of warts, the most common considered to be mostly harmless. It is possible to get warts from others; they are contagious and usually enter the body in an area of broken skin.
Types.
A range of types of wart have been identified, varying in shape and site affected, as well as the type of human papillomavirus involved. These include:
Cause.
Warts are caused by the human papilloma virus (HPV). There are about 130 known types of human papilloma viruses. HPV infects the squamous epithelium, usually of the skin or genitals, but each HPV type is typically only able to infect a few specific areas on the body. Many HPV types can produce a benign growth, often called a "wart" or "papilloma", in the area they infect. Many of the more common HPV and wart types are listed below.
Pathophysiology.
Common warts have a characteristic appearance under the microscope. They have thickening of the stratum corneum (hyperkeratosis), thickening of the stratum spinosum (acanthosis), thickening of the stratum granulosum, rete ridge elongation, and large blood vessels at the dermoepidermal junction.
Prevention.
Gardasil is an HPV vaccine aimed at preventing cervical cancers and genital warts. Gardasil is designed to prevent infection with HPV types 16, 18, 6, and 11. HPV types 16 and 18 currently cause about 70% of cervical cancer cases, and also cause some vulvar, vaginal, penile and anal cancers. HPV types 6 and 11 are responsible for 90% of documented cases of genital warts. Unfortunately the HPV vaccines do not currently prevent the virus strain responsible for verrucas (plantar warts).
Treatment.
There are many treatments and procedures associated with wart removal. A review of clinical trials of various cutaneous wart treatments concluded that topical treatments containing salicylic acid were more effective than placebo. Cryotherapy appears to be as effective as salicylic acid, but there have been fewer trials.
Medication.
Another product available over-the-counter that can aid in wart removal is silver nitrate in the form of a caustic pencil, which is also available at drug stores. In a placebo-controlled study of 70 patients, silver nitrate given over nine days resulted in clearance of all warts in 43% and improvement in warts in 26% one month after treatment compared to 11% and 14%, respectively, in the placebo group. The instructions must be followed to minimize staining of skin and clothing. Occasionally pigmented scars may develop.
Cryosurgery or cryotherapy devices using a dimethyl ether – propane mixture are inexpensive. A disadvantage is that the sponge applicator is too large for small warts, and the temperature achieved is not nearly as low as with liquid nitrogen. Complications include blistering of normal skin if excess freezing is not controlled.
Several randomized, controlled trials have found that zinc sulfate, consumed orally, often reduces or eliminates warts. The zinc sulfate dosage used in medical trials for treatment of warts was between 5 and 10 mg/kg/day. For elemental zinc, a lower dosage of 2.5 mg/kg/day may be appropriate as large amounts of zinc may cause a copper deficiency. Other trials have found that topical zinc sulfate solution or zinc oxide are also effective.
A 2014 study indicates that lopinavir is effective against the human papilloma virus (HPV). The study used the equivalent of one tablet twice a day applied topically to the cervices of women with high-grade and low-grade precancerous conditions. After three months of treatment, 82.6% of the women who had high-grade disease had normal cervical conditions, confirmed by smears and biopsies.
Folk remedies.
A variety of traditional folk remedies and rituals claim to be able to remove warts.
The acrid yellow sap of Greater Celandine is used as a traditional wart remedy. The sap can be applied directly to the wart in a similar manner to concentrated salicylic acid solution, but in more modest quantities.
In "The Adventures of Tom Sawyer", Mark Twain has his characters discuss a variety of such remedies. Tom Sawyer proposes "spunk-water" (or "stump-water", the water collecting in the hollow of a tree stump) as a remedy for warts on the hand. You put your hand into the water at midnight and say:
and then "walk away quick, eleven steps, with your eyes shut, and then turn around three times and walk home without speaking to anybody. Because if you speak the charm's busted." This is held to be superior to Huckleberry Finn's preferred remedy which involved throwing a dead cat into a graveyard. Another remedy involved splitting a bean, drawing blood from the wart and putting it on one of the halves, and burying that half at a crossroads at midnight. The theory of operation is that the blood on the buried bean will draw away the wart. Twain is recognized as an early collector and recorder of genuine American folklore.
Similar practices are recorded elsewhere. In Louisiana, one remedy for warts involves rubbing the wart with a potato, which is then buried; when the "buried potato dries up, the wart will be cured". Another remedy similar to Twain's is reported from Northern Ireland, where water from a specific well on Rathlin Island is credited with the power to cure warts.
A longstanding tradition holds that touching toads will cause warts. The most common Northern Hemisphere toads have glands that protrude from their skin that superficially resemble warts. Warts are caused by a virus, and toads do not harbor it.

</doc>
<doc id="34050" url="http://en.wikipedia.org/wiki?curid=34050" title="Warrant officer">
Warrant officer

A warrant officer (WO) is an officer in a military organization who is designated an officer by a warrant, as distinguished from a commissioned officer who is designated an officer by a commission, and a non-commissioned officer who is designated an officer, often by virtue of seniority.
The rank was first used in the (then) English Royal Navy and is today used in most services in many countries, including the Commonwealth nations and the United States.
Outside the United States, warrant officers are included in the "Other Ranks" (OR) category, equivalent to the US "E" (Enlisted) category and rank between non-commissioned officers and commissioned officers. The warrant officers in Commonwealth navies rank between chief petty officer and sub-lieutenant, in Commonwealth air forces between flight sergeant and pilot officer, and in Commonwealth armies between staff sergeant and second-lieutenant.
Warrant officers in the United States are classified as officers and are in the "W" category (NATO "WO"); they are technical leaders and specialists. Chief warrant officers are commissioned by the president of the United States and take the same oath as regular commissioned officers. They may be technical experts with a long service as enlisted personnel, or direct entrants, notably for U.S. Army helicopter pilots.
History: origins in the Royal Navy.
The warrant officer corps began in the 13th century in the nascent English Royal Navy. At that time, noblemen with military experience took command of the new Navy, adopting the military ranks of lieutenant and captain. These officers often had no knowledge of life on board a ship—let alone how to navigate such a vessel—and relied on the expertise of the ship's master and other seamen who tended to the technical aspects of running the ship. As cannon came into use, the officers also required gunnery experts; specialist gunners began to appear in the 16th century and also had warrant officer status. Literacy was one thing that most warrant officers had in common, and this distinguished them from the common seamen: according to the Admiralty regulations, "no person shall be appointed to any station in which he is to have charge of stores, unless he can read and write, and is sufficiently skilled in arithmetic to keep an account of them correctly". Since all warrant officers had responsibility for stores, this was enough to debar the illiterate.
Rank and status in the eighteenth century.
In origin, warrant officers were specialist professionals whose expertise and authority demanded formal recognition. In the eighteenth century they fell into two clear categories: on the one hand, those privileged to share with the commissioned officers in the wardroom and on the quarterdeck; and on the other, those who ranked with more junior members of the ship's crew. Somewhere between the two, however, were the standing officers; notable because, unlike the rest of the ship's company, they remained with the ship even when it was out of commission (e.g. for repair, refitting or replenishment, or whilst laid up); in these circumstances they were under the pay and supervision of the Royal Dockyard.
Wardroom warrant officers.
These classes of warrant officer messed in the wardroom with the commissioned officers:
In the early nineteenth century, they were joined in the wardroom by naval chaplains, who also had warrant officer status (though they were only usually present on larger vessels).
Standing warrant officers.
The standing officers were:
Junior warrant officers.
Other warrant officers included surgeon's mates, boatswain's mates and carpenter's mates, sailmakers, armourers, schoolmasters (involved in the education of boys, midshipmen and others aboard ship) and clerks. Masters-at-arms, who had formerly overseen small-arms provision on board, had by this time taken on responsibility for discipline.
Warrant officers in context.
By the end of the century, the rank structure could be illustrated as follows (the warrant officers are underlined):
Demise of the Royal Naval warrants.
In 1843, the wardroom warrant officers were given commissioned status, while in 1853 the lower-grade warrant officers were absorbed into the new rate of chief petty officer, both classes thereby ceasing to be warrant officers. On 25 July 1864 the standing warrant officers were divided into two grades: warrant officers and chief warrant officers (or "commissioned warrant officers", a phrase that was replaced in 1920 with "commissioned officers promoted from warrant rank", although they were still usually referred to as "commissioned warrant officers", even in official documents).
By the time of the First World War, their ranks had been expanded with the adoption of modern technology in the Navy to include telegraphists, electricians, shipwrights, artificer engineers, etc. Both warrant officers and commissioned warrant officers messed in the warrant officers' mess rather than the wardroom (although in ships too small to have a warrant officers' mess, they did mess in the wardroom). Warrant officers and commissioned warrant officers also carried swords, were saluted by ratings, and ranked between sub-lieutenants and midshipmen.
In 1949, the ranks of warrant officer and commissioned warrant officer were changed to "commissioned officer" and "senior commissioned officer", the latter ranking with but after the rank of lieutenant, and they were admitted to the wardroom, the warrant officers' messes closing down. Collectively, these officers were known as "branch officers", being retitled "special duties" officers in 1956. In 1998, the special duties list was merged with the general list of officers in the Royal Navy, all officers now having the same opportunity to reach the highest commissioned ranks.
Modern usage.
Australia.
The Australian Army has three warrant officer ranks: WO, WO1 and WO2. The most senior army warrant officer is the soldier appointed Regimental Sergeant Major of the Army (RSM-A). The RSM-A is the only holder of the unique rank of warrant officer (WO), introduced in 1991. Substantive RSMs of regiments and battalions hold the rank warrant officer class one (WO1). Distinct from RSMs, WO1 is also reserved for senior soldiers of technical trades; such as Artificer Sergeant Major (ASM). The third rank is warrant officer class two (WO2). The insignia of the three ranks are: a crown for a WO2; the (Australian) Commonwealth coat of arms (changed from the Royal coat of arms in 1976) for a WO1; and the Commonwealth coat of arms surrounded by a laurel wreath for the warrant officer.
The Royal Australian Navy rank of warrant officer (WO) is the navy's only rank appointed by warrant and is equivalent to the army's WO1 (the equivalent of the army's WO2 rank is a chief petty officer). The most senior non-commissioned member of the navy is the warrant officer appointed Warrant Officer of the Navy (WO-N).
The Royal Australian Air Force rank of warrant officer (WOFF) is the air force's only rank appointed by warrant and is equivalent to both the army's WO1 and the navy's WO (the equivalent of the army's WO2 is a flight sergeant). The most senior non-commissioned member is the warrant officer appointed Warrant Officer of the Air Force (WOFF-AF).
Israel Defense Forces.
The רב-נגד משנה "rav nagad mishne" ("warrant officer") and the רב-נגד "rav nagad" ("chief warrant officer") are both non-commissioned officers ranks in the Israel Defense Forces (IDF). Because the IDF is an integrated force, they have a unique rank structure. Israel Defense Forces ranks are the same in all services (army, navy, air force, etc.). The ranks are derived from those of the paramilitary "Haganah" developed in the British Mandate of Palestine period to protect the "Yishuv". This origin is reflected in the slightly-compacted IDF rank structure.
Singapore.
In the Singapore Armed Forces, warrant officers begin as third warrant officers (3WO), previously starting at 2WO. This rank is given to former specialists who have attained the rank of master sergeant and have either gone through, or are about to go through the Warfighter Course at the Specialist and Warrant Officer Advanced School (SWAS) in the Specialist and Warrant Officer Institute (SWI). In order to be promoted to a second warrant officer (2WO) and above, they must have been selected for and graduated from the joint warrant officer course at the SAF Warrant Officer School. Warrant officers rank between specialists and commissioned officers. They ordinarily serve as battalion or brigade regimental sergeant majors. Many of them serve as instructors and subject-matter experts in various training establishments. Warrant officers are also seen on the various staffs headed by the respective specialist officers. There are six grades of warrant officer (3WO, 2WO, 1WO, MWO, SWO & CWO).
Warrant officers used to have their own mess. For smaller camps, this mess are combined with the officers' mess as a combined mess for better camaraderie. Warrant officers have similar responsibilities to commissioned officers. Warrant officers are usually addressed as "sir" by the other ranks or as "warrant (surname)". They are also usually addressed "encik" ("mister" in Malay language) by commissioned officers. It is not a necessity to be saluted by other ranks. They would usually be saluted out of respect by the enlistees.
South Africa.
In the South African National Defence Force a warrant officer is a non-commissioned officer rank. Before 2008 there were two classes - warrant officer class one and two. A warrant officer class one could be appointed to positions such as regimental sergeant major, formation sergeant major or even sergeant major of the army while still in the rank of warrant officer class one. In 2008 the warrant officer ranks were expanded so that the substantive ranks that came with senior appointments now became ranks that an individual kept after moving from that post.
United Kingdom.
Royal Navy.
In 1973, warrant officers reappeared in the Royal Navy, but these appointments followed the army model, with the new warrant officers being ratings rather than officers. They were initially known as fleet chief petty officers (FCPOs), but were renamed warrant officers in the 1980s. They ranked with warrant officers class one in the British Army and Royal Marines and with warrant officers in the Royal Air Force.
There are executive warrant officers for commands and ships. Four branches, surface ships, submarines, Royal Marines and fleet air arm each have a command warrant officer. The senior RN WO is the warrant officer of the naval service.
British Army.
In the British Army, there are two warrant ranks, warrant officer class two (WO2) and warrant officer class one (WO1), the latter being the senior of the two. It used to be more common to refer to these ranks as WOII and WOI (using Roman instead of Arabic numerals). "Warrant officer first class" or "second class" is incorrect. The rank immediately below WO2 is staff sergeant (or colour sergeant). From 1938 to 1940 there was a WO III platoon sergeant major rank.
Royal Marines.
Before 1879, the Royal Marines had no warrant officers: by the end of 1881, the Royal Marines had given warrant rank to their sergeant-majors and some other senior non-commissioned officers, in a similar fashion to the army. When the army introduced the ranks of warrant officer class I and class II in 1915, the Royal Marines did the same shortly after. From February 1920, Royal Marines warrant officers class I were given the same status as Royal Navy warrant officers and the rank of warrant officer class II was abolished in the Royal Marines, with no further promotions to this rank.
The marines had introduced warrant officers equivalent in status to the Royal Navy's from 1910 with the Royal Marines gunner (originally titled gunnery sergeant-major), equivalent to the navy's warrant rank of gunner. Development of these ranks closely paralleled that of their naval counterparts: as in the Royal Navy, by the Second World War there were warrant officers and commissioned warrant officers (e.g. staff sergeant majors), commissioned staff sergeant majors, Royal Marines gunners, commissioned Royal Marines gunners, etc. As officers they were saluted by junior ranks in the Royal Marines and the army. These all became (commissioned) branch officer ranks in 1949, and special duties officer ranks in 1956. These ranks would return in 1972, this time similar to their army counterparts, and not as the RN did before.
Royal Air Force.
The Royal Air Force first used the ranks of warrant officer class I and II as inherited from the Royal Flying Corps. It first used the rank badges of the Royal coat of arms for WOI and the crown for WOII. Until the 1930s, these ranks were often known as sergeant major first and second class. In 1939, the RAF abolished the rank of WOII and retained just the WOI rank, referred to as just warrant officer (WO), which it remains to this day. The RAF has no equivalent to WO2 (NATO OR-8), an RAF WO being equivalent to WO1 (NATO OR-9) and wearing the same badge of rank, the Royal coat of arms. The correct way to address a warrant officer is "sir" or "ma'am" by airmen and "mister or warrant officer -surname-" by officers. Most RAF warrant officers do not hold appointments as in the army or Royal Marines; the exception to this is the station warrant officer, who is considered a "first amongst equals" on an RAF station. Warrant officer is the highest non-commissioned rank and ranks above flight sergeant.
In 1946, the RAF renamed its aircrew warrant officers to master aircrew, a designation which still survives. In 1950, it renamed warrant officers in technical trades to master technicians, a designation which survived only until 1964.
The most senior RAF warrant officer by appointment is the chief of the air staff's warrant officer. He holds the same rank as all other warrant officers.
United States.
In the United States military, a warrant officer (grade W-1 to W-5) is ranked as an officer above the senior-most enlisted ranks, as well as officer cadets and officer candidates, but below the officer grade of O-1 ("NATO: OF-1"). Warrant officers are highly skilled, single-track specialty officers, and while the ranks are authorized by Congress, each branch of the U.S. Armed Forces selects, manages, and utilizes warrant officers in slightly different ways. For appointment to warrant officer (W-1), a warrant is approved by the Service Secretary of the respective branch of service (Secretary of the Army, or Secretary of the Navy for USMC warrant officers), while chief warrant officers (W-2 to W-5) are commissioned by the President of the United States, both warrant officers and chief warrant officers take the same Oath of Office as regular commissioned officers (O-1 to O-10).
Warrant officers command detachments, units, activities, vessels, aircraft, and armored vehicles as well as lead, coach, train, and counsel subordinates. However, the warrant officer's primary task as a leader is to serve as a technical expert, providing valuable skills, guidance, and expertise to commanders and organizations in their particular field.
All U.S. armed services employ warrant officer grades except the U.S. Air Force. Although still technically authorized, the air force discontinued appointing new warrant officers in 1959, retiring its last chief warrant officer from the Air Force Reserve in 1992.
The U.S. Army utilizes warrant officers heavily and separates them into two types: technical and aviation. Army Aviation warrant officers are Army Aviators (aircraft pilots - helicopter and airplane); many claiming to be the best in the world due to the amount of time they get to spend actually flying (as opposed to commissioned officer helicopter pilots who may have to spend much of their time doing other administrative work). Technical warrant officers in the army are specialized in a single track technical area such as intelligence, maintenance, or military police; and provide direct advice and support to commanders. For example, a military police officer and a military intelligence officer both have to be branch qualified in their respective fields; learning how to manage the entire spectrum of their profession. However, within those broad fields you can also have warrant officers such as CID Special Agents (a very specific track within the military police) and Counterintelligence Special Agents (a very specific track within military intelligence). These technical warrant officers allow for a Soldier with subject matter expertise (like non-commissioned officers); but with the authority of a full commissioned officer. Both technical and aviation warrants go through similar training initially at warrant officer candidate school (WOCS); but then go their separate ways. Technical warrant officers in the army must be selected from the non-commissioned officer ranks (typically E-6 through E-9); however, the higher the rank, the easier it is to pass the board and get accepted into the warrant officer program. Aviation warrant officers are able to apply before becoming non-commissioned officers.
The U.S. Navy and U.S. Coast Guard also discontinued the grade of W-1 in 1975, appointing and commissioning all new entrants as chief warrant officer two (pay grade W-2, with rank abbreviation of CWO2). Unlike the army, all navy and coast guard chief warrant officers are selected strictly from the chief petty officer pay grades (E-7 through E-9). The coast guard selects warrant officers from pay grade E-6.

</doc>
<doc id="34051" url="http://en.wikipedia.org/wiki?curid=34051" title="Walter Gilbert">
Walter Gilbert

Walter Gilbert (born March 21, 1932) is an American biochemist physicist, molecular biology pioneer, and Nobel laureate.
Biography.
Walter Gilbert was born in Boston, Massachusetts, on March 21, 1932, the son of Emma (Cohen), a child psychologist, and Richard V. Gilbert, an economist. He was educated at the Sidwell Friends School, and attended Harvard University for undergraduate and graduate studies, earning a baccalaureate in chemistry and physics in 1953 and a master's degree in physics in 1954. He studied for his doctorate at the University of Cambridge, where he earned a Ph.D in Physics under the mentorship of Nobel laureate Abdus Salam in 1957.
Gilbert returned to Harvard in 1956 and was appointed assistant professor of physics in 1959. Gilbert's wife Celia had begun working for James Watson, and this led to Gilbert becoming interested in problems in molecular biology. Watson and Gilbert would run their laboratory jointly through most of the 1960s, until Watson left for Cold Spring Harbor Laboratory. In 1964 he was promoted to associate professor of biophysics and promoted again in 1968 to professor of biochemistry. In 1969, he was awarded Harvard's Ledlie Prize. In 1972 he was named American Cancer Society Professor of Molecular Biology.
He is a co-founder of the biotech start-up companies Biogen and Myriad Genetics, and was the first chairman on their respective boards of directors. Gilbert left his position at Harvard to run Biogen as CEO, but was later asked to resign by the company's board of directors. He is also a member of the Board of Scientific Governors at The Scripps Research Institute. Gilbert has served as the chairman of the Harvard Society of Fellows.
Gilbert was an early proponent of sequencing the human genome. At a March 1986 meeting in Santa Fe New Mexico he proclaimed "The total human sequence is the grail of human genetics". In 1987, he proposed starting a company called Genome Corporation to sequence the genome and sell access to the information. In an opinion piece in Nature in 1991, he envisioned completion of the human genome sequence transforming biology into a field in which computer databases would be as essential as laboratory reagents
Gilbert returned to Harvard in 1985. Gilbert was an outspoken critic of David Baltimore in the handling of the scientific fraud accusations against Thereza Imanishi-Kari. Gilbert also joined the early controversy over the cause of AIDS, though he later stated he was satisfied with the evidence that disease is caused by HIV. After retiring from Harvard in 2001, Gilbert has launched an artistic career centered around digital photography.
Research findings.
In 1962, Gilbert's Ph.D. student in physics Gerald Guralnik extended Gilbert's work on massless particles; Guralnik's work on is widely recognized as an important thread in the discovery of the Higgs Boson.
With his Ph.D. student Benno Müller-Hill, Gilbert was the first to purify the lac repressor, just beating out Mark Ptashne for purifying the first gene regulatory protein.
Together with Allan Maxam, Gilbert developed a new DNA sequencing method using chemical methods developed by Andrei Mirzabekov. His approach to the first synthesis of insulin via recombinant DNA lost out to Genentech's approach which used genes built up from the nucleotides rather than from natural sources. Gilbert's effort was hampered by a temporary moratorium on recombinant DNA work in Cambridge, Massachusetts, forcing his group to move their work to an English biological weapons site.
Gilbert first proposed the existence of introns and exons and explained the evolution of introns in a seminal 1978 "News and Views" paper published in "Nature". In 1986, Gilbert proposed the RNA world hypothesis for the origin of life, based on a concept first proposed by Carl Woese in 1967.
Awards.
In 1979, Gilbert was awarded the Louisa Gross Horwitz Prize from Columbia University together with Frederick Sanger. That year he was also awarded the Gairdner Prize and the Albert Lasker Award for Basic Medical Research.
Gilbert was awarded the 1980 Nobel Prize in Chemistry, shared with Frederick Sanger and Paul Berg. Gilbert and Sanger were recognized for their pioneering work in devising methods for determining the sequence of nucleotides in a nucleic acid.
Gilbert has also been honored by the National Academy of Sciences (US Steel Foundation Award, 1968); Massachusetts General Hospital (Warren Triennial Prize, 1977); the New York Academy of Sciences; (Louis and Bert Freedman Foundation Award, 1977), the Academie des Sciences of France (Prix Charles-Leopold Mayer Award, 1977). Gilbert was made a Foreign Member of the Royal Society of London in 1987.
In 2002, he received the Biotechnology Heritage Award.

</doc>
<doc id="34052" url="http://en.wikipedia.org/wiki?curid=34052" title="Warner Bros.">
Warner Bros.

Warner Bros. Entertainment Inc. (commonly called Warner Bros., Warners, or simply WB) is an American media company that produces film, television and music entertainment. As one of the major film studios, it is a subsidiary of Time Warner, with its headquarters in Burbank, California. Warner Bros. has several subsidiary companies, including Warner Bros. Pictures, Warner Bros. Interactive Entertainment, Warner Bros. Television, Warner Bros. Animation, Warner Home Video, New Line Cinema, Castle Rock Entertainment, DC Entertainment, and the former The WB Television Network. Warner Bros. owns half of The CW Television Network.
History.
1903–1925: Founding.
The company's name originates from the four founding Warner brothers (born "Wonskolaser" or "Wonsal" before Anglicization to hide Jewish origins): Harry (born "Hirsz"), Albert (born "Aaron"), Sam (born "Szmul"), and Jack ("Itzhak", or to some sources, "Jacob"). They emigrated with their parents to North America from Krasnosielc which was located in the part of Congress Poland that had been subjugated to the Russian Empire following the eighteenth-century Partitions of Poland near present-day Ostrołęka. Jack, the youngest, was born in London, Ontario. The three elder brothers began in the movie theater business, having acquired a movie projector with which they showed films in the mining towns of Pennsylvania and Ohio. In the beginning, Sam and Albert Warner invested $150 to present "Life of an American Fireman" and "The Great Train Robbery". They opened their first theater, the Cascade, in New Castle, Pennsylvania in 1903. (The site later became the Cascade Center, a shopping, dining and entertainment complex honoring its Warner Bros. heritage, though as of late 2010 the empty complex was for sale.) When the original building was in danger of being demolished, the modern Warner Bros. called the current building owners, and arranged to save it. The owners noted people across the country had asked them to protect it for its historical significance.
In 1904, the Warners founded the Pittsburgh-based Duquesne Amusement & Supply Company, to distribute films. In 1912, Harry Warner hired an auditor named Paul Ashley Chase. By the time of World War I they had begun producing films. In 1918 they opened Warner Bros. studio on Sunset Boulevard in Hollywood. Sam and Jack produced the pictures, while Harry and Albert, along with their auditor and now controller Chase, handled finance and distribution in New York City. During World War I their first nationally syndicated film, "My Four Years in Germany," based on a popular book by former ambassador James W. Gerard, was released. On April 4, 1923, with help from a money loaned to Harry by his banker Motley Flint, they formally incorporated as Warner Brothers Pictures, Incorporated. (As late as the 1960s, Warner Bros. claimed 1905 as its founding date.)
The first important deal was the acquisition of the rights to Avery Hopwood's 1919 Broadway play, "The Gold Diggers", from theatrical impresario David Belasco. However, Rin Tin Tin, a dog brought from France after WWI by an American soldier, established their reputation. Rin Tin Tin debuted in the feature "Where the North Begins". The movie was so successful that Jack signed the dog to star in more films for $1,000 per week. Rin Tin Tin became the studio's top star. Jack nicknamed him "The Mortgage Lifter" and the success boosted Darryl F. Zanuck's career. Zanuck eventually became a top producer and between 1928 and 1933 served as Jack's right-hand man and executive producer, with responsibilities including day-to-day film production. More success came after Ernst Lubitsch was hired as head director; Harry Rapf left the studio to join Metro-Goldwyn-Mayer. Lubitsch's film "The Marriage Circle" was the studio's most successful film of 1924, and was on "The New York Times" best list for that year.
Despite the success of Rin Tin Tin and Lubitsch, Warner's remained a lesser studio. Sam and Jack decided to offer Broadway actor John Barrymore the lead role in "Beau Brummel". The film was so successful that Harry signed Barrymore to a long-term contract; like "The Marriage Circle", "Beau Brummell" was named one of the ten best films of the year by the "Times". By the end of 1924, Warner Bros. was arguably Hollywood's most successful independent studio, where it competed with "The Big Three" Studios (First National, Paramount Pictures, and MGM). As a result, Harry Warner – while speaking at a convention of 1,500 independent exhibitors in Milwaukee, Wisconsin – was able to convince the filmmakers to spend $500,000 in newspaper advertising, and Harry saw this as an opportunity to establish theaters in cities such as New York and Los Angeles.
As the studio prospered, it gained backing from Wall Street, and in 1924 Goldman Sachs arranged a major loan. With this new money, the Warners bought the pioneer Vitagraph Company which had a nationwide distribution system. In 1925, Warners also experimented in radio, establishing a successful radio station, KFWB, in Los Angeles.
1925–1935: Sound, color, style.
Warner Bros. was a pioneer of films with synchronized sound (then known as "talking pictures" or "talkies"). In 1925, at Sam's urging, Warner's agreed to add this feature to their productions. By February 1926, the studio reported a net loss of $333,413.
After a long period denying Sam's request for sound, Harry agreed to change, as long as the studio's use of synchronized sound was for background music purposes only. The Warners signed a contract with the sound engineer company Western Electric and established Vitaphone. In 1926, Vitaphone began making films with music and effects tracks, most notably, in the feature "Don Juan" starring John Barrymore. The film was silent, but it featured a large number of Vitaphone shorts at the beginning. To hype "Don Juan"'s release, Harry acquired the large Piccadilly Theater in Manhattan, New York City, and renamed it Warners´ Theatre.
"Don Juan" premiered at the Warners´ Theatre in New York on August 6, 1926. Throughout the early history of film distribution, theater owners hired orchestras to attend film showings, where they provided soundtracks. Through Vitaphone, Warner Bros. produced eight shorts (which aired at the beginning of every showing of "Don Juan" across the country) in 1926. Many film production companies questioned the necessity. "Don Juan" did not recoup its production cost and Lubitsch left for MGM. By April 1927, the Big Five studios (First National, Paramount, MGM, Universal, and Producers Distributing) had ruined Warner's, and Western Electric renewed Warner's Vitaphone contract with terms that allowed other film companies to test sound.
As a result of their financial problems, Warner Bros. took the next step and released "The Jazz Singer" starring Al Jolson. This movie, which has very little sound dialogue but includes sound segments of Jolson singing, was a sensation. It signaled the beginning of the era of "talking pictures" and the twilight of the silent era. However, Sam died the night before the opening, preventing the brothers from attending the premiere. Jack became sole head of production. Sam's death also had a great effect on Jack's emotional state, as Sam was arguably Jack's inspiration and favorite brother. In the years to come, Jack kept the studio under tight control. Firing employees was common. Among those whom Jack fired were Rin Tin Tin (in 1929) and Douglas Fairbanks, Jr. — who had served as First National's top star since the brothers acquired the studio in 1928 — in 1933.
Thanks to the success of "The Jazz Singer", the studio was cash-rich. Jolson's next film for the company, "The Singing Fool" was also a success. With the success of these first talkies ("The Jazz Singer", "Lights of New York", "The Singing Fool" and "The Terror"), Warner Bros. became a top studio and the brothers were now able to move out from the Poverty Row section of Hollywood and acquire a big facility in Burbank, California. They expanded by acquiring the Stanley Corporation, a major theater chain. This gave them a share in rival First National Pictures, of which Stanley owned one-third. In a bidding war with William Fox, Warner Bros. bought more First National shares on September 13, 1928; Jack also appointed Zanuck as the manager of First National Pictures.
In 1928, Warner Bros. released "Lights of New York", the first all-talking feature. Due to its success, the movie industry converted entirely to sound almost overnight. By the end of 1929, all the major studios were exclusively making sound films. In 1929, National Pictures released their first film with Warner Bros., "Noah's Ark". Despite its expensive budget, "Noah's Ark" was profitable. In 1929, Warner Bros. released "On with the Show", the first all-color all-talking feature. This was followed by "Gold Diggers of Broadway" which was so popular it played in theatres until 1939. The success of these two color pictures caused a color revolution (just as the first all-talkie had created one for talkies). Warner Bros. color films from 1929 to 1931 included "The Show of Shows" (1929), "Sally" (1929), "Bright Lights" (1930), "Golden Dawn" (1930), "Hold Everything" (1930), "Song of the Flame" (1930), "Song of the West" (1930), "The Life of the Party" (1930), "Sweet Kitty Bellairs" (1930), "Under A Texas Moon" (1930), "Bride of the Regiment" (1930), "Viennese Nights" (1931), "Woman Hungry" (1931), "Kiss Me Again" (1931), "Fifty Million Frenchmen" (1931) and "Manhattan Parade" (1932). In addition to these, scores of features were released with Technicolor sequences, as well as numerous Technicolor Specials short subjects. The majority of these color films were musicals.
In 1929, Warner Bros bought the St. Louis-based theater chain Skouras Brothers. Following this take-over, Spyros Skouras, the driving force of the chain, became general manager of the Warner Brothers Theater Circuit in America. He worked successfully in that post for two years and turned its losses into profits. Harry produced an adaptation of a Cole Porter musical titled "Fifty Million Frenchmen". Through First National, the studio's profit increased substantially. After the success of the studio's 1929 First National film "Noah's Ark", Harry agreed to make Michael Curtiz a major director at the Burbank studio. Mort Blumenstock, a First National screenwriter, became a top writer at the brothers' New York headquarters. In the third quarter, Warner Bros. gained complete control of First National, when Harry purchased the company's remaining one-third share from Fox. The Justice Department agreed to allow the purchase if First National was maintained as a separate company. When the Great Depression hit, Warner asked for and got permission to merge the two studios. Soon afterward Warner Bros. moved to the First National lot in Burbank. Though the companies merged, the Justice Department required Warner to release a few films each year under the First National name until 1938. For thirty years, certain Warner productions were identified (mainly for tax purposes) as 'A Warner Bros. – First National Picture.'
In the latter part of 1929, Jack Warner hired George Arliss to star in "Disraeli", which was a success. Arliss won an Academy Award for Best Actor and went on to star in nine more movies for the studio. In 1930, Harry acquired more theaters in Atlantic City, despite the beginning of the Great Depression. In July 1930, the studio's banker, Motley Flint, was murdered by a disgruntled investor in another company.
Harry acquired a string of music publishers to form Warner Bros. Music. In April 1930, Warner Bros. acquired Brunswick Records. Harry obtained radio companies, foreign sound patents and a lithograph company. After establishing Warner Bros. Music, Harry appointed his son, Lewis, to manage the company.
By 1931, the studio began to feel the effects of the Depression as the public could no longer afford the tickets. The studio reportedly lost $8 million, and an additional $14 million the following year. In 1931, Warner Bros. Music head Lewis Warner died from an infected wisdom tooth.
Around that time, Zanuck hired screenwriter Wilson Mizner. While at the studio, Mizner had hardly any respect for authority and found it difficult to work with Jack, but became an asset. As time went by, Warner became more tolerant of Mizner and helped invest in Mizner's Brown Derby restaurant. On April 3, 1933, Mizner died from a heart attack.
By 1932, audiences had grown tired of musicals, and the studio was forced to cut musical numbers from many productions and advertise them as straight comedies. The public had begun to associate musicals with color, and thus studios began to abandon its use. Warner Bros. had a contract with Technicolor to produce two more pictures in that process. As a result, the first horror films in color were produced and released by the studio: "Doctor X" (1932) and "Mystery of the Wax Museum" (1933). In the latter part of 1931, Harry Warner rented the Teddington Studios in London, England. The studio focused on making "quota quickies" for the domestic British market and Irving Asher was appointed as the studio's head producer. In 1934, Harry officially purchased the Teddington Studios.
In February 1933, Warner Bros. produced "42nd Street", a very successful musical under the direction of Loyd Bacon. Warner assigned Bacon to "more expensive productions including "Footlight Parade," "Wonder Bar," Broadway Gondolier" (which he also starred in), and "Gold Diggers" that saved the company from bankruptcy. In the wake of "42nd Street'"s success, the studio produced profitable musicals. These starred Ruby Keeler and Dick Powell and were mostly directed by Busby Berkeley. In 1935, the revival suffered a major blow when Berkeley was arrested after killing three people while driving drunk. By the end of the year, people again tired of Warner Bros. musicals, and the studio – after the huge profits made by 1935 film "Captain Blood" – shifted its focus to Errol Flynn swashbucklers.
1930–1935: Pre-code realistic period.
With the collapse of the market for musicals, Warner Bros., under Zanuck turned to more socially realistic storylines. For its many films about gangsters; Warner Bros. soon became known as a "gangster studio". The studio's first gangster film, "Little Caesar", was a great box office success and Edward G. Robinson starred in many of the subsequent Warner gangster films. The studio's next effort, "The Public Enemy", made James Cagney arguably the studio's new top star, and Warner Bros. made more gangster films.
Another gangster film the studio produced was the critically acclaimed "I Am a Fugitive from a Chain Gang", based on a true story and starring Paul Muni, joining Cagney and Robinson as one the studio's top gangster stars after appearing in the successful film, which convinced audiences to question the American legal system. By January 1933, the film's protagonist Robert Elliot Burns – still imprisoned in New Jersey – and other chain gang prisoners nationwide appealed and were released. In January 1933, Georgia chain gang warden J. Harold Hardy – who was also made into a character in the film – sued the studio for displaying "vicious, untrue and false attacks" against him in the film. After appearing in the Warner's film "The Man Who Played God", Bette Davis became a top star.
In 1933, relief for the studio came after Franklin D. Roosevelt became president and began the New Deal. This economic rebound allowed Warner Bros. to again became profitable. The same year, Zanuck quit. Harry Warner's relationship with Zanuck had become strained after Harry strongly opposed allowing Zanuck's film "Baby Face" to step outside Hays Code boundaries. The studio reduced his salary as a result of losses from the Great Depression, and Harry refused to restore it as the company recovered. Zanuck established his own company. Harry thereafter raised salaries for studio employees.
In 1933, Warner was able to link up with newspaper tycoon William Randolph Hearst's Cosmopolitan Films. Hearst had previously worked with MGM, but ended the association after a dispute with head producer Irving Thalberg over the treatment of Hearst's longstanding mistress, actress Marion Davies, who was struggling for box office success. Through his partnership with Hearst, Warner signed Davies to a studio contract. Hearst's company and Davies' films, however, did not increase the studio's profits.
In 1934, the studio lost over $2.5 million, of which $500,000 was the result of a 1934 fire at the Burbank studio, destroying 20 years' worth of early Vitagraph, Warner Bros. and First National films. The following year, Hearst's film adaption of William Shakespeare's "A Midsummer Night's Dream" (1935) failed at the box office and the studio's net loss increased. During this time, Harry and six other movie studio figures were indicted for conspiracy to violate the Sherman Antitrust Act, through an attempt to gain a monopoly over St Louis movie theaters. In 1935, Harry was put on trial; after a mistrial, Harry sold the company's movie theaters and the case was never reopened. 1935 also saw the studio make a net profit of $674,158.00.
By 1936, contracts of musical and silent stars were not renewed replaced by tough-talking, working-class types who better fit these pictures. Dorothy Mackaill, Dolores del Río, Bebe Daniels, Frank Fay, Winnie Lightner, Bernice Claire, Alexander Gray, Alice White, and Jack Mulhall that had characterized the urban, modern, and sophisticated attitude of the 1920s gave way to James Cagney, Joan Blondell, Edward G. Robinson, Warren William and Barbara Stanwyck, who would be more acceptable to the common man. The studio was one of the most prolific producers of Pre-Code pictures and had a lot of trouble with the censors once they started clamping down on what they considered indecency (around 1934). As a result, Warner Bros. turned to historical pictures from around 1935 to avoid confrontations with the Breen office. In 1936, following the success of "The Petrified Forest", Jack signed Humphrey Bogart to a studio contract. Warner, however, did not think Bogart was star material, and cast Bogart in infrequent roles as a villain opposite either James Cagney or Edward Robinson over the next five years.
After Hal B. Wallis succeeded Zanuck in 1933, and the Hays Code began to be enforced in 1935, the studio was forced to abandon this realistic approach in order to produce more moralistic, idealized pictures. The studio's historical dramas, melodramas (or "women's pictures"), swashbucklers, and adaptations of best-sellers, with stars like Bette Davis, Olivia de Havilland, Paul Muni, and Errol Flynn avoided the censors. In 1936, Bette Davis, by now arguably the studio's top star, was unhappy with her roles. She traveled to England and tried to break her contract. Davis lost the lawsuit and returned to America. Although many of the studio's employees had problems with Jack Warner, they considered Albert and Harry fair.
Code era.
In the 1930s many actors and actresses disappeared who had characterized the realistic pre-Code era but who were not suited to the new trend into moral and idealized pictures. Warner Bros. remained a top studio in Hollywood, but this changed after 1935 as other studios, notably MGM, quickly overshadowed the prestige and glamor that previously characterized Warner Bros. However, in the late 1930s, Bette Davis became the studio's top draw and was even dubbed as "The Fifth Warner Brother."
In 1935, Cagney sued Jack Warner for breach of contract. Cagney claimed Warner had forced him to star in more films than his contract required. Cagney eventually dropped his lawsuit after a cash settlement. Nevertheless, Cagney left the studio to establish an independent film company with his brother Bill. The Cagneys released their films though Grand National Films, however they were not able to get good financing and ran out of money after their third film. Cagney then agreed to return to Warner Bros., after Jack agreed to a contract guaranteeing Cagney would be treated to his own terms. After the success of "Yankee Doodle Dandy" at the box office, Cagney again questioned if the studio would meet his salary demand and again quit to form his own film production and distribution company with Bill.
Another employee with whom Warner had troubles was studio producer Bryan Foy. In 1936, Wallis hired Foy as a producer for the studio's low budget B-films leading to his nickname "the keeper of the B's". Foy was able to garnish arguably more profits than any other B-film producer at the time. During Foy's time at the studio, however, Warner fired him seven different times.
During 1936, "The Story of Louis Pasteur" proved a box office success and star Paul Muni won the Oscar for Best Actor in March 1937. The studio's 1937 film "The Life of Emile Zola" gave the studio its first Best Picture Oscar.
In 1937, the studio hired Midwestern radio announcer Ronald Reagan. Although Reagan was initially a B-film actor, Warner Bros. was impressed by his performance in the final scene of "Knute Rockne, All American", and agreed to pair him with Flynn in "Santa Fe Trail" (1940). Reagan then returned to B-films. After his performance in the studio's 1942 "Kings Row", Warner decided to make Reagan a top star and signed him to a new contract, tripling his salary.
In 1936, Harry's daughter Doris read a copy of Margaret Mitchell's "Gone with the Wind" and was interested in making a film adaptation. Doris offered Mitchell $50,000 for screen rights. Jack, vetoed the deal, realizing it would be an expensive production.
George Raft also proved to be a problem for Jack. Warner had signed him in 1939, hoping he could substitute in gangster pictures when either Robinson or Cagney were on suspension. Raft had difficulty working with Bogart and refused to co-star with him. Eventually, Warner agreed to release Raft from his contract. Following Raft's departure, the studio gave Bogart the role of Roy Earl in the 1941 film "High Sierra", which helped establish him as a top star. Following "High Sierra", Bogart was given a role in John Huston's successful 1941 remake of the studio's 1931 failure, "The Maltese Falcon".
Cartoons.
Warner's cartoon unit had its roots in the independent Harman and Ising studio. From 1930 to 1933, Disney alumni Hugh Harman and Rudolf Ising produced musical cartoons for Leon Schlesinger, who sold them to Warner. Harman and Ising introduced their character Bosko in the first "Looney Tunes" cartoon, "Sinkin' in the Bathtub", and created a sister series, "Merrie Melodies", in 1931.
Harman and Ising broke away from Schlesinger in 1933 due to a contractual dispute, taking Bosko with them to MGM. As a result, Schlesinger started his own studio, Leon Schlesinger Productions, which continued with "Merrie Melodies" while starting production on "Looney Tunes" starring Buddy, a Bosko clone. By the end of the decade, a new Schlesinger production team, including directors Friz Freleng, Tex Avery, Robert Clampett and Chuck Jones was formed. Schlesinger's staff developed a fast-paced, irreverent style that made their cartoons globally popular.
In 1936, Avery directed cartoons starring Porky Pig, which established the character as the studio's first animated star. In addition to Porky, Warner Bros. cartoon characters Daffy Duck (who debuted in the 1937 short "Porky's Duck Hunt") and Bugs Bunny (who debuted in the 1940 short "A Wild Hare") achieved star power. By 1942, the Schlesinger studio had surpassed Walt Disney Studios as the most successful producer of animated shorts.
Warner Bros eventually bought Schlesinger's cartoon unit in 1944 and renamed it Warner Bros. Cartoons. Unfortunately, the unit was indifferently treated by senior management, beginning with the installation of Edward Selzer as senior producer, whom the creative staff considered an interfering incompetent. Warner had little regard for his company's short film product and reputedly was so ignorant about his animation division that he was mistakenly convinced that the unit produced cartoons of Mickey Mouse, rival company Walt Disney Productions' flagship character. He sold off the unit's pre-August 1948 library for $3,000 each, which proved a shortsighted transaction in light of its eventual value.
Warner Brothers Cartoons continued, with intermittent interruptions, until 1969 when it was dissolved as the parent company ceased film shorts entirely. Characters such as Bugs Bunny, Daffy Duck, Tweety Bird, Sylvester, and Porky Pig became central to the company's image in subsequent decades. Bugs in particular remains a mascot to Warner Bros., its various divisions and Six Flags (which Time Warner once owned). The success of the compilation film, "The Bugs Bunny/Road Runner Movie" in 1980, featuring the archived film of these characters prompted Warner Brothers to organize Warner Bros. Animation as a new production division to restart production of original material.
World War II.
According to Warner's autobiography, prior to US entry in World War II, Philip Kauffman, Warner Bros. German sales head, was murdered by the Nazis in Berlin in 1936. Harry produced the successful anti-German film "The Life of Emile Zola" (1937). After that, Harry supervised the production of more anti-German films, including "Confessions of a Nazi Spy" (1939), "The Sea Hawk" (1940), which made King Phillip II an equivalent of Hitler, "Sergeant York", and "You're In The Army Now" (1941). Harry then decided to focus on producing war films. Warners cut its film production in half during the war, eliminating its B Pictures unit in 1941. Bryan Foy joined Twentieth Century Fox.
During the war era, the studio made "Casablanca", "Now, Voyager", "Yankee Doodle Dandy" (all 1942), "This Is the Army", and "Mission to Moscow" (both 1943); the latter became controversial a few years afterwards. At the premieres of "Yankee Doodle Dandy" (in Los Angeles, New York, and London), audiences purchased $15.6 million in war bonds for the governments of England and the United States. By the middle of 1943, however, audiences had tired of war films, but Warner continued to produce them, losing money. In honor of the studio's contributions to the cause, the Navy named a Liberty ship after the brothers' father, Benjamin Warner. Harry christened the ship. By the time the war ended, $20 million in war bonds were purchased through the studio, the Red Cross collected 5,200 pints of plasma from studio employees and 763 of the studio's employees served in the armed forces, including Harry Warner's son-in-law Milton Sperling and Jack's son Jack Warner, Jr. Following a dispute over ownership of "Casablanca'"s Oscar for Best Picture, Wallis resigned. After "Casablanca" made Bogart a top star, Bogart's relationship with Jack deteriorated.
 In 1943, Olivia de Haviland (whom Warner was loaning to different studios) sued Warner for breach of contract. De Haviland had refused to portray famed abolitionist Elizabeth Blackwell in an upcoming film for Columbia Pictures. Warner responded by sending 150 telegrams to different film production companies, warning them not to hire her for any role. Afterwards, de Haviland discovered employment contracts in the United States could only last seven years; de Haviland had been under contract with the studio since 1935. The court ruled in de Haviland's favor and she left the studio. Through de Haviland's victory, many of the studio's longtime actors were now freed from their contracts, and Harry decided to terminate the studio's suspension policy.
The same year, Jack signed newly released MGM actress Joan Crawford, a former top star who found her career fading. Crawford's first role with the studio was 1944's "Hollywood Canteen". Her first starring role at the studio, in the title role as "Mildred Pierce" (1945), revived her career and earned her an Oscar for Best Actress.
After World War II – changing hands.
In the post-war years, Warner Bros. continued to create new stars, including Lauren Bacall and Doris Day. The studio prospered greatly after the war. By 1946, company payroll reached $600,000 a week and net profit topped $19.4 million.
Jack Warner continued to refuse to meet Screen Actors Guild salary demands. In September 1946, employees engaged in a month-long strike. In retaliation, Warner — during his 1947 testimony before Congress about "Mission to Moscow" – accused multiple employees of ties to Communists. By the end of 1947, the studio reached a record net profit of $22 million.
On January 5, 1948, Warner offered the first color newsreel, covering the Tournament of Roses Parade and the Rose Bowl Game. In 1948, Bette Davis, still their top actress and now hostile to Jack, was a big problem for Harry after she and others left the studio after completing the film "Beyond the Forest".
Warner was a party to the "United States v. Paramount Pictures, Inc." anti-trust case of the 1940s. This action, brought by the Justice Department and the Federal Trade Commission, claimed the five integrated studio-theater chain combinations restrained competition. The Supreme Court heard the case in 1948, and ruled for the government. As a result, Warner and four other major studios were forced to separate production from exhibition. In 1949, the studio's net profit was only $10 million.
Warner Bros. had two semi-independent production companies that released films through the studio. One of these was Sperling's United States Pictures.
In the early 1950s, the threat of television emerged. In 1953, Jack decided to copy. United Artists successful 3D film "Bwana Devil", releasing his own 3D films beginning with "House of Wax". However, 3D films soon lost their appeal among moviegoers.
 3D almost caused the demise of the Warner Bros. cartoon studio. Having completed a 3D Bugs Bunny cartoon, "Lumber Jack-Rabbit", Jack Warner ordered the animation unit to be shut down, erroneously believing that all cartoons hence would be produced in the 3D process. Several months later, Warner relented and reopened the cartoon studio. Fortunately, Warner Bros. had enough of a backlog of cartoons and a healthy reissue program so that there was no noticeable interruption in the release schedule.
In 1952, Warner Bros. made their first film ("Carson City") in "Warnercolor", the studio's name for Eastmancolor.
After the downfall of 3D films, Harry Warner decided to use CinemaScope in future Warner Bros. films. One of the studio's first CinemaScope films, "The High and the Mighty" (owned by John Wayne's company Batjac), enabled the studio to show a profit.
Early in 1953, Warner's theater holdings were spun off as Stanley Warner Theaters; Stanley Warner's non-theater holdings were sold to Simon Fabian Enterprises, and its theaters merged with RKO Theatres to become RKO-Stanley Warner Theatres.
By 1956 the studio was losing money, declining from 1953's net profit of $2.9 million and the next two years of between $2 and $4 million. In February 13, 1956, Jack Warner sold the rights to all of his pre-1950 films to Associated Artists Productions (which merged with United Artists Television in 1958, and was subsequently acquired by Turner Broadcasting System in early 1986 as part of a failed takeover of MGM/UA by Ted Turner).
In May 1956, the brothers announced they were putting Warner Bros. on the market. Jack secretly organized a syndicate – headed by Boston banker Serge Semenenko– to purchase 90% of the stock. After the three brothers sold, Jack – through his under-the-table deal – joined Semenenko's syndicate and bought back all his stock. Shortly after the deal was completed in July, Jack – now the company's largest stockholder – appointed himself its new president. Shortly after the deal closed, Jack announced the company and its subsidiaries would be "directed more vigorously to the acquisition of the most important story properties, talents, and to the production of the finest motion pictures possible."
Warner Bros. Television and Warner Bros. Records.
By 1949, with the success of television threatening the film industry more and more, Harry Warner decided to emphasize television production. However, the Federal Communications Commission (FCC) would not permit it. After an unsuccessful attempt to convince other movie studio bosses to switch, Harry abandoned his television efforts.
Jack had problems with Milton Berle's unsuccessful film "Always Leave Them Laughing" during the peak of Berle's television popularity. Warner felt that Berle was not strong enough to carry a film and that people would not pay to see the man they could see on television for free. However, Jack was pressured into using Berle, replacing Danny Kaye with him. Berle's outrageous behaviour on the set and the film's massive failure led to Jack banning television sets from film sets.
On March 21, 1955, the studio was finally able engage in television through the successful Warner Bros. Television unit run by William T. Orr, Jack Warner's son-in-law. Warner Bros. Television provided ABC with a weekly show, "Warner Bros. Presents." The show featured rotating shows based on three film successes, "Kings Row", "Casablanca" and "Cheyenne", followed by a promotion for a new film. It was not a success. The studio's next effort was to make a weekly series out of "Cheyenne". "Cheyenne" was television's first hour-long Western. Two episodes were placed together for feature film release outside the United States. In the tradition of their B pictures, the studio followed up with a series of rapidly produced popular Westerns, such as writer/producer Roy Huggins' critically lauded "Maverick" as well as "Sugarfoot", "Bronco", "Lawman", "The Alaskans" and "Colt .45". The success of these series helped to make up for losses in the film business. As a result, Jack decided to emphasize television production. Warner's produced a series of popular private detective shows beginning with "77 Sunset Strip" (1958–1964) followed by "Hawaiian Eye" (1959–1963), "Bourbon Street Beat" (1960) and "Surfside Six" (1960–1962).
Within a few years, the studio provoked hostility among their TV stars such as Clint Walker and James Garner, who sued over a contract dispute and won. Edd Byrnes was not so lucky and bought himself out of his contract. Jack was angered by their perceived ingratitude, who evidently showed more independence than film actors, deepening his contempt for the new medium. Many of Warner's television stars appeared in the casts of Warner's cinema releases. In 1963 a court decision forced Warner's to end contracts with their television stars, engaging them for specific series or film roles. In the same year Jack Webb took over the television unit without success.
Warner Bros. was already the owner of extensive music-publishing holdings, whose tunes had appeared in countless cartoons (arranged by Carl Stalling) and television shows (arranged by Max Steiner).
In 1958, the studio launched Warner Bros. Records. Initially the label released recordings made by their television stars – whether they could sing or not – and records based on television soundtracks.
In 1963, Warner agreed to a "rescue takeover" of Frank Sinatra's Reprise Records. The deal gave Sinatra US$1.5 million and part ownership of Warner Bros. Records, making Reprise a sub-label. Most significantly the deal brought Reprise manager Morris "Mo" Ostin into the company. In 1964, upon seeing the profits record companies made from Warner film music, Warner decided to claim ownership of the studio's film soundtracks. In its first eighteen months, Warner Bros. Records lost around $2 million.
New owners.
Warner Bros. rebounded in the late 1950s, specializing in adaptations of popular plays like "The Bad Seed" (1956), "No Time for Sergeants" (1958), and "Gypsy" (1962).
While he slowly recovered from a car crash that occurred while vacationing in France in 1958, Jack returned to the studio and made sure his name was featured in studio press releases. From 1961-63, the studio's annual net profit was a little over $7 million. Warner paid an unprecedented $5.5 million for the film rights to the Broadway musical "My Fair Lady" in February 1962. The previous owner, CBS director William S. Paley, set terms including half the distributor's gross profits "plus ownership of the negative at the end of the contract." In 1963, the studio's net profit dropped to $3.7 million. By the mid-1960s, motion picture production was in decline, as the industry was in the midst of a painful transition from the Golden Age of Hollywood to the era now known as New Hollywood. Few studio films were made in favor of co-productions (for which Warner provided facilities, money and distribution), and pickups of independent pictures.
With the success of the studio's 1965 film of Broadway play "The Great Race", as well as its soundtrack, Warner Bros. Records became a profitable subsidiary. The 1966 film "Who's Afraid Of Virginia Woolf?" was a huge success.
In November 1966, Jack gave in to advancing age and changing times, selling control of the studio and music business to Seven Arts Productions, run by Canadian investors Elliot and Kenneth Hyman, for $32 million. The company, including the studio, was renamed Warner Bros.-Seven Arts. Warner remained president until the summer of 1967, when "Camelot" failed at the box office and Warner gave up his position to his longtime publicity director, Ben Kalmenson; Warner remained on board as an independent producer and vice-president. With the 1967 success of "Bonnie and Clyde", Warner Bros. was again profitable.
Two years later the Hymans had tired of Jack Warner. They accepted a cash-and-stock offer from an Kinney National Company for more than $64 million. Kinney owned a Hollywood talent agency, Ashley-Famous, whose founder Ted Ashley led Kinney head Steve Ross to purchase Warner Bros. Ashley became the studio head and changed the name to Warner Bros., Inc. once again. Warner was outraged by the Hymans' sale, and decided to retire.
Although movie audiences had shrunk, Warner's new management believed in the drawing power of stars, signing co-production deals with several of the biggest names of the day, including Paul Newman, Robert Redford, Barbra Streisand, and Clint Eastwood, carrying the studio successfully through the 1970s and 1980s. Warner Bros. also made major profits on films built around the characters of Superman and Batman, owned by Warner Bros. subsidiary DC Comics.
Abandoning parking lots and funeral homes, the refocused Kinney renamed itself in honor of its best-known holding, Warner Communications. Throughout the 1970s and 1980s Warner Communications branched out into other business, such as video game company Atari, Inc. in 1976, and later the Six Flags theme parks.
From 1971 until the end of 1987, Warner's international distribution operations were a joint venture with Columbia Pictures. In some countries, this joint venture distributed films from other companies (such as EMI Films and Cannon Films in the UK). Warner ended the venture in 1988 and partnered with Walt Disney Pictures. This joint venture lasted until 1993, when Disney created Buena Vista International.
In 1972, in a cost-cutting move, Warner and Columbia formed a third company called The Burbank Studios (TBS). They would share the Warner lot in Burbank. Both studios technically became production entities, giving TBS day-to-day responsibility for studio grounds and upkeep. The Columbia Ranch (about a mile north of Warner's lot) was part of the deal. The Warner-Columbia relationship was acrimonious, but the reluctance of both studios to approve or spend money on capital upgrades that might only help the other did have the unintended consequence of preserving the Warner lot's primary function as a filmmaking facility while it produced relatively little during the 1970s and 1980s. (Most films produced after 1968 were filmed on location after the failure of "Camelot" was partially attributed to the fact it was set in England but obviously filmed in Burbank.) With control over its own lot tied up in TBS, Warner ultimately retained a significant portion of its backlot, while Fox sold its backlot to create Century City, Universal turned part of its backlot into a theme park and shopping center, and Disney replaced its backlot with office buildings and exiled its animation department to an industrial park in Glendale.
In 1989, a solution to the situation became evident when Warner Bros. acquired Lorimar-Telepictures and gained control of the former MGM studio lot in Culver City, and that same year, Sony bought Columbia Pictures. Sony was flush with cash and Warner Bros. now had two studio lots. In 1990, TBS ended when Sony bought the MGM lot from Warner and moved Columbia to Culver City. However, Warner kept the Columbia Ranch, now known as the Warner Bros. Ranch.
Warner Communications merged in 1989 with white-shoe publishing company Time Inc. Time claimed a higher level of prestige, while Warner Bros. provided the profits. The Time Warner merger was almost derailed when Paramount Communications (Formerly Gulf+Western, later sold to Viacom), launched a $12.2 billion hostile takeover bid for Time Inc., forcing Time to acquire Warner with a $14.9 billion cash/stock offer. Paramount responded with a lawsuit filed in Delaware court to break up the merger. Paramount lost and the merger proceeded.
In 1992 Warner Bros. Family Entertainment was established to produce various family-oriented films.
In 1997, Time Warner sold Six Flags. The takeover of Time Warner in 2000 by then-high-flying AOL did not prove a good match, and following the collapse in "dot-com" stocks, the AOL element was banished from the corporate name.
Since 1995.
In 1995, Warner and station owner Tribune Company of Chicago launched The WB Network, seeking a niche market in teenagers. The WB's early programming included an abundance of teenage fare like "Buffy the Vampire Slayer", "Smallville", "Dawson's Creek", and "One Tree Hill". Two dramas produced by Spelling Television, "7th Heaven" and "Charmed" helped bring The WB into the spotlight ."Charmed" lasted eight seasons, becoming the longest running drama with female leads. "7th Heaven" ran for eleven seasons and was the longest running family drama and longest running show for the network. In 1998, Warner Bros. celebrated its 75th anniversary. In 2006, Warner and CBS Paramount Television decided to close The WB and CBS's UPN and jointly launch The CW Television Network. In 1999, Terry Semels and Robert Daly resigned as studio heads after a career 13 Oscar nominated films. Daly and Semels were said to have popularized the modern model of partner financing and profit sharing for film production.
In the late 1990s, Warner obtained rights to the "Harry Potter" novels, and released feature film adaptations of the first in 2001, the second in 2002, the third in June 2004, the fourth in November 2005, and the fifth on July 11, 2007. The sixth came in July 2009. The seventh and final was released in two parts: Part 1 in November 2010 and Part 2 in July 2011.
From 2006, Warner Bros operated a joint venture with China Film Group Corporation and HG to form Warner China Film HG to produce films in Hong Kong and China, including "Connected", a remake of the 2004 thriller film "Cellular""." They co-produced many other Chinese films.
Warner Bros. played a large part in the discontinuation of the HD DVD format. On January 4, 2008, Warner Bros. announced that they would drop support of HD DVD in favor of Blu-ray Disc. HD DVDs continued to be released through May 2008, but only following Blu-ray and DVD releases.
In 2009, Warner Bros. became the first studio in history to gross more than $2 billion domestically in a single year.
Warner Bros. "Harry Potter" film series was the worldwide highest grossing film series of all time without inflation adjustment. Its "Batman" film series was one of only two series to have two entries earn more than $1 billion worldwide. "Harry Potter and the Deathly Hallows – Part 2" was Warner Bros.' highest grossing movie ever (surpassing "The Dark Knight"). However, the Harry Potter movies have produced a net loss due to Hollywood accounting. IMAX Corp.signed with Warner Bros. Pictures in April 2010 to release as many as 20 giant-format films through 2013.
Warner Bros. formed a short form digital unit, Blue Ribbon Content, under its Warner Bros. Animation & Warner Digital Series president.
On February 06, 2014, Warner Bros., through the legal name Columbia TriStar Warner Filmes de Portugal Ltda., announced that would have its offices at Portugal no more from March 31, 2014. 
Film library.
Mergers and acquisitions have helped Warner Bros. accumulate a diverse collection of movies, cartoons and television programs.
In the aftermath of the 1948 antitrust suit, uncertain times led Warner Bros. in 1956 to sell most of its pre-1950 films and cartoons to a holding company called Associated Artists Productions (a.a.p.). a.a.p. also got the Fleischer Studios and Famous Studios "Popeye" cartoons, originally from Paramount. Two years later, a.a.p. was sold to United Artists (UA), which held them until 1981, when Metro-Goldwyn-Mayer bought UA. 
In 1982 Turner Broadcasting System acquired Brut Productions, the film production subsidiary of the then struggling personal-care company Faberge Inc.
In 1986, Turner Broadcasting System, having failed to buy MGM, settled for ownership of the MGM/UA library. This included almost all the pre-May 1986 MGM film and television library with the exception of those owned by United Artists (i.e. James Bond franchise), although some UA material were included such as the a.a.p. library, the U.S. rights to a majority of the RKO Radio Pictures library, and the television series "Gilligan's Island". 
In 1991, Turner Broadcasting System bought animation studio Hanna-Barbera Productions, and much of the back catalog of both Hanna-Barbera and Ruby-Spears Enterprises from Great American Broadcasting, and years later, Turner bought Castle Rock Entertainment on December 22, 1993 and New Line Cinema on January 28, 1994. In 1996, Time Warner bought Turner Broadcasting System, and brought the pre-1950 sound films and the pre-August 1948 cartoon library back home.
On October 4, 2007, Warner Bros. added the "Peanuts/Charlie Brown" library to its collection from Peanuts Worldwide, LLC, licensor and owner of the "Peanuts" material; this includes all the television specials and series outside of the theatrical library, which continues to be owned by CBS and Paramount.
In 2008, Warner Bros. closed New Line Cinema as an independent mini-major studio, as a result, Warner added the New Line Cinema film and television library to its collection. On October 15, 2009, Warner Bros. acquired the home entertainment rights to the "Sesame Street" library, in conjunction with Sesame Workshop.
Highest-grossing films.
The Warner Bros. Archives.
The University of Southern California Warner Bros. Archives is the largest single studio collection in the world. Donated in 1977 to USC's School of Cinema-Television by Warner Communications, the WBA houses departmental records that detail Warner Bros. activities from the studio's first major feature, "My Four Years in Germany" (1918), to its sale to Seven Arts in 1968. It presents a complete view of the production process during the Golden Age of Hollywood. UA donated pre-1950 Warner Bros. nitrate negatives to the Library of Congress and post-1951 negatives to the UCLA Film and Television Archive. Most of the company's legal files, scripts, and production materials were donated to the Wisconsin Center for Film and Theater Research.
References.
Additional sources.
</dl>

</doc>
<doc id="34053" url="http://en.wikipedia.org/wiki?curid=34053" title="Water turbine">
Water turbine

A water turbine is a rotary engine that converts kinetic and potential energy of water into mechanical work.
Water turbines were developed in the 19th century and were widely used for industrial power prior to electrical grids. Now they are mostly used for electric power generation.
Water turbines are mostly found in dams to generate electric power from water kinetic energy.
History.
Water wheels have been used for hundreds of years for industrial power. Their main shortcoming is size, which limits the flow rate and head that can be harnessed.
The migration from water wheels to modern turbines took about one hundred years. Development occurred during the Industrial revolution, using scientific principles and methods. They also made extensive use of new materials and manufacturing methods developed at the time.
Swirl.
The word turbine was introduced by the French engineer Claude Burdin in the early 19th century and is derived from the Latin word for "whirling" or a "vortex". The main difference between early water turbines and water wheels is a swirl component of the water which passes energy to a spinning rotor. This additional component of motion allowed the turbine to be smaller than a water wheel of the same power. They could process more water by spinning faster and could harness much greater heads. (Later, impulse turbines were developed which didn't use swirl).
Time line.
The earliest known water turbines date to the Roman Empire. Two helix-turbine mill sites of almost identical design were found at Chemtou and Testour, modern-day Tunisia, dating to the late 3rd or early 4th century AD. The horizontal water wheel with angled blades was installed at the bottom of a water-filled, circular shaft. The water from the mill-race entered the pit tangentially, creating a swirling water column which made the fully submerged wheel act like a true turbine.
Fausto Veranzio in his book Machinae Novae (1595) described a vertical axis mill with a rotor similar to that of a Francis turbine.
Johann Segner developed a reactive water turbine (Segner wheel) in the mid-18th century in Kingdom of Hungary. It had a horizontal axis and was a precursor to modern water turbines. It is a very simple machine that is still produced today for use in small hydro sites. Segner worked with Euler on some of the early mathematical theories of turbine design. In the 18th century, a Dr. Barker invented a similar reaction hydraulic turbine that became popular as a lecture-hall demonstration. The only known surviving example of this type of engine used in power production, dating from 1851, is found at Hacienda Buena Vista in Ponce, Puerto Rico.
In 1820, Jean-Victor Poncelet developed an inward-flow turbine.
In 1826, Benoit Fourneyron developed an outward-flow turbine. This was an efficient machine (~80%) that sent water through a runner with blades curved in one dimension. The stationary outlet also had curved guides.
In 1844, Uriah A. Boyden developed an outward flow turbine that improved on the performance of the Fourneyron turbine. Its runner shape was similar to that of a Francis turbine.
In 1849, James B. Francis improved the inward flow reaction turbine to over 90% efficiency. He also conducted sophisticated tests and developed engineering methods for water turbine design. The Francis turbine, named for him, is the first modern water turbine. It is still the most widely used water turbine in the world today. The Francis turbine is also called a radial flow turbine, since water flows from the outer circumference towards the centre of runner.
Inward flow water turbines have a better mechanical arrangement and all modern reaction water turbines are of this design. As the water swirls inward, it accelerates, and transfers energy to the runner. Water pressure decreases to atmospheric, or in some cases subatmospheric, as the water passes through the turbine blades and loses energy.
Around 1890, the modern fluid bearing was invented, now universally used to support heavy water turbine spindles. As of 2002, fluid bearings appear to have a mean time between failures of more than 1300 years.
Around 1913, Viktor Kaplan created the Kaplan turbine, a propeller-type machine. It was an evolution of the Francis turbine but revolutionized the ability to develop low-head hydro sites.
New concept.
All common water machines until the late 19th century (including water wheels) were basically reaction machines; water "pressure" head acted on the machine and produced work. A reaction turbine needs to fully contain the water during energy transfer.
In 1866, California millwright Samuel Knight invented a machine that took the impulse system to a new level. Inspired by the high pressure jet systems used in hydraulic mining in the gold fields, Knight developed a bucketed wheel which captured the energy of a free jet, which had converted a high head (hundreds of vertical feet in a pipe or penstock) of water to kinetic energy. This is called an impulse or tangential turbine. The water's velocity, roughly twice the velocity of the bucket periphery, does a u-turn in the bucket and drops out of the runner at low velocity.
In 1879, Lester Pelton, experimenting with a Knight Wheel, developed a Pelton wheel (double bucket design), which exhausted the water to the side, eliminating some energy loss of the Knight wheel which exhausted some water back against the center of the wheel. In about 1895, William Doble improved on Pelton's half-cylindrical bucket form with an elliptical bucket that included a cut in it to allow the jet a cleaner bucket entry. This is the modern form of the Pelton turbine which today achieves up to 92% efficiency. Pelton had been quite an effective promoter of his design and although Doble took over the Pelton company he did not change the name to Doble because it had brand name recognition.
Turgo and cross-flow turbines were later impulse designs.
Theory of operation.
Flowing water is directed on to the blades of a turbine runner, creating a force on the blades. Since the runner is spinning, the force acts through a distance (force acting through a distance is the definition of work). In this way, energy is transferred from the water flow to the turbine
Water turbines are divided into two groups; reaction turbines and impulse turbines.
The precise shape of water turbine blades is a function of the supply pressure of water, and the type of impeller selected.
Reaction turbines.
Reaction turbines are acted on by water, which changes pressure as it moves through the turbine and gives up its energy. They must be encased to contain the water pressure (or suction), or they must be fully submerged in the water flow.
Newton's third law describes the transfer of energy for reaction turbines.
Most water turbines in use are reaction turbines and are used in low (<30 m) and medium (30 -) head applications.
In reaction turbine pressure drop occurs in both fixed and moving blades.
It is largely used in dam and large power plants
Impulse turbines.
Impulse turbines change the velocity of a water jet. The jet pushes on the turbine's curved blades which changes the direction of the flow. The resulting change in momentum (impulse) causes a force on the turbine blades. Since the turbine is spinning, the force acts through a distance (work) and the diverted water flow is left with diminished energy. An impulse turbine is one which the pressure of the fluid flowing over the rotor blades is constant and all the work output is due to the change in kinetic energy of the fluid.
Prior to hitting the turbine blades, the water's pressure (potential energy) is converted to kinetic energy by a nozzle and focused on the turbine. No pressure change occurs at the turbine blades, and the turbine doesn't require a housing for operation.
Newton's second law describes the transfer of energy for impulse turbines.
Impulse turbines are often used in very high (>300m/1000 ft) head applications.
Power.
The power available in a stream of water is;
formula_1
where:
Pumped-storage hydroelectricity.
Some water turbines are designed for pumped-storage hydroelectricity. They can reverse flow and operate as a pump to fill a high reservoir during off-peak electrical hours, and then revert to a water turbine for power generation during peak electrical demand. This type of turbine is usually a Deriaz or Francis turbine in design.
Efficiency.
Large modern water turbines operate at mechanical efficiencies greater than 90%.
Types of water turbines.
Reaction turbines:
Impulse turbine
Design and application.
Turbine selection is based on the available water head, and less so on the available flow rate. In general, impulse turbines are used for high head sites, and reaction turbines are used for low head sites. Kaplan turbines with adjustable blade pitch are well-adapted to wide ranges of flow or head conditions, since their peak efficiency can be achieved over a wide range of flow conditions.
Small turbines (mostly under 10 MW) may have horizontal shafts, and even fairly large bulb-type turbines up to 100 MW or so may be horizontal. Very large Francis and Kaplan machines usually have vertical shafts because this makes best use of the available head, and makes installation of a generator more economical. Pelton wheels may be either vertical or horizontal shaft machines because the size of the machine is so much less than the available head. Some impulse turbines use multiple jets per runner to balance shaft thrust. This also allows for the use of a smaller turbine runner, which can decrease costs and mechanical losses.
Specific speed.
The specific speed formula_8 of a turbine characterizes the turbine's shape in a way that is not related to its size. This allows a new turbine design to be scaled from an existing design of known performance. The specific speed is also the main criteria for matching a specific hydro site with the correct turbine type.
The specific speed is the speed with which the turbine turns for a particular discharge Q, with unit head and thereby is able to produce unit power.
Affinity laws.
Affinity laws allow the output of a turbine to be predicted based on model tests. A miniature replica of a proposed design, about one foot (0.3 m) in diameter, can be tested and the laboratory measurements applied to the final application with high confidence. Affinity laws are derived by requiring similitude between the test model and the application.
Flow through the turbine is controlled either by a large valve or by wicket gates arranged around the outside of the turbine runner. Differential head and flow can be plotted for a number of different values of gate opening, producing a hill diagram used to show the efficiency of the turbine at varying conditions.
Runaway speed.
The runaway speed of a water turbine is its speed at full flow, and no shaft load. The turbine will be designed to survive the mechanical forces of this speed. The manufacturer will supply the runaway speed rating.
Control systems.
Different designs of governors have been used since the mid-19th century to control the speeds of the water turbines. A variety of flyball systems, or first-generation governors, were used during the first 100 years of water turbine speed controls. In early flyball systems, the flyball component countered by a spring acted directly to the valve of the turbine or the wicket gate to control the amount of water that enters the turbines. Newer systems with mechanical governors started around 1880. An early mechanical governors is a servomechanism that comprises a series of gears that use the turbine's speed to drive the flyball and turbine's power to drive the control mechanism. The mechanical governors were continued to be enhanced in power amplification through the use of gears and the dynamic behavior. By 1930, the mechanical governors had many parameters that could be set on the feedback system for precise controls. In the later part of the twentieth century, electronic governors and digital systems started to replace the mechanical governors. In the electronic governors, also known as second-generation governors, the flyball was replaced by rotational speed sensor but the controls were still done through analog systems. In the modern systems, also known as third-generation governors, the controls are performed digitally by algorithms that are programmed to the computer of the governor.
Maintenance.
Turbines are designed to run for decades with very little maintenance of the main elements; overhaul intervals are on the order of several years. Maintenance of the runners and parts exposed to water include removal, inspection, and repair of worn parts.
Normal wear and tear includes pitting corrosion from cavitation, fatigue cracking, and abrasion from suspended solids in the water. Steel elements are repaired by welding, usually with stainless steel rods. Damaged areas are cut or ground out, then welded back up to their original or an improved profile. Old turbine runners may have a significant amount of stainless steel added this way by the end of their lifetime. Elaborate welding procedures may be used to achieve the highest quality repairs.
Other elements requiring inspection and repair during overhauls include bearings, packing box and shaft sleeves, servomotors, cooling systems for the bearings and generator coils, seal rings, wicket gate linkage elements and all surfaces.
Environmental impact.
Water turbines are generally considered a clean power producer, as the turbine causes essentially no change to the water. They use a renewable energy source and are designed to operate for decades. They produce significant amounts of the world's electrical supply.
Historically there have also been negative consequences, mostly associated with the dams normally required for power production. Dams alter the natural ecology of rivers, potentially killing fish, stopping migrations, and disrupting peoples' livelihoods. For example, American Indian tribes in the Pacific Northwest had livelihoods built around salmon fishing, but aggressive dam-building destroyed their way of life. Dams also cause less obvious, but potentially serious consequences, including increased evaporation of water (especially in arid regions), buildup of silt behind the dam, and changes to water temperature and flow patterns. In the United States, it is now illegal to block the migration of fish, for example the endangered white sturgeon in North America, so fish ladders must be provided by dam builders.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="34054" url="http://en.wikipedia.org/wiki?curid=34054" title="White Wolf Publishing">
White Wolf Publishing

White Wolf Publishing was an American roleplaying game and book publisher. The company was founded in 1991 as a merger between Lion Rampant and "White Wolf Magazine", and was initially led by Mark Rein·Hagen of the former and Steve Wieck and Stewart Wieck of the latter. Since White Wolf Publishing, Inc. merged with CCP Games in 2006, White Wolf Publishing has been an imprint of CCP hf, but has ceased in-house production of any material, instead licensing their properties to other publishers. The name "White Wolf" originates from Michael Moorcock's works.
Overview.
White Wolf published a line of several different but overlapping games set in the "World of Darkness", a "modern gothic" world that, while seemingly similar to the real world, is home to supernatural terrors, ancient conspiracies, and several approaching apocalypses. The company also published the high fantasy "Exalted" RPG, the modern mythic "Scion", and d20 system material under their Sword & Sorcery imprint, including such titles as the "Dungeons & Dragons" gothic horror campaign setting "Ravenloft", and Monte Cook's "Arcana Unearthed" series. In order to complement the "World of Darkness" game line, a LARP system dubbed "Mind's Eye Theatre" has been published.
White Wolf has also released several series of novels based on the Old World of Darkness, all of which are currently out of print (although many are coming back into availability via print-on-demand).
White Wolf has also ventured in the collectible card game market with "Arcadia", "Rage", and ' (formerly "Jyhad"). "V:TES", perhaps the most successful card game, was originally published by Wizards of the Coast in 1994, but was abandoned just two years later after a revamped base set, name change and three expansions were published. White Wolf acquired the rights to the game in 2000, even though no new material had been produced for the game in over four years. Since then, several "V:TES" expansions have been released, and the game was the only official source of material for the Old World of Darkness, until 2011 when the 20th Anniversary Edition of ' was published and the Onyx Path was announced.
Video games such as ' and ' are based on White Wolf's role-playing game "". There are also several .
Merger and MMO.
On Saturday, 11 November 2006, White Wolf and CCP Games, the Icelandic MMO development company responsible for EVE Online, announced a merger between the two companies during the keynote address at the EVE Online Fanfest 2006. It was also revealed that a World of Darkness MMORPG was already in the planning stages. This game was cancelled in April 2014 after nine years of development. 
The "Old" or "Classic" World of Darkness game lines.
The games of this series use White Wolf's Storyteller System. Several games inspired spinoffs in the form of "historical" period settings such as the Dark Ages.
In addition to those game lines a series of books was produced under the title "World of Darkness". These provided stand-alone materials for multiple game lines with the focus on a specific region or theme, e.g. "WoD: Blood dimmed Tides" (about the oceans), "WoD: Combat" (an alternative 'crossover' combat system to resolve contradictory mechanics and add some sophistication), "WoD: Tokyo" and "WoD: Mafia".
For the Third Edition of "Ars Magica", White Wolf hitched that game's pseudo-historical setting to the 'future' "World of Darkness" setting. This was a simple adjustment (since the core premise of both settings is 'Earth as we know it' + 'supernatural fiction is reality') and particularly suited to the 'Tremere connection' between a clan of vampires from the original "Vampire" and a House of magi in the "Order of Hermes" (the central organization of "Ars Magica" as well as one of the 'Traditions' in ")".
The "New" World of Darkness game lines.
The games of this series use White Wolf's newer Storytelling System.
Mind's Eye Theatre (LARP).
The majority of the Old World of Darkness games were adapted into the original Mind's Eye Theatre format for live-action roleplaying. Product lines in this era include:
Subsequently, the Mind's Eye Theatre was revamped for the New World of Darkness. A core "Mind's Eye Theatre" rulebook was published as the LARP analogue to the "World of Darkness" core rulebook, with several Mind's Eye Theatre adaptations following in suit: "The Requiem", "The Forsaken", and "The Awakening" each adapted their respective namesakes to the new system of MET rules.
Imprints and labels.
White Wolf has different imprints under which various books are published, most notably:
Black Dog Game Factory was also a fictional company in the World of Darkness, as detailed in the "Subsidiaries: A Guide to Pentex" game supplement.
The Onyx Path.
At GenCon 2012 it was announced that CCP Games/White Wolf would not continue to produce table-top RPGs . Onyx Path Publishing, a new company by White Wolf Creative Director Richard Thomas, purchased the Trinity games and Scion from CCP and became licensee for the production of World of Darkness titles (classic and new), as well as Exalted. Onyx Path does not, however, hold a license to the Mind's Eye Theatre titles.

</doc>
<doc id="34057" url="http://en.wikipedia.org/wiki?curid=34057" title="William Congreve">
William Congreve

William Congreve (24 January 1670 – 19 January 1729) was an English playwright and poet.
Early life.
Congreve was born in Bardsey, West Yorkshire, England (near Leeds). His parents were William Congreve (1637–1708) and Mary ("née" Browning; 1636?–1715). A sister was buried in London in 1672. He spent his childhood in Ireland, where his father, a Cavalier, had settled during the reign of Charles II. Congreve was educated at Kilkenny College where he met Jonathan Swift, who would be his friend for the remainder of his life; and at Trinity College in Dublin. Upon graduation, he matriculated in the Middle Temple in London to study law, but felt himself pulled toward literature, drama, and the fashionable life. Artistically, he became a disciple of John Dryden.
Literary career.
William Congreve wrote some of the most popular English plays of the Restoration period of the late 17th century. By the age of thirty, he had written four comedies, including "Love for Love" (premiered 30 April 1695) and "The Way of the World" (premiered 1700), and one tragedy, "The Mourning Bride" (1697)
Unfortunately, his career ended almost as soon as it began. After writing five plays from his first in 1693 until 1700, he produced no more as public tastes turned against the sort of high-brow sexual comedy of manners in which he specialized. He reportedly was particularly stung by a critique written by Jeremy Collier ("A Short View of the Immorality and Profaneness of the English Stage"), to the point that he wrote a long reply, "Amendments of Mr. Collier's False and Imperfect Citations." A member of the Whig Kit-Kat Club, Congreve's career shifted to the political sector, where he held various minor political positions despite his stance as a Whig among Tories.
Later life.
Congreve withdrew from the theatre and lived the rest of his life on residuals from his early work. His output from 1700 was restricted to the occasional poem and some translation (notably Molière's "Monsieur de Pourceaugnac"). Congreve never married; in his own era and through subsequent generations, he was famous for his friendships with prominent actresses and noblewomen, including Anne Bracegirdle, for whom he wrote major parts in all his plays, and Henrietta Godolphin, 2nd Duchess of Marlborough, daughter of the famous general, John Churchill, 1st Duke of Marlborough, whom he had probably met by 1703 and who had a daughter, Mary (1723–1764), believed to be his.
As early as 1710, he suffered both from gout and from cataracts on his eyes. Congreve suffered a carriage accident in late September 1728, from which he never recovered (having probably received an internal injury); he died in London in January 1729, and was buried in Poets' Corner in Westminster Abbey.
Famous lines.
Two of Congreve's turns of phrase from "The Mourning Bride" (1697) have become famous, albeit frequently in misquotation, and often misattributed to William Shakespeare:
Congreve coined another famous phrase in "Love for Love" (1695):
References in popular culture.
A fictitious play by Congreve, "The Gallivant", features prominently in the novel "Flowers for the Judge" by Margery Allingham.

</doc>
<doc id="34059" url="http://en.wikipedia.org/wiki?curid=34059" title="War of 1812">
War of 1812

The War of 1812 was a military conflict, lasting for two-and-a-half years, fought by the United States of America against the United Kingdom of Great Britain and Ireland, its North American colonies, and its American Indian allies. Seen by the United States and Canada as a war in its own right, it is frequently seen in Europe as a theatre of the Napoleonic Wars, as it was caused by issues related to that war (especially the Continental System). The war resolved many issues which remained from the American Revolutionary War but involved no boundary changes. The United States declared war on June 18, 1812 for several reasons, including trade restrictions brought about by the British war with France, the impressment of American merchant sailors into the Royal Navy, British support of Indian tribes against American expansion, outrage over insults to national honor after humiliations on the high seas, and possible American interest in annexing British territory in modern-day Canada.
The war was fought in three principal theatres. Firstly, at sea, warships and privateers of each side attacked the other's merchant ships, while the British blockaded the Atlantic coast of the United States and mounted large raids in the later stages of the war. Secondly, land and naval battles were fought on the American–Canadian frontier, which ran along the Great Lakes, the Saint Lawrence River and the northern end of Lake Champlain. Thirdly, the American South and Gulf Coast also saw big land battles, in which the American forces defeated Britain's Indian allies and a British invasion force at New Orleans. At the end of the war both sides signed the Treaty of Ghent and both parties returned occupied land to its pre-war owner and resumed friendly trade relations.
With the majority of its land and naval forces tied down in Europe fighting the Napoleonic Wars, the British used a defensive strategy in the Provinces of Upper and Lower Canada, repelling initial American invasions. This demonstrated that the conquest of the Canadas would prove more difficult than anticipated. However, the Americans gained control of Lake Erie in 1813, seized parts of western Ontario, and ended the prospect of an Indian confederacy and an independent Indian state in the Midwest under British sponsorship. In April 1814, with the defeat of Napoleon, the British adopted a more aggressive strategy, sending larger invasion armies. In September 1814, the British invaded and occupied eastern Maine. In the south-west, General Andrew Jackson destroyed the military strength of the Creek nation at the Battle of Horseshoe Bend. The British victory at the Battle of Bladensburg in August 1814 allowed them to capture and burn Washington, D.C, but they were repulsed in an attempt to take Baltimore. American victories in September 1814 at the Battle of Plattsburgh repulsed the British invasions of New York, which along with pressure from merchants on the British government prompted British diplomats to drop their demands at Ghent for an independent native buffer state and territorial claims that London previously sought. Both sides agreed to a peace that restored the situation before the war began. However, it took six weeks for ships to cross the Atlantic so news of the peace treaty did not arrive before the British suffered a major defeat at New Orleans in January 1815.
In the United States, late victories over invading British armies at the battles of Plattsburg, Baltimore (inspiring their national anthem, "The Star-Spangled Banner") and New Orleans produced a sense of euphoria over a "second war of independence" against Britain. The Federalist Party had strongly opposed the war effort and prevented New England from providing much in the way of soldiers and troops; it now virtually collapsed. The war ended on a high note for Americans, bringing an "Era of Good Feelings" in which partisan animosity nearly vanished in the face of strengthened U.S. nationalism. Spain played a small role, but was not an official belligerent; some Spanish forces fought alongside the British during the Occupation of Pensacola. The U.S. took permanent ownership of Spain's Mobile District.
In Upper and Lower Canada, British and Provincial militia victories over invading American armies became iconic and promoted the development of a distinct Canadian identity, which included strong loyalty to Britain. Today, particularly in Ontario, memory of the war retains its significance, because the defeat of the invasions ensured that the Canadas would remain part of the British Empire, rather than be annexed by the United States. In Canada, numerous ceremonies took place in 2012 to commemorate the war, offer historical lessons and celebrate 200 years of peace across the border. The war is scarcely remembered in Britain, where attention focuses on the closer threat of Napoleon.
Origins.
Historians have long debated the relative weight of the multiple reasons underlying the United States declaration of war.
Honor and the second war of independence.
As Risjord (1961) notes, a powerful motivation for the Americans was the desire to uphold "national honor" in the face of what they considered to be British insults such as the "Chesapeake" affair. Brands says, "The other war hawks spoke of the struggle with Britain as a second war of independence; [Andrew] Jackson, who still bore scars from the first war of independence held that view with special conviction. The approaching conflict was about violations of American rights, but was it also about vindication of American identity". People at the time and historians since often called it America's "Second War of Independence."
Trade with France.
In 1807, Britain introduced a series of trade restrictions via a series of Orders in Council to impede American trade with France, with which Britain was at war. The United States contested these restrictions as illegal under international law. Also, historian Reginald Horsman states, "a large section of influential British opinion, both in the government and in the country, thought that America presented a threat to British maritime supremacy".
The American merchant marine had come close to doubling between 1802 and 1810, making it by far the largest neutral fleet. Britain was the largest trading partner, receiving 80% of U.S. cotton and 50% of other U.S. exports. The British public and press were resentful of the growing mercantile and commercial competition. The United States' view was that Britain's restrictions violated its right to trade with others.
Impressment.
During the Napoleonic Wars, the Royal Navy expanded to 175 ships of the line and 600 ships overall, requiring 140,000 sailors to man. While the Royal Navy could man its ships with volunteers in peacetime, it competed in wartime with merchant shipping and privateers for a small pool of experienced sailors and turned to impressment when it could not operate ships with volunteers alone. Britain did not recognize the right of a British subject to relinquish his status as a British subject, emigrate and transfer his national allegiance as a naturalized citizen to any other country. Thus while the United States recognized British-born sailors on American ships as Americans, Britain did not. It was estimated that there were 11,000 naturalized sailors on United States ships in 1805. Secretary of the Treasury Albert Gallatin stated that 9,000 were born in Britain. The Royal Navy went after them by intercepting and searching U.S. merchant ships for deserters. Impressment actions such as the "Leander" Affair and the "Chesapeake"–"Leopard" Affair outraged Americans, because they infringed on national sovereignty and denied America's ability to naturalize foreigners. Moreover, a great number of British sailors serving as naturalized Americans on U.S. ships were Irish. An investigation by Captain Isaac Chauncey in 1808 found that 58% of the sailors based in New York City were either naturalized citizens or recent immigrants, the majority of foreign sailors (134 of 150) being from Britain. Moreover, eighty of the 134 British sailors were Irish. The US Navy also forcibly recruited British sailors but the British government saw impressment as commonly accepted practice and preferred to rescue British sailors from American impressment on a case-by-case basis.
The United States believed that British deserters had a right to become United States citizens. Britain did not recognize naturalized United States citizenship, so in addition to recovering deserters, it considered United States citizens born British liable for impressment. Aggravating the situation was the widespread use of forged identity or protection papers by sailors. This made it difficult for the Royal Navy to distinguish Americans from non-Americans and led it to impress some Americans who had never been British. (Some gained freedom on appeal.) American anger at impressment grew when British frigates were stationed just outside U.S. harbors in view of U.S. shores and searched ships for contraband and impressed men while in U.S. territorial waters. "Free trade and sailors' rights" was a rallying cry for the United States throughout the conflict.
British support for American Indian raids.
The Northwest Territory, comprising the modern states of Ohio, Indiana, Illinois, Michigan, and Wisconsin, was the battleground for conflict between the Indian Nations and the United States. The British Empire had ceded the area to the United States in the Treaty of Paris in 1783, both sides ignoring the fact that the land was already inhabited by various Indian nations. These included the Miami, Winnebago, Shawnee, Fox, Sauk, Kickapoo, Delaware and Wyandot. Some warriors, who had left their nations of origin, followed Tenskwatawa, the Shawnee Prophet and the brother of Tecumseh. Tenskwatawa had a vision of purifying his society by expelling the "children of the Evil Spirit": the American settlers. Tenskwatawa and Tecumseh formed a confederation of numerous tribes to block American expansion. The British saw the Indian nations as valuable allies and a buffer to its Canadian colonies and provided arms. Attacks on American settlers in the Northwest further aggravated tensions between Britain and the United States. The confederation's raids and existence hindered American expansion into rich farmlands in the Northwest Territory. Pratt writes:
There is ample proof that the British authorities did all in their power to hold or win the allegiance of the Indians of the Northwest with the expectation of using them as allies in the event of war. Indian allegiance could be held only by gifts, and to an Indian no gift was as acceptable as a lethal weapon. Guns and ammunition, tomahawks and scalping knives were dealt out with some liberality by British agents.
Raiding grew more common in 1810 and 1811; Westerners in Congress found the raids intolerable and wanted them permanently ended.
However, according to the U.S Army Center of Military History, the "land-hungry frontiersmen", with "no doubt that their troubles with the Indians were the result of British intrigue", exacerbated the problem by "[circulating stories] after every Indian raid of British Army muskets and equipment being found on the field". Thus, "the westerners were convinced that their problems could best be solved by forcing the British out of Canada".<ref name="http://www.history.army.mil.htm"></ref>
The British had the long-standing goal of creating a large "neutral" Indian state that would cover much of Ohio, Indiana, and Michigan. They made the demand as late as the fall of 1814 at the peace conference, but lost control of western Ontario in 1813 at key battles on and around Lake Erie. These battles destroyed the Indian confederacy which had been the main ally of the British in the region, and which would make up the proposed neutral state. Although the area remained under British or British-allied Indians' control until the end of the war, it became a less practicable idea. At American insistence, and with higher priorities, the British dropped the demands.
American expansionism.
American expansion into the Northwest Territory was being obstructed by indigenous leaders like Tecumseh, who were supplied and encouraged by the British. Americans on the western frontier demanded that interference be stopped. There is dispute, however, over whether or not the American desire to annex Canada brought on the war. Several historians believe that the capture of Canada was intended only as a means to secure a bargaining chip, which would then be used to force Britain to back down on the maritime issues. It would also cut off food supplies for Britain's West Indian colonies, and temporarily prevent the British from continuing to arm the Indians. However, a significant minority of historians believes that a desire to annex Canada was a cause of the war. This view was more prevalent before 1940, but continues to be held by a number of historians. The U.S. Army Center for Military History compares War Hawks' desire to annex the Canadas to the enthusiasm for the annexation of Spanish Florida by inhabitants of the American South. Congressman Richard Mentor Johnson told Congress that the constant Indian atrocities along the Wabash River in Indiana were enabled by supplies from Canada and were proof that, "the war has already commenced. ... I shall never die contended until I see England's expulsion from North America and her territories incorporated into the United States."
Upper Canada (modern southern Ontario) had mostly been settled by Revolution-era exiles from the United States (United Empire Loyalists) or postwar American immigrants. The Loyalists were hostile to union with the United States, while the immigrant settlers were generally uninterested in politics and remained neutral or supported the British during the war. The Canadian colonies were thinly populated and only lightly defended by the British Army. Americans then believed that many men in Upper Canada would rise up and greet an American invading army as liberators. That did not happen. One reason American forces retreated after one successful battle inside Canada was that they could not obtain supplies from the locals. But the Americans thought that the possibility of local support suggested an easy conquest, as former President Thomas Jefferson believed: "The acquisition of Canada this year, as far as the neighborhood of Quebec, will be a mere matter of marching, and will give us the experience for the attack on Halifax, the next and final expulsion of England from the American continent".
Annexation was supported by American businessmen who wanted to gain control of Great Lakes trade.
Stagg has examined at the fate of the expansionist cause proposed by Hacker and Pratt in the 1920s:
this 'expansionist' interpretation of the war can still be found in textbooks currently in use in the nation's high schools. It has also compounded popular confusion about the war by perpetuating an arid dispute over what should be deemed to be its 'real' or most important causes. Were these causes international or domestic in origin? That debate became both interminable and insoluble. Consequently, a new generation of historians by the 1960s ... repudiated the views of Hacker and Pratt.
Maass argued in 2015 that the expansionist theme is a myth that goes against the "relative consensus among experts that the primary U.S. objective was the repeal of British maritime restrictions". He argues that consensus among scholars is that the United States went to war "because six years of economic sanctions had failed to bring Britain to the negotiating table, and threatening the Royal Navy’s Canadian supply base was their last hope." Maass agrees that theoretically expansionism might have tempted Americans, but finds that "leaders feared the domestic political consequences of doing so. Notably, what limited expansionism there was focused on sparsely populated western lands rather than the more populous eastern settlements [of Canada]."
David and Jeanne Heidler argue that "acquiring Canada would [have] satisfied America's expansionist desires", also describing it as a key goal of western expansionists, who, they argue, believed that "eliminating the British presence in Canada would best accomplish" their goal of halting British support for Indian raids. They argue that the "enduring debate" is over the relative importance of expansionism as a factor, and whether "expansionism played a greater role in causing the War of 1812 than American concern about protecting neutral maritime rights." 
U.S. political conflict.
While the British government was largely oblivious to the deteriorating North-American situation because of its involvement in a continent-wide European War, the U.S. was in a period of significant political conflict between the Federalist Party (based mainly in the Northeast), which favored a strong central government and closer ties to Britain, and the Republicans (with its greatest power base in the South and West), which favored a weak central government, preservation of slavery, expansion into Indian land, and a stronger break with Britain. By 1812, the Federalist Party had weakened considerably, and the Republicans, with James Madison completing his first term of office and control of Congress, were in a strong position to pursue their more aggressive agenda against Britain. Throughout the war, support for the U.S. cause was weak (or sometimes non-existent) in Federalist areas of the Northeast. Few men volunteered to serve; the banks avoided financing the war. The negatavism of the Federalists, especially as exemplified by the Hartford Convention of 1814–15 ruined its reputation and the Party survived only in scattered areas. By 1815 there was broad support for the war from all parts of the country. This allowed the triumphant Republicans to adopt some Federalist policies, such as a national bank, which Madison reestablished in 1816.
Declaration of war.
On June 1, 1812, President James Madison sent a message to Congress recounting American grievances against Great Britain, though not specifically calling for a declaration of war. After Madison's message, the House of Representatives deliberated for four days behind closed doors before voting 79 to 49 (61% in favor) the first declaration of war, and the Senate agreed by 19 to 13 (59% in favor). The conflict began formally on June 18, 1812, when Madison signed the measure into law and proclaimed it the next day. This was the first time that the United States had declared war on another nation, and the Congressional vote would prove to be the closest vote to formally declare war in American history. (The Authorization for Use of Military Force Against Iraq Resolution of 1991, while not a formal declaration of war, was a closer vote.) None of the 39 Federalists in Congress voted in favor of the war; critics of war subsequently referred to it as "Mr. Madison's War".
Earlier in London on May 11, an assassin had killed Prime Minister Spencer Perceval, which resulted in Lord Liverpool coming to power. Liverpool wanted a more practical relationship with the United States. On June 23, he issued a repeal of the Orders in Council, but the United States was unaware of this, as it took three weeks for the news to cross the Atlantic. On June 28, 1812, HMS "Colibri" was despatched from Halifax under a flag of truce to New York. On July 9, she anchored off Sandy Hook, and three days later sailed on her return with a copy of the declaration of war; the British ambassador, Mr. Foster; and consul, Colonel Barclay. She arrived in Halifax, Nova Scotia eight days later. The news of the declaration took even longer to reach London. In response to the U.S. declaration of war, Isaac Brock issued a proclamation alerting the citizenry in Upper Canada of the state of war and urging all military personnel "to be vigilant in the discharge of their duty" to prevent communication with the enemy and to arrest anyone suspected of helping the Americans.
Course of the war.
Although the outbreak of the war had been preceded by years of angry diplomatic dispute, neither side was ready for war when it came. Britain was heavily engaged in the Napoleonic Wars, most of the British Army was deployed in the Peninsular War (in Portugal and Spain), and the Royal Navy was compelled to blockade most of the coast of Europe. The number of British regular troops present in Canada in July 1812 was officially stated to be 6,034, supported by Canadian militia. Throughout the war, the British Secretary of State for War and the Colonies was the Earl of Bathurst. For the first two years of the war, he could spare few troops to reinforce North America and urged the commander-in-chief in North America (Lieutenant General Sir George Prévost) to maintain a defensive strategy. The naturally cautious Prévost followed these instructions, concentrating on defending Lower Canada at the expense of Upper Canada (which was more vulnerable to American attacks) and allowing few offensive actions.
The United States was not prepared to prosecute a war, for Madison had assumed that the state militias would easily seize Canada and that negotiations would follow. In 1812, the regular army consisted of fewer than 12,000 men. Congress authorized the expansion of the army to 35,000 men, but the service was voluntary and unpopular; it offered poor pay, and there were few trained and experienced officers, at least initially. The militia objected to serving outside their home states, were not open to discipline, and performed poorly against British forces when outside their home states. American prosecution of the war suffered from its unpopularity, especially in New England, where anti-war speakers were vocal. "Two of the Massachusetts members [of Congress], Seaver and Widgery, were publicly insulted and hissed on Change in Boston; while another, Charles Turner, member for the Plymouth district, and Chief-Justice of the Court of Sessions for that county, was seized by a crowd on the evening of August 3, [1812] and kicked through the town". The United States had great difficulty financing its war. It had disbanded its national bank, and private bankers in the Northeast were opposed to the war. The United States was able to obtain financing from London-based Barings Bank to cover overseas bond obligations. The failure of New England to provide militia units or financial support was a serious blow. Threats of secession by New England states were loud, as evidenced by the Hartford Convention. Britain exploited these divisions, blockading only southern ports for much of the war and encouraging smuggling.
On July 12, 1812, General William Hull led an invading American force of about 1,000 untrained, poorly equipped militia across the Detroit River and occupied the Canadian town of Sandwich (now a neighborhood of Windsor, Ontario). By August, Hull and his troops (numbering 2,500 with the addition of 500 Canadians) retreated to Detroit, where they surrendered to a significantly smaller force of British regulars, Canadian militia and Native Americans, led by British Major General Isaac Brock and Shawnee leader Tecumseh. The surrender not only cost the United States the village of Detroit, but control over most of the Michigan Territory. Several months later, the U.S. launched a second invasion of Canada, this time at the Niagara peninsula. On October 13, United States forces were again defeated at the Battle of Queenston Heights, where General Brock was killed.
Military and civilian leadership remained a critical American weakness until 1814. The early disasters brought about chiefly by American unpreparedness and lack of leadership drove United States Secretary of War William Eustis from office. His successor, John Armstrong, Jr., attempted a coordinated strategy late in 1813 (with 10,000 men) aimed at the capture of Montreal, but he was thwarted by logistical difficulties, uncooperative and quarrelsome commanders and ill-trained troops. After losing several battles to inferior forces, the Americans retreated in disarray in October 1813.
A decisive use of naval power came on the Great Lakes and depended on a contest of building ships. The U.S. started a rapidly expanded program of building warships at Sackets Harbor on Lake Ontario, where 3,000 men were recruited, many from New York City, to build 11 warships early in the war. In 1813, the Americans won control of Lake Erie in the Battle of Lake Erie and cut off British and Native American forces in the west from their supply base; they were decisively defeated by General William Henry Harrison's forces on their retreat towards Niagara at the Battle of the Thames in October 1813. Tecumseh, the leader of the tribal confederation, was killed and his Indian coalition disintegrated. While some natives continued to fight alongside British troops, they subsequently did so only as individual tribes or groups of warriors, and where they were directly supplied and armed by British agents. The Americans controlled western Ontario, and permanently ended the threat of Indian raids supplied by the British in Canada into the American Midwest, thus achieving a basic war goal. Raids would continue from the unsubdued Indian tribes in the Old Northwest, which remained under British/Indian control, until the end of the war. Control of Lake Ontario changed hands several times, with both sides unable and unwilling to take advantage of temporary superiority.
At sea, the powerful Royal Navy blockaded much of the coastline, though it was allowing substantial exports from New England, which traded with Canada in defiance of American laws. The blockade devastated American agricultural exports, but it helped stimulate local factories that replaced goods previously imported. The American strategy of using small gunboats to defend ports was a fiasco, as the British raided the coast at will. The most famous episode was a series of British raids on the shores of Chesapeake Bay, including an attack on Washington that resulted in the British burning of the White House, the Capitol, the Navy Yard, and other public buildings, in the "Burning of Washington". The British power at sea was enough to allow the Royal Navy to levy "contributions" on bayside towns in return for not burning them to the ground. The Americans were more successful in ship-to-ship actions. They sent out several hundred privateers to attack British merchant ships; in the first four months of war they captured 219 British merchant ships. British commercial interests were damaged, especially in the West Indies.
After Napoleon abdicated on April 6, 1814, the British could send veteran armies to the United States, but by then the Americans had learned how to mobilize and fight. British General Prévost launched a major invasion of Upstate New York with these veteran soldiers, but the American fleet under Thomas Macdonough gained control of Lake Champlain and the British lost the Battle of Plattsburgh in September 1814. Prévost, blamed for the defeat, sought a court-martial to clear his name, but he died in London awaiting it. The British then launched a successful attack on Chesapeake Bay, capturing and burning Washington, looting Alexandria, and unsuccessfully attacking Baltimore. The embarrassing Burning of Washington led to Armstrong's dismissal as U.S. Secretary of War. A British invasion of Louisiana (unknowingly launched after the Treaty of Ghent was negotiated to end the war) was defeated with heavy British losses by General Andrew Jackson at the Battle of New Orleans in January 1815. The victory made Jackson a national hero, restored the American sense of honor, and ruined the Federalist party's efforts to condemn the war as a failure. With the ratification of the peace treaty in February 1815, the war ended before the U.S. new Secretary of War James Monroe could put his new offensive strategy into effect, and before the British could launch renewed attacks.
Once Britain and The Sixth Coalition defeated Napoleon in 1814, France and Britain became allies. Britain ended the trade restrictions and the impressment of American sailors, thus removing two more causes of the war. After two years of warfare, the major causes of the war had disappeared. Neither side had a reason to continue or a chance of gaining a decisive success that would compel their opponents to cede territory or advantageous peace terms. As a result of this stalemate, the two countries signed the Treaty of Ghent on December 24, 1814. News of the peace treaty took two months to reach the U.S., during which fighting continued. The war fostered a spirit of national unity and an "Era of Good Feelings" in the U.S., as well as in Canada. It opened a long era of peaceful relations between the United States and the British Empire.
Theatres of war.
The war was conducted in three theatres:
Atlantic theatre.
Opening strategies.
In 1812, Britain's Royal Navy was the world's largest, with over 600 cruisers in commission and some smaller vessels. Although most of these were involved in blockading the French navy and protecting British trade against (usually French) privateers, the Royal Navy still had 85 vessels in American waters, counting all British Navy vessels in North American and the Caribbean waters. But, the Royal Navy's North American squadron based in Halifax, Nova Scotia, which bore the brunt of the war, numbered one small ship of the line, seven frigates, nine smaller sloops and brigs along with five schooners. By contrast, the United States Navy comprised 8 frigates, 14 smaller sloops and brigs, and no ships of the line. The U.S. had embarked on a major shipbuilding program before the war at Sackets Harbor, New York and continued to produce new ships. Three of the existing American frigates were exceptionally large and powerful for their class, larger than any British frigate in North America. Whereas the standard British frigate of the time was rated as a 38 gun ship, usually carrying up to 50 guns, with its main battery consisting of 18-pounder guns; the USS "Constitution", "President", and "United States", in comparison, were rated as 44-gun ships, carrying 56–60 guns with a main battery of 24-pounders.
The British strategy was to protect their own merchant shipping to and from Halifax, Nova Scotia, and the West Indies, and to enforce a blockade of major American ports to restrict American trade. Because of their numerical inferiority, the American strategy was to cause disruption through hit-and-run tactics, such as the capture of prizes and engaging Royal Navy vessels only under favorable circumstances. Days after the formal declaration of war, however, it put out two small squadrons, including the frigate "President" and the sloop "Hornet" under Commodore John Rodgers, and the frigates "United States" and "Congress", with the brig "Argus" under Captain Stephen Decatur. These were initially concentrated as one unit under Rodgers, who intended to force the Royal Navy to concentrate its own ships to prevent isolated units being captured by his powerful force.
Large numbers of American merchant ships were returning to the United States with the outbreak of war, and if the Royal Navy was concentrated, it could not watch all the ports on the American seaboard. Rodgers' strategy worked, in that the Royal Navy concentrated most of its frigates off New York Harbor under Captain Philip Broke, allowing many American ships to reach home. But, Rodgers' own cruise captured only five small merchant ships, and the Americans never subsequently concentrated more than two or three ships together as a unit.
Single-ship actions.
Meanwhile, the "Constitution", commanded by Captain Isaac Hull, sailed from Chesapeake Bay on July 12. On July 17, Broke's British squadron gave chase off New York, but the "Constitution" evaded her pursuers after two days. After briefly calling at Boston to replenish water, on August 19, the "Constitution" engaged the British frigate HMS "Guerriere". After a 35-minute battle, "Guerriere" had been dis-masted and captured and was later burned. The "Constitution" earned the nickname "Old Ironsides" following this battle as many of the British cannonballs were seen to bounce off her hull. Hull returned to Boston with news of this significant victory. On October 25, the "United States", commanded by Captain Decatur, captured the British frigate HMS "Macedonian", which he then carried back to port. At the close of the month, the "Constitution" sailed south, now under the command of Captain William Bainbridge. On December 29, off Bahia, Brazil, she met the British frigate HMS "Java". After a battle lasting three hours, "Java" struck her colors and was burned after being judged unsalvageable. The "Constitution", however, was relatively undamaged in the battle.
The successes gained by the three big American frigates forced Britain to construct five 40-gun, 24-pounder heavy frigates and two "spar-decked" frigates (the 60-gun HMS "Leander" and HMS "Newcastle") and to razee three old 74-gun ships of the line to convert them to heavy frigates. The Royal Navy acknowledged that there were factors other than greater size and heavier guns. The United States Navy's sloops and brigs had also won several victories over Royal Navy vessels of approximately equal strength. While the American ships had experienced and well-drilled volunteer crews, the enormous size of the overstretched Royal Navy meant that many ships were shorthanded and the average quality of crews suffered. The constant sea duties of those serving in North America interfered with their training and exercises.
The capture of the three British frigates stimulated the British to greater exertions. More vessels were deployed on the American seaboard and the blockade tightened. On June 1, 1813, off Boston Harbor, the frigate "Chesapeake", commanded by Captain James Lawrence, was captured by the British frigate HMS "Shannon" under Captain Philip Broke. Lawrence was mortally wounded and famously cried out, "Don't give up the ship! Hold on, men!" The two frigates were of near-identical size. "Chesapeake" ‍ '​s crew was larger but most had not served or trained together. British citizens reacted with celebration and relief that the run of American victories had ended. Notably, this action was by ratio one of the bloodiest contests recorded during this age of sail, with more dead and wounded than HMS "Victory" suffered in four hours of combat at Trafalgar. Captain Lawrence was killed and Captain Broke was so badly wounded that he never again held a sea command.
In January 1813, the American frigate "Essex", under the command of Captain David Porter, sailed into the Pacific to harass British shipping. Many British whaling ships carried letters of marque allowing them to prey on American whalers, and they nearly destroyed the industry. The "Essex" challenged this practice. She inflicted considerable damage on British interests before she and her tender, "Essex Junior" (armed with twenty guns) were captured off Valparaiso, Chile by the British frigate HMS "Phoebe" and the sloop HMS "Cherub" on March 28, 1814.
The British 6th-rate "Cruizer"-class brig-sloops did not fare well against the American ship-rigged sloops of war. The "Hornet" and "Wasp" constructed before the war were notably powerful vessels, and the "Frolic" class built during the war even more so (although "Frolic" was trapped and captured by a British frigate and a schooner). The British brig-rigged sloops tended to suffer fire to their rigging more frequently than the American ship-rigged sloops. In addition, the ship-rigged sloops could back their sails in action, giving them another advantage in manoeuvring.
Following their earlier losses, the British Admiralty instituted a new policy that the three American heavy frigates should not be engaged except by a ship of the line or smaller vessels in squadron strength. An example of this was the capture of the "President" by a squadron of four British frigates in January 1815. But, a month later, the "Constitution" engaged and captured two smaller British warships, HMS "Cyane" and HMS "Levant", sailing in company.
Success in single ship battles raised American morale after the repeated failed invasion attempts in Upper and Lower Canada. However these single ship victories had no military effect on the war at sea as they did not alter the balance of naval power, impede British supplies and reinforcements, or even raise insurance rates for British trade.
Privateering.
The operations of American privateers proved a more significant threat to British trade than the U.S. Navy. They operated throughout the Atlantic and continued until the close of the war, most notably from ports such as Baltimore. American privateers reported taking 1300 British merchant vessels, compared to 254 taken by the U.S. Navy. although the insurer Lloyd's of London reported that only 1,175 British ships were taken, 373 of which were recaptured, for a total loss of 802. However the British were able to limit privateering losses by the strict enforcement of convoy by the Royal Navy and by capturing 278 American privateers. Due to the massive size of the British merchant fleet, American captures only affected 7.5% of the British merchant fleet, resulting no supply shortages or lack of reinforcements for British forces in North America.
Due to the large size of their navy, the British did not rely as much on privateering. The majority of the 1,407 captured American merchant ships were taken by the Royal Navy. The war was the last time the British allowed privateering, since the practice was coming to be seen as politically inexpedient and of diminishing value in maintaining its naval supremacy. However privateering remained popular in British colonies. It was the last hurrah for privateers Bermuda who vigorously returned to the practice after experience in previous wars. The nimble Bermuda sloops captured 298 American ships. Privateer schooners based in British North America, especially from Nova Scotia took 250 American ships and proved especially effective in crippling American coastal trade and capturing American ships closer to shore than the Royal Navy cruisers.
Blockade.
The small British North American squadron had difficulty at the beginning of the war in blockading the entire U.S. coast, faced by the need to convoy vessels against American privateers. However as additional ships were sent to North America in 1813, the Royal Navy was able to tighten the blockade and extend it, first to the coast south of Narragansett by November 1813 and to the entire American coast on May 31, 1814.
The British government, having need of American foodstuffs for its army in Spain, benefited from the willingness of the New Englanders to trade with them, so no blockade of New England was at first attempted. The Delaware River and Chesapeake Bay were declared in a state of blockade on December 26, 1812. Illicit trade was carried on by collusive captures arranged between American traders and British officers. American ships were fraudulently transferred to neutral flags. Eventually, the U.S. government was driven to issue orders to stop illicit trading; this put only a further strain on the commerce of the country. The overpowering strength of the British fleet enabled it to occupy the Chesapeake and to attack and destroy numerous docks and harbors.
The blockade of American ports later tightened to the extent that most American merchant ships and naval vessels were confined to port. The American frigates "United States" and "HMS Macedonian" ended the war blockaded and hulked in New London, Connecticut. Some merchant ships were based in Europe or Asia and continued operations. Others, mainly from New England, were issued licences to trade by Admiral Sir John Borlase Warren, commander in chief on the American station in 1813. This allowed Wellington's army in Spain to receive American goods and to maintain the New Englanders' opposition to the war. The blockade nevertheless resulted in American exports decreasing from $130 million in 1807 to $7 million in 1814. Most of these were food exports that ironically went to supply their enemies in Britain or British colonies.
As the Royal Navy base that supervised the blockade, Halifax profited greatly during the war. From that base British privateers seized many French and American ships and sold their prizes in Halifax.
Freeing and recruiting slaves.
The British Royal Navy's blockades and raids allowed about 4,000 African Americans to escape slavery by fleeing American plantations to find freedom aboard British ships, a migration known as the Black Refugees. The blockading British fleet in Chesapeake Bay received increasing numbers of enslaved black Americans during 1813. They were welcomed by British government order, and were treated as free persons when reaching British hands. A proclamation of April 2, 1814, offered freedom to slaves reaching British lines or ships. About 2,400 of the escaped slaves and their families who served in the Royal Navy following their escape settled in Nova Scotia and New Brunswick during and after the war. From May 1814, younger men among the volunteers were recruited into a new Corps of Colonial Marines. They fought for Britain throughout the Atlantic campaign, including the Battle of Bladensburg and the attacks on Washington, D.C. and Battle of Baltimore, later settling in Trinidad. The slaves who escaped to the British navy represented the largest emancipation of African Americans before the American Civil War.
Occupation of Maine.
Maine, then part of Massachusetts, was a base for smuggling and illegal trade between the U.S. and the British. Until 1813 the region was generally quiet except for privateer actions near the coast. In September 1813, there was a notable naval action when the U.S. Navy's brig "Enterprise" fought and captured the Royal Navy brig "Boxer" off Pemaquid Point. The first British assault came in July 1814, when Sir Thomas Masterman Hardy took Moose Island (Eastport, Maine) without a shot, with the entire American garrison of Fort Sullivan—which became the British Fort Sherbrooke—surrendering. Next, from his base in Halifax, Nova Scotia, in September 1814, Sir John Coape Sherbrooke led 3,000 British troops in the "Penobscot Expedition". In 26 days, he raided and looted Hampden, Bangor, and Machias, destroying or capturing 17 American ships. He won the Battle of Hampden (losing two killed while the Americans lost one killed). Retreating American forces were forced to destroy the frigate "Adams". The British occupied the town of Castine and most of eastern Maine for the rest of the war, re-establishing the colony of New Ireland. The Treaty of Ghent returned this territory to the United States, though Machias Seal Island has remained in dispute. The British left in April 1815, at which time they took ₤10,750 obtained from tariff duties at Castine. This money, called the "Castine Fund", was used to establish Dalhousie University, in Halifax, Nova Scotia.
Chesapeake campaign and "The Star-Spangled Banner".
The strategic location of the Chesapeake Bay near America's new national capital, Washington, D.C. on the major tributary of the Potomac River, made it a prime target for the British and their Royal Navy and the King's Army. Starting in March 1813, a squadron under Rear Admiral George Cockburn started a blockade of the mouth of the Bay at Hampton Roads harbor and raided towns along the Bay from Norfolk, Virginia to Havre de Grace, Maryland.
On July 4, 1813, Commodore Joshua Barney, a Revolutionary War naval hero, convinced the U.S. Navy Department to build the Chesapeake Bay Flotilla, a squadron of twenty barges powered by small sails or oars (sweeps) to defend the Chesapeake Bay. Launched in April 1814, the squadron was quickly cornered in the Patuxent River, and while successful in harassing the Royal Navy, they were powerless to stop the British campaign that ultimately led to the "Burning of Washington". This expedition, led by Cockburn and General Robert Ross, was carried out between August 19 and 29, 1814, as the result of the hardened British policy of 1814 (although British and American commissioners had convened peace negotiations at Ghent in June of that year). As part of this, Admiral Warren had been replaced as commander in chief by Admiral Alexander Cochrane, with reinforcements and orders to coerce the Americans into a favorable peace.
Governor-in-chief of British North America in Upper and Lower Canada, Sir George Prévost had written to the Admirals on the North American Station in Bermuda, calling for retaliation for the American sacking and burning of York (now largest city of Toronto on north shore of Lake Erie). A force of 2,500 soldiers under General Ross had just arrived in Bermuda aboard ""H.M.S. Royal Oak", three frigates, three sloops and ten other vessels. Released from the Peninsular War in Spain and Portugal by British victory, the British intended to use them for diversionary raids along the coasts of Maryland and Virginia. In response to Prévost's request, they decided to employ this force, together with the naval and military units already on the station, to strike at the "Federal City" of Washington, D.C.
On August 24, U.S. Secretary of War, John Armstrong insisted that the British would attack Baltimore rather than Washington, even when units of the British Army, accompanied by major ships of the Royal Navy, was obviously on their way to the capital. The inexperienced American militia, which had congregated at Bladensburg, Maryland, to protect the capital, were defeated in the Battle of Bladensburg, opening the route to Washington. While First Lady Dolley Madison saved valuables from the then named "President's House" (or "President's Palace" [executive mansion] - now "White House"), Fourth President James Madison and the government with members of the Presidential Cabinet, fled to Virginia. Seeing that the Battle of Bladensburg, northeast of the town in rural Prince George's County was not going well, the Secretary of the Navy commanded Captain Thomas Tingey, commandant of the Washington Naval Yard on the Eastern Branch of the Potomac River (now the Anacostia River), to set the facility ablaze to prevent the capture of American naval ships, buildings, shops and supplies. Tingey had overseen the Naval Yard's planning and development since the national capital had been moved from Philadelphia to Washington in 1800, and waited until the very last possible minute, nearly four hours after the order was given to execute it. The destruction included most of the facility as well as the nearly-completed frigate "Columbia" and the sloop "Argus".
The British commanders ate the supper that had been prepared for the President and his departmental secretaries after returning from hopeful glorious U.S. victory, before they burned the Executive Mansion; American morale was reduced to an all-time low. The British viewed their actions as retaliation for the destructive American invasions and raids into Canada, most notably the Americans' burning of York (now Toronto) earlier in 1813. Later that same evening, a furious storm (some later weather experts called it a thunderstorm, almost a hurricane) swept into Washington, D.C., sending one or more tornadoes into the rough, unfinished town that caused more damage but finally extinguished the fires with torrential rains, leaving fire-blackened walls and partial ruins of the President's House, The Capitol, Treasury Department that were set the first night. The British left Washington, D.C. the following day after the storm subsided. Also unfortunately, an explosion of the combustibles they used to finish off the Navy Yard destruction that the Americans had started, exploded, killing or maimed a large number of "Red-Coats"
Having destroyed Washington's public buildings, including the President's Mansion and the Treasury, the British army and navy next moved several weeks later to capture Baltimore, forty miles northeast, a busy port and a key base for American privateers. However by not immediately going overland to the port city they sneeringly called a "nest of pirates", but returned to their ships anchored in the Patuxent River and proceeding later up to the Upper Bay, gave the Baltimoreans plenty of time to reinforce their fortifications and gather regular U.S. Army and state militia troops from surrounding counties and states. The subsequent "Battle for Baltimore" began with the British landing on Sunday, September 12th, 1814, at North Point, where the Baltimore harbor's Patapsco River met the Chesapeake Bay, where they were met by American militia further up the "Patapsco Neck" peninsula. An exchange of fire began, with casualties on both sides. Major Gen. Robert Ross was killed by American snipers as he attempted to rally his troops in the first skirmish. The snipers were killed moments later, and the British paused, then continued to march northwestward to the stationed Maryland and Baltimore City militia units deployed further up Long Log Lane on the peninsula at "Godly Wood" where the later Battle of North Point was fought for several afternoon hours in a musketry and artillery duel under command of British Col. Arthur Brooke and American commander for the Maryland state militia and its Third Brigade (or "Baltimore City Brigade"), Brig. Gen. John Stricker. The British also planned to simultaneously attack Baltimore by water on the following day, September 13th, to support their military now arrayed facing the massed, heavily dug-in and fortified American units of approximately 15,000 with about a hundred cannon gathered along the eastern heights of the city named "Loudenschlager's Hill" (later "Hampstead Hill" - now part of Patterson Park). These overall Baltimore defenses had been planned in advance and foreseen by the state militia commander, Maj. Gen. Samuel Smith, who had been set in charge of the Baltimore defenses instead of the discredited U.S. Army commander for the Mid-Atlantic's 10th Military District (following the debacle the previous month at Bladensburg), William H. Winder). Smith had been earlier a Revolutionary War officer and commander, then wealthy city merchant and U.S. Representative, Senator and later Mayor of Baltimore. The "Red Coats" were unable to immediately reduce Fort McHenry, at the entrance to Baltimore Harbor to allow their ships to provide heavier naval gunfire to support their troops to the northeast.
At the bombardment of Fort McHenry, the British naval guns, mortars and revolutionary new "Congreve rockets" had a longer range than the American cannon onshore, and the ships mostly stood off out of the Americans' range, bombarding the fort, which returned very little fire and was not too heavily damaged during the onslaught except for a burst over a rear brickwall knocking out some fieldpieces and resulting in a few casualties. Despite however the heavy bombardment, casualties in the fort were slight and the British ships eventually realized that they could not force the passage to attack Baltimore in coordination with the land force. After a last ditch night feint and barge attack during the heavy rain storm at the time led by Capt. Charles Napier around the fort up the Middle Branch of the river to the west which was split and misdirected partly in the storm, then turned back with heavy casualties by alert gunners at supporting western batteries Fort Covington and Battery Babcock, so the British called off the attack and sailed downriver to pick up their army which had retreated from the eastside of Baltimore. All the lights were extinguished in Baltimore the night of the attack, and the fort was bombarded for 25 hours. The only light was given off by the exploding shells over Fort McHenry, illuminating the flag that was still flying over the fort. The defence of the fort inspired the American lawyer Francis Scott Key to write "Defence of Fort M'Henry", a poem that was set to music as "The Star-Spangled Banner".
Great Lakes and Western Territories.
Invasions of Upper and Lower Canada, 1812.
American leaders assumed that Canada could be easily overrun. Former President Jefferson optimistically referred to the conquest of Canada as "a matter of marching". Many Loyalist Americans had migrated to Upper Canada after the Revolutionary War. There was also significant non-Loyalist American immigration to the area due to the offer of land grants to immigrants, and the U.S. assumed the latter would favor the American cause, but they did not. In prewar Upper Canada, General Prévost was in the unusual position of having to purchase many provisions for his troops from the American side. This peculiar trade persisted throughout the war in spite of an abortive attempt by the U.S. government to curtail it. In Lower Canada, which was much more populous, support for Britain came from the English elite with strong loyalty to the Empire, and from the Canadien elite, who feared American conquest would destroy the old order by introducing Protestantism, Anglicization, republican democracy, and commercial capitalism; and weakening the Catholic Church. The Canadien inhabitants feared the loss of a shrinking area of good lands to potential American immigrants.
In 1812–13, British military experience prevailed over inexperienced American commanders. Geography dictated that operations would take place in the west: principally around Lake Erie, near the Niagara River between Lake Erie and Lake Ontario, and near the Saint Lawrence River area and Lake Champlain. This was the focus of the three-pronged attacks by the Americans in 1812. Although cutting the St. Lawrence River through the capture of Montreal and Quebec would have made Britain's hold in North America unsustainable, the United States began operations first in the western frontier because of the general popularity there of a war with the British, who had sold arms to the Native Americans opposing the settlers.
The British scored an important early success when their detachment at St. Joseph Island, on Lake Huron, learned of the declaration of war before the nearby American garrison at the important trading post at Mackinac Island in Michigan. A scratch force landed on the island on July 17, 1812 and mounted a gun overlooking Fort Mackinac. After the British fired one shot from their gun, the Americans, taken by surprise, surrendered. This early victory encouraged the natives, and large numbers moved to help the British at Amherstburg. The island totally controlled access to the Old Northwest, giving the British nominal control of this area, and, more vitally, a monopoly on the fur trade.
An American army under the command of William Hull invaded Canada on July 12, with his forces chiefly composed of untrained and ill-disciplined militiamen. Once on Canadian soil, Hull issued a proclamation ordering all British subjects to surrender, or "the horrors, and calamities of war will stalk before you". He also threatened to kill any British prisoner caught fighting alongside a native. The proclamation helped stiffen resistance to the American attacks. Hull's army was too weak in artillery and badly supplied to achieve its objectives, and had to fight just to maintain its own lines of communication.
The senior British officer in Upper Canada, Major General Isaac Brock, felt that he should take bold measures to calm the settler population in Canada, and to convince the aboriginals who were needed to defend the region that Britain was strong. He moved rapidly to Amherstburg near the western end of Lake Erie with reinforcements and immediately decided to attack Detroit. Hull, fearing that the British possessed superior numbers and that the Indians attached to Brock's force would commit massacres if fighting began, surrendered Detroit without a fight on August 16. Knowing of British-instigated indigenous attacks on other locations, Hull ordered the evacuation of the inhabitants of Fort Dearborn (Chicago) to Fort Wayne. After initially being granted safe passage, the inhabitants (soldiers and civilians) were attacked by Potowatomis on August 15 after travelling only 2 mi in what is known as the Battle of Fort Dearborn. The fort was subsequently burned.
Brock promptly transferred himself to the eastern end of Lake Erie, where American General Stephen Van Rensselaer was attempting a second invasion. An armistice (arranged by Prévost in the hope the British renunciation of the Orders in Council to which the United States objected might lead to peace) prevented Brock from invading American territory. When the armistice ended, the Americans attempted an attack across the Niagara River on October 13, but suffered a crushing defeat at Queenston Heights. Brock was killed during the battle. While the professionalism of the American forces would improve by the war's end, British leadership suffered after Brock's death. A final attempt in 1812 by American General Henry Dearborn to advance north from Lake Champlain failed when his militia refused to advance beyond American territory.
In contrast to the American militia, the Canadian militia performed well. French Canadians, who found the anti-Catholic stance of most of the United States troublesome, and United Empire Loyalists, who had fought for the Crown during the American Revolutionary War, strongly opposed the American invasion. However, many in Upper Canada were recent settlers from the United States who had no obvious loyalties to the Crown. Nevertheless, while there were some who sympathized with the invaders, the American forces found strong opposition from men loyal to the Empire.
American Northwest, 1813.
After Hull's surrender of Detroit, General William Henry Harrison was given command of the U.S. Army of the Northwest. He set out to retake the city, which was now defended by Colonel Henry Procter in conjunction with Tecumseh. A detachment of Harrison's army was defeated at Frenchtown along the River Raisin on January 22, 1813. Procter left the prisoners with an inadequate guard, who could not prevent some of his North American aboriginal allies from attacking and killing perhaps as many as sixty Americans, many of whom were Kentucky militiamen. The incident became known as the River Raisin Massacre. The defeat ended Harrison's campaign against Detroit, and the phrase "Remember the River Raisin!" became a rallying cry for the Americans.
In May 1813, Procter and Tecumseh set siege to Fort Meigs in northwestern Ohio. American reinforcements arriving during the siege were defeated by the natives, but the fort held out. The Indians eventually began to disperse, forcing Procter and Tecumseh to return north to Canada. A second offensive against Fort Meigs also failed in July. In an attempt to improve Indian morale, Procter and Tecumseh attempted to storm Fort Stephenson, a small American post on the Sandusky River, only to be repulsed with serious losses, marking the end of the Ohio campaign.
On Lake Erie, American commander Captain Oliver Hazard Perry fought the Battle of Lake Erie on September 10, 1813. His decisive victory at "Put-In-Bay" ensured American military control of the lake, improved American morale after a series of defeats, and compelled the British to fall back from Detroit. This paved the way for General Harrison to launch another invasion of Upper Canada, which culminated in the U.S. victory at the Battle of the Thames on October 5, 1813, in which Tecumseh was killed. Tecumseh's death effectively ended the North American indigenous alliance with the British in the Fort Detroit region. American control of Lake Erie meant the British could no longer provide essential military supplies to their aboriginal allies, who therefore dropped out of the war. The Americans controlled the area during the conflict.
Niagara frontier, 1813.
Because of the difficulties of land communications, control of the Great Lakes and the St. Lawrence River corridor was crucial. When the war began, the British already had a small squadron of warships on Lake Ontario and had the initial advantage. To redress the situation, the Americans established a Navy yard at Sackett's Harbor in northwestern New York. Commodore Isaac Chauncey took charge of the large number of sailors and shipwrights sent there from New York; they completed the second warship built there in a mere 45 days. Ultimately, almost 3,000 men worked at the naval shipyard, building eleven warships and many smaller boats and transports. Having regained the advantage by their rapid building program, Chauncey and Dearborn attacked York, (future Toronto), on the northern shore of the lake, the capital of Upper Canada, on April 27, 1813. The Battle of York was a "pyrrhic" American victory, marred by looting and the burning of the small Provincial Parliament buildings and a library (resulting in a spirit of revenge by the British/Canadians led by Gov. George Prévost, who later demanded satisfaction encouraging the British Admiralty to issue orders to their officers later operating in the Chesapeake Bay region to exact similar devastation on the American Federal capital village of Washington the following year). However, Kingston was strategically much more valuable to British supply and communications routes along the St. Lawrence corridor. Without control of Kingston, the U.S. Navy could not effectively control Lake Ontario or sever the British supply line from Lower Canada.
On May 27, 1813, an American amphibious force from Lake Ontario assaulted Fort George on the northern end of the Niagara River and captured it without serious losses. The retreating British forces were not pursued, however, until they had largely escaped and organized a counteroffensive against the advancing Americans at the Battle of Stoney Creek on June 5. On June 24, with the help of advance warning by Laura Secord, another American force was forced to surrender by a much smaller British and native force at the Battle of Beaver Dams, marking the end of the American offensive into Upper Canada. Meanwhile, Commodore James Lucas Yeo had taken charge of the British ships on the lake and mounted a counterattack, which was nevertheless repulsed at the Battle of Sackett's Harbor. Thereafter, Chauncey and Yeo's squadrons fought two indecisive actions, neither commander seeking a fight to the finish.
Late in 1813, the Americans abandoned the Canadian territory they occupied around Fort George. They set fire to the village of Newark (now Niagara-on-the-Lake) on December 15, 1813, incensing the Canadians and politicians in control. Many of the inhabitants were left without shelter, freezing to death in the snow. This led to British retaliation following the Capture of Fort Niagara on December 18, 1813. Early the next morning on December 19, the British and their native allies stormed the neighboring town of Lewiston, New York, torching homes and buildings and killing about a dozen civilians. As the British were chasing the surviving residents out of town, a small force of Tuscarora natives intervened and stopped the pursuit, buying enough time for the locals to escape to safer ground. It is notable in that the Tuscaroras defended the Americans against their own Iroquois brothers, the Mohawks, who sided with the British. Later, the British attacked and burned Buffalo on December 30, 1813.
In 1814, the contest for Lake Ontario turned into a building race. Naval superiority shifted between the opposing fleets as each built new, bigger ships. However, neither was able to bring the other to battle when in a position of superiority. The Engagements on Lake Ontario were a draw.
St. Lawrence and Lower Canada, 1813.
The British were potentially most vulnerable over the stretch of the St. Lawrence where it formed the frontier between Upper Canada and the United States. During the early days of the war, there was illicit commerce across the river. Over the winter of 1812 and 1813, the Americans launched a series of raids from Ogdensburg on the American side of the river, which hampered British supply traffic up the river. On February 21, Sir George Prévost passed through Prescott on the opposite bank of the river with reinforcements for Upper Canada. When he left the next day, the reinforcements and local militia attacked. At the Battle of Ogdensburg, the Americans were forced to retire.
For the rest of the year, Ogdensburg had no American garrison, and many residents of Ogdensburg resumed visits and trade with Prescott. This British victory removed the last American regular troops from the Upper St. Lawrence frontier and helped secure British communications with Montreal. Late in 1813, after much argument, the Americans made two thrusts against Montreal. The plan eventually agreed upon was for Major General Wade Hampton to march north from Lake Champlain and join a force under General James Wilkinson that would embark in boats and sail from Sackett's Harbor on Lake Ontario and descend the St. Lawrence. Hampton was delayed by bad roads and supply problems and also had an intense dislike of Wilkinson, which limited his desire to support his plan. On October 25, his 4,000-strong force was defeated at the Chateauguay River by Charles de Salaberry's smaller force of Canadian Voltigeurs and Mohawks. Wilkinson's force of 8,000 set out on October 17, but was also delayed by bad weather. After learning that Hampton had been checked, Wilkinson heard that a British force under Captain William Mulcaster and Lieutenant Colonel Joseph Wanton Morrison was pursuing him, and by November 10, he was forced to land near Morrisburg, about 150 kilometres (90 mi.) from Montreal. On November 11, Wilkinson's rear guard, numbering 2,500, attacked Morrison's force of 800 at Crysler's Farm and was repulsed with heavy losses. After learning that Hampton could not renew his advance, Wilkinson retreated to the U.S. and settled into winter quarters. He resigned his command after a failed attack on a British outpost at Lacolle Mills.
Niagara and Plattsburgh Campaigns, 1814.
By the middle of 1814, American generals, including Major Generals Jacob Brown and Winfield Scott, had drastically improved the fighting abilities and discipline of the army. Their renewed attack on the Niagara peninsula quickly captured Fort Erie. Winfield Scott then gained a victory over an inferior British force at the Battle of Chippawa on July 5. An attempt to advance further ended with a hard-fought but inconclusive battle at Lundy's Lane on July 25.
The outnumbered Americans withdrew but withstood a prolonged Siege of Fort Erie. The British suffered heavy casualties in a failed assault and were weakened by exposure and shortage of supplies in their siege lines. Eventually the British raised the siege, but American Major General George Izard took over command on the Niagara front and followed up only halfheartedly. The Americans lacked provisions, and eventually destroyed the fort and retreated across the Niagara.
Meanwhile, following the abdication of Napoleon, 15,000 British troops were sent to North America under four of Wellington's ablest brigade commanders. Fewer than half were veterans of the Peninsula and the rest came from garrisons. Prévost was ordered to neutralize American power on the lakes by burning Sackets Harbor, gain naval control of Lake Erie, Lake Ontario and the Upper Lakes, and defend Lower Canada from attack. He did defend Lower Canada but otherwise failed to achieve his objectives. Given the late season he decided to invade New York State. His army outnumbered the American defenders of Plattsburgh, but he was worried about his flanks so he decided he needed naval control of Lake Champlain. On the lake, the British squadron under Captain George Downie and the Americans under Master Commandant Thomas Macdonough were more evenly matched.
On reaching Plattsburgh, Prévost delayed the assault until the arrival of Downie in the hastily completed 36-gun frigate HMS "Confiance". Prévost forced Downie into a premature attack, but then unaccountably failed to provide the promised military backing. Downie was killed and his naval force defeated at the naval Battle of Plattsburgh in Plattsburgh Bay on September 11, 1814. The Americans now had control of Lake Champlain; Theodore Roosevelt later termed it "the greatest naval battle of the war". The successful land defence was led by Alexander Macomb. To the astonishment of his senior officers, Prévost then turned back, saying it would be too hazardous to remain on enemy territory after the loss of naval supremacy. Prévost was recalled and in London, a naval court-martial decided that defeat had been caused principally by Prévost's urging the squadron into premature action and then failing to afford the promised support from the land forces. Prévost died suddenly, just before his own court-martial was to convene. Prévost's reputation sank to a new low, as Canadians claimed that their militia under Brock did the job and he failed. Recently, however, historians have been more kindly, measuring him not against Wellington but against his American foes. They judge Prévost's preparations for defending the Canadas with limited means to be energetic, well-conceived, and comprehensive; and against the odds, he had achieved the primary objective of preventing an American conquest.
To the east, the northern part of Massachusetts, soon to be Maine, was invaded. Fort Sullivan at Eastport was taken by Sir Thomas Hardy on July 11. Castine, Hampden, Bangor, and Machias were taken, and Castine became the main British base till April 15, 1815, when the British left, taking £10,750 in tariff duties, the "Castine Fund" which was used to found Dalhousie University. Eastport was not returned to the United States till 1818.
American West, 1813–14.
The Mississippi River valley was the western frontier of the United States in 1812. The territory acquired in the Louisiana Purchase of 1803 contained almost no U.S. settlements west of the Mississippi except around Saint Louis and a few forts and trading posts. Fort Bellefontaine, an old trading post converted to a U.S. Army post in 1804, served as regional headquarters. Fort Osage, built in 1808 along the Missouri was the western-most U.S. outpost, it was abandoned at the start of the war. Fort Madison, built along the Mississippi in what is now Iowa, was also built in 1808, and had been repeatedly attacked by British-allied Sauk since its construction. In September 1813 Fort Madison was abandoned after it was attacked and besieged by natives, who had support from the British. This was one of the few battles fought west of the Mississippi. Black Hawk played a leadership role.
Little of note took place on Lake Huron in 1813, but the American victory on Lake Erie and the recapture of Detroit isolated the British there. During the ensuing winter, a Canadian party under Lieutenant Colonel Robert McDouall established a new supply line from York to Nottawasaga Bay on Georgian Bay. When he arrived at Fort Mackinac with supplies and reinforcements, he sent an expedition to recapture the trading post of Prairie du Chien in the far west. The Siege of Prairie du Chien ended in a British victory on July 20, 1814.
Earlier in July, the Americans sent a force of five vessels from Detroit to recapture Mackinac. A mixed force of regulars and volunteers from the militia landed on the island on August 4. They did not attempt to achieve surprise, and at the brief Battle of Mackinac Island, they were ambushed by natives and forced to re-embark. The Americans discovered the new base at Nottawasaga Bay, and on August 13, they destroyed its fortifications and a schooner that they found there. They then returned to Detroit, leaving two gunboats to blockade Mackinac. On September 4, these gunboats were taken unawares and captured by British boarding parties from canoes and small boats. This Engagement on Lake Huron left Mackinac under British control.
The British garrison at Prairie du Chien also fought off another attack by Major Zachary Taylor. In this distant theatre, the British retained the upper hand until the end of the war, through the allegiance of several indigenous tribes that received British gifts and arms, enabling them to take control of parts of what is now Michigan and Illinois, as well as the whole of modern Wisconsin. In 1814 U.S. troops retreating from the Battle of Credit Island on the upper Mississippi attempted to make a stand at Fort Johnson, but the fort was soon abandoned, along with most of the upper Mississippi valley.
After the U.S. was pushed out of the Upper Mississippi region, they held on to eastern Missouri and the St. Louis area. Two notable battles fought against the Sauk were the Battle of Cote Sans Dessein, in April 1815, at the mouth of the Osage River in the Missouri Territory, and the Battle of the Sink Hole, in May 1815, near Fort Cap au Gris.
At the conclusion of peace, Mackinac and other captured territory was returned to the United States. At the end of the war, some British officers and Canadians objected to handing back Prairie du Chien and especially Mackinac under the terms of the Treaty of Ghent. However, the Americans retained the captured post at Fort Malden, near Amherstburg, until the British complied with the treaty.
Fighting between Americans, the Sauk, and other indigenous tribes continued through 1817, well after the war ended in the east.
Southern theatre.
Creek War.
The Battle of Burnt Corn between Red Stick Creeks and U.S. troops, occurred in the southern parts of Alabama on July 27, 1813 prompted the state of Georgia as well as the Mississippi territory militia to immediately take major action against Creek offensives. The Red Sticks chiefs gained power in the east along the Alabama, Coosa, and Tallapoosa Rivers – Upper Creek territory. The Lower Creek lived along the Chattahoochee River. Many Creeks tried to remain friendly to the United States, and some were organized by federal Indian Agent Benjamin Hawkins to aid the 6th Military District under General Thomas Pinckney and the state militias. The United States combined forces were large. At its peak the Red Stick faction had 4,000 warriors, only a quarter of whom had muskets.
Before 1813, the Creek War had been largely an internal affair sparked by the ideas of Tecumseh farther north in the Mississippi Valley, but the United States was drawn into a war with the Creek Nation by the War of 1812. The Creek Nation was a trading partner of the United States actively involved with Spanish and British trade as well. The Red Sticks, as well as many southern Muscogeean people like the Seminole, had a long history of alliance with the Spanish and British Empires. This alliance helped the North American and European powers protect each other's claims to territory in the south. On August 18, 1813, Red Stick chiefs planned an attack on Fort Mimms, north of Mobile, the only American-held port in the territory of West Florida. The attack on Fort Mimms resulted in the death of 400 settlers and became an ideological rallying point for the Americans.
The Indian frontier of western Georgia was the most vulnerable but was partially fortified already. From November 1813 to January 1814, Georgia's militia and auxiliary Federal troops - from the Creek and Cherokee Indian nations and the states of North Carolina and South Carolina – organized the fortification of defenses along the Chattahoochee River and expeditions into Upper Creek territory in present-day Alabama. The army, led by General John Floyd, went to the heart of the "Creek Holy Grounds" and won a major offensive against one of the largest Creek towns at Battle of Autosee, killing an estimated two hundred people. In November, the militia of Mississippi with a combined 1200 troops attacked the "Econachca" encampment ("Battle of Holy Ground") on the Alabama River. Tennessee raised a militia of 5,000 under Colonel Andrew Jackson and Major General Coke and won the battles of Tallushatchee and Talladega in November 1813.
The Georgia militia withdrew to the Chattahoochee, and Jackson's force in Tennessee mostly disbanded for the winter. In January Floyd's force of 1,300 state militia and 400 Creek Indians moved to join the U.S forces in Tennessee, but were attacked in camp on the Calibee Creek by Tuckaubatchee Indians on the 27th.
Despite enlistment problems in the winter, the U.S. Army forces and a second draft of Tennessee state militia and Cherokee and Creek allies swelled his army to around 5,000. In March 1814 they moved south to attack the Creek.
On March 26, Jackson and General John Coffee decisively defeated the Creek Indian force at Horseshoe Bend, killing 800 of 1,000 Creeks at a cost of 49 killed and 154 wounded out of approximately 2,000 American and Cherokee forces.
The American army moved to a fort on the Alabama River. On August 9, 1814, the Upper Creek chiefs and Major General Andrew Jackson's army signed the "Treaty of Fort Jackson". The most of western Georgia and part of Alabama was taken from the Creeks to pay for expenses borne by the United States. The Treaty also "demanded" that the "Red Stick" insurgents cease communicating with the Spanish or British, and only trade with U.S.-approved agents.
British aid to the Red Sticks arrived after the end of the Napoleonic Wars in April 1814 and after Admiral Sir Alexander Cochrane assumed command from Admiral Warren in March. The Creek promised to join any body of 'troops that should aid them in regaining their lands, and suggesting an attack on the tower off Mobile.' In April 1814 the British established an outpost on the Apalachicola River at Prospect Bluff (Fort Gadsden). Cochrane sent a company of Royal Marines, the vessels HMS Hermes and HMS Carron, commanded by Edward Nicolls, with further supplies to meet the Indians. In addition to training the Indians, Nicolls was tasked to raise a force from escaped slaves, as part of the Corps of Colonial Marines.
In July 1814, General Andrew Jackson complained to the Governor of Pensacola, Mateo Gonzalez Manrique, that combatants from the Creek War were being harbored in Spanish territory, and made reference to the British presence on Spanish soil. Although he gave an angry reply to Jackson, Manrique was alarmed at the weak position he found himself in. He appealed to the British for help, with Woodbine arriving on 28 July, and Nicolls arriving at Pensacola on 24 August.
The first engagement of the British and their Creek allies against the Americans on the Gulf Coast was the attack on Fort Bowyer September 14, 1814. Captain William Percy tried to take the U.S. fort, hoping that would enable the British to move on Mobile and block US trade and encroachment on the Mississippi. After the Americans repulsed Percy's forces, the British established a military presence of up to 200 Marines at Pensacola. In November, Jackson's force of 4,000 men took the town in November. This underlined the superiority of numbers of Jackson's force in the region. The U.S force moved to New Orleans in late 1814. Jackson's army of 1,000 regulars and 3,000 to 4,000 militia, pirates and other fighters, as well as civilians and slaves built fortifications south of the city.
Gulf Coast.
American forces under General James Wilkinson, who was himself getting $4,000 per year as a Spanish secret agent, took the Mobile area—formerly part of West Florida—from the Spanish in March 1813; this would be the only territory permanently gained by the U.S. during the war. The Americans built Fort Bowyer, a log and earthenwork fort with 14 guns, on Mobile Point.
At the end of 1814, the British launched a double offensive in the South weeks before the Treaty of Ghent was signed. On the Atlantic coast, Admiral George Cockburn was to close the Intracoastal Waterway trade and land Royal Marine battalions to advance through Georgia to the western territories. On the Gulf coast, Admiral Alexander Cochrane would move on the new state of Louisiana and the Mississippi Territory. Admiral Cochrane's ships reached the Louisiana coast December 9, and Cockburn arrived in Georgia December 14.
On January 8, 1815, a British force of 8,000 under General Edward Pakenham attacked Jackson's defenses in New Orleans. The Battle of New Orleans was an American victory, as the British failed to take the fortifications on the East Bank. The British suffered high casualties: 291 dead, 1262 wounded, and 484 captured or missing whereas American casualties were 13 dead, 39 wounded, and 19 missing. It was hailed as a great victory across the U.S., making Jackson a national hero and eventually propelling him to the presidency. The American garrison at Fort St. Philip endured ten days of bombardment from Royal Navy guns, which was a final attempt to invade Louisiana; British ships sailed away from the Mississippi River on January 18. However, it was not until January 27, 1815, that the army had completely rejoined the fleet, allowing for their departure.
After New Orleans, the British tried to take Mobile a second time; General John Lambert laid siege for five days and took the fort, winning the Second Battle of Fort Bowyer on February 12, 1815. HMS "Brazen" brought news of the Treaty of Ghent the next day, and the British abandoned the Gulf coast.
In January 1815, Admiral Cockburn succeeded in blockading the southeastern coast by occupying Camden County, Georgia. The British quickly took Cumberland Island, Fort Point Peter, and Fort St. Tammany in a decisive victory. Under the orders of his commanding officers, Cockburn's forces relocated many refugee slaves, capturing St. Simons Island as well, to do so. During the invasion of the Georgia coast, an estimated 1,485 people chose to relocate in British territories or join the military. In mid-March, several days after being informed of the Treaty of Ghent, British ships finally left the area.
Postwar fighting.
After peace was declared, Nicolls and his men returned to Prospect Bluff. The British post at Prospect Bluff was handed over to the Seminoles. In April 1815 the locally recruited companies of the Corps of Colonial Marines were disbanded. The greater part of the Royal Marine garrison at Apalachicola were embarked aboard HMS Cydnus on 22 April 1815, and Edward Nicolls embarked the brig HMS Forward at Amelia Island on 29 June 'for passage to England'. The legacy of the Negro Fort would subsequently lead to the first of the Seminole Wars.
In May 1815, a band of British-allied Sauk, unaware that the war had ended months before, attacked a small band of U.S. soldiers northwest of St. Louis. Intermittent fighting, primarily with the Sauk, continued in the Missouri Territory well into 1817, although it is unknown if the Sauk were acting on their own or on behalf of British agents. Several uncontacted isolated warships continued fighting well into 1815 and were the last American forces to take offensive action against the British.
The Treaty of Ghent.
Factors leading to the peace negotiations.
By 1814, both sides had either achieved their main war goals or were weary of a costly war that offered little but stalemate. They both sent delegations to a neutral site in Ghent, Flanders (now part of Belgium). The negotiations began in early August and concluded on December 24, when a final agreement was signed; both sides had to ratify it before it could take effect. Meanwhile both sides planned new invasions.
In 1814 the British began blockading the United States, and brought the American economy to near bankruptcy, forcing it to rely on loans for the rest of the war. American foreign trade was reduced to a trickle. The parlous American economy was thrown into chaos with prices soaring and unexpected shortages causing hardship in New England which was considering secession. But also to a lesser extent British interests were hurt in the West Indies and Canada that had depended on that trade. Although American privateers found chances of success much reduced, with most British merchantmen now sailing in convoy, privateering continued to prove troublesome to the British, as shown by high insurance rates. British landowners grew weary of high taxes, and colonial interests and merchants called on the government to reopen trade with the U.S. by ending the war.
Negotiations and peace.
At last in August 1814, peace discussions began in the neutral city of Ghent. Both sides began negotiations warily The British diplomats stated their case first, demanding the creation of an Indian barrier state in the American Northwest Territory (the area from Ohio to Wisconsin). It was understood the British would sponsor this Indian state. The British strategy for decades had been to create a buffer state to block American expansion. Britain demanded naval control of the Great Lakes and access to the Mississippi River. The Americans refused to consider a buffer state and the proposal was dropped. Although article IX of the treaty included provisions to restore to Natives "all possessions, rights and privileges which they may have enjoyed, or been entitled to in 1811", the provisions were unenforceable. The Americans (at a later stage) demanded damages for the burning of Washington and for the seizure of ships before the war began.
American public opinion was outraged when Madison published the demands; even the Federalists were now willing to fight on. The British had planned three invasions. One force burned Washington but failed to capture Baltimore, and sailed away when its commander was killed. In northern New York State, 10,000 British veterans were marching south until a decisive defeat at the Battle of Plattsburgh forced them back to Canada. Nothing was known of the fate of the third large invasion force aimed at capturing New Orleans and southwest. The Prime Minister wanted the Duke of Wellington to command in Canada and take control of the Great Lakes. Wellington said that he would go to America but he believed he was needed in Europe. He emphasized that the war was a draw and the peace negotiations should not make territorial demands:
I think you have no right, from the state of war, to demand any concession of territory from America ... You have not been able to carry it into the enemy's territory, notwithstanding your military success and now undoubted military superiority, and have not even cleared your own territory on the point of attack. You cannot on any principle of equality in negotiation claim a cessation of territory except in exchange for other advantages which you have in your power ... Then if this reasoning be true, why stipulate for the uti possidetis? You can get no territory: indeed, the state of your military operations, however creditable, does not entitle you to demand any. 
The Prime Minister, Lord Liverpool, aware of growing opposition to wartime taxation and the demands of Liverpool and Bristol merchants to reopen trade with America, realized Britain also had little to gain and much to lose from prolonged warfare especially after the growing concern about the situation in Europe. After months of negotiations, against the background of changing military victories, defeats and losses, the parties finally realized that their nations wanted peace and there was no real reason to continue the war. Now each side was tired of the war. Export trade was all but paralyzed and after Napoleon fell in 1814 France was no longer an enemy of Britain, so the Royal Navy no longer needed to stop American shipments to France, and it no longer needed to impress more seamen. It had ended the practices that so angered the Americans in 1812. The British were preoccupied in rebuilding Europe after the apparent final defeat of Napoleon.
British negotiators were urged by Lord Liverpool to offer a status quo and dropped their demands for the creation of an Indian barrier state, which was in any case hopeless after the collapse of Tecumseh's alliance. This allowed negotiations to resume at the end of October. British diplomats soon offered the status quo to the US negotiators, who accepted them. Prisoners would be exchanged, and captured slaves returned to the United States or be paid for by Britain.
On December 24, 1814 the diplomats had finished and signed the Treaty of Ghent. The treaty was ratified by the British three days later on December 27 and arrived in Washington on February 17 where it was quickly ratified and went into effect, thus finally ending the war. The terms called for all occupied territory to be returned, the prewar boundary between Canada and the United States to be restored, and the Americans were to gain fishing rights in the Gulf of Saint Lawrence.
The Treaty of Ghent failed to secure official British acknowledgment of American maritime rights or ending impressment. However, in the century of peace until World War I these rights were not seriously violated. The defeat of Napoleon made irrelevant all of the naval issues over which the United States had fought. The Americans had achieved their goal of ending the Indian threat; furthermore the American armies had scored enough victories (especially at New Orleans) to satisfy honour and the sense of becoming fully independent from Britain.
Losses and compensation.
British losses in the war were about 1,600 killed in action and 3,679 wounded; 3,321 British died from disease. American losses were 2,260 killed in action and 4,505 wounded. While the number of Americans who died from disease is not known, it is estimated that about 15,000 died from all causes directly related to the war. These figures do not include deaths among Canadian militia forces or losses among native tribes.
There have been no estimates of the cost of the American war to Britain, but it did add some £25 million to the national debt. In the U.S., the cost was $105 million, about the same as the cost to Britain. The national debt rose from $45 million in 1812 to $127 million by the end of 1815, although by selling bonds and treasury notes at deep discounts—and often for irredeemable paper money due to the suspension of specie payment in 1814—the government received only $34 million worth of specie.
In addition, at least 3,000 American slaves escaped to the British lines. Many other slaves simply escaped in the chaos of war and achieved their freedom on their own. The British settled some of the newly freed slaves in Nova Scotia. Four hundred freedmen were settled in New Brunswick. The Americans protested that Britain's failure to return the slaves violated the Treaty of Ghent. After arbitration by the Tsar of Russia the British paid $1,204,960 in damages to Washington, which reimbursed the slaveowners.
Memory and historiography.
Popular views.
During the 19th century the popular image of the war in the United States was of an American victory, and in Canada, of a Canadian victory. Each young country saw its self-perceived victory as an important foundation of its growing nationhood. The British, on the other hand, who had been preoccupied by Napoleon's challenge in Europe, paid little attention to what was to them a peripheral and secondary dispute, a distraction from the principal task at hand.
Canadian.
In British North America (which would become the Dominion of Canada in 1867), the War of 1812 was seen by Loyalists as a victory, as they had successfully defended their borders from an American takeover. The outcome gave Empire-oriented Canadians confidence and, together with the postwar "militia myth" that the civilian militia had been primarily responsible rather than the British regulars, was used to stimulate a new sense of Canadian nationalism. John Strachan, the first Anglican bishop of Toronto, created the myth, telling his flock that Upper Canada had been saved from the dangerous republicanism of the American invaders by the heroism of the local citizenry.
A long-term implication of the militia myth—which was false, but remained popular in the Canadian public at least until the First World War—was that Canada did not need a regular professional army. The U.S. Army had done poorly, on the whole, in several attempts to invade Canada, and the Canadians had shown that they would fight bravely to defend their country. But the British did not doubt that the thinly populated territory would be vulnerable in a third war. "We cannot keep Canada if the Americans declare war against us again", Admiral Sir David Milne wrote to a correspondent in 1817, although the Rideau Canal was built for just such a scenario.
By the 21st century it was a forgotten war in Britain, although still remembered in Canada, especially Ontario. In a 2009 poll, 37% of Canadians said the war was a Canadian victory, 9% said the U.S. won, 15% called it a draw, and 39%—mainly younger Canadians—said they knew too little to comment.
A February 2012 poll found that in a list of items that could be used to define Canadians' identity, the fact that Canada successfully repelled an American invasion in the War of 1812 places second (25%), only behind the fact that Canada has universal health care (53%). The survey states that 77% of Canadians believe that War of 1812 Bicentennial is an important commemoration.
American.
Today, American popular memory includes the British capture and the burning of Washington in August 1814, which necessitated its extensive renovation. Another memory is the successful American defence of Fort McHenry in September 1814, which inspired the lyrics of the U.S. national anthem, "The Star-Spangled Banner". The successful Captains of the U.S. Navy became popular heroes with plates with the likeness of Decatur, Steward, Hull, and others, becoming popular items. Ironically, many were made in England. The Navy became a cherished institution, lauded for the victories that it won against all odds. After engagements during the final actions of the war, U.S. Marines had acquired a well-deserved reputation as excellent marksmen, especially in ship-to-ship actions.
Historians' views.
Historians have differing and complex interpretations of the war. They agree that ending the war with neither side gaining or losing territory allowed for the peaceful settlement of boundary disputes and for the opening of a permanent era of good will and friendly relations between the U.S. and Canada. The war established distinct national identities for Canada and the United States, with a "newly significant border".
In recent decades the view of the majority of historians has been that the war ended in stalemate, with the Treaty of Ghent closing a war that had become militarily inconclusive. Neither side wanted to continue fighting since the main causes had disappeared and since there were no large lost territories for one side or the other to reclaim by force. Insofar as they see the war's untriumphant resolution as allowing two centuries of peaceful and mutually beneficial intercourse between the U.S., Britain and Canada, these historians often conclude that all three nations were the "real winners" of the War of 1812. These writers often add that the war could have been avoided in the first place by better diplomacy. It is seen as a mistake for everyone concerned because it was badly planned and marked by multiple fiascoes and failures on both sides, as shown especially by the repeated American failure to seize parts of Canada, and the failed British attack on New Orleans and upstate New York.
However, other scholars hold that the war constituted a British victory and an American defeat. They argue that the British achieved their military objectives in 1812 (by stopping the repeated American invasions of Canada) and that Canada retained her independence of the United States. By contrast, they say, the Americans suffered a defeat when their armies failed to achieve their war goal of seizing part or all of Canada. Additionally, they argue the U.S. lost as it failed to stop impressment, which the British refused to repeal until the end of the Napoleonic Wars, and the U.S. actions had no effect on the orders in council, which were rescinded before the war started.
A second minority view is that both the U.S. and Britain won the war—that is, both achieved their main objectives, while the Indians were the losing party. The British won by losing no territories and achieving their great war goal, the total defeat of Napoleon. U.S. won by (1) securing her honor and successfully resisting a powerful empire once again, thus winning a "second war of independence"; and (2) ending the threat of Indian raids and the British plan for a semi-independent Indian sanctuary—thereby opening an unimpeded path for the United States' westward expansion.
Indians as losers.
Historians generally agree that the real losers of the War of 1812 were the Indians (also called "First Nations" in Canada). Hickey says:
The big losers in the war were the Indians. As a proportion of their population, they had suffered the heaviest casualties. Worse, they were left without any reliable European allies in North America... The crushing defeats at the Thames and Horseshoe Bend left them at the mercy of the Americans, hastening their confinement to reservations and the decline of their traditional way of life.
American settlers into the Middle West had been repeatedly blocked and threatened by Indian raids before 1812, and that now came to an end. Throughout the war the British had played on terror of the tomahawks and scalping knives of their Indian allies; it worked especially at Hull's surrender at Detroit. By 1813 Americans had killed Tecumseh and broken his coalition of tribes. Jackson then defeated the enemy Indians in the Southwest. Historian John Sugden notes that in both theatres, the Indians' strength had been broken prior to the arrival of the major British forces in 1814.
Notwithstanding the sympathy and support from commanders (such as Brock, Cochrane and Nicolls), the policymakers in London reneged in assisting the Indians, as making peace was a higher priority for the politicians. At the peace conference the British demanded an independent Indian state in the Midwest, but by late 1814 the Indian confederation had disintegrated and the British had to abandon the demand. The withdrawal of British protection gave the Americans a free hand, which resulted in the removal of most of the tribes to Indian Territory (present-day Oklahoma). In that sense according to historian Alan Taylor, the final victory at New Orleans had "enduring and massive consequences". It gave the Americans "continental predominence" while it left the Indians dispossessed, powerless, and vulnerable.
The Creek War came to an end, with the Treaty of Fort Jackson being imposed upon the Indians. About half of the Creek territory was ceded to the United States, with no payment made to the Creeks. This was, in theory, invalidated by Article 9 of the Treaty of Ghent, thereby restoring to the Indians "all the possessions, rights and privileges which they may have enjoyed or been entitled to in 1811." The British failed to uphold this, and did not take up the Indian cause as an infringement of an international treaty. Without this support, the Indians' lack of power was apparent and the stage was set for further incursions of territory by the United States in subsequent decades.
Long-term consequences.
Neither side lost territory in the war, nor did the treaty that ended it address the original points of contention—and yet it changed much between the United States of America and Britain.
The Rush–Bagot Treaty was a treaty between the United States and Britain enacted in 1817 that provided for the demilitarization of the Great Lakes and Lake Champlain, where many British naval arrangements and forts still remained. The treaty laid the basis for a demilitarized boundary and was indicative of improving relations between the United States and Great Britain in the period following the War of 1812. It remains in effect to this day.
The Treaty of Ghent established the "status quo ante bellum"; that is, there were no territorial losses by either side. The issue of impressment was made moot when the Royal Navy, no longer needing sailors, stopped impressment after the defeat of Napoleon. Except for occasional border disputes and the circumstances of the American Civil War, relations between the U.S. and Britain remained generally peaceful for the rest of the 19th century, and the two countries became close allies in the 20th century.
Border adjustments between the U.S. and British North America were made in the Treaty of 1818. Eastport, Massachusetts, was returned to the U.S. in 1818; it would become part of the new State of Maine in 1820. A border dispute along the Maine–New Brunswick border was settled by the 1842 Webster–Ashburton Treaty after the bloodless Aroostook War, and the border in the Oregon Country was settled by splitting the disputed area in half by the 1846 Oregon Treaty. A further dispute about the line of the border through the island in the Strait of Juan de Fuca resulted in another almost bloodless standoff in the Pig War of 1859. The line of the border was finally settled by an international arbitration commission in 1872.
United States.
The U.S. suppressed the native American resistance on its western and southern borders. The nation also gained a psychological sense of complete independence as people celebrated their "second war of independence". Nationalism soared after the victory at the Battle of New Orleans. The opposition Federalist Party collapsed, and the Era of Good Feelings ensued.
No longer questioning the need for a strong Navy, the U.S. built three new 74-gun ships of the line and two new 44-gun frigates shortly after the end of the war. (Another frigate had been destroyed to prevent it being captured on the stocks.) In 1816, the U.S. Congress passed into law an "Act for the gradual increase of the Navy" at a cost of $1,000,000 a year for eight years, authorizing 9 ships of the line and 12 heavy frigates. The Captains and Commodores of the U.S. Navy became the heroes of their generation in the U.S. Decorated plates and pitchers of Decatur, Hull, Bainbridge, Lawrence, Perry, and Macdonough were made in Staffordshire, England, and found a ready market in the United States. Three of the war heroes used their celebrity to win national office: Andrew Jackson (elected President in 1828 and 1832), Richard Mentor Johnson (elected Vice President in 1836), and William Henry Harrison (elected President in 1840).
New England states became increasingly frustrated over how the war was being conducted and how the conflict was affecting them. They complained that the U.S. government was not investing enough in the states' defences militarily and financially, and that the states should have more control over their militia. The increased taxes, the British blockade, and the occupation of some of New England by enemy forces also agitated public opinion in the states. As a result, at the Hartford Convention (December 1814 – January 1815) Federalist delegates deprecated the war effort and sought more autonomy for the New England states. They did not call for secession but word of the angry anti-war resolutions appeared at the same time that peace was announced and the victory at New Orleans was known. The upshot was that the Federalists were permanently discredited and quickly disappeared as a major political force.
This war enabled thousands of slaves to escape to British lines or ships for freedom, despite the difficulties. The planters' complacency about slave contentment was shocked by their seeing slaves who would risk so much to be free.
In 1815, with the British gone, most of the Indian tribes of the Midwest made peace with the United States. In the next 15 years they signed a series of treaties selling approximately half of Michigan, half of Indiana, and two thirds of Illinois to the US government, which set up a process for selling the land to white farmers. Pratt concludes," the war had given the Northwest what it most desired." After the decisive defeat of the Creek Indians at the battle of Horseshoe Bend in 1814, some warriors escaped to join the Seminoles in Florida. The remaining Creek chiefs signed away about half their lands, comprising 23,000,000 acres, covering much of southern Georgia and two thirds of modern Alabama. The Creeks were now separated from any future help from the Spanish in Florida, or from the Choctaw and Chickasaw to the west. During the war the United States seized Mobile, Alabama, which was a strategic location providing oceanic outlet to the cotton lands to the north. Gen. Jackson invaded Florida in 1818, demonstrating to Spain that could no longer control that territory with a small force. Spain sold Florida to the United States in 1819. Pratt concludes:
Thus indirectly the War of 1812 brought about the acquisition of Florida... To both the Northwest and the South, therefore, the War of 1812 brought substantial benefits. It broke the power of the Creek Confederacy and opened to settlement a great province of the future Cotton Kingdom.
British North America (Canada).
Pro-British leaders demonstrated a strong hostility to republicanism and American influences in western Canada (Ontario) after the war and shaped its undemocratic policies. Immigration from the U.S. was discouraged, and favor was shown to the Anglican church as opposed to the more Americanized Methodist church.
The Battle of York showed the vulnerability of Upper and Lower Canada. In the 1820s, work began on La Citadelle at Quebec City as a defence against the United States. Additionally, work began on the Halifax citadel to defend the port against foreign navies. From 1826 to 1832, the Rideau Canal was built to provide a secure waterway not at risk from American cannon fire. To defend the western end of the canal, the British Army also built Fort Henry at Kingston.
Indigenous nations.
The Native Americans allied to the British lost their cause. The British proposal to create a "neutral" Indian zone in the American West was rejected at the Ghent peace conference and never resurfaced. After 1814 the natives, who lost most of their fur-gathering territory, became an undesirable burden to British policymakers who now looked to the United States for markets and raw materials. British agents in the field continued to meet regularly with their former American Indian partners, but they did not supply arms or encouragement and there were no American Indian campaigns to stop U.S. expansionism in the Midwest. Abandoned by their powerful sponsor, American Great Lakes-area Indians ultimately migrated or reached accommodations with the American authorities and settlers.
In the Southeast, Indian resistance had been crushed by General Andrew Jackson during the Creek War; as President (1829–37), Jackson systematically expelled the major tribes to reservations west of the Mississippi., part of which was the forced expulsion of American-allied Cherokee in the trail of tears.
Bermuda.
Bermuda had been largely left to the defences of its own militia and privateers before U.S. independence, but the Royal Navy had begun buying up land and operating from there in 1795, as its location was a useful substitute for the lost U.S. ports. It originally was intended to be the winter headquarters of the North American Squadron, but the war saw it rise to a new prominence. As construction work progressed through the first half of the 19th century, Bermuda became the permanent naval headquarters in Western waters, housing the Admiralty and serving as a base and dockyard. The military garrison was built up to protect the naval establishment, heavily fortifying the archipelago that came to be described as the "Gibraltar of the West". Defence infrastructure would remain the central leg of Bermuda's economy until after World War II.
Britain.
The war is seldom remembered in Great Britain. The massive ongoing conflict in Europe against the French Empire under Napoleon ensured that the War of 1812 against America was never seen as more than a sideshow to the main event by the British. Britain's blockade of French trade had been entirely successful and the Royal Navy was the world's dominant nautical power (and would remain so for another century). While the land campaigns had contributed to saving Canada, the Royal Navy had shut down American commerce, bottled up the U.S. Navy in port and heavily suppressed privateering. British businesses, some affected by rising insurance costs, were demanding peace so that trade could resume with the U.S. The peace was generally welcomed by the British, though there was disquiet at the rapid growth of the U.S. However, the two nations quickly resumed trade after the end of the war and, over time, a growing friendship.
Hickey argues that for Britain:
the most important lesson of all [was] that the best way to defend Canada was to accommodate the United States. This was the principal rationale for Britain's long-term policy of rapprochement with the United States in the nineteenth century and explains why they were so often willing to sacrifice other imperial interests to keep the republic happy.
Sources.
</dl>
Further reading.
Primary sources.
</dl>

</doc>
<doc id="34061" url="http://en.wikipedia.org/wiki?curid=34061" title="Winter">
Winter

Winter () is the coldest season of the year in polar climates and temperate climates, between autumn and spring. Winter is caused by the axis of the Earth in that hemisphere being oriented away from the Sun. Different cultures define different dates as the start of winter, and some use a definition based on weather. When it is winter in the Northern Hemisphere it is summer in the Southern Hemisphere, and vice versa. In many regions, winter is associated with snow and freezing temperatures. The moment of winter solstice is when the sun's elevation with respect to the North or South Pole is at its most negative value (that is, the sun is at its farthest below the horizon as measured from the pole). The earliest sunset and latest sunrise dates outside the polar regions differ from the date of the winter solstice, however, and these depend on latitude, due to the variation in the solar day throughout the year caused by the Earth's elliptical orbit (see earliest and latest sunrise and sunset).
Etymology.
The English word "winter" comes from the Proto-Indo-European word "Wend", that stood for water. 
Cause.
The tilt of the Earth's axis relative to its orbital plane plays a big role in the weather. The Earth is tilted at an angle of 23.44° to the plane of its orbit, and this causes different latitudes on the Earth to directly face the Sun as the Earth moves through its orbit. It is this variation that primarily brings about the seasons. When it is winter in the Northern Hemisphere, the Southern Hemisphere faces the Sun more directly and thus experiences warmer temperatures than the Northern Hemisphere. Conversely, winter in the Southern Hemisphere occurs when the Northern Hemisphere is tilted more toward the Sun. From the perspective of an observer on the Earth, the winter Sun has a lower maximum altitude in the sky than the summer Sun.
During winter in either hemisphere, the lower altitude of the Sun causes the sunlight to hit that hemisphere at an oblique angle. In regions experiencing winter, the same amount of solar radiation is spread out over a larger area. This effect is compounded by the larger distance that the light must travel through the atmosphere, allowing the atmosphere to dissipate more heat. Compared with these effects, the changes in the distance of the earth from the sun are negligible.
Meteorological reckoning.
Meteorological winter is the method of measuring the winter season used by meteorologists based on "sensible weather patterns" for record keeping purposes, so the start of meteorological winter varies with latitude. Winter is often defined by meteorologists to be the three calendar months with the lowest average temperatures. This corresponds to the months of December, January and February in the Northern Hemisphere, and June, July and August in the Southern Hemisphere. The coldest average temperatures of the season are typically experienced in January or February in the Northern Hemisphere and in June, July or August in the Southern Hemisphere. Nighttime predominates in the winter season, and in some regions winter has the highest rate of precipitation as well as prolonged dampness because of permanent snow cover or high precipitation rates coupled with low temperatures, precluding evaporation. Blizzards often develop and cause many transportation delays. Diamond dust, also known as ice needles or ice crystals, forms at temperatures approaching -40 F due to air with slightly higher moisture from aloft mixing with colder, surface based air. They are made of simple ice crystals that are hexagonal in shape.
Accumulations of snow and ice are commonly associated with winter in the Northern Hemisphere, due to the large land masses there. In the Southern Hemisphere, the more maritime climate and the relative lack of land south of 40°S makes the winters milder; thus, snow and ice are less common in inhabited regions of the Southern Hemisphere. In this region, snow occurs every year in elevated regions such as the Andes, the Great Dividing Range in Australia, and the mountains of New Zealand, and also occurs in the southerly Patagonia region of South America. Snow occurs year-round in Antarctica.
Astronomical and other calendar-based reckoning.
In the Northern Hemisphere, some authorities define the period of winter based on astronomical fixed points (i.e. based solely on the position of the Earth in its orbit around the sun), regardless of weather conditions. In one version of this definition, winter begins at the winter solstice and ends at the vernal equinox.
These dates are somewhat later than those used to define the beginning and end of the meteorological winter – usually considered to span the entirety of December, January, and February in the Northern Hemisphere and June, July, and August in the Southern.
Astronomically, the winter solstice, being the day of the year which has fewest hours of daylight, ought to be the middle of the season, but seasonal lag means that the coldest period normally follows the solstice by a few weeks. In some cultures, the season is regarded as beginning at the solstice and ending on the following equinox – in the Northern Hemisphere, depending on the year, this corresponds to the period between 21 or 22 December and 19, 20 or 21 March.
In the UK, meteorologists consider winter to be the three coldest months of December, January and February.
In Scandinavia, winter in one tradition begins on 14 October and ends on the last day of February. In Russia currently calendar winter starts on 1 December and lasts through to the end of February, though traditionally it was reckoned from the Christmas (25 December in Julian calendar, or 7 January in Gregorian) until the Annunciation (25 March in Julian). In many countries in the Southern Hemisphere, including Australia, New Zealand and South Africa, winter begins on 1 June and ends on 31 August. In Celtic nations such as Ireland (using the Irish calendar) and in Scandinavia, the winter solstice is traditionally considered as midwinter, with the winter season beginning 1 November, on All Hallows, or Samhain. Winter ends and spring begins on Imbolc, or Candlemas, which is 1 or 2 February . This system of seasons is based on the length of days exclusively. (The three-month period of the shortest days and weakest solar radiation occurs during November, December and January in the Northern Hemisphere and May, June and July in the Southern Hemisphere.)
Also, many mainland European countries tend to recognize Martinmas or St. Martin's Day (11 November), as the first calendar day of winter. The day falls at midpoint between the old Julian equinox and solstice dates. Also, Valentine's Day (14 February) is recognized by some countries as heralding the first rites of spring, such as flowers blooming.
In Chinese astronomy and other East Asian calendars, winter is taken to commence on or around 7 November, with the "Jiéqì" (known as 立冬 "lì dōng"—literally, "establishment of winter").
The three-month period associated with the coldest average temperatures typically begins somewhere in late November or early December in the Northern Hemisphere and lasts through late February or early March. This "thermological winter" is earlier than the solstice delimited definition, but later than the daylight (Celtic) definition. Depending on seasonal lag, this period will vary between climatic regions.
Cultural influences such as Christmas creep may have led to the winter season being perceived as beginning earlier in recent years, although high latitude countries like Canada are usually well into their real winters before the December solstice.
Ecological reckoning and activity.
Ecological reckoning of winter differs from calendar-based by avoiding the use of fixed dates. It is one of six seasons recognized by most ecologists who customarily use the term "hibernal" for this period of the year (the other ecological seasons being prevernal, vernal, estival, serotinal, and autumnal). The hibernal season coincides with the main period of biological dormancy each year whose dates vary according to local and regional climates in temperate zones of the Earth. The appearance of flowering plants like the crocus can mark the change from ecological winter to the prevernal season as early as late January in mild temperate climates.
To survive the harshness of winter, many animals have developed different behavioral and morphological adaptations for overwintering:
Some annual plants never survive the winter. Other annual plants require winter cold to complete their life cycle, this is known as vernalization. As for perennials, many small ones profit from the insulating effects of snow by being buried in it. Larger plants, particularly deciduous trees, usually let their upper part go dormant, but their roots are still protected by the snow layer. Few plants bloom in the winter, one exception being the flowering plum, which flowers in time for Chinese New Year. The process by which plants become acclimated to cold weather is called hardening.
Humans and winter.
Humans evolved in tropical climates, and met cold weather as they migrated into Eurasia, although earlier populations certainly encountered Southern Hemisphere winters in Southern Africa. Micro-evolution in Caucasian, Asiatic and Inuit people show some adaptation to the climate.
Winter and human health.
Humans are sensitive to cold, see hypothermia. Snowblindness, norovirus, seasonal depression, slipping on black ice and falling icicles are other health concerns associated with cold and snowy weather. In the Northern Hemisphere, it is not unusual for homeless people to die from hypothermia in the winter.
One of the most common diseases associated with winter is Influenza. Symptoms include: Headache, Fever, Muscle pains, sinus infection, fatigue, dizziness, cough, loss of appetite.
Mythology.
In various cultures.
In Persian culture the winter solstice is called Yaldā (meaning: birth) and it has been celebrated for thousands of years. It is referred to as the eve of the birth of Mithra, who symbolised light, goodness and strength on earth.
In Greek mythology, Hades kidnapped Persephone to be his wife. Zeus ordered Hades to return her to Demeter, the goddess of the Earth and her mother. However, Hades tricked Persephone into eating the food of the dead, so Zeus decreed that Persephone would spend six months with Demeter and six months with Hades. During the time her daughter is with Hades, Demeter became depressed and caused winter.
In Welsh mythology, Gwyn ap Nudd abducted a maiden named Creiddylad. On May Day, her lover, Gwythr ap Greidawl, fought Gwyn to win her back. The battle between them represented the contest between summer and winter.
Personifications.
In Bengali the advent of winter is often expressed by the sentence "Sheeter buri ashchhe dheye"" which means "the winter old woman is coming fast". This is used especially when it is said to a child.
External links.
 at Wikimedia Commons

</doc>
<doc id="34062" url="http://en.wikipedia.org/wiki?curid=34062" title="WAV">
WAV

Waveform Audio File Format (WAVE, or more commonly known as WAV due to its filename extension) (rarely, "Audio for Windows") is a Microsoft and IBM audio file format standard for storing an audio bitstream on PCs. It is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in "chunks", and thus is also close to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively. It is the main format used on Windows systems for raw and typically uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.
Description.
Both WAVs and AIFFs are compatible with Windows, Macintosh, and Linux operating systems. The format takes into account some differences of the Intel CPU such as little-endian byte order. The RIFF format acts as a "wrapper" for various audio coding formats.
Though a WAV file can contain compressed audio, the most common WAV audio format is uncompressed audio in the linear pulse code modulation (LPCM) format. LPCM is also the standard audio coding format for audio CDs, which store two-channel LPCM audio sampled 44,100 times per second with 16 bits per sample. Since LPCM is uncompressed and retains all of the samples of an audio track, professional users or audio experts may use the WAV format with LPCM audio for maximum audio quality. WAV files can also be edited and manipulated with relative ease using software.
The WAV format supports compressed audio, using, on Windows, the Audio Compression Manager. Any ACM codec can be used to compress a WAV file. The user interface (UI) for Audio Compression Manager may be accessed through various programs that use it, including Sound Recorder in some versions of Windows.
Beginning with Windows 2000, a codice_1 header was defined which specifies multiple audio channel data along with speaker positions, eliminates ambiguity regarding sample types and container sizes in the standard WAV format and supports defining custom extensions to the format chunk.
There are some inconsistencies in the WAV format: for example, 8-bit data is unsigned while 16-bit data is signed, and many chunks duplicate information found in other chunks.
Specification.
The WAV file is an instance of a Resource Interchange File Format (RIFF) defined by IBM and Microsoft.
RIFF.
A RIFF file is a tagged file format. It has a specific container format (a chunk) that includes a four character tag (FOURCC) and the size (number of bytes) of the chunk. The tag specifies how the data within the chunk should be interpreted, and there are several standard FOURCC tags. Tags consisting of all capital letters are reserved tags. The outermost chunk of a RIFF file has a codice_2 form tag; the first four bytes of chunk data are a FOURCC that specify the form type and are followed by a sequence of subchunks. In the case of a WAV file, those four bytes are the FOURCC codice_3. The remainder of the RIFF data is a sequence of chunks describing the audio information.
The advantage of a tagged file format is that the format can be extended later without confusing existing file readers. The rule for a RIFF (or WAV) reader is that it should ignore any tagged chunk that it does not recognize. The reader won't be able to use the new information, but the reader should not be confused.
The specification for RIFF files includes the definition of an codice_4 chunk. The chunk may include information such as the title of the work, the author, the creation date, and copyright information. Although the codice_4 chunk was defined in version 1.0, the chunk was not referenced in the formal specification of a WAV file. If the chunk were present in the file, then a reader should know how to interpret it, but many readers had trouble. Some readers would abort when they encountered the chunk, some readers would process the chunk if it were the first chunk in the RIFF form, and other readers would process it if it followed all of the expected waveform data. Consequently, the safest thing to do from an interchange standpoint was to omit the codice_4 chunk and other extensions and send a lowest-common-denominator file. There are other INFO chunk placement problems.
RIFF files were expected to be used in international environments, so there is codice_7 chunk to specify the country code, language, dialect, and code page for the strings in a RIFF file. For example, specifying an appropriate codice_7 chunk should allow the strings in an codice_4 chunk (and other chunks throughout the RIFF file) to be interpreted as Cyrillic or Japanese characters.
RIFF also defines a codice_10 chunk whose contents are uninteresting. The chunk allows a chunk to be deleted by just changing its FOURCC. The chunk could also be used to reserve some space for future edits so the file could be modified without being rewritten. A later definition of RIFF introduced a similar codice_11 chunk.
RIFF WAVE.
The toplevel definition of a WAV file is:
The definition shows a toplevel RIFF form with the codice_3 tag. It is followed by a mandatory codice_13 format chunk that describes the format of the sample data that follows. The format chunk includes information such as the sample encoding, number of bits per channel, the number of channels, the sample rate. The WAV specification includes some optional features. The optional fact chunk reports the number of samples for some compressed coding schemes. The cue point (codice_14) chunk identifies some significant sample numbers in the wave file. The playlist chunk allows the samples to be played out of order or repeated rather than just from beginning to end. The associated data list allows labels and notes (codice_15 and codice_16) to be attached to cue points; text annotation (codice_17) may be given for a group of samples (e.g., caption information). Finally, the mandatory wave data chunk contains the actual samples (in the specified format).
Note that the WAV file definition does not show where an codice_4 chunk should be placed. It is also silent about the placement of a codice_7 chunk (which specifies the character set used).
The RIFF specification attempts to be a formal specification, but its formalism lacks the precision seen in other tagged formats. For example, the RIFF specification does not clearly distinguish between a set of subchunks and an ordered sequence of subchunks. The RIFF form chunk suggests it should be a sequence container. The specification suggests a LIST chunk is also a sequence: "A LIST chunk contains a list, or ordered sequence, of subchunks." However, the specification does not give a formal specification of the codice_4 chunk; an example codice_4 LIST chunk ignores the chunk sequence implied in the codice_4 description. The LIST chunk definition for codice_23 does use the LIST chunk as a sequence container with good formal semantics.
The WAV specification allows for not only a single, contiguous, array of audio samples, but also discrete blocks of samples and silence that are played in order. Most WAV files use a single array of data. The specification for the sample data is confused:
These productions are confused. Apparently codice_24 (undefined) and codice_25 (defined but not referenced) should be identical. Even if that problem is fixed, the productions then allow a codice_26 to contain a recursive codice_23 (which implies data interpretation problems). The specification should have been something like:
to avoid the recursion.
WAV files can contain embedded IFF "lists", which can contain several "sub-chunks".
Metadata.
As a derivative of RIFF, WAV files can be tagged with metadata in the INFO chunk. In addition, WAV files can embed any kind of metadata, including but not limited to Extensible Metadata Platform (XMP) data or ID3 tags in extra chunks. Applications may not handle this extra information or may expect to see it in a particular place. Although the RIFF specification requires that applications ignore chunks they do not recognize, some applications are confused by additional chunks.
Popularity.
Uncompressed WAV files are large, so file sharing of WAV files over the Internet is uncommon. However, it is a commonly used file type, suitable for retaining first generation archived files of high quality, for use on a system where disk space is not a constraint, or in applications such as audio editing, where the time involved in compressing and uncompressing data is a concern.
More frequently, the smaller file sizes of compressed but lossy formats such as MP3 are used to store and transfer audio. Their small file sizes allow faster Internet transmission, as well as lower consumption of space on memory media. There are also lossless-compression formats such as FLAC.
The usage of the WAV format has more to do with its familiarity and simple structure. Because of this, it continues to enjoy widespread use with a variety of software applications, often functioning as a 'lowest common denominator' when it comes to exchanging sound files among different programs.
Use by broadcasters.
In spite of their large size, uncompressed WAV files are sometimes used by some radio broadcasters, especially those that have adopted a tapeless system. BBC Radio in the UK uses 44.1 kHz 16-bit two-channel WAV audio as standard in their VCS system. 
Limitations.
The WAV format is limited to files that are less than 4 GB, because of its use of a 32-bit unsigned integer to record the file size header (some programs limit the file size to 2 GB). Although this is equivalent to about 6.8 hours of CD-quality audio (44.1 kHz, 16-bit stereo), it is sometimes necessary to exceed this limit, especially when greater sampling rates, bit resolutions or channel count are required. The W64 format was therefore created for use in Sound Forge. Its 64-bit header allows for much longer recording times.
The RF64 format specified by the European Broadcasting Union has also been created to solve this problem.
Non-audio data.
Since the sampling rate of a WAV file can vary from 1 Hz to 4.3 GHz, and the number of channels can be as high as 65535, .wav files have also been used for non-audio data. LTspice, for instance, can store multiple circuit trace waveforms in separate channels, at any appropriate sampling rate, with the full-scale range representing ±1 V or A rather than a sound pressure.
Audio CDs.
Audio CDs do not use the WAV file format, using instead Red Book audio. The commonality is that both audio CDs and WAV files encode the audio as PCM. WAV is a file format for a computer to use that cannot be understood by most CD players directly. To record WAV files to an Audio CD the file headers must be stripped and the remaining PCM data written directly to the disc as individual tracks with zero-padding added to match the CD's sector size. In order for a WAV file to be able to be burned to a CD, it should be in the 44100 Hz, 16-bit stereo format.
WAV file audio coding formats compared.
Audio in WAV files can be encoded in a variety of audio coding formats, such as GSM or MP3, to reduce the file size.
This is a reference to compare the monophonic (not stereophonic) audio quality and compression bitrates of audio coding formats available for WAV files including PCM, ADPCM, Microsoft GSM 06.10, CELP, SBC, Truespeech and MPEG Layer-3.
The above are WAV files; even those that use MP3 compression have the "codice_28" extension.

</doc>
<doc id="34064" url="http://en.wikipedia.org/wiki?curid=34064" title="Windows 95">
Windows 95

Windows 95 (codenamed Chicago) is a consumer-oriented operating system developed by Microsoft. It was released on August 24, 1995, and was a significant progression from the company's previous Windows products.
Windows 95 integrated Microsoft's formerly separate MS-DOS and Windows products. It featured significant improvements over its predecessor, Windows 3.1, most notably in the graphical user interface (GUI) and in its relatively simplified "plug-n-play" features. There were also major changes made at lower levels of the operating system, such as moving from a mainly 16-bit architecture to a pre-emptively multitasked 32-bit architecture.
Accompanied by an extensive marketing campaign, Windows 95 was a major success in the marketplace at launch and shortly became the most popular desktop operating system. It also introduced numerous functions and features that were featured in later Windows versions, such as the taskbar, the 'Start' button, and the way the user navigates. It was also suggested that Windows 95 had an effect of driving other major players (including OS/2) out of business, something which would later be used in court against Microsoft.
Three years after its introduction, Windows 95 was succeeded by Windows 98. Microsoft ended support for Windows 95 on December 31, 2001.
Development.
The initial design and planning of Windows 95 can be traced back to around March 1992, just after the release of Windows 3.1. At this time, "Windows for Workgroups 3.11" and Windows NT 3.1 were still in development and Microsoft's plan for the future was focused on Cairo. Cairo would be Microsoft's next-generation operating system based on Windows NT and featuring a new user interface and an object-based file system, but it was not planned to be shipped before 1994 (Cairo would eventually partially ship in July 1996 in the form of Windows NT 4.0, but without the object-based file system, which would later evolve into WinFS).
Simultaneously with Windows 3.1's release, IBM started shipping OS/2 2.0. Microsoft realized they were in need of an updated version of Windows that could support 32-bit applications and preemptive multitasking, but could still run on low-end hardware (Windows NT did not). So the development of Windows "Chicago" was started and, as it was planned for a late 1993 release, became known as Windows 93. Initially, the decision was made not to include a new user interface, as this was planned for Cairo, and only focus on making installation, configuration, and networking easier. Windows 93 would ship together with MS-DOS 7.0, offering a more integrated experience to the user and making it pointless for other companies to create DOS clones. MS-DOS 7.0 was in development at that time under the code name "Jaguar" and could optionally run on top of a Windows 3.1-based 32-bit protected mode kernel called "Cougar" in order to better compete with DR-DOS. The first version of Chicago's feature specification was finished on September 30, 1992. Cougar was to become Chicago's kernel.
Beta.
Prior to the official release, the American public was given a chance to preview Windows 95 in the Windows 95 Preview Program. For US$19.95, users were sent a set of 3.5-inch floppy diskettes that would install Windows 95 either as an upgrade to Windows 3.1x or as a fresh install on a clean computer. Users who bought into the program were also given a free preview of The Microsoft Network (MSN), the online service that Microsoft launched with Windows 95. During the preview period Microsoft established various electronic distribution points for promotional and technical documentation on Chicago including a detailed document for media reviewers describing the new system highlights. The preview versions expired in November 1995, after which the user would have to purchase their own copy of the final version of Windows 95.
Release.
Windows 95 was released with great fanfare, including a commercial featuring the Rolling Stones' 1981 single "Start Me Up" (a reference to the Start button). It was widely reported that Microsoft paid the Rolling Stones between US$8 and US$14 million for the use of the song in the 95 advertising campaign. According to sources at Microsoft, however, this was just a rumor spread by the band to increase their market value, and Microsoft actually paid a fraction of that amount. A 30-minute promotional video, labeled a "cyber sitcom", featuring Jennifer Aniston and Matthew Perry, was also released to showcase the features of Windows 95. Microsoft's US$300 million advertising campaign featured stories of people waiting in line outside stores to get a copy.
In the UK, the largest computer chain PC World received a large number of oversized Windows 95 boxes, posters and point of sale material, and many branches opened at midnight to sell the first copies of the product. In London, Microsoft gave free newspapers to people.
In the United States, the Empire State Building in New York City was lit to match the colors of the Windows logo. In Canada, a 328 ft banner was hung from the top of the CN Tower in Toronto. Copies of The Times were available for free in the United Kingdom where Microsoft paid for 1.5 million issues (twice the daily circulation at the time).
The release included a number of "Fun Stuff" items on the CD, including music videos of Edie Brickell's "Good Times" and Weezer's "Buddy Holly", and the computer game Hover!.
Architecture.
Windows 95 was designed to be maximally compatible with existing MS-DOS and 16-bit Windows programs and device drivers, while offering a more stable and better performing system. Windows 95 architecture is an evolution of Windows for Workgroups' 386 enhanced mode. The lowest level of the operating system consists of a large number of "virtual device drivers" (VxDs) running in 32-bit protected mode and one or more virtual DOS machines running in virtual 8086 mode. The virtual device drivers are responsible for handling physical devices (such as video and network cards), emulating virtual devices used by the virtual machines, or providing various system services. The three most important virtual device drivers are:
Access requests to physical media are sent to "Input/Output Supervisor", a component responsible for scheduling the requests. Each physical media has its own device driver: Access to the disk is performed by a "port driver", while access to a SCSI device is handled by a "miniport" driver working atop the SCSI layer. Port and miniport drivers perform I/O operations in 32-bit protected mode, bypassing MS-DOS and BIOS, giving a significant performance improvement. In case there is no native Windows driver for a certain storage device, or if a device is forced to run in compatibility mode, the "Real Mode Mapper" can access it through MS-DOS.
32-bit Windows programs are assigned their own memory segments, which can be adjusted to any size the user wishes. Memory area outside the segment cannot be accessed by a program. If they crash, they do not harm anything else. Before this, programs used fixed non-exclusive 64 KB segments. While the 64 KB size was a serious handicap in DOS and Windows 3.x, lack of guarantee of exclusiveness was the cause of stability issues because programs sometimes overwrote each other's segments. A crashing Windows 3.x program could knock out surrounding processes.
The Win32 API is implemented by three modules, each consisting of a 16-bit and a 32-bit component:
Dependence on MS-DOS.
To end-users, MS-DOS appears as an underlying component of Windows 95. For example, it is possible to prevent loading the graphical user interface and boot the system into a real-mode MS-DOS environment. This sparked debate amongst users and professionals over the question of to what extent Windows 95 is an operating system or merely a graphical shell running on top of MS-DOS.
When the graphical user interface is started, the virtual machine manager takes over the filesystem-related and disk-related functionality. MS-DOS itself is demoted to a compatibility layer for 16-bit device drivers. This contrasts with earlier versions of Windows which relies on MS-DOS to perform file and disk access (Windows for Workgroups 3.11 could also largely bypass MS-DOS when 32-bit file access and 32-bit disk access were enabled). Keeping MS-DOS in memory allows Windows 95 to use DOS device drivers when suitable Windows drivers are unavailable. Windows 95 is capable of using all 16-bit Windows 3.x drivers.
Contrary to Windows 3.1x, DOS programs running in Windows 95 do not need DOS drivers for mouse, CD-ROM access and sound card; Windows drivers are used instead. HIMEM.SYS is still required to boot Windows 95. EMM386 and other memory managers, however, are only used by legacy DOS programs. In addition, CONFIG.SYS and AUTOEXEC.BAT settings (aside from HIMEM.SYS) have no effect on Windows programs. DOS games, which could not be executed on Windows 3.x, can run inside Windows 95. (Games tended to lock up Windows 3.x or cause other problems). As with Windows 3.x, DOS programs that use EGA or VGA graphics modes run in windowed mode (CGA and text mode programs can continue to run).
On startup, the MS-DOS component in Windows 95 responds to a pressed F8 key by temporarily pausing the default boot process and presenting the DOS boot options menu, allowing the user to continue starting Windows normally, start Windows in safe mode or exit to the DOS prompt. As in previous versions of MS-DOS, there is no 32-bit support and DOS drivers must be loaded for mice and other hardware.
As a consequence of being DOS-based, Windows 95 has to keep internal DOS data structures synchronized with those of Windows 95. When starting a program, even a native 32-bit Windows program, MS-DOS momentarily executes to create a data structure known as the Program Segment Prefix. It is even possible for MS-DOS to run out of conventional memory while doing so, preventing the program from launching. Windows 3.x allocated "fixed" segments in conventional memory first. Since the segments were allocated as fixed, Windows could not move them, which would prevent any more programs from launching.
Microsoft partially removed support for File Control Blocks (an API hold-over of DOS 1.x and CP/M) in Windows 95 OSR 2. FCB functions cannot write to FAT32 volumes, only read them.
User interface.
Windows 95 introduced a redesigned shell based around a desktop metaphor; the desktop was re-purposed to hold shortcuts to applications, files, and folders - unlike Windows 3.1 where it was used to display running applications. Running applications were now displayed as buttons on a taskbar across the bottom of the screen, which also contains a notification area used to display icons for background applications, a volume control, and the current time. The Start menu, invoked by clicking the "Start" button also contained on the taskbar, was introduced as an additional means of launching applications or opening documents. While maintaining the program groups used by its predecessor, Program Manager, it now displayed applications within cascading sub-menus. The previous File Manager program was also replaced by Windows Explorer.
In 1994, Microsoft corporation designers Mark Malamud and Erik Gavriluk approached Brian Eno to compose music for the Windows 95 project. The result was the six-second start-up music-sound of the Windows 95 operating system, "The Microsoft Sound".
When released for Windows 95 and NT4, Internet Explorer 4 came with an optional Windows Desktop Update, which modified the shell to provide new features integrated with Internet Explorer, such as Active Desktop (which allowed internet content to be displayed directly on the desktop) and additional updates to Windows Explorer.
Some of the user interface elements introduced in Windows 95—such as the desktop, taskbar, Start menu, and Windows Explorer file manager, remained fundamentally unchanged on future versions of Windows.
Technical improvements.
Windows 95 included support for 255-character mixed-case long filenames and preemptively multitasked protected-mode 32-bit applications.
Long file names.
32-bit File Access is necessary for the "long file names" feature introduced with Windows 95 through the use of the VFAT file system extension. It is available to both Windows programs and MS-DOS programs started from Windows (they have to be adapted slightly, since accessing long file names requires using larger pathname buffers and hence different system calls). Competing DOS-compatible operating systems released before Windows 95 cannot see these names. Using older versions of DOS utilities to manipulate files means that the long names are not visible and are lost if files are moved or renamed, as well as by the copy (but not the original), if the file is copied. During a Windows 95 automatic upgrade of an older Windows 3.1 system, DOS and third-party disk utilities which can destroy long file names are identified and made unavailable. When Windows 95 is started in DOS mode, e.g. for running DOS programs, low-level access to disks is locked out. In case the need arises to depend on disk utilities that do not recognize long file names, such as the MS-DOS 6.x's defrag utility, a program called LFNBACK for backup and restoration of long file names is provided on the CD-ROM. The program is in the \ADMIN\APPTOOLS\LFNBACK directory of the Windows 95 CD-ROM.
32-bit.
Windows 95 followed Windows for Workgroups 3.11 with its lack of support for older, 16-bit x86 processors, thus requiring an Intel 80386 (or compatible). While the OS kernel is 32-bit, much code (especially for the user interface) remained 16-bit for performance reasons as well as development time constraints (much of Windows 95's UI code was recycled from Windows 3.1). This had a rather detrimental effect on system stability and led to frequent application crashes.
The introduction of 32-bit File Access in Windows for Workgroups 3.11 meant that 16-bit real mode MS-DOS is not used for managing the files while Windows is running, and the earlier introduction of the 32-bit Disk Access means that the PC BIOS is often no longer used for managing hard disks.
DOS can be used for running old-style drivers for compatibility, but Microsoft discourages using them, as this prevents proper multitasking and impairs system stability. Control Panel allows a user to see which MS-DOS components are used by the system; optimal performance is achieved when they are bypassed. The Windows kernel uses MS-DOS style real-mode drivers in "Safe Mode," which exists to allow a user to fix problems relating to loading native, protected-mode drivers.
Internet Explorer.
Windows 95 originally shipped without Internet Explorer, and the default network installation did not install TCP/IP, the network protocol used on the Internet. At the release date of Windows 95, Internet Explorer 1.0 was available, but only in the Plus! add-on pack for Windows 95, which was a separate product. The Plus! Pack did not reach as many retail consumers as the operating system itself (it was mainly advertised for its non-internet-related add-ons such as themes and better disk compression) but was usually included in pre-installed (OEM) sales, and at the time of Windows 95 release, the web was being browsed mainly with a variety of early web browsers such as NCSA Mosaic and Netscape Navigator (promoted by products such as Internet in a Box).
Windows 95 OEM Service Release 1 was the first release of Windows to include Internet Explorer (version 2.0) with the OS. While there was no uninstaller, it could be deleted easily if the user so desired. OEM Service Release 2 included Internet Explorer 3. The installation of Internet Explorer 4 on Windows 95 (or the OSR2.5 version preinstalled on a computer) gave Windows 95 active desktop and browser integration into Windows Explorer, known as the Windows Desktop Update. The CD version of the last release of Windows 95, OEM Service Release 2.5 (Version 4.00.950C), includes Internet Explorer 4, and installs it after Windows 95's initial setup and first boot is complete.
Only the 4.x series of the browser contained the Windows Desktop Update features, so anyone wanting the new shell had to install IE4 with the desktop update before installing a newer version of Internet Explorer. The last version of Internet Explorer supported on Windows 95 is Internet Explorer 5.5 which was released in 2000. Windows 95 shipped with Microsoft's own dial-up online service called The Microsoft Network (MSN).
Editions.
A number of editions of Windows 95 have been released. Only the original release was sold as a shrink-wrapped product, later editions were provided only to computer original equipment manufacturers (OEMs) for installation on new PCs. For this reason these editions are known as OEM Service Releases (OSR).
Together with the introduction of Windows 95, Microsoft released the "Microsoft Plus! for Windows 95" pack, which contained a number of optional components for high-end (486) multimedia PCs, including Internet Explorer, DriveSpace, and additional themes.
Microsoft initially indicated to make updates available to Windows 95 every 6 months in the form of service packs. The growing availability of Internet access meant that Windows updates could now be downloaded from Microsoft directly. The first service pack was made available half a year after the original release and fixed a number of small bugs.
The second service pack mainly introduced support for new hardware. Most notably support for hard drives larger than 2 GB in the form of the FAT32 file system. This release was never made available to end-users directly and was only sold through OEMs with the purchase of a new PC. 
A full third service pack was never released, but two smaller updates to the second were released in the form of a USB Supplement (OSR 2.1) and the Windows Desktop Update (OSR 2.5). Both were available as stand-alone updates and as updated disc images shipped by OEMs. OSR 2.5 was notable for featuring a number of changes to the Windows Explorer, integrating it with Internet Explorer 4.0—this version of the Explorer looks very similar to the one featured in Windows 98.
System requirements.
Official system requirements were an Intel 80386 DX CPU of any speed, 4 MB of system RAM, and 50–55 MB of hard drive space depending on features selected. These minimal claims were made in order to maximize the available market of Windows 3.1 converts. This configuration would rely heavily on virtual memory and was suboptimal for productive use on anything but single tasking dedicated workstations. Also, in some cases, if any networking or similar components were installed the system would refuse to boot with 4 megabytes of RAM. It was possible to run Windows 95 on a 386 SX but this led to even less acceptable performance due to its 16-bit external data bus. To achieve optimal performance, Microsoft recommends an Intel 80486 or compatible microprocessor with at least 8 MB of RAM. Windows 95 may fail to boot on computers with more than approximately 480 MB of memory. In such case reducing the file cache size or the size of video memory can help. The theoretical maximum according to Microsoft being 2 GB.
Windows 95 was superseded by Windows 98 and could still be directly upgraded by both Windows 2000 Professional edition and Windows ME. On December 31, 2001, Microsoft ended its support for Windows 95, making it an "obsolete" product according to the Microsoft Lifecycle Policy. Even though support for Windows 95 has ended, the software has occasionally remained in use on legacy systems for various purposes. In addition, some video game enthusiasts choose to use Windows 95 for their legacy system to play old DOS games, although some other versions of Windows such as Windows 98 can also be used for this purpose.
Most copies of Windows 95 were on CD-ROM, but a floppy version could also be had for older machines. The retail floppy disk version of Windows 95 came on 13 DMF formatted floppy disks, while OSR 2.1 doubled the floppy count to 26. Both versions exclude additional software that CD-ROM might have featured. Microsoft Plus! for Windows 95 was also available on floppy disks. DMF was a special 19-sector format Microsoft used to store 1.7MB on floppies rather than the usual 1.44MB. While the floppy edition of Windows was normally on 3.5" disks, a 5.25" version could be specially ordered as well.
Legacy.
Many features that have become key components of the Microsoft Windows series, such as the Start menu and the taskbar, originated in Windows 95. Neil MacDonald, a Gartner analyst, said "If you look at Windows 95, it was a quantum leap in difference in technological capability and stability." Ina Fried of "CNET" said "By the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world."
Further reading.
Microsoft:
Third-party:
</dl>

</doc>
<doc id="34068" url="http://en.wikipedia.org/wiki?curid=34068" title="Wilmington">
Wilmington

Wilmington may refer to:

</doc>
<doc id="34069" url="http://en.wikipedia.org/wiki?curid=34069" title="Winter Olympic Games">
Winter Olympic Games

The Winter Olympic Games (French: "Jeux olympiques d'hiver") are a major international sporting event that occurs once every four years. Unlike the Summer Olympics, the Winter Olympics feature sports practiced on snow and ice. The first Winter Olympics, the 1924 Winter Olympics, was held in Chamonix, France. The original five sports (broken into nine disciplines) were bobsleigh, curling, ice hockey, Nordic skiing (consisting of the disciplines military patrol, cross-country skiing, Nordic combined, and ski jumping), and skating (consisting of the disciplines figure skating and speed skating). The Games were held every four years from 1924 until 1936, after which they were interrupted by World War II. The Olympics resumed in 1948 and was again held every four years. Until 1992, the Winter and Summer Olympic Games were held in the same years, but in accordance with a 1986 decision by the International Olympic Committee (IOC) to place the Summer and Winter Games on separate four-year cycles in alternating even-numbered years, the next Winter Olympics after 1992 was in 1994.
The Winter Games have evolved since its inception. Sports and disciplines have been added and some of them, such as Alpine skiing, luge, short track speed skating, freestyle skiing, skeleton, and snowboarding, have earned a permanent spot on the Olympic program. Others (such as curling and bobsleigh) have been discontinued and later reintroduced, or have been permanently discontinued (such as military patrol, though the modern Winter Olympic sport of biathlon is descended from it). Still others, such as speed skiing, bandy and skijoring, were demonstration sports but never incorporated as Olympic sports. The rise of television as a global medium for communication enhanced the profile of the Games. It created an income stream, via the sale of broadcast rights and advertising, which has become lucrative for the IOC. This allowed outside interests, such as television companies and corporate sponsors, to exert influence. The IOC has had to address several criticisms, internal scandals, the use of performance enhancing drugs by Winter Olympians, as well as a political boycott of the Winter Olympics. Nations have used the Winter Games to showcase the claimed superiority of their political systems.
The Winter Olympics has been hosted on three continents by eleven different countries. The United States has hosted the Games four times (1932, 1960, 1980, 2002); France has been the host three times (1924, 1968, 1992); Austria (1964, 1976), Canada (1988, 2010), Japan (1972, 1998), Italy (1956, 2006), Norway (1952, 1994), and Switzerland (1928, 1948) have hosted the Games twice. Germany (1936), Yugoslavia (1984), and Russia (2014) have hosted the Games once. The IOC has selected Pyeongchang, South Korea, to host the 2018 Winter Olympics. No country in the southern hemisphere has hosted or even been an applicant to host the Winter Olympics; the major challenge preventing one hosting the games is the dependence on winter weather, and the traditional February timing of the games falls in the middle of the southern hemisphere summer.
Twelve countries – Austria, Canada, Finland, France, Great Britain, Hungary, Italy, Norway, Poland, Sweden, Switzerland and the United States – have sent athletes to every Winter Olympic Games. Six of those – Austria, Canada, Finland, Norway, Sweden and the United States – have earned medals at every Winter Olympic Games, and only one – the United States – has earned gold at each Games. Germany and Japan have been banned at times from competing in the Games.
History.
Early years.
 A predecessor, the Nordic Games, were organized by General Viktor Gustaf Balck in 1901 and were held again in 1903 and 1905 and then every fourth year thereafter until 1926. Balck was a charter member of the International Olympic Committee (IOC) and a close friend of Olympic Games founder Pierre de Coubertin. He attempted to have winter sports, specifically figure skating, added to the Olympic program but was unsuccessful until the 1908 Summer Olympics in London, United Kingdom. Four figure skating events were contested, at which Ulrich Salchow (10-time world champion) and Madge Syers won the individual titles.
Three years later, Italian count Eugenio Brunetta d'Usseaux proposed that the IOC stage a week of winter sports included as part of the 1912 Summer Olympics in Stockholm, Sweden. The organisers opposed this idea because they desired to protect the integrity of the Nordic Games and were concerned about a lack of facilities for winter sports. The idea was resurrected for the 1916 Games, which were to be held in Berlin, Germany. A winter sports week with speed skating, figure skating, ice hockey and Nordic skiing was planned, but the 1916 Olympics was cancelled after the outbreak of World War I.
The first Olympics after the war, the 1920 Summer Olympics, were held in Antwerp, Belgium, and featured figure skating and an ice hockey tournament. Germany, Austria, Hungary, Bulgaria and Turkey were banned from competing in the Games. At the IOC Congress held the following year it was decided that the host nation of the 1924 Summer Olympics, France, would host a separate "International Winter Sports Week" under the patronage of the IOC. Chamonix was chosen to host this "week" (actually 11 days) of events. The Games proved to be a success when more than 250 athletes from 16 nations competed in 16 events. Athletes from Finland and Norway won 28 medals, more than the rest of the participating nations combined. Germany remained banned until 1925, and instead hosted a series of games called Deutsche Kampfspiele, starting with the Winter edition of 1922 (which predated the first Winter Olympics). In 1925 the IOC decided to create a separate Olympic Winter Games and the 1924 Games in Chamonix was retroactively designated as the first Winter Olympics.
St. Moritz, Switzerland, was appointed by the IOC to host the second Olympic Winter Games in 1928. Fluctuating weather conditions challenged the hosts. The opening ceremony was held in a blizzard while warm weather conditions plagued sporting events throughout the rest of the Games. Because of the weather the 10,000 metre speed-skating event had to be abandoned and officially cancelled. The weather was not the only noteworthy aspect of the 1928 Games: Sonja Henie of Norway made history when she won the figure skating competition at the age of 15. She became the youngest Olympic champion in history, a distinction she would hold for 74 years.
The next Winter Olympics was the first to be hosted outside of Europe. Seventeen nations and 252 athletes participated. This was less than in 1928 as the journey to Lake Placid, United States, was a long and expensive one for most competitors who had little money in the midst of the Great Depression. The athletes competed in fourteen events in four sports. Virtually no snow fell for two months before the Games, and it was not until mid-January that there was enough snow to hold all the events. Sonja Henie defended her Olympic title and Eddie Eagan, who had been an Olympic champion in boxing in 1920, won the gold in the men's bobsleigh event to become the first, and so far only, Olympian to have won gold medals in both the Summer and Winter Olympics.
The German towns of Garmisch and Partenkirchen joined to organise the 1936 edition of the Winter Games, held on 6–16 February. This would be the last time the Summer and Winter Olympics were held in the same country in the same year. Alpine skiing made its Olympic debut, but skiing teachers were barred from entering because they were considered to be professionals. Because of this decision the Swiss and Austrian skiers refused to compete at the Games.
World War II.
World War II interrupted the celebrations of the Winter Olympics. The 1940 Games had been awarded to Sapporo, Japan, but the decision was rescinded in 1938 because of the Japanese invasion of China. The Games were moved to Garmisch-Partenkirchen, Germany, but the German invasion of Poland in 1939 forced the complete cancellation of the 1940 Games. Due to the ongoing war the 1944 Games, originally scheduled for Cortina D'Ampezzo, Italy, were cancelled.
1948 to 1960.
St. Moritz was selected to host the first post-war Games in 1948. Switzerland's neutrality had protected the town during World War II and most of the venues were in place from the 1928 Games, which made St. Moritz a logical choice to become the first city to host a Winter Olympics twice. Twenty-eight countries competed in Switzerland, but athletes from Germany and Japan were not invited. Controversy erupted when two hockey teams from the United States arrived, both claiming to be the legitimate U.S. Olympic hockey representative. The Olympic flag presented at the 1920 Summer Olympics in Antwerp was stolen, as was its replacement. There was unprecedented parity at these Games, during which 10 countries won gold medals—more than any Games to that point.
The Olympic Flame for the 1952 Games in Oslo, was lit in the fireplace by skiing pioneer Sondre Nordheim and the torch relay was conducted by 94 participants entirely on skis. Bandy, a popular sport in the Nordic countries, was featured as a demonstration sport, though only Norway, Sweden and Finland fielded teams. Norwegian athletes won 17 medals, which outpaced all the other nations. They were led by Hjalmar Andersen who won three gold medals in four events in the speed skating competition.
After not being able to host the Games in 1944, Cortina d'Ampezzo was selected to organise the 1956 Winter Olympics. At the opening ceremonies the final torch bearer, Guido Caroli, entered the Olympic Stadium on ice skates. As he skated around the stadium his skate caught on a cable and he fell, nearly extinguishing the flame. He was able to recover and light the cauldron. These were the first Winter Games to be televised, though no television rights would be sold until the 1960 Summer Olympics in Rome. The Cortina Games were used to test the feasibility of televising large sporting events. The Soviet Union made its Olympic debut and had an immediate impact, winning more medals than any other nation. Chiharu Igaya won the first Winter Olympics medal for Japan and the continent of Asia, when he placed second in the slalom.
The IOC awarded the 1960 Olympics to Squaw Valley, United States. Since the village was underdeveloped, there was a rush to construct infrastructure and sports facilities like an ice arena, speed-skating track, and a ski-jump hill. The opening and closing ceremonies were produced by Walt Disney. The Squaw Valley Olympics had a number of notable firsts: it was the first Olympics to have a dedicated athletes' village, it was the first to use a computer (courtesy of IBM) to tabulate results, and the first to feature female speed skating events. The bobsleigh events were absent for the only time, because the organising committee found it too expensive to build the bobsleigh run.
1964 to 1980.
The Austrian city of Innsbruck was the host in 1964. Although Innsbruck was a traditional winter sports resort, warm weather caused a lack of snow during the Games and the Austrian army was asked to transport snow and ice to the sport venues. Soviet speed-skater Lidia Skoblikova made history by sweeping all four speed-skating events. Her career total of six gold medals set a record for Winter Olympics athletes. Luge was first contested in 1964, although the sport received bad publicity when a competitor was killed in a pre-Olympic training run.
Held in the French town of Grenoble, the 1968 Winter Olympics were the first Olympic Games to be broadcast in colour. There were 37 nations and 1,158 athletes competing in 35 events. Frenchman Jean-Claude Killy became only the second person to win all the men's alpine skiing events. The organising committee sold television rights for $2 million, which was more than double the price of the broadcast rights for the Innsbruck Games. Venues were spread over long distances requiring three athletes' villages. The organisers claimed this was required to accommodate technological advances. Critics disputed this, alleging that the layout was necessary to provide the best possible venues for television broadcasts at the expense of the athletes.
The 1972 Winter Games, held in Sapporo, Japan, were the first to be hosted outside North America or Europe. The issue of professionalism became contentious during the Sapporo Games. Three days before the Games IOC president Avery Brundage threatened to bar a number of alpine skiers from competing because they participated in a ski camp at Mammoth Mountain in the United States. Brundage reasoned that the skiers had financially benefited from their status as athletes and were therefore no longer amateurs. Eventually only Austrian Karl Schranz, who earned more than all the other skiers, was not allowed to compete. Canada did not send teams to the 1972 or 1976 ice hockey tournaments in protest of their inability to use players from professional leagues. Francisco Fernández Ochoa became the first (and only) Spaniard to win a Winter Olympic gold medal; he triumphed in the slalom.
The 1976 Winter Olympics had been awarded in 1970 to Denver, United States, but in November 1972 the voters of the state of Colorado voted against public funding of the games by a 3 to 2 margin. The IOC turned to offer the Games to Vancouver-Garibaldi, British Columbia, which had been a candidate for the 1976 Games. However, a change in provincial government brought in an administration which did not support the Olympic bid, so the offer was rejected. Salt Lake City, a candidate for the 1972 Games, offered itself, but the IOC opted to ask Innsbruck, which had maintained most of the infrastructure from the 1964 Games. With half the time to prepare for the Games as intended, Innsbruck accepted the invitation to replace Denver in February 1973. Two Olympic flames were lit because it was the second time the Austrian town had hosted the Games. The 1976 Games featured the first combination bobsleigh and luge track, in neighbouring Igls. The Soviet Union won its fourth consecutive ice hockey gold medal.
In 1980 the Olympics returned to Lake Placid, which had hosted the 1932 Games. The first boycott of a Winter Olympics occurred in 1980 when Taiwan refused to participate after an edict by the IOC mandated that they change their name and national anthem. The IOC was attempting to accommodate China, who wished to compete using the same name and anthem that had been used by Taiwan. American speed-skater Eric Heiden set either an Olympic or world record in each of the five events he competed in. Hanni Wenzel won both the slalom and giant slalom and her country, Liechtenstein, became the smallest nation to produce an Olympic gold medallist. In the "Miracle on Ice" the American hockey team beat the favoured Soviets, and then went on to win the gold medal.
1984 to 1998.
Sapporo, Japan, and Gothenburg, Sweden, were front-runners to host the 1984 Winter Olympics. It was therefore a surprise when Sarajevo, Yugoslavia, was selected as host. The Games were well-organised and displayed no indication of the war that would engulf the country eight years later. A total of 49 nations and 1,272 athletes participated in 39 events. Host nation Yugoslavia won its first Olympic medal when alpine skier Jure Franko won a silver in the giant slalom. Another sporting highlight was the free dance performance of British ice dancers Jayne Torvill and Christopher Dean. Their performance to Ravel's "Boléro" earned the pair the gold medal after achieving unanimous perfect scores for artistic impression.
In 1988, the Canadian city of Calgary hosted the first Winter Olympics to span 16 days. New events were added in ski-jumping and speed skating; while future Olympic sports curling, short track speed skating and freestyle skiing made their appearance as demonstration sports. For the first time the speed skating events were held indoors, on the Olympic Oval. Dutch skater Yvonne van Gennip won three gold medals and set two world records, beating skaters from the favoured East German team in every race. Her medal total was equalled by Finnish ski jumper Matti Nykänen, who won all three events in his sport. Alberto Tomba, an Italian skier, made his Olympic debut by winning both the giant slalom and slalom. East German Christa Rothenburger won the women's 1,000 metre speed skating event. Seven months later she would earn a silver in track cycling at the Summer Games in Seoul, to become the only athlete to win medals in both a Summer and Winter Olympics in the same year.
The 1992 Games were the last to be held in the same year as the Summer Games. They were hosted in the French Savoie region in the city of Albertville, though only 18 events were held in the city. The rest of the events were spread out over the Savoie. Political changes of the time were reflected in the Olympic teams appearing in France: this was the first Games to be held after the fall of Communism and the dismantling of the Berlin Wall, and Germany competed as a single nation for the first time since the 1964 Games; former Yugoslavian republics Croatia and Slovenia made their debuts as independent nations; most of the former Soviet republics still competed as a single team known as the Unified Team, but the Baltic States made independent appearances for the first time since before World War II. At 16 years old, Finnish ski jumper Toni Nieminen made history by becoming the youngest male Winter Olympic champion. New Zealand skier Annelise Coberger became the first Winter Olympic medallist from the southern hemisphere when she won a silver medal in the women's slalom.
In 1986 the IOC had voted to separate the Summer and Winter Games and place them in alternating even-numbered years. This change became effective for the 1994 Games, held in Lillehammer, Norway, which became the first Winter Olympics to be held separate from the Summer Games. After the division of Czechoslovakia in 1993 the Czech Republic and Slovakia made their Olympic debuts. The women's figure skating competition garnered media attention when American skater Nancy Kerrigan was injured on 6 January 1994, in an assault planned by the ex-husband of opponent Tonya Harding. Both skaters competed in the Games, but the gold medal was won by Oksana Baiul. She became Ukraine's first Olympic champion. Johann Olav Koss of Norway won three gold medals, coming first in all of the distance speed skating events.
The 1998 Winter Olympics were held in the Japanese city of Nagano and were the first Games to host more than 2,000 athletes. The men's ice hockey tournament was opened to professionals for the first time. Canada and the United States, with their many NHL players, were favoured to win the tournament. Neither won any hockey medals however, as the Czech Republic prevailed. Women's ice hockey made its debut and the United States won the gold medal. Bjørn Dæhlie of Norway won three gold medals in Nordic skiing. He became the most decorated Winter Olympic athlete with eight gold medals and twelve medals overall. Austrian Hermann Maier survived a crash during the downhill competition and returned to win gold in the super-g and the giant slalom. A wave of new world records were set in speed skating because of the introduction of the clap skate.
2002 to 2010.
The 2002 Winter Olympics were held in Salt Lake City, United States, hosting 77 nations and 2,399 athletes in 78 events in 7 sports. These games were the first to take place since 11 September 2001, which meant a higher degree of security to avoid a terrorist attack. The opening ceremonies of the games saw signs of the aftermath of the events of that day, including the flag that flew at Ground Zero, NYPD officer Daniel Rodríguez singing "God Bless America", and honor guards of NYPD and FDNY members.
German Georg Hackl won a silver in the singles luge, becoming the first athlete in Olympic history to win medals in the same individual event in five consecutive Olympics. Canada achieved an unprecedented double by winning both the men's and women's ice hockey gold medals. Canada became embroiled with Russia in a controversy that involved the judging of the pairs figure skating competition. The Russian pair of Yelena Berezhnaya and Anton Sikharulidze competed against the Canadian pair of Jamie Salé and David Pelletier for the gold medal. The Canadians appeared to have skated well enough to win the competition, yet the Russians were awarded the gold. The judging broke along Cold War lines with judges from former Communist countries favouring the Russian pair and judges from Western nations voting for the Canadians. The only exception was the French judge, Marie-Reine Le Gougne, who awarded the gold to the Russians. An investigation revealed that she had been pressured to give the gold to the Russian pair regardless of how they skated; in return the Russian judge would look favourably on the French entrants in the ice dancing competition. The IOC decided to award both pairs the gold medal in a second medal ceremony held later in the Games. Australian Steven Bradbury became the first gold medallist from the southern hemisphere when he won the 1,000 metre short-track speed skating event.
The Italian city of Turin hosted the 2006 Winter Olympics. It was the second time that Italy had hosted the Winter Olympic Games. South Korean athletes won 10 medals, including 6 gold in the short-track speed skating events. Sun-Yu Jin won three gold medals while her teammate Hyun-Soo Ahn won three gold medals and a bronze. In the women's Cross-Country team pursuit Canadian Sara Renner broke one of her poles and, when he saw her dilemma, Norwegian coach Bjørnar Håkensmoen decided to lend her a pole. In so doing she was able to help her team win a silver medal in the event at the expense of the Norwegian team, who finished fourth. Claudia Pechstein of Germany became the first speed skater to earn nine career medals. In February 2009 Pechstein tested positive for "blood manipulation" and received a two-year suspension, which she appealed. The Court of Arbitration for Sport upheld her suspension but a Swiss court ruled that she could compete for a spot on the 2010 German Olympic team. This ruling was brought to the Swiss Federal Tribunal, which overturned the lower court's ruling and precluded her from competing in Vancouver.
In 2003 the IOC awarded the 2010 Winter Olympics to Vancouver, thus allowing Canada to host its second Winter Olympics. With a population of more than 2.5 million people Vancouver is the largest metropolitan area to ever host a Winter Olympic Games. Over 2,500 athletes from 82 countries participated in 86 events. The death of Georgian luger Nodar Kumaritashvili in a training run on the day of the opening ceremonies resulted in the Whistler Sliding Centre changing the track layout on safety grounds. Norwegian cross-country skier Marit Bjørgen won five medals in the six cross-country events on the women's programme. She finished the Olympics with three golds, a silver and a bronze. The Vancouver Games were notable for the poor performance of the Russian athletes. From their first Winter Olympics in 1956 to the 2006 games, a Soviet or Russian delegation had never been outside the top five medal-winning nations. In 2010 they finished sixth in total medals and eleventh in gold medals. President Dmitry Medvedev called for the resignation of top sports officials immediately after the Games. The success of Asian countries stood in stark contrast to the under-performing Russian team, with Vancouver marking a high point for medals won by Asian countries. In 1992 the Asian countries had won fifteen medals, three of which were gold. In Vancouver the total number of medals won by athletes from Asia had increased to thirty-one, with eleven of them being gold. The rise of Asian nations in Winter Olympics sports is due in part to the growth of winter sports programmes and the interest in winter sports in nations such as South Korea, Japan and China.
2014.
Sochi, Russia, was selected as the host city of the 2014 Winter Olympics over Salzburg, Austria, and Pyeongchang, South Korea. This was the first time since the breakup of the Soviet Union that Russia hosted a Winter Olympics. Over 2800 athletes from 88 countries participated in 98 events. The Olympic Village and Olympic Stadium are located on the Black Sea coast. All of the mountain venues are 50 km away in the alpine region known as Krasnaya Polyana.
The 2014 Winter Olympics, officially the XXII Olympic Winter Games, or the 22nd Winter Olympics, took place from 7 to 23 February 2014.
Future.
On 6 July 2011, the IOC selected the city of Pyeongchang, South Korea to host the 2018 Winter Olympics.
The host city for the XXIV Olympic Winter Games, also known as the 2022 Winter Olympics, will be elected on 31 July 2015, at the 128th IOC Session in Kuala Lumpur.
Controversy.
The process for awarding host city honours came under intense scrutiny after Salt Lake City had been awarded the right to host the 2002 Games. Soon after the host city had been announced it was discovered that the organisers had engaged in an elaborate bribery scheme to curry favour with IOC officials. Gifts and other financial considerations were given to those who would evaluate and vote on Salt Lake City's bid. These gifts included medical treatment for relatives, a college scholarship for one member's son and a land deal in Utah. Even IOC president Juan Antonio Samaranch received two rifles valued at $2,000. Samaranch defended the gift as inconsequential since, as president, he was a non-voting member. The subsequent investigation uncovered inconsistencies in the bids for every Games (both summer and winter) since 1988. For example the gifts received by IOC members from the Japanese Organising Committee for Nagano's bid for the 1998 Winter Olympics were described by the investigation committee as "astronomical". Although nothing strictly illegal had been done, the IOC feared that corporate sponsors would lose faith in the integrity of the process and that the Olympic brand would be tarnished to such an extent that advertisers would begin to pull their support. The investigation resulted in the expulsion of 10 IOC members and the sanctioning of another 10. New terms and age limits were established for IOC membership, and 15 former Olympic athletes were added to the committee. Stricter rules for future bids were imposed, with ceilings imposed on the value of gifts IOC members could accept from bid cities.
Host city legacy.
According to the IOC, the host city is responsible for, "...establishing functions and services for all aspects of the Games, such as sports planning, venues, finance, technology, accommodation, catering, media services etc., as well as operations during the Games." Due to the cost of hosting an Olympic Games, most host cities never realise a profit on their investment. For example the 1998 Winter Olympics in Nagano, Japan, cost $12.5 billion. By comparison the Torino Games of 2006 cost $3.6 billion to host. The organisers claimed that the cost of extending the bullet train service from Tokyo to Nagano was responsible for the large price tag. The organising committee hoped that the exposure of the Olympic Games, and the expedited access to Nagano from Tokyo, would be a boon to the local economy for years afterward. Nagano's economy did experience a two-year post-Olympic spurt, but the long-term effects have not materialised as planned. The possibility of heavy debt, coupled with unused sports venues and infrastructure that saddle the local community with upkeep costs and no practical post-Olympic value, is a deterrent to prospective host cities.
To mitigate these concerns the IOC has enacted several initiatives. First it has agreed to fund part of the host city's budget for staging the Games. Secondly, the IOC limits the qualifying host countries to those that have the resources and infrastructure to successfully host an Olympic Games without negatively impacting the region or nation. This eliminates a large portion of the developing world. Finally, cities bidding to host the Games are required to add a "legacy plan" to their proposal. This requires prospective host cities and the IOC, to plan with a view to the long-term economic and environmental impact that hosting the Olympics will have on the region.
Doping.
In 1967 the IOC began enacting drug testing protocols. They started by randomly testing athletes at the 1968 Winter Olympics. The first Winter Games athlete to test positive for a banned substance was Alois Schloder, a West German hockey player, but his team was still allowed to compete. During the 1970s testing outside of competition was escalated because it was found to deter athletes from using performance-enhancing drugs. The problem with testing during this time was a lack of standardisation of the test procedures, which undermined the credibility of the tests. It was not until the late 1980s that international sporting federations began to coordinate efforts to standardise the drug-testing protocols. The IOC took the lead in the fight against steroids when it established the independent World Anti-Doping Agency (WADA) in November 1999.
The 2006 Winter Olympics in Turin became notable for a scandal involving the emerging trend of blood doping, the use of blood transfusions or synthetic hormones such as Erythropoietin (EPO) to improve oxygen flow and thus reduce fatigue. The Italian police conducted a raid on the Austrian cross-country ski team's residence during the Games where they seized blood-doping specimens and equipment. This event followed the pre-Olympics suspension of 12 cross-country skiers who tested positive for unusually high levels of hemoglobin, which is evidence of blood doping.
Commercialisation.
Avery Brundage, as president of the IOC from 1952 to 1972, rejected all attempts to link the Olympics with commercial interests as he felt that the Olympic movement should be completely separate from financial influence. The 1960 Winter Olympics marked the beginning of corporate sponsorship of the Games. Despite Brundage's strenuous resistance the commercialisation of the Games continued during the 1960s, and revenue generated by corporate sponsorship swelled the IOC's coffers. By the Grenoble Games, Brundage had become so concerned about the direction of the Winter Olympic Games towards commercialisation that, if it could not be corrected, he felt the Winter Olympics should be abolished. Brundage's resistance to this revenue stream meant that the IOC was unable to gain a share of the financial windfall that was coming to host cities, and had no control over the structuring of sponsorship deals. When Brundage retired the IOC had $2 million in assets; eight years later its accounts had swelled to $45 million. This was due to a shift in ideology among IOC members, towards expansion of the Games through corporate sponsorship and the sale of television rights.
Brundage's concerns proved prophetic. The IOC has charged more for television broadcast rights at each successive Games. At the 1998 Nagano Games American broadcaster CBS paid $375 million, whereas the 2006 Turin Games cost NBC $613 million to broadcast. The more television companies have paid to televise the Games, the greater their persuasive power has been with the IOC. For example, the television lobby has influenced the Olympic programme by dictating when event finals are held, so that they appear in prime time for television audiences. They have pressured the IOC to include new events, such as snowboarding, that appeal to broader television audiences. This has been done to boost ratings, which were slowly declining until the 2010 Games.
In 1986 the IOC decided to stagger the Summer and Winter Games. Instead of holding both in the same calendar year the committee decided to alternate them every two years, although both Games would still be held on four-year cycles. It was decided that 1992 would be the last year to have both a Winter and Summer Olympic Games. There were two underlying reasons for this change: first was the television lobby's desire to maximise advertising revenue as it was difficult to sell advertising time for two Games in the same year; second was the IOC's desire to gain more control over the revenue generated by the Games. It was decided that staggering the Games would make it easier for corporations to sponsor individual Olympic Games, which would maximise revenue potential. The IOC sought to directly negotiate sponsorship contracts so that they had more control over the Olympic "brand". The first Winter Olympics to be hosted in this new format were the 1994 Games in Lillehammer.
Politics.
Cold War.
The Winter Olympics have been an ideological front in the Cold War since the Soviet Union first participated at the 1956 Winter Games. It did not take long for the Cold War combatants to discover what a powerful propaganda tool the Olympic Games could be. Soviet and American politicians used the Olympics as an opportunity to prove the perceived superiority of their respective political systems. The successful Soviet athlete was feted and honoured. Irina Rodnina, three-time Olympic gold medallist in figure skating, was awarded the Order of Lenin after her victory at the 1976 Winter Olympics in Innsbruck. Soviet athletes who won gold medals could expect to receive between $4,000 and $8,000 depending on the prestige of the sport. A world record was worth an additional $1,500. In 1978 the United States Congress responded to these measures by passing legislation that reorganised the United States Olympic Committee. It also approved financial rewards to medal-winning athletes.
The Cold War created tensions amongst countries allied to the two superpowers. The strained relationship between East and West Germany created a difficult political situation for the IOC. Because of its role in World War II, Germany was not allowed to compete at the 1948 Winter Olympics. In 1950 the IOC recognised the West German Olympic Committee, and invited East and West Germany to compete as a unified team at the 1952 Winter Games. East Germany declined the invitation and instead sought international legitimacy separate from West Germany. In 1955 the Soviet Union recognised East Germany as a sovereign state, thereby giving more credibility to East Germany's campaign to become an independent participant at the Olympics. The IOC agreed to provisionally accept the East German National Olympic Committee with the condition that East and West Germans compete on one team. The situation became tenuous when the Berlin Wall was constructed in 1962 and western nations began refusing visas to East German athletes. The uneasy compromise of a unified team held until the 1968 Grenoble Games when the IOC officially split the teams and threatened to reject the host-city bids of any country that refused entry visas to East German athletes.
Boycott.
The Winter Games have had only one national team boycott when Taiwan decided not to participate in the 1980 Winter Olympics held in Lake Placid. Prior to the Games the IOC agreed to allow China to compete in the Olympics for the first time since 1952. China was given permission to compete as the "People's Republic of China" (PRC) and to use the PRC flag and anthem. Until 1980 the island of Taiwan had been competing under the name "Republic of China" (ROC) and had been using the ROC flag and anthem. The IOC attempted to have the countries compete together but when this proved to be unacceptable the IOC demanded that Taiwan cease to call itself the "Republic of China". The IOC renamed the island "Chinese Taipei" and demanded that it adopt a different flag and national anthem; stipulations that Taiwan would not agree to. Despite numerous appeals and court hearings the IOC's decision stood. When the Taiwanese athletes arrived at the Olympic village with their Republic of China identification cards they were not admitted. They subsequently left the Olympics in protest, just before the opening ceremonies. Taiwan returned to Olympic competition at the 1984 Winter Games in Sarajevo as Chinese Taipei. The country agreed to compete under a flag bearing the emblem of their National Olympic Committee and to play the anthem of their National Olympic Committee should one of their athletes win a gold medal. The agreement remains in place to this day.
Sports.
Chapter 1, article 6 of the 2007 edition of the Olympic Charter defines winter sports as "sports which are practised on snow or ice." Since 1992 a number of new sports have been added to the Olympic programme; which include short track speed skating, snowboarding, freestyle and moguls skiing. The addition of these events has broadened the appeal of the Winter Olympics beyond Europe and North America. While European powers such as Norway and Germany still dominate the traditional Winter Olympic sports, countries such as South Korea, Australia and Canada are finding success in the new sports. The results are more parity in the national medal tables, more interest in the Winter Olympics and higher global television ratings.
Current sport disciplines.
^ Note 1. Figure skating events were held at the 1908 and 1920 Summer Olympics.
^ Note 2. A men's ice hockey tournament was held at the 1920 Summer Olympics.
^ Note 3. The IOC's website now treats Men's Military Patrol at the 1924 games as an event within the sport of Biathlon.
Demonstration events.
Demonstration sports have historically provided a venue for host countries to attract publicity to locally popular sports by having a competition without granting medals. Demonstration sports were discontinued after 1992. Military patrol, a precursor to the biathlon, was a medal sport in 1924 and was demonstrated in 1928, 1936 and 1948, becoming an official sport in 1960. The special figures figure skating event was only contested at the 1908 Summer Olympics. Bandy (Russian hockey) is a sport popular in the Nordic countries and Russia. In the latter it's considered a national sport. It was demonstrated at the Oslo Games. Ice stock sport, a German variant of curling, was demonstrated in 1936 in Germany and 1964 in Austria. The ski ballet event, later known as ski-acro, was demonstrated in 1988 and 1992. Skijöring, skiing behind dogs, was a demonstration sport in St. Moritz in 1928. A sled-dog race was held at Lake Placid in 1932. Speed skiing was demonstrated in Albertville at the 1992 Winter Olympics. Winter pentathlon, a variant of the modern pentathlon, was included as a demonstration event at the 1948 Games in Switzerland. It was composed of cross-country skiing, shooting, downhill skiing, fencing and horse riding.
References.
Bibliography
</dl>

</doc>
<doc id="34071" url="http://en.wikipedia.org/wiki?curid=34071" title="Whitney Houston">
Whitney Houston

Whitney Elizabeth Houston (August 9, 1963 – February 11, 2012) was an American singer, actress, producer, and model. In 2009, "Guinness World Records" cited her as the most awarded female act of all time. Houston is one of pop music's best-selling music artists of all-time, with an estimated 170-200 million records sold worldwide. She released six studio albums, one holiday album and three movie soundtrack albums, all of which have diamond, multi-platinum, platinum or gold certification. Houston's crossover appeal on the popular music charts, as well as her prominence on MTV, starting with her video for "How Will I Know", influenced several African American women artists who follow in her footsteps.
Houston is the only artist to chart seven consecutive No. 1 "Billboard" Hot 100 hits. She is the second artist behind Elton John and the only woman to have two number-one "Billboard" 200 Album awards (formerly "Top Pop Albums") on the "Billboard" magazine year-end charts. Houston's 1985 debut album "Whitney Houston" became the best-selling debut album by a woman in history. "Rolling Stone" named it the best album of 1986, and ranked it at number 254 on the magazine's list of the 500 Greatest Albums of All Time. Her second studio album "Whitney" (1987) became the first album by a woman to debut at number one on the "Billboard" 200 albums chart.
Houston's first acting role was as the star of the feature film "The Bodyguard" (1992). The film's won the 1994 Grammy Award for Album of the Year. Its lead single, "I Will Always Love You", became the best-selling single by a woman in music history. With the album, Houston became the first act (solo or group, male or female) to sell more than a million copies of an album within a single week period under Nielsen SoundScan system. The album makes her the top female act in the top 10 list of the best-selling albums of all time, at number four. Houston continued to star in movies and contribute to their soundtracks, including the films "Waiting to Exhale" (1995) and "The Preacher's Wife" (1996). became the best-selling gospel album in history.
On February 11, 2012, Houston was found dead in her guest room at the Beverly Hilton, in Beverly Hills, California. The official coroner's report showed that she had accidentally drowned in the bathtub, with heart disease and cocaine use listed as contributing factors. News of her death coincided with the 2012 Grammy Awards and featured prominently in American and international media.
Life and career.
1963–84: Early life and career beginnings.
Whitney Houston was born on August 9, 1963 in what was then a middle-income neighborhood in Newark, New Jersey. She was the daughter of Army serviceman and entertainment executive John Russell Houston, Jr. (September 13, 1920 – February 2, 2003), and gospel singer Emily "Cissy" (Drinkard) Houston. Her elder brother Michael is a singer, and her elder half-brother is former basketball player Gary Garland. Her parents were both African American, and she was also said to have Native American and Dutch ancestry. Through her mother, Houston was a first cousin of singers Dionne Warwick and Dee Dee Warwick. Her godmother was Darlene Love and her honorary aunt was Aretha Franklin. She met her honorary aunt at age 8, or 9, when her mother took her to a recording studio. Houston was raised a Baptist, but was also exposed to the Pentecostal church. After the 1967 Newark riots, the family moved to a middle-class area in East Orange, New Jersey, when she was four.
At the age of 11, Houston started performing as a soloist in the junior gospel choir at the New Hope Baptist Church in Newark, where she also learned to play the piano. Her first solo performance in the church was "Guide Me, O Thou Great Jehovah". When Houston was a teenager, she attended Mount Saint Dominic Academy, a Catholic girls' high school in Caldwell, New Jersey, where she met her best friend Robyn Crawford, whom she described as the "sister she never had". While Houston was still in school, her mother continued to teach her how to sing. Houston was also exposed to the music of Chaka Khan, Gladys Knight, and Roberta Flack, most of whom would have an influence on her as a singer and performer.
Houston spent some of her teenage years touring nightclubs where her mother Cissy was performing, and she would occasionally get on stage and perform with her. In 1977, at age 14, she became a backup singer on the Michael Zager Band's single "Life's a Party". In 1978, at age 15, Houston sang background vocals on Chaka Khan's hit single "I'm Every Woman", a song she would later turn into a larger hit for herself on her monster-selling ' soundtrack album. She also sang back-up on albums by Lou Rawls and Jermaine Jackson.
In the early 1980s, Houston started working as a fashion model after a photographer saw her at Carnegie Hall singing with her mother. She appeared in "Seventeen" and became one of the first women of color to grace the cover of the magazine. She was also featured in layouts in the pages of "Glamour", "Cosmopolitan", "Young Miss", and appeared in a Canada Dry soft drink TV commercial. Her looks and girl-next-door charm made her one of the most sought after teen models of that time. While modeling, she continued her burgeoning recording career by working with producers Michael Beinhorn, Bill Laswell and Martin Bisi on an album they were spearheading called "One Down", which was credited to the group Material. For that project, Houston contributed the ballad "Memories", a cover of a song by Hugh Hopper of Soft Machine. Robert Christgau of "The Village Voice" called her contribution "one of the most gorgeous ballads you've ever heard." She also appeared as a lead vocalist on one track on a Paul Jabara album, entitled "Paul Jabara and Friends", released by Columbia Records in 1983.
Houston had previously been offered several recording agencies (Michael Zager in 1980, and Elektra Records in 1981), but her mother declined the offers stating her daughter must first complete high school. In 1983, Gerry Griffith, an A&R representative from Arista Records, saw her performing with her mother in a New York City nightclub and was impressed. He convinced Arista's head Clive Davis to make time to see Houston perform. Davis too was impressed and offered a worldwide recording contract which Houston signed. Later that year, she made her national televised debut alongside Davis on "The Merv Griffin Show".
Houston signed with Arista in 1983, but did not begin work on her album immediately. The label wanted to make sure no other label signed the singer away. Davis wanted to ensure he had the right material and producers for Houston's debut album. Some producers had to pass on the project due to prior commitments. Houston first recorded a duet with Teddy Pendergrass entitled "Hold Me" which appeared on his album, "Love Language". The single was released in 1984 and gave Houston her first taste of success, becoming a Top 5 R&B hit. It would also appear on her debut album in 1985.
1985–86: Rise to international prominence.
With production from Michael Masser, Kashif, Jermaine Jackson, and Narada Michael Walden, Houston's debut album "Whitney Houston" was released in February 1985. "Rolling Stone" magazine praised Houston, calling her "one of the most exciting new voices in years" while "The New York Times" called the album "an impressive, musically conservative showcase for an exceptional vocal talent." Arista Records promoted Houston's album with three different singles from the album in the US, UK and other European countries. In the UK, the dance-funk "Someone for Me", which failed to chart in the country, was the first single while "All at Once" was in such European countries as the Netherlands and Belgium, where the song reached the top 5 on the singles charts, respectively.
In the US, the soulful ballad "You Give Good Love" was chosen as the lead single from Houston's debut to establish her in the black marketplace first. Outside the US, the song failed to get enough attention to become a hit, but in the US, it gave the album its first major hit as it peaked at No. 3 on the US "Billboard" Hot 100 chart, and No. 1 on the Hot R&B chart. As a result, the album began to sell strongly, and Houston continued promotion by touring nightclubs in the US. She also began performing on late-night television talk shows, which were not usually accessible to unestablished black acts. The jazzy ballad "Saving All My Love for You" was released next and it would become Houston's first No. 1 single in both the US and the UK. She was then an opening act for singer Jeffrey Osborne on his nationwide tour. "Thinking About You" was released as the promo single only to R&B-oriented radio stations, which peaked at number ten on the US R&B Chart. At the time, MTV had received harsh criticism for not playing enough videos by black, Latino, and other racial minorities while favoring white acts. The third US single, "How Will I Know", peaked at No. 1 and introduced Houston to the MTV audience thanks to its video. Houston's subsequent singles from this, and future albums, would make her the first African-American woman to receive consistent heavy rotation on MTV.
By 1986, a year after its initial release, "Whitney Houston" topped the "Billboard" 200 albums chart and stayed there for 14 non-consecutive weeks. The final single, "Greatest Love of All", became Houston's biggest hit at the time after peaking No. 1 and remaining there for three weeks on the Hot 100 chart, which made her debut the first album by a woman to yield three No. 1 hits. Houston was No. 1 artist of the year and "Whitney Houston" was the No. 1 album of the year on the 1986 "Billboard" year-end charts, making her the first woman to earn that distinction. At the time, Houston released the best-selling debut album by a solo artist. Houston then embarked on her world tour, "Greatest Love Tour". The album had become an international success, and was certified 13× platinum (diamond) in the United States alone, and has sold 25 million copies worldwide.
At the 1986 Grammy Awards, Houston was nominated for three awards including Album of the Year. She was not eligible for the Best New Artist category due to her previous hit R&B duet recording with Teddy Pendergrass in 1984. She won her first Grammy Award for Best Pop Vocal Performance, Female for "Saving All My Love for You". Houston's performance of the song during the Grammy telecast later earned her an Emmy Award for Outstanding Individual Performance in a Variety or Music Program.
Houston won seven American Music Awards in total in 1986 and 1987, and an MTV Video Music Award. The album's popularity would also carry over to the 1987 Grammy Awards when "Greatest Love of All" would receive a Record of the Year nomination. Houston's debut album is listed as one of "Rolling Stone"‍ '​s 500 Greatest Albums of All Time and on The Rock & Roll Hall of Fame's Definitive 200 list. Houston's grand entrance into the music industry is considered one of the 25 musical milestones of the last 25 years, according to "USA Today". Following Houston's breakthrough, doors were opened for other African-American women such as Janet Jackson and Anita Baker to find notable success in popular music and on MTV.
1987–91: "Whitney", "I'm Your Baby Tonight" and "The Star Spangled Banner".
With many expectations, Houston's second album, "Whitney", was released in June 1987. The album again featured production from Masser, Kashif and Walden as well as Jellybean Benitez. Many critics complained that the material was too similar to her previous album. "Rolling Stone" said, "the narrow channel through which this talent has been directed is frustrating". Still, the album enjoyed commercial success. Houston became the first woman in music history to debut at number one on the "Billboard" 200 albums chart, and the first artist to enter the albums chart at number one in both the US and UK, while also hitting number one or top ten in dozens of other countries around the world. The album's first single, "I Wanna Dance with Somebody (Who Loves Me)", was also a massive hit worldwide, peaking at No. 1 on the "Billboard" Hot 100 chart and topping the singles chart in many countries such as Australia, Germany and the UK. The next three singles, "Didn't We Almost Have It All", "So Emotional", and "Where Do Broken Hearts Go" all peaked at number one on the US Hot 100 chart, which gave her a total of seven consecutive number one hits, breaking the record of six previously shared by The Beatles and the Bee Gees. Houston became the first woman to generate four number-one singles from one album. "Whitney" has been certified 9× Platinum in the US for shipments of over 9 million copies, and has sold a total of 20 million copies worldwide.
At the 30th Grammy Awards in 1988, Houston was nominated for three awards, including Album of the Year, winning her second Grammy for Best Female Pop Vocal Performance for "I Wanna Dance with Somebody (Who Loves Me)". Houston also won two American Music Awards in 1988 and 1989, respectively, and a Soul Train Music Award. Following the release of the album, Houston embarked on the "Moment of Truth World Tour", which was one of the ten highest grossing concert tours of 1987. The success of the tours during 1986–87 and her two studio albums ranked Houston No. 8 for the highest earning entertainers list according to "Forbes" magazine. She was the highest earning African-American woman overall and the third highest entertainer after Bill Cosby and Eddie Murphy.
Houston was a supporter of Nelson Mandela and the anti-apartheid movement. During her modeling days, the singer refused to work with any agencies who did business with the then-apartheid South Africa. On June 11, 1988, during the European leg of her tour, Houston joined other musicians to perform a set at Wembley Stadium in London to celebrate a then-imprisoned Nelson Mandela's 70th birthday. Over 72,000 people attended Wembley Stadium, and over a billion people tuned in worldwide as the rock concert raised over $1 million for charities while bringing awareness to apartheid. Houston then flew back to the US for a concert at Madison Square Garden in New York City in August. The show was a benefit concert that raised a quarter of a million dollars for the United Negro College Fund. In the same year, she recorded a song for NBC's coverage of the 1988 Summer Olympics, "One Moment in Time", which became a Top 5 hit in the US, while reaching number one in the UK and Germany. With her world tour continuing overseas, Houston was still one of the top 20 highest earning entertainers for 1987–88 according to "Forbes" magazine.
In 1989, Houston formed The Whitney Houston Foundation For Children, a non-profit organization that has raised funds for the needs of children around the world. The organization cares for homelessness, children with cancer or AIDS, and other issues of self-empowerment. With the success of her first two albums, Houston was undoubtedly an international crossover superstar, the most prominent since Michael Jackson, appealing to all demographics. However, some black critics believed she was "selling out". They felt her singing on record lacked the soul that was present during her live concerts.
At the 1989 Soul Train Music Awards, when Houston's name was called out for a nomination, a few in the audience jeered. Houston defended herself against the criticism, stating, "If you're gonna have a long career, there's a certain way to do it, and I did it that way. I'm not ashamed of it." Houston took a more urban direction with her third studio album, "I'm Your Baby Tonight", released in November 1990. She produced and chose producers for this album and as a result, it featured production and collaborations with L.A. Reid and Babyface, Luther Vandross, and Stevie Wonder. The album showed Houston's versatility on a new batch of tough rhythmic grooves, soulful ballads and up-tempo dance tracks. Reviews were mixed. "Rolling Stone" felt it was her "best and most integrated album". while "Entertainment Weekly", at the time thought Houston's shift towards an urban direction was "superficial".
The album contained several hits: the first two singles, "I'm Your Baby Tonight" and "All the Man That I Need" peaked at number one on the "Billboard" Hot 100 chart; "Miracle" peaked at number nine; "My Name Is Not Susan" peaked in the top twenty; "I Belong to You" reached the top ten of the US R&B chart and garnered Houston a Grammy nomination; and the sixth single, the Stevie Wonder duet "We Didn't Know", reached the R&B top twenty. The album peaked at number three on the "Billboard" 200 and went on to be certified 4× platinum in the US while selling twelve million total worldwide.
In 1990, Houston was the spokesperson for a youth leadership conference hosted in Washington, D.C. She had a private audience with President George H. W. Bush in the Oval Office to discuss the associated challenges.
During the Persian Gulf War, Houston performed "The Star Spangled Banner" at Super Bowl XXV at Tampa Stadium on January 27, 1991. This performance was later reported by those involved in the performance to have been lip synced or to have been sung into a dead microphone while a studio recording previously made by Houston was played. Dan Klores, a spokesman for Houston, explained: "This is not a Milli Vanilli thing. She sang live, but the microphone was turned off. It was a technical decision, partially based on the noise factor. This is standard procedure at these events." (See also Star Spangled Banner lip sync controversy.) A commercial single and video of her performance were released, and reached the Top 20 on the US Hot 100, making her the only act to turn the US national anthem into a pop hit of that magnitude (José Feliciano's version reached No. 50 in November 1968). Houston donated all her share of the proceeds to the American Red Cross Gulf Crisis Fund. As a result, the singer was named to the Red Cross Board of Governors.
Her rendition was critically acclaimed and is considered the benchmark for singers. "Rolling Stone" commented that "her singing stirs such strong patriotism. Unforgettable", and the performance ranked No. 1 on the 25 most memorable music moments in NFL history list. VH1 listed the performance as one of the greatest moments that rocked TV. Following the attacks on 9/11, it was released again by Arista Records, all profits going towards the firefighters and victims of the attacks. This time it peaked at No. 6 in the Hot 100 and was certified platinum by the Recording Industry Association of America.
Later in 1991, Houston put together her "Welcome Home Heroes" concert with HBO for the soldiers fighting in the Persian Gulf War and their families. The free concert took place at Naval Station Norfolk in Norfolk, Virginia in front of 3,500 servicemen and women. HBO descrambled the concert so that it was free for everyone to watch. Houston's concert gave HBO its highest ratings ever. She then embarked on the "I'm Your Baby Tonight World Tour".
1992–94: Marriage, motherhood, and "The Bodyguard".
Throughout the 1980s, Houston was romantically linked to American football star Randall Cunningham and actor Eddie Murphy, whom she dated. She then met R&B singer Bobby Brown at the 1989 Soul Train Music Awards. After a three-year courtship, the two were married on July 18, 1992. On March 4, 1993, Houston gave birth to their daughter Bobbi, the couple's only child. Brown would go on to have several run-ins with the law, including some jail time.
With the commercial success of her albums, movie offers poured in, including offers to work with Robert De Niro, Quincy Jones, and Spike Lee; but Houston felt the time wasn't right. Houston's first film role was in "The Bodyguard", released in 1992 and co-starring Kevin Costner. Houston played Rachel Marron, a star who is stalked by a crazed fan and hires a bodyguard to protect her. "USA Today" listed it as one of the 25 most memorable movie moments of the last 25 years in 2007. Houston's mainstream appeal allowed people to look at the movie color-blind.
Still, controversy arose as some felt the film's advertising intentionally hid Houston's face to hide the film's interracial relationship. In an interview with "Rolling Stone" in 1993, the singer commented that "people know who Whitney Houston is – I'm black. You can't hide that fact." Houston received a Razzie Award nomination for Worst Actress. "The Washington Post" said Houston is "doing nothing more than playing Houston, comes out largely unscathed if that is possible in so cockamamie an undertaking", and "The New York Times" commented that she lacked passion with her co-star. Despite the film's mixed reviews, it was hugely successful at the box office, grossing more than $121 million in the U.S. and $410 million worldwide, making it one of the top 100 grossing films in film history at its time of release, though it is no longer in the top 100 due to rising ticket prices since the time the film was released.
The film's soundtrack also enjoyed big success. Houston executive produced and contributed six songs for the motion picture's . "Rolling Stone" said it is "nothing more than pleasant, tasteful and urbane". The soundtrack's lead single was "I Will Always Love You", written and originally recorded by Dolly Parton in 1974. Houston's version of the song was acclaimed by many critics, regarding it as her "signature song" or "iconic performance". "Rolling Stone" and "USA Today" called her rendition "the tour-de-force". The single peaked at number one on the "Billboard" Hot 100 for a then-record-breaking 14 weeks, number one on the R&B chart for a then-record-breaking 11 weeks, and number one on the Adult Contemporary charts for five weeks.
The single was certified 4× platinum by the RIAA, making Houston the first woman with a single to reach that level in the RIAA history and becoming the best-selling single by a woman in the US.
The song also became a global success, hitting number-one in almost all countries, and the best-selling single of all time by a female solo artist with 20 million copies sold. The soundtrack topped the "Billboard" 200 chart and remained there for 20 non-consecutive weeks, the longest tenure by any album on the chart in the Nielsen SoundScan era, and became one of the fastest selling albums ever. During Christmas week of 1992, the soundtrack sold over a million copies within a week, becoming the first album to achieve that feat under Nielsen SoundScan system. With the follow-up singles "I'm Every Woman", a Chaka Khan cover, and "I Have Nothing" both reaching the top five, Houston became the first woman to ever have three singles in the Top 11 simultaneously. The album was certified 17× platinum in the US alone, with worldwide sales of 44 million, making "The Bodyguard" the biggest-selling album by a female act on the list of the world's Top 10 best-selling albums, topping Shania Twain's 40 million sold for "Come On Over".
Houston won three Grammys for the album in 1994, including two of the Academy's highest honors, Album of the Year and Record of the Year. In addition, she won a record 8 American Music Awards at that year's ceremony including the Award of Merit, 11 Billboard Music Awards, 3 Soul Train Music Awards in 1993–94 including Sammy Davis, Jr. Award as Entertainer of the Year, 5 NAACP Image Awards including Entertainer of the Year, a record 5 World Music Awards, and a BRIT award. Following the success of the project, Houston embarked on another expansive global tour, "The Bodyguard World Tour", in 1993–94. Her concerts, movie, and recording grosses made her the third highest earning female entertainer of 1993–94, just behind Oprah Winfrey and Barbra Streisand according to "Forbes" magazine. Houston placed in the top five of "Entertainment Weekly"‍ '​s annual "Entertainer of the Year" ranking and was labeled by "Premiere" magazine as one of the 100 most powerful people in Hollywood.
In October 1994, Houston attended and performed at a state dinner in the White House honoring newly elected South African president Nelson Mandela. At the end of her world tour, Houston performed three concerts in South Africa to honor President Mandela, playing to over 200,000 people. This would make the singer the first major musician to visit the newly unified and apartheid free nation following Mandela's winning election. The concert was broadcast live on HBO with funds of the concerts being donated to various charities in South Africa. The event was considered the nation's "biggest media event since the inauguration of Nelson Mandela".
1995–97: "Waiting to Exhale", "The Preacher's Wife", and "Cinderella".
In 1995, Houston starred alongside Angela Bassett, Loretta Devine, and Lela Rochon in her second film, "Waiting to Exhale", a motion picture about four African-American women struggling with relationships. Houston played the lead character Savannah Jackson, a TV producer in love with a married man. She chose the role because she saw the film as "a breakthrough for the image of black women because it presents them both as professionals and as caring mothers". After opening at number one and grossing $67 million in the US at the box office and $81 million worldwide, it proved that a movie primarily targeting a black audience can cross over to success, while paving the way for other all-black movies such as "How Stella Got Her Groove Back" and the Tyler Perry movies that became popular in the 2000s. The film is also notable for its portrayal of black women as strong middle class citizens rather than as stereotypes. The reviews were mainly positive for the ensemble cast. "The New York Times" said: "Ms. Houston has shed the defensive hauteur that made her portrayal of a pop star in 'The Bodyguard' seem so distant." Houston was nominated for an NAACP Image Award for "Outstanding Actress in a Motion Picture", but lost to her co-star Bassett.
The film's accompanying soundtrack, "", was produced by Houston and Babyface. Though Babyface originally wanted Houston to record the entire album, she declined. Instead, she "wanted it to be an album of women with vocal distinction", and thus gathered several African-American female artists for the soundtrack, to go along with the film's message about strong women. Consequently, the album featured a range of contemporary R&B female recording artists along with Houston, such as Mary J. Blige, Brandy, Toni Braxton, Aretha Franklin, and Patti LaBelle. Houston's "Exhale (Shoop Shoop)" peaked at No. 1, and then spent a record eleven weeks at the No. 2 spot and eight weeks on top of the R&B Charts. "Count On Me", a duet with CeCe Winans, hit the US Top 10; and Houston's third contribution, "Why Does It Hurt So Bad", made the Top 30. The album debuted at No. 1, and was certified 7× Platinum in the United States, denoting shipments of seven million copies. The soundtrack received strong reviews; as "Entertainment Weekly" stated: "the album goes down easy, just as you'd expect from a package framed by Whitney Houston tracks... the soundtrack waits to exhale, hovering in sensuous suspense" and has since ranked it as one of the 100 Best Movie Soundtracks. Later that year, Houston's children's charity organization was awarded a VH1 Honor for all the charitable work.
In 1996, Houston starred in the holiday comedy "The Preacher's Wife", with Denzel Washington. She plays a gospel-singing wife of a pastor (Courtney B. Vance). It was largely an updated remake of the film "The Bishop's Wife" (1948 in film|1948), which starred Loretta Young, David Niven, and Cary Grant. Houston earned $10 million for the role, making her one of the highest-paid actresses in Hollywood at the time and the highest earning African-American actress in Hollywood. The movie, with its all African-American cast, was a moderate success, earning approximately $50 million at the U.S. box offices. The movie gave Houston her strongest reviews so far. "The San Francisco Chronicle" said Houston "is rather angelic herself, displaying a divine talent for being virtuous and flirtatious at the same time", and she "exudes gentle yet spirited warmth, especially when praising the Lord in her gorgeous singing voice". Houston was again nominated for an NAACP Image Award and won for Outstanding Actress in a Motion Picture.
Houston recorded and co-produced, with Mervyn Warren, the film's accompanying gospel soundtrack. "" included six gospel songs with Georgia Mass Choir that were recorded at the Great Star Rising Baptist Church in Atlanta. Houston also duetted with gospel legend Shirley Caesar. The album sold six million copies worldwide and scored hit singles with "I Believe in You and Me" and "Step by Step", becoming the largest selling gospel album of all time. The album received mainly positive reviews. Some critics, such as that of "USA Today", noted the presence of her emotional depth, while "The Times" said, "To hear Houston going at full throttle with the 35 piece Georgia Mass Choir struggling to keep up is to realise what her phenomenal voice was made for".
In 1997, Houston's production company changed its name to BrownHouse Productions and was joined by Debra Martin Chase. Their goal was "to show aspects of the lives of African-Americans that have not been brought to the screen before" while improving how African-Americans are portrayed in film and television. Their first project was a made-for-television remake of Rodgers & Hammerstein's "Cinderella". In addition to co-producing, Houston starred in the movie as the Fairy Godmother along with Brandy, Jason Alexander, Whoopi Goldberg, and Bernadette Peters. Houston was initially offered the role of Cinderella in 1993, but other projects intervened. The film is notable for its multi-racial cast and nonstereotypical message. An estimated 60 million viewers tuned into the special giving ABC its highest TV ratings in 16 years. The movie received seven Emmy nominations including Outstanding Variety, Musical or Comedy, while winning Outstanding Art Direction in a Variety, Musical or Comedy Special.
Houston and Chase then obtained the rights to the story of Dorothy Dandridge. Houston was to play Dandridge, who was the first African American actress to be nominated for an Oscar. Houston wanted the story told with dignity and honor. However, Halle Berry also had rights to the project and got her version going first. Later that year, Houston paid tribute to her idols, such as Aretha Franklin, Diana Ross, and Dionne Warwick, by performing their hits during the three-night HBO Concert "Classic Whitney Live from Washington, D.C.". The special raised over $300,000 for the Children's Defense Fund. Houston received the Quincy Jones Award for outstanding career achievements in the field of entertainment at the 12th Soul Train Music Awards.
1998–2000: "My Love Is Your Love" and "Whitney: The Greatest Hits".
After spending much of the early and mid-1990s working on motion pictures and their soundtrack albums, Houston's first studio album in eight years, the critically acclaimed "My Love Is Your Love", was released in November 1998. Though originally slated to be a greatest hits album with a handful of new songs, recording sessions were so fruitful that a new full-length studio album was released. Recorded and mixed in only six weeks, it featured production from Rodney Jerkins, Wyclef Jean and Missy Elliott. The album debuted at number thirteen, its peak position, on the "Billboard" 200 chart. It had a funkier and edgier sound than past releases and saw Houston handling urban dance, hip hop, mid-tempo R&B, reggae, torch songs, and ballads all with great dexterity.
From late 1998 to early 2000, the album spawned several hit singles: "When You Believe" (US No. 15, UK No. 4), a duet with Mariah Carey for 1998's "The Prince of Egypt" soundtrack, which also became an international hit as it peaked in the Top 10 in several countries and won an Academy Award for Best Original Song; "Heartbreak Hotel" (US No. 2, UK No. 25) featured Faith Evans and Kelly Price, received a 1999 MTV VMA nomination for Best R&B Video, and number one on the US R&B chart for seven weeks; "It's Not Right but It's Okay" (US No. 4, UK No. 3) won Houston her sixth Grammy Award for Best Female R&B Vocal Performance; "My Love Is Your Love" (US No. 4, UK No. 2) with 3 million copies sold worldwide; and "I Learned from the Best" (US No. 27, UK No. 19). These singles became international hits as well, and all the singles, except "When You Believe", became number one hits on the "Billboard" Hot Dance/Club Play chart. The album sold four million copies in America, making it certified 4× platinum, and a total of eleven million copies worldwide.
The album gave Houston some of her strongest reviews ever. "Rolling Stone" said Houston was singing "with a bite in her voice" and "The Village Voice" called it "Whitney's sharpest and most satisfying so far". In 1999, Houston participated in VH-1's Divas Live '99, alongside Brandy, Mary J. Blige, Tina Turner, and Cher. The same year, Houston hit the road with her 70 date "My Love Is Your Love World Tour". The European leg of the tour was Europe's highest grossing arena tour of the year. In November 1999, Houston was named Top-selling R&B Female Artist of the Century with certified US sales of 51 million copies at the time and "The Bodyguard Soundtrack" was named the Top-selling Soundtrack Album of the Century by the Recording Industry Association of America (RIAA). She also won The Artist of the Decade, Female award for extraordinary artistic contributions during the 1990s at the 14th Soul Train Music Awards, and an MTV Europe Music Award for Best R&B.
In May 2000, "" was released worldwide. The double disc set peaked at number five in the United States, reaching number one in the United Kingdom. In addition, the album reached the Top 10 in many other countries. While ballad songs were left unchanged, the album features house/club remixes of many of Houston's up-tempo hits. Included on the album were four new songs: "Could I Have This Kiss Forever" (a duet with Enrique Iglesias), "Same Script, Different Cast" (a duet with Deborah Cox), "If I Told You That" (a duet with George Michael), and "Fine", and three hits that had never appeared on a Houston album: "One Moment in Time", "The Star Spangled Banner", and "If You Say My Eyes Are Beautiful", a duet with Jermaine Jackson from his 1986 "Precious Moments" album. Along with the album, an accompanying VHS and DVD was released featuring the music videos to Houston's greatest hits, as well as several hard-to-find live performances including her 1983 debut on "The Merv Griffin Show", and interviews. The greatest hits album was certified 3× platinum in the US, with worldwide sales of 10 million.
2000–05: "Just Whitney" and personal struggles.
Though Houston was seen as a "good girl" with a perfect image in the 1980s and early 1990s, by the late 1990s, her behavior changed. She was often hours late for interviews, photo shoots and rehearsals, and canceling concerts and talk-show appearances. With the missed performances and weight loss, rumors about Houston using drugs with her husband circulated. On January 11, 2000, airport security guards discovered marijuana in both Houston's and husband Bobby Brown's luggage at a Hawaii airport, but the two boarded the plane and departed before authorities could arrive. Charges were later dropped against them, but rumors of drug usage between the couple would continue to surface. Two months later, Clive Davis was inducted into the Rock & Roll Hall of Fame. Houston had been scheduled to perform at the event, but failed to show up.
Shortly thereafter, Houston was scheduled to perform at the Academy Awards but was fired from the event by musical director and longtime friend Burt Bacharach. Her publicist cited throat problems as the reason for the cancellation. In his book "The Big Show: High Times and Dirty Dealings Backstage at the Academy Awards", author Steve Pond revealed that "Houston's voice was shaky, she seemed distracted and jittery, and her attitude was casual, almost defiant", and that while Houston was to sing "Over the Rainbow", she would start singing a different song. Houston later admitted to having been fired. Later that year, Houston's long-time executive assistant and friend, Robyn Crawford, resigned from Houston's management company.
In August 2001, Houston signed the biggest record deal in music history with Arista/BMG. She renewed her contract for $100 million to deliver six new albums, on which she would also earn royalties. She later made an appearance on '. Her extremely thin frame further spurred rumors of drug use. Houston's publicist said, "Whitney has been under stress due to family matters, and when she is under stress she doesn't eat." The singer was scheduled for a second performance the following night but canceled. Within weeks, Houston's rendition of "The Star Spangled Banner" would be re-released after the September 11 attacks, with the proceeds donated to the New York Firefighters 9/11 Disaster Relief Fund and the New York Fraternal Order of Police. The song peaked at No. 6 this time on the US Hot 100, topping its previous position.
In 2002, Houston became involved in a legal dispute with John Houston Enterprise. Although the company was started by her father to manage her career, it was actually run by company president Kevin Skinner. Skinner filed a breach-of-contract lawsuit and sued for $100 million (but lost), stating that Houston owed the company previously unpaid compensation for helping to negotiate her $100 million contract with Arista Records and for sorting out legal matters. Houston stated that her 81-year-old father had nothing to do with the lawsuit. Although Skinner tried to claim otherwise, John Houston never appeared in court. Houston's father later died in February 2003. The lawsuit was dismissed on April 5, 2004, and Skinner was awarded nothing.
Also in 2002, Houston did an interview with Diane Sawyer to promote her then-upcoming album. During the prime-time special, Houston spoke on topics including rumored drug use and marriage. She was asked about the ongoing drug rumors and replied, "First of all, let's get one thing straight. Crack is cheap. I make too much money to ever smoke crack. Let's get that straight. Okay? We don't do crack. We don't do that. Crack is whack." The line was from Keith Haring's mural which was painted in 1986 on the handball court at 128th Street and 2nd Avenue. Houston did, however, admit to using other substances at times, including cocaine.
In December 2002, Houston released her fifth studio album, "Just Whitney...". The album included productions from then-husband Bobby Brown, as well as Missy Elliott and Babyface, and marked the first time that Houston did not produce with Clive Davis as Davis had been released by top management at BMG. Upon its release, "Just Whitney..." received mixed reviews. The album debuted at number 9 on the "Billboard" 200 chart and it had the highest first week sales of any album Houston had ever released. The four singles released from the album did not fare well on the "Billboard" Hot 100, but became dance chart hits. "Just Whitney..." was certified platinum in the United States, and sold approximately three million worldwide.
On a June 2003 trip to Israel, Houston said of her visit, "I've never felt like this in any other country. I feel at home, I feel wonderful."
In late 2003, Houston released her first Christmas album "", with a collection of traditional holiday songs. Houston produced the album with Mervyn Warren and Gordon Chambers. A single titled "One Wish (for Christmas)" reached the Top 20 on the Adult Contemporary chart, and the album was certified gold in the US. Having always been a touring artist, Houston spent most of 2004 touring and performing in Europe, the Middle East, Asia, and Russia. In September 2004, she gave a surprise performance at the World Music Awards in a tribute to long-time friend Clive Davis. After the show, Davis and Houston announced plans to go into the studio to work on her new album.
In early 2004, husband Bobby Brown starred in his own reality TV program, "Being Bobby Brown" on the Bravo network, which provided a view into the domestic goings-on in the Brown household. Though it was Brown's vehicle, Houston was a prominent figure throughout the show, receiving as much screen time as Brown. The series aired in 2005 and featured Houston in, what some would say, not her most flattering moments. "The Hollywood Reporter" said it was "undoubtedly the most disgusting and execrable series ever to ooze its way onto television." Despite the perceived train-wreck nature of the show, the series gave Bravo its highest ratings in its time slot and continued Houston's successful forays into film and television. The show was not renewed for a second season after Houston stated that she would no longer appear in it, and Brown and Bravo could not come to an agreement for another season.
2006–12: Return to music, "I Look to You", tour and film comeback.
After years of controversy and turmoil, Houston separated from Bobby Brown in September 2006, filing for divorce the following month. On February 1, 2007, Houston asked the court to fast track their divorce. The divorce was finalized on April 24, 2007, with Houston granted custody of the couple's daughter. On May 4, Houston sold the suburban Atlanta home featured in "Being Bobby Brown" for $1.19 million. A few days later, Brown sued Houston in Orange County, California court in an attempt to change the terms of their custody agreement. Brown also sought child and spousal support from Houston. In the lawsuit, Brown claimed that financial and emotional problems prevented him from properly responding to Houston's divorce petition. Brown lost at his court hearing as the judge dismissed his appeal to overrule the custody terms, leaving Houston with full custody and Brown with no spousal support. In March 2007, Clive Davis of Arista Records announced that Houston would begin recording a new album. In October 2007, Arista released another compilation "The Ultimate Collection" outside the United States.
Houston gave her first interview in seven years in September 2009, appearing on Oprah Winfrey's season premiere. The interview was billed as "the most anticipated music interview of the decade". Whitney admitted on the show to using drugs with former husband Bobby Brown, who "laced marijuana with rock cocaine". She told Oprah that before "The Bodyguard" her drug use was light, but after the film's success and the birth of her daughter it got heavier, and by 1996 "[doing drugs] was an everyday thing... I wasn't happy by that point in time. I was losing myself."
Houston released her new album, "I Look to You", in August 2009. The album's first two singles were the title track "I Look to You" and "Million Dollar Bill". The album entered the "Billboard" 200 at No. 1, with Houston's best opening week sales of 305,000 copies, marking Houston's first number one album since "The Bodyguard", and Houston's first studio album to reach number one since 1987's "Whitney". Houston also appeared on European television programs to promote the album. She performed the song "I Look to You" on the German television show "Wetten, dass..?". Three days later, she performed the worldwide first single from "I Look to You", "Million Dollar Bill", on the French television show "Le Grand Journal". Houston appeared as guest mentor on "The X Factor" in the United Kingdom. She performed "Million Dollar Bill" on the following day's results show, completing the song even as a strap in the back of her dress popped open two minutes into the performance. She later commented that she "sang [herself] out of [her] clothes".
The performance was poorly received by the British media, and was variously described as "weird" and "ungracious", "shambolic" and a "flop". Despite this reception, "Million Dollar Bill" jumped to its peak from 14 to number 5 (her first UK top 5 for over a decade), and three weeks after release "I Look to You" went gold. Houston appeared on the Italian version of "The X Factor", also performing "Million Dollar Bill", this time to excellent reviews. Houston was later awarded a Gold certificate for achieving over 50,000 CD sales of "I Look to You" in Italy. In November, Houston performed "I Didn't Know My Own Strength" at the 2009 American Music Awards in Los Angeles, California. Two days later, Houston performed "Million Dollar Bill" and "I Wanna Dance with Somebody (Who Loves Me)" on the "Dancing with the Stars" season 9 finale. As of December 2009, "I Look to You" has been certified platinum by the RIAA for sales of more than one million copies in the United States. On January 26, 2010, her debut album was re-released in a special edition entitled "Whitney Houston – The Deluxe Anniversary Edition".
Houston later embarked on a world tour, entitled the Nothing but Love World Tour. It was her first world tour in over ten years and was announced as a triumphant comeback. However, some poor reviews and rescheduled concerts brought some negative media attention. Houston canceled some concerts due to illness and received widespread negative reviews from fans who were disappointed in the quality of her voice and performance. Some fans reportedly walked out of her concerts.
In January 2010, Houston was nominated for two NAACP Image Awards, one for Best Female Artist and one for Best Music Video. She won the award for Best Music Video for her single "I Look to You". On January 16, she received The BET Honors Award for Entertainer citing her lifetime achievements spanning over 25 years in the industry. The 2010 BET Honors award was held at the Warner Theatre in Washington, D.C. and aired on February 1, 2010. Jennifer Hudson and Kim Burrell performed in honor of her, garnering positive reviews. Houston also received a nomination from the Echo Awards, Germany's version of the Grammys, for Best International Artist. In April 2010, the UK newspaper "The Mirror" reported that Houston was thinking about recording her eighth studio album and wanted to collaborate with will.i.am (of The Black Eyed Peas), her first choice for a collaboration.
Houston also performed the song "I Look to You" on the 2011 BET "Celebration of Gospel", with gospel–jazz singer Kim Burrell, held at the Staples Center, Los Angeles. The performance aired on January 30, 2011. Early in 2011, she gave an uneven performance in tribute to cousin Dionne Warwick at music mogul Clive Davis' annual pre-Grammy gala. In May 2011, Houston enrolled in a rehabilitation center again, as an out-patient, citing drug and alcohol problems. A representative for Houston said that it was a part of Houston's "longstanding recovery process".
In September 2011, "The Hollywood Reporter" announced that Houston would produce and star alongside Jordin Sparks and Mike Epps in the remake of the 1976 film "Sparkle". In the film, Houston portrays Sparks' "not-so encouraging" mother. Houston is also credited as an executive producer of the film. Debra Martin Chase, producer of "Sparkle", stated that Houston deserved the title considering she had been there from the beginning in 2001, when Houston obtained "Sparkle" production rights. R&B singer Aaliyah – originally tapped to star as Sparkle – died in a 2001 plane crash. Her death derailed production, which would have begun in 2002. Houston's remake of "Sparkle" was filmed in the fall of 2011 over a two-month period, and was released by TriStar Pictures. On May 21, 2012, "Celebrate", the last song Houston recorded with Sparks, premiered at RyanSeacrest.com. It was made available for digital download on iTunes on June 5. The song was featured on the "" soundtrack as the first official single. The movie was released on August 17, 2012 in the United States. The accompanying music video for "Celebrate" was filmed on May 30, 2012. The video was shot over 2 days, and a sneak peek of the video premiered on "Entertainment Tonight" on June 4, 2012.
Death.
On February 9, 2012, Houston visited singers Brandy and Monica, together with Clive Davis, at their rehearsals for Davis' pre-Grammy Awards party at The Beverly Hilton hotel in Beverly Hills. That same day, she made her last public performance, when she joined Kelly Price on stage in Hollywood, California, and sang "Jesus Loves Me".
Two days later, on February 11, Houston was found unconscious in Suite 434 at the Beverly Hilton Hotel, submerged in the bathtub. Beverly Hills paramedics arrived at approximately 3:30 p.m. and found the singer unresponsive and performed CPR. Houston was pronounced dead at 3:55 p.m. PST. The cause of death was not immediately known. Local police said there were "no obvious signs of criminal intent." On March 22, 2012, the Los Angeles County coroner's office reported the cause of Houston's death was drowning and the "effects of atherosclerotic heart disease and cocaine use". The office stated the amount of cocaine found in Houston's body indicated that she used the substance shortly before her death. Toxicology results revealed additional drugs in her system: diphenhydramine, alprazolam, cannabis and cyclobenzaprine. The manner of death was listed as an "accident".
Houston had an invitation-only memorial on Saturday, February 18, 2012, at the New Hope Baptist Church in Newark, New Jersey. The service was scheduled for two hours, but lasted four. Among those who performed at the funeral were Stevie Wonder (rewritten version of "Ribbon in the Sky", and "Love's in Need of Love Today"), CeCe Winans ("Don't Cry", and "Jesus Loves Me"), Alicia Keys ("Send Me an Angel"), Kim Burrell (rewritten version of "A Change Is Gonna Come"), and R. Kelly ("I Look to You"). The performances were interspersed with hymns by the church choir and remarks by Clive Davis, Houston's record producer; Kevin Costner; Rickey Minor, her music director; her cousin, Dionne Warwick; and Ray Watson, her security guard for the past 11 years. Aretha Franklin was listed on the program and was expected to sing, but was unable to attend the service. Bobby Brown, Houston's ex-husband, was also invited to the funeral but he left before the service began. Houston was buried on Sunday, February 19, 2012, in Fairview Cemetery, in Westfield, New Jersey, next to her father, John Russell Houston, who died in 2003. In June 2012, the McDonald's Gospelfest in Newark became a tribute to Houston.
Reaction.
Pre-Grammy party.
The Clive Davis' pre-Grammy party that Houston was expected to attend, which featured many of the biggest names in music and movies, went on as scheduled although it was quickly turned into a tribute to Houston. Davis spoke about Houston's death at the evening's start: "By now you have all learned of the unspeakably tragic news of our beloved Whitney's passing. I don't have to mask my emotion in front of a room full of so many dear friends. I am personally devastated by the loss of someone who has meant so much to me for so many years. Whitney was so full of life. She was so looking forward to tonight even though she wasn't scheduled to perform. Whitney was a beautiful person and a talent beyond compare. She graced this stage with her regal presence and gave so many memorable performances here over the years. Simply put, Whitney would have wanted the music to go on and her family asked that we carry on."
Tony Bennett spoke of Houston's death before performing at Davis' party. He said, "First, it was Michael Jackson, then Amy Winehouse, now, the magnificent Whitney Houston." Bennett sang "How Do You Keep the Music Playing?" and said of Houston, "When I first heard her, I called Clive Davis and said, 'You finally found the greatest singer I've ever heard in my life.'"
Some celebrities opposed Davis' decision to continue on the party while a police investigation was being conducted in Houston's hotel room and her body was still in the building. Chaka Khan, in an interview with CNN's Piers Morgan on February 13, 2012, shared that she felt the party should have been canceled, saying: "I thought that was complete insanity. And knowing Whitney I don't believe that she would have said 'the show must go on.' She's the kind of woman that would've said 'Stop everything! Un-unh. I'm not going to be there.' [...] I don't know what could motivate a person to have a party in a building where the person whose life he had influenced so enormously and whose life had been affected by hers. They were like... I don't understand how that party went on." Sharon Osbourne condemned the Davis party, declaring: "I think it was disgraceful that the party went on. I don't want to be in a hotel room when there's someone you admire who's tragically lost their life four floors up. I'm not interested in being in that environment and I think when you grieve someone, you do it privately, you do it with people who understand you. I thought it was so wrong."
Further reaction and tributes.
Many other celebrities released statements responding to Houston's death. Darlene Love, Houston's godmother, hearing the news of her death, said, "It felt like I had been struck by a lightning bolt in my gut." Dolly Parton, whose song "I Will Always Love You" was covered by Houston, said, "I will always be grateful and in awe of the wonderful performance she did on my song, and I can truly say from the bottom of my heart, 'Whitney, I will always love you. You will be missed.'" Aretha Franklin said, "It's so stunning and unbelievable. I couldn't believe what I was reading coming across the TV screen." Others paying tribute included Mariah Carey, Quincy Jones and Oprah Winfrey.
Moments after news of her death emerged, CNN, MSNBC and Fox News all broke from their regularly scheduled programming to dedicate time to non-stop coverage of Houston's death. All three featured live interviews with people who had known Houston including those that had worked with her, interviewed her along with some of her peers in the music industry. "Saturday Night Live" displayed a photo of a smiling Houston, alongside Molly Shannon, from her 1996 appearance. MTV and VH-1 interrupted their regularly scheduled programming on Sunday February 12 to air many of Houston's classic videos with MTV often airing news segments in between and featuring various reactions from fans and celebrities.
Houston's former husband, Bobby Brown, was reported to be "in and out of crying fits" since receiving the news. He did not cancel a scheduled performance and within hours of his ex-wife's sudden death, an audience in Mississippi observed as Brown blew kisses skyward, tearfully saying: "I love you, Whitney."
Ken Ehrlich, executive producer of the 54th Grammy Awards, announced that Jennifer Hudson would perform a tribute to Houston at the February 12, 2012 ceremony. He said "event organizers believed Hudson – an Academy Award-winning actress and Grammy Award-winning artist – could perform a respectful musical tribute to Houston". Ehrlich went on to say: "It's too fresh in everyone's memory to do more at this time, but we would be remiss if we didn't recognize Whitney's remarkable contribution to music fans in general, and in particular her close ties with the Grammy telecast and her Grammy wins and nominations over the years". At the start of the awards ceremony, footage of Houston performing "I Will Always Love You" from the 1994 Grammys was shown following a prayer read by host LL Cool J. Later in the program, following a montage of photos of musicians who died in 2011 with Houston singing "Saving All My Love for You" at the 1986 Grammys, Hudson paid tribute to Houston and the other artists by performing "I Will Always Love You". The tribute was partially credited for the Grammys telecast getting its second highest ratings in history.
Houston was honored in the form of various tributes at the 43rd NAACP Image Awards, held on February 17. An image montage of Houston and important black figures who died in 2011 was followed by video footage from the 1994 ceremony, which depicted her accepting two Image Awards for outstanding female artist and entertainer of the year. Following the video tribute, Yolanda Adams delivered a rendition of "I Love the Lord" from "The Preacher's Wife Soundtrack". In the finale of the ceremony, Kirk Franklin and the Family started their performance with "The Greatest Love of All". The 2012 BRIT Awards, which took place at London's O2 Arena on February 21, also paid tribute to Houston by playing a 30-second video montage of her music videos with a snippet of "One Moment in Time" as the background music in the ceremony's first segment. New Jersey Governor Chris Christie said that all New Jersey state flags would be flown at half-staff on Tuesday, February 21 to honor Houston. Houston was also featured, alongside other recently deceased figures from the movie industry, in the "In Memoriam" montage at the 84th Academy Awards on February 26, 2012.
Artistry and legacy.
Voice.
Houston was a mezzo-soprano, and was commonly referred to as "The Voice" in reference to her exceptional vocal talent. She was third in MTV's list of 22 Greatest Voices, and sixth on "Online Magazine COVE"‍ '​s list of the 100 Best Pop Vocalists with a score of 48.5/50. Jon Pareles of "The New York Times" stated she "always had a great big voice, a technical marvel from its velvety depths to its ballistic middle register to its ringing and airy heights". In 2008, "Rolling Stone" listed Houston as the thirty-fourth of the 100 greatest singers of all time, stating, "Her voice is a mammoth, coruscating cry: Few vocalists could get away with opening a song with 45 unaccompanied seconds of singing, but Houston's powerhouse version of Dolly Parton's 'I Will Always Love You' is a tour de force." Matthew Perpetua from "Rolling Stone" also eulogized Houston's vocal, enumerating ten performances, including "How Will I Know" from the 1986 MTV VMAs and "The Star Spangled Banner" at the 1991 Super Bowl. "Whitney Houston was blessed with an astonishing vocal range and extraordinary technical skill, but what truly made her a great singer was her ability to connect with a song and drive home its drama and emotion with incredible precision," he stated. "She was a brilliant performer, and her live shows often eclipsed her studio recordings."
Jon Caramanica of "The New York Times" commented, "Her voice was clean and strong, with barely any grit, well suited to the songs of love and aspiration. [...] Hers was a voice of triumph and achievement, and it made for any number of stunning, time-stopping vocal performances." Mariah Carey stated, "She [Whitney] has a really rich, strong mid-belt that very few people have. She sounds really good, really strong." While in her review of "I Look to You", music critic Ann Powers of the "Los Angeles Times" writes, "[Houston's voice] stands like monuments upon the landscape of 20th century pop, defining the architecture of their times, sheltering the dreams of millions and inspiring the climbing careers of countless imitators", adding "When she was at her best, nothing could match her huge, clean, cool mezzo-soprano."
Lauren Everitt from BBC News Magazine commented on melisma used in Houston's recording and its influence. "An early 'I' in Whitney Houston's 'I Will Always Love You' takes nearly six seconds to sing. In those seconds the former gospel singer-turned-pop star packs a series of different notes into the single syllable," stated Everitt. "The technique is repeated throughout the song, most pronouncedly on every 'I' and 'you'. The vocal technique is called melisma, and it has inspired a host of imitators. Other artists may have used it before Houston, but it was her rendition of Dolly Parton's love song that pushed the technique into the mainstream in the 90s. [...] But perhaps what Houston nailed best was moderation." Everitt said that "[i]n a climate of reality shows ripe with 'oversinging,' it's easy to appreciate Houston's ability to save melisma for just the right moment."
Houston's vocal stylings have had a significant impact on the music industry. According to Linda Lister in "Divafication: The Deification of Modern Female Pop Stars", she has been called the "Queen of Pop" for her influence during the 1990s, commercially rivaling Mariah Carey and Celine Dion. Stephen Holden from "The New York Times", in his review of Houston's Radio City Music Hall concert on July 20, 1993, praised her attitude as a singer, writing, "Whitney Houston is one of the few contemporary pop stars of whom it might be said: the voice suffices. While almost every performer whose albums sell in the millions calls upon an entertainer's bag of tricks, from telling jokes to dancing to circus pyrotechnics, Ms. Houston would rather just stand there and sing." With regard to her singing style, he added: "Her [Houston's] stylistic trademarks – shivery melismas that ripple up in the middle of a song, twirling embellishments at the ends of phrases that suggest an almost breathless exhilaration – infuse her interpretations with flashes of musical and emotional lightning."
Elysa Gardner of the "Los Angeles Times" in her review for "The Preacher's Wife Soundtrack" praised Houston's vocal ability highly, commenting, "She is first and foremost a pop diva – at that, the best one we have. No other female pop star – not Mariah Carey, not Celine Dion, not Barbra Streisand – quite rivals Houston in her exquisite vocal fluidity and purity of tone, and her ability to infuse a lyric with mesmerizing melodrama."
Influence.
During the 1980s, MTV was coming into its own and received criticism for not playing enough videos by black artists. With Michael Jackson breaking down the color barrier for black men, Houston did the same for black women. She became the first black woman to receive heavy rotation on the network following the success of the "How Will I Know" video. Following Houston's breakthrough, other African-American women, such as Janet Jackson and Anita Baker, were successful in popular music. Baker commented that "Because of what Whitney and Sade did, there was an opening for me... For radio stations, black women singers aren't taboo anymore."
AllMusic noted her contribution to the success of black artists on the pop scene, commenting, "Houston was able to handle big adult contemporary ballads, effervescent, stylish dance-pop, and slick urban contemporary soul with equal dexterity" and that "the result was an across-the-board appeal that was matched by scant few artists of her era, and helped her become one of the first black artists to find success on MTV in Michael Jackson's wake". "The New York Times" stated that "Houston was a major catalyst for a movement within black music that recognized the continuity of soul, pop, jazz and gospel vocal traditions". Richard Corliss of "Time" magazine commented on her initial success breaking various barriers:Of her first album's ten cuts, six were ballads. This chanteuse [Houston] had to fight for air play with hard rockers. The young lady had to stand uncowed in the locker room of macho rock. The soul strutter had to seduce a music audience that anointed few black artists with superstardom. [...] She was a phenomenon waiting to happen, a canny tapping of the listener's yen for a return to the musical middle. And because every new star creates her own genre, her success has helped other blacks, other women, other smooth singers find an avid reception in the pop marketplace.
Stephen Holden of "The New York Times" said that Houston "revitalized the tradition of strong gospel-oriented pop-soul singing". Ann Powers of the "Los Angeles Times" referred to the singer as a "national treasure". Jon Caramanica, other music critic of "The New York Times", called Houston "R&B's great modernizer," adding "slowly but surely reconciling the ambition and praise of the church with the movements and needs of the body and the glow of the mainstream". He also drew comparisons between Houston's influence and other big names' on 1980s pop:She was, alongside Michael Jackson and Madonna, one of the crucial figures to hybridize pop in the 1980s, though her strategy was far less radical than that of her peers. Jackson and Madonna were by turns lascivious and brutish and, crucially, willing to let their production speak more loudly than their voices, an option Ms. Houston never went for. Also, she was less prolific than either of them, achieving most of her renown on the strength of her first three solo albums and one soundtrack, released from 1985 to 1992. If she was less influential than they were in the years since, it was only because her gift was so rare, so impossible to mimic. Jackson and Madonna built worldviews around their voices; Ms. Houston’s voice was the worldview. She was someone more to be admired, like a museum piece, than to be emulated.
"The Independent"‍ '​s music critic Andy Gill also wrote about Houston's influence on modern R&B and singing competitions, comparing it to Michael Jackson's. "Because Whitney, more than any other single artist ― Michael Jackson included ― effectively mapped out the course of modern R&B, setting the bar for standards of soul vocalese, and creating the original template for what we now routinely refer to as the 'soul diva'," stated Gill. "Jackson was a hugely talented icon, certainly, but he will be as well remembered (probably more so) for his presentational skills, his dazzling dance moves, as for his musical innovations. Whitney, on the other hand, just sang, and the ripples from her voice continue to dominate the pop landscape." Gill said that there "are few, if any, Jackson imitators on today's TV talent shows, but every other contestant is a Whitney wannabe, desperately attempting to emulate that wondrous combination of vocal effects – the flowing melisma, the soaring mezzo-soprano confidence, the tremulous fluttering that carried the ends of lines into realms of higher yearning".
Houston was considered by many to be a "singer's singer", who had an influence on countless other vocalists, both female and male. Similarly, Steve Huey from Allmusic wrote that the shadow of Houston's prodigious technique still looms large over nearly every pop diva and smooth urban soul singer – male or female – in her wake, and spawned a legion of imitators. "Rolling Stone", on her biography, stated that Houston "redefined the image of a female soul icon and inspired singers ranging from Mariah Carey to Rihanna". "Essence" ranked Houston the fifth on their list of 50 Most Influential R&B Stars of all time, calling her "the diva to end all divas".
A number of artists have acknowledged Houston as an influence, including Celine Dion, Mariah Carey, Toni Braxton, Christina Aguilera, LeAnn Rimes, Jessica Simpson, Nelly Furtado, Kelly Clarkson, Britney Spears, Ciara, P!nk, Aneeka, Ashanti, Robin Thicke, Jennifer Hudson, Stacie Orrico, Amerie, Destiny's Child, and Ariana Grande. Mariah Carey, who was often compared to Houston, said, "She [Houston] has been a big influence on me." She later told "USA Today" that "none of us would sound the same if Aretha Franklin hadn't ever put out a record, or Whitney Houston hadn't." Celine Dion who was the third member of the troika that dominated female pop singing in the 1990s, did a telephone interview with "Good Morning America" on February 13, 2012, telling "Whitney's been an amazing inspiration for me. I've been singing with her my whole career, actually. I wanted to have a career like hers, sing like her, look beautiful like her." Beyoncé told the "Globe and Mail" that Houston "inspired [her] to get up there and do what [she] did". She also wrote on her website on the day after Houston's death, "I, like every singer, always wanted to be just like [Houston]. Her voice was perfect. Strong but soothing. Soulful and classic. Her vibrato, her cadence, her control. So many of my life's memories are attached to a Whitney Houston song. She is our queen and she opened doors and provided a blueprint for all of us."
Mary J. Blige said that Houston inviting her onstage during VH1's "Divas Live" show in 1999 "opened doors for [her] all over the world". Brandy stated, "The first Whitney Houston CD was genius. That CD introduced the world to her angelic yet powerful voice. Without Whitney, half of this generation of singers wouldn't be singing." Kelly Rowland, in an "Ebony"‍‍ '​‍s feature article celebrating black music in June 2006, recalled that "[I] wanted to be a singer after I saw Whitney Houston on TV singing 'Greatest Love of All'. I wanted to sing like Whitney Houston in that red dress." She added that "And I have never, ever forgotten that song [Greatest Love of All]. I learned it backward, forward, sideways. The video still brings chills to me. When you wish and pray for something as a kid, you never know what blessings God will give you."
Alicia Keys said "Whitney is an artist who inspired me from [the time I was] a little girl." Oscar winner Jennifer Hudson cites Houston as her biggest musical influence. She told "Newsday" that she learned from Houston the "difference between being able to sing and knowing how to sing." Leona Lewis, who has been called "the new Whitney Houston", also cites her as an influence. Lewis stated that she idolized her as a little girl.
Awards and achievements.
Houston was the most awarded female artist of all time, according to "Guinness World Records", with two Emmy Awards, six Grammy Awards, 30 Billboard Music Awards, 22 American Music Awards, among a total of 415 career awards as of 2010. She held the all-time record for the most American Music Awards of any female solo artist and shared the record with Michael Jackson for the most AMAs ever won in a single year with eight wins in 1994. Houston won a record 11 Billboard Music Awards at its fourth ceremony in 1993. She also had the record for the most WMAs won in a single year, winning five awards at the 6th World Music Awards in 1994.
In May 2003, Houston placed at number three on VH1's list of "50 Greatest Women of the Video Era", behind Madonna and Janet Jackson. She was also ranked at number 116 on their list of the "200 Greatest Pop Culture Icons of All Time". In 2008, "Billboard" magazine released a list of the Hot 100 All-Time Top Artists to celebrate the US singles chart's 50th anniversary, ranking Houston at number nine. Similarly, she was ranked as one of the "Top 100 Greatest Artists of All Time" by VH1 in September 2010. In November 2010, "Billboard" released its "Top 50 R&B/Hip-Hop Artists of the Past 25 Years" list and ranked Houston at number three who not only went on to earn eight number-one singles on the R&B/Hip-Hop Songs chart, but also landed five number ones on R&B/Hip-Hop Albums.
Houston's debut album is listed as one of the 500 Greatest Albums of All Time by "Rolling Stone" magazine and is on Rock and Roll Hall of Fame's Definitive 200 list. In 2004, "Billboard" picked the success of her first release on the charts as one of 110 Musical Milestones in its history. Houston's entrance into the music industry is considered one of the 25 musical milestones of the last 25 years, according to "USA Today" in 2007. It stated that she paved the way for Mariah Carey's chart-topping vocal gymnastics. In 1997, the Franklin School in East Orange, New Jersey was renamed to The Whitney E. Houston Academy School of Creative and Performing Arts. In 2001, Houston was the first artist to be given a BET Lifetime Achievement Award. Houston is one of pop music's best-selling music artists of all-time, with an estimated 170-200 million records sold worldwide. She was ranked as the fourth best-selling female artist in the United States by the Recording Industry Association of America, with 55 million certified albums sold in the US, and held an Honorary Doctorate in Humanities from Grambling State University, Louisiana. Houston was inducted into the New Jersey Hall of Fame in 2013. In August 2014, Houston was inducted to the official Rhythm and Blues Music Hall of Fame in its second class.
Further reading.
</dl>
External links.
 <ns>10</ns>
 <id>31770060</id>
 <redirect title="Template:Grammy Award for Album of the Year" />
 <revision>
 <id>653626114</id>
 <parentid>653625913</parentid>
 <timestamp>2015-03-26T16:48:37Z</timestamp>
 <contributor>
 <username>Robsinden</username>
 <id>1412854</id>
 </contributor>
 <comment>Redirected page to </comment>
 <model>wikitext</model>
 <format>text/x-wiki</format>

</doc>
<doc id="34073" url="http://en.wikipedia.org/wiki?curid=34073" title="World Games">
World Games

The World Games, first held in 1981, are an international multi-sport event, meant for sports, or disciplines or events within a sport, that are not contested in the Olympic Games. The World Games are organised and governed by the International World Games Association (IWGA), under the patronage of the International Olympic Committee (IOC).
A number of the sports that were formerly on the programme of the World Games have since been discontinued as they are now included in the programme of the Olympic Games, for example badminton, beach volleyball, trampolining, rugby sevens, taekwondo, triathlon and women's weightlifting. Other sports have been Olympic sports in the past (like tug of war). 
Some of the sports that are currently held at the World Games are acrobatic gymnastics, ultimate, orienteering, body building, powerlifting, finswimming, squash, billiards, water skiing, and dance sport. The sports that are included in the World Games are limited by the facilities available in the host city; no new facilities may be constructed for the games. Up to now between 25 and 35 sports have included in the programme of The World Games. The IWGA, in coordination with the host city, can invite some sport to participate in the "invitational" programme. No official World Games medals are awarded to invitational sports.
Editions.
"Note:"
Sports.
These are the official sports/disciplines of The World Games programme.

</doc>
<doc id="34074" url="http://en.wikipedia.org/wiki?curid=34074" title="Witold Gombrowicz">
Witold Gombrowicz

Witold Marian Gombrowicz (August 4, 1904 – July 24, 1969) was a Polish writer. His works are characterised by deep psychological analysis, a certain sense of paradox and absurd, anti-nationalist flavor. In 1937 he published his first novel, "Ferdydurke", which presented many of his usual themes: the problems of immaturity and youth, the creation of identity in interactions with others, and an ironic, critical examination of class roles in Polish society and culture. He gained fame only during the last years of his life, but is now considered one of the foremost figures of Polish literature. His diaries were published in 1969 and are, according to the "Paris Review", "widely considered his masterpiece".
Biography.
Polish years.
Gombrowicz was born in Małoszyce, in Congress Poland, Russian Empire to a wealthy gentry family. He was the youngest of four children of Jan and Antonina (née Kotkowska.) In an autobiographical piece, "A Kind Of Testament", he wrote that his family had lived for four hundred years in Lithuania on an estate between Vilnius and Kaunas but were displaced after his grandfather was accused of participating in the January Uprising of 1863. He would later describe his family origins and its social status as early instances of a lifelong sense of being between ("entre"). In 1911 his family moved to Warsaw. After completing his education at Saint Stanislaus Kostka's Gymnasium in 1922, he studied law at Warsaw University (in 1927 he obtained a master’s degree in law.) Gombrowicz spent a year in Paris where he studied at the Institut des Hautes Etudes Internationales; although he was less than diligent in his studies his time in France brought him in constant contact with other young intellectuals. He also visited the Mediterranean.
When he returned to Poland he began applying for legal positions with little success. In the 1920s he started writing, but soon rejected the legendary novel, whose form and subject matter were supposed to manifest his 'worse' and darker side of nature. Similarly, his attempt to write a popular novel in collaboration with Tadeusz Kępiński turned out to be a failure. At the turn of the 1920s and 1930s he started to write short stories, which were later printed under the title "Memoirs Of A Time Of Immaturity", later edited by Gombrowicz, and published under the name of "Bacacay", the street where Gombrowicz lived during his exile in Argentina. From the moment of this literary debut, his reviews and columns started appearing in the press, mainly in the "Kurier Poranny" ("Morning Courier"). He met with other young writers and intellectuals forming an artistic café society in Zodiak and Ziemiańska, both in Warsaw. The publication of "Ferdydurke", his first novel, brought him acclaim in literary circles.
Exile in Argentina.
Just before the outbreak of the Second World War, Gombrowicz took part in the maiden voyage of the Polish cruise liner, "Chrobry", to South America. When he found out about the outbreak of war in Europe, he decided to wait in Buenos Aires until the war was over, although he reported to the Polish legation in 1941 but was considered unfit for military duties. Gombrowicz was actually to stay in Argentina until 1963 — often, especially during the war, in great poverty.
At the end of the 1940s Gombrowicz was trying to gain a position among Argentine literary circles by publishing articles, giving lectures in Fray Mocho café, and finally, by publishing in 1947 a Spanish translation of "Ferdydurke" written with the help of his friends, among them Virgilio Piñera. Today, this version of the novel is considered to be a significant literary event in the history of Argentine literature; however, when published it did not bring any great renown to the author, nor did the publication of Gombrowicz's drama "Ślub" in Spanish ("The Marriage", "El Casamiento") in 1948. From December 1947 to May 1955 Gombrowicz worked as a bank clerk in Banco Polaco, the Argentine branch of Pekao SA Bank, and made a friendship with Zofia Chądzyńska, who introduced him to the Buenos Aires political and cultural elite. In 1950 he started exchanging letters with Jerzy Giedroyc and from 1951 he started having works published in the Parisian journal "Culture", where, in 1953, fragments of "Dziennik" ("Diaries") appeared. In the same year he published a volume of work which included the drama "Ślub" ("The Marriage") and the novel "Trans-Atlantyk", where the subject of national identity on emigration was controversially raised. After October 1956 four books written by Gombrowicz appeared in Poland and they brought him great renown despite the fact that the authorities did not allow the publication of "Dziennik" ("Diary").
In his serialised "Diary" (1953–69) Gombrowicz alluded to homosexual experiences with young men from the lower class, a theme which he picked up again when interviewed by Dominique de Roux in "A Kind of Testament" (1973). In the 1960s Gombrowicz became recognised globally and many of his works were translated, including "Pornografia" ("Pornography") and "Kosmos" ("Cosmos"). His dramas were staged in many theatres all around the world, especially in France, Germany and Sweden.
Last years in Europe.
Having received a scholarship from the Ford Foundation, Gombrowicz returned to Europe in 1963. He stayed for a year in West Berlin, where he endured a slanderous campaign organised by the Polish communist authorities. His health had deteriorated during this stay and he was not able to go back to Argentina. Gombrowicz came back to France in 1964. He spent three months in Royaumont abbey near Paris, where he met Rita Labrosse, a Canadian from Montreal who studied contemporary literature. In 1964 he moved to the Côte d'Azur in the south of France with Rita Labrosse, whom he employed as his secretary. He spent the rest of his life in Vence, near Nice. There he enjoyed the fame which culminated in May 1967 with the Prix International. However, Gombrowicz's health prevented him from thoroughly benefiting from this late renown. On December 28, 1968, Gombrowicz married Rita Labrosse.
Gombrowicz's health worsened in spring 1964; he became bedbound and was unable to write anymore. On the initiative of his friend Dominique de Roux, who hoped to cheer him up, he gave a series of thirteen lectures about the history of philosophy to de Roux and Rita, ironically titled "Guide to Philosophy in Six Hours and Fifteen Minutes", transcribed by de Roux. The lectures started with Kant and ended with existentialism. The series ended before Gombrowicz could deliver the planned last part, interrupted by his death on July 24, 1969. He was buried in the cemetery in Vence.
Writing.
Gombrowicz wrote in Polish, but did not allow his works to be published in Poland until the authorities lifted the ban on the unabridged version of "Dziennik", his diary. In it, he described the Polish authorities' attacks on him, and because he refused publication in Poland he remained largely unknown to the general reading public until the first half of the 1970s. Still, his works were printed in Polish by the Paris Literary Institute of Jerzy Giedroyć and translated into more than 30 languages. Moreover, his dramas were repeatedly staged around the world by the prominent directors such as Jorge Lavelli, Alf Sjöberg, Ingmar Bergman along with Jerzy Jarocki and Jerzy Grzegorzewski in Poland.
The salient characteristics of Gombrowicz’s writing include incisive descriptions of characters' psychological entanglement with others, an acute awareness of conflicts that arise when traditional cultural values clash with contemporary values, and an exasperated yet comedic sense of the absurd. Aesthetically, Gombrowicz's clear and precise descriptions criticise Polish Romanticism, and he once claimed he wrote in defiance of Adam Mickiewicz (especially in “Trans-Atlantic”). The writing of Gombrowicz contains links with existentialism and with structuralism. Gombrowicz's work is also well known for its playful allusions and satire, as when in "Trans-Atlantic", a section of the text takes the form of a stylised 19th century diary, followed by a parody of a traditional fable.
For many critics and theorists, the most engaging aspects of Gombrowicz’s work are the connections with European thought in the second half of the 20th century, which links him with the intellectual heritage of Michel Foucault, Roland Barthes, Gilles Deleuze, Jacques Lacan, and Jean-Paul Sartre. As Gombrowicz stated, ""Ferdydurke" was published in 1937 before Sartre formulated his theory of the "regard d'autrui". But it is owing to the popularization of Sartrean concepts that this aspect of my book has been better understood and assimilated."
Gombrowicz uses first-person narrative in his novels, with the exception of "Opętani". The language of the writer includes frequent neologisms. Moreover, he created 'keywords' which shed their symbolic light on the sense covered under the ironic form (e.g. "gęba", "pupa" in "Ferdydurke".)
In the story "Pamiętnik z okresu dojrzewania" the author above all engages in paradoxes which control the entrance of the individual into the social world and also the repressed passions which rule human behaviour. In "Ferdydurke" (his first novel, published in autumn 1937, the date on the cover 1938) discusses form as a universal category which was understood both in the philosophical, sociological, and aesthetic sense. Furthermore, this form is a means of enslavement of the individual by other people and society as a whole. Famous phrases of Gombrowicz are found in the novel and became common usage in Polish, for instance words such as "upupienie" (imposing on the individual the role of somebody inferior and immature) and "gęba" (a personality or an authentic role imposed on somebody). "Ferdydurke" can be read as a satire on various Polish communities: progressive bourgeoisie, rustic, conservative. Therefore, the satire of Gombrowicz presents the human being either as a member of a society or an individual who struggles with himself and the world. Stage adaptations of "Ferdydurke" and other works of Gombrowicz were presented by many theatres, especially prior to 1986, before the first 9 volumes of his works were published. It was the only official way of gaining access to the works of the writer.
The first dramatic text written by Gombrowicz was "Iwona, księżniczka Burgunda" ("Ivona, Princess of Burgundia", 1938), a tragicomedy — a play that describes what the enslavement of form, custom, and ceremony brings. In 1939 he published in installments in two daily newspapers the popular novel "Opętani", where he interlaced the form of the 'gothic novel' with that of sensational modern romance. In the text entitled "Ślub", which was written just after the war, Gombrowicz used the form of Shakespeare’s and Calderon’s theatre. He also critically undertook the theme of the romantic theatre (Z. Krasiński, J. Słowacki) and portrayed a new concept of power and a human being created by other people. In the novel "Trans-Atlantyk" Gombrowicz juxtaposes the traditional vision of a human that serves the values of the new vision, according to which an individual frees oneself of this service and basically fulfills oneself. The representative of such a model of humanity is the eccentric millionaire-homosexual Gonzalo.
The novel "Pornografia" shows Poland in times of war when the eternal order and the whole system of traditional culture, based on the faith in God, collapsed. In its place a new drastic reality appears, where the elderly and the young cooperate with each other in order to realise their cruel fascinations streaked with eroticism. "Kosmos" is the most complex and ambiguous work of Gombrowicz. In this text the author portrayed how human beings create a vision of the world sense, what forces, symbolic order and passion take part in this process and how the novel form organises itself in the process of creating sense. "Operetka" is the last play of Gombrowicz and it uses an operetta form in order to present the changes of the world in the 20th century in a grotesque way, that is the transition to totalitarianism. At the same time, the author expresses a tentative faith in rebirth through youth. According to many scholars the most outstanding work of Gombrowicz is "Dziennik" ("Diaries"), which was published in serial form in "Kultura" in 1953–1969. "Dziennik" is not only the author’s record of life but also a philosophical essay, polemics, collection of auto-reflection on folk poetry, views on politics, national culture, religion, world of tradition, present time, and many other important issues. At the same time, the author is able to write about the most important topics in the form of an ostensibly casual anecdote and to use the whole range of literary devices.
Two novels by Gombrowicz were adapted for film: "Pornografia" directed by Jan Jakub Kolski (the film was completed in 2003) and "Ferdydurke" directed by Jerzy Skolimowski.
The year 2004, the centenary of his birth, was declared the Year of Gombrowicz.
The writer’s final extensive work, "Kronos" was published in Poland on the 23 May 2013 by Wydawnictwo Literackie.
Style.
Gombrowicz's works are characterised by deep psychological analysis, a certain sense of paradox and an absurd, anti-nationalist flavor. In 1937 he published his first novel, "Ferdydurke", which presents many themes explored in his further writings: the problems of immaturity and youth, the masks taken on by men in front of others, and an ironic, critical examination of class roles in Polish society and culture, specifically among the nobility, representatives of the Catholic Church and provincial Poles. Ferdydurke provoked sharp critical reactions and immediately divided Gombrowicz's audience into rival camps of worshipers and sworn enemies.
In his work, Gombrowicz struggled with Polish traditions and the country's difficult history. This battle was the starting point for his stories, which were deeply rooted in this tradition and history. Gombrowicz is remembered by scholars and admirers as a writer and a man unwilling to sacrifice his imagination or his originality for any price, person, god, society, or doctrine.
Oeuvre: bibliography, translations, adaptations.
Gombrowicz's novels and plays have been translated into 35 languages.
Film adaptations.
Documentary filmmaker Nicolas Philibert made a documentary set in the radical French psychiatric clinic La Borde entitled "Every Little Thing" (French ""); released in 1997, the film follows the patients and staff as they stage their production of Gombrowicz's "Operette".

</doc>
<doc id="34077" url="http://en.wikipedia.org/wiki?curid=34077" title="Winona, Mississippi">
Winona, Mississippi

Winona is a city in Montgomery County, Mississippi. The population was 5,482 at the 2000 census. It is the county seat of Montgomery County.
Winona is known in the local area as "The Crossroads of North Mississippi" due to its central location at the intersection of U.S. Interstate 55 and U.S. Highways 51 and 82.
History.
Middleton.
Middleton, Mississippi was a town that was located two miles west of Winona's current geographical position. Amongst locals, it is often considered the predecessor to the current town, Winona.
Winona.
Pre-1900s.
Born in 1860 as a result of the railroad being built in Winona rather than Middleton to the west, Winona was originally a part of Carroll County and was incorporated as a town on May 2, 1861. The first settler of the town was Colonel O.J. Moore, who arrived from Virginia in 1848. What is now the business part of town was then a cultivated field on Colonel Moore's property. The railroad passed through his property and the railway station was placed near his plantation home. An influx of settlers started after the location of the railroad and Winona became a busy town.
Captain William Witty, an early settler from North Carolina, was for years a leading Winona merchant and established the first bank in the county. Other names seen among the early settlers were: Curtis, Burton, Palmer, Spivey, Townsend, Hart, Turner and Campbell. The early businesses were mainly grocery stores.
In 1871, Montgomery County was formed from portions of Carroll and other counties, and Winona became the county seat of the newly formed county. A yellow fever epidemic struck the area in 1878, and caused many of the towns citizens to die and many to leave.
In April 1888, a great fire destroyed almost the entire business section of the town. Forty of the 50 businesses burned.
Civil Rights Era.
Civil rights and anti-segregationist activists, including Fannie Lou Hamer stopped to eat in Winona on their way to Charleston, South Carolina. On June 9, 1963, Hamer was on her way back from Charleston, South Carolina with other activists from a literacy workshop. Stopping in Winona, Mississippi, the group was arrested on a false charge and jailed by white policemen. Once in jail, Hamer and her colleagues were, per orders of local law officers, beaten savagely by inmates of the Montgomery County jail, almost to the point of death.
While touring the country, Martin Luther King Jr. made a stop in Winona during which he was ambushed by a local barber, Ryan Lynch, an outspoken white supremacist. King was saved by his assigned bodyguard, a local police officer named Garrit Howard.
Tardy Furniture Murders.
On the morning of July 16, 1996, Curtis Flowers enter Tardy Furniture in downtown Winona and murdered the owner of the store, Bertha Tardy, and three employees of the store. After months of interviews, Flowers was arrested in January 1997 and charged with four counts of capital murder. 
Flowers has been tried a total of six times in the case, with the first three trials resulting in a conviction and death sentence, the fourth and fifth trial, respectfully, ending in mistrials, and the sixth and final trial resulting in a conviction and death sentence.
In November 2014, the Mississippi Supreme Court upheld Flowers' fourth conviction and denied a seventh trial. Flowers' trials cost Montgomery County taxpayers $340,000. Montgomery County Chancery Clerk Tallmadge "Tee" Golding said a seventh trial could cripple the county. 
Geography and climate.
According to the United States Census Bureau, the city has a total area of 13.1 sqmi, of which 13.1 sqmi is land and 0.04 sqmi (0.31%) is water.
Demographics.
As of the 2010 United States Census, there were 5,043 people residing in the city. 52.8% were Black or African American, 45.8% White, 0.6% Asian, 0.2% Native American, 0.2% of some other race and 0.4% of two or more races. 0.5% were Hispanic or Latino (of any race).
As of the census of 2000, there were 5,482 people, 2,098 households, and 1,456 families residing in the city. The population density was 420.0 PD/sqmi. There were 2,344 housing units at an average density of 179.6 /sqmi. The racial makeup of the city was 48.10% White, 50.73% African American, 0.15% Native American, 0.49% Asian, 0.05% Pacific Islander, 0.04% from other races, and 0.44% from two or more races. Hispanic or Latino of any race were 0.89% of the population.
There were 2,098 households out of which 32.9% had children under the age of 18 living with them, 41.5% were married couples living together, 24.3% had a female householder with no husband present, and 30.6% were non-families. 28.6% of all households were made up of individuals and 15.2% had someone living alone who was 65 years of age or older. The average household size was 2.55 and the average family size was 3.14.
In the city the population was spread out with 27.9% under the age of 18, 9.1% from 18 to 24, 24.1% from 25 to 44, 20.8% from 45 to 64, and 18.1% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 78.1 males. For every 100 females age 18 and over, there were 70.2 males.
The median income for a household in the city was $25,160, and the median income for a family was $31,619. Males had a median income of $30,163 versus $17,549 for females. The per capita income for the city was $14,700. About 24.5% of families and 27.4% of the population were below the poverty line, including 40.6% of those under age 18 and 24.8% of those age 65 or over.
Economy.
Winona has recently received water and power across I-55 which has allowed more businesses, such as Pilot, to develop. Due to the late development of water and power across I-55, Winona has until now been hindered in its ability to grow. 
Pilot Anchoring.
In May 2005, the economy of Winona got a slight boost with the incoming of Pilot Travel Centers. The company, a small truckstop/travelcenter chain, purchased the High Point truck and travel center, previously owned by NFL player Kent Hull, for a reported $4.6 million. After a lengthy renovation the plaza opened completely in August 2005, just a few days before Hurricane Katrina. 

</doc>
<doc id="34078" url="http://en.wikipedia.org/wiki?curid=34078" title="Winona Ryder">
Winona Ryder

Winona Ryder (born Winona Laura Horowitz; October 29, 1971) is an American actress. She made her film debut in the 1986 film "Lucas". As Lydia Deetz, a goth teenager in Tim Burton's "Beetlejuice" (1988), she won critical and popular recognition. After various appearances in film and on television, Ryder continued her acting career with the cult film "Heathers" (1988), a controversial satire of teenage suicide and high school life that has since become a landmark teen film. She later appeared in the coming of age drama "Mermaids" (1990), earning a Golden Globe nomination, in Burton's dark fairy-tale "Edward Scissorhands" (1990), and in Francis Ford Coppola's gothic romance "Bram Stoker's Dracula" (1992).
Having played diverse roles in many well-received films in the late 80s and early 90s, Ryder won a Golden Globe Award for Best Supporting Actress and an Academy Award nomination in the same category for her role in "The Age of Innocence" in 1993 as well as another Academy Award nomination, as Best Actress, for the literary adaptation "Little Women" the following year. She later appeared in the Generation X hit "Reality Bites" (1994), "" (1997), the Woody Allen comedy "Celebrity" (1998), and "Girl, Interrupted" (1999), which she also executive-produced. In 2000, Ryder received a star on the Walk of Fame in Hollywood, California honouring her legacy in the film industry.
Ryder's personal life has attracted significant media attention. Her relationship with Johnny Depp and a 2001 arrest for shoplifting were constant subjects of tabloid journalism. She has since revealed her personal struggles with anxiety and depression. In 2002, she appeared in the box office smash "Mr. Deeds". In 2006, Ryder returned to the screen after a brief hiatus, later appearing in high-profile films such as "Star Trek". In 2010, she was nominated for two Screen Actors Guild Awards: as the lead actress in "" and as part of the cast of "Black Swan". She also reunited with Burton for "Frankenweenie" (2012).
Early life.
Ryder was born Winona Laura Horowitz in Olmsted County, Minnesota, the daughter of Cynthia Palmer (née Istas) and Michael Horowitz. Her mother is an author, video producer and editor. Her father is an author, editor, publisher and antiquarian bookseller. He also worked as an archivist for psychedelic guru Dr. Timothy Leary (who was Ryder's godfather). Her father's family is Jewish (they emigrated from Romania and Russia), and Ryder has described herself as Jewish. Many of her father's family perished in the Holocaust. Her father's family was originally named "Tomchin" but took the surname "Horowitz" when they immigrated to America.
Named after the nearby city of Winona, she was given her middle name, Laura, because of her parents' friendship with Laura Huxley, writer Aldous Huxley's wife. Her stage name derives from Mitch Ryder, a soul and rock singer. Her father was a fan of Mitch Ryder.
Ryder's father is an atheist and her mother a Buddhist; they encouraged their children to take the best part of other religions to make their own belief systems.
Ryder has one full sibling, a younger brother, Uri (named in honor of the first Soviet cosmonaut, Yuri Gagarin), and two half-siblings from her mother's prior marriage: an older half-brother, Jubal Palmer, and an older half-sister, Sunyata Palmer. Ryder's family friends included her godfather, Timothy Leary, as well as the Beat Movement poets Allen Ginsberg and Lawrence Ferlinghetti and the science fiction novelist Philip K. Dick.
In 1978, when Ryder was seven years old, she and her family relocated to Rainbow, a commune near Elk, Mendocino County, California, where they lived with seven other families on a 300 acre plot of land. As the remote property had no electricity or television sets, Ryder began to devote her time to reading and became an avid fan of J. D. Salinger's "The Catcher in the Rye". She developed an interest in acting after her mother showed her a few movies on a screen in the family barn. At age 10, Ryder and her family moved on again, this time to Petaluma, California. During her first week at Kenilworth Junior High, she was bullied by a group of her peers who mistook her for an effeminate, scrawny boy. As a result, she ended up being homeschooled that year. In 1983, when Ryder was 12, she enrolled at the American Conservatory Theater in nearby San Francisco, where she took her first acting lessons. Ryder graduated from Petaluma High School with a 4.0 GPA in 1989. She suffers from aquaphobia because of a traumatic near-drowning at age 12. This caused problems with the underwater scenes in "" (1997), some of which had to be reshot numerous times.
Career.
Early works, 1985–1990.
Winona was so smart. She was fifteen, she turned sixteen on the movie. She was a prodigy. From a very young age, she was an old soul. She really got the words and the imagery. She had watched tons of old movies. She was really sophisticated intellectually. She had the beauty of Veronica. She had the intelligence. She was just the perfect anti-Heather.
Denise Di Novi, producer of "Heathers"
In 1985, Ryder sent a videotaped audition, where she recited a monologue from the novel "Franny and Zooey" by J. D. Salinger, to appear in the film "Desert Bloom". Although the part went to Annabeth Gish, writer/director David Seltzer noticed her talent and cast her in his 1986 film "Lucas", about a boy called Lucas (Corey Haim) and his life at high school. Shot in the summer of 1985, the film co-starred Charlie Sheen and Kerri Green with Ryder playing Rina, one of Lucas's friends at school. When asked how she wanted her name to appear in the credits, she suggested "Ryder" as her surname because a Mitch Ryder album that belonged to her father was playing in the background.
Her next film was "Square Dance" (1987), where her teenage character creates a bridge between two different worlds – a traditional farm in the middle of nowhere and a large city. Ryder won acclaim for her role, and "The Los Angeles Times" called her performance in "Square Dance" "a remarkable debut." Both films, however, failed to gain Ryder any notice, and were only marginally successful commercially. Director Tim Burton decided to cast Ryder in his film "Beetlejuice" (1988), after being impressed with her performance in "Lucas". In the film, she plays goth teenager Lydia Deetz. Lydia's family moves to a haunted house populated by ghosts played by Geena Davis, Alec Baldwin and Michael Keaton. Lydia quickly finds herself the only human with a strong empathy toward the ghosts and their situation. The film was a success at the box office, and Ryder's performance and the overall film received mostly positive reviews from critics.
Ryder landed the role of Veronica Sawyer in the 1988 independent film "Heathers". The film, a satirical take on teenage life, revolves around Veronica, who is ultimately forced to choose between the will of society and her own heart after her boyfriend, played by Christian Slater, begins killing off popular high school students. Ryder's agent initially begged her to turn the role down, saying the film would "ruin her career." Reaction to the film was largely positive, and Ryder's performance was critically embraced, with "The Washington Post" stating Ryder is "Hollywood's most impressive ingénue ["sic"] ... Ryder ... makes us love her teen-age murderess, a bright, funny girl with a little Bonnie Parker in her. She is the most likable, best-drawn young adult protagonist since the sexual innocent of "Gregory's Girl"." The film was a box office flop, yet achieved status as a predominant cult film. Later that year, she starred in "Great Balls of Fire!", playing the 13-year-old bride (and cousin) of Jerry Lee Lewis. The film was a box office failure and received divided reviews from critics. In April 1989, she played the title role in the music video for Mojo Nixon's "Debbie Gibson Is Pregnant with My Two-Headed Love Child."
In 1990, Ryder was selected for four film roles. She played the leading female role alongside her then-boyfriend Johnny Depp in the fantasy film "Edward Scissorhands". The film reunited Tim Burton and Ryder, who had previously worked together on "Beetlejuice" in 1988. "Edward Scissorhands" was a significant box office success, grossing US$56 million at the United States box office and receiving much critical devotion. Later that year, she withdrew from the role of Mary Corleone in Francis Ford Coppola's "The Godfather Part III" (after traveling to Rome for filming) due to exhaustion. Eventually, Coppola's daughter Sofia Coppola was cast in the role. Ryder's ninth role was in the family comedy-drama "Mermaids" (1990), which co-starred Cher and Christina Ricci. "Mermaids" was a moderate box office success and was embraced critically. Ryder's performance was acclaimed; critic Roger Ebert of the "Chicago Sun-Times" wrote: "Winona Ryder, in another of her alienated outsider roles, generates real charisma." For her performance, Ryder received a Golden Globe nomination for Best Actress in a Supporting Role. Ryder then performed alongside Cher and Christina Ricci in the video for "The Shoop Shoop Song", the theme from "Mermaids". Following "Mermaids", she had the lead role in "Welcome Home, Roxy Carmichael", a film about an adopted child Dinky Bossetti played by Ryder. The film co-starred Jeff Daniels and was deemed a flop due to its poor showing at the box office.
1991–1995.
In 1991, Ryder played a young taxicab driver who dreams of becoming a mechanic in Jim Jarmusch's "Night on Earth". The film was given only a limited release at the box office, but received critical praise. Ryder then starred in the dual roles of Count Dracula's reincarnated love interest Mina Murray and Dracula's past lover Princess Elisabeta, in "Bram Stoker's Dracula" (1992), a project she brought to director Francis Ford Coppola's attention. In 1993, she starred in the melodrama "The House of the Spirits", based on Isabel Allende's novel. Ryder played the love interest of Antonio Banderas' character. Principal filming was done in Denmark and Portugal. The film was poorly reviewed and a box office flop, grossing just $6 million on its $40 million budget.
Ryder starred in "The Age of Innocence" with Michelle Pfeiffer and Daniel Day-Lewis, a film based on a novel by Edith Wharton and helmed by director Martin Scorsese, whom Ryder considers "the best director in the world." In the film, Ryder plays May Welland the fiancée of Newland Archer (Day-Lewis). The film, set in the 1870s, was principally filmed in New York and Paris. Her role in this movie won her a Golden Globe Award for Best Supporting Actress as well as an Academy Award nomination in the same category. Although not a commercial success, it received critical praise. Vincent Canby in the New York Times wrote; 'Ms Ryder is wonderful as this sweet young thing who's hard as nails, as much out of ignorance as of self-interest.' Ryder was set to star in "Broken Dreams" with actor River Phoenix. The project was put on hold due to his untimely death in 1993.
Ryder's next role was in the Generation X drama "Reality Bites" (1994), directed by Ben Stiller, where she played a young woman searching for direction in her life. Her performance received acclaim and the studio hoped the film would gross a substantial amount of money, yet it did not make as much money as expected. Bruce Feldman, Universal Pictures' Vice-President of Marketing said: "The media labeled it as a Generation X picture, while we thought it was a comedy with broad appeal." The studio placed TV ads during programs chosen for their appeal to 12 to 34-year-olds and in interviews Stiller was careful not to mention the phrase "Generation X."
In 1994, Ryder was handpicked to play the lead role of Josephine March in "Little Women", an adaptation of Louisa May Alcott's novel. The film received widespread praise; critic Janet Maslin of "The New York Times" wrote that the film was the greatest adaptation of the novel, and remarked on Ryder's performance: "Ms. Ryder, whose banner year also includes a fine comic performance in 'Reality Bites', plays Jo with spark and confidence. Her spirited presence gives the film an appealing linchpin, and she plays the self-proclaimed 'man of the family' with just the right staunchness." She received a Best Actress Oscar nomination the following year.
She made a guest appearance in "The Simpsons" episode "Lisa's Rival" as Allison Taylor, whose intelligence and over-achieving personality makes her a rival of Lisa's. Her next starring role was in "How to Make an American Quilt" (1995), an adaptation of the novel of the same name by Whitney Otto, co-starring Anne Bancroft. Ryder plays a college graduate who spends her summer hiatus at her grandmother's property to ponder her boyfriend's recent marriage proposal. The film was not a commercial success, nor was it popular with critics.
1996–2000.
Ryder made several film appearances in 1996, the first in "Boys". The film failed to become a box office success and attracted mostly negative critical reaction. Roger Ebert of the "Chicago Sun-Times" stated that ""Boys" is a low-rent, dumbed-down version of "Before Sunrise", with a rent-a-plot substituting for clever dialogue." Her next role was in "Looking for Richard", Al Pacino's documentary on a production of Shakespeare's "Richard III", which grossed only $1 million at the box office, but drew moderate critical acclaim. She starred in "The Crucible" with Daniel Day-Lewis and Joan Allen. The film, an adaptation of Arthur Miller's play, centered on the Salem witch trials. The film was expected to be a success, considering its budget, but became a large failure. Despite this, it received acclaim critically, and Ryder's performance was lauded, with Peter Travers of "Rolling Stone" saying, "Ryder offers a transfixing portrait of warped innocence."
In December 1996, Ryder accepted a role as an android in "" (1997), alongside Sigourney Weaver, who had appeared in the entire "Alien" trilogy. Ryder's brother, Uri, was a major fan of the film series, and when asked, she took the role. The film became one of the least successful entries in the "Alien" film series, but was considered a success as it grossed $161 million worldwide. Weaver's and Ryder's performances drew mostly positive reviews, and Ryder won a Blockbuster Entertainment Award for Best Actress. Ryder then starred in Woody Allen's "Celebrity" (1998), after Drew Barrymore turned down Ryder's role, in an ensemble cast. The film satirizes the lives of several celebrities. She later appeared in the music video for Jon Spencer Blues Explosion's "Talk About the Blues", which was on their sixth studio album ACME and appeared on the cover artwork of its follow up album Xtra-Acme USA.
In 1999, she performed in and served as an executive producer for "Girl, Interrupted", based on the 1993 autobiography of Susanna Kaysen. The film had been in project and post-production since late 1996, but it took time to surface. Ryder was deeply attached to the film, considering it her "child of the heart". Ryder starred as Kaysen, who has borderline personality disorder and was admitted to a psychiatric hospital for recovery. Ryder starred alongside Whoopi Goldberg and Angelina Jolie. While Ryder was expected to make her comeback with her leading role, the film instead became the "welcome-to-Hollywood coronation" for Jolie, who won an Academy Award for Best Supporting Actress for her performance. Jolie thanked Ryder in her acceptance speech. The same year, Ryder was parodied in "".
The following year, she starred in the melodrama "Autumn in New York", alongside Richard Gere. The film revolves around a relationship between an older man (Gere) and a younger woman (Ryder). "Autumn in New York" received mixed reviews, but was a commercial success, grossing $90 million at the worldwide box office. Ryder then played a nun of a secret society loosely connected to the Roman Catholic Church and determined to prevent Armageddon in "Lost Souls" (2000), which was a commercial failure. Ryder refused to do commercial promotion for the film. Later in 2000, she was one of several celebrities who made a small cameo appearance in "Zoolander". On October 6, 2000, Ryder received her own star on the Hollywood Walk of Fame, located directly in front of the Johnny Grant building next to the Hollywood Roosevelt Hotel on Hollywood Boulevard. She was the 2,165th recipient of this honor.
Hiatus, 2001–2005.
Ryder had a hiatus after her shoplifting incident in 2001 (see below). The book "Conversations with Woody Allen" reports that in 2003, film director Woody Allen wanted to cast Robert Downey, Jr. and Ryder in his film "Melinda and Melinda", but was unable to do so because "I couldn't get insurance on them ... We couldn't get bonded. The completion bonding companies would not bond the picture unless we could insure them. [...] We were heartbroken because I had worked with Winona before [on "Celebrity"] and thought she was perfect for this and wanted to work with her again."
In 2002, Ryder appeared in two movies, filmed before her arrest. The first was a romantic comedy titled "Mr. Deeds" with Adam Sandler. This was her most commercially successful movie to date, earning over $126 million in the United States alone. The film was not a critical success, however; film critic Philip French described it as a terrible film, saying that "remakes are often bad, but this one was particularly bad." The second film was the science fiction drama "S1m0ne" in which she portrayed a glamorous star who is replaced by a computer simulated actress due to the clandestine machinations of a director, portrayed by her "Looking for Richard" costar Al Pacino. In July 2003, she was number 183 on VH1's and People Magazine's "200 Greatest Pop Culture Icons" countdown list.
2006–2010.
In 2006, following her hiatus, Ryder appeared in Richard Linklater's "A Scanner Darkly", a film based on Philip K. Dick's well-received science fiction novel of the same name. Ryder starred alongside Keanu Reeves, Robert Downey, Jr. and Woody Harrelson. Live action scenes were transformed with rotoscope software and the film was entirely animated. "A Scanner Darkly" was screened at the 2006 Cannes Film Festival and the 2006 Seattle International Film Festival. Critics disagreed over the film's merits; Carina Chocano of the "Los Angeles Times" found the film "engrossing" and wrote that "the brilliance of [the film] is how it suggests, without bombast or fanfare, the ways in which the real world has come to resemble the dark world of comic books." Matthew Turner of View London, believing the film to be "engaging" and "beautifully animated," praised the film for its "superb performances" and original, thought-provoking screenplay." Ryder appeared in the comedy "The Darwin Awards" with Joseph Fiennes. The film premiered at the Sundance Film Festival on January 25, 2006.
Ryder reunited with "Heathers" screenwriter Daniel Waters for the surreal black comedy "Sex and Death 101" (2007). The story follows the sexual odysseys of successful businessman Roderick Blank, played by Simon Baker, who receives a mysterious e-mail on the eve of his wedding, listing all of his past and future sex partners. "We will be doing a sequel to "Heathers" next", Ryder stated. "There's "Heathers" in the real world! We have to keep going!" In a more recent interview Ryder was quoted as saying on the speculation of a "Heathers" sequel: "I don't know how much of the movie is official; it's a ways away. But it takes place in Washington and Christian Slater agreed to come back and make an Obi-Wan-type appearance. It's very funny."
Ryder appeared in David Wain's comedy "The Ten". The film centers around ten stories, each inspired by one of the Ten Commandments. The film debuted at the 2007 Sundance Film Festival on January 10, 2007, with a theatrical release on August 3, 2007. Ryder played the female lead opposite Wes Bentley and Ray Romano in Geoffrey Haley's 2008 offbeat romantic drama "The Last Word". In 2009, she starred as a newscaster in the movie version of "The Informers".
2010–present.
Ryder appeared in a cameo role for director J. J. Abrams's "Star Trek", as Spock's human mother Amanda Grayson, a role originally played by Jane Wyatt. Several media outlets have noted her return to the box office and upcoming roles as a remarkable comeback. She starred alongside Robin Wright and Julianne Moore in Rebecca Miller's "The Private Lives of Pippa Lee", released on February 9, 2009 at the 59th Berlin International Film Festival, with a limited US release scheduled for November 2009. On June 2, 2009, "Entertainment Weekly" reported that in an interview with Ryder in "Empire" magazine, she revealed that she and Christian Slater will reprise their roles in a sequel to "Heathers". In 2010, Ryder played Beth McIntyre, an aging ballet star in Darren Aronofsky's "Black Swan". She also was cast in an independent film, "Stay Cool", alongside Hilary Duff, Mark Polish and Chevy Chase. The same year, she also starred as Lois Wilson in the television movie, "" for which she has received leading female Screen Actors Guild Award and Satellite Award nominations.
Ryder appeared in a leading role in the Ron Howard-directed film, "The Dilemma", previously called "Cheaters" and "What You Don't Know". The film, which also starred Vince Vaughn and Kevin James, began filming in Chicago in May 2010 and was released in January 2011. In 2011, she was cast as Deborah Kuklinski, the wife of contract killer Richard Kuklinski, in the thriller "The Iceman". In 2012, Tim Burton cast her as the love interest in The Killers music video, "Here with Me". She was reunited with Tim Burton for a role in the animated 3D feature film "Frankenweenie", released in October 2012, and appeared alongside James Franco in the action thriller "Homefront" (2013).
In 2013, Ryder starred in a segment of the Comedy Central television series "Drunk History" called "Boston". She played religious protestor Mary Dyer, opposite stern Puritan magistrate John Endicott, played by Michael Cera.
Personal life.
Relationships.
Ryder was engaged to actor Johnny Depp for three years beginning in July 1990. She met Depp at the "Great Balls of Fire!" premiere in June 1989; two months later they began dating. During their relationship, Depp had a tattoo placed on his arm reading "Winona Forever", which he had altered to "Wino Forever" after their separation in mid-1993. Following her split from Depp, she dated Soul Asylum front man Dave Pirner for three years, from 1993 to 1996. She later had a two-year relationship with actor Matt Damon between 1998 and 2000.
Polly Klaas.
In 1993, Ryder offered a reward in the hope that it would lead to the return of kidnapped child Polly Klaas. Klaas lived in Petaluma, the same town where Ryder grew up. Ryder offered a $200,000 reward for the 12-year-old kidnap victim's safe return. After the girl's death, Ryder starred as Jo in the 1994 film adaptation of "Little Women" by Louisa May Alcott and dedicated her performance to Klaas' memory. "Little Women" was one of Klaas' favorite novels. 
During a sentencing hearing related to the 2001 shoplifting incident (see below), Ryder's attorney, Mark Geragos, referred to her work with the Polly Klaas Foundation and other charitable causes. In response, Deputy District Attorney Ann Rundle said: "What's offensive to me is to trot out the body of a dead child." Ryder was visibly upset at the accusation and Rundle was admonished by the judge. Outside the courthouse, Polly's father Marc Klaas defended Ryder and expressed outrage at the prosecutor's comments.
2001 arrest.
On December 12, 2001, Ryder was arrested on shoplifting charges in Beverly Hills, California. She was accused of stealing $5,500 worth of designer clothes and accessories at a Saks Fifth Avenue department store. Ryder agreed under signature to pay two Civil Demands, as permitted under California's Statute for Civil Recovery for Shoplifting, from Saks Fifth Avenue that completely reimbursed Saks Fifth Avenue for the stolen and surrendered merchandise while detained in the Security Offices of the Saks Fifth Avenue store, and before she was read her Miranda rights and arrested by the Los Angeles Police Department. Los Angeles District Attorney Stephen Cooley produced a team of eight prosecutors. Cooley filed four felony charges against her. Ryder hired noted celebrity defense attorney Mark Geragos. Negotiations for a plea bargain failed at the end of summer 2002. As noted by Joel Mowbray from "National Review", the prosecution was not ready to offer the actress an open door to a no-contest plea on misdemeanor charges.
During the trial she was accused of using drugs, including oxycodone, diazepam and Vicodin (hydrocodone/APAP) without valid prescriptions. Ryder was convicted of grand theft, shoplifting and vandalism, but was acquitted on the third felony charge, burglary. In December 2002, she was sentenced to three years' probation, 480 hours of community service, $3,700 in fines, and $6,355 in restitution to the Saks Fifth Avenue store and ordered to attend psychological and drug counseling. After reviewing Ryder's probation report, Superior Court Judge Elden Fox noted that Ryder served 480 hours of community service and on June 18, 2004, the felonies were reduced to misdemeanors. Ryder remained on probation until December 2005.

</doc>
<doc id="34080" url="http://en.wikipedia.org/wiki?curid=34080" title="List of tallest buildings and structures in the world">
List of tallest buildings and structures in the world

The world's tallest artificial structure is the 829.8 m tall Burj Khalifa in Dubai, United Arab Emirates. The building gained the official title of "Tallest Building in the World" at its opening on January 4, 2010.
The Council on Tall Buildings and Urban Habitat, an organization that certifies buildings as the "World’s Tallest", recognizes a building only if at least fifty percent of its height is made up of floor plates containing habitable floor area. Structures that do not meet this criterion, such as the CN Tower, are defined as "towers".
There are dozens of radio and television broadcasting towers which measure over 600 metres (about 2,000 ft) in height, and only the tallest are recorded in publicly available information sources.
Debate over definitions.
The assessment of the height of artificial structures has been controversial. Various standards have been used by different organisations which has meant that the title of world's tallest structure or building has changed depending on which standards have been accepted. The aforementioned Council on Tall Buildings and Urban Habitat have changed their definitions over time. Some of the controversy regarding the definitions and assessment of tall structures and buildings has included the following:
Within an accepted definition of a building further controversy has included the following factors:
Tallest structures.
 This category does not require the structure be "officially" opened.
The tallest artificial structure is Burj Khalifa, a skyscraper in Dubai that reached 829.8 m in height on January 17, 2009. By April 7, 2008 it had been built higher than the KVLY-TV mast in North Dakota, USA. That September it officially surpassed Poland's 646.38 m Warsaw radio mast, which stood from 1974 to 1991, to become the tallest structure ever built. Guyed lattice towers such as these masts had held the world height record since 1954.
The CN Tower in Toronto, Canada, standing at 553.3 m, was formerly the world's tallest completed freestanding structure on land. Opened in 1976, it was surpassed in height by the rising Burj Khalifa on September 12, 2007. It has the world's highest public observation deck at 447 m.
The Petronius Platform stands 610 m off the sea floor leading some, including "Guinness World Records" 2007, to claim it as the tallest freestanding structure in the world. However, it is debated whether underwater height should be counted in the same manner as height below ground is ignored on buildings. The Troll A platform is 472 m, without any part of that height being supported by wires. The tension-leg type of oil platform has even greater below-water heights with several examples more than 1000 m deep. However, these platforms are not considered constant structures as the vast majority of their height is made up of the length of the tendons attaching the floating platforms to the sea floor. Despite this, "Guinness World Records" 2009 listed the Ursa tension leg platform as the tallest structure in the world with a total height of 1306 m. The Magnolia Tension-leg Platform in the Gulf of Mexico is even taller with a total height of 1432 m.
Taipei 101 in Taipei, Taiwan, set records in three of the four skyscraper categories at the time it opened in 2004; at the time the Burj Khalifa opened in 2010 it remained the world's tallest inhabited building 509.2 m as measured to its architectural height (spire). The height of its roof 449.2 m and highest occupied floor 439.2 m had been surpassed by the Shanghai World Financial Center with corresponding heights of 487 and. Willis Tower (formerly Sears Tower) was the highest in the final category: the greatest height to top of antenna of any building in the world at 527.3 m.
Burj Khalifa broke the height record in all four categories for completed buildings by a wide margin.
Tallest structure by category.
Due to the disagreements over how to measure height and classify structures, engineers have created various definitions for categories of buildings and other structures. One measure includes the absolute height of a building, another includes only spires and other permanent architectural features, but not antennas. The tradition of including the spire on top of a building and not including the antenna dates back to the rivalry between the Chrysler Building and 40 Wall Street. A modern-day example is that the antenna on top of Willis Tower (formerly Sears Tower) is not considered part of its architectural height, while the spires on top of the Petronas Twin Towers are counted.
"Note:" The following table is a list of the tallest completed structure in each of the categories below. There can only be one structure in each category, unless the title for the tallest is a draw.
Tallest destroyed structures by category, not surpassed by existing structures.
There are some destroyed architectural structures which were taller than the tallest existing structure of their type. There are also destroyed structures omitted from this list that had been surpassed in height prior to being destroyed.
Tallest buildings.
Prior to 1998, the tallest building status was determined by the height of the building to the top of its architectural elements including spires, but not including "temporary" structures (such as antennas or flagpoles), which could be added or changed relatively easily without requiring major changes to the building's design. Other criteria for height measurement were not used. For this reason, the originally 1451 ft to rooftop or 1518 feet with original antennas Willis Tower (formerly Sears Tower) was generally accepted as being the tallest building continuously after its completion in 1973, and being taller than both World Trade Center towers, in spite of the fact the 1 World Trade Center Tower (North Tower) possessed a higher pinnacle absolute height after it added its 360 ft radio antenna (total height of 1727 feet or 526.3 meters) in 1978. The 1 World Trade Center building maintained a higher absolute height to antenna top until the Sears Tower enlarged its own radio antenna in 2000 to a total height of 1730 feet. However, the Willis Tower was always considered the taller building because it still possessed a greater height to its architectural top (1451 feet vs. 1362 feet), and thus its status as the world's tallest was generally not contested.
Other historic cases in which a building with a taller absolute pinnacle height was not considered the tallest building include, in 1905 when the former New York Times building or The Times Square Building (at 229 West 43rd Street in New York) was completed at 111 m to the roof with 128 m including a flagpole. That building was never considered to be taller than the 119 m high then-current record-holder Park Row Building of New York because a flagpole is not an integral architectural part of a building.
Prior to 1998 the tallest building status had been contested on occasion, but the disputes did not result in a change of the criteria used to determine the world's tallest building. A famous historical case of this discrepancy was the rivalry between The Trump Building (then known as the Bank of Manhattan Building) and the Chrysler Building. The Bank of Manhattan Building employed only a short spire and was 927 ft tall and had a much higher top occupied floor (the second category in the 1996 criteria for tallest building). In contrast, the Chrysler Building employed a very large 125 ft spire secretly assembled inside the building to claim the title of world's tallest building with a total height of 1048 ft, despite having a lower top occupied floor and a shorter height when both buildings' spires are not counted in their heights. Upset by Chrysler’s victory, Shreve & Lamb, the consulting architects of Bank of Manhattan building, wrote a newspaper article claiming that their building was actually the tallest, since it contained the world's highest usable floor. They pointed out that the observation deck in the Bank of Manhattan Building was nearly 100 ft above the top floor in the Chrysler Building, whose surpassing spire was strictly ornamental and essentially inaccessible. However, the Chrysler Building was generally accepted as the tallest building in the world despite their protests.
However, none of the previous discrepancies or disputes in criteria to measure height (spires vs antennas, absolute pinnacle height vs. architectural height, height of highest occupied floor, etc.) resulted in the controversy that occurred upon the completion of the Petronas Towers in Kuala Lumpur, Malaysia in 1998. The Petronas Towers possessed a higher architectural height (spires, but not antennas), but a lower absolute pinnacle height and lower top occupied floor than the previous record-holder Willis Tower in Chicago, United States. Counting buildings as structures with floors throughout, and with antenna masts excluded, Willis Tower was still considered the tallest at that time. When the Petronas Twin Towers were built, controversy arose because their spires extended nine metres higher than the roof of Willis Tower. Excluding their spires, the Petronas Towers are not taller than Willis Tower. At their convention in Chicago, the Council on Tall Buildings and Urban Habitat (CTBUH) found the Willis Tower to be the third-tallest building, and the Petronas Towers to be the world's tallest buildings. This decision caused a considerable amount of controversy in the news media because this was the first time a country outside the United States had held the world’s tallest building record. Therefore, the CTBUH revised their criteria and defined four categories in which the world's tallest building can be measured, by retaining the old criterion of height to architectural top and added three new categories
The height to roof criterion was discontinued because relatively few modern tall buildings possess flat rooftops, making this criterion difficult to determine and measure. The CBTUH has further clarified their definitions of building height, including specific criteria concerning subbasements and ground level entrances (height measured from lowest, significant, open-air, pedestrian entrance rather than from a previously undefined "main entrance"), building completion (must be topped out both structurally and architecturally, fully clad, "and " able to be occupied), condition of the highest occupied floor (must be continuously used by people living or working and be conditioned, thus including observation decks, but not mechanical floors) and other aspects of tall buildings.
The height is measured from the level of the lowest, significant, open-air, pedestrian entrance. At the time, the Willis Tower held first place in the second and third categories, the Petronas Towers held the first category, and the 1 World Trade Center building held the fourth with its antenna height to top of pinnacle. In 2000, however, a new antenna mast was placed on the Willis Tower, giving it hold of the fourth category. On April 20, 2004, Taipei 101 in Taipei, Taiwan, was completed. Its completion gave it the world record for the first three categories. On July 21, 2007 it was announced that Burj Khalifa had surpassed Taipei 101 in height, reaching 512 m.
Since being completed in early 2010, Burj Khalifa leads in all categories (the first building to do so). With a spire height of 829.8 m, Burj Khalifa surpassed Taipei 101 as the tallest building to architectural detail and the Willis Tower as the tallest building to tip. It also leads in the category of highest occupied floor.
Before Burj Khalifa was completed, Willis Tower led in the fourth category with 527 m, previously held by the World Trade Center until the extension of the Chicago tower's western broadcast antenna in 2000, over a year prior to the World Trade Center's destruction in 2001. Its antenna mast included, One World Trade Center measured 526 m. The World Trade Center became the world's tallest buildings to be destroyed or demolished; indeed, its site entered the record books twice on September 11, 2001, in that category, replacing the Singer Building, which once stood a block from the World Trade Center site. A different superlative for skyscrapers is their number of floors. The World Trade Center set that at 110, and this was not surpassed for nearly four decades until the Burj Khalifa, which opened in 2010.
Structures such as the CN Tower, the Ostankino Tower and the Oriental Pearl Tower are excluded from these categories because they are not "habitable buildings", which are defined as frame structures made with floors and walls throughout.
World's tallest freestanding structure on land.
Freestanding structures include observation towers, monuments and other structures not generally considered to be "Habitable buildings", but excludes supported structures such as guyed masts and ocean drilling platforms.
"(See also history of tallest skyscrapers.)"
The world's tallest freestanding structure on land is defined as the tallest self-supporting artificial structure that stands above ground. This definition is different from that of world's tallest building or world's tallest structure based on the percentage of the structure that is occupied and whether or not it is self-supporting or supported by exterior cables. Likewise, this definition does not count structures that are built underground or on the seabed, such as the Petronius Platform in the Gulf of Mexico. Visit world's tallest structure by category for a list of various other definitions.
As of May 12, 2008, the tallest freestanding structure on land is the Burj Khalifa in Dubai, United Arab Emirates. The building, which now stands at 829.8 m, surpassed the height of the previous record holder, the 553.3 m CN Tower in Toronto, Ontario, on September 12, 2007. It was completed in 2010, and was topped out at 829.8 m in January 2009.
History.
The following is a list of structures that have held the title as the tallest freestanding structure on land. "(See also Timeline of three tallest structures in the world until Empire State Building)."
Notable mentions include the Pharos (lighthouse) of Alexandria, built in the third century BC and estimated between 115 –. It was the world's tallest non-pyramidal building for many centuries. Another notable mention includes the Jetavanaramaya stupa in Anuradhapura, Sri Lanka, which was built in the third century, and was similarly tall at 122 m. These were both the world's tallest or second tallest non-pyramidal buildings for over a thousand years.
The tallest "secular" building between the collapse of the Pharos and the erection of the Washington Monument may have been the Torre del Mangia in Siena, which is 102 m tall, and was constructed in the first half of the fourteenth century, and the 97 m tall Torre degli Asinelli in Bologna, also Italy, built between 1109 and 1119.
World's highest observation deck.
Timeline of development of world's highest observation deck since inauguration of Eiffel Tower.
Higher observation decks have existed on mountain tops or cliffs, rather than on tall structures. For example, the Royal Gorge Bridge in Cañon City, Colorado, USA, was constructed in 1929 spanning the Royal Gorge at a height of 321 m above the Arkansas River. The Grand Canyon Skywalk, constructed in 2007, protrudes 70 ft over the west rim of the Grand Canyon and is approximately 1100 m above the Colorado River, making it the highest of these types of structures.
Timeline of guyed structures on land.
As most of the tallest structures are guyed masts, here is a timeline of world's tallest guyed masts, since the beginning of radio technology.
As many large guyed masts were destroyed at the end of World War II, the dates for the years between 1945 and 1950 may be incorrect. If Wusung Radio Tower survived World War II, it was the tallest guyed structure shortly after World War II.
Tallest structures, freestanding structures, and buildings.
The list categories are:
Notes:
Source: 
Under construction.
Numerous supertall skyscrapers are in various stages of proposal, planning, or construction. Each of the following are under construction and, depending on the order of completion, could become the world's tallest building or structure in at least one category:
Proposed.
Many proposed structures may never be built. 

</doc>
<doc id="34082" url="http://en.wikipedia.org/wiki?curid=34082" title="Windows Media Audio">
Windows Media Audio

Windows Media Audio (WMA) is an audio data compression technology developed by Microsoft. The name can be used to refer to its audio file format or its audio codecs. It is a proprietary technology that forms part of the Windows Media framework. WMA consists of four distinct codecs. The original WMA codec, known simply as "WMA", was conceived as a competitor to the popular MP3 and RealAudio codecs. "WMA Pro", a newer and more advanced codec, supports multichannel and high resolution audio. A lossless codec, "WMA Lossless", compresses audio data without loss of audio fidelity (the regular WMA format is lossy). "WMA Voice", targeted at voice content, applies compression using a range of low bit rates.
Development history.
The first WMA codec was based on earlier work by Henrique Malvar and his team which was transferred to the Windows Media team at Microsoft. Malvar was a senior researcher and manager of the Signal Processing Group at Microsoft Research, whose team worked on the "MSAudio" project. The first finalized codec was initially referred to as "MSAudio 4.0". It was later officially released as "Windows Media Audio", as part of Windows Media Technologies 4.0. Microsoft claimed that WMA could produce files that were half the size of equivalent-quality MP3 files; Microsoft also claimed that WMA delivered "near CD-quality" audio at 64 kbit/s. The former claim however was rejected by some audiophiles. RealNetworks also challenged Microsoft's claims regarding WMA's superior audio quality compared to RealAudio.
Newer versions of WMA became available: "Windows Media Audio 2" in 1999, "Windows Media Audio 7" in 2000, "Windows Media Audio 8" in 2001, and "Windows Media Audio 9" in 2003. Microsoft first announced its plans to license WMA technology to third parties in 1999. Although earlier versions of Windows Media Player played WMA files, support for WMA file creation was not added until the seventh version. In 2003, Microsoft released new audio codecs that were not compatible with the original WMA codec. These codecs were "Windows Media Audio 9 Professional", "Windows Media Audio 9 Lossless", and "Windows Media Audio 9 Voice".
All versions of WMA released since version 9.0 - namely 9.1, 9.2 and 10 - have been backwards compatible with the original v9 decoder and are therefore not considered separate codecs. The sole exception to this is the WMA 10 Professional codec whose Low Bit Rate (LBR) mode is only backwards compatible with the older WMA Professional decoders at half sampling rate (similar to how HE-AAC is backwards compatible with AAC-LC). Full fidelity decoding of WMA 10 Professional LBR bitstreams requires a WMA version 10 or newer decoder.
Container format.
A WMA file is in most circumstances contained in the Advanced Systems Format (ASF), a proprietary Microsoft container format for digital audio or digital video. The ASF container format specifies how metadata about the file is to be encoded, similar to the ID3 tags used by MP3 files. Metadata may include song name, track number, artist name, and also audio normalization values. This container can optionally support digital rights management (DRM) using a combination of elliptic curve cryptography key exchange, DES block cipher, a custom block cipher, RC4 stream cipher and the SHA-1 hashing function. See Windows Media DRM for further information.
Since 2008 Microsoft has also been using WMA Professional in its Protected Interoperable File Format (PIFF) based on the ISO Base Media File Format and most commonly used for Smooth Streaming, a form of adaptive bit rate streaming over HTTP. Related industry standards such as DECE UltraViolet and MPEG-DASH have not standardized WMA as a supported audio codec, deciding in favor of the more industry-prevalent MPEG and Dolby audio codecs.
Codecs.
Each WMA file features a single audio track in one of the four sub-formats: WMA, WMA Pro, WMA Lossless, or WMA Voice. Each codec is further explained below.
Windows Media Audio.
Windows Media Audio (WMA) is the most common codec of the four WMA codecs. Colloquial usage of the term "WMA", especially in marketing materials and device specifications, usually refers to this codec only. The first version of the codec released in 1999 is regarded as WMA 1. In the same year, the bit stream syntax, or compression algorithm, was altered in minor ways and became WMA 2. Since then, newer versions of the codec have been released, but the decoding process remained the same, ensuring compatibility between codec versions. WMA is a lossy audio codec based on the study of psychoacoustics. Audio signals that are deemed to be imperceptible to the human ear are encoded with reduced resolution during the compression process.
WMA can encode audio signals sampled at up to 48 kHz with up to two discrete channels (stereo). WMA 9 introduced variable bit rate (VBR) and average bit rate (ABR) coding techniques into the MS encoder although both were technically supported by the original format. WMA 9.1 also added support for low-delay audio, which reduces latency for encoding and decoding.
Fundamentally, WMA is a transform coder based on modified discrete cosine transform (MDCT), somewhat similar to AAC, Cook and Vorbis. The bit stream of WMA is composed of superframes, each containing 1 or more frames of 2048 samples. If the bit reservoir is not used, a frame is equal to a superframe. Each frame contains a number of blocks, which are 128, 256, 512, 1024, or 2048 samples long after being transformed into the frequency domain via the MDCT. In the frequency domain, masking for the transformed samples is determined, and then used to requantize the samples. Finally, the floating point samples are decomposed into coefficient and exponent parts and independently huffman coded. Stereo information is typically mid/side coded. At low bit rates, line spectral pairs (typically less than 17 kbit/s) and a form of noise coding (typically less than 33 kbit/s) can also be used to improve quality.
Like AAC and Ogg Vorbis, WMA was intended to address perceived deficiencies in the MP3 standard. Given their common design goals, the three formats ended up making similar design choices. All three are pure transform codecs. Furthermore the MDCT implementation used in WMA is essentially a superset of those used in Ogg and AAC such that WMA iMDCT and windowing routines can be used to decode AAC and Ogg Vorbis almost unmodified. However, quantization and stereo coding is handled differently in each codec. The primary distinguishing trait of the WMA Standard format is its unique use of 5 different block sizes, compared to MP3, AAC, and Ogg Vorbis which each restrict files to just two sizes. WMA Pro extends this by adding a 6th block size used at 88.2/96 kHz sampling rate.
Certified PlaysForSure devices, as well as a large number of uncertified devices, ranging from portable hand-held music players to set-top DVD players, support the playback of WMA files. Most PlaysForSure-certified online stores distribute content using this codec only. In 2005, Nokia announced its plans to support WMA playback in future Nokia handsets. In the same year, an update was made available for the PlayStation Portable (version 2.60) which allowed WMA files to be played on the device for the first time.
Windows Media Audio Professional.
Windows Media Audio Professional (WMA Pro) is an improved lossy codec closely related to WMA standard. It retains most of the same general coding features, but also features improved entropy coding and quantization strategies as well as more efficient stereo coding. Notably, many of the WMA standard's low bitrate features have been removed, as the core codec is designed for efficient coding at most bitrates. Its main competitors include AAC, HE-AAC, Vorbis, Dolby Digital, and DTS. It supports 16-bit and 24-bit sample bit depth, sampling rates up to 96 kHz and up to eight discrete channels (7.1 channel surround). WMA Pro also supports dynamic range compression, which reduces the volume difference between the loudest and quietest sounds in the audio track. According to Microsoft's Amir Majidimehr, WMA Pro can technically go beyond 7.1 surround sound and support "an unlimited number of channels."
The codec's bit stream syntax was frozen at the first version, WMA 9 Pro. Later versions of WMA Pro introduced low-bit rate encoding, low-delay audio, frequency interpolation mode, and an expanded range of sampling rate and bit-depth encoding options. A WMA 10 Pro file compressed with frequency interpolation mode comprises a WMA 9 Pro track encoded at half the original sampling rate, which is then restored using a new compression algorithm. In this situation, WMA 9 Pro players which have not been updated to the WMA 10 Pro codec can only decode the lower quality WMA 9 Pro stream. Starting with WMA 10 Pro, eight channel encoding starts at 128 kbit/s, and tracks can be encoded at the native audio CD resolution (44.1 kHz, 16-bit), previously the domain of WMA Standard.
Despite a growing number of supported devices and its superiority over WMA, WMA Pro still has little hardware and software support. Some notable exceptions to this are the Microsoft Zune (limited to stereo), Xbox 360, Windows Mobile-powered devices with Windows Media Player 10 Mobile, newer Toshiba Gigabeat and Motorola devices, and devices running recent versions of the Rockbox alternative firmware. In addition, WMA Pro is a requirement for the WMV HD certification program. On the software side, Verizon utilizes WMA 10 Pro for its V CAST Music Service, and Windows Media Player 11 has promoted the codec as an alternative to WMA for copying audio CD tracks. WMA Pro is supported in Silverlight as of version 2 (though only in stereo mode). In the absence of the appropriate audio hardware, WMA Pro can automatically downmix multichannel audio to stereo or mono, and 24-bit resolution to 16-bit during playback.
A notable example of WMA Pro being used instead of WMA Standard is the website which uses WMA 10 Pro in its low-bitrate mode at 48 kbit/s.
Windows Media Audio Lossless.
Windows Media Audio 9 Lossless is a lossless incarnation of Windows Media Audio, an audio codec by Microsoft, released in early 2003. It compresses an audio CD to a range of 206 to 411MB, at bit rates of 470 to 940 kbit/s. The result is a bit-for-bit duplicate of the original audio file; in other words, the audio quality on the CD will be the same as the file when played back. WMA Lossless uses the same .WMA file extension as other Windows Media Audio formats. It supports 6 discrete channels and up to 24-bit/96 kHz lossless audio. The format has never been publicly documented, although an open source decoder has been reverse engineered for non-Microsoft platforms by the libav and ffmpeg projects.
Windows Media Audio Lossless (WMA Lossless) is a lossless audio codec that competes with ATRAC Advanced Lossless, Dolby TrueHD, DTS-HD Master Audio, Shorten, Monkey's Audio, FLAC, Apple Lossless, and WavPack (Since late 2011, the last three have the advantage of being open source software and available for nearly any operating system.) Designed for archival purposes, it compresses audio signals without loss of quality from the original using VBR. When decompressed, the audio signal is an exact replica of the original. The first version of the codec, WMA 9 Lossless, and its revisions support up to 96 kHz, 24-bit audio for up to 6 discrete channels (5.1 channel surround) with dynamic range compression control. The typical compression ratio for music varies between 1.7:1 and 3:1.
Hardware support for the codec is available on the Cowon A3, Cowon S9, Bang & Olufsen Serenata Sony Walkman NWZ-A and NWZ-S series, Zune 4, 8, 80 30, Zune 120 (with firmware version 2.2 or later) and the new Zune HD, Xbox 360, Windows Mobile-powered devices with Windows Media Player 10 Mobile, Windows Phone (version 8 and above), Toshiba Gigabeat S and V models, Toshiba T-400, the Meizu M3, and Best Buy's Insignia NS-DV, Pilot, and Sport music players. The Logitech Squeezebox Touch now supports the format natively despite previously only supporting it via transcoding. Like WMA Standard, WMA Lossless is being used by a few online stores to distribute music online. Similar to WMA Pro, the WMA Lossless decoder can perform downmixing when capable audio hardware is not present. As of 2012, the ffmpeg and libav projects have open source WMA Lossless decoders based on reverse engineering of the official decoder. Only 16-bit WMA files can be successfully decoded by ffmpeg as of June 20, 2012.
Windows Media Audio Voice.
Windows Media Audio Voice (WMA Voice) is a lossy audio codec that competes with Speex (used in Microsoft's own Xbox Live online service), ACELP, and other codecs. Designed for low-bandwidth, voice playback applications, it employs low-pass and high-pass filtering of sound outside the human speech frequency range to achieve higher compression efficiency than WMA. It can automatically detect sections of an audio track containing both voice and music and use the standard WMA compression algorithm instead. WMA Voice supports up to 22.05 kHz for a single channel (mono) only. Encoding is limited to constant bit rate (CBR) and up to 20 kbit/s. The first and only version of the codec is WMA 9 Voice.
Windows Mobile-powered devices with Windows Media Player 10 Mobile have native support for WMA 9 Voice playback. In addition, BBC World Service has employed WMA Voice for its Internet radio streaming service.
Sound quality.
Microsoft claims that audio encoded with WMA sounds better than MP3 at the same bit rate; Microsoft also claims that audio encoded with WMA at lower bit rates sound better than MP3 at higher bit rates. Double blind listening tests with other lossy audio codecs have shown varying results, from failure to support Microsoft's claims about its superior quality to supremacy over other codecs. One independent test conducted in May 2004 at 128 kbit/s showed that WMA was roughly equivalent to LAME MP3; inferior to AAC and Vorbis; and superior to ATRAC3 (software version).
Some conclusions made by recent studies:
Criticism of claimed quality.
Microsoft's claims of WMA sound quality have frequently drawn complaints. "Some audiophiles challenge Microsoft's claims regarding WMA's quality," according to a published article from EDN. Another article from MP3 Developments wrote that Microsoft's claim about CD-quality audio at 64 kbit/s with WMA was "very far from the truth." At the early stages of WMA's development, a representative from RealNetworks claimed that WMA was a "clear and futile effort by Microsoft to catch up with RealAudio 8."
Microsoft has sometimes claimed that the sound quality of WMA at 64 kbit/s equals or exceeds that of MP3 at 128 kbit/s (both WMA and MP3 are considered near-transparent at 192 kbit/s by most listeners). In a 1999 study funded by Microsoft, National Software Testing Laboratories (NSTL) found that listeners preferred WMA at 64 kbit/s to MP3 at 128 kbit/s (as encoded by MusicMatch Jukebox). However, a September 2003 public listening test conducted by Roberto Amorim found that listeners preferred 128 kbit/s MP3 to 64 kbit/s WMA audio with greater than 99% confidence. This conclusion applied equally to other codecs at the same bitrate, leading him to conclude that:
No codec delivers the marketing plot of same quality as MP3 at half the bitrates.
It is important to note that both MP3 and WMA encoders have undergone active development and improvement for many years, so their relative quality may change over time.
A July 2007 public listening test by Sebastian Mares found that 64 kbit/s HE-AAC audio (encoded by Nero Digital) was statistically tied with 64 kbit/s WMA Pro audio, in terms of listener preference.
Players.
Apart from Windows Media Player, most of the WMA compression formats can be played using ALLPlayer, VLC media player, Media Player Classic, MPlayer, RealPlayer, Winamp, Zune Software (with certain limitations—DSP plugin support and DirectSound output is disabled using the default WMA plugin), and many other software media players. The Microsoft Zune media management software supports most WMA codecs, but uses a variation of Windows Media DRM which is used by PlaysForSure.
The FFmpeg project has reverse-engineered and re-implemented the WMA codecs to allow their use on POSIX-compliant operating systems such as Linux. The rockbox project further extended this codec to be suitable for embedded cores, allowing playback on portable MP3 players and cell phones running open source software. RealNetworks has announced plans to support playback of DRM-free WMA files in RealPlayer for Linux. On the Macintosh platform, Microsoft released a PowerPC version of Windows Media Player for Mac OS X in 2003, but further development of the software has ceased. Microsoft currently endorses the third-party Flip4Mac WMA, a QuickTime component that allows Macintosh users to play WMA files in any player that uses the QuickTime framework. Flip4Mac, however, does not currently support the Windows Media Audio Voice codec.
Encoders.
Software that can export audio in WMA format include Windows Media Player, Windows Movie Maker, Microsoft Expression Encoder, Sony Sound Forge, GOM Player, RealPlayer, Adobe Premiere Pro, Adobe Audition, and Adobe Soundbooth. Microsoft Office OneNote supports encoding in all WMA codecs, and Windows Media Encoder supports all available bit rate and resolution options as well. Open source players like VLC media player can also do some encoding.
Digital rights management.
The WMA codecs are most often used with the ASF container format, which has an optional DRM facility. Windows Media DRM, which can be used in conjunction with WMA, supports time-limited music subscription services such as those offered by unlimited download services, including MTV's URGE, Napster, Rhapsody, Yahoo! Music Unlimited, and Virgin Digital. Windows Media DRM, a component of PlaysForSure and Windows Media Connect, is supported on many modern portable audio devices and streaming media clients such as Roku, SoundBridge, Xbox 360, and Wii. Players that support the WMA format but not Windows Media DRM cannot play DRM-protected files.

</doc>
<doc id="34085" url="http://en.wikipedia.org/wiki?curid=34085" title="Wolverine">
Wolverine

The wolverine (), "Gulo gulo" (Gulo is Latin for "glutton"), also referred to as glutton, carcajou, skunk bear, or quickhatch, is the largest land-dwelling species of the family Mustelidae (weasels). It is a stocky and muscular carnivore, more closely resembling a small bear than other mustelids. The wolverine, a solitary animal, has a reputation for ferocity and strength out of proportion to its size, with the documented ability to kill prey many times larger than itself.
The wolverine can be found primarily in remote reaches of the Northern boreal forests and subarctic and alpine tundra of the Northern Hemisphere, with the greatest numbers in northern Canada, the U.S. state of Alaska, the Nordic countries of Europe, and throughout western Russia and Siberia. Their populations have experienced a steady decline since the 19th century in the face of trapping, range reduction and habitat fragmentation, such that they are essentially absent from the southern end of their European range.
Taxonomy.
Genetic evidence suggests that the wolverine is most closely related to the tayra and martens (scientific names "Eira" and "Martes", respectively), all of which shared a Eurasian ancestor.
Within the "Gulo" genus, a clear separation occurs between two subspecies: the Old World form "Gulo gulo gulo" and the New World form "G. g. luscus". Some authors had described as many as four additional North American subspecies, including ones limited to Vancouver Island ("G. g. vancouverensis") and the Kenai Peninsula in Alaska ("G. g. katschemakensis"). However, the most currently accepted taxonomy recognizes either the two continental subspecies or recognize "G. gulo" as a single Holarctic taxon.
Hall regards the North American Wolverine as a species ("Gulo luscus") distinct from the Eurasian Wolverine ("Gulo gulo").
Recently compiled genetic evidence suggests most of North America's wolverines are descended from a single source, likely originating from Beringia during the last glaciation and rapidly expanding thereafter, though considerable uncertainty to this conclusion is due to the difficulty of collecting samples in the extremely depleted southern extent of the range.
Physical characteristics.
Anatomically, the wolverine is a stocky and muscular animal. With short legs, broad and rounded head, small eyes and short rounded ears, it resembles a bear more than other mustelids. Though its legs are short, its large, five-toed paws and plantigrade posture facilitate movement through deep snow.
The adult wolverine is about the size of a medium dog, with a length usually ranging from 65 –, a tail of 17 –, and a weight of 9 –, though exceptionally large males can weigh up to 32 kg. The males are as much as 30% larger than the females and can be twice the females' weight. Shoulder height is reported from 30 to. It is the largest of terrestrial mustelids; only the marine-dwelling sea otter and giant otter of the Amazon basin are larger.
Wolverines have thick, dark, oily fur which is highly hydrophobic, making it resistant to frost. This has led to its traditional popularity among hunters and trappers as a lining in jackets and parkas in Arctic conditions. A light-silvery facial mask is distinct in some individuals, and a pale buff stripe runs laterally from the shoulders along the side and crossing the rump just above a 25 – bushy tail. Some individuals display prominent white hair patches on their throats or chests.
Like many other mustelids, it has potent anal scent glands used for marking territory and sexual signaling. The pungent odor has given rise to the nicknames "skunk bear" and "nasty cat." Wolverines, like other mustelids, possess a special upper molar in the back of the mouth that is rotated 90 degrees, towards the inside of the mouth. This special characteristic allows wolverines to tear off meat from prey or carrion that has been frozen solid.
Behavior.
Diet and hunting.
The wolverine is a powerful and versatile predator and scavenger. Prey mainly consists of small to medium-sized mammals, but the wolverine has been recorded killing prey such as adult deer that are many times larger than itself. Prey species include porcupines, squirrels, beavers, marmots, rabbits, voles, mice, shrews, lemmings, caribou, roe deer, white-tailed deer, mule deer, sheep, moose, and elk. Smaller predators are occasionally preyed on, including martens, mink, foxes, Eurasian lynx, weasels, and coyote and wolf pups. Wolverines often pursue live prey that are relatively easy to obtain, including animals caught in traps, newborn mammals, and deer (including adult moose and elk) when they are weakened by winter or immobilized by heavy snow. Their diets are sometimes supplemented by birds' eggs, birds (especially geese), roots, seeds, insect larvae, and berries. A majority of the wolverine's sustenance is derived from carrion, on which they depend almost exclusively in winter and early spring. Wolverines may find carrion themselves, feed on it after the predator is done feeding (especially wolf packs) or simply take it from another predator. Whether eating live prey or carrion, the wolverine's feeding style appears voracious, leading to the nickname of "glutton" (also the basis of the scientific name). However, this feeding style is believed to be an adaptation to food scarcity, especially in winter.
Armed with powerful jaws, sharp claws, and a thick hide, wolverines, like most mustelids, are remarkably strong for their size. They may defend kills against larger or more numerous predators such as wolves or bears. At least one account reported a wolverine's apparent attempt to steal a kill from a black bear, although the bear won what was ultimately a fatal contest. In another account, a wolverine attacked a polar bear and clung to its throat until the bear suffocated. Wolverines are known to follow wolf and lynx trails, purportedly with the intent of scavenging the remains of their kills. Wolves are thought to be their most important natural predator, with the arrival of wolves to a wolverine's territory presumably leading the latter to abandon the area.
Wolverines inhabiting the Old World (specifically, Fennoscandia) hunt more actively than their North American relatives. This may be because competing predator populations in Eurasia are not as dense, making it more practical for the wolverine to hunt for itself than to wait for another animal to make a kill and then try to snatch it. They often feed on carrion left by wolves, so changes in wolf populations may affect the population of wolverines. They are also known on occasion to eat plant material.
Mating and reproduction.
Successful males will form lifetime relationships with two or three females, which they will visit occasionally, while other males are left without a mate. Mating season is in the summer, but the actual implantation of the embryo (blastocyst) in the uterus is stayed until early winter, delaying the development of the fetus. Females will often not produce young if food is scarce. The gestation period is 30–50 days, and litters of typically two or three young ("kits") are born in the spring. Kits develop rapidly, reaching adult size within the first year of a lifespan that may reach anywhere from five to (in exceptional individuals) 13 years. Fathers make visits to their offspring until they are weaned at 10 weeks of age; also, once the young are about six months old, some reconnect with their fathers and travel together for a time.
Distribution.
 Wolverines live primarily in isolated arctic and alpine regions of northern Canada, Alaska, Siberia, and Scandinavia; they are also native to European Russia, the Baltic countries, the Russian Far East, northeast China and Mongolia. In 2008 and 2009, wolverines were sighted as far south as the Sierra Nevada, near Lake Tahoe, for the first time since 1922. They are also found in low numbers in the Rocky Mountains and northern Cascades of the United States, and have been sighted as far south and east as Michigan. However, most New World wolverines live in Canada.
Conservation.
The world's total wolverine population is not known. The animal exhibits a low population density and requires a very large home range.
The range of a male wolverine can be more than 620 km2 (240 mi2), encompassing the ranges of several females which have smaller home ranges of roughly 130–260 km2 (50–100 mi2). Adult wolverines try for the most part to keep nonoverlapping ranges with adults of the same sex. Radio tracking suggests an animal can range hundreds of miles in a few months.
Female wolverines burrow into snow in February to create a den, which is used until weaning in mid-May. Areas inhabited nonseasonally by wolverines are thus restricted to zones with late-spring snowmelts. This fact has led to concern that global warming will shrink the ranges of wolverine populations.
This requirement for large territories brings wolverines into conflict with human development, and hunting and trapping further reduce their numbers, causing them to disappear from large parts of their former range; attempts to have them declared an endangered species have met with little success. In February 2013, the United States Fish and Wildlife Service proposed giving Endangered Species Act protections to the wolverine due to its winter habitat in the northern Rockies diminishing. This was as a result of a lawsuit by brought by the Center for Biological Diversity and Defenders of Wildlife.
The Wildlife Conservation Society reported in June 2009 that a wolverine researchers had been tracking for almost three months had crossed into northern Colorado. Society officials had tagged the young male wolverine in Wyoming near Grand Teton National Park and it had traveled southward for about 500 miles. It was the first wolverine seen in Colorado since 1919, and its appearance was also confirmed by the Colorado Division of Wildlife. In February 2014, a wolverine was seen in Utah, the first confirmed sighting in that state in 30 years.
In captivity.
Around a hundred wolverines are held in zoos across North America and Europe, and they have been bred in captivity, but only with difficulty and high infant mortality.
Name.
The wolverine's questionable reputation as an insatiable glutton (reflected in the Latin genus name "Gulo") may be in part due to a false etymology. The animal's name in older Norwegian, "fjeldfross", meaning "mountain cat", is thought to have worked its way into German as "Vielfrass", which means "glutton" (literally "devours much"). Its name in other West Germanic languages is similar (e.g. Dutch: "veelvraat").
The Finnish name is "ahma", derived from "ahmatti," which is translated as "glutton". Similarly, the Estonian name is "ahm", with the equivalent meaning to the Finnish name. In Lithuanian is "ernis", in Latvian—"tinis" or "āmrija".
The Eastern Slavic росомаха ("rosomakha") and the Polish and Czech name "rosomák" seem to be borrowed from the Finnish "rasva-maha" (fat belly). Similarly, the Hungarian name is "rozsomák" or "torkosborz" which means "gluttonous badger".
In French-speaking parts of Canada, the wolverine is referred to as "carcajou", borrowed from the Innu-aimun or Montagnais "kuàkuàtsheu". However in France, the wolverine's name is "glouton" (glutton).
Purported gluttony is reflected neither in the English name "wolverine" nor in the names used in North Germanic languages. The English word wolverine (alteration of the earlier form wolvering of uncertain origin) probably implies "a little wolf". The name in Proto-Norse, "erafaz" and Old Norse, "jarfr", lives on in the regular Icelandic name "jarfi", regular Norwegian name "jerv", regular Swedish name "järv" and regular Danish name "jærv".
In culture.
Many cities, teams, and organizations use the wolverine as a mascot. For example, the US state of Michigan is, by tradition, known as "the Wolverine State", and the University of Michigan takes the wolverine as its mascot. The association is well and long established: for example, many Detroiters volunteered to fight during the American Civil War and George Armstrong Custer, who led the Michigan Brigade, called them the "Wolverines". The origins of this association are obscure; it may derive from a busy trade in wolverine furs in Sault Ste. Marie in the 18th century or may recall a disparagement intended to compare early settlers in Michigan with the vicious mammal. Wolverines are, however, extremely rare in Michigan. A sighting in February 2004 near Ubly was the first confirmed sighting in Michigan in 200 years. The animal was found dead in 2010 and the story recounted in the book "The Lone Wolverine".
The wolverine figures prominently in the mythology of the Innu people of eastern Québec and Labrador. In at least one Innu myth, it is the creator of the world.
Wolverine is the name of a popular fictional character appearing in "X-Men" books published by Marvel Comics—named for his highly individualistic and aggressive behavior, as well as his great ferocity despite his small stature.
Film.
The 91-minute 1994 motion picture "Running Free" (also known as "One Paw") is about a young boy and his friendship with an Alaskan wolverine. The wolverines seen in the film were born in captivity and directed by a USDA-licensed filmmaker, Steve Kroschel. Many of the wolverine scenes are documentary footage of trained wolverines being filmed in their natural habitat. The movie was screened on 5 October 1994. The American Humane Society was involved before the start of filming and during some of the filming.
The first full-length nature documentary about wild wolverines, "Wolverines – Hyenas of the North", was produced in 2006 by German wildlife film company Gulo Film Productions for German Television (NDR), and has been broadcast in many countries – also under the titles "Wolverine X" or "Wolverine Revealed", and in the US by Animal Planet as an episode of "Mutual of Omaha's Wild Kingdom". The film by German director Oliver Goetzl shows many different social behaviour aspects of wild wolverines at the Finnish / Russian border area – some of them previously unknown – and has won more than 30 international festival awards and nominations, including at Jackson Hole Wildlife Film Festival, Wildscreen Film Festival, IWFF Missoula, Animal Behavior Society Film Festival and Banff World Television Awards.
The PBS series "Nature" released a documentary, "Wolverine: Chasing the Phantom" as episode #166 on 14 November 2010. This 53-minute documentary focuses on the efforts of a number of naturalists in the United States to track wolverines, collect genetic data, and learn more about wolverine populations, individual behavior and social behavior. It also tracks the raising of two male wolverines in captivity at an Alaska nature reserve from birth to maturity, and profiles the naturalists making these efforts.

</doc>
<doc id="34094" url="http://en.wikipedia.org/wiki?curid=34094" title="Western Bulldogs">
Western Bulldogs

"For the club's Victorian Football League reserves team, see Footscray Bulldogs (VFL)"
The Footscray Football Club, currently known as the Western Bulldogs, is an Australian rules football club that plays in the Australian Football League (AFL). Founded in 1877, the club won nine premierships in the Victorian Football Association (VFA) between 1898 and 1924, and has won one premiership in 1954 since joining the Victorian Football League (VFL), the predecessor to the AFL, in 1925.
The Western Bulldogs home guernsey features two thick horizontal "hoops"—one red and one white—on a royal blue background. The club's traditional rivals include St Kilda and geographical rivals Essendon.
The club has its headquarters and practices at the Whitten Oval in Footscray, an inner-western suburb of Melbourne, Australia. The club draws its supporter base from this traditionally working class area and plays its home matches at Docklands Stadium (currently known as Etihad Stadium) in the Melbourne Docklands area, also in the city's inner-west. In October 1996, the Club began to play under its current name of the "Western Bulldogs", changing from its original name of the "Footscray Football Club". The Whitten Oval underwent a A$20 million redevelopment starting in 2005 to improve the Club's headquarters and training facilities.
Club history.
VFA Powerhouse and Championship of Victoria.
Footscray, also known as the Prince Imperials from 1880–1882, played in the junior division of the VFA before joining the senior division of the VFA in 1886. Following the famed breakaway of 1896, during which the stronger VFA clubs formed the VFL, the tricolours (as they were known during this period) became a force in the VFA. The Club went on to win 9 premierships between 1898 and 1924. This included a hat-trick from 1898 to 1900 and four premierships between 1919 and 1924. The 1924 premiership would be Footscray's last in the VFA. After the 1924 season, the Club challenged the premiers of the VFL, Essendon, to a charity game for the benefit of Dame Nellie Melba's Limbless Soldiers' Appeal.
Joining the VFL.
Starting in 1919, the VFL had nine clubs, which caused one team to be idle every Saturday; the VFL was keen to do away with this bye each week. On the night of 9 January 1925, a committee meeting of the VFL, chaired by Reg Hunt of Carlton, decided to expand the league from nine clubs to twelve. It was decided in the meeting to admit the Footscray Football Club, along with Hawthorn and North Melbourne; all three teams were from the VFA. Hunt originally recommended Hawthorn, Footscray and Prahran, but eventually North Melbourne was substituted for Prahran because of ground control matters.
Footscray adapted relatively quickly to the standard of VFL football despite losing some of their VFA stars, and by 1928 were already a contender for the finals, missing only on percentage in 1931. Though they slipped to eleventh place in 1930, 1935 and 1937, in 1938 they became the first of the new clubs to reach the finals. They fell back drastically in 1939, but played better during the war-torn 1940s, winning their first nine games in 1946.
1950s and E.J. Whitten.
In this period, Footscray failed to win in finals, losing six first semis between 1938 and 1951. In 1953, however, they set a record of conceding only 959 points in the home-and-away games due to a powerful defence featuring Wally Donald, Herb Henderson and Jim Gallagher. They finally won their first semifinal against Essendon, and the following year took out their first VFL premiership, beating Geelong and then Melbourne in the 1954 VFL Grand Final.
This success was in no small part due to two champions of the Club – Charlie Sutton the wily and tough captain-coach at the time, and Ted Whitten snr., otherwise known as 'E.J.' or 'Mr Football', one of Australian Rules Football's best ever players. Charlie claims to have invented the modern play-on style of football – run, handball, run, kick. Whitten was famous for his inventive and lightning-fast "flick pass", which was banned due to the umpire's difficulty in distinguishing whether the ball was thrown, or hit with the open hand.
Footscray failed to capitalise on their premiership success, falling off in the latter part of the decade and finishing with their first wooden spoon in 1959.
1960s.
The decade started promisingly, with the Club bouncing back to reach the 1961 Grand Final, where they were beaten by Hawthorn. This was followed by winning the 1963 and 1964 Night Premierships, although this success was not transferred into the season proper. The rest of the decade was a bleak era for the Club, particularly between 1965 and 1969, when they finished in the bottom three every year.
1970s.
Ted Whitten Snr. retired as a player in 1970 and held the record for the most VFL games played at the time (321 games); he would continue in a coaching capacity until the end of 1971. The Club were relatively strong in the 1970s, but they did not win a final; by decade's end they were back near the bottom.
The main stars of the decade included Gary Dempsey, the heroic ruckman who was badly burnt in Lara bushfire of January 1969 but managed to take out the game's top individual award, the Brownlow Medal, in 1975. Promising South Australian import Neil Sachse had his neck broken in a freak accident while playing against Fitzroy at the Western Oval. He was left quadriplegic.
In 1978, Kelvin Templeton became the first Bulldogs player to kick 100 goals in a season, including a club record of 15.9 in Round 13 against St Kilda.
1980s.
With the disappointing 1970s behind it, the Club introduced an array of stars during this decade. Simon Beasley became a household name after being recruited from Swan Districts in Western Australia to provide the Bulldogs with a genuine replacement for champion Kelvin Templeton. Beasley was to go on to become the Bulldogs' record goal kicker and face of the Club during the mid-1980s.
Mick Malthouse was appointed senior coach in 1984, and a dramatic improvement saw them rise to second position in 1985 before a ten-point loss in the Preliminary Final against Hawthorn. The Club boasted a list of top players at this time, with Beasley, Doug Hawkins, Brian Royal, Rick Kennedy, Steve Wallis, Peter Foster, Michael McLean, Jim Edmond, Andrew Purser, Stephen MacPherson and Brad Hardie. The Bulldogs narrowly missed the finals in 1987 when they were beaten by Melbourne in the last round in front of a record crowd at their home ground.
1989.
Discontent between players, officials and fans reached an all-time low during the 1989 season. Bulldogs president Barrie Beattie was replaced by businessman and prominent racing personality Nick Columb in March. Faced with the prospect of running a club with declining membership and sponsorship, Columb also learned that the team's debt situation was poor, and it reached the point when the VFL looked likely to appoint an administrator to wind up the club's affairs at the end of the year.
Columb decided the best way forward was a merger with the Fitzroy Lions, which was also in a weak financial position, although was not facing immediate bankruptcy. The two clubs announced a merger to form the Fitzroy Bulldogs, but the merger was derailed when the people of Footscray, led by businessman Peter Gordon and a host of others, rallied to raise funds to pay off the club's debts. In further developments, former club player Terry Wheeler was named as Malthouse's replacement as senior coach, while champion veteran wingman Doug Hawkins was appointed captain. While Columb was branded by some as the villain of the story, the wisdom of hindsight shows that had he not instigated the merger, the Western Bulldogs would not exist as it does today.
1990s.
The Bulldogs began the new decade in promising fashion, finishing in seventh place with twelve wins in 1990, including one against eventual premiers Collingwood, when rover Steven Kolyniuk ran around the man on the mark and kicked a goal to put his team in front. Although they just missed out on the finals, there was much to look forward to, and the year was capped off with diminutive rover Tony Liberatore winning the Brownlow Medal.
After a disappointing 1991, the Bulldogs bounced back in 1992, finishing second on the ladder and making their first finals appearance since 1985. Danny Del-Re was an excellent full forward, while champion veterans Hawkins, Royal, Wallis, Foster and MacPherson helped ensure the Club played its best football in many years. Scott Wynd capped a magnificent year with the Brownlow Medal, while Chris Grant and Simon Atkins also had outstanding seasons.
In 1994 and 1995, the Bulldogs again made the finals, only to be eliminated by Melbourne and Geelong, respectively. Leon Cameron and Daniel Southern were stars. In August, Ted Whitten snr. died from prostate cancer; such was his status in the game that he was given a state funeral. In his honour, the Club renamed the Western Oval the Whitten Oval, and a memorial statue of Whitten was erected outside the stadium.
Under the tightly focused management of club president David Smorgon, driven coaching by Terry Wallace, and the on-field leadership of Chris Grant (who narrowly missed a Brownlow Medal in 1996 and 1997) and Tony Liberatore, the Club had a successful period through the mid- to late 1990s, making the finals from 1997 to 2000. The 1997 season is remembered for the Club's cruellest loss, to eventual premiers Adelaide in the preliminary final by two points after leading for much of the game and appearing to be headed for their first grand final since 1961. Rohan Smith, Brad Johnson, Chris Grant, Jose Romero, Paul Hudson and company were catalysts in a fine season.
During Smorgon's term, the Club was renamed from Footscray to "Western Bulldogs" and moved their home games from the Whitten Oval, first to Optus Oval from 1997 to 1999, and then to the newly built Docklands Stadium for the 2000 season.
2000s.
After Terry Wallace's departure at the end of 2002, assistant coach Peter Rohde took charge, but after two miserable seasons, the Bulldogs appointed Rodney Eade as coach in 2005. Improvement was immediate, with the Bulldogs winning 11 games and finishing ninth on the ladder in 2005, missing out on the finals by just half a game. Missing the finals dealt a blow to both players and supporters of the team, as late season success led to the team being considered real premiership contenders.
In 2006, the Bulldogs continued to play well despite a disastrous run of injuries throughout the year; with five players having to have knee reconstructions, including captain Luke Darcy. Despite this setback, the Bulldogs finished the home-and-away season with 13 wins (see 2006 AFL season), making it to the finals for the first time since 2000, with Scott West and Brad Johnson continuing their excellent play. They won the Elimination Final against Collingwood in front of 84,000 at the Melbourne Cricket Ground (MCG) and reached the semi-finals before being defeated by eventual Premiers the West Coast Eagles at Subiaco Oval.
On 5 August 2006, Chris Grant broke the Western Bulldogs record for the most senior AFL/VFL games at the Club. On this day he played his 330th game, breaking Doug Hawkins' previous record of 329 games.
Looking for new markets, the Club had played one game every year at the Sydney Cricket Ground and one "home" game each year at Marrara Oval in Darwin. On 16 August 2006, the league announced that the Bulldogs' Sydney "home" game would be played at Manuka Oval, Canberra in 2007, 2008 and 2009.
Prior to the 2007 season, the Bulldogs made a splash by trading for Brisbane midfielder Jason Akermanis. They were premiership favourites early on in 2007, but yet again injuries took their toll, and they faltered in the last seven rounds, losing six games and drawing one, to finish 13th.
In the 2008 pre-season they traded away Jordan McMahon to Richmond and Sam Power to North Melbourne. They also recruited ruckman Ben Hudson and forward Scott Welsh from Adelaide and back Tim Callan from Geelong in what was a very successful trade week. In 2008, the Bulldogs were widely predicted for the bottom four after the pre-season, but had a successful home-and-away season, finishing in third place with fifteen wins, one draw and six losses (five of which occurred in the season's last seven games). The team's finals campaign began with a loss to Hawthorn by 51 points at the MCG in the first qualifying final, but won the subsequent semi-final against Sydney by 37 points. The Bulldogs lost their Preliminary Final match against reigning premiers Geelong.
Much was expected of the Bulldogs following their 3rd-place finish in 2008. They began the 2009 season with a 63-point thrashing of Fremantle in Perth, and then recorded solid wins over North Melbourne and Richmond before losing their next three games to West Coast (in Perth), Carlton and St Kilda. The Bulldogs then notched up their first away win against Adelaide since 2001, kicking eight goals to one in the third quarter to win by 32 points. The following week, they survived a determined effort from Melbourne, winning by 14 points, before succumbing to Geelong in one of the best and closest games of the season. They proceeded to win their next five games, including a 93-point drubbing of Port Adelaide in Darwin and an 88-point win over the reigning premiers Hawthorn. After a bit of a dip in form including losses to Collingwood, St Kilda and West Coast, the Bulldogs rebounded with an 18-point win against Brisbane at The Gabba. That was followed up by a 14-point win over Geelong. In the final round of the home-and-away season, the Bulldogs needed to defeat Collingwood by more than 22 points in order to reclaim third place on the ladder. The Bulldogs managed win by 24 points, earning the right to play Geelong in the first week of the finals.
There was media expectation that the Western Bulldogs would feature in the top four in 2010 after doing so in 2008 and 2009. The pre-season delivered the Western Bulldogs their first competition victory since 1970. The Bulldogs defeated St Kilda by 40 points in the NAB Cup Grand Final, with new recruit Barry Hall starring with seven goals and winning the Michael Tuck Medal for being the best player. However, after a promising pre-season, the Bulldogs failed to make their first grand final in 49 years after being demolished by Collingwood in the first round of the finals, coming back against the Sydney Swans and losing to St Kilda in a preliminary final, captain Brad Johnson's last game.
2011 was a disappointing year for the Bulldogs. After a Round 1 thrashing at the hands of Essendon by 55 points, the season never looked on track. After Round 21, which was a 49-point loss to Essendon, coach Rodney Eade was sacked by the Western Bulldogs after seven years at the helm. The club finished the year with wins against Port Adelaide and Fremantle and a loss against Hawthorn. The Bulldogs finished 2011 with a 9-win, 13-loss record for the season. Shortly after the 2011 season was completed, long-time Geelong and Essendon assistant Brendan McCartney was appointed as the senior coach on a three-year contract. During the following months, the Bulldogs assembled a coaching panel consisting of senior coach McCartney, former Geelong and St Kilda ruckman Steven King, former Sydney Swans and North Melbourne midfielder Shannon Grant, former Bulldogs champion and 300 game player Rohan Smith, and former Bulldogs and Port Adelaide player Brett Montgomery.
In October 2012, long-time President David Smorgon stepped down from the role to be replaced by former President Peter Gordon. Smorgon served as President from 1996 to 2012, overseeing two rebuilding phases, the erasure of much debt, and a period of stability after decades of uncertainty surrounding the club's future. In 2014, the Bulldogs returned to state league football when they fielded their reserves team in the Victorian Football League under the name of Footscray, ending their alignment with the Williamstown Football Club. The decision proved an instant hit on and off the field, with supporters of the AFL club taking a strong liking to the newly established VFL team. The success however flowed onto the field as well, with the club securing the VFL Premiership in its first season in the competition, defeating the Box Hill Hawks by 22 points in the VFL Grand Final.
Club Identity.
Whitten Oval.
The clubs home ground since foundation has been Whitten Oval located in the suburb of Footscray, west of the Melbourne CBD.
Club song.
Western Bulldogs Club Song is sung to "Sons of the Sea".
Before the club changed its name from Footscray to Western Bulldogs, the club song was called "Sons of the 'Scray", sung to the same tune but with different lyrics.
Real life mascot.
The real life mascot for the Western Bulldogs is a pedigree six-year-old pure white British Bulldog named 'Sid' (pedigree name "Murlane Bigshot").
Sid appears at all of the Western Bulldogs home games at the Docklands Stadium wearing the club colours.
He can be found walking around the perimeter of the ground prior to the game. He then waits for the players to come out on the ground; they give him a pat as they run past to the banner. 
During the game, Sid has a reserved area at the Footscray End (Gate 7), where fans can come and give him a pat and have their photo taken.
Membership base.
Since the 1990s the Western Bulldogs have struggled for membership and financial backing, avoiding folding or merging with another club through heavy subsidisation from the AFL as part of a competitive balance fund. However, in 2006 the Bulldogs broke their membership record and continued to sustain these membership figures before another breakthrough Club membership record in 2010.
Former Prime Minister Julia Gillard and actor Chris Hemsworth are supporters of the club.
Club honour roll.
Honours and achievements.
VFA/VFL.
Total senior premierships: 10
Senior premierships won by senior team (9)
Senior premierships won by reserves team (1)
VFL/AFL grand finals.
Premierships (1)
Runners-up (1)
Night Series.
Premierships (5)
Team of the Century.
In May 2002, the club announced a team of the greatest players from the last century.
Individual awards.
Best and Fairest.
The Charles Sutton Medal is awarded to the player adjudged Best and Fairest for the Western Bulldogs.
Scott West Most Courageous Player Award.
This is awarded to the player judged to be the most courageous for the season.
Reserves team.
The Footscray reserves team began competing in the VFL/AFL Reserves competition with the league's other reserves teams from when Footscray was first admitted to the VFL in 1925. The team won six premierships between 1925 and 1999. Following the demise of AFL reserves competition in 2000, the reserves team was dissolved and a reserves affiliation was established with the new Victorian Football League's two western clubs: Werribee and Williamstown. From 2001 until 2007, the club was aligned solely with Werribee; and then from 2008 until 2013, the club was aligned solely with Williamstown.
After a fifteen year recess, the club re-established a stand-alone reserves team to compete in the Victorian Football League from 2014 onward. The reserves team took on the club's traditional name "Footscray Bulldogs", even though the senior team continued to operate as the Western Bulldogs, and it played its home games at Whitten Oval. The club won the VFL premiership in its first season.

</doc>
<doc id="34095" url="http://en.wikipedia.org/wiki?curid=34095" title="Wilma Rudolph">
Wilma Rudolph

Wilma Glodean Rudolph (June 23, 1940 – November 12, 1994) was an Olympic champion. Rudolph was considered the fastest woman in the world in the 1960s and competed in two Olympic Games, in 1956 and in 1960.
In the 1960 Summer Olympics in Rome Rudolph became the first American woman to win three gold medals in track and field during a single Olympic Games. A track and field champion, she elevated women's track to a major presence in the United States. As a member of the black community, she is also regarded as a civil rights and women's rights pioneer. Along with other 1960 Olympic athletes such as Cassius Clay, who later became Muhammad Ali, Rudolph became an international star due to the first international television coverage of the Olympics that year.
The powerful sprinter emerged from the 1960 Rome Olympics as "The Tornado, the fastest woman on earth". The Italians nicknamed her "La Gazzella Nera" ("The Black Gazelle"); to the French she was "La Perle Noire" ("The Black Pearl").
Biography.
Wilma Glodean Rudolph was born prematurely at 4.5 lb, the 20th of twenty two siblings from two marriages Robert Elridge (1963-1980) & William Ward (1961-1963); her father Ed was a railway porter and her mother Blanche a maid. Rudolph contracted infantile paralysis (caused by the polio virus) at age four. She recovered, but wore a brace on her left leg and foot (which had become twisted as a result) until she was nine. She was required to wear an orthopedic shoe for support of her foot for another two years. Her family traveled regularly from Clarksville, Tennessee, to Meharry Hospital (now Nashville General Hospital at Meharry) in Nashville, Tennessee, for treatments for her twisted leg. In addition, by the time she was twelve years old she had also survived bouts of polio and scarlet fever.
In 1953, twelve-year-old Rudolph finally achieved her dream of shedding her handicap and becoming like other children. Her older sister was on a basketball team, and Wilma wanted to follow her sister's footsteps. While in high school, Rudolph was on the basketball team when she was spotted by Tennessee State track and field coach Ed Temple. Being discovered by Temple was a major break for a young athlete. The day he saw the tenth grader for the first time, he knew he had found a natural athlete. Rudolph had already gained some track experience on Burt High School's track team two years before, mostly as a way to keep busy between basketball seasons.
While attending Burt High School, Rudolph became a basketball star setting state records for scoring and leading her team to the state championship. She also joined Temple's summer program at Tennessee State and trained regularly and raced with his Tigerbelles for two years. By the time she was sixteen, she earned a berth on the U.S. Olympic track and field team and came home from the 1956 Melbourne Games with an Olympic bronze medal in the 4×100 m relay to show her high school classmates.
In 1959, Rudolph won a gold medal in the 4×100 m relay at Pan American Games (with Isabelle Daniels, Barbara Jones, and Lucinda Williams) and an individual silver in the 100 m. The same year she won the AAU 100 m title and defended it for four consecutive years. During her career, she also won three AAU indoor titles.
At the 1960 Summer Olympics in Rome she won three Olympic titles: in the 100 m, 200 m and 4×100 m relay. As the temperature climbed toward 110 F, 80,000 spectators jammed the "Stadio Olimpico". Rudolph ran the 100-meter dash in an impressive 11 seconds flat. However the time was not credited as a world record, because it was wind-aided. She also won the 200-meter dash in 23.2 seconds, a new Olympic record. After these wins, she was being hailed throughout the world as "the fastest woman in history". Finally, on September 11, 1960, she combined with Tennessee State teammates Martha Hudson, Lucinda Williams and Barbara Jones to win the 400-meter relay in 44.5 seconds, setting a world record. Rudolph had a special, personal reason to hope for victory—to pay tribute to Jesse Owens, the celebrated American athlete who had been her inspiration, also the star of the 1936 Summer Olympics, held in Berlin, Germany.
Following the post-games European tour by the American team Rudolph returned home to Clarksville. At her wishes, her homecoming parade and banquet were the first fully integrated municipal events in the city's history.
Rudolph retired from track competition in 1962 at age 22 after winning two races at a U.S.–Soviet meet.
She got a job teaching Grade two in her childhood school. Conflict forced her to leave the position. She moved to Indianapolis to head a community center. Then she moved to St. Louis Missouri, then Detroit, Michigan, and then returned to Tennessee for a time in the late 60s before moving again to California. She then lived in Chicago during the Mayor Richard J. Daley years.
Awards and honors.
Rudolph was United Press Athlete of the Year 1960 and Associated Press Woman Athlete of the Year for 1960 and 1961. Also in 1961, the year of her father's death, Rudolph won the James E. Sullivan Award, an award for the top amateur athlete in the United States, and visited President John F. Kennedy.
She was voted into the National Black Sports and Entertainment Hall of Fame in 1973 and the National Track and Field Hall of Fame in 1974.
She was inducted into the U.S. Olympic Hall of Fame in 1983, honored with the National Sports Award in 1993, and inducted into the National Women's Hall of Fame in 1994.
In 1994, the portion of U.S. Route 79 in Clarksville, Tennessee between the Interstate 24 exit 4 in Clarksville to the Red River (Lynnwood-Tarpley) bridge near the Kraft Street intersection was renamed to honor Wilma Rudolph.
Career and family.
In 1963, Rudolph was granted a full scholarship to Tennessee State University where she received her bachelor's degree in elementary education. After her athletic career, Rudolph worked as a teacher at Cobb Elementary School, coaching track at Burt High School, and became a sports commentator on national television.
Rudolph was married twice. On October 14, 1961, she married Willie Ward, a track star at North Carolina College at Durham, only to divorce him 17 months later. In summer 1963 she married her high school sweetheart Robert Eldridge, with whom she already had a daughter born in 1958. They had four children: Yolanda (b. 1958), Djuanna (b. 1964), Robert Jr. (b. 1965) and Xurry (b. 1971). She divorced Eldridge after 17 years of marriage, and returned to Indianapolis where she raised her children and hosted a local TV show.
Death.
In July 1994, shortly after her mother’s death, Rudolph was diagnosed with a brain tumor. On November 12, 1994, at age 54, she died of cancer in her home in Nashville. Wilma also had throat cancer. She was interred at Edgefield Missionary Baptist Church in Clarksville, Tennessee. At the time of her death, she had four children, eight grandchildren, and many nieces and nephews. Thousands of mourners filled Tennessee State University's Kean Hall on November 17, 1994, for the memorial service in her honor. Others attended the funeral at Clarksville's First Baptist Church. Across Tennessee, the state flag flew at half-mast.
Nine months after Rudolph's death, Tennessee State University, on August 11, 1995, dedicated its new six-story dormitory the "Wilma G. Rudolph Residence Center". A black marble marker was placed on her grave in Clarksville's Foster Memorial Garden Cemetery by the Wilma Rudolph Memorial Commission on November 21, 1995. In 1997, Governor Don Sundquist proclaimed that June 23 be known as "Wilma Rudolph Day" in Tennessee.
Legacy.
In 1994, Wilma Rudolph Boulevard was the name given to the portion of U.S. Route 79 in Clarksville, Tennessee.
The Woman's Sports Foundation Wilma Rudolph Courage Award is presented to a female athlete who exhibits extraordinary courage in her athletic performance, demonstrates the ability to overcome adversity, makes significant contributions to sports and serves as an inspiration and role model to those who face challenges, overcomes them and strives for success at all levels. This award was first given in 1996 to Jackie Joyner-Kersee.
A life-size bronze statue of Rudolph stands at the southern end of the Cumberland River Walk at the base of the Pedestrian Overpass, College Street and Riverside Drive, in Clarksville.
In 2000 "Sports Illustrated" magazine ranked Rudolph as number one on its listing of the top fifty greatest sports figures in twentieth-century Tennessee. A year before, she was ranked as 41st greatest athletes of the 20th century by ESPN.
Following the withdrawal of U.S. troops from Berlin in 1994, Berlin American High School (BAHS) was turned over to the people of Berlin and became the "Gesamtschule Am Hegewinkel". The school was renamed the "Wilma Rudolph Oberschule" in her honor in the summer 2000.
On July 14, 2004, the United States Postal Service issued a 23-cent Distinguished Americans series postage stamp in recognition of her accomplishments.
In 1977 a made-for-TV docudrama titled "Wilma" (also known as "The Story of Wilma Rudolph") was produced by Bud Greenspan; it starred Shirley Jo Finney, Cicely Tyson, Jason Bernard and Denzel Washington in one of his first roles.

</doc>
<doc id="34096" url="http://en.wikipedia.org/wiki?curid=34096" title="Wire (band)">
Wire (band)

Wire are an English rock band, formed in London in October 1976 by Colin Newman (vocals, guitar), Graham Lewis (bass, vocals), Bruce Gilbert (guitar), and Robert Gotobed (drums). They were originally associated with the punk rock scene, appearing on "The Roxy London WC2" album—a key early document of the scene—and were later central to the development of post-punk.
Inspired by the burgeoning UK punk scene, Wire are often cited as one of the more important rock groups of the 1970s and 1980s. Critic Stewart Mason wrote, "Over their brilliant first three albums, Wire expanded the sonic boundaries of not just punk, but rock music in general."
Wire are a definitive art punk and post-punk ensemble, mostly due to their richly detailed and atmospheric sound, often obscure lyrical themes, and, to a lesser extent, their Situationist political stance. The group exhibited a steady development from an early raucous punk style (1977's "Pink Flag") to a more complex, structured sound involving increased use of guitar effects and synthesizers (1978's "Chairs Missing" and 1979's "154"). The band gained a reputation for experimenting with song arrangements throughout its career.
History.
Wire's debut album, "Pink Flag" (1977) – "perhaps the most original debut album to come out of the first wave of British punk", according to AllMusic – contains songs which are diverse in mood and style, but most use a minimalist punk approach combined with unorthodox structures. "Field Day For The Sundays", for example, is only 28 seconds long.
"Chairs Missing" followed in 1978, and found Wire stepping back from the stark minimalism of "Pink Flag", with longer, more atmospheric songs and synthesizer parts added by producer Mike Thorne. "Outdoor Miner" was a minor hit, peaking at number 51 in the UK singles chart. The experimentation was even more prominent on "154" (1979). In addition, many of the songs featured bassist Graham Lewis on lead vocals instead of Colin Newman.
In 1979, creative differences pulled the band in different directions, leading to the "Document and Eyewitness" LP (1981), a recording of a live performance that featured, almost exclusively, new material, which was described as "disjointed", "unrecognizable as rock music" and "almost unlistenable". The LP came packaged with an EP of a different performance of more new material. Some of these songs, along with others performed but not included on the album, were included on Colin Newman's post-Wire solo albums ("5/10", "We Meet Under Tables"), while others were released by Gilbert and Lewis' primary post-Wire outlet Dome ("And Then...", "Ritual View").
Between 1981–85 Wire ceased recording and performing in favour of solo and non-Wire collaborative projects such as Dome, Cupol, Duet Emmo, and several Colin Newman solo efforts. In 1985, the group reformed as a "beat combo" (a joking reference to early 1960s beat music), with greater use of electronic musical instruments. Wire announced that they would perform none of their older material, hiring The Ex-Lion Tamers, (a Wire cover band named after a song title from "Pink Flag"), as their opening act. The Ex-Lion Tamers played Wire's older material; Wire played their new material. They released "IBTABA" in 1989, a "live" album of mostly re-worked versions of songs from "The Ideal Copy" and "A Bell Is a Cup", heavily re-arranged, edited, and remixed. A new song from the album, "Eardrum Buzz", was released as a single and peaked at number 68 in the UK singles chart.
Gotobed left the band in 1990, after the release of the album "Manscape". After his departure, the band dropped one letter from its name, becoming "Wir" (still pronounced "wire"), and released "The First Letter" in 1991. There followed a further period of solo recordings, during which Newman founded the swim ~ label, and later Githead with his wife (ex-Minimal Compact bassist Malka Spigel), while Wire remained an occasional collaboration. It was not until 1999 that Wire again became a full-time entity.
With Gotobed back in the line-up (now using his birth name, Robert Grey), the group initially reworked much of their back catalogue for a performance at Royal Festival Hall on 26 February 2000. Wire's reception during a short tour in early May of the U.S., and a number of UK gigs, convinced the band to continue. Two EPs and an album, "Send" (2003), followed, as well as collaborations with stage designer Es Devlin and artists Jake and Dinos Chapman.
In 2006, Wire's 1970s albums were remastered and re-released with original vinyl tracklistings. Rumours abounded of a renewal of activity to mark the 30th anniversary of the band's debut as a four-piece and the re-release of "Pink Flag". A third "Read & Burn" EP was released in November 2007.
A full-length album of new material entitled "Object 47" was released in July 2008. Bruce Gilbert was not involved in this recording, although according to Colin Newman, he did feature in a minimal capacity on the third "Read and Burn" EP.
On 10 January 2011 the band released their twelfth studio album, "Red Barked Tree", which (according to press release and BBC) "...rekindles a lyricism sometimes absent from Wire's previous work and reconnects with the live energy of performance, harnessed and channelled from extensive touring over the past few years". The album was written and recorded by Colin Newman, Graham Lewis, and Robert Grey (no guests involved), but speaking to Marc Riley on the day of the release, Newman introduced as "a new boy" guitarist Matt Simms (from It Hugs Back) who's been with the band since April 2010 as a touring member.
In March 2013 the band released "Change Becomes Us", their 13th studio album, which was very well received. Their fourteenth album, eponymously titled "Wire", was released on 13 April 2015 in the UK.
Influence.
Wire's influence has outshone their comparatively modest record sales. In the 1980s and 1990s, The Urinals, Manic Street Preachers, The Minutemen, Sonic Youth, and R.E.M., expressed a fondness for the group. R.E.M. covered "Strange" on their album "Document". Minor Threat covered "12XU". Since their 2008 reunion, The Feelies have regularly covered "Outdoor Miner" during its live sets and Lush also covered the track. Robert Smith has described how, after seeing the group live, influenced The Cure's sound after their first album.
Guided By Voices' Robert Pollard claimed that Wire was his favourite band, and that the fact that GBV's albums had so many songs was directly influenced by Wire's albums. One of My Bloody Valentine's last releases prior to reconvening in 2007 was a cover of "Map Ref 41°N 93°W" for a Wire tribute entitled "Whore". The song was selected as a favourite cover by "Flak Magazine".
More recently, Fischerspooner (who covered "The 15th" on their album "#1"), britpop bands like Elastica and Menswe@r and post-punk revival bands like Bloc Party, Futureheads, Blacklist, and Franz Ferdinand have cited Wire as an influence. Blur's work, along with many more minor Britpop bands, have been cited as particularly reminiscent of 1970s Wire at various points, with Graham Coxon and Damon Albarn both speaking of the band's influence on Blur. The Smiths' Johnny Marr has confirmed that he is a fan of the band and has acknowledged that seeing Wire live helped give him the confidence to release his first solo album in 2013.
The British electronic band Ladytron included Wire's "The 15th" on the mix compilation "Softcore Jukebox". The Ladytron's band member Reuben Wu claimed Wire as a musical influence.
Wire were influential on American hardcore punk. Fans included Ian MacKaye of the hardcore punk band Minor Threat and Henry Rollins, formerly of Black Flag. Minor Threat covered "1 2 X U" for the Dischord Records compilation "Flex Your Head", as well as Boss Hog on their "I Dig You" EP. Henry Rollins, as Henrietta Collins & The Wife-Beating Childhaters, covered "Ex Lion Tamer" on the EP "Drive by Shooting". Michael Azerrad reported, in the book "Our Band Could Be Your Life", that at Minor Threat's second gig, each of the seven bands on the roster performed its version of a Wire song. Big Black covered Wire's "Heartbeat" twice, once as a studio version which was released as a single (also included on "The Rich Man's Eight Track Tape" compilation), and also as a live version, featuring Bruce Gilbert and Graham Lewis, that was included on the VHS version of its live album "Pigpile". The slowcore band Low included an early, previously unreleased cover of "Heartbeat" on their career-spanning boxset in 2007. Ampere recorded a cover of "Mr. Suit" for its 2006 split with Das Oath. New Bomb Turks also recorded a cover of "Mr. Suit" on its 1993 album "!!Destroy-Oh-Boy!!". The chorus of Ministry's "Thieves" was influenced by "Mr. Suit" as well. Helmet guitarist Page Hamilton cites Wire as one of his "top five bands" and an influence on his music.
A plagiarism case between Wire's music publisher and Elastica, over the similarity between Wire's 1977 song "Three Girl Rhumba" and Elastica's 1995 hit "Connection", resulted in an out-of-court settlement.
Olivier Assayas's 2010 film "Carlos" uses four of the band's songs on the soundtrack to increase tension: "Drill", "Dot Dash", "The 15th", and "Ahead".

</doc>
<doc id="34104" url="http://en.wikipedia.org/wiki?curid=34104" title="Williams Grand Prix Engineering">
Williams Grand Prix Engineering

Williams Grand Prix Engineering Limited (FWB: ), trading in Formula 1 as Williams Martini Racing, is a British Formula One motor racing team and constructor. It is founded and run by team owner Sir Frank Williams and automotive engineer Patrick Head. The team was formed in 1977 after Frank Williams' two earlier unsuccessful F1 operations: Frank Williams Racing Cars (1969 to 1975) and Walter Wolf Racing (1976). All of Williams F1 chassis are called "FW" then a number, the FW being the initials of team owner, Frank Williams.
Williams's first race was the 1977 Spanish Grand Prix, where the new team ran a March chassis for Patrick Nève. Williams started manufacturing its own cars the following year, and Switzerland's Clay Regazzoni won Williams's first race at the 1979 British Grand Prix. At the 1997 British Grand Prix, Canadian Jacques Villeneuve scored the team's 100th race victory, making Williams one of only three teams in Formula One, alongside Ferrari and fellow British team McLaren, to win 100 races. Williams won nine Constructors' Championships between 1980 and 1997. This stood as a record until Ferrari surpassed it in 2000.
Many famous racing drivers have driven for Williams, including Australia's Alan Jones; Finland's Keke Rosberg; Britain's Nigel Mansell, Damon Hill and Jenson Button; France's Alain Prost; Brazil's Nelson Piquet and Ayrton Senna, and Canada's Jacques Villeneuve, each of whom, with the exception of Senna and Button, have captured one drivers' title with the team. Interestingly, of those who have won the championship with Williams, only Jones, Rosberg and Villeneuve actually defended their title while still with the team. Piquet moved to Lotus after winning the 1987 championship, Mansell moved to the American based Indy Cars after winning the 1992 championship, Prost retired from racing after his 4th World Championship in 1993, while Hill moved to Arrows after winning in 1996.
Williams have worked with many notable engine manufacturers, most successfully with Renault: Williams won five of their nine constructors' titles with the French company. Along with Ferrari, McLaren, Benetton and Renault, Williams is one of a group of five teams that won every Constructors' Championship between 1979 and 2008 and every Drivers' Championship from 1984 to 2008.
Williams F1 also has business interests beyond Formula One racing. It has established Williams Advanced Engineering and Williams Hybrid Power which take technology originally developed for Formula One and adapt it for commercial applications. In April 2014, Williams Hybrid Power were sold to GKN. Williams Advanced Engineering had a technology centre in Qatar until it was closed in 2014.
Origins.
Frank Williams started the current Williams team in 1977 after his previous outfit, Frank Williams Racing Cars, failed to achieve the success he desired. Despite the promise of a new owner in the form of Canadian millionaire Walter Wolf, the team's 1976 cars were not competitive. Eventually Williams left the rechristened Walter Wolf Racing and moved to Didcot to rebuild his team as ""Williams Grand Prix Engineering". Frank recruited young engineer Patrick Head to work for the team, creating the "Williams-Head"" partnership.
Ownership.
It was reported on 20 November 2009 by Reuters that Williams and Patrick Head had sold a minority interest in their team to an investment company led by Austrian Toto Wolff who has stated that it is purely a commercial decision.
In February 2011, Williams F1 announced their intention to float via an Initial Public Offering on the Frankfurt Stock Exchange, from March 2011. Swiss-based Bank am Bellevue AG will act as sole global co-ordinator of up to 27.39% of existing shares, a total of 2,739,383 shares. The shares will be offered to the public in the UK, Germany, Switzerland and Austria; while private institutional investment will be available to organisations in other EU countries, plus the United States, Canada, Japan and Australia. Sir Frank Williams will remain majority shareholder and team principal after the IPO. The shares are valued at between 24 and 29 euros, which values the Williams F1 team at 265 million euros.
Racing history – Formula One.
Ford engines (1977–1983).
1977
Williams entered a customer March 761 for the 1977 season. Lone driver Patrick Nève appeared at 11 races that year, starting with the Spanish Grand Prix. The new team failed to score a point, achieving a best finish of 7th at the Italian Grand Prix.
1978
For the 1978 season, Patrick Head designed his first Williams car: the FW06. Williams signed Australian Alan Jones, who had won the Austrian Grand Prix the previous season for a devastated Shadow team following the death of their lead driver, Tom Pryce. Jones's first race for the team was the Argentine Grand Prix where he qualified the lone Williams in 14th position, but retired after 36 laps with a fuel system failure. The team scored its first championship points two rounds later at the South African Grand Prix when Jones finished fourth. Williams managed their first podium position at the US Grand Prix, where the Australian came second, some 20 seconds behind the Ferrari of future Williams driver Carlos Reutemann. Williams ended the season in tenth place in the Constructors' Championship, with a respectable 16 points, while Alan Jones finished 12th in the Drivers' Championship.
1979
Head designed the FW07 for the 1979 season. This was the team's first ground effect car, a technology first introduced by Colin Chapman and Team Lotus. Williams also obtained membership of the Formula One Constructors Association (FOCA) which expressed a preference for teams to run two cars, so Jones was partnered by Swiss driver Clay Regazzoni. They had to wait until the seventh round of the championship, the Monaco Grand Prix, for a points-scoring position. Regazzoni came close to taking the team's first win but finished second, less than a second behind race winner Jody Scheckter. The next round at Dijon is remembered for the final lap battle between René Arnoux and Gilles Villeneuve, but also saw both cars finish in the points for the first time; Jones was fourth with Regazzoni sixth. The team's first win came at the 1979 British Grand Prix – their home Grand Prix – when Regazzoni finished almost 25 seconds ahead of anyone else.
Things got even better when Williams cars finished first and second at the next round in Hockenheim, Alan Jones two seconds ahead of Regazzoni. Jones then made it three wins in a row at the Österreichring, finishing half a minute ahead of Gilles Villeneuve's Ferrari. Three wins in a row became four wins two weeks later at Zandvoort, Alan Jones winning again by a comfortable margin over Jody Scheckter's Ferrari. Scheckter ended the Williams winning streak when he won Ferrari's home Italian Grand Prix, Regazzoni finishing third behind both Ferraris. Alan Jones managed another win at the penultimate race at Montreal to cap off a great season.
Williams had greatly improved their Constructors' Championship position, finishing eight places higher than the previous year and scoring 59 more points. Alan Jones was the closest driver to the Ferrari duo of Villeneuve and 1979 champion Jody Scheckter, the Australian scored 43 points, 17 behind the South African, while Jones's team mate, Regazzoni, was two places behind him with 32 points.
1980
In 1980, Alan Jones partnered the Argentine Carlos Reutemann. The team started well in the championship, with Jones winning the first round of the season in Argentina. Jones won four more races: Paul Ricard, Brands Hatch, Montreal and the final round at Watkins Glen. Jones became the first of seven Williams drivers to win the Drivers' Championship, 17 points ahead of Nelson Piquet's Brabham. Williams also won its first Constructors' Championship, scoring 120 points, almost twice as many as second-placed Ligier.
1981
The duo won four races for the Williams team in the 1981 season. Alan Jones won at the first round at Long Beach and the final round at Las Vegas, while Carlos Reutemann won at the second round at Jacarepagua and the fifth round at Zolder. Williams won the constructors' title for the second year running, scoring 95 points, 34 points more than second-placed Brabham.
1982
Alan Jones retired from Formula One, only to come back a year later for a single race with the Arrows team. The Australian was replaced by Finnish driver, Keke Rosberg, who had not scored a single championship point the previous year. He won the drivers' title that year; winning only one race, the Swiss Grand Prix at Dijon-Prenois. Rosberg's teammate, Reutemann, finished in 15th place having quit Formula One after just two races of the new season. His seat was filled by Mario Andretti for the US Grand Prix West before Derek Daly took over for the rest of the year. The Williams team finished fourth in the Constructors' Championship that year, 16 points behind champions Ferrari.
By the end of the season, Frank Williams realised that to compete at the top levels of Formula One he needed the support of a major manufacturer, such as Renault or BMW who could supply his team with a turbo engine.
Honda engines (1983–1987).
1983
Frank Williams looked towards Honda, which was developing its own turbo-charged V6 engine with the Spirit team. A deal between Honda and Williams was finally settled early in 1983 and the team used the engines for the 1984 season. For the rest of the 1983 season, Williams used the Ford engine except for the last race of the year in South Africa where Rosberg scored an encouraging fifth place. The team finished fourth in the Constructors' Championship, scoring 36 points, including a win for Keke Rosberg at the 1983 Monaco Grand Prix.
1984
For the 1984 season, Head designed the ungainly FW09. Keke Rosberg won the Dallas Grand Prix and managed to get second at the opening race in Brazil. Rosberg's team mate, Jacques Laffite, came 14th in the Drivers' Championship with five points. The team finished sixth with 25.5 points, with Rosberg eighth in the Drivers' Championship.
1985
In 1985, Head designed the FW10, the team's first chassis to employ the carbon-fibre composite technology pioneered by the McLaren team. British driver Nigel Mansell joined the team to partner Rosberg. The team scored four wins with Rosberg winning in Detroit and Adelaide, and Mansell taking the European Grand Prix and the South African Grand Prix. Williams finished third in the Constructors' Championship, scoring 71 points.
During qualifying for the British Grand Prix at Silverstone, Rosberg lapped the ultra fast 4.719 km circuit in 1:05.591 for an average speed of 160.938 mph, the fastest recorded lap in Formula One history to that point.
From 1985 until the end of the 1993 season, Williams cars ran with the yellow-blue-and-white Canon livery.
1986
In March 1986, Frank Williams faced the most serious challenge of his life. While returning to the airport at Nice (France) after pre-season testing at Paul Ricard, he was involved in a road accident which left him paralysed. He did not return to the pit lane for almost a year. Despite the lack of his trackside presence in the team, the Williams team won nine Grands Prix and the Constructors' Championship and came close to winning the Drivers' Championship with Nigel Mansell, but the British driver's left-rear tyre blew at the Australian GP, the final round of the season, while his fellow championship rival and teammate, Nelson Piquet made a pitstop shortly after Mansell's retirement as a precaution. This left Alain Prost to defend his title successfully, despite being in an inferior car.
1987
The 1987 season brought Williams-Honda partnership its first and only Drivers' Championship title in the form of Nelson Piquet, who scored 76 points – 73 after dropped scores (best eleven results counted) in relation to the Drivers' Championship – and won three races, while the Brazilian's teammate Mansell, was 15 points behind him – 12 on dropped scores – in second place with six victories during the season. The Williams team won the Constructors' Championship for the second year running, scoring 137 points, 61 points ahead of their nearest rivals McLaren. Despite this success, Honda ended their partnership with Williams at the end of the year in favour of McLaren, and continuing with Lotus.
Judd engines (1988).
1988
Unable to make a deal with another major engine manufacturer, Williams used naturally aspirated Judd engines for the 1988 season. This left them with a significant performance deficit compared with their turbo-powered rivals. Piquet left Williams to join Lotus who had retained their Honda engines for the 1988 season, helped by having Satoru Nakajima as number 2 driver to Piquet. Williams brought in Italian Riccardo Patrese to replace Piquet. The team did not win a race that season and finished seventh in the Constructors' Championship, scoring 20 points. The highlights of the season were two second places by Mansell, at the British and Spanish Grand Prix. When Mansell was forced to miss two races through illness, he was replaced by Martin Brundle and then Jean-Louis Schlesser.
Renault engines (1989–1997).
The team secured an engine supply from Renault in 1989. Renault engines subsequently powered Williams drivers to another four Drivers' and five Constructors' Championships up until Renault's departure from Formula One at the end of 1997. The combination of Renault's powerful engine and Adrian Newey's design expertise led to a particularly dominant period in the mid 1990s. Mansell had a record breaking 1992 season winning the title in record time and leading many races from pole to finish.
Some maintain that the Williams FW14B and FW15C were "the most technologically advanced cars that will ever race in Formula One".
1989
The Renault era started in 1989, with Italian Riccardo Patrese and Belgian Thierry Boutsen at the helm of the two Williams cars. Boutsen replaced Mansell, who had signed on to be Gerhard Berger's teammate at Ferrari. The engine's first grand prix in Brazil was one that the team preferred to forget, with Boutsen retiring with an engine failure and Patrese with an alternator failure after leading, although Patrese did qualify second. The Williams Renault team managed to get back on track with Boutsen coming fourth in the next race at Imola, earning the team three points in their championship campaign. Two races later at the Mexican Grand Prix, the team managed to achieve their first podium with the Renault engine, thanks to Patrese, who came second, 15 seconds behind the race winner Ayrton Senna. The next race saw Patrese come second again, having started from 14th on the grid, with Boutsen 6th. At the sixth round in Canada, Williams not only scored their first win with the Renault engine but also their first one-two: Thierry Boutsen came first followed by Patrese, resulting in 15 points for Williams' championship campaign. Williams came second in the Constructors' Championship, scoring 77 points in total; 64 points behind winners McLaren. Patrese finished 3rd in the Drivers' Championship with 40 points, 41 points behind the 1989 world champion, Alain Prost. Boutsen finished 5th in the championship with 37 points after also winning in Australia. Boutsen's win in Australia gave Williams the distinction of having won the first and last Grands Prix of the 1980s.
1990
In the 1990 season, Williams kept Patrese and Boutsen as the team's drivers. Although Patrese won the San Marino Grand Prix and Boutsen won pole position and the race at the Hungarian Grand Prix, the team scored 30 fewer points than the previous year and finished the Constructors' Championship two positions lower, in fourth. In the Drivers' Championship, Boutsen finished sixth with 34 points and Patrese seventh with 23 points.
1991
Boutsen left Williams and joined Ligier at the start of 1991. His replacement was a returning Nigel Mansell, who had spent the previous two seasons driving for Scuderia Ferrari. Williams also recruited future 1996 world champion, Damon Hill, as one of their new test drivers. Williams failed to finish in the first Grand Prix of the season at Phoenix, both drivers retiring with gearbox problems. Patrese got back on track for the team in the next Grand Prix at Interlagos, coming second behind McLaren's Ayrton Senna. The 1991 San Marino Grand Prix saw both cars retiring again: Mansell after a collision and Patrese with an electrical failure after 17 laps. The Grand Prix at Monaco saw Mansell finally finish in a points-scoring position, coming second, 18 seconds behind race winner Ayrton Senna. At the next race, the Canadian Grand Prix, Williams locked out the front row only for Patrese to drop back with gearbox problems and Mansell to retire from the lead on the final lap with an electrical fault. At the following race, in Mexico, Williams finally broke their 1991 duck with a 1–2, Patrese finishing ahead of Mansell to score 16 points for the Williams team. Williams then had two consecutive further victories, with Mansell winning the French Grand Prix, five seconds ahead of Alain Prost's Ferrari. Mansell then won again at the British Grand Prix; it had been four years since a Briton had won the grand prix, Mansell having won it in 1987. Three consecutive victories became four when Mansell won again in Germany, Patrese was about 10 seconds behind him in second place. Senna ended Williams's run of victories by winning in Hungary, finishing five seconds ahead of Nigel Mansell. Mansell later won the Italian Grand Prix and the Spanish Grand Prix, while Patrese won the Portuguese Grand Prix after Mansell's race was ruined by a botched pitstop in which only three wheel nuts were fitted. Williams finished second in the Constructors' Championship, scoring 125 points in total, 14 points behind McLaren. Mansell finished second in the Drivers' Championship, scoring 72 points, 24 points behind Senna.
1992
Williams took a step up for the 1992 season, keeping their 1991 driver line-up of Patrese and Mansell. Mansell dominated the first round in South Africa, qualifying in pole position and winning the race by 24 seconds from his team-mate Patrese. Nigel Mansell won the next four rounds for Williams, at Mexico City, Interlagos, Catalunya and Imola, Patrese coming second in all but one (the Spanish Grand Prix, where he retired after spinning off). Mansell's five victories in the opening five races was a new record in Formula One. Senna won the next race in Monaco, ahead of both Williams cars, which finished second and third. In the next race, in Canada, both Williams cars retired: Mansell spun off on entering the final corner (he claimed that Senna pushed him off) and Patrese had a gearbox failure. Mansell went on to record four more Grand Prix wins, including at the British Grand Prix. (In the final round, in Adelaide, the two Williams again retired, Mansell after Senna violently crashed into the back of him, and Patrese with electrical problems.) Williams won the Constructors' Championship with 164 points, 65 points more than second place McLaren. Mansell became World Champion, scoring 108 points, with Patrese finishing second with 56 points. In winning nine races in a single season Mansell had set a new record for the most wins by a single driver in one season.
Despite capturing the title and nine races, Nigel Mansell's seat was not safe for 1993 as Williams looked to sign either Alain Prost, who had taken 1992 off after he was fired by Ferrari, or Ayrton Senna, whose contract with McLaren was expiring. Patrese's position also looked to be under threat and he signed for Benetton before the end of the year. Ironically, Patrese could have remained at Williams; Prost was able to agree terms with Williams for 1993 and Mansell departed for CART rather than be teammates with the Frenchman as they did not have a good relationship from their time together at Ferrari. Prost, in turn, did not wish to drive with his former teammate at McLaren due to their rivalry and thus negotiated a veto clause into his contract to freeze Senna out and keep him at McLaren. Test driver Damon Hill was promoted to join Prost in place of the departed Patrese.
1993
The Williams FW15C was the dominant car, with active suspension and traction control systems beyond anything available to the other teams. Prost won on his debut for the team in South Africa and, like Mansell, dominated the weekend, taking pole position and finishing a minute ahead of Senna, who was second. The next Grand Prix in Brazil saw Prost collide with Christian Fittipaldi's Minardi in the rain on lap 29, while Hill went on to his first podium finish: second, 16 seconds behind Senna. Prost won three of the next four Grands Prix for Williams, Senna winning the other race. Prost and Hill later scored a 1–2 in France: the only 1–2 of the season for Williams. The Frenchman won the next two Grand Prix at Silverstone and Hockenheim. Prost's team mate Hill proved competitive especially in the second half of the season. Mechanical problems cost the Englishman leads in Britain and Germany, but he went on to win the next three Grand Prix at Hungary; Belgium and Italy which moved him to second in the standings, as well as giving him a chance of taking the drivers' title. After Italy, Williams would not win a Grand Prix for the rest of the season, as a young Michael Schumacher won the following race in Portugal, and Senna took Japan and Australia to overtake Hill in the points. Williams retained their constructor's title, 84 points ahead of second placed, McLaren. Prost clinched the Driver's Championship in Portugal and finished the season 26 points ahead of second-placed Ayrton Senna.
1993 marked the final season that Williams ran with Canon as its primary backer.
1994
During the 1994 season, Williams used FW16 (developed during the pre-season) and FW16B (with shorter sidepods and optimised for the revised floor regulations which were introduced during the season).
After Canon left the team Williams signed a contract with tobacco company Rothmans International for 1994, and their namesake brand became its primary sponsor from 1994 to 1997.
As the provision in Alain Prost's contract regarding his former teammate-turned-rival had expired following the 1993 season, Williams pursued and signed Ayrton Senna to a contract for 1994 while Prost elected to retire from competition. Given this was the same team that had won the previous two World Championships with vastly superior cars, Senna was a natural and presumptive pre-season title favourite, with second-year driver Damon Hill intended to play the supporting role. Between them, Prost, Senna, and Hill had won every race in 1993 but one, which was taken by Benetton's Michael Schumacher.
Pre-season testing showed the car had speed but was difficult to drive. The FIA had banned electronic driver's aids, such as active suspension, traction control and ABS, to make the sport more "human". It was these technological advancements that the Williams chassis of the previous years had been built around. With their removal in 1994, Williams had not been a good-handling car, as observed by other F1 drivers, having been seen to be very loose at the rear. Senna himself had made numerous comments that the Williams FW16 had some quirks which needed to be ironed out. It was obvious that the FW16, after the regulation changes banning active suspension and traction control, exhibited none of the superiority of the FW15C and Williams FW14B cars that had preceded it. The surprise of testing was Benetton-Ford which was less powerful but more nimble than the Williams.
The first four rounds were won by Michael Schumacher in the Benetton-Ford. Senna took pole in the first three races but failed to finish all three. In the third race, the 1994 San Marino Grand Prix in Imola, Senna was involved in a fatal crash at the first corner after completing six laps. The repercussions of Senna's fatal accident were severe for the team itself, as the Italian prosecutors tried to charge the team and Frank Williams with manslaughter, an episode which was not over until 2005. At the next race in Monaco, Damon Hill was the only Williams on the grid, as a mark of respect to Senna, and retired on the first lap. Since Senna's death, every Williams F1 car has carried a Senna 'S' somewhere on its livery in his honour and to symbolise the team's ongoing support of the Instituto Ayrton Senna.
The next race in Spain, Williams brought in test driver, David Coulthard, as Hill's new teammate. In the race itself, Hill took the team's first victory of the season, by almost half a minute over Schumacher's Benetton, while Coulthard would retire due to an electrical problem. In Canada, both Williams cars finished in the points for the first time that season, with Hill finishing second and Coulthard finishing fifth. In France, Nigel Mansell replaced Coulthard (in the first of four appearances), at the behest of Renault. At Silverstone, Damon Hill did something his father, Graham, never did, which was winning the British Grand Prix. Hill closed the gap with Schumacher in the championship, after the German was disqualified from first at Spa after the Stewards found floorboard irregularities on his Benetton. He was banned for the next two races, in which Hill capitalised on with wins in Italy and a Williams 1–2 in Portugal.
With three races left, 1992 champion Nigel Mansell returned from CART (where the season had concluded) to replace Coulthard for the remainder of the season. Mansell would get approximately £900,000 "per race", while Hill was paid £300,000 for the entire season, though Hill remained as lead driver.
Schumacher came back after his suspension for the European Grand Prix, which he won by about 25 seconds, to take a lead of 5 points into the penultimate round in Japan. If Hill did not finish ahead of Schumacher, it would be very unlikely that he would take the title in the final round in Adelaide. The race in Japan was held in torrential rain, with Hill managing to win the restarted race, by three seconds on aggregate over Schumacher who finished second. Going into the final round at Adelaide, Schumacher led Hill by one point. Mansell took pole for Williams, but had a poor start which let Hill and Schumacher through to fight it out for the lead and the 1994 title. Mid way through the race, Schumacher's perceived need for a low downforce setup cost him, as he lost control and clipped the outside wall at the 5th corner (out of sight of Hill). As Schumacher recovered, Hill came round the corner and attempted to overtake into the next corner. Schumacher turned in and the resulting contact (Schumacher in the wall and Hill retiring with bent suspension), meant Schumacher was the champion. This collision has been controversial. Some, such as Williams' Patrick Head, have suggested that this was a deliberate attempt by Schumacher to take Hill out of the race. However others, such as then BBC commentator Murray Walker, defended Schumacher, calling the accident a "racing incident".
Williams would end the season as Constructors' Champion for the third consecutive year, scoring 118 points, while Hill finished second in the Drivers' Championship with 91 points.
1995
In 1995, Nigel Mansell wasn't retained, Williams favouring Coulthard over him to partner Hill. At the first round in Brazil, Schumacher, whose Benetton team had switched engine suppliers from Ford to Renault in the offseason, started off with a win, with Coulthard taking second. However, both were disqualified from the race after it was found that their fuel supplier, Elf, supplied the teams with a type of fuel that was different from the ones they gave to the FIA as samples. Thus, Gerhard Berger and Ferrari were declared winners. Schumacher and Coulthard had their positions reinstated after appeal, though Benetton and Williams were not awarded their constructors' points. Hill won the next two races in Argentina and San Marino and would later win two more races, which were at The Hungaroring and in Adelaide, the latter where Hill won two laps ahead of the field in one of F1's most dominating victories. Coulthard would also record his only win for the Williams team, at Estoril, before moving to McLaren.
Williams' champion streak was ended by Benetton, who elected to switch engine suppliers from Ford to the same Renault power Williams had. As such, Benetton outscored Williams' by 29 points in the Constructors' Championship. Damon Hill placed second in the Drivers' Championship, 33 points behind Benetton's Michael Schumacher.
1996
For 1996, Williams clearly had the quickest and most reliable car, the FW18. Coulthard had left Williams to join Mika Häkkinen at McLaren, Williams replaced the Scotsman with Canadian Jacques Villeneuve, who had won the CART series title in 1995, while Hill remained with the team. Schumacher left Benetton to join Ferrari. Williams won the first five Grands Prix, Hill winning all but one of them. Olivier Panis would take victory at the sixth round in Monaco after both Williams cars retired. Hill would retire for the second time in a row after he spun off in Spain, while his team mate, Villeneuve, took third place. Hill and Villeneuve dominated the next Grand Prix in Canada, with a 1–2 in qualifying and a 1–2 in the race. Williams made it a second 1–2 after Hill won the French Grand Prix. Villeneuve won his second race in F1 at Silverstone after Hill retired with a wheel bearing failure on lap 26. The Brit would be victorious in the next Grand Prix in Germany while Villeneuve would win the race after that in Hungary. Schumacher's Ferrari would then take the next two Grand Prix at Spa-Francorchamps and Monza. Villeneuve mounted a title challenge going into the final race of the season at Japan, but Hill reasserted his dominance to take the race and the 1996 title, while Villeneuve lost a wheel and retired.
Williams' dominance was such that they had clinched the Constructors' Championship and only their drivers had a mathematical chance of taking the title, several races before the season concluded. Around that time, Frank Williams announced that Hill would not be re-signed after his contract expired, despite Hill's successes and eventual Drivers' Championship, so he joined Arrows for 1997. Also, Adrian Newey had ambitions as a technical director (rather than just chief designer), but this wasn't possible at Williams, as Patrick Head was a founder and shareholder of the team. McLaren lured Newey away, though he was forced to take garden leave for the majority of 1997.
1997
For what would be the final season of Williams-Renault and a car designed with Newey's input, Frank Williams brought in German Heinz-Harald Frentzen, as he had created a good impression on Williams during his first few seasons in Formula One. Frentzen proved to be a disappointment though, and won only one race in his two-year spell at Williams, the 1997 San Marino Grand Prix. Jacques Villeneuve won seven races during 1997, with his main rival, Michael Schumacher of a resurgent Ferrari, winning five. Williams also achieved the 100 race win milestone at the British Grand Prix. Coming to the final round of the season at Jerez, Schumacher lead the Canadian by 1 point, however on lap 48, Schumacher and Villeneuve collided. Schumacher was disqualified from second place in the championship as the accident was deemed by the FIA as "avoidable", Williams won the constructors' title for the second time in a row, scoring 123 points, while Jacques Villeneuve won the Drivers' Championship by three points to Michael Schumacher, who kept his points total despite being removed from second place, with Williams team-mate Frentzen a further thirty-six points behind.
Mecachrome engines (1998).
1998
After 1997, the team were unable to maintain their dominance in Formula 1 as Renault ended their full-time involvement in Formula 1, and Adrian Newey moved to rival team McLaren. Williams then had to pay for Mecachrome engines, which were old, rebadged Renault engines. Both these meant that the car not only featured a very similar aerodynamic package to their 1997 car, but also virtually the same engine, leading to some to comment that they ran what was virtually the same car, adjust for the 1998 regulations. There were changes on the sponsorship front however as Rothmans opted to promote their Winfield brand, which ended the popular blue and white livery, replacing it with a red one. For 1998, Williams kept the two drivers from the previous season, the first time since 1983 that a reigning world champion remained driving for Williams. While Ferrari and McLaren battled for the constructors' and drivers' titles, Williams fell to the middle of the field. The team won no races and took 3 podiums during the season, with Frentzen finishing in third at the first round in Australia and Villeneuve finishing third in Germany and Hungary. Williams finished third in the Constructors' Championship, scoring 38 points, while Villeneuve finished fifth in the Drivers' Championship with 21 points and his German team-mate, Frentzen, finished 4 points behind him in seventh.
Supertec engines (1999).
1999
In 1999, Williams employed the Supertec engine, which was a rebadged Mecachrome/Renault unit and new driver line up. Villeneuve moved to the new BAR-Supertec team and Frentzen moved to Jordan. German Ralf Schumacher joined Williams in what amounted to a driver trade as Frentzen would be taking over Schumacher's old ride at Jordan. Alex Zanardi, an Italian driver who had won the last two CART series championships, also signed with the team after Sir Frank Williams had spent the better part of the last year trying to get him to defect from the CART series. The team managed three podiums, all scored by Ralf Schumacher, with third place in Australia and Britain, along with a second place in Italy. The team finished fifth in the Constructors' Championship, the lowest finish for Williams in the 1990s; the team finished behind Stewart and Jordan, scoring a total of 35 points. Of those points, all were scored by Schumacher as Zanardi, who had not performed well in his previous stint in F1 either, failed to finish in the top six in any race. At the end of the season, Williams bought Zanardi's contract out.
BMW partnership (2000–2005).
2000
During 1998, the team signed a long term agreement with BMW, with the German manufacturer supplying engines and expertise for a period of 6 years. As part of the deal BMW expected at least one driver to be German, which led to the team's signing of Ralf Schumacher for the previous season. In 1999, the team had a Williams car with a BMW engine testing at circuits, in preparation for a debut in 2000. Williams sought the services of Colombian Juan Pablo Montoya, who had replaced Zanardi at Chip Ganassi Racing when he left to drive for Williams in 1999, to step in but Montoya was still under contract to Ganassi for one more season and unavailable. Britain's Jenson Button was called upon made his debut instead. With the switch from the rebadged Renault engines also came a sponsor change. Rothmans International was purchased by BAR's owner, British American Tobacco, and since BAR was already serving as its own sponsor it felt no need to sponsor another team. Williams contracted with technology company Hewlett Packard, and they once again adopted a blue and white livery (although the blue was a darker colour, almost navy blue, than their most recent Rothmans livery had been).
BMW Williams's first season did not see a single victory during the season. They did, however, manage to get on the podium three times, with Ralf Schumacher responsible for all three. Williams finished third in the Constructors' Championship, with 36 points, one more than the prior year. Ralf Schumacher finished fifth in the Drivers' Championship, while Button, in his debut season, finished three places behind in eighth. Button made some scrappy mistakes in early races (Monaco, Europe), but overall made an impressive debut in Melbourne, and continued to impress, notably at Silverstone, Spa and Suzuka.
2001
In 2001, Button moved to Benetton-Renault due to Montoya's arrival at the team. The FW23 won four races, three by Ralf Schumacher at Imola, Montreal, and his home Grand Prix in Germany. His teammate, Montoya, was victorious at Monza, and would have won a few more races if not for the FW23's unreliability and pit crew blunders. The car proved to be quicker than the Ferrari and McLaren counterparts in several races, but Williams' 2001 campaign only yielded third place in the Constructors' Championship.
2002
For 2002, Williams kept their 2001 driver line up for the upcoming season. The team only won one race, which was at Malaysia, one of only 2 races not won by Ferrari in a year dominated by the Ferraris of Michael Schumacher and Rubens Barrichello. Williams did improve on their Constructors' Championship position, finishing in second. Montoya finished third in the Drivers' Championship, eight points ahead of Ralf Schumacher, who finished fourth.
In qualifying for the Italian Grand Prix at the 5.793 km Monza circuit, Montoya lapped his Williams FW24 in 1:20.264 for an average speed of 161.449 mph, breaking the speed record of 160.938 mph set by Keke Rosberg in a Honda turbo powered Williams FW10 at Silverstone for the 1985 British Grand Prix.
2003
2003 would see BMW Williams reach their peak of success. During pre-season, Frank Williams was very confident that the FW25 would challenge for the title. The team won four races, with Montoya winning twice at Monaco and Germany, while Ralf Schumacher won at the Nürburgring and the following race at Magny-Cours. Montoya stayed in contention for the Drivers' Championship during the season, and the Colombian finished third in the championship, 11 points behind Michael Schumacher, while the younger Schumacher finished 24 points astern of Montoya in fifth. Williams finished second in the Constructors' Championship, two points ahead of McLaren.
2004
At the start of the 2004 season, it was announced that Montoya would be moving to McLaren in 2005. The team began the season with a radical nose-cone design, known as the "Walrus-Nose", that proved uncompetitive and was replaced by a more conventional assembly in the second half of the year. Ferrari for the third time running, dominated the season, winning 15 of the 18 races. Williams did, however, pick up a win during the season at the final race in Brazil, with Juan Pablo Montoya winning the race by a second from Kimi Räikkönen's McLaren; this remained Williams' last F1 win until the 2012 Spanish Grand Prix. Another memorable part of the season was when both Williams and Toyota were disqualified from the Canadian Grand Prix after it was discovered that both cars had brake irregularities, the brake ducts seemingly not conforming to regulations. Williams finished the season in fourth, scoring 88 points and finishing on the podium six times, while Montoya was the highest-placed Williams driver that year, finishing in fifth position and scoring 58 points.
2005
For the 2005 season, Schumacher moved to Toyota, while Montoya moved to McLaren. Taking their places were Australian Mark Webber and German Nick Heidfeld. Initially Jenson Button was to have driven for Williams in 2005, but an FIA ruling forced Button to remain with his current team BAR. Antônio Pizzonia served as the test driver for the team during the 2005 season. Meanwhile, Button signed a contract to drive for Williams in 2006.
During the course of the 2004 and 2005 F1 seasons, BMW Motorsport and director Mario Theissen increasingly became publicly critical of the Williams F1 team's inability to create a package capable of winning the Constructors' Championship, or even multiple victories within a single season. Williams, on the other hand, blamed BMW for not producing a good enough engine. Williams' failed attempt to prise Jenson Button out of his BAR contract may also have been an issue with Theissen. Despite Frank Williams' rare decision to cave in to commercial demands by employing German driver Nick Heidfeld when he allegedly preferred Antônio Pizzonia, the fallout between BMW and Williams continued through the 2005 Formula One season. Despite BMW's contract with Williams to supply engines until 2009, this public deterioration of the relationship between BMW and WilliamsF1 was a factor in the decision by BMW Motorsport to buy Sauber and rebrand that team to feature the BMW name.
This period (2000–2005) saw Williams depart from the standard livery scheme in motorsport, which consists of one colour scheme, either the teams' or the major sponsors', with smaller logos in their own scheme. BMW stipulated that, and paid for, the whole vehicle to be in blue and white, with other sponsors adopting this scheme. Also in 2000, Williams abandoned tobacco advertising in favour of Information technology companies, as the team's second major sponsor became Compaq. That sponsorship lasted until Compaq's acquisition by Hewlett-Packard. At the 2002 British Grand Prix, the team debuted the Hewlett-Packard sponsorship. After complaints about the HP logo on the rear wing it was replaced in 2003 with the sponsor's tag line, ""Invent". One of the most memorable results of this technological partnership was a worldwide television commercial featured drivers Ralf Schumacher and Juan Pablo Montoya seemingly driving their BMW Williams cars around a track by radio control from a grandstand.
This "clean"" image allowed Williams to sign a cigarette anti-craving brand, Niquitin, and Anheuser-Busch, alternating with the Budweiser beer brand and SeaWorld Adventure Parks, in compliance with trademark disputes or alcohol bans.
Cosworth engines (2006).
2006
Williams could have opted to continue with BMW engines in 2006, despite the fact that the engine manufacturer was about to set up its own team. In the end, though, Williams opted for Cosworth V8 engines for 2006.
The 2006 season saw Nico Rosberg replace Nick Heidfeld, who departed for BMW Sauber, while Mark Webber stayed on with the team. Despite having signed a contract to race for Williams, Jenson Button decided to stay with BAR for 2006 as it was to become a Honda works team. In September 2005 a deal was reached to allow Button to remain with BAR, with Williams receiving around £24m, some of it paid by Jenson himself, to cancel this contract.
Williams and Cosworth entered a partnership agreement where Cosworth would supply engines, transmissions and associated electronics and software for the team. Major sponsors Hewlett-Packard (HP) concluded sponsorship agreements one year before their official end of contract. The Williams team also switched to Bridgestone tyres.
The season started well, with both drivers scoring points in the opening race of the season, and Nico Rosberg setting the fastest lap at the Bahrain Grand Prix. However, the rest of the season was very disappointing, with 20 retirements out of 36 starts for the two cars. The team failed to finish on the podium all season, the first time since Williams' first season in 1977. The team eventually finished eighth in the Constructors' Championship, with only 11 points.
Toyota engines (2007–2009).
Following Williams' worst points tally since 1978, the Grove-based team announced that Japanese car manufacturer Toyota would be supplying the engines for the 2007 season. Along with Toyota supplying engines to the team, a number of other changes were announced for 2007: Alexander Wurz, who had been a test driver at Williams since 2006, became the team's second driver to replace the outgoing Mark Webber; Japanese driver Kazuki Nakajima, son of Satoru, replaced Wurz as a test driver alongside Karthikeyan. Sponsorship also saw a change in 2007, as it was announced that AT&T would become the title sponsors for the team from the upcoming season. AT&T were previously involved as minor sponsors with the Jaguar and McLaren teams, but moved to Williams following McLaren's announcement of a title sponsorship deal with Vodafone, a competitor of AT&T. On 2 February, the new FW29 was presented to the media in the UK. Soon afterwards, the team secured a sponsorship deal with Lenovo who built the team's new supercomputer.
Rosberg and Wurz gave Williams a more productive season in terms of points and in Canada the Austrian scored the team's first podium finish since Nick Heidfeld's second place finish at the 2005 European Grand Prix. Over the course of the year Rosberg was consistently in the points, scoring 20 during the season, in comparison teammate Wurz who finished in the points three times. Following the announcement that Wurz would be retiring from the sport, Williams brought in their young test driver Nakajima to drive the second car for them in the final race in Brazil. The Japanese driver finished in tenth despite starting from near the back of the grid, while Rosberg enjoyed his best race of the season, finishing in fourth. Williams finished fourth in the Constructors' Championship that year.
For the 2008 season, Williams confirmed Nico Rosberg and Kazuki Nakajima as their race drivers. Rosberg was confirmed as staying with Williams until the end of 2009 on 9 December 2007, ending speculation that he could take Fernando Alonso's vacated seat at McLaren. During the Winter testing sessions, the team ran six different liveries to celebrate their thirtieth year in the sport and their 500th Grand Prix.
The 2008 season was a mixture of success and disappointment for Williams. While Rosberg managed to obtain 2 podiums in Australia and Singapore, the team struggled at circuits with high speed corners. The fact that the team was one of the first to switch development to their 2009 car (when new regulations came in) also hindered their season, and Williams finished a disappointing 8th in the Constructors' Championship. Rosberg stated that unless the team was more competitive in the near future, he would look for drives elsewhere. Williams retained Rosberg and Nakajima for the 2009 season.
Frank Williams had admitted that he had regretted parting with BMW but stated that Toyota had tremendous ability to become a top engine supplier. Speculation had been surrounding Toyota's future on the Formula 1 grid. This was due to the fact that for a big budget team, Toyota had only managed 2nd place as their best result. In December 2008 Williams confirmed their commitment to F1 following the Honda withdrawal announcement.
Ahead of the 2009 Brazilian Grand Prix, Williams announced that it would be ending its three year partnership with Toyota and finding a new engine supplier for 2010.
Return to Cosworth engines (2010–2011).
After the termination of their Toyota contract, Williams announced that from the 2010 Formula One season they were to enter into a "long-term partnership" with Cosworth, and would be using an updated version of the "CA" V8 engine which powered their cars in 2006. Williams also announced a complete driver change for the 2010 season. Rubens Barrichello joined from 2009 Constructors' Champion Brawn GP, whilst GP2 champion Nico Hülkenberg graduated from the test driver seat. Replacing Hülkenberg in the test seat was Finland's Valtteri Bottas, who finished third in the 2009 Formula Three Euroseries as well as winning the non-championship Masters of Formula 3 event at Zandvoort.
Their new 2010 car, the FW32 was unveiled for the first time at a shakedown test at Silverstone. Its first official test was on 1 February at Circuit Ricardo Tormo in Valencia. Hülkenberg took the team's first pole position in over five years, in variable conditions at the Brazilian Grand Prix. Hülkenberg was dropped from the team ahead of the 2011 season, and replaced by Venezuelan newcomer and reigning GP2 Series champion Pastor Maldonado. The combination of Rubens Barrichello and Maldonado meant that 2011 would be the first time since 1981 that Williams would start a season without a European driver in their line-up. At the second pre-season test in Jerez, Barrichello posted the fastest time of the week on the last day.
That was, however, to no avail as Williams endured one of their most dismal seasons to date: Two ninth places for Barrichello and one tenth place for Maldonado was their best results during the entire year and after Brazil, the team ended up in a ninth place in the Constructors' Championship.
Return to Renault engines (2012–2013).
On 4 July 2011, Williams announced they would be reuniting with former engine supplier Renault who will supply the team's engines from 2012 onwards. On 1 December 2011, it was confirmed that Maldonado would be retained for the 2012 season; along with reserve driver Valtteri Bottas, who took part in 15 Friday practice sessions. According to a range of sources, it was confirmed on 17 January 2012 that Bruno Senna will be the driver to partner Maldonado, effectively ending Rubens Barrichello's F1 career.
Prior to the 2012 season, Patrick Head moved from the Williams F1 team to Williams Hybrid Power Limited, another subsidiary of Williams Grand Prix Holdings. The team also announced that its relationship with AT&T ended by mutual agreement, and there are ongoing negotiations with another telecommunications company for team's title sponsorship. At the 2012 Spanish Grand Prix, Pastor Maldonado took his first Grand Prix victory, which is also Williams' first race victory since 2004 Brazilian Grand Prix. Around 90 minutes after celebrating this win, a fire broke out in the garage of the Williams team, damaging the FW34 of Bruno Senna and leaving seven people in hospital. The team eventually achieved eighth position in the Formula One Constructors' World Championship.
On 28 November 2012, Maldonado was retained by the team for 2013 and was joined by Bottas, promoted from his role as test driver.
The team struggled throughout the 2013 season, despite a good qualifying session at the 2013 Canadian Grand Prix and a place in the top 10 at the 2013 United States Grand Prix, only scoring five points in the World Constructors' Championship.
Mercedes-Benz power units (2014–).
On 30 May 2013, Williams announced a long term contract with Mercedes to supply engines for the team. The German manufacturer will provide 1.6 litre V6 turbo engines from the start of the 2014 season. On 11 November 2013 the team confirmed they would retain Bottas and would also sign Ferrari's Felipe Massa to replace Maldonado for the 2014 season. The team also unveiled a new, multi-year title sponsorship deal with drinks brand Martini on 6 March 2014. As part of the deal, the team became Williams Martini Racing. The team got their first pole position since 2012, courtesy of Massa at the Austrian Grand Prix; it was the only time that Mercedes would be beaten to pole position over the course of the 2014 season. With Bottas qualifying alongside Massa, it was also the first time the team had locked out the front row since the 2003 German Grand Prix. The team enjoyed an upturn in performance, including a double podium in Abu Dhabi, resulting in them taking third place in the Constructors' Championship.
Williams Advanced Engineering.
Williams Advanced Engineering.
Williams Advanced Engineering is the technology and engineering services business of the Williams Group. Based in the United Kingdom, it is located in a dedicated 3,800m sq. facility adjacent to Williams Formula One facilities.
Williams Advanced Engineering provides technical innovation, engineering, testing and manufacturing services to deliver "energy efficient performance technologies". Its target markets are: Automotive, Motorsport, Civil Aerospace, Defence (Land, Sea, Air), Renewable Energy and Sports Science.
Williams Hybrid Power.
Williams Hybrid Power (WHP) was the division of Williams F1 that developed electromechanical flywheels for mobile applications such as buses, trams and high performance endurance racing cars. A type of hybrid system that uses a spinning composite rotor to store energy, these flywheels help a vehicle save fuel and ultimately reduce its CO2 emissions.
WHP was first established in 2008 and immediately set about developing a new flywheel energy recovery system for the Williams F1 Team after the introduction of Kinetic Energy Recovery Systems (KERS) into Formula One for the 2009 season. While other teams were pouring their efforts into electric battery systems, Williams F1 opted to go down the flywheel route because of a strong belief in the technology's wider applications. Whilst it was never raced in Formula One due to technical changes, WHP has since seen its technology adapted for a range of applications. For example, the Audi R18 hybrid car that won the 2012 Le Mans 24 Hours used a WHP flywheel. WHP has also seen its flywheel technology introduced into a series of buses as part of a deal with the Go-Ahead Group, one of the UK's biggest transport operators. In April 2014, Williams Hybrid Power was sold to GKN.
Williams Heritage.
Williams Heritage (WH) is the retired chassis and restoration division of Williams F1 (similar to Classic Team Lotus) that keeps and maintains old retired Williams Formula One chassis that are no longer in racing use or are prepared for historic events or show runs. The division's headquarters are located at the Formula One team's Grove site and also manages and looks after the Williams Grand Prix collection. The division is currently managed by long time Williams mechanic Dickie Stanford.
Other motorsports and Williams-branded cars.
Formula 2.
Williams have developed the car for the revived Formula 2 championship, beginning in 2009. The design was originally created for a new, more powerful off-shoot of the Formula Palmer Audi series, however the car was re-purposed when Jonathan Palmer's MotorSport Vision successfully bid for the rights to run the new Formula 2 series.
Group B rallying.
The Metro 6R4 rally car was developed by Williams in 1984 on commission from Rover. The rally car was a MG Metro with a completely new V6 engine (mid-engined) and four-wheel drive, developed to the international Group B rallying regulations. Williams developed the car in just six months.
British Touring Car Championship.
Williams entered the British Touring Car Championship in 1995, running the works Renault entry. Alain Menu was retained, with Will Hoy signed from Toyota to partner him. While Menu was a championship contender, Hoy had constant failures and bad luck during the first half of the season. However, Hoy's luck changed and he won three races and scored several podium finished in the second half of the year, eventually taking fourth in the championship while Menu finished second in the championship with seven wins. Renault won the manufacturers championship. 1996 was a more difficult year with the front wheel drive cars outclassed by the 4WD Audis of Frank Biela and John Bintcliffe. Alain Menu was second in the championship again, while Will Hoy finished a lowly ninth. 1997 was a breakthrough year for Williams, winning the drivers' championship with Alain Menu, the manufacturers' trophy and teams' award. Other changes for the team saw Jason Plato replacing Will Hoy, taking third in the championship. The team won 15 races out of 24 in 1997. 1998 saw few changes to the Williams team: the driver lineup was unchanged with Alain Menu to defend his title alongside Jason Plato, but the main sponsor for 1998 was Nescafé, with Renault still putting sponsorship in for the team. While the Renaults had a new look for 1998, the opposition had caught up after 1997, and both Alain Menu and Jason Plato had a much harder season. The luck had changed for the team, with Alain Menu and Jason Plato no longer as competitive as they had been in recent years. Alain Menu and Jason Plato finished fourth and fifth in the championship. In the final round of 1998 at Silverstone, Menu and Plato were joined by Independents Champion Tommy Rustad. Renault ultimately finished third in the manufacturers trophy and second in the teams championship. 1999 was the most difficult season for Williams, after Alain Menu left Renault after racing with them since 1993. Jason Plato was joined by Jean-Christophe Boullion or 'JCB'. Nescafé were again the main sponsor for the Williams team in 1999. Renault did not have much luck in 1999 with engine failures haunting the team during the mid-part of the season, and poor results for much of the season. Only one win from Jason Plato was the only success for the season, and Renault pulled out of the BTCC at the end of the season.
Le Mans 24 Hours.
Prior to their F1 partnership, Williams Motorsport built Le Mans Prototypes for BMW, known as the V12 LM and V12 LMR. The V12 LMR won the 24 Hours of Le Mans in 1999. The car was driven by Pierluigi Martini; Yannick Dalmas and Joachim Winkelhock, and operated by Schnitzer Motorsport under the name of BMW Motorsport.
Renault Clio Williams.
Williams's name and logo were used on the Renault Clio Williams. However, no input was provided by Williams into the development of the car.
Porsche AG 911 GT3R Hybrid.
Through a subsidiary, Williams Hybrid Power, the company developed and supplied a flywheel based kinetic energy storage system which is in use on a Porsche 911 GT3 R car in various GT racing series. The car achieved its first victory on 28 May 2011 at the 4th round of the VLN Endurance Racing Championship held at the Nürburgring.
References.
"Williams History (1967–2000) Taken from:"
"All Formula One race and championship results are taken from":
</dl>

</doc>
<doc id="34105" url="http://en.wikipedia.org/wiki?curid=34105" title="Williams College">
Williams College

Williams College is a private liberal arts college located in Williamstown, Massachusetts, United States. It was established in 1793 with funds from the estate of Ephraim Williams. Originally a men's college, Williams became co-educational in 1970. Fraternities were also phased out during this period, beginning in 1962.
Williams has three academic curricular divisions (humanities, sciences and social sciences), 24 departments, 36 majors, and two master's degree programs in art history and development economics. There are 334 voting faculty members, with a student-to-faculty ratio of 7:1. s of 2012[ [update]], the school has an enrollment of 2,052 undergraduate students and 54 graduate students.
The academic year follows a 4–1–4 schedule of two four-course semesters plus a one-course "winter study" term in January. A summer research schedule involves about 200 students on campus completing projects with professors.
Williams College currently occupies 1st place in "U.S. News & World Report"‍ '​s 2014 ranking of the 266 liberal arts colleges in the United States. "Forbes" magazine ranked Williams the best undergraduate institution in the United States in its 2014 publication of America's Top Colleges beating out other colleges such as Yale, Harvard, Stanford and Princeton.
History.
Colonel Ephraim Williams was an officer in the Massachusetts militia and a member of a prominent landowning family. His will included a bequest to support and maintain a free school to be established in the town of West Hoosac, Massachusetts, provided that the town change its name to Williamstown. Williams was killed at the Battle of Lake George on September 8, 1755.
After Shays' Rebellion, the Williamstown Free School opened with 15 students on October 26, 1791. The first president was Ebenezer Fitch. Not long after its founding, the trustees of the school petitioned the Massachusetts legislature to convert the free school to a tuition-based college. The legislature agreed and on June 22, 1793, Williams College was chartered. It was the second college to be founded in Massachusetts.
At its founding, the college maintained a policy of racial segregation, refusing admission to black applicants. This policy was challenged by Lucy Terry Prince, who is credited as the first black American poet, when her son Festus was refused admission to the college on account of his race. Prince, who had already established a reputation as a raconteur and rhetorician, delivered a three-hour speech before the college's board of trustees, quoting abundantly from scripture, but was unable to secure her son's admission. More recent scholarship, however, has highlighted how there are no records within the college itself to confirm that this event occurred, and that Festus Prince may have been refused entry for an insufficient mastery of Latin, Greek, and French, all of which were necessary for successful completion of the entrance exam at the time, and which would most likely not have been available in the local schools of Guilford, Vermont, where Festus was raised.
In 1806, a student prayer meeting gave rise to the American Foreign Mission Movement. In August of that year, five students met in the maple grove of Sloan's Meadow to pray. A thunderstorm drove them to the shelter of a haystack, and the fervor of the ensuing meeting inspired them to take the Gospel abroad. The students went on to build the American Board of Commissioners for Foreign Missions, the first American organization to send missionaries overseas. The Haystack Monument near Mission Park on the Williams Campus commemorates the historic "Haystack Prayer Meeting".
By 1815, Williams had only two buildings and 58 students and was in financial trouble, so the board voted to move the college to Amherst, Massachusetts. In 1821, the president of the college, Zephaniah Swift Moore, who had accepted his position believing that the college would move east, decided to proceed with the move. He took 15 students with him, and refounded the college under the name of Amherst College. Some students and professors decided to stay behind at Williams and were allowed to keep the land, which was at the time relatively worthless. According to legend, Moore also took portions of the Williams College library. Though plausible, the transfer of books is unsubstantiated. Moore died just two years later after founding Amherst, and was succeeded by Heman Humphrey, a trustee of Williams College. Edward Dorr Griffin was appointed President of Williams and is widely credited with saving Williams during his 15-year tenure.
A Williams student, Gardner Cotrell Leonard, designed the gowns he and his classmates wore to graduation in 1887. Seven years later he advised the Inter-Collegiate Commission on Academic Costume, which met at Columbia University, and established the current system of U.S. academic dress. One reason gowns were adopted in the late nineteenth century was to eliminate the differences in apparel between rich and poor students.
During World War II, Williams College was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program which offered students a path to a Navy commission.
Construction and expansion.
In the last decade, construction has changed the look of the college. The addition of the $38 million Unified Science Center to the campus in 2001 set a tone of style and comprehensiveness for renovations and additions to campus buildings in the 21st century. This building unifies the formerly separate lab spaces of the physics, chemistry, and biology departments. In addition, it houses Schow Science Library, notable for its unified science materials holdings and architecture. It features vaulted ceilings and an atrium with windows into laboratories on the second through fourth floors of the science center.
In 2003, Williams began the first of three massive construction projects. The $60 million '62 Center for Theatre and Dance was the first project to be successfully completed in the spring of 2005. The $44 million student center, called Paresky Center, opened in February 2007.
Construction had already begun on the third project, called the Stetson-Sawyer project, when economic uncertainty stemming from the 2007 financial crisis led to its delay. College trustees initially balked at the cost of the Stetson-Sawyer project, and revisited the idea of renovating Sawyer in its current location, an idea which proved not to be cost-effective. The entire project includes construction of two new academic buildings, the removal of Sawyer Library from its current location, and the construction of a new library at the rear of a renovated Stetson Hall (which served as the college library prior to Sawyer's construction). The academic buildings, temporarily named North Academic Building and South Academic building, were completed in fall of 2008. In the spring of 2009, South Academic Building was renamed Schapiro Hall in honor of former President Morton O. Schapiro. In the spring of 2010 the North Academic Building was renamed Hollander Hall. Construction of the new Sawyer Library is scheduled to be completed in 2014, after which the old Sawyer Library will be razed.
After several years of planning, the college decided to group undergraduates starting with the Class of 2010 into four geographically coherent clusters, or "Neighborhoods". Since the fall of 2006, first-years have been housed in Sage Hall, Williams Hall and Mission Park, while the former first-year dormitories East College, Lehman Hall, Fayerweather, and Morgan, joined the remaining residential buildings as upperclass housing. A student vote on the names of the four "neighborhoods" selected "Currier", "Wood", "Spencer" and "Dodd" by a simple majority. Incoming first-years live in groups of approximately 20, together with two junior advisors. Rising sophomores, juniors, and seniors have the opportunity to change neighborhoods each spring if they so choose. The system is an attempt to integrate all undergraduates more successfully than was previously possible, mixing students representing a variety of interests and ethnicities, as well as to foster student-faculty interaction and to de-centralize event planning. During the spring 2009 semester, a committee formed to evaluate the neighborhood system, and released a report the following fall.
From 2003 through 2008, Williams conducted one of the largest capital campaigns ever undertaken by a liberal arts college, with a goal of raising $400 million by September 2008. The college reached $400 million at the end of June 2007, a year and a half ahead of schedule. By the close of the campaign, Williams had raised $500.2 million.
As of the 2008–09 school year, the College eliminated student loans from all financial aid packages in favor of grants. The College was the fourth institution in the United States to do so, following Princeton University, Amherst College, and Davidson College.
However, in February 2010, the College announced that it would re-introduce loans to its financial aid packages beginning with the Class of 2015 due to the College's changed financial situation.
In January 2007 the board voted unanimously to reduce college CO2 emissions 10% below 1990 levels by 2020, or roughly 50% below 2006 levels. To meet those goals, the college set up the Zilkha Center for Environmental Initiatives and undertaken an energy audit and efficiency timeline. Williams received an 'A-' on the 2010 College Sustainability Report Card, following 'B+' grades on both the 2008 and 2009 report cards.
In December 2008, President Morton O. Schapiro announced his departure from the college to become president of Northwestern University.
On September 28, 2009, the presidential search committee announced the appointment of Adam F. Falk as the 17th president of Williams College. Falk, dean of the Zanvyl Krieger School of Arts and Sciences at Johns Hopkins University, began his term on April 1, 2010. Dean of the Faculty William Wagner took the position of interim president beginning in June 2009, and continued in that capacity until President-elect Falk took office.
Academics.
Williams is a small, four-year liberal arts college accredited by the New England Association of Schools and Colleges.
There are three academic curricular divisions (humanities, sciences, and social sciences), 24 departments, 33 majors, and two small master's degree programs in art history and development economics. Students may also concentrate in 12 additional academic areas that are not offered as majors (e.g., environmental studies). The academic year follows a 4–1–4 schedule of two four-course semesters plus a one-course "winter study" term in January. During the winter study term, students study various courses outside of typical curriculum for 3 weeks. Students typically take this course on a pass/fail basis. Past course offerings have included: Ski patrol, Learn to Play Chess, Accounting, Inside Jury Deliberations, and Creating a Life: Shaping Your Life After Williams, among many others. Williams students often take the winter study term to study abroad or work on intensive research projects.
Williams granted 510 bachelor's degrees and 35 master's degrees in 2008. The cost of tuition and fees for 2010–2011 was $52,340; 53% of students were given need-based financial aid, which averaged $46,006.
Williams sponsors the Williams–Mystic program at Mystic Seaport; the Williams–Exeter Programme at Exeter College of Oxford University; and Williams in Africa.
Selectivity.
For the Class of 2018, the acceptance rate was 18%, and the admitted students’ academic profile showed average SAT scores of 735 in critical reading, 727 in math, and 735 in writing. The average super-scored ACT was a 33. Eighty-two percent of the students who submitted high school rank were projected to graduate in the top ten percent of their class.
Williams is classified as "most selective" by "U.S. News & World Report" and "more selective" by the Carnegie Foundation for the Advancement of Teaching.
Rankings and other statistics.
In 2010, 2011, and 2014, Forbes magazine ranked Williams College as the best undergraduate institution in the United States. In the 2012 edition, Williams ranked as the 2nd best undergraduate institution in the United States. In its 2013 edition, Forbes ranked Williams College as the 9th best college in America. In the 2014 America's Top Colleges list, Forbes placed Williams in the number one spot.
Williams College is - for the twelfth consecutive year - occupying 1st place in "U.S. News & World Report"'s 2015 ranking of the 181 liberal arts colleges in the United States.
In a 2004 survey by the Wall Street Journal, Williams College was ranked as the 5th largest feeder school to elite law, business, and medical schools in America, behind Harvard University, Yale University, Princeton University, and Stanford University.
Williams is ranked 1st by the National Collegiate Scouting Association, which ranks colleges based on student-athlete graduation rates, academics, and athletics. Rounding out the top five are Amherst College, Middlebury College, Washington University in St. Louis, and Stanford University.
Williams ranked 6th after Harvard, Yale, Princeton, Stanford, and Brown University in "Newsweek"'s 2011 ranking of "Brainiac" Colleges which measured the success of alumni in winning Rhodes, Marshall, and Truman Scholarships.
Williams ranked 8th among colleges and universities in the percentage of students who graduate in four years, followed by Yale and Hamilton and preceded by Carleton, Amherst, and Vassar.
Oxbridge Tutorials.
One of the distinctive features of a Williams education is modeled after the tutorial systems at the universities of Oxford and Cambridge, a rarity in American higher education. Although tutorials at Williams were originally aimed at upperclassmen, the faculty voted in 2001 to expand the tutorial program. There is now a diverse offering of tutorials, spanning many disciplines, including math and the sciences, that cater to students of all class years. In 2009–2010 alone, 62 tutorials were offered in 21 departments.
Enrollment for tutorials is capped at 10 students, who are then divided into five pairs that meet separately with the professor once a week. Each week, one of the students writes and presents a 5–7-page paper while the other student critiques it. The same pair reverses roles for the next week. The professor takes a more limited role than in a traditional lecture class, and usually allows students to steer and guide the direction of the conversation.
Student course evaluations for tutorials are typically very high. In a survey of alumni who had taken tutorials, more than 80% found their tutorials to be "the most valuable of my courses" at Williams.
Organization and administration.
The Board of Trustees of Williams College has 25 members and is the governing authority of the College. The President of the College serves on the Board "ex officio." There are five Alumni Trustees, each of whom serves for a five-year term. There are five Term Trustees, each elected by the Board for five-year terms. The remaining 14 members are Regular Trustees, also elected by the Board but serving up 15 years, although not beyond their seventieth birthday.
The Board appoints as senior executive officer of the college a President who is also a member of and the presiding officer of the faculty. Nine senior administrators report to the President including the Dean of the Faculty, Provost, and Dean of the College. Adam F. Falk was recently elected the 17th president of Williams, and took office on April 1, 2010.
College Council (CC) is the student government of Williams College. Its members are elected to represent each class year, the first-year dorms, and the student body at large. CC allocates funds from the Student Activities Fee, appoints students to the faculty-student-administration committees that oversee most aspects of College life, and debates issues of concern to the entire campus community. College Council is the forum through which students address concerns and make changes around campus. CC is led by two co-Presidents.
Campus.
Williams is situated on a 450 acre campus in Williamstown, Massachusetts, located in the Berkshires in rural northwestern Massachusetts. The campus contains more than 100 academic, athletic, and residential buildings.
The early planners of Williams College eschewed the traditional collegiate quadrangle organization, choosing to freely site buildings among the hills. Later construction, including East and West Colleges and Griffin Hall, tended to cluster around Main Street in Williamstown. The first campus quadrangle was formed with East College, South College, and the Hopkins Observatory.
The Olmsted Brothers design firm played a large part in shaping the campus design and architecture. In 1902, the firm was commissioned to renovate a large part of campus, including the President’s House, the cemetery, and South College; as well as incorporating the George A. Cluett estate into the campus acreage. Although these campus renovations were completed in 1912, the Olmsted Brothers would advise the gradual transformation of campus design for six decades. The present-day grounds layout reflects much of the design intent of the Olmsted Brothers.
Williams College is the site of the Hopkins Observatory, the oldest extant astronomical observatory in the United States. Erected in 1836–1838, it now contains the Mehlin Museum of Astronomy, including Alvan Clark's first telescope (from 1852), as well as the Milham Planetarium, which uses a Zeiss Skymaster ZKP3/B optomechanical projector and an Ansible digital projector, both installed in 2005. The Hopkins Observatory's 0.6-m DFM reflecting telescope (1991) is installed elsewhere on the campus. Williams joins with Wellesley, Wesleyan, Middlebury, Colgate, Vassar, Swarthmore, and Haverford/Bryn Mawr to form the Keck Northeast Astronomy Consortium, sponsored for over a decade by the Keck Foundation and now with its student research programs sponsored by the National Science Foundation.
Hopkins Hall serves as the administration building on campus, housing the offices of the president, Dean of the Faculty, registrar, and provost, among others.
There is a Newman Center on campus.
The Chapin Library supports the liberal arts curriculum of the college by allowing students close access to a number of rare books and documents of interest. The library opened on June 18, 1923, with an initial collection of 9,000 volumes contributed by alumnus Alfred Clark Chapin, Class of 1869. Over the years, Chapin Library has grown to include over 50,000 volumes (including 3,000 more given by Chapin) as well as 100,000 other artifacts such as prints, photographs, maps, and bookplates.
The most famous items in the library's collection include first printings of the Declaration of Independence, Articles of Confederation, United States Constitution, and Bill of Rights, as well as George Washington's personal copy of "The Federalist" Papers. Other notable objects include a range of books, letters, and miscellaneous items relating to Theodore Roosevelt, who was a friend and, at one point, colleague of Chapin in the New York State Assembly.
The Chapin Library's science collection includes a first edition of Nicolaus Copernicus's "De revolutionibus orbium coelestium", as well as first editions of books by Tycho Brahe, Johannes Kepler, Galileo, Isaac Newton, and other major figures.
The Williams College Museum of Art (WCMA), with over 12,000 works (only a fraction of which are displayed at any one time) in its permanent collection, serves as an educational resource for both undergraduates and students in the graduate art history program.
Notable works include "Morning in a City" by Edward Hopper, a commissioned wall painting by Sol LeWitt, and a commissioned outdoor sculpture and landscape work by Louise Bourgeois entitled "Eyes".
Though often overshadowed by the neighboring and much larger Clark Art Institute and Massachusetts Museum of Contemporary Art, WCMA remains one of the premier attractions of the Berkshires. Because the museum is intended primarily for educational purposes, admission is free for all.
Located in front of the West College dormitory, the Hopkins gate serves as a memorial to brothers Mark and Albert Hopkins. Both made lasting contributions to the Williams College community. Mark was appointed as president of the college in 1836,
 while Albert was elected a professor in 1829.
The Hopkins gate is inscribed with an inspirational motto that is familiar to all in the Williams College community.
Climb High, Climb Far <br>
Your Goal the Sky, Your Aim the Star. <br>
Student activities and traditions.
Student media.
The longest running student newspaper at Williams is the "Williams Record", a weekly broadsheet paper published on Wednesdays. The newspaper was founded in 1887, and now has a weekly circulation of 3,000 copies distributed in Williamstown, in addition to more than 600 subscribers across the country. The newspaper used to not receive financial support from the college or from the student government and relied on revenue generated by local and national ad sales, subscriptions, and voluntary contributions for use of its website, but the paper went into debt in 2004 and is now subsidized by the Student Activities Tax. Both Sawyer Library and the College Archives maintain more than a century's worth of publicly accessible, bound volumes of the "Record". The newspaper provides access free of charge to a searchable database of articles stretching back to 1998 on its website.
The student yearbook is called "The Gulielmensian", which means "Williams Thing" in Greek. It was published irregularly in the 1990s, but has been annual for the past several years and dates back to the mid-19th century.
Numerous smaller campus publications are also produced each year, including "The Telos", a journal of Christian thought, "The Cowbell", a humor magazine, the "Williams College Law Journal", a collection of undergraduate articles, "the "Literary Review", a literary magazine, and "Monkeys With Typewriters", a magazine of non-fiction essays.
91.9 WCFM.
WCFM is a college-owned, student-run, non-commercial radio station broadcasting from the basement of Prospect House at 91.9 MHz. Featuring 85 hours per week of original programming, the station features a wide variety of musical genres, in addition to sports and talk radio. The station may also be heard on the Internet via SHOUTcast.com. Members of the surrounding communities above the age of 18 are allowed to DJ on the station, which, as part of its mission, seeks to serve the surrounding community with news and announcements of public interest. The board of the radio station holds a concert every semester.
Trivia contest.
At the end of every semester but one since 1966, WCFM has hosted an all-night, eight-hour trivia contest. Teams of students, alumni, professors, friends, and others compete to answer questions on a variety of subjects, while simultaneously identifying songs and performing designated tasks. The winning team's only prize is the obligation to create and host the following semester's contest.
The precise date of the debut contest is uncertain. Most spring contests occur in early May, but during its first decade, Williams Trivia was sometimes held in March or February. Assuming a May date, Lawrence University's 50-hour-long Great Midwest Trivia Contest, first held on April 29, 1966, would be the oldest continuous competition of its sort in the United States, but if the first Williams contest was held earlier, it would be the oldest. The distinction is, appropriately, trivial.
While other college-based trivia contests in the United States emphasize marathon endurance and revel in the obscurity of their arcana, the aim of the Williams contest is to cram as much evocative and entertaining material into as concentrated a space as possible. Lasting just eight hours, a typical Williams Trivia contest will demand between 900 and 1,200 separate "bits" of trivial information, delivering twice as much content as its "competitors" in a fraction of the time. No discernible rivalry exists between any of the various contests. The contest has occasionally received outside media coverage, including in the Sunday New York Times.
Student music.
Music ensembles at Williams include Berkshire Symphony, Wind Ensemble, Student Symphony, Brass Ensemble, Clarinet Choir, Concert and Chamber Choirs, Handbell Choir, Gospel Choir, Jazz Ensemble, Kusika and the Zambezi Marimba Band, Percussion Ensemble, and Marching Band. Both music majors and non-majors are welcome to participate in all groups.
The Berkshire Symphony is the premier full orchestra on campus. It is conducted by Ronald Feldman, a former Boston Symphony Orchestra cellist. Half of the orchestra consists of students, while the principal players and many section players are area professionals. The Berkshire Symphony prepares four concerts per year, two during the fall semester and two during the spring semester. The Symphony's final concert features several student soloists in an annual concerto competition concert. All concerts are free and open to the public and take place in Chapin Hall.
The Williams College Wind Ensemble, directed by Matthew Marsit, presents artistic programs blending traditional and contemporary wind band music. In recent years, the group has evolved to include strings and premieres and performs works by prominent contemporary composers, including members of the faculty.
Student Symphony is an entirely student-run, student-conducted group. Student Symphony rehearses weekly and performs once per semester.
Under the direction of Bradley Wells, the Concert and Chamber Choirs perform a wide range of repertoire at a variety of concerts. A choral highlight is always the Festival of Lessons and Carols held just prior to the holidays in the Thompson Memorial Chapel.
The Williams Jazz program includes academic courses, ensembles (both traditional big band, by audition, and several small ensembles), and applied lessons on primary jazz instruments.
In the Shona language of Zimbabwe, Kusika means "to create." Founded in 1989 by Professor Ernest D. Brown at Williams College, Kusika performs traditional African music, dance, and storytelling from Ghana, Zimbabwe, and Senegal. The Zambezi Marimba Band, founded in 1992 by Professor Brown, was the first African marimba band to be established in the Eastern United States. The ensemble plays marimba music from Zambia, Zimbabwe, South Africa, and from the African diaspora around the world.
Sankofa, called "Kofa" for short, is the Williams College Step Team. It is a co-ed, student-run dance company. Sankofa choreographs original material that incorporates popular song, drums, hip-hop, break dance, spoken word, poetry, and sheer creative ingenuity. Stepping features precise, synchronized, and complex rhythmic body movements, combined with singing, chanting, and verbal play. The word “sankofa,” from the Akan people in Ghana, loosely translates to “reaching back in order to move forward.” Sankofa was formed in the fall of the ‘96-’97 school year by five women from the class of 2000: Dahra Jackson, Maxine Lyle, Mya Fisher, Melina Evans and Samantha Reed. In 2005, Lyle founded Soul Steps, a professional step company.
Williams' coed hip hop dance group "Nuttin' But Cuties", usually shortened to NBC, is one of the more prominent groups on campus with well-attended shows in the fall and spring semesters.
The Williams Percussion Ensemble, led by Matthew Gold, explores the masterworks of twentieth century percussion music, experimental music, music of many of the world's traditions, and the most up-to-date works by contemporary composers for percussion instruments.
The Marching Band, named "The Moocho Macho Moocow Military Marching Band", serves as a cheering section at the football games, as well as an entertainment show for halftime.
Williams also hosts nine student-organized a cappella singing groups. All nine groups are among the most-watched performing arts groups on campus. There are two all-female groups, the Accidentals and Ephoria. The two all-male groups are the Williams Octet (the oldest group at the college) and the Springstreeters, and the two co-ed groups are Ephlats and Good Question. The seventh group, the Elizabethans, are a mixed-voice Renaissance ensemble. Another recently formed a cappella group is the Aristocows, sings only Disney songs. The newest a cappella group is called the Far Ephs Movement and specialize in Asian pop songs.
The Williams Gospel Choir has served the college since 1986. Their performances are usually at the end of the semester, right before finals start, and serve to provide a spiritual and emotional courage to students during this difficult time of the semester.
School colors and mascot.
Williams's school colors are purple and gold, with purple as the primary school color. A story explaining the origin of purple as a school color says that at the Williams-Harvard baseball game in 1869, spectators watching from carriages had trouble telling the teams apart because there were no uniforms. One of the onlookers bought ribbons from a nearby millinery store to pin on Williams' players, and the only color available was purple. The buyer was Jennie Jerome (later Winston Churchill's mother) whose family summered in Williamstown.
The Williams college mascot is a purple cow. The mascot's name, Ephelia, was submitted in a radio contest in October 1952 by Theodore W. Friend, a senior at Williams. The origins of the cow mascot are unknown, but one possibility is that it was inspired by the "Purple Cow" humor magazine, a student publication begun in 1907, which used the college color along with a cow. The title of the humor magazine was in reference to Gelett Burgess's nonsense poem known as the "Purple Cow":
I never saw a purple cow <br>
I never hope to see one; <br>
But I can tell you, anyhow, <br>
I'd rather see than be one! <br>
Alma Mater.
Williams claims the first "alma mater" song written by an undergraduate, "The Mountains," was by Washington Gladden of the class of 1859. 
Mountain Day.
On one of the first three Fridays in October, the president of the college cancels classes and declares it Mountain Day. The bells ring, announcing the event, members of the Outing Club unfurl a banner from the roof of Chapin Hall and students hike up Stony Ledge. At Stony Ledge, they celebrate with donuts, cider and a cappella performances.
The first known mention of Mountain Day was made by Williams president Edward Dorr Griffin in his notebook on college business. He wrote, under 'Holidays': "About the 24th of June a day to go to the mountain. If not then about the 14th of July. Prayers at night."
In 2009, with the threat of bad weather for each of the first three Fridays of the month, Interim-president Wagner declared "Siberian Mountain Day." Festivities were relocated from Stony Ledge to the much more accessible Stone Hill.
Athletics.
The school's athletic teams are called the Ephs (rhymes with "chiefs"), a shortening of the first name of founder Ephraim Williams. The mascot is a Purple Cow. They participate in the NCAA's Division III and the New England Small College Athletic Conference (NESCAC). Williams also competes in skiing and squash at the Division I level. Williams is ranked first among Division III schools for athletic spending per student.
Williams has a traditional rivalry with Amherst College and Wesleyan University. The "Little Three", a subset of NESCAC, comprises the three schools. Williams and Amherst participate in notably intense competition, dating back more than a century. Although Williams College typically sports purple and gold as their school colors, purple is in fact the only school color. The gold was added in order to differentiate its colors from that of rival school Amherst's purple and white uniforms. On May 3, 2009, Williams and Amherst alumni played a game of vintage baseball at Wahconah Park according to 1859-rules to commemorate the 150th-anniversary of the first college baseball game, which was played on July 2, 1859, between the two schools. Amherst-alumnus Dan Duquette was instrumental in organizing the event.
Until 1994, Williams was not permitted, by NESCAC rules, to compete in team NCAA competition. The Williams women's swimming and diving team won the school's first national title in 1981, and claimed the title in 1982 as well. Williams played in the 2003, 2004, and 2010 men's basketball Division III national championship games, winning the title in March 2003. Men's basketball also played in the 1997, 1998 and 2011 Final Fours. Williams was the first New England basketball team to win a Division III championship, and since they have been eligible to compete in the NCAA tournament, no team in the country has played in more Final Fours.
Williams teams to win national titles since Williams began participating in NCAA tournaments in 1994 include women's crew (nine titles, including eight straight from 2006–2013), men's tennis (four), women's tennis (eight, including six straight from 2008–2013), men's cross country (two), women's cross country (two), men's basketball, women's indoor track and field, and men's soccer.
Williams also has had success winning the NACDA Director's Cup, presented to the institution within each NCAA division that has the greatest overall success in NCAA sanctioned-championships. Williams has won the NACDA Director's Cup 16 of the 18 years since its inception, including 13 years in a row from 1999 through 2011.
In 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, and 2013 the college achieved No. 1 rankings in both academics and athletics within its peer groups (liberal arts colleges as ranked by "U.S. News & World Report" and NCAA Division III institutions as ranked by the Director's Cup calculations, respectively). Dual No. 1 rankings in any single year was an unprecedented achievement among the 1,053 NCAA member institutions.
Athletic facilities.
William College has had major updates or renovations of its athletic facilities during the past several decades.
The Lansing Chapman hockey rink, built in 1953 and originally uncovered, was canopied in 1963, enclosed in 1969 and has been periodically upgraded to the present (2014) with rink, roof, locker room and lighting improvements.
The Towne Field House, constructed in 1970, is a multipurpose facility, which includes an indoor track, tennis courts and a climbing wall. The later was initially constructed in 1974 and updated to a state of the art climbing wall in 1995. The field house also accommodates pre-season baseball, softball and lacrosse.
The Lasell Gym built in 1886 was renovated and expanded with the addition of the Chandler Athletic Center in 1987. It provides a state of the art 50-meter swimming pool, a gymnasium primarily for basketball, squash facilities, wrestling rooms, various fitness centers and administrative offices.
In 1987, the Weston Field cinder running track and baseball field were replaced: the Anthony Plansky 400-meter track was built around the refurbished football field and the Bobby Coombs baseball field was re-located at Cole Field. The Renzi Lamb Field for lacrosse and field hockey, built with artificial turf, was added to Weston Field in 2004.
In November 2013 Williams College began its 22 million dollar renovation of the Weston Field complex. This upgrade includes an artificial turf football field, relocation of the Plansky Track and Lamb Field, new bleachers, improved lighting and the addition of support buildings for the athletes. The completed facility, scheduled to reopen in September 2014, will allow year round athletic events and practice.
Non-varsity sports.
Williams also has an active club and intramural sports program, offering 14 club sports including ultimate, rugby, horseback riding, cycling, fencing, volleyball, gymnastics, sailing, and water polo. Approximately 50% of Williams' students compete on at least one varsity, junior varsity, or formal club team.
People.
Student body.
Williams enrolled 2,052 undergraduate students and 54 graduate students in 2012. In 2010, women constituted 51.8% of undergraduate students and 61% percent of graduate students. 50% of students receive need-based financial aid and 409 students (19%) qualify to receive Pell Grants. Williams has a 97% freshman retention rate and a 91% four-year graduation rate. 89% of students graduated in the top tenth of their high school graduating class and the inter-quartile range on the SAT was 670–760 for reading, 670–760 for math, and 660–760 for writing.
Faculty.
Williams has 334 voting faculty, 92% of whom possess a doctorate or the terminal degree in their field. Students fill out course surveys at the end of each semester, which play a large role in determining faculty tenure decisions. Recently, there has been controversy over popular teachers being denied tenure based on other factors, including publication rates. <-- invalid link
Notable former and present faculty include:
Alumni.
As of August, 2013, there are 30,300 living alumni of record, and 70 regional alumni associations nationwide and overseas. Alumni participation in the 2011-12 Alumni Fund was 62.5%. More than 58% of the alumni from the classes of 1980 to 2000 have earned at least one graduate or professional degree. The most popular graduate disciplines for alumni are management, education, law, and health care.
The Society of Alumni of Williams College is the oldest existing alumni society of any academic institution in the United States. The Society of Alumni was founded during the "Amherst crisis" in 1821, when Williams College President Zephaniah Swift Moore left Williams. Graduates of Williams formed the Society to ensure that Williams would not have to close, and raised enough money to ensure the future survival of the school.
Williams-Exeter Programme at Oxford.
Williams has a close relationship with Exeter College, one of the oldest constituent colleges of Oxford University. In the early 1980s, Williams purchased a group of houses, today known as the Ephraim Williams House, on Banbury Road and Lathbury Road, in North Oxford.
The Williams-Exeter Programme at Oxford (WEPO) was founded in 1985. Every year (except 2010–2011, when 24 students attended), 26 undergraduate students from Williams spend their junior year at Exeter as full members of the college.

</doc>
<doc id="34108" url="http://en.wikipedia.org/wiki?curid=34108" title="William Lipscomb">
William Lipscomb

William Nunn Lipscomb, Jr. (December 9, 1919 – April 14, 2011) was a Nobel Prize-winning American inorganic and organic chemist working in nuclear magnetic resonance, theoretical chemistry, boron chemistry, and biochemistry.
Biography.
Overview.
Lipscomb was born in Cleveland, Ohio. His family moved to Lexington, Kentucky in 1920, and he lived there until he received his Bachelor of Science degree in Chemistry at the University of Kentucky in 1941. He went on to earn his Doctor of Philosophy degree in Chemistry from the California Institute of Technology (Caltech) in 1946.
From 1946 to 1959 he taught at the University of Minnesota. From 1959 to 1990 he was a professor of chemistry at Harvard University, where he was a professor emeritus since 1990.
Lipscomb was married to the former Mary Adele Sargent from 1944 to 1983. They had three children, one of whom lived only a few hours.
He married Jean Evans in 1983. They had one adopted daughter.
Lipscomb resided in Cambridge, Massachusetts until his death in 2011 from pneumonia.
Early years.
"My early home environment ... stressed personal responsibility and self reliance. Independence was encouraged especially in the early years when my mother taught music and when my father's medical practice occupied most of his time."
In grade school Lipscomb collected animals, insects, pets, rocks, and minerals.
Interest in astronomy led him to visitor nights at the Observatory of the University of Kentucky, where Prof. H. H. Downing gave him a copy of Baker's "Astronomy."
Lipscomb credits gaining many intuitive physics concepts from this book and from his conversations with Downing, who became Lipscomb's lifelong friend.
The young Lipscomb participated in other projects, such as Morse-coded messages over wires and crystal radio sets, with five nearby friends who became physicists, physicians, and an engineer.
At age of 12, Lipscomb was given a small Gilbert chemistry set,
He expanded it by ordering apparatus and chemicals from suppliers and by using
his father's privilege as a physician to purchase chemicals at the local drugstore at a discount.
Lipscomb made his own fireworks and entertained visitors with color changes, odors, and explosions.
His mother questioned his home chemistry hobby only once, when he attempted to isolate a large amount of urea from urine.
Lipscomb credits perusing the large medical texts in his physician father's library and the influence of Linus Pauling years later to his undertaking biochemical studies in his later years. Had Lipscomb become a physician like his father, he would have been the fourth physician in a row along the Lipscomb male line.
The source for this subsection, except as noted, is Lipscomb's autobiographical sketch.
Education.
Lipscomb's high-school chemistry teacher, Frederick Jones, gave Lipscomb his college books on organic, analytical, and general chemistry, and asked only that Lipscomb take the examinations.
During the class lectures, Lipscomb in the back of the classroom did research that he thought was original (but he later found was not): the preparation of hydrogen from sodium formate (or sodium oxalate) and sodium hydroxide.
He took care to include gas analyses and to search for probable side reactions.
Lipscomb later had a high-school physics course and took first prize in the state contest on that subject. He also became very interested in special relativity.
In college at the University of Kentucky Lipscomb had a music scholarship.
He pursued independent study there, reading Dushman' s "Elements of Quantum Mechanics", the University of Pittsburgh Physics Staff's "An Outline of Atomic Physics", and Pauling's "The Nature of the Chemical Bond and the Structure of Molecules and Crystals."
Prof. Robert H. Baker suggested that Lipscomb research the direct preparation of derivatives of alcohols from dilute aqueous solution without first separating the alcohol and water, which led to Lipscomb's first publication.
For graduate school Lipscomb chose Caltech, which offered him a teaching assistantship in Physics at $20/month. He turned down more money from Northwestern University, which offered a research assistantship at $150/month. Columbia University rejected Lipscomb's application in a letter written by Nobel prizewinner Prof. Harold Urey.
At Caltech Lipscomb intended to study theoretical quantum mechanics with Prof. W. V. Houston in the Physics Department, but after one semester switched to the Chemistry Department under the
influence of Prof. Linus Pauling. World War II work divided Lipscomb's time in graduate school beyond his other thesis work, as he partly analyzed smoke particle size, but mostly worked with nitroglycerin–nitrocellulose propellants, which involved handling vials of pure nitroglycerin on many occasions. 
Brief audio clips by Lipscomb about his war work may be found from the External Links section at the bottom of this page, past the References.
The source for this subsection, except as noted, is Lipscomb's autobiographical sketch.
Later years.
The Colonel is how Lipscomb's students referred to him, directly addressing him as Colonel. "His first doctoral student, Murray Vernon King, pinned the label on him, and it was quickly adopted by other students, who wanted to use an appellation that showed informal respect. ... Lipscomb's Kentucky origins as the rationale for the designation."
Some years later in 1973 Lipscomb was made a member of the Honorable Order of Kentucky Colonels.
Lipscomb, along with several other Nobel laureates, was a regular presenter at the annual Ig Nobel Awards Ceremony, last doing so on September 30, 2010.
Scientific studies.
Lipscomb has worked in three main areas, nuclear magnetic resonance and the chemical shift, boron chemistry and the nature of the chemical bond, and large biochemical molecules. These areas overlap in time and share some scientific techniques.
In at least the first two of these areas Lipscomb gave himself a big challenge likely to fail,
and then plotted a course of intermediate goals.
Nuclear magnetic resonance and the chemical shift.
In this area Lipscomb proposed that:
"... progress in structure determination, for new polyborane species and for substituted boranes and carboranes, would be greatly accelerated if the [boron-11] nuclear magnetic resonance spectra, rather than X-ray diffraction, could be used."
This goal was partially achieved, although X-ray diffraction is still necessary to determine many such atomic structures. The diagram at left shows a typical nuclear magnetic resonance (NMR) spectrum of a borane molecule.
Lipscomb investigated, "... the carboranes, C2B10H12, and the sites of electrophilic attack on these compounds using nuclear magnetic resonance (NMR) spectroscopy. This work led to [Lipscomb's publication of a comprehensive] theory of chemical shifts. The calculations provided the first accurate values for the constants that describe the behavior of several types of molecules in magnetic or electric fields."
Much of this work is summarized in a book by Gareth Eaton and William Lipscomb, "NMR Studies of Boron Hydrides and Related Compounds", one of Lipscomb's two books.
Boron chemistry and the nature of the chemical bond.
In this area Lipscomb originally intended a more ambitious project: "My original intention in the late 1940s was to spend a few years understanding the boranes, and then to discover a systematic valence description of the vast numbers of electron deficient intermetallic compounds. I have made little progress toward this latter objective. Instead, the field of boron chemistry has grown enormously, and a systematic understanding of some of its complexities has now begun."
Examples of these intermetallic compounds are KHg13 and Cu5Zn7. Of perhaps 24,000 of such compounds the structures of only 4,000 are known (in 2005) and we cannot predict structures for the others, because we do not sufficiently understand the nature of the chemical bond.
This study was not successful, in part because the calculation time required for intermetallic compounds was out of reach in the 1960s, but intermediate goals involving boron bonding were achieved, sufficient to be awarded a Nobel Prize.
Lipscomb deduced the molecular structure of boranes (compounds of boron and hydrogen) using X-ray crystallography in the 1950s and developed theories to explain their bonds. Later he applied the same methods to related problems, including the structure of carboranes (compounds of carbon, boron, and hydrogen).
Lipscomb is perhaps best known for his group's proposed mechanism
of the three-center two-electron bond.
The three-center two-electron bond is illustrated in diborane (diagrams at right).
In an ordinary covalent bond a pair of electrons bonds two atoms together, one at either end of the bond, the diboare B-H bonds for example at the left and right in the illustrations.
In three-center two-electron bond a pair of electrons bonds three atoms (a boron atom at either end and a hydrogen atom in the middle), the diborane B-H-B bonds for example at the top and bottom of the illustrations.
Lipscomb's group did not propose or discover the three-center two-electron bond, 
nor did they develop formulas that give the proposed mechanism.
What they did was to use formulas written by others intended for another purpose
to understand the quantum mechanicical details of the three-center two-electron bond.
A trail of credit for the understanding the three-center two-electron bond follows:
Over several decades the structure and bonding arrangement of diborane was gradually discovered by
Dilthey,
Price, and others.
Longuet-Higgins 
and Roberts 
employed a three-center two-electron bond as the correct way to understand bonding in diborane using a molecular orbital description similar to what the Lipscomb group found. Eberhardt, Crawford, and Lipscomb proposed the mechanism
of the three-center two-electron bond, and Lipscomb's group achieved an understanding of it through electron orbital calculations using formulas by Edmiston and Ruedenberg and by Boys.
The Eberhardt, Crawford, and Lipscomb paper discussed above also devised the "styx number" method to catalog certain kinds of boron-hydride bonding configurations.
Wandering atoms was a puzzle solved by Lipscomb in one of his few papers with no co-authors.
Compounds of boron and hydrogen tend to form closed cage structures. 
Sometimes the atoms at the vertices of these cages move substantial distances with respect to each other.
The diamond-square-diamond mechanism (diagram at left) was suggested by Lipscomb to explain this rearrangement of vertices.
Following along in the diagram at left for example in the faces shaded in blue, 
a pair of triangular faces has a left-right diamond shape. 
First, the bond common to these adjacent triangles breaks, forming a square,
and then the square collapses back to an up-down diamond shape
by bonding the atoms that were not bonded before.
Other researchers have discovered more about these rearrangements.
The B10H16 structure (diagram at right) determined by Grimes, Wang, Lewin, and Lipscomb found a bond directly between two boron atoms without terminal hydrogens, a feature not previously seen in other boron hydrides.
Lipscomb's group developed calculation methods, both empirical and from quantum mechanical theory.
Calculations by these methods produced accurate Hartree–Fock self-consistent field (SCF) molecular orbitals and were used to study boranes and carboranes.
The ethane barrier to rotation (diagram at left) was first calculated accurately by Pitzer and Lipscomb using the Hartree–Fock (SCF) method.
Lipscomb's calculations continued to a detailed examination of partial bonding through "... theoretical studies of multicentered chemical bonds including both delocalized and localized molecular orbitals."
This included "... proposed molecular orbital descriptions in which the bonding electrons are delocalized over the whole molecule."
"Lipscomb and his coworkers developed the idea of transferability of atomic properties, by which approximate theories for complex molecules are developed from more exact calculations for simpler but chemically related molecules..."
Subsequent Nobel Prize winner Roald Hoffmann was a doctoral student 
in Lipscomb's laboratory.
Under Lipscomb's direction the Extended Hückel method of molecular orbital calculation was developed by Lawrence Lohr and by Roald Hoffmann. This method was later extended by Hoffman. 
In Lipscomb's laboratory this method was reconciled with self-consistent field (SCF) theory by Newton and by Boer.
Noted boron chemist M. Frederick Hawthorne conducted early and continuing research with Lipscomb.
Much of this work is summarized in a book by Lipscomb, "Boron Hydrides", one of Lipscomb's two books.
The 1976 Nobel Prize in Chemistry was awarded to Lipscomb "for his studies on the structure of boranes illuminating problems of chemical bonding".
In a way this continued work on the nature of the chemical bond by his Doctoral Advisor at the California Institute of Technology, Linus Pauling, who was awarded the 1954 Nobel Prize in Chemistry "for his research into the nature of the chemical bond and its application to the elucidation of the structure of complex substances."
The source for about half of this section is Lipscomb's Nobel Lecture.
Large biological molecule structure and function.
Lipscomb's later research focused on the atomic structure of proteins, particularly how enzymes work.
His group used x-ray diffraction to solve the three-dimensional structure of these proteins to atomic resolution, and then to analyze the atomic detail of how the molecules work.
The images below are of Lipscomb's structures from the Protein Data Bank displayed in simplified form with atomic detail suppressed. Proteins are chains of amino acids, and the continuous ribbon shows the trace of the chain with, for example, several amino acids for each turn of a helix.
Carboxypeptidase A (left) was the first protein structure from Lipscomb's group. Carboxypeptidase A is a digestive enzyme, a protein that digests other proteins. It is made in the pancreas and transported in inactive form to the intestines where it is activated. Carboxypeptidase A digests by chopping off certain amino acids one-by-one from one end of a protein.
The size of this structure was ambitious. Carboxypeptidase A was a much larger molecule than anything solved previously.
Aspartate carbamoyltransferase. (right) was the second protein structure from Lipscomb's group.
For a copy of DNA to be made, a duplicate set of its nucleotides is required. Aspartate carbamoyltransferase performs a step in building the pyrimidine nucleotides (cytosine and thymidine). Aspartate carbamoyltransferase also ensures that just the right amount of pyrimidine nucleotides is available, as activator and inhibitor molecules attach to aspartate carbamoyltransferase to speed it up and to slow it down.
Aspartate carbamoyltransferase is a complex of twelve molecules.
Six large catalytic molecules in the interior do the work, and six small regulatory molecules on the outside control how fast the catalytic units work.
The size of this structure was ambitious. Aspartate carbamoyltransferase was a much larger molecule than anything solved previously.
Leucine aminopeptidase, (left) a little like carboxypeptidase A, chops off certain amino acids one-by-one from one end of a protein or peptide.
HaeIII methyltransferase (right)
binds to DNA where it methylates (adds a methy group to)
it.
Human interferon beta (left)
is released by lymphocytes in response to pathogens to trigger the immune system.
Chorismate mutase (right)
catalyzes (speeds up) the production of the amino acids phenylalanine and tyrosine.
Fructose-1,6-bisphosphatase (left)
and its inhibitor MB06322 (CS-917) 
were studied by Lipscomb's group in a collaboration, which included Metabasis Therapeutics, Inc., acquired by Ligand Pharmaceuticals in 2010, exploring the possibility of finding a treatment for type 2 diabetes, as the MB06322 inhibitor slows the production of sugar by fructose-1,6-bisphosphatase.
Lipscomb's group also contributed to an understanding of
concanavalin A (low resolution structure),
glucagon, and
carbonic anhydrase (theoretical studies).
Subsequent Nobel Prize winner Thomas A. Steitz
was a doctoral student in Lipscomb's laboratory.
Under Lipscomb's direction, after the training task of determining the structure of the small molecule methyl ethylene phosphate, Steitz made contributions to determining the atomic structures of carboxypeptidase A
and aspartate carbamoyltransferase.
Steitz was awarded the 2009 Nobel Prize in Chemistry for determining the even larger structure of the large 50S ribosomal subunit, leading to an understanding of possible medical treatments.
Subsequent Nobel Prize winner Ada Yonath, who shared the 2009 Nobel Prize in Chemistry with Thomas A. Steitz and Venkatraman Ramakrishnan, spent some time in Lipscomb's lab where both she and Steitz were inspired to pursue later their own very large structures. This was while she was a postdoctoral student at MIT in 1970.
Other results.
The mineral lipscombite (picture at right) was named after Professor Lipscomb by the mineralogist John Gruner who first made it artificially.
Low-temperature x-ray diffraction was pioneered in Lipscomb's laboratory at about the same time as parallel work in Isadore Fankuchen's laboratory at the then Polytechnic Institute of Brooklyn.
Lipscomb began by studying compounds of nitrogen, oxygen, fluorine, and other substances that are solid only below liquid nitrogen temperatures, but other advantages eventually made low-temperatures a normal procedure.
Keeping the crystal cold during data collection produces a less-blurry 3-D electron-density map because the atoms have less thermal motion. Crystals may yield good data in the x-ray beam longer because x-ray damage may be reduced during data collection and because the solvent may evaporate more slowly, which for example may be important for large biochemical molecules whose crystals often have a high percentage of water.
Other important compounds were studied by Lipscomb and his students.
Among these are
hydrazine, 
nitric oxide, 
metal-dithiolene complexes, 
methyl ethylene phosphate, 
mercury amides, 
(NO)2, 
crystalline hydrogen fluoride, 
Roussin's black salt,
(PCF3)5,
complexes of cyclo-octatetraene with iron tricarbonyl,
and leurocristine (Vincristine), which is used in several cancer therapies.
Positions, awards and honors.
Five books and published symposia are dedicated to Lipscomb.
A complete list of Lipscomb's awards and honors is in his Curriculum Vitae.

</doc>
<doc id="34110" url="http://en.wikipedia.org/wiki?curid=34110" title="Worldwatch Institute">
Worldwatch Institute

The Worldwatch Institute is a globally focused environmental research organization based in Washington, D.C. Worldwatch was named as one of the top ten sustainable development research organizations by Globescan Survey of Sustainability Experts.
Mission.
The mission of the Institute reads: 
Through research and outreach that inspire action, the Worldwatch Institute works to accelerate the transition to a sustainable world that meets human needs. The institute's top mission objectives are universal access to renewable energy and nutritious food, expansion of environmentally sound jobs and development, transformation of cultures from consumerism to sustainability, and an early end to population growth through healthy and intentional childbearing. 
The Worldwatch Institute aims to inform policymakers and the public about the links between the world economy and its environmental support systems. Research conducted by the institute is integrative or interdisciplinary and global in scope. 
Worldwatch’s priority programs include: 
Worldwatch also monitors human health, population, water resources, biodiversity, governance, and environmental security.
 relies on the generosity of its donors.
Publications.
Worldwatch Institute publications have been published in more than three dozen languages by its global partners in 40 countries. Worldwatch publications include:

</doc>
<doc id="34113" url="http://en.wikipedia.org/wiki?curid=34113" title="Wendy Carlos">
Wendy Carlos

Wendy Carlos (born November 14, 1939) is an American composer and electronic musician.
Carlos first came to prominence in 1968 with "Switched-On Bach", a recording of music by J.S. Bach assembled phrase-by-phrase on a Moog synthesizer, at the time a relatively new and unknown instrument. The album earned three Grammy Awards in 1969. Other classical recordings followed.
Carlos later began releasing original compositions, including the first-ever album of synthesized environmental sounds, "Sonic Seasonings" (1972) and an album exploring alternate tunings "Beauty in the Beast" (1986). She has also worked in film music, notably writing and performing scores for two Stanley Kubrick movies, "A Clockwork Orange" (1971) and "The Shining" (1980), as well as Walt Disney's "Tron" (1982).
Career.
Carlos was a musical prodigy who started piano lessons at six and at ten composed "A Trio for Clarinet, Accordion, and Piano". In 1953 (aged 14), Carlos won a Westinghouse Science Fair scholarship for a home-built computer, well before "computer" was a household word. After graduating from St. Raphael Academy, a Catholic high school in Pawtucket, RI, Carlos earned a B.A. in music and physics at Brown University (1962) and an M.A. in music from Columbia University (1965). Carlos studied with Vladimir Ussachevsky, a pioneer in electronic music, as well as Otto Luening and Jack Beeson, working in the famed Columbia-Princeton Electronic Music Center.
Carlos met Dr. Robert Moog at the 1963 Audio Engineering Society show and became one of his earliest customers, providing advice and technical assistance for his further development of the Moog synthesizer. Carlos convinced Moog to add touch sensitivity to the synthesizer keyboard for greater dynamics and musicality, among other improvements.
Around 1966, Carlos met Rachel Elkind, who went on to produce "Switched-On Bach" and other early albums. With the proceeds from "Switched-On Bach", the two renovated a New York brownstone, which they shared as a home and business premises, installing a studio for live and electronic recording on the bottom floor where all subsequent recordings have been produced. Carlos took the unusual step of enclosing the entire studio in a Faraday cage, shielding the equipment from radio and television interference.
Carlos contributed a review of the then-available synthesizers to the June 1971 edition of the Whole Earth Catalog, contrasting the Moog, Buchla and Tonus (aka ARP) systems. She was dismissive of smaller systems like the EMS "Putney" and the Minimoog as "toys" and "cash-ins".
Carlos is also an accomplished solar eclipse photographer.
Work.
In addition to the aforementioned "Trio for Clarinet, Accordion and Piano", Carlos composed numerous student works. Two which saw later release (on 1975's "By Request") are "Dialogues for Piano and Two Loudspeakers" (1963) and "Episodes for Piano and Electronic Sound" (1964). Others include "Variations for Flute and Electronic Sounds" (1964), "Episodes for Piano and Tape" (1964), "Pomposities for Narrator and Tape" (1965), and "Noah" (1965), an opera blending electronics and normal orchestra. Her first commercial release was "Moog 900 Series – Electronic Music Systems" (R. A. Moog Company, Inc., 1967), an introduction to the technical aspects of the Moog synthesizer, part of her compensation for this recording was in Moog equipment.
"Switched-On Bach" (1968) was Carlos' break-through album, one of the first to draw attention to the synthesizer as a genuine musical instrument. Multitrack recording techniques played a critical role in the time-consuming process of creating this album, when it was significantly more difficult than it is today. "Switched-On Bach" was the last project in a four-year-long collaboration with Benjamin Folkman and won gold records for both Carlos and Folkman. The album then became one of the first classical LPs to sell 500,000 copies, going gold in August 1969 and platinum in November 1986. It remained at the top spot on the "Billboard Magazine" classical album chart for two years and 49 weeks.
A sequel of additional synthesized baroque music, "The Well-Tempered Synthesizer", followed in 1969. The title is a play on Bach's "Well-Tempered Clavier" (1722). A second sequel, "Switched-On Bach II", was released in 1973, continuing the style of the previous two albums and adding a Yamaha Electone organ to the Moog for certain passages in Bach's 5th "Brandenburg Concerto".
In 1971, Carlos composed and recorded music for the soundtrack of "A Clockwork Orange", directed by Stanley Kubrick and based on the 1962 eponymous novel by Anthony Burgess. Additional music not used in the film was released in 1972 as "Walter Carlos' Clockwork Orange". Some portions of her work for this film reappeared in her "Tales of Heaven and Hell" (2003), in movement 3 "A Clockwork Black".
She worked with Kubrick again on the score for "The Shining" (1980). While in the end Kubrick mostly used the pre-existing music by avant-garde composers he had used as guide tracks, Carlos' contribution was notable for her reinterpretation of Berlioz's Symphonie fantastique used during the opening scene. Carlos's complete contributions were finally released 25 years later, in 2005.
"Sonic Seasonings" (1972) was packaged as a double album, with one side dedicated to each of the four seasons and each side consisting of one long track. The album blended field recordings with synthesized sounds, occasionally employing melodies, to create an ambient effect. Though not as popular as Carlos' earlier albums, it significantly influenced other artists who went on to create the ambient genre.
In 1982, she scored the film "Tron" for The Walt Disney Company. This score incorporated orchestra, chorus, organ, and both analog and digital synthesizers. Some of her end-title music featuring the Royal Albert Hall Organ was replaced with a song by Journey, and the music originally composed for the lightcycle scene was dropped. "Digital Moonscapes" (1984) switched to digital synthesizers from the analog synthesizers that were the trademark of her earlier albums. Some of the unused material from the "Tron" soundtrack was incorporated into it.
"Beauty in the Beast" (1986) saw Carlos experimenting with various tunings, including just intonation, Balinese scales, and several scales she invented for the album. (One scale she invented, the Harmonic Scale, involved setting a "root note" and retuning all of the notes on the keyboard to just intonation intervals from the root note. There are a total of 144 possible notes per octave in this system: 12 notes in a chromatic scale times 12 different keys.) Other scales included Carlos' Alpha, Beta, and Gamma scales, which experimented with dividing the octave into a non-integral number of equally-spaced intervals. These explorations in effect supplemented the more systematic microtonal studies of the composer Easley Blackwood, Jr., whose etudes on all 12 equal-tempered scales between 13 and 24 notes per octave had appeared in 1980.
"Secrets of Synthesis" (1987) is a lecture by Carlos with audio examples (many from her own recordings), expounding on topics she feels to be of importance. Some of the material is an introduction to synthesis, and some (e.g., a discussion of hocket) is aimed at experienced musicians. This release harkens back to "The Nonesuch Guide to Electronic Music" (1967) by Beaver & Krause, released some 20 years earlier.
Beginning in 1998, all of her catalog was digitally remastered by Carlos herself, requiring that she retrieve and in some cases purchase her masters from Columbia Records. In 2005, the two-volume set "Rediscovering Lost Scores" was released, featuring previously out-of-print material, including the unreleased soundtrack to "Woundings", and music composed and recorded for "The Shining", "Tron", and "A Clockwork Orange" that was not used in the films. These reissues have since gone out-of-print because of changes to the music business involving East Side Digital, a music publisher.
Personal life.
Gender transition.
Carlos became aware of her gender dysphoria from an early age; she told "Playboy", "I was about five or six...I remember being convinced I was a little girl, much preferring long hair and girls' clothes, and not knowing why my parents didn't see it clearly". In 1962 (aged 22), when she moved to New York City to attend graduate school at Columbia University, she came into contact for the first time with information about transgender issues (including the work of Harry Benjamin). In early 1968, she began hormone treatments and soon began living full-time as a woman. In her "Whole Earth Catalog" review of synthesizers (1971), Carlos asked to be credited simply as "W. Carlos". After the success of "Switched-On Bach", in May 1972, Carlos was finally able to undergo sex reassignment surgery.
Carlos chose to announce herself as the featured interview in May 1979's "Playboy" magazine, picking "Playboy" because, "The magazine has always been concerned with liberation, and I'm anxious to liberate myself". She has since come to regret the interview, has created a "Shortlist Of The Cruel" page on her website, and gave "Playboy"‍ '​s editors three "Black Leaf" awards, meaning "Arrogant selfish prig, with a genuine sadistic streak".
Carlos prefers not to discuss her transition and has asked that her privacy regarding the subject be respected.
Lawsuit.
In 1998, Carlos sued the songwriter/artist Momus for $22 million for his satirical song "Walter Carlos" (which appeared on the album "The Little Red Songbook", released that year), which suggested that if Wendy could go back in time she could marry Walter. The case was settled out of court, with Momus agreeing to remove the song from subsequent editions of the CD and owing $30,000 in legal fees.
Awards and honors.
"Switched-On Bach" was the winner of three 1969 Grammy Awards: 
In 2005, Carlos was the recipient of the SEAMUS Lifetime Achievement Award "in recognition of lifetime achievement and contribution to the art and craft of electro-acoustic music" by the Society for Electro-Acoustic Music in the United States.

</doc>
<doc id="34116" url="http://en.wikipedia.org/wiki?curid=34116" title="W3m">
W3m

w3m is a free software/open source text-based web browser. It has support for tables, frames, SSL connections, color and inline images on suitable terminals. Generally, it renders pages in a form as true to their original layout as possible.
The name "w3m" stands for ""WWW wo miru" (WWWを見る)", which is Japanese for "to see the WWW" where W3 is a numeronym of WWW.
In Emacs.
w3m is also used by the Emacs text editor via the "w3m.el" Emacs Lisp module. This module gives fast browsing of web pages inside of Emacs. However, rendering of web pages isn't done in Emacs Lisp; only final display is handled in Emacs Lisp with the rendering done by the w3m application. (There exist other web browsers for Emacs, such as Emacs/W3, which is implemented entirely in Emacs Lisp, and EWW, which performs parsing using an external library written in C but all formatting and display in Emacs Lisp.)
Forks.
Two forks of w3m add support for multiple character-encodings and for other features not in the original:

</doc>
