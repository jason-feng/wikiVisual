<doc id="41234" url="http://en.wikipedia.org/wiki?curid=41234" title="Heterodyne">
Heterodyne

Heterodyning is a radio signal processing technique invented in 1901 by Canadian inventor-engineer Reginald Fessenden, in which new frequencies are created by combining or mixing two frequencies. Heterodyning is useful for frequency shifting signals into a new frequency range, and is also involved in the processes of modulation and demodulation. The two frequencies are combined in a nonlinear signal-processing device such as a vacuum tube, transistor, or diode, usually called a "mixer". In the most common application, two signals at frequencies "f1" and "f2" are mixed, creating two new signals, one at the sum "f1" + "f2" of the two frequencies, and the other at the difference "f1" − "f2". These new frequencies are called heterodynes. Typically only one of the new frequencies is desired, and the other signal is filtered out of the output of the mixer. Heterodynes are related to the phenomenon of "beats" in acoustics.
A major application of the heterodyne process is in the superheterodyne radio receiver circuit, which is used in virtually all modern radio receivers.
History.
In 1901, Reginald Fessenden demonstrated a direct-conversion heterodyne receiver or beat receiver as a method of making continuous wave radiotelegraphy signals audible. Fessenden's receiver did not see much application because of its local oscillator's stability problem. While complex isochronous electromechanical oscillators existed, a stable yet inexpensive local oscillator would not be available until Lee de Forest's invention of the triode vacuum tube oscillator. In a 1905 patent, Fessenden stated the frequency stability of his local oscillator was one part per thousand.
Early spark gap radio transmitters sent information exclusively by means of radio telegraphy. In radio telegraphy, the characters of text messages are translated into the short duration dots and long duration dashes of Morse code that are broadcast as bursts of radio waves. The heterodyne detector was not needed to hear the signals produced by these spark gap transmitters. The transmitted damped wave signals were amplitude modulated at an audio frequency by the spark. A simple detector produced an audible buzzing sound in the radiotelegraph operator's headphones that could be transcribed back into alpha-numeric characters.
With the advent of the arc converter, continuous wave (CW) transmitters were adopted. CW Morse code signals are not amplitude modulated, so a different detector was needed. The direct-conversion detector was invented to make continuous wave radio-frequency signals audible.
The "heterodyne" or "beat" receiver has a local beat frequency oscillator (BFO) that produces a radio signal adjusted to be close in frequency to the incoming signal being received. When the two signals are mixed, a "beat" frequency equal to the difference between the two frequencies is created. By adjusting the local oscillator frequency correctly, the beat frequency is in the audio range, and can be heard as a tone in the receiver's earphones whenever the transmitter signal is present. Thus the Morse code "dots" and "dashes" are audible as beeping sounds. This technique is still used in radio telegraphy, the local oscillator now being called the beat frequency oscillator or BFO. Fessenden coined the word "heterodyne" from the Greek roots "hetero-" "different", and "dyn-" "power" (cf. ).
Superheterodyne receiver.
The most important and widely used application of the heterodyne technique is in the superheterodyne receiver (superhet), invented by U.S. engineer Edwin Howard Armstrong in 1918. In this circuit, the incoming radio frequency signal from the antenna is mixed with a signal from a local oscillator and converted by the heterodyne technique to a somewhat lower fixed frequency signal called the intermediate frequency (IF). This IF signal is amplified and filtered, before being applied to a detector which extracts the audio signal, which is sent to the loudspeaker.
The advantage of this technique is that the different frequencies of the different stations received are all converted to the "same" IF before amplification and filtering. The complicated amplifier and bandpass filter stages, which in previous receivers had to be made tunable to work at the different station frequencies, in the superheterodyne can be built to work at one fixed frequency, the IF, simplifying their design. Another advantage is that the IF is at a considerably lower frequency than the RF frequency of the incoming radio signal.
The superior superheterodyne system replaced the earlier TRF and regenerative receiver designs, and since the 1930s almost all commercial radio receivers have been superheterodynes.
Applications.
Heterodyning, also called "frequency conversion", is used very widely in communications engineering to generate new frequencies and move information from one frequency channel to another. Besides its use in the superheterodyne circuit which is found in almost all radio and television receivers, it is used in radio transmitters, modems, satellite communications and set-top boxes, radar, radio telescopes, telemetry systems, cell phones, cable television converter boxes and headends, microwave relays, metal detectors, atomic clocks, and military electronic countermeasures (jamming) systems.
Up and down converters.
In large scale telecommunication networks such as telephone network trunks, microwave relay networks, cable television systems, and communication satellite links, large bandwidth capacity links are shared by many individual communication channels by using heterodyning to move the frequency of the individual signals up to different frequencies, which share the channel. This is called frequency division multiplexing (FDM).
For example, a coaxial cable used by a cable television system can carry 500 television channels at the same time because each one is given a different frequency, so they don't interfere with one another. At the cable source or headend, electronic upconverters convert each incoming television channel to a new, higher frequency. They do this by mixing the television signal frequency, "fCH" with a local oscillator at a much higher frequency "fLO", creating a heterodyne at the sum "fCH"+"fLO", which is added to the cable. At the consumer's home, the cable set top box has a downconverter that mixes the incoming signal at frequency "fCH"+"fLO" with the same local oscillator frequency "fLO" creating the difference heterodyne, converting the television channel back to its original frequency: ("fCH"+"fLO") − "fLO" = "fCH". Each channel is moved to a different higher frequency. The original lower basic frequency of the signal is called the baseband, while the higher channel it is moved to is called the passband.
Analog videotape recording.
Many analog videotape systems rely on a downconverted color subcarrier in order to record color information in their limited bandwidth. These systems are referred to as "heterodyne systems" or "color-under systems". For instance, for NTSC video systems, the VHS (and S-VHS) recording system converts the color subcarrier from the NTSC standard 3.58 MHz to ~629 kHz. PAL VHS color subcarrier is similarly downconverted (but from 4.43 MHz). The now-obsolete 3/4" U-matic systems use a heterodyned ~688 kHz subcarrier for NTSC recordings (as does Sony's Betamax, which is at its basis a 1/2″ consumer version of U-matic), while PAL U-matic decks came in two mutually incompatible varieties, with different subcarrier frequencies, known as Hi-Band and Low-Band. Other videotape formats with heterodyne color systems include Video-8 and Hi8.
The heterodyne system in these cases is used to convert quadrature phase-encoded and amplitude modulated sine waves from the broadcast frequencies to frequencies recordable in less than 1 MHz bandwidth. On playback, the recorded color information is heterodyned back to the standard subcarrier frequencies for display on televisions and for interchange with other standard video equipment.
Some U-matic (3/4″) decks feature 7-pin mini-DIN connectors to allow dubbing of tapes without a heterodyne up-conversion and down-conversion, as do some industrial VHS, S-VHS, and Hi8 recorders.
Music synthesis.
The theremin, an electronic musical instrument, traditionally uses the heterodyne principle to produce a variable audio frequency in response to the movement of the musician's hands in the vicinity of one or more antennas, which act as capacitor plates. The output of a fixed radio frequency oscillator is mixed with that of an oscillator whose frequency is affected by the variable capacitance between the antenna and the thereminist as that person moves her or his hand near the pitch control antenna. The difference between the two oscillator frequencies produces a tone in the audio range.
The ring modulator is a type of heterodyne incorporated into some synthesizers or used as a stand-alone audio effect.
Optical heterodyning.
Optical heterodyne detection (an area of active research) is an extension of the heterodyning technique to higher (visible) frequencies. This technique could greatly improve optical modulators, increasing the density of information carried by optical fibers. It is also being applied in the creation of more accurate atomic clocks based on directly measuring the frequency of a laser beam. See NIST subtopic 9.07.9-4.R for a description of research on one system to do this.
Since optical frequencies are far beyond the manipulation capacity of any feasible electronic circuit, all photon detectors are inherently energy detectors not oscillating electric field detectors. However, since energy detection is inherently "square-law" detection, it intrinsically mixes any optical frequencies present on the detector. Thus, sensitive detection of specific optical frequencies necessitates optical heterodyne detection, in which two different (close-by) wavelengths of light illuminate the detector so that the oscillating electrical output corresponds to the difference between their frequencies. This allows extremely narrow band detection (much narrower than any possible color filter can achieve) as well as precision measurements of phase and frequency of a light signal relative to a reference light source, as in a laser Doppler vibrometer.
This phase sensitive detection has been applied for Doppler measurements of wind speed, and imaging through dense media. The high sensitivity against background light is especially useful for lidar.
In optical Kerr effect (OKE) spectroscopy, optical heterodyning of the OKE signal and a small part of the probe signal produces a mixed signal consisting of probe, heterodyne OKE-probe and homodyne OKE signal. The probe and homodyne OKE signals can be filtered out, leaving the heterodyne signal for detection.
Mathematical principle.
Heterodyning is based on the trigonometric identity:
The product on the left hand side represents the multiplication ("mixing") of a sine wave with another sine wave. The right hand side shows that the resulting signal is the difference of two sinusoidal terms, one at the sum of the two original frequencies, and one at the difference, which can be considered to be separate signals.
Using this trigonometric identity, the result of multiplying two sine wave signals, formula_2 and formula_3 can be calculated:
The result is the sum of two sinusoidal signals, one at the sum "f1" + "f2" and one at the difference "f1" - "f2" of the original frequencies
Mixer.
The two signals are combined in a device called a "mixer". It can be seen from the previous section that an ideal mixer would be a device that multiplies the two signals. Some widely used mixer circuits, such as the Gilbert cell, operate in this way, but they are limited to lower frequencies. However, any "nonlinear" electronic component will also multiply signals applied to it, producing heterodyne frequencies in its output, so a variety of nonlinear components are used as mixers. A nonlinear component is one in which the output current or voltage is a nonlinear function of its input. Most circuit elements in communications circuits are designed to be linear. This means they obey the superposition principle; if "F(v)" is the output of a linear element with an input of "v":
So if two sine wave signals at frequencies "f"1 and "f"2 are applied to a linear device, the output is simply the sum of the outputs when the two signals are applied separately with no product terms. So the function "F" must be nonlinear to create heterodynes (mixer products). A perfect multiplier only produces mixer products at the sum and difference frequencies ("f"1±"f"2), but more general nonlinear functions produce higher order mixer products: "n"⋅"f"1+"m"⋅"f"2 for integers "n" and "m". Some mixer designs, such as double-balanced mixers, suppress some high order undesired products, while other designs, such as harmonic mixers exploit high order differences.
Examples of nonlinear components that are used as mixers are vacuum tubes and transistors biased near cutoff (class C), and diodes. Ferromagnetic core inductors driven into saturation can also be used at lower frequencies. In nonlinear optics, crystals that have nonlinear characteristics are used to mix laser light beams to create heterodynes at optical frequencies.
Output of a mixer.
To demonstrate mathematically how a nonlinear component can multiply signals and generate heterodyne frequencies, the nonlinear function "F" can be expanded in a power series (MacLaurin series):
To simplify the math, the higher order terms above "α2" will be indicated by an ellipsis (". . .") and only the first terms will be shown. Applying the two sine waves at frequencies "ω1" = 2π"f1" and "ω2" = 2π"f2" to this device:
It can be seen that the second term above contains a product of the two sine waves. Simplifying with trigonometric identities:
So the output contains sinusoidal terms with frequencies at the sum "ω1" + "ω2" and difference "ω1" - "ω2" of the two original frequencies. It also contains terms at the original frequencies and at multiples of the original frequencies 2"ω1", 2"ω2", 3"ω1", 3"ω2", etc.; the latter are called harmonics, as well as more complicated terms at frequencies of M"ω1" + N"ω2", called intermodulation products. These unwanted frequencies, along with the unwanted heterodyne frequency, must be filtered out of the mixer output by an electronic filter to leave the desired heterodyne.

</doc>
<doc id="41236" url="http://en.wikipedia.org/wiki?curid=41236" title="Heuristic routing">
Heuristic routing

Heuristic is an adjective used in relation to methods of learning, discovery, or problem solving. 
Routing is the process of selecting paths to specific destinations. 
 According to Schuster (1974):
The heuristic approach to problem solving consists of applying human intelligence, experience, common sense and certain rules of thumb (or heuristics) to develop an acceptable, but not necessarily an optimum, solution to a problem. Of course, determining what constitutes an acceptable solution is part of the task of deciding which approach to use; but broadly defined, an acceptable solution is one that is both reasonably good (close to optimum) and derived within reasonable effort, time, and cost constraints. Often the effort (manpower, computer, and other resources) required, the time limits on when the solution is needed, and the cost to compile, process, and analyze all the data required for deterministic or other complicated procedures preclude their usefulness or favor the faster, simpler heuristic approach. 
Thus, the heuristic approach is generally used when deterministic techniques or are not available, economical, or practical. (p.9)
Heuristic routing is a system used to describe how data is delivered when problems in a network topology arise. Heuristic routing is achieved using specific algorithms to determine the best, although not always optimal, path to a destination. When an interruption in a network topology occurs, the software running on the networking electronics calculates another route to the desired destination via an alternate available path. 
Heuristic routing is also used for vehicular traffic using the highway and transportation network of the world, but that is beyond the scope of this article. 
Heuristic routing: Routing in which data, such as time delay, extracted from incoming messages, during specified periods and over different routes, are used to determine the optimum routing for transmitting data back to the sources. 
Note: Heuristic routing allows a measure of route optimization based on recent empirical knowledge of the state of the network.
IP routing.
The routing protocols in use today are based on one of two algorithms: Distance Vector or Link State. Distance Vector algorithms broadcast routing information to all neighboring routers. Link State routing protocols build a topographical map of the entire network based on updates from neighbor routers, and then use the Dijkstra algorithm to compute the shortest path to each destination.
Metrics used are based on the number of hops, delay, throughput, traffic, and reliability. 
Distance vector
RIP uses number of hops, or gateways traversed, as its metric. 
IGRP uses bandwidth, delay, hop count, link reliability, load, and MTU.
EIGRP uses the (DUAL) Diffusing Update Algorithm. 
BGP uses the Distance Vector algorithm
Link state
OSPF uses the Dijkstra algorithm.
References.
 This article incorporates public domain material from websites or documents of the .
See also.
Heuristic algorithm
Ford–Fulkerson algorithm
Bellman–Ford algorithm

</doc>
<doc id="41237" url="http://en.wikipedia.org/wiki?curid=41237" title="Hierarchical routing">
Hierarchical routing

Hierarchical routing is a method of routing in networks that is based on hierarchical addressing.
Background.
Most Transmission Control Protocol/Internet Protocol (TCP/IP) routing is based on a two-level hierarchical routing in which an IP address is divided into a network portion and a host portion. Gateways use only the network portion until an IP datagram reaches a gateway that can deliver it directly. Additional levels of hierarchical routing are introduced by the addition of subnetworks.
Description.
Hierarchical routing is the procedure of arranging routers in a hierarchical manner. A good example would be to consider a corporate intranet. Most corporate intranets consist of a high speed backbone network. Connected to this backbone are routers which are in turn connected to a particular workgroup. These workgroups occupy a unique LAN. The reason this is a good arrangement is because even though there might be dozens of different workgroups, the span (maximum hop count to get from one host to any other host on the network) is 2. Even if the workgroups divided their LAN network into smaller partitions, the span could only increase to 4 in this particular example. 
Considering alternative solutions with every router connected to every other router, or if every router was connected to 2 routers, shows the convenience of hierarchical routing. It decreases the complexity of network topology, increases routing efficiency, and causes much less congestion because of fewer routing advertisements. With hierarchical routing, only core routers connected to the backbone are aware of all routes. Routers that lie within a LAN only know about routes in the LAN. Unrecognized destinations are passed to the default route.

</doc>
<doc id="41239" url="http://en.wikipedia.org/wiki?curid=41239" title="High-performance equipment">
High-performance equipment

High-performance equipment describes telecommunications equipment that
"Note:" Requirements for global and tactical high-performance equipment may differ.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41240" url="http://en.wikipedia.org/wiki?curid=41240" title="Hop">
Hop

Hop or hops usually refers to a kind of small jump, usually using only one leg can also be on two.
It can also refer to:

</doc>
<doc id="41242" url="http://en.wikipedia.org/wiki?curid=41242" title="Horn">
Horn

Horn or Horns may refer to:

</doc>
<doc id="41243" url="http://en.wikipedia.org/wiki?curid=41243" title="Hotline">
Hotline

A hotline is a point-to-point communications link in which a call is automatically directed to the preselected destination without any additional action by the user when the end instrument goes off-hook. An example would be a phone that automatically connects to emergency services on picking up the receiver. Therefore, dedicated hotline phones do not need a rotary dial or keypad. A hotline can also be called an automatic signaling, ringdown, or off-hook service.
For crises and service.
True hotlines cannot be used to originate calls other than to preselected destinations. However, in common or colloquial usage, a "hotline" often refers to a call center reachable by dialing a standard telephone number, or sometimes the phone numbers themselves.
This is especially the case with 24-hour, noncommercial numbers, such as police tip hotlines or suicide crisis hotlines, which are manned around the clock and thereby give the appearance of real hotlines. Increasingly, however, the term is found being applied to any customer service telephone number.
Between states.
USA–Russia.
The most famous hotline between states is the Moscow–Washington hotline, which is also known as the "red telephone", although telephones have never been used in this capacity. This direct communications link was established on 20 June 1963, in the wake of the Cuban Missile Crisis, and utilized teletypewriter technology, later replaced by telecopier and then by electronic mail.
USA–UK.
Already during World War II—two decades before the hotline Washington–Moscow was established—there was a hotline between No. 10 Downing Street and the Cabinet War Room bunker under the Treasury, Whitehall; with the White House in Washington. From 1943–1946, this link was made secure by using the very first voice encryption machine, called SIGSALY.
Russia–China.
A hotline connection between Moscow and Beijing was used during the 1969 frontier confrontation between the two countries. The Chinese however refused the Russian peace attempts and ended the communications link. After a reconciliation between the former enemies, the hotline between China and Russia was revived in 1996.
Russia–France.
On his visit to the Soviet Union in 1966, French President Charles de Gaulle announced that a hotline would be established between Paris and Moscow. The line was upgraded from a telex to a high-speed fax machine in 1989.
Russia–UK.
A London–Moscow hotline was not formally established until a treaty of friendship between the two countries in 1992. An upgrade was announced when Foreign Secretary William Hague visited Moscow in 2011.
India–Pakistan.
On 20 June 2004, both India and Pakistan agreed to extend a nuclear testing ban and to set up a hotline between their foreign secretaries aimed at preventing misunderstandings that might lead to nuclear war. The hotline was set up with the assistance of United States military officers.
USA–China.
The United States and China set up a defense hotline in 2008, but it has rarely been used in crises.
China–India.
India and China announced a hotline for the foreign ministers of both countries while reiterating their commitment to strengthening ties and building "mutual political trust".
China–Japan.
In February 2013, the Senkaku Islands dispute gave renewed impetus to a China–Japan hotline, which had been agreed to but due to rising tensions had not been established.
North and South Korea.
The Seoul–Pyongyang hotline was opened on 18 August 1972 and maintained by the Red Cross. North Korea deactivated the hotline on 11 March 2013, as part of increasing tensions on the Korean peninsula.

</doc>
<doc id="41244" url="http://en.wikipedia.org/wiki?curid=41244" title="Hybrid (biology)">
Hybrid (biology)

In biology a hybrid is mix of two animals or plants of different breeds, varieties, species or genera. Using genetics terminology, it may be defined as follows.
From a taxonomic perspective, hybrid refers to:
Etymology.
According to the Oxford English Dictionary, the word is derived from Latin "hybrida", meaning the "offspring of a tame sow and a wild boar", "child of a freeman and slave", etc. The term entered into popular use in English in the 19th century, though examples of its use have been found from the early 17th century.
Types of hybrids.
Depending on the parents, there are a number of different types of hybrids;
Interspecific hybrids.
Interspecific hybrids are bred by mating two species, normally from within the same genus. The offspring display traits and characteristics of both parents. The offspring of an interspecific cross are very often sterile; thus, hybrid sterility prevents the movement of genes from one species to the other, keeping both species distinct. Sterility is often attributed to the different number of chromosomes the two species have, for example donkeys have 62 chromosomes, while horses have 64 chromosomes, and mules and hinnies have 63 chromosomes. Mules, hinnies, and other normally sterile interspecific hybrids cannot produce viable gametes, because differences in chromosome structure prevent appropriate pairing and segregation during meiosis, meiosis is disrupted, and viable sperm and eggs are not formed. However, fertility in female mules has been reported with a donkey as the father.
Most often other processes occurring in plants and animals keep gametic isolation and species distinction. Species often have different mating or courtship patterns or behaviors, the breeding seasons may be distinct and even if mating does occur antigenic reactions to the sperm of other species prevent fertilization or embryo development. Hybridisation is much more common among organisms that spawn indiscriminately, like soft corals and among plants.
While it is possible to predict the genetic composition of a backcross "on average", it is not possible to accurately predict the composition of a particular backcrossed individual, due to random segregation of chromosomes. In a species with two pairs of chromosomes, a twice backcrossed individual would be predicted to contain 12.5% of one species' genome (say, species A). However, it may, in fact, still be a 50% hybrid if the chromosomes from species A were lucky in two successive segregations, and meiotic crossovers happened near the telomeres. The chance of this is fairly high: formula_1 (where the "two times two" comes about from two rounds of meiosis with two chromosomes); however, this probability declines markedly with chromosome number and so the actual composition of a hybrid will be increasingly closer to the predicted composition.
Hybrids are often named by the portmanteau method, combining the names of the two parent species. For example, a zeedonk is a cross between a zebra and a donkey. Since the traits of hybrid offspring often vary depending on which species was mother and which was father, it is traditional to use the father's species as the first half of the portmanteau. For example, a liger is a cross between a male lion and a female tiger, while a tigon is a cross between a male tiger and a female lion.
Domestic and wild hybrids.
Hybrids between domesticated and wild animals in particular may be problematic. Breeders of domesticated species discourage crossbreeding with wild species, unless a deliberate decision is made to incorporate a trait of a wild ancestor back into a given breed or strain. Wild populations of animals and plants have evolved naturally over millions of years through a process of natural selection in contrast to human controlled selective breeding or artificial selection for desirable traits from the human point of view. Normally, these two methods of reproduction operate independently of one another. However, an intermediate form of selective breeding, wherein animals or plants are bred by humans, but with an eye to adaptation to natural region-specific conditions and an acceptance of natural selection to weed out undesirable traits, created many ancient domesticated breeds or types now known as landraces.
Many times, domesticated species live in or near areas which also still hold naturally evolved, region-specific wild ancestor species and subspecies. In some cases, a domesticated species of plant or animal may become feral, living wild. Other times, a wild species will come into an area inhabited by a domesticated species. Some of these situations lead to the creation of hybridized plants or animals, a cross between the native species and a domesticated one. This type of crossbreeding, termed genetic pollution by those who are concerned about preserving the genetic base of the wild species, has become a major concern. Hybridization is also a concern to the breeders of purebred species as well, particularly if the gene pool is small and if such crossbreeding or hybridization threatens the genetic base of the domesticated purebred population.
The concern with genetic pollution of a wild population is that hybridized animals and plants may not be as genetically strong as naturally evolved region specific wild ancestors wildlife which can survive without human husbandry and have high immunity to natural diseases. The concern of purebred breeders with wildlife hybridizing a domesticated species is that it can coarsen or degrade the specific qualities of a breed developed for a specific purpose, sometimes over many generations. Thus, both purebred breeders and wildlife biologists share a common interest in preventing accidental hybridization.
Hybrid species.
While not very common, a few animal species have been recognized as being the result of hybridization. The Lonicera fly is an example of a novel animal species that resulted from natural hybridization. The American red wolf appears to be a hybrid species between gray wolf and coyote, although its taxonomic status has been a subject of controversy. The European edible frog appears to be a species, but is actually a semi-permanent hybrid between pool frogs and marsh frogs. The edible frog population is dependent on the presence of at least one of the parents species to be maintained.
Hybrid species of plants are much more common than animals. Many of the crop species are hybrids, and hybridization appear to be an important factor in speciation in some plant groups.
Examples of hybrid animals.
Insect hybrids.
Hybrids should not be confused with genetic chimeras such as that between sheep and goat known as the geep. Wider interspecific hybrids can be made via in vitro fertilization or somatic hybridization, however the resulting cells are not able to develop into a full organism. An example of interspecific hybrid cell lines is humster (hamster × human) cells.
Hybrid plants.
Many hybrids are created by humans, but natural hybrids occur as well. Plant species hybridize more readily than animal species, and the resulting hybrids are more often fertile hybrids and may reproduce, though there still exist sterile hybrids and selective hybrid elimination where the offspring are less able to survive and are thus eliminated before they can reproduce. A number of plant species are the result of hybridization and polyploidy with many plant species easily cross pollinating and producing viable seeds, the distinction between each species is often maintained by geographical isolation or differences in the flowering period. Since plants hybridize frequently without much work, they are often created by humans in order to produce improved plants. These improvements can include the production of more or improved seeds, fruits or other plant parts for consumption, or to make a plant more winter or heat hardy or improve its growth and/or appearance for use in horticulture. Much work is now being done with hybrids to produce more disease resistant plants for both agricultural and horticultural crops. In many groups of plants hybridization has been used to produce larger and more showy flowers and new flower colors.
Many plant genera and species have their origins in polyploidy. Autopolyploidy results from the sudden multiplication in the number of chromosomes in typical normal populations caused by unsuccessful separation of the chromosomes during meiosis. Tetraploids (plants with four sets of chromosomes rather than two) are common in a number of different groups of plants and over time these plants can differentiate into distinct species from the normal diploid line. In "Oenothera lamarchiana" the diploid species has 14 chromosomes, this species has spontaneously given rise to plants with 28 chromosomes that have been given the name "Oenothera gigas". When hybrids are formed between the tetraploids and the diploid population, the resulting offspring tend to be sterile triploids, thus effectively stopping the intermixing of genes between the two groups of plants (unless the diploids, in rare cases, produce unreduced gametes).
Another form of polyploidy called allopolyploidy occurs when two different species mate and produce polyploid hybrids. Usually the typical chromosome number is doubled, and the four sets of chromosomes can pair up during meiosis, thus the polyploids can produce offspring. Usually, these offspring can mate and reproduce with each other but cannot back-cross with the parent species. Allopolyploids may be able to adapt to new habitats that neither of their parent species inhabited.
Sterility in a non-polyploid hybrid is often a result of chromosome number; if parents are of differing chromosome pair number, the offspring will have an odd number of chromosomes, leaving them unable to produce chromosomally balanced gametes. While this is undesirable in a crop such as wheat, where growing a crop which produces no seeds would be pointless, it is an attractive attribute in some fruits. Triploid bananas and watermelons are intentionally bred because they produce no seeds (and are parthenocarpic).
Heterosis.
Hybrids are sometimes stronger than either parent variety, a phenomenon most common with plant hybrids, which when present is known as "hybrid vigor" (heterosis) or heterozygote advantage. A transgressive phenotype is a phenotype displaying more extreme characteristics than either of the parent lines. Plant breeders make use of a number of techniques to produce hybrids, including line breeding and the formation of complex hybrids. An economically important example is hybrid maize (corn), which provides a considerable seed yield advantage over open pollinated varieties. Hybrid seed dominates the commercial maize seed market in the United States, Canada and many other major maize producing countries.
Examples of plant hybrids.
The multiplication symbol × (not italicised) indicates a hybrid in the Latin binomial nomenclature. Placed before the binomial it indicates a hybrid between species from different genera (intergeneric hybrid):-
Interspecific plant hybrids include:
Some natural hybrids:
Hybrids in nature.
Hybridization between two closely related species is actually a common occurrence in nature but is also being greatly influenced by anthropogenic changes as well. Hybridization is a naturally occurring genetic process where individuals from two genetically distinct populations mate. As stated above, it can occur both intraspecifically, between different distinct populations within the same species, and interspecifically, between two different species. Hybrids can be either sterile/not viable or viable/fertile. This affects the kind of effect that this hybrid will have on its and other populations that it interacts with. Many hybrid zones are known where the ranges of two species meet, and hybrids are continually produced in great numbers. These hybrid zones are useful as biological model systems for studying the mechanisms of speciation (Hybrid speciation). Recently DNA analysis of a bear shot by a hunter in the North West Territories confirmed the existence of naturally-occurring and fertile grizzly–polar bear hybrids. There have been reports of similar supposed hybrids, but this is the first to be confirmed by DNA analysis. In 1943, Clara Helgason described a male bear shot by hunters during her childhood. He was large and off-white with hair all over his paws. The presence of hair on the bottom of the feet suggests it was a natural hybrid of Kodiak and Polar bear.
Anthropogenic hybridization.
Changes to the environment caused by humans, such as fragmentation and Introduced species, are becoming more widespread. This increases the challenges in managing certain populations that are experiencing introgression, and is a focus of conservation genetics.
Introduced species and habitat fragmentation.
Humans have been introducing species world wide to environments for a long time both directly such as establishing a population to be used as a biological control and indirectly such as accidental escapes of individuals out of agriculture. This causes drastic global effects on various populations with hybridization being one of the reasons introduced species can be so detrimental.
When habitats become broken apart, one of two things can occur, genetically speaking. The first is that populations that were once connected can be cut off from one another, preventing their genes from interacting. Occasionally, this will result in a population of one species breeding with a population of another species as a means of surviving such as the case with the red wolves. Their population numbers being so small, they needed another means of survival. Habitat fragmentation also led to the influx of generalist species into areas where they would not have been, leading to competition and in some cases interbreeding/incorporation of a population into another. In this way, habitat fragmentation is essentially an indirect method of introducing species to an area.
The hybridization continuum.
There is a kind of continuum with three semi-distinct categories dealing with anthropogenic hybridization: hybridization without Introgression, hybridization with widespread introgression, and essentially a Hybrid swarm. Depending on where a population falls along this continuum, the management plans for that population will change. Hybridization is currently an area of great discussion within Wildlife management and habitat management fields. Global climate change is creating other changes such as difference in population distributions which are indirect causes for an increase in anthropogenic hybridization.
Consequences.
Hybridization can be a less discussed way toward extinction than within detection of where a population lies along the hybrid continuum. The dispute of hybridization is how to manage the resulting hybrids. When a population experiences hybridization with substantial introgression, there still exists parent types of each set of individuals. When a complete hybrid swarm is created, all the individuals are hybrids.
Management of hybrids.
Conservationists disagree on when is the proper time to give up on a population that is becoming a hybrid swarm or to try and save the still existing pure individuals. Once it becomes a complete mixture, we should look to conserve those hybrids to avoid their loss. Most leave it as a case-by-case basis, depending on detecting of hybrids within the group. It is nearly impossible to regulate hybridization via policy because hybridization can occur beneficially when it occurs "naturally" and there is the matter of protecting those previously mentioned hybrid swarms because if they are the only remaining evidence of prior species, they need to be conserved as well.
In some species, hybridisation plays an important role in evolutionary biology. While most hybrids are disadvantaged as a result of genetic incompatibility, the fittest survive, regardless of species boundaries. They may have a beneficial combination of traits allowing them to exploit new habitats or to succeed in a marginal habitat where the two parent species are disadvantaged. This has been seen in experiments on sunflower species. Unlike mutation, which affects only one gene, hybridisation creates multiple variations across genes or gene combinations simultaneously. Successful hybrids could evolve into new species within 50-60 generations. This leads some scientists to speculate that life is a genetic continuum rather than a series of self-contained species.
Where there are two closely related species living in the same area, less than 1 in 1000 individuals are likely to be hybrids because animals rarely choose a mate from a different species (otherwise species boundaries would completely break down). In some closely related species there are recognized "hybrid zones".
Some species of Heliconius butterflies exhibit dramatic geographical polymorphism of their wing patterns, which act as aposematic signals advertising their unpalatability to potential predators. Where different-looking geographical breeds abut, inter-racial hybrids are common, healthy and fertile. Heliconius hybrids can breed with other hybrid individuals and with individuals of either parental group. These hybrid backcrosses are disadvantaged by natural selection because they lack the parental form's warning coloration, and are therefore not avoided by predators.
A similar case in mammals is hybrid White-Tail/Mule Deer. The hybrids don't inherit either parent's escape strategy. White-tail Deer dash while Mule Deer bound. The hybrids are easier prey than the parent species.
In birds, healthy Galapagos Finch hybrids are relatively common, but their beaks are intermediate in shape and less efficient feeding tools than the specialised beaks of the parental species so they lose out in the competition for food. Following a major storm in 1983, the local habitat changed so that new types of plants began to flourish, and in this changed habitat, the hybrids had an advantage over the birds with specialised beaks - demonstrating the role of hybridization in exploiting new ecological niches. If the change in environmental conditions is permanent or is radical enough that the parental species cannot survive, the hybrids become the dominant form. Otherwise, the parental species will re-establish themselves when the environmental change is reversed, and hybrids will remain in the minority.
Natural hybrids may occur when a species is introduced into a new habitat. In Britain, there is hybridisation of native European Red Deer and introduced Chinese Sika Deer. Conservationists want to protect the Red Deer, but the environment favors the Sika Deer genes. There is a similar situation with White-headed Ducks and Ruddy Ducks.
Expression of parental traits in hybrids.
When two distinct types of organisms breed with each other, the resulting hybrids typically have intermediate traits (e.g., one parent has red flowers, the other has white, and the hybrid, pink flowers). Commonly, hybrids also combine traits seen only separately in one parent or the other (e.g., a bird hybrid might combine the yellow head of one parent with the orange belly of the other). Most characteristics of the typical hybrid are of one of these two types, and so, in a strict sense, are not really new. However, an intermediate trait does differ from those seen in the parents (e.g., the pink flowers of the intermediate hybrid just mentioned are not seen in either of its parents). Likewise, combined traits are new when viewed as a combination.
In a hybrid, any trait that falls outside the range of parental variation is termed heterotic. Heterotic hybrids do have new traits, that is, they are not intermediate. "Positive heterosis" produces more robust hybrids, they might be stronger or bigger; while the term "negative heterosis" refers to weaker or smaller hybrids. Heterosis is common in both animal and plant hybrids. For example, hybrids between a lion and a tigress ("ligers") are much larger than either of the two progenitors, while a tigon (lioness × tiger) is smaller. Also the hybrids between the Common Pheasant ("Phasianus colchicus") and domestic fowl ("Gallus gallus") are larger than either of their parents, as are those produced between the Common Pheasant and hen Golden Pheasant ("Chrysolophus pictus"). Spurs are absent in hybrids of the former type, although present in both parents.
When populations hybridize, often the first generation (F1) hybrids are very uniform. Typically, however, the individual members of subsequent hybrid generations are quite variable. High levels of variability in a natural population, then, are indicative of hybridity. Researchers use this fact to ascertain whether a population is of hybrid origin. Since such variability generally occurs only in later hybrid generations, the existence of variable hybrids is also an indication that the hybrids in question are fertile.
Genetic mixing and extinction.
Regionally developed ecotypes can be threatened with extinction when new alleles or genes are introduced that alter that ecotype. This is sometimes called genetic mixing. Hybridization and introgression of new genetic material can lead to the replacement of local genotypes if the hybrids are more fit and have breeding advantages over the indigenous ecotype or species. These hybridization events can result from the introduction of non native genotypes by humans or through habitat modification, bringing previously isolated species into contact. Genetic mixing can be especially detrimental for rare species in isolated habitats, ultimately affecting the population to such a degree that none of the originally genetically distinct population remains.
Effect on biodiversity and food security.
In agriculture and animal husbandry, the Green Revolution's use of conventional hybridization increased yields by breeding "high-yielding varieties". The replacement of locally indigenous breeds, compounded with unintentional cross-pollination and crossbreeding (genetic mixing), has reduced the gene pools of various wild and indigenous breeds resulting in the loss of genetic diversity. Since the indigenous breeds are often well-adapted to local extremes in climate and have immunity to local pathogens this can be a significant genetic erosion of the gene pool for future breeding. Therefore, commercial plant geneticists strive to breed "widely adapted" cultivars to counteract this tendency.
Limiting factors.
A number of conditions exist that limit the success of hybridization, the most obvious is great genetic diversity between most species. But in animals and plants that are more closely related hybridization barriers can include morphological differences, differing times of fertility, mating behaviors and cues, physiological rejection of sperm cells or the developing embryo.
In plants, barriers to hybridization include blooming period differences, different pollinator vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and structural differences of the chromosomes.
Mythical, legendary and religious hybrids.
Ancient folktales often contain mythological creatures, sometimes these are described as hybrids (e.g., Hippogriff as the offspring of a griffin and a horse, and the Minotaur which is the offspring of Pasiphaë and a white bull). More often they are kind of chimera, i.e., a composite of the physical attributes of two or more kinds of animals, mythical beasts, and often humans, with no suggestion that they are the result of interbreeding, e.g., Harpies, mermaids, and centaurs.
In the Bible, the Old Testament contains several passages which talk about a first generation of hybrid giants who were known as the Nephilim. The Book of Genesis (6:4) states that "the sons of God went to the daughters of humans and had children by them". As a result, the offspring was born as hybrid giants who became mighty heroes of old and legendary famous figures of ancient times. In addition, the Book of Numbers (13:33) says that the descendants of Anak came from the Nephilim, whose bodies looked exactly like men, but with an enormous height. According to the apocryphal Book of Enoch the Nephilim were wicked sons of fallen angels who had lusted with attractive women.

</doc>
<doc id="41245" url="http://en.wikipedia.org/wiki?curid=41245" title="Hybrid balance">
Hybrid balance

In telecommunications, a hybrid balance is an expression of the degree of electrical symmetry between two impedances connected to two conjugate sides of a hybrid coil or resistance hybrid. It is usually expressed in dB. 
If the respective impedances of the branches of the hybrid that are connected to the conjugate sides of the hybrid are known, hybrid balance may be computed by the formula for return loss. 
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41246" url="http://en.wikipedia.org/wiki?curid=41246" title="Hybrid coil">
Hybrid coil

A hybrid coil (or bridge transformer, or sometimes hybrid) is a transformer that has three windings, and which is designed to be configured as a circuit having four branches, (i.e. ports) that are conjugate in pairs. 
A signal arriving on one branch is divided between the two adjacent branches but does not appear at the opposite branch. In the schematic diagram, the signal into W splits between X and Z, and no signal passes to Y. Similarly, signals into X split to W and Y with none to Z, etc. 
Correct operation requires matched characteristic impedance at all four ports.
Explanation.
The primary use of a voiceband hybrid coil is to convert between 2-wire and 4-wire operation in sequential sections of a communications circuit, for example in a four-wire terminating set. Such conversion was necessary when repeaters were introduced in a 2-wire circuit, a frequent practice at early 20th century telephony. Without hybrids, the output of one amplifier feeds directly into the input of the other, resulting in a howling situation (upper diagram). By using hybrids, the outputs and inputs are isolated, resulting in correct 2-wire repeater operation. Late in the century, this practice became rare but hybrids continued in use in line cards.
Hybrid coil circuit diagrams.
Hybrids are realized using transformers. Two versions of transformer hybrids were used, the single transformer version providing unbalanced outputs with one end grounded, and the double transformer version providing balanced ports. 
Single transformer hybrid.
For use in 2-wire repeaters, the single transformer version suffices, since amplifiers in the repeaters have grounded inputs and outputs. X, Y, and Z share a common ground. As shown at left, signal into W, the 2-wire port, will appear at X and Z. But since Y is bridged from center of coil to center of X and Z, no signal appears. Signal into X will appear at W and Y. But signal at Z is the difference of what appears at Y and, through the transformer coil, at W, which is zero. Similar reasoning proves both pairs, W & Y, X & Z, are conjugates.
Double transformer hybrid.
When both the 2-wire and the 4-wire circuits must be balanced, double transformer hybrids are used, as shown at right. Signal into port W splits between X and Z, but due to reversed connection to the windings, cancel at port Y. Signal into port X goes to W and Y. But due to reversed connection to ports W and Y, Z gets no signal. Thus the pairs, W & Y, X & Z, are conjugates.
Applications.
Hybrids are used in telephones (see telephone hybrid) to reduce the sidetone, or volume of microphone output that was fed back to the earpiece. Without this, the phone user's own voice would be louder in the earpiece than the other party's. Such hybrids also had their windings so arranged as to act as an impedance matching transformer, matching the low-impedance carbon button transmitter to the higher impedance parts of the system. Today, the transformer version of the hybrid has been replaced by resistor networks and compact IC versions, which uses integrated circuit electronics to do the job of the hybrid coil.
Radio-frequency hybrids are used to split radio signals, including television. The splitter divides the antenna signal to feed multiple receivers.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41247" url="http://en.wikipedia.org/wiki?curid=41247" title="Hybrid routing">
Hybrid routing

Hybrid routing is the routing of telephone calls in which numbering plans and routing tables are used to permit the colocation, in the same area code, of switches using a deterministic routing scheme with switches using a non-deterministic routing scheme, such as flood search routing. 
"Note:" Routing tables are constructed with no duplicate numbers, so that direct distance dialing service can be provided to all network subscribers. This may require the use of 10-digit telephone numbers.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41248" url="http://en.wikipedia.org/wiki?curid=41248" title="Hydroxyl ion absorption">
Hydroxyl ion absorption

Hydroxyl ion absorption is the absorption in optical fibers of electromagnetic waves, including the near-infrared, due to the presence of trapped hydroxyl ions remaining from water as a contaminant. 
The hydroxyl (OH−) ion, can penetrate glass during or after product fabrication, resulting in significant attenuation of discrete optical wavelengths, "e.g.", centred at 1.383 μm, used for communications via optical fibres.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41250" url="http://en.wikipedia.org/wiki?curid=41250" title="Identifier">
Identifier

An identifier is a name that identifies (that is, labels the identity of) either a unique object or a unique "class" of objects, where the "object" or class may be an idea, physical [countable] object (or class thereof), or physical [noncountable] substance (or class thereof). The abbreviation ID often refers to identity, identification (the process of identifying), or an identifier (that is, an instance of identification). An identifier may be a word, number, letter, symbol, or any combination of those. 
The words, numbers, letters, or symbols may follow an encoding system (wherein letters, digits, words, or symbols "stand for" (represent) ideas or longer names) or they may simply be arbitrary. When an identifier follows an encoding system, it is often referred to as a code or ID code. Identifiers that do not follow any encoding scheme are often said to be arbitrary IDs; they are arbitrarily assigned and have no greater meaning. (Sometimes identifiers are called "codes" even when they are actually arbitrary, whether because the speaker believes that they have deeper meaning or simply because he is speaking casually and imprecisely.) 
ID codes inherently carry metadata along with them. (For example, when you know that the food package in front of you has the identifier "2011-09-25T15:42Z-MFR5-P02-243-45", you not only have that data, you also have the metadata that tells you that it was packaged on September 25, 2011, at 3:42pm UTC, manufactured by Licensed Vendor Number 5, at the Peoria, IL, USA plant, in Building 2, and was the 243rd package off the line in that shift, and was inspected by Inspector Number 45.) Arbitrary identifiers carry no metadata. (For example, if your food package just says 100054678214, its ID may not tell you anything except identity—no date, manufacturer name, production sequence rank, or inspector number.) 
In some cases, even arbitrary identifiers such as sequential serial numbers leak too much information (see German tank problem).
Opaque identifiers -- identifiers designed to avoid leaking even that small amount of information --
include "really opaque pointers" and Version 4 UUIDs.
The unique identifier (UID) is an identifier that refers to "only one instance"—only one particular object in the universe. A part number is an identifier, but it is not a "unique" identifier—for that, a serial number is needed, to identify "each instance" of the part design. Thus the "identifier" "Model T" identifies the "class" "(model)" of automobiles that Ford's Model T comprises; whereas the "unique identifier" "Model T Serial Number 159,862" identifies one specific member of that class—that is, one particular Model T car, owned by one specific person. 
The concepts of "name" and "identifier" are denotatively equal, and the terms are thus denotatively synonymous; but they are not always connotatively synonymous, because code names and ID numbers are often connotatively distinguished from names in the sense of traditional natural language naming. For example, both "Jamie Zawinski" and "Netscape employee number 20" are identifiers for the same specific human being; but normal English-language connotation may consider "Jamie Zawinski" a "name" and not an "identifier", whereas it considers "Netscape employee number 20" an "identifier" but not a "name". This is an emic distinction rather than an etic one.
Metadata.
In metadata, an identifier is a language-independent label, sign or token that uniquely identifies an object within an identification scheme.
The suffix identifier is also used as a representation term when naming a data element.
In computer science.
In computer science, identifiers (IDs) are lexical tokens that name entities. Identifiers are used extensively in virtually all information processing systems. Identifying entities makes it possible to refer to them, which is essential for any kind of symbolic processing.
In computer languages.
In computer languages, identifiers are tokens (also called symbols) which name language entities. Some of the kinds of entities an identifier might denote include variables, types, labels, subroutines, and packages.
Which character sequences constitute identifiers depends on the lexical grammar of the language. A common rule is alphanumeric sequences, with underscore also allowed, and with the condition that it not begin with a digit (to simplify lexing by avoiding confusing with integer literals) – so codice_1 are allowed, but codice_2 is not – this is the definition used in earlier versions of C and C++, Python 2, and many other languages. Later versions of these languages, along with many other modern languages support almost all Unicode characters in an identifier. However, a common restriction is not to permit whitespace characters and language operators; this simplifies tokenization by making it free-form and context-free. For example, forbidding codice_3 in identifiers (due to its use as a binary operation) means that codice_4 and codice_5 can be tokenized the same, while if it were allowed, codice_4 would be an identifier, not an addition. Whitespace in identifier is particularly problematic, as if spaces are allowed in identifiers, then a clause such as codice_7 is legal, with codice_8 as an identifier, but tokenizing this requires the phrasal context of being in the condition of an if clause. Some languages do allow spaces in identifiers, however, such as ALGOL 68 and some ALGOL variants – for example, the following is a valid statement: codice_9 which could be entered as codice_10 (keywords are represented in boldface, concretely via stropping). In ALGOL this was possible because keywords are syntactically differentiated, so there is no risk of collision or ambiguity, spaces are eliminated during the line reconstruction phase, and the source was processed via scannerless parsing, so lexing could be context-sensitive.
In most languages, some character sequences have the lexical form of an identifier but are known as keywords – for example, codice_11 is frequently a keyword for an if clause, but lexically is of the same form as codice_12 or codice_13 namely a sequence of letters. This overlap can be handled in various ways: these may be forbidden from being identifiers – which simplifies tokenization and parsing – in which case they are reserved words; they may both be allowed but distinguished in other ways, such as via stropping; or keyword sequences may be allowed as identifiers and which sense is determined from context, which requires a context-sensitive lexer. Non-keywords may also be reserved words (forbidden as identifiers), particularly for forward compatibility, in case a word may become a keyword in future. In a few languages, e.g., PL/1, the distinction is not clear.
For implementations of programming languages that are using a compiler, identifiers are often only compile time entities. That is, at runtime the compiled program contains references to memory addresses and offsets rather than the textual identifier tokens (these memory addresses, or offsets, having been assigned by the compiler to each identifier).
In languages that support reflection, such as interactive evaluation of source code (using an interpreter or an incremental compiler), identifiers are also runtime entities, sometimes even as first-class objects that can be freely manipulated and evaluated. In Lisp, these are called symbols.
Compilers and interpreters do not usually assign any semantic meaning to an identifier based on the actual character sequence used. However, there are exceptions.
For example:
In some languages such as Go, identifiers uniqueness is based on their spelling and their visibility.
In HTML an identifier is one of the possible attributes of an HTML element. It is unique within the document.
Ambiguity.
Identifiers (IDs) versus Unique identifiers (UIDs).
Many resources may carry multiple identifiers. Typical examples are: 
The inverse is also possible, where multiple resources are represented with the same identifier (discussed below).
Implicit context and namespace conflicts.
Many codes and nomenclatural systems originate within a small namespace. Over the years, some of them bleed into larger namespaces (as people interact in ways they formerly hadn't, e.g., cross-border trade, scientific collaboration, military alliance, and general cultural interconnection or assimilation). When such dissemination happens, the limitations of the original naming convention, which had formerly been latent and moot, become painfully apparent, often necessitating retronymy, synonymity, translation/transcoding, and so on. Such limitations generally accompany the shift away from the original context to the broader one. Typically the system shows implicit context (context was formerly assumed, and narrow), lack of capacity (e.g., low number of possible IDs, reflecting the outmoded narrow context), lack of extensibility (no features defined and reserved against future needs), and lack of specificity and disambiguating capability (related to the context shift, where longstanding uniqueness encounters novel nonuniqueness). Within computer science, this problem is called naming collision. The story of the origination and expansion of the CODEN system provides a good case example in a recent-decades, technical-nomenclature context. The capitalization variations seen with specific designators reveals an instance of this problem occurring in natural languages, where the proper noun/common noun distinction (and its complications) must be dealt with. A universe in which every object had a UID would not need any namespaces, which is to say that it would constitute one gigantic namespace; but human minds could never keep track of, or semantically interrelate, so many UIDs.
Identifiers in various disciplines.
A small sample of 

</doc>
<doc id="41251" url="http://en.wikipedia.org/wiki?curid=41251" title="Image antenna">
Image antenna

In telecommunications and antenna design, an image antenna is an electrical mirror-image of an antenna element formed by the radio waves reflecting from a conductive surface called a ground plane, such as the surface of the earth. It is used as a geometrical technique in calculating the radiation pattern of the antenna.
When a radio antenna is mounted above a conductive surface such as the earth, the radio waves directed down toward the surface reflect off it. The radiation received at a distant point is the sum of two contributions: the waves that travel directly from the antenna to the point, and the waves that reach the point after reflecting off the ground plane. Because of the reflection, these second waves appear to come from a second antenna behind the plane, just as a visible object in front of a flat mirror forms a virtual image that seems to lie behind the mirror. The radiation pattern of the antenna is exactly the same as it would be if the ground plane were replaced by a mirror image of the antenna, located an equal distance behind the plane. This second apparent source of radio waves is the image antenna.
The image antenna is used in calculating electric field vectors, magnetic field vectors, and electromagnetic fields emanating from the real antenna, particularly in the vicinity of the antenna and along the ground. Each charge and current in the real antenna has its counterpart in the image, and may also be considered as a source of radiation. 
To form an image of the antenna above it, the ground plane need not be grounded to the Earth. Many antenna types, such as reflective array antennas, use flat surfaces of metal or metal screen to reflect radio waves from the antenna elements, and these can be analyzed using image antennas. If there is more than one reflective surface in the antenna, as in a corner reflector antenna, each surface forms its own image of the antenna elements. In order to form an image, the ground plane surface must generally have dimensions of at least a quarter-wavelength of the radio waves used.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41254" url="http://en.wikipedia.org/wiki?curid=41254" title="Improved-definition television">
Improved-definition television

Improved-definition television (IDTV) or enhanced-quality television transmitters and receivers exceed the performance requirements of the NTSC standard, while remaining within the general parameters of NTSC emissions standards. 
IDTV improvements may be made at the television transmitter or receiver. Improvements include enhancements in encoding, digital filtering, scan interpolation, interlaced line scanning, and ghost cancellation. 
IDTV improvements must allow the TV signal to be transmitted and received in the standard 4:3 aspect ratio.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41255" url="http://en.wikipedia.org/wiki?curid=41255" title="Independent clock">
Independent clock

In telecommunications networks, independent clocks are free-running precision clocks located at the nodes which are used for synchronization.
Variable storage buffers, installed to accommodate variations in transmission delay between nodes, are made large enough to accommodate small time (phase) departures among the nodal clocks that control transmission. Traffic may occasionally be interrupted to allow the buffers to be emptied of some or all of their stored data.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41256" url="http://en.wikipedia.org/wiki?curid=41256" title="Index-matching material">
Index-matching material

In optics, an index-matching material is a substance, usually a liquid, cement (adhesive), or gel, which has an index of refraction that closely approximates that of another object (such as a lens, material, fiber-optic, etc.).
When two substances with the same index are next to each other, light passes from one to the other with neither reflection nor refraction. As such, they are used for various purposes in science, engineering, and art.
For example, in a popular home experiment, a glass rod is made almost invisible by immersing it in an index-matched transparent fluid such as mineral spirits.
In microscopy.
In light microscopy, oil immersion is a technique used to increase the resolution of a microscope. This is achieved by immersing both the objective lens and the specimen in a transparent oil of high refractive index, thereby increasing the numerical aperture of the objective lens.
Immersion oils are transparent oils that have specific optical and viscosity characteristics necessary for use in microscopy. Typical oils used have an index of refraction around 1.515. An oil immersion objective is an objective lens specially designed to be used in this way. The index of the oil is typically chosen to match the index of the microscope lens glass, and of the cover slip.
For more details, see the main article, oil immersion. Some microscopes also use other index-matching materials besides oil; see water immersion objective and solid immersion lens.
In fiber optics.
In fiber optics and telecommunications, an index-matching material may be used in conjunction with pairs of mated connectors or with mechanical splices to reduce signal reflected in the guided mode (known as return loss) (see: Optical fiber connector). Without the use of an index-matching material, Fresnel reflections will occur at the smooth end faces of a fiber unless there is no fiber-air interface or other significant mismatch in refractive index. These reflections may be as high as -14 dB ("i.e.," 14 dB below the optical power of the incident signal). When the reflected signal returns to the transmitting end, it may be reflected again and return to the receiving end at a level that is (28 plus twice the fiber loss) dB below the direct signal. The reflected signal will also be delayed by twice the delay time introduced by the fiber. The twice-reflected, delayed signal superimposed on the direct signal may noticeably degrade an analog baseband intensity-modulated video signal. Conversely, for digital transmission, the reflected signal will often have no practical effect on the detected signal seen at the decision point of the digital optical receiver except in marginal cases where bit-error ratio is significant. However, certain digital transmitters such as those employing a Distributed Feedback Laser may be affected by back reflection and then fall outside specifications such as Side Mode Suppression Ratio, potentially degrading system bit error ratio, so networking standards intended for DFB lasers may specify a back-reflection tolerance such as -10 dB for transmitters so that they remain within specification even without index matching. This back-reflection tolerance might be achieved using an optical isolator or by way of reduced coupling efficiency.
For some applications, instead of standard polished connectors (e.g. FC/PC), angle polished connectors (e.g. FC/APC) may be used, whereby the non-perpendicular polish angle greatly reduces the ratio of reflected signal launched into the guided mode even in the case of a fiber-air interface.
In art conservation.
If a sculpture is broken into several pieces, art conservators may reattach the pieces using an adhesive such as Paraloid B-72 or epoxy. If the sculpture is made of a transparent or semitransparent material (such as glass), the seam where the pieces are attached will usually be much less noticeable if the refractive index of the adhesive matches the refractive index of the surrounding object. Therefore, art conservators may measure the index of objects and then use an index-matched adhesive. Similarly, losses (missing sections) in transparent or semitransparent objects are often filled using an index-matched material.
In optical component adhesives.
Certain optical components, such as a Wollaston prism or Nicol prism, are made of multiple transparent pieces that are directly attached to each other. The adhesive is usually index-matched to the pieces. Historically, Canada balsam was used in this application, but it is now more common to use epoxy or other synthetic adhesives.

</doc>
<doc id="41258" url="http://en.wikipedia.org/wiki?curid=41258" title="Inductive coupling">
Inductive coupling

In electrical engineering, two conductors are referred to as mutual-inductively coupled or magnetically coupled when they are configured such that change in current through one wire induces a voltage across the ends of the other wire through electromagnetic induction. The amount of inductive coupling between two conductors is measured by their mutual inductance.
The coupling between two wires can be increased by winding them into coils and placing them close together on a common axis, so the magnetic field of one coil passes through the other coil. The two coils may be physically contained in a single unit, as in the primary and secondary sides of a transformer, or may be separated. Coupling may be intentional or unintentional. 
Unintentional coupling is called cross-talk, and is a form of electromagnetic interference. Inductive coupling favors low frequency energy sources. High frequency energy sources generally use capacitive coupling.
An inductively coupled transponder comprises an electronic data carrying device, usually a single microchip, and a large coil that functions as an antenna. Inductively coupled transponders are almost always operated passively.
Magnetic coupling transfers torque from one magnetic gear to another.
Some diver propulsion vehicles and remotely operated underwater vehicles use magnetic coupling
to transfer torque from the electric motor to the prop. Magnetic gearing is also being explored for use in utility scale wind turbines as a means of enhancing reliability.
The magnetic coupling has several advantages over a traditional stuffing box.
Uses.
Devices that use inductive coupling include:
Low frequency induction.
Low frequency induction is an unwanted form of inductive coupling, which can occur when a metallic pipeline is installed parallel to a high-voltage power line. The pipeline, which is a conductor, and is insulated from the earth by its protective coating, can develop voltages which are hazardous to personnel operating valves or otherwise contacting the pipeline.

</doc>
<doc id="41259" url="http://en.wikipedia.org/wiki?curid=41259" title="Information-bearer channel">
Information-bearer channel

In telecommunications, an information-bearer channel is one of: 

</doc>
<doc id="41262" url="http://en.wikipedia.org/wiki?curid=41262" title="Information-transfer transaction">
Information-transfer transaction

A transaction is a change of state, an information-transfer transaction is a transaction in which one of the following changes occurs: content, ownership, location, format, etc. An information-transfer transaction usually consists of three consecutive phases called the access phase, the information transfer phase, and the disengagement phase. Examples of these consecutive phases are the copying and transporting of information. Once a transaction occurs there are also costs to consider, which are associated with that certain transaction. When it comes to the transfer of information some transaction costs include time and means (money).
History of Information-transfer transactions.
There are many social systems and devices that have contributed to information-transfer transactions; starting from people writing letters using postal systems to emailing using information technology. Two main examples of information-transfer transactions technology development is the copying and transportation of information.
History of Copying.
Copying is the process of duplicating information with the change of location or format of the original information. The transfer transaction of information through copying has been going on for ages and there has been many advances in technology to decrease the time it takes to make copies of said information. The art of copying started with people having to write a copy out by hand, then the printing press, all the way to digital copying with ICTs. These developments lead to quicker information-transfer transactions in the form of distributing copies of original information to others through a changes of location or format.
History of Transporting.
Transporting is the movement of information with the change of location or ownership of the original information. The transfer transaction of information through transporting has been going on for ages and there has been social and technological developments to decrease the time it takes for information to change ownership or location. The transportation of information started with people sending letters by foot, then by horse, the public and international postal service, all the way down to technology networks. It is these developments which lead to the ability to send information further and quicker through information-transfer transactions.
Transaction Costs.
Every time a transaction occurs, there are always costs to be considered. In the case of information-transfer transactions, one most consider the costs of time and means (money). Both of these costs are corollated with one another in that to decrease one, you must increase the other. For example, say Person #1 sends a letter through the mail, while Person #2 sends letters through email. For Person #1 to send their letter they had to buy paper, means of writing, envelopes, stamps, etc., while Person #2 needed to buy a source of electricity, internet, computer technology, etc. to send an email. It seems like Person #1 has lower transaction costs then Person #2 in terms of means; however, when you look at both information-transfer transactions in terms of time that is a different story. For Person #1 although they had little costs in sending their letter, the time it takes for the transfer of that letter is about 3+ days, while Person #2's transfer through email happens in less than minutes, but they endured high mean costs. Therefore, for information-transfer transaction times to decrease, the costs of means have to increase and vice versa.
Telecommunication.
In telecommunication, an information-transfer transaction is a coordinated sequence of user and telecommunications system actions that cause information present at a source user to become present at a destination user.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41264" url="http://en.wikipedia.org/wiki?curid=41264" title="Input">
Input


</doc>
<doc id="41265" url="http://en.wikipedia.org/wiki?curid=41265" title="Insertion gain">
Insertion gain

In telecommunication, insertion gain is the gain resulting from the insertion of a device in a transmission line, expressed as the ratio of the signal power delivered to that part of the line following the device to the signal power delivered to that same part before insertion. If the resulting number is less than unity, an "insertion loss" is indicated. 
Insertion gain is usually expressed in dB.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41266" url="http://en.wikipedia.org/wiki?curid=41266" title="Insertion loss">
Insertion loss

In telecommunications, insertion loss is the loss of signal power resulting from the insertion of a device in a transmission line or optical fiber and is usually expressed in decibels (dB). 
If the power transmitted to the load before insertion is "P"T and the power received by the load after insertion is "P"R, then the insertion loss in dB is given by,
Electronic filters.
Insertion loss is a figure of merit for an electronic filter and this data is generally specified with a filter. Insertion loss is defined as a ratio of the signal level in a test configuration without the filter installed (|"V"1|) to the signal level with the filter installed (|"V"2|). This ratio is described in dB by the following equation:
Note that, for most filters, |"V"2| will be smaller than |"V"1|. In this case, the insertion loss is positive and measures how much smaller the signal is after adding the filter.
Link with Scattering parameters.
In case the two measurement ports use the same reference impedance, the insertion loss (formula_3) is defined as: 
and not, as often mistakenly thought, by: 
Here formula_6 and formula_7 are two of the scattering parameters. It is the extra loss produced by the introduction of the DUT between the 2 reference planes of the measurement. The extra loss can be introduced by intrinsic loss in the DUT and/or mismatch. In case of extra loss the insertion loss is defined to be positive.

</doc>
<doc id="41267" url="http://en.wikipedia.org/wiki?curid=41267" title="Inside plant">
Inside plant

In telecommunication, the term inside plant has the following meanings: 
Around the turn of the 21st century, DSLAMs became an important part of telephone company inside plant. Inside plant will also have distribution frames and other equipment including passive optical network (name depends on the Service Provider).
Power.
A typical power system for a switching office in an inside plant consists of the elements listed below: 
For safety and reliability reasons, it is desirable that all telecommunications loads be DC powered with minimal AC-powered devices used. Telcordia , contains detailed industry requirements for using power in an inside plant.
Both integrated and isolated bonding networks as per Telcordia , are a technically viable means to ground and bond the equipment in a safe and effective manner. However, the integrated or mesh bonding schemes are preferred over isolated bonding networks because of the added costs and efforts required to manage, control, and maintain the isolation for the equipment, particularly during equipment upgrade and modifications to the plant. This preference is based on a pragmatic desire for lower costs and ease of management, and to simplify operations during plant modifications/upgrades.
For a comprehensive analysis of the energy efficiency and environmental soundness of a power system, one should ideally consider a wider range of factors than strict energy conversion AC-to-DC power efficiency. These environmental factors cover a wider vision known as Industrial Ecology within which each manufacturing step of the products need to be considered from a Design for Environment (DfE) factors standpoint.

</doc>
<doc id="41268" url="http://en.wikipedia.org/wiki?curid=41268" title="Intelligent Network">
Intelligent Network

The Intelligent Network (IN) is the standard network architecture specified in the . It is intended for fixed as well as mobile telecom networks. It allows operators to differentiate themselves by providing value-added services in addition to the standard telecom services such as PSTN, ISDN and GSM services on mobile phones.
The intelligence is provided by network nodes on the service layer, distinct from the switching layer of the core network, as opposed to solutions based on intelligence in the core switches or telephone equipment. The IN nodes are typically owned by telecommunications operators (telecommunications service providers).
IN is supported by the Signaling System #7 (SS7) protocol between telephone network switching centers and other network nodes owned by network operators.
History and key concepts.
The IN concepts, architecture and protocols were originally developed as standards by the ITU-T which is the standardization committee of the International Telecommunication Union, prior to this a number of telecommunications providers had proprietary IN solutions. The primary aim of the IN was to enhance the core telephony services offered by traditional telecommunications networks, which usually amounted to making and receiving voice calls, sometimes with call divert. This core would then provide a basis upon which operators could build services in addition to those already present on a standard telephone exchange.
A complete description of the IN emerged in a set of ITU-T standards named to , or Capability Set One (CS-1) as they became known. The standards defined a complete architecture including the architectural view, state machines, physical implementation and protocols. They were universally embraced by telecom suppliers and operators, although many variants were derived for use in different parts of the world (see Variants below).
Following the success of CS-1, further enhancements followed in the form of CS-2. Although the standards were completed, they were not as widely implemented as CS-1, partly because of the increasing power of the variants, but also partly because they addressed issues which pushed traditional telephone exchanges to their limits.
The major driver behind the development of the IN system was the need for a more flexible way of adding sophisticated services to the existing network. Before IN was developed, all new feature and/or services that were to be added had to be implemented directly in the core switch systems. This made for very long release cycles as the bug hunting and testing had to be extensive and thorough to prevent the network from failing. With the advent of IN, most of these services (such as toll free numbers and geographical number portability) were moved out of the core switch systems and into self-serving nodes (IN), thus creating a modular and more secure network that allowed the service providers themselves to develop variations and value-added services to their networks without submitting a request to the core switch manufacturer and wait for the long development process. The initial use of IN technology was for number translation services, e.g. when translating toll free numbers to regular PSTN numbers. But much more complex services have since been built on IN, such as Custom Local Area Signaling Services (CLASS) and prepaid telephone calls.
SS7 architecture.
The main concepts (functional view) surrounding IN services or architecture are connected with SS7 architecture:
Protocols.
The core elements described above use standard protocols to communicate with each other. The use of standard protocols allows different manufacturers to concentrate on different parts of the architecture and be confident that they will all work together in any combination.
The interfaces between the SSP and the SCP are SS7 based and may look familiar to those familiar with TCP/IP protocols. The SS7 protocols implement much of the OSI seven-layer model. This means that the IN standards only had to define the application layer which was called the Intelligent Networks Application Part or INAP. The INAP messages are encoded using ASN.1.
The interface between the SCP and the SDP is defined in the standards to be an X.500 Directory Access Protocol or DAP. A more lightweight interface called LDAP has emerged from the IETF which is considerably simpler to implement, so many SCPs have implemented that instead.
Variants.
The core CS-1 specifications were adopted and extended by other standards bodies. European flavours were developed by ETSI, American flavours were developed by ANSI and Japanese variants also exist. The main reasons for producing variants in each region was to ensure interoperability between equipment manufactured and deployed locally (for example different versions of the underlying SS7 protocols exist between the regions).
New functionality was also added which meant that variants diverged from each other and the main ITU-T standard. The biggest variant was called Customised Applications for Mobile networks Enhanced Logic, or CAMEL for short. This allowed for extensions to be made for the mobile phone environment, and allowed mobile phone operators to offer the same IN services to subscribers while they are roaming as they receive in the home network.
CAMEL has become a major standard in its own right and is currently maintained by 3GPP. The last major release of the standard was CAMEL phase 4. It is the only IN standard currently being actively worked on.
The "Advanced Intelligent Network" (AIN) is the variant of Intelligent Network developed for North America by Bellcore (now Telcordia).
The standardization of the AIN was performed by Bellcore (now Telcordia Technologies) on behalf of the major US operators.
The original goal of AIN was AIN 1.0, which was specified in the early 1990s ("AIN Release 1", Bellcore SR-NWT-002247, 1993). AIN 1.0 proved technically infeasible to implement, which led to the definition of simplified AIN 0.1 and AIN 0.2 specifications. In North America, Telcordia SR-3511 (originally known as TA-1129+) and GR-1129-CORE protocols are used to link switches with the IN systems such as Service Control Points (SCPs) or Service Nodes. SR-3511 details a TCP/IP-based protocol which directly connects the SCP and Service Node. GR-1129-CORE provides generic requirements for an ISDN based protocol which connects the SCP to the Service Node via the SSP.
Future.
While activity in development of IN standards has declined in recent years, there are many systems deployed across the world which use this technology. The architecture has proved to be not only stable, but also a continuing source of revenue with new services added all the time. Manufacturers continue to support the equipment and obsolescence is not an issue.
Nevertheless, new technologies and architectures are emerging, especially in the area of VoIP and SIP. More attention is being paid to the use of APIs in preference to protocols like INAP and new standards have emerged in the form of JAIN and Parlay. From a technical view, the SCE is beginning to move away from its proprietary graphical origins and is moving towards a Java application server environment.

</doc>
<doc id="41269" url="http://en.wikipedia.org/wiki?curid=41269" title="Intensity modulation">
Intensity modulation

In optical communications, intensity modulation (IM) is a form of modulation in which the optical power output of a source is varied in accordance with some characteristic of the modulating signal. The envelope of the modulated optical signal is an analog of the modulating signal in the sense that the instantaneous power of the envelope is an analog of the characteristic of interest in the modulating signal. 
Recovery of the modulating signal is usually by direct detection, not heterodyning. However, optical heterodyne detection is possible and has been actively studied since 1979. Bell Laboratories had a working, but impractical, system in 1969. Heterodyne and homodyne systems are of interest because they are expected to produce an increase in sensitivity of up to 20 dB allowing longer hops between islands for instance. Such systems also have the important advantage of very narrow channel spacing in optical frequency-division multiplexing (OFDM) systems. OFDM is a step beyond wavelength-division multiplexing (WDM). Normal WDM using direct detection does not achieve anything like the close channel spacing of radio frequency FDM.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41270" url="http://en.wikipedia.org/wiki?curid=41270" title="Intercept">
Intercept

Intercept may refer to:

</doc>
<doc id="41271" url="http://en.wikipedia.org/wiki?curid=41271" title="Interchangeability">
Interchangeability

Interchangeability can refer to:

</doc>
<doc id="41272" url="http://en.wikipedia.org/wiki?curid=41272" title="Interchange circuit">
Interchange circuit

In telecommunication, an interchange circuit is a circuit that facilitates the exchange of data and signaling information between data terminal equipment (DTE) and data circuit-terminating equipment (DCE). 
An interchange circuit can carry many types of signals and provide many types of service features, such as control signals, timing signals, and common return functions.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41273" url="http://en.wikipedia.org/wiki?curid=41273" title="Intercharacter interval">
Intercharacter interval

In telecommunications, the intercharacter interval is the time interval between the end of the stop signal of one character and the beginning of the start signal of the next character of an asynchronous transmission. 
The intercharacter interval may be of any duration. The signal sense of the intercharacter interval is always the same as the sense of the stop element, "i.e.", "1" or "mark."
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41274" url="http://en.wikipedia.org/wiki?curid=41274" title="Interconnect facility">
Interconnect facility

Interconnect facility: In a communications network, one or more communications links that (a) are used to provide local area communications service among several locations and (b) collectively form a node in the network. 
An interconnect facility may include network control and administrative circuits as well as the primary traffic circuits. 
An interconnect facility may use any medium available and may be redundant.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41275" url="http://en.wikipedia.org/wiki?curid=41275" title="Interface">
Interface

Interface may refer to:

</doc>
<doc id="41276" url="http://en.wikipedia.org/wiki?curid=41276" title="Interface functionality">
Interface functionality

In telephony, interface functionality is the characteristic of interfaces that allows operators to support transmission, switching, and signaling functions identical to those used in the enhanced services provided by the carrier. 
As part of its comparably efficient interconnection (CEI) offering, the carrier must make available standardized telephone networking hardware and software interfaces that are able to support transmission, switching, and signaling functions identical to those used in the enhanced services provided by the carrier.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41277" url="http://en.wikipedia.org/wiki?curid=41277" title="Interface standard">
Interface standard

In telecommunications, an interface standard is a standard that describes one or more functional characteristics (such as code conversion, line assignments, or protocol compliance) or physical characteristics (such as electrical, mechanical, or optical characteristics) necessary to allow the exchange of information between two or more (usually different) systems or pieces of equipment. Communications protocols are an example. 
An interface standard may include operational characteristics and acceptable levels of performance. 
In the military community, interface standards permit command and control functions to be performed using communication and computer systems.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41278" url="http://en.wikipedia.org/wiki?curid=41278" title="Interference filter">
Interference filter

An interference filter or dichroic filter is an optical filter that reflects one or more spectral bands or lines and transmits others, while maintaining a nearly zero coefficient of absorption for all wavelengths of interest. An interference filter may be high-pass, low-pass, bandpass, or band-rejection. 
An interference filter consists of multiple thin layers of dielectric material having different refractive indices. There also may be metallic layers. In its broadest meaning, interference filters comprise also etalons that could be implemented as tunable interference filters. Interference filters are wavelength-selective by virtue of the interference effects that take place between the incident and reflected waves at the thin-film boundaries. The important characteristic of the filter is the form of the leaving signal. It is considered that the best form is a rectangle.
Bandpass filters are normally designed for normal incidence. However, when the angle of incidence of the incoming light is increased from zero, the central wavelength of the filter decreases, resulting in partial tunability. The transmission band widens and the maximum transmission decreases. If λ"c" is the central wavelength, λ0 is the central wavelength at normal incidence, and "n"* is the filter effective index of refraction, then:
For example, for λ0=1550 nm, "n"*=1.5, Δλ = λ0−λc=32 nm, the rotation angle is
θ = 17.7°. This corresponds to C-band or L-band in 1550 nm fiber-optic communications window. Equipped with a stepper motor and electronics, a tunable optical filter that tunes center transmission wavelength over C-band or L-band by remote control can be achieved. See diagram below for its working principle and tunable optical filter devices

</doc>
<doc id="41280" url="http://en.wikipedia.org/wiki?curid=41280" title="Intermediate distribution frame">
Intermediate distribution frame

An intermediate distribution frame (IDF) is a distribution frame in a central office or customer premises, which cross-connects the user cable media to individual user line circuits and may serve as a distribution point for multipair cables from the main distribution frame (MDF) or combined distribution frame (CDF) to individual cables connected to equipment in areas remote from these frames.
IDFs are used for telephone exchange central office, customer-premise equipment, wide area network (WAN), and local area network (LAN) environments, among others.
In central office environments the IDF may contain circuit termination equipment from various auxiliary components. In WAN and LAN environments IDFs can hold devices of different types including backup systems (hard drives or other media as self-contained, or as RAIDs, CD-ROMs, etc.), networking (switches, hubs, routers), and connections (fiber optics, coaxial, category cables) and so on.

</doc>
<doc id="41281" url="http://en.wikipedia.org/wiki?curid=41281" title="Intermediate-field region">
Intermediate-field region

Intermediate-field region: For an antenna, the transition region--lying between the near-field region and the far-field region--in which the field strength of an electromagnetic wave is dependent upon the inverse distance, inverse square of the distance, and the inverse cube of the distance from the antenna. For an antenna that is small compared to the wavelength in question, the intermediate-field region is considered to exist at all distances between 0.1 wavelength and 1.0 wavelength from the antenna. "Synonyms": intermediate field, intermediate zone, transition zone.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41284" url="http://en.wikipedia.org/wiki?curid=41284" title="IP address spoofing">
IP address spoofing

In computer networking, IP address spoofing or IP spoofing is the creation of Internet Protocol (IP) packets with a source IP address, with the purpose of concealing the identity of the sender or impersonating another computing system.
Background.
The basic protocol for sending data over the Internet network and many other computer networks is the Internet Protocol ("IP"). The header of each IP packet contains, among other things, the numerical source and destination address of the packet. The source address is normally the address that the packet was sent from. By forging the header so it contains a different address, an attacker can make it appear that the packet was sent by a different machine. The machine that receives spoofed packets will send a response back to the forged source address, which means that this technique is mainly used when the attacker does not care about the response or the attacker has some way of guessing the response.
Applications.
IP spoofing is most frequently used in denial-of-service attacks. In such attacks, the goal is to flood the victim with overwhelming amounts of traffic, and the attacker does not care about receiving responses to the attack packets. Packets with spoofed addresses are thus suitable for such attacks. They have additional advantages for this purpose—they are more difficult to filter since each spoofed packet appears to come from a different address, and they hide the true source of the attack. Denial of service attacks that use spoofing typically randomly choose addresses from the entire IP address space, though more sophisticated spoofing mechanisms might avoid unroutable addresses or unused portions of the IP address space. The proliferation of large botnets makes spoofing less important in denial of service attacks, but attackers typically have spoofing available as a tool, if they want to use it, so defenses against denial-of-service attacks that rely on the validity of the source IP address in attack packets might have trouble with spoofed packets. Backscatter, a technique used to observe denial-of-service attack activity in the Internet, relies on attackers' use of IP spoofing for its effectiveness.
IP spoofing can also be a method of attack used by network intruders to defeat network security measures, such as authentication based on IP addresses. This method of attack on a remote system can be extremely difficult, as it involves modifying thousands of packets at a time. This type of attack is most effective where trust relationships exist between machines. For example, it is common on some corporate networks to have internal systems trust each other, so that users can log in without a username or password provided they are connecting from another machine on the internal network (and so must already be logged in). By spoofing a connection from a trusted machine, an attacker may be able to access the target machine without authentication.
Legitimate uses of IP spoofing.
Spoofed IP packets are not incontrovertible evidence of malicious intent: in performance testing of websites, hundreds or even thousands of "vusers" (virtual users) may be created, each executing a test script against the website under test, in order to simulate what will happen when the system goes "live" and a large number of users log on at once.
Since each user will normally have its own IP address, commercial testing products (such as HP LoadRunner and WebLOAD) can use IP spoofing, allowing each user its own "return address" as well.
Services vulnerable to IP spoofing.
Configuration and services that are vulnerable to IP spoofing:
Defense against spoofing attacks.
Packet filtering is one defense against IP spoofing attacks. The gateway to a network usually performs ingress filtering, which is blocking of packets from outside the network with a source address inside the network. This prevents an outside attacker spoofing the address of an internal machine. Ideally the gateway would also perform egress filtering on outgoing packets, which is blocking of packets from inside the network with a source address that is not inside. This prevents an attacker within the network performing filtering from launching IP spoofing attacks against external machines.
It is also recommended to design network protocols and services so that they do not rely on the IP source address for authentication.
Upper layers.
Some upper layer protocols provide their own defense against IP spoofing attacks. For example, Transmission Control Protocol (TCP) uses sequence numbers negotiated with the remote machine to ensure that arriving packets are part of an established connection. Since the attacker normally can't see any reply packets, the sequence number must be guessed in order to hijack the connection. The poor implementation in many older operating systems and network devices, however, means that TCP sequence numbers can be predicted.
Other definitions.
The term spoofing is also sometimes used to refer to "header forgery", the insertion of false or misleading information in e-mail or netnews headers. Falsified headers are used to mislead the recipient, or network applications, as to the origin of a message. This is a common technique of spammers and sporgers, who wish to conceal the origin of their messages to avoid being tracked down.

</doc>
<doc id="41285" url="http://en.wikipedia.org/wiki?curid=41285" title="Interoperability">
Interoperability

Interoperability is a property of a product or system, whose interfaces are completely understood, to work with other products or systems, present or future, without any restricted access or implementation.
Other elements of definition.
While the term was initially defined for information technology or systems engineering services to allow for information exchange, a more broad definition takes into account social, political, and organizational factors that impact system to system performance. Task of building coherent services for users when the individual components are technically different and managed by different organizations
Syntactic interoperability.
If two or more systems are capable of communicating and exchanging data, they are exhibiting syntactic interoperability. Specified data formats, communication protocols and the like are fundamental. XML or SQL standards are among the tools of syntactic interoperability. This is also true for lower-level data formats, such as ensuring alphabetical characters are stored in a same variation of ASCII or a Unicode format (for English or international text) in all the communicating systems.
Syntactical interoperability is a necessary condition for further interoperability.
Semantic interoperability.
Beyond the ability of two or more computer systems to exchange information, semantic interoperability is the ability to automatically interpret the information exchanged meaningfully and accurately in order to produce useful results as defined by the end users of both systems. To achieve semantic interoperability, both sides must refer to a common information exchange reference model. The content of the information exchange requests are unambiguously defined: what is sent is the same as what is understood. The possibility of promoting this result by user-driven convergence of disparate interpretations of the same information has been object of study by research prototypes such as S3DB.
Cross-domain interoperability.
Multiple social, organizational, political, legal entities working together for a common interest and/or information exchange.
Interoperability and open standards.
Interoperability must be distinguished from open standards. Although the goal of each is to provide effective and efficient exchange between computer systems, the mechanisms for accomplishing that goal differ. Open standards imply interoperability "ab-initio," i.e. by definition, while interoperability does not, by itself, imply wider exchange between a range of products, or similar products from several different vendors, or even between past and future revisions of the same product. Interoperability may be developed "post-facto," as a special measure between two products, while excluding the rest, or when a vendor is forced to adapt its system to make it interoperable with a dominant system.
Open standards.
Open standards rely on a broadly consultative and inclusive group including representatives from vendors, academics and others holding a stake in the development. That discusses and debates the technical and economic merits, demerits and feasibility of a proposed common protocol. After the doubts and reservations of all members are addressed, the resulting common document is endorsed as a "common standard." This document is subsequently released to the public, and henceforth becomes an "open standard." It is usually published and is available freely or at a nominal cost to any and all comers, with "no further encumbrances." Various vendors and individuals (even those who were not part of the original group) can use the standards document to make products that implement the common protocol defined in the standard, and are thus interoperable by design, with no specific liability or advantage for any customer for choosing one product over another on the basis of standardised features. The vendors' products compete on the quality of their implementation, user interface, ease of use, performance, price, and a host of other factors, while keeping the customers data intact and transferable even if he chooses to switch to another competing product for business reasons.
"Post Facto" Interoperability.
"Post-facto" interoperability may be the result of the absolute market dominance of a particular product in contravention of any applicable standards, or if any effective standards were not present at the time of that product's introduction. The vendor behind that product can then choose to "ignore" any forthcoming standards and not co-operate in any standardisation process at all, using its near-monopoly to insist that its product sets the "de facto" standard by its very market dominance. This is not a problem if the product's implementation is open "and" minimally encumbered, but it may as well be both closed and heavily encumbered (e.g. by patent claims). Because of the network effect, achieving interoperability with such a product is both critical for any other vendor if it wishes to remain relevant in the market, and difficult to accomplish because of lack of co-operation on equal terms with the original vendor, who may well see the new vendor as a potential competitor and threat. The newer implementations often rely on clean-room reverse engineering in the absence of technical data to achieve interoperability. The original vendors can provide such technical data to others, often in the name of 'encouraging competition,' but such data are invariably encumbered, and may be of limited use. Availability of such data is "not" equivalent to an open standard, because:
Telecommunications.
In telecommunication, the term can be defined as:
In two-way radio, interoperability is composed of three dimensions:
Search.
Search interoperability refers to the ability of two or more information collections to be searched by a single query.
Specifically related to web-based search, the challenge of interoperability stems from the fact designers of web resources typically have little or no need to concern themselves with exchanging information with other web resources. Federated Search technology, which does not place format requirements on the data owner, has emerged as one solution to search interoperability challenges. In addition, standards, such as OAI-PMH, RDF, and SPARQL, have emerged recently that also help address the issue of search interoperability related to web resources. Such standards also address broader topics of interoperability, such as allowing data mining.
Software.
With respect to software, the term interoperability is used to describe the capability of different programs to exchange data via a common set of exchange formats, to read and write the same file formats, and to use the same protocols. (The ability to execute the same binary code on different processor platforms is 'not' contemplated by the definition of interoperability.) The lack of interoperability can be a consequence of a lack of attention to standardization during the design of a program. Indeed, interoperability is not taken for granted in the non-standards-based portion of the computing world.
According to ISO/IEC 2382-01, "Information Technology Vocabulary, Fundamental Terms", interoperability is defined as follows: ""The capability to communicate, execute programs, or transfer data among various functional units in a manner that requires the user to have little or no knowledge of the unique characteristics of those units"." 
Note that the definition is somewhat ambiguous because the "user" of a program can be another program and, if the latter is a portion of the set of program that is required to be interoperable, it might well be that it does need to have knowledge of the characteristics of other units.
This definition focuses on the technical side of interoperability, while it has also been pointed out that interoperability is often more of an organizational issue: often interoperability has a significant impact on the organizations concerned, raising issues of ownership (do people want to share their data?), labor relations (are people prepared to undergo training?) and usability. In this context, a more apt definition is captured in the term "business process interoperability".
Interoperability can have important economic consequences; for example, research has estimated the cost of inadequate interoperability in the U.S. capital facilities industry to be $15.8 billion a year. If competitors' products are not interoperable (due to causes such as patents, trade secrets or coordination failures), the result may well be monopoly or market failure. For this reason, it may be prudent for user communities or governments to take steps to encourage interoperability in various situations. In the United Kingdom, for example, there is an eGovernment-based interoperability initiative called e-GIF while in the United States there is the NIEM initiative. Standards Defining Organizations (SDOs) provide open public software specifications to facilitate interoperability; examples include the Oasis-Open organization and buildingSMART (formerly the International Alliance for Interoperability). As far as user communities, Neutral Third Party is creating standards for business process interoperability. Another example of a neutral party is the RFC documents from the Internet Engineering Task Force (IETF).
The (Open Service for Lifecycle Collaboration) Community is working on finding a common standard in order that software tools can share and exchange data e.g. bugs, tasks, requirements etc. The final goal is to agree on an open standard for interoperability of open source [Application Lifecycle Management|ALM] tools.
Organizations Dedicated to Interoperability.
Many organizations are dedicated to interoperability. All have in common that they want to push the development of the World Wide Web towards the semantic web. Some concentrate on eGovernment, eBusiness or data exchange in general. Internationally, Network Centric Operations Industry Consortium facilitates global interoperability across borders, language and technical barriers. In Europe, for instance, the European Commission and its IDABC programme issue the European Interoperability Framework. IDABC was succeeded by the ISA programme. They also initiated the Semantic Interoperability Centre Europe (SEMIC.EU). A (EULIS) was established in 2006, as a consortium of European National Land Registers. The aim of the service is to establish a single portal through which customers are provided with access to information about individual properties, about land and property registration services, and about the associated legal environment. In the United States, the government's CORE.gov service provides a collaboration environment for component development, sharing, registration, and reuse and related to this is the work and component repository. The National Institute of Standards and Technology serves as an agency for measurement standards.
Medical industry.
New technology is being introduced in hospitals and labs at an ever-increasing rate. The need for “plug-and-play” interoperability – the ability to take a medical device out of its box and easily make it work with one’s other devices – has attracted great attention from both healthcare providers and industry.
The Board approved the following definition of interoperability on April 5, 2013:
"In healthcare, interoperability is the ability of different information technology systems and software applications to communicate, exchange data, and use the information that has been exchanged. Data exchange schema and standards should permit data to be shared across clinicians, lab, hospital, pharmacy, and patient regardless of the application or application vendor. Interoperability means the ability of health information systems to work together within and across organizational boundaries in order to advance the effective delivery of healthcare for individuals and communities. There are three levels of health information technology interoperability: 1) Foundational; 2) Structural; and 3) Semantic."
They also have introduced at HIMSS15 a comprehensive interoperability testing and certification program.
eGovernment.
Speaking from an eGovernment perspective, interoperability refers to the collaboration ability of cross-border services for citizens, businesses and public administrations. Exchanging data can be a challenge due to language barriers, different specifications of formats and varieties of categorisations. Many more hindrances can be identified.
If data is interpreted differently, collaboration is limited, takes longer and is not efficient. For instance if a citizen of country A wants to purchase land in country B, the person will be asked to submit the proper address data. Address data in both countries include: Full name details, street name and number as well as a post code. The order of the address details might vary. In the same language it is not an obstacle to order the provided address data; but across language barriers it becomes more and more difficult. If the language requires other characters it is almost impossible, if no translation tools are available.
Hence eGovernment applications need to exchange data in a semantically interoperable manner. This saves time and money and reduces sources of errors. Fields of practical use are found in every policy area, be it justice, trade or participation etc. Clear concepts of interpretation patterns are required.
Public safety.
Interoperability is an important issue for law enforcement, fire fighting, EMS, and other public health and safety departments, because first responders need to be able to communicate during wide-scale emergencies. It has been a major area of investment and research over the last 12 years. Traditionally, agencies could not exchange information because they operated widely disparate hardware that was incompatible. Agencies' information systems such as computer-aided dispatch systems (CAD) and records management systems (RMS) functioned largely in isolation, so-called "information islands." Agencies tried to bridge this isolation with inefficient, stop-gap methods while large agencies began implementing limited interoperable systems. These approaches were inadequate and, in the U.S.A., the lack of interoperability in the public safety realm become evident during the 9/11 attacks on the Pentagon and World Trade Center structures. Further evidence of a lack of interoperability surfaced when agencies tackled the aftermath of the Hurricane Katrina disaster.
In contrast to the overall national picture, some states, including Utah, have already made great strides forward. The Utah Highway Patrol and other departments in Utah have created a statewide data-sharing network using technology from a company based in Bountiful, Utah, .
The Commonwealth of Virginia is one of the leading states in the United States when it comes to improving interoperability and is continually recognized as a National Best Practice by Department of Homeland Security (DHS). Virginia's proven practitioner-driven Governance Structure ensures that all the right players are involved in decision making, training & exercises, planning efforts, etc. The Interoperability Coordinator leverages a regional structure to better allocate grant funding around the Commonwealth so that all areas have an opportunity to improve communications interoperability. Virginia's strategic plan for communications is updated yearly to include new initiatives for the Commonwealth – all projects and efforts are tied to this plan, which is aligned with the National Emergency Communications Plan, authored by Department of Homeland Security's Office of Emergency Communications (OEC).
The seeks to enhance interoperability statewide. The (SIEC), established by the legislature in 2003, works to assist emergency responder agencies (police, fire, sheriff, medical, hazmat, etc.) at all levels of government (city, county, state, tribal, federal) to define interoperability for their local region.
Washington recognizes collaborating on system design and development for wireless radio systems enables emergency responder agencies to efficiently provide additional services, increase interoperability, and reduce long-term costs.
This important work saves the lives of emergency personnel and the citizens they serve.
The U.S. government is making a concerted effort to overcome the nation's lack of public safety interoperability. The Department of Homeland Security's Office for Interoperability and Compatibility (OIC) is pursuing the and CADIP programs, which are designed to help agencies as they integrate their CAD and other IT systems.
The OIC launched CADIP in August 2007. This project will partner the OIC with agencies in several locations, including Silicon Valley. This program will use case studies to identify the best practices and challenges associated with linking CAD systems across jurisdictional boundaries. These lessons will create the tools and resources public safety agencies can use to build interoperable CAD systems and communicate across local, state, and federal boundaries.
Forces.
Force interoperability is defined in NATO as the ability of the forces of two or more nations to train, exercise and operate effectively together in the execution of assigned missions and tasks. Additionally NATO defines interoperability more generally as the ability to act together coherently, effectively and efficiently to achieve Allied tactical, operational and strategic objectives.
At the strategic level, interoperability is an enabler for coalition building. It facilitates meaningful contributions by coalition partners. At this level, interoperability issues center on harmonizing the world views, strategies, doctrines, and force structures. Interoperability is an element of coalition willingness to work together over the long term to achieve and maintain shared interests against common threats. Interoperability at the operational and tactical levels is where strategic/political interoperability and technological interoperability come together to help allies shape the environment, manage crises, and win wars. The benefits of interoperability at the operational and tactical levels generally derive from the fungibility or interchangeability of force elements and units. Technological interoperability reflects the interfaces between organizations and systems. It focuses on communications and computers but also involves the technical capabilities of systems and the resulting mission compatibility or incompatibility between the systems and data of coalition partners. At the technological level, the benefits of interoperability come primarily from their impacts at the operational and tactical levels in terms of enhancing fungibility and flexibility.
Achieving software interoperability.
Software Interoperability is achieved through five interrelated ways:
Each of these has an important role in reducing variability in intercommunication software and enhancing a common understanding of the end goal to be achieved.
Interoperability as a question of power and market dominance.
Interoperability tends to be regarded as an issue for experts and its implications for daily living are sometimes underrated. The European Union Microsoft competition case shows how interoperability concerns important questions of power relationships. In 2004, the European Commission found that Microsoft had abused its market power by deliberately restricting interoperability between Windows work group servers and non-Microsoft work group servers. By doing so, Microsoft was able to protect its dominant market position for work group server operating systems, the heart of corporate IT networks. Microsoft was ordered to disclose complete and accurate interface documentation, which will enable rival vendors to compete on an equal footing (“the interoperability remedy”). As of June 2005 the Commission is market testing a new proposal by Microsoft to do this, having rejected previous proposals as insufficient.
Interoperability has also surfaced in the Software patent debate in the European Parliament (June/July 2005). Critics claim that because patents on techniques required for interoperability are kept under RAND (reasonable and non discriminatory licensing) conditions, customers will have to pay license fees twice: once for the product and, in the appropriate case, once for the patent protected programme the product uses.
Railways.
Railways have greater or lesser interoperability depending on conforming to standards of gauge, couplings, brakes, signalling, communications, loading gauge, structure gauge, and operating rules, to mention a few parameters. For passenger rail service, different railway platform height and width clearance standards may also cause interoperability problems.
North American freight railroads are highly interoperable, but systems in Europe, Asia, Africa, Central and South America, and Australia are much less so. The parameter most difficult to overcome (at reasonable cost) is incompatibility of gauge, though variable gauge axle systems such as the SUW 2000 are starting to be used.

</doc>
<doc id="41286" url="http://en.wikipedia.org/wiki?curid=41286" title="Interposition trunk">
Interposition trunk

In telecommunication, the term interposition trunk has the following meanings: 
1. A single direct transmission channel, "e.g.," voice-frequency circuit, between two positions of a large switchboard to facilitate the interconnection of other circuits appearing at the respective switchboard positions. 
2. Within a technical control facility, a single direct transmission circuit, between positions in a testboard or patch bay, which circuit facilitates testing or patching between the respective positions. 
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41287" url="http://en.wikipedia.org/wiki?curid=41287" title="Intersymbol interference">
Intersymbol interference

In telecommunication, intersymbol interference (ISI) is a form of distortion of a signal in which one symbol interferes with subsequent symbols. This is an unwanted phenomenon as the previous symbols have similar effect as noise, thus making the communication less reliable. ISI is usually caused by multipath propagation or the inherent non-linear frequency response of a channel causing successive symbols to "blur" together.
The presence of ISI in the system introduces errors in the decision device at the receiver output. Therefore, in the design of the transmitting and receiving filters, the objective is to minimize the effects of ISI, and thereby deliver the digital data to its destination with the smallest error rate possible.
Ways to fight intersymbol interference include adaptive equalization and error correcting codes.
Causes.
Multipath propagation.
One of the causes of intersymbol interference is what is known as multipath propagation in which a wireless signal from a transmitter reaches the receiver via many different paths. The causes of this include reflection (for instance, the signal may bounce off buildings), refraction (such as through the foliage of a tree) and atmospheric effects such as atmospheric ducting and ionospheric reflection. Since the various paths are of different lengths, this results in the different versions of the signal arriving at the receiver at different times. These delays mean that part or all of a given symbol will be spread into the subsequent symbols, thereby interfering with the correct detection of those symbols. Additionally, the various paths often distort the amplitude and/or phase of the signal thereby causing further interference with the received signal.
Bandlimited channels.
Another cause of intersymbol interference is the transmission of a signal through a bandlimited channel, i.e., one where the frequency response is zero above a certain frequency (the cutoff frequency). Passing a signal through such a channel results in the removal of frequency components above this cutoff frequency; in addition, components of the frequency below the cutoff frequency may also be attenuated by the channel.
This filtering of the transmitted signal affects the shape of the pulse that arrives at the receiver. The effects of filtering a rectangular pulse; not only change the shape of the pulse within the first symbol period, but it is also spread out over the subsequent symbol periods. When a message is transmitted through such a channel, the spread pulse of each individual symbol will interfere with following symbols.
Bandlimited channels are present in both wired and wireless communications. The limitation is often imposed by the desire to operate multiple independent signals through the same area/cable; due to this, each system is typically allocated a piece of the total bandwidth available. For wireless systems, they may be allocated a slice of the electromagnetic spectrum to transmit in (for example, FM radio is often broadcast in the 87.5 MHz - 108 MHz range). This allocation is usually administered by a government agency; in the case of the United States this is the Federal Communications Commission (FCC). In a wired system, such as an optical fiber cable, the allocation will be decided by the owner of the cable.
The bandlimiting can also be due to the physical properties of the medium - for instance, the cable being used in a wired system may have a cutoff frequency above which practically none of the transmitted signal will propagate.
Communication systems that transmit data over bandlimited channels usually implement pulse shaping to avoid interference caused by the bandwidth limitation. If the channel frequency response is flat and the shaping filter has a finite bandwidth, it is possible to communicate with no ISI at all. Often the channel response is not known beforehand, and an adaptive equalizer is used to compensate the frequency response.
Effects on eye patterns.
One way to study ISI in a PCM or data transmission system experimentally is to apply the received wave to the vertical deflection plates of an oscilloscope and to apply a sawtooth wave at the transmitted symbol rate R (R = 1/T) to the horizontal deflection plates. The resulting display is called an eye pattern because of its resemblance to the human eye for binary waves. The interior region of the eye pattern is called the eye opening. An eye pattern provides a great deal of information about the performance of the pertinent system.
An eye pattern, which overlays many samples of a signal, can give a graphical representation of the
signal characteristics. The first image below is the eye pattern for a binary phase-shift keying (PSK) system in which a one is represented by an amplitude of -1 and a zero by an amplitude of +1. The current sampling time is at the center of the image and the previous and next sampling times are at the edges of the image. The various transitions from one sampling time to another (such as one-to-zero, one-to-one and so forth) can clearly be seen on the diagram.
The noise margin - the amount of noise required to cause the receiver to get an error - is given by the distance between the signal and the zero amplitude point at the sampling time; in other words, the further from zero at the sampling time the signal is the better. For the signal to be correctly interpreted, it must be sampled somewhere between the two points where the zero-to-one and one-to-zero transitions cross. Again, the further apart these points are the better, as this means the signal will be less sensitive to errors in the timing of the samples at the receiver.
The effects of ISI are shown in the second image which is an eye pattern of the same system when operating over a multipath channel. The effects of receiving delayed and distorted versions of the signal can be seen in the loss of definition of the signal transitions. It also reduces both the noise margin and the window in which the signal can be sampled, which shows that the performance of the system will be worse (i.e. it will have a greater bit error ratio).
Countering ISI.
There are several techniques in telecommunication and data storage that try to work around the problem of intersymbol interference.
Intentional Intersymbol interference.
There exist also coded modulation systems that intentionally builds a controlled amount of ISI into the system at the transmitter side, this is known under the name Faster-than-Nyquist Signaling. Such a design trades a computational complexity penalty at the receiver against a Shannon capacity gain of the overall transceiver system. See for a recent survey of this technique.

</doc>
<doc id="41288" url="http://en.wikipedia.org/wiki?curid=41288" title="Inverse-square law">
Inverse-square law

In physics, an inverse-square law is any physical law stating that a specified physical quantity or intensity is inversely proportional to the square of the distance from the source of that physical quantity. Mathematically formulated:
The divergence of a vector field which is the resultant of radial inverse-square law fields with respect to one or more sources is everywhere proportional to the strength of the local sources, and hence zero outside sources. Newton's law of universal gravitation follows an inverse-square law, as do the effects of electric, magnetic, light, sound, and radiation phenomena.
Justification.
The inverse-square law generally applies when some force, energy, or other conserved quantity is evenly radiated outward from a point source in three-dimensional space. Since the surface area of a sphere (which is 4π"r"2 ) is proportional to the square of the radius, as the emitted radiation gets farther from the source, it is spread out over an area that is increasing in proportion to the square of the distance from the source. Hence, the intensity of radiation passing through any unit area (directly facing the point source) is inversely proportional to the square of the distance from the point source. Gauss's law applies to, and can be used with any physical quantity that acts in accord to the inverse-square relationship.
Occurrences.
Gravitation.
Gravitation is the attraction of two objects with mass. Newton's law states:
If the distribution of matter in each body is spherically symmetric, then the objects can be treated as point masses without approximation, as shown in the shell theorem. Otherwise, if we want to calculate the attraction between massive bodies, we need to add all the point-point attraction forces vectorially and the net attraction might not be exact inverse square. However, if the separation between the massive bodies is much larger compared to their sizes, then to a good approximation, it is reasonable to treat the masses as point mass while calculating the gravitational force.
As the law of gravitation, this law was suggested in 1645 by Ismael Bullialdus. But Bullialdus did not accept Kepler’s second and third laws, nor did he appreciate Christiaan Huygens’s solution for circular motion (motion in a straight line pulled aside by the central force). Indeed, Bullialdus maintained the sun's force was attractive at aphelion and repulsive at perihelion. Robert Hooke and Giovanni Alfonso Borelli both expounded gravitation in 1666 as an attractive force (Hooke’s lecture "On gravity" at the Royal Society, London, on 21 March; Borelli's "Theory of the Planets", published later in 1666). Hooke's 1670 Gresham lecture explained that gravitation applied to "all celestiall bodys" and added the principles that the gravitating power decreases with distance and that in the absence of any such power bodies move in straight lines. By 1679, Hooke thought gravitation had inverse square dependence and communicated this in a letter to Isaac Newton. Hooke remained bitter about Newton claiming the invention of this principle, even though Newton's "Principia" acknowledged that Hooke, along with Wren and Halley, had separately appreciated the inverse square law in the solar system, as well as giving some credit to Bullialdus.
Electrostatics.
The force of attraction or repulsion between two electrically charged particles, in addition to being directly proportional to the product of the electric charges, is inversely proportional to the square of the distance between them; this is known as Coulomb's law. The deviation of the exponent from 2 is less than one part in 1015.
Light and other electromagnetic radiation.
The intensity (or illuminance or irradiance) of light or other linear waves radiating from a point source (energy per unit of area perpendicular to the source) is inversely proportional to the square of the distance from the source; so an object (of the same size) twice as far away, receives only one-quarter the energy (in the same time period).
More generally, the irradiance, "i.e.," the intensity (or power per unit area in the direction of propagation), of a spherical wavefront varies inversely with the square of the distance from the source (assuming there are no losses caused by absorption or scattering).
For example, the intensity of radiation from the Sun is 9126 watts per square meter at the distance of Mercury (0.387 AU); but only 1367 watts per square meter at the distance of Earth (1 AU)—an approximate threefold increase in distance results in an approximate ninefold decrease in intensity of radiation.
For non-isotropic radiators such as parabolic antennas, headlights, and lasers, the effective origin is located far behind the beam aperture. If you are close to the origin, you don't have to go far to double the radius, so the signal drops quickly. When you are far from the origin and still have a strong signal, like with a laser, you have to travel very far to double the radius and reduce the signal. This means you have a stronger signal or have antenna gain in the direction of the narrow beam relative to a wide beam in all directions of an isotropic antenna.
In photography and theatrical lighting, the inverse-square law is used to determine the "fall off" or the difference in illumination on a subject as it moves closer to or further from the light source. For quick approximations, it is enough to remember that doubling the distance reduces illumination to one quarter; or similarly, to halve the illumination increase the distance by a factor of 1.4 (the square root of 2), and to double illumination, reduce the distance to 0.7 (square root of 1/2). When the illuminant is not a point source, the inverse square rule is often still a useful approximation; when the size of the light source is less than one-fifth of the distance to the subject, the calculation error is less than 1%.
The fractional reduction in electromagnetic fluence (Φ) for indirectly ionizing radiation with increasing distance from a point source can be calculated using the inverse-square law. Since emissions from a point source have radial directions, they intercept at a perpendicular incidence. The area of such a shell is 4π"r" 2 where "r" is the radial distance from the center. The law is particularly important in diagnostic radiography and radiotherapy treatment planning, though this proportionality does not hold in practical situations unless source dimensions are much smaller than the distance. As stated in fourier theory of heat "as the point source is magnification by distances , its radiation is dilute proportional to the sin of the angle, of the increasing circumference arc from the point of origin" 
Example.
Let the total power radiated from a point source, for example, an omnidirectional isotropic antenna, be "P". At large distances from the source (compared to the size of the source), this power is distributed over larger and larger spherical surfaces as the distance from the source increases. Since the surface area of a sphere of radius "r" is "A" = 4"πr" 2, then intensity "I" (power per unit area) of radiation at distance "r" is
The energy or intensity decreases (divided by 4) as the distance "r" is doubled; measured in dB it would decrease by 6.02 dB per doubling of distance.
Acoustics.
In acoustics one usually measures the sound pressure at a given distance "r" from the source using the 1/r law. Since intensity is proportional to the square of pressure amplitude, this is just a variation on the inverse-square law.
Example.
In acoustics, the sound pressure of a spherical wavefront radiating from a point source decreases by 50% as the distance "r" is doubled; measured in dB, the decrease is still 6.02 dB, since dB represents an intensity ratio. The behaviour is not inverse-square, but is inverse-proportional (inverse distance law):
The same is true for the component of particle velocity formula_4 that is in-phase with the instantaneous sound pressure formula_5:
In the near field is a quadrature component of the particle velocity that is 90° out of phase with the sound pressure and does not contribute to the time-averaged energy or the intensity of the sound. The sound intensity is the product of the RMS sound pressure and the "in-phase" component of the RMS particle velocity, both of which are inverse-proportional. Accordingly, the intensity follows an inverse-square behaviour:
Field theory interpretation.
For an irrotational vector field in three-dimensional space the inverse-square law corresponds to the property that the divergence is zero outside the source. This can be generalized to higher dimensions. Generally, for an irrotational vector field in "n"-dimensional Euclidean space, the intensity "I" of the vector field falls off with the distance "r" following the inverse ("n" − 1)th power law
given that the space outside the source is divergence free. 
History.
John Dumbleton of the 14th-century Oxford Calculators, was one of the first to express functional relationships in graphical form. He gave a proof of the mean speed theorem stating that "the latitude of a uniformly difform movement corresponds to the degree of the midpoint" and used this method to study the quantitative decrease in intensity of illumination in his "Summa logicæ et philosophiæ naturalis" (ca. 1349), stating that it was not linearly proportional to the distance, but was unable to expose the Inverse-square law.
In proposition 9 of Book 1 in his book "Ad Vitellionem paralipomena, quibus astronomiae pars optica traditur" (1604), the astronomer Johannes Kepler argued that the spreading of light from a point source obeys an inverse square law:
Original: "Sicut se habent spharicae superificies, quibus origo lucis pro centro est, amplior ad angustiorem: ita se habet fortitudo seu densitas lucis radiorum in angustiori, ad illamin in laxiori sphaerica, hoc est, conversim. Nam per 6. 7. tantundem lucis est in angustiori sphaerica superficie, quantum in fusiore, tanto ergo illie stipatior & densior quam hic."
"Translation": Just as [the ratio of] spherical surfaces, for which the source of light is the center, [is] from the wider to the narrower, so the density or fortitude of the rays of light in the narrower [space], towards the more spacious spherical surfaces, that is, inversely. For according to [propositions] 6 & 7, there is as much light in the narrower spherical surface, as in the wider, thus it is as much more compressed and dense here than there.
In 1645 in his book "Astronomia Philolaica" …, the French astronomer Ismaël Bullialdus (1605 – 1694) refuted Johannes Kepler's suggestion that "gravity" weakens as the inverse of the distance; instead, Bullialdus argued, "gravity" weakens as the inverse square of the distance:
Original: "Virtus autem illa, qua Sol prehendit seu harpagat planetas, corporalis quae ipsi pro manibus est, lineis rectis in omnem mundi amplitudinem emissa quasi species solis cum illius corpore rotatur: cum ergo sit corporalis imminuitur, & extenuatur in maiori spatio & intervallo, ratio autem huius imminutionis eadem est, ac luminus, in ratione nempe dupla intervallorum, sed eversa."
"Translation": As for the power by which the Sun seizes or holds the planets, and which, being corporeal, functions in the manner of hands, it is emitted in straight lines throughout the whole extent of the world, and like the species of the Sun, it turns with the body of the Sun; now, seeing that it is corporeal, it becomes weaker and attenuated at a greater distance or interval, and the ratio of its decrease in strength is the same as in the case of light, namely, the duplicate proportion, but inversely, of the distances [that is, 1/d²].
In England, the Anglican bishop Seth Ward (1617 – 1689) publicized the ideas of Bullialdus in his critique "In Ismaelis Bullialdi astronomiae philolaicae fundamenta inquisitio brevis" (1653) and publicized the planetary astronomy of Kepler in his book "Astronomia geometrica" (1656).
In 1663-1664, the English scientist Robert Hooke was writing his book "Micrographia" (1666) in which he discussed, among other things, the relation between the height of the atmosphere and the barometric pressure at the surface. Since the atmosphere surrounds the earth, which itself is a sphere, the volume of atmosphere bearing on any unit area of the earth's surface is a truncated cone (which extends from the earth's center to the vacuum of space ; obviously only the section of the cone from the earth's surface to space bears on the earth's surface). Although the volume of a cone is proportional to the cube of its height, Hooke argued that the air's pressure at the earth's surface is instead proportional to the height of the atmosphere because gravity diminishes with altitude. Although Hooke did not explicitly state so, the relation that he proposed would be true only if gravity decreases as the inverse square of the distance from the earth's center.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41289" url="http://en.wikipedia.org/wiki?curid=41289" title="Ionospheric reflection">
Ionospheric reflection

Ionospheric reflection is a bending, through a complex process involving reflection and refraction, of electromagnetic waves propagating in the ionosphere back toward the Earth. 
The amount of bending depends on the extent of penetration (which is a function of frequency), the angle of incidence, polarization of the wave, and ionospheric conditions, such as the ionization density. It is negatively affected by incidents of ionospheric absorption.
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41290" url="http://en.wikipedia.org/wiki?curid=41290" title="Ionospheric sounding">
Ionospheric sounding

In telecommunication and radio science, an ionospheric sounding is a technique that provides real-time data on high-frequency ionospheric-dependent radio propagation, using a basic system consisting of a synchronized transmitter and receiver. 
The time delay between transmission and reception is translated into effective ionospheric layer altitude. Vertical incident sounding uses a collocated transmitter and receiver and involves directing a range of frequencies vertically to the ionosphere and measuring the values of the reflected returned signals to determine the effective ionosphere layer altitude. This technique is also used to determine the critical frequency. Oblique sounders use a transmitter at one end of a given propagation path, and a synchronized receiver, usually with an oscilloscope-type display (ionogram), at the other end. The transmitter emits a stepped- or swept-frequency signal which is displayed or measured at the receiver. The measurement converts time delay to effective altitude of the ionospheric layer. The ionogram display shows the effective altitude of the ionospheric layer as a function of frequency.

</doc>
<doc id="41291" url="http://en.wikipedia.org/wiki?curid=41291" title="Isochronous">
Isochronous

A sequence of events is isochronous if the events occur regularly, or at equal time intervals. The term "isochronous" is used in several technical contexts, but usually refers to the primary subject maintaining a constant period or interval (the reciprocal of frequency), despite variations in other measurable factors in the same system.
Isochronous timing differs from synchronous timing, in that the latter refers to relative timing between two or more sequences of events.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41292" url="http://en.wikipedia.org/wiki?curid=41292" title="Isochronous burst transmission">
Isochronous burst transmission

Isochronous burst transmission is a method of data transmission. In a data network where the information-bearer channel rate is higher than the input data signaling rate, transmission is performed by interrupting, at controlled intervals, the data stream being transmitted. 
"Note 1:" Burst transmission in isochronous form enables communication between data terminal equipment (DTE) and data networks that operate at dissimilar data signaling rates, such as when the information-bearer channel rate is higher than the DTE output data signaling rate. 
"Note 2:" The binary digits are transferred at the information-bearer channel rate. The data transfer is interrupted at intervals in order to produce the required average data signaling rate. 
"Note 3:" The interruption is always for an integral number of unit intervals. 
"Note 4:" Isochronous burst transmission has particular application where envelopes are being transferred between data circuit terminating equipment (DCE) and only the bytes contained within the envelopes are being transferred between the DCE and the DTE. "Synonyms": burst isochronous "(deprecated)", interrupted isochronous transmission.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41293" url="http://en.wikipedia.org/wiki?curid=41293" title="Isochronous signal">
Isochronous signal

In telecommunication, an isochronous signal is a signal in which the time interval separating any two significant instants is equal to the unit interval or a multiple of the unit interval. Variations in the time intervals are constrained within specified limits.
"Isochronous" is a characteristic of one signal, while "synchronous" indicates a relationship between two or more signals.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41295" url="http://en.wikipedia.org/wiki?curid=41295" title="Jerkiness">
Jerkiness

Jerkiness, sometimes called strobing or choppy, describes the perception of individual still images in a motion picture.
Motion pictures are made from still images shown in rapid sequence. Provided there is sufficient continuity between the images and provided the sequence is shown fast enough, the central nervous system interprets the sequence as continuous motion.
In conventional cinematography, the images are filmed and displayed at 24 frames per second, at which speed jerkiness is not normally discernible.
Television screens refresh at even higher frequencies. PAL and SÉCAM television (the standards in Europe) refresh at 25 or 50 (HDTV) frames per second. NTSC television displays (the standard in North America) refresh at 29.97 frames per second.
However, some technologies cannot process or carry data fast enough for sufficiently high frame rates. For example, viewing motion pictures by Internet connection generally necessitates a greatly reduced frame rate, making jerkiness clearly apparent.
Animated cartoon films are typically made at reduced frame rates (accomplished by shooting several film frames of the individual drawings) so as to limit production costs, with the result that jerkiness tends to be apparent.
Strobing can also refer to cross colour and Moiré patterning:
The former is where any high frequency luminance content of the picture, close to the TV systems colour sub-carrier frequency, is interpreted by the analogue receiver's decoder as colour information.
Moiré is where an interference pattern is produced by fine scene detail beating with the line (or even pixel) structure of the device used to analyse or display the scene.

</doc>
<doc id="41296" url="http://en.wikipedia.org/wiki?curid=41296" title="Jitter">
Jitter

Jitter is the deviation from true periodicity of a presumed periodic signal in electronics and telecommunications, often in relation to a reference clock source. Jitter may be observed in characteristics such as the frequency of successive pulses, the signal amplitude, or phase of periodic signals. Jitter is a significant, and usually undesired, factor in the design of almost all communications links (e.g., USB, PCI-e, SATA, OC-48). In clock recovery applications it is called "timing jitter".
Jitter can be quantified in the same terms as all time-varying signals, e.g., root mean square (RMS), or peak-to-peak displacement. Also like other time-varying signals, jitter can be expressed in terms of spectral density (frequency content).
"Jitter period" is the interval between two times of maximum effect (or minimum effect) of a signal characteristic that varies regularly with time. "Jitter frequency", the more commonly quoted figure, is its inverse. ITU-T G.810 classifies jitter frequencies below 10 Hz as wander and frequencies at or above 10 Hz as jitter.
Jitter may be caused by electromagnetic interference (EMI) and crosstalk with carriers of other signals. Jitter can cause a display monitor to flicker, affect the performance of processors in personal computers, introduce clicks or other undesired effects in audio signals, and loss of transmitted data between network devices. The amount of tolerable jitter depends on the affected application.
Sampling jitter.
In analog to digital and digital to analog conversion of signals, the sampling is normally assumed to be periodic with a fixed period—the time between every two samples is the same. If there is jitter present on the clock signal to the analog-to-digital converter or a digital-to-analog converter, the time between samples varies and instantaneous signal error arises. The error is proportional to the slew rate of the desired signal and the absolute value of the clock error. Various effects such as noise (random jitter), or spectral components (periodic jitter) can come about depending on the pattern of the jitter in relation to the signal. In some conditions, less than a nanosecond of jitter can reduce the effective bit resolution of a converter with a Nyquist frequency of 22 kHz to 14 bits.
This is a consideration in high-frequency signal conversion, or where the clock signal is especially prone to interference.
Packet jitter in computer networks.
In the context of computer networks, jitter is the variation in latency as measured in the variability over time of the packet latency across a network. A network with constant latency has no variation (or jitter). Packet jitter is expressed as an average of the deviation from the network mean latency. However, for this use, the term is imprecise. The standards-based term is "packet delay variation" (PDV). PDV is an important quality of service factor in assessment of network performance.
Compact disc seek jitter.
In the context of digital audio extraction from compact discs, seek jitter causes extracted audio samples to be doubled-up or skipped entirely if the Compact Disc drive re-seeks. The problem occurs because the Red Book does not require block-accurate addressing during seeking. As a result, the extraction process may restart a few samples early or late, resulting in doubled or omitted samples. These glitches often sound like tiny repeating clicks during playback. A successful approach to correction in software involves performing overlapping reads and fitting the data to find overlaps at the edges. Most extraction programs perform seek jitter correction. CD manufacturers avoid seek jitter by extracting the entire disc in one continuous read operation, using special CD drive models at slower speeds so the drive does not re-seek.
A jitter meter is a testing instrument for measuring clock jitter values, and is used in manufacturing DVD and CD-ROM discs.
Due to additional sector level addressing added in the Yellow Book, CD-ROM data discs are not subject to seek jitter.
Jitter metrics.
For clock jitter, there are three commonly used metrics: "absolute jitter", "period jitter", and "cycle to cycle jitter".
Absolute jitter is the absolute difference in the position of a clock's edge from where it would ideally be.
Period jitter (aka "cycle jitter") is the difference between any one clock period and the ideal/average clock period. Accordingly, it can be thought of as the discrete-time derivative of absolute jitter. Period jitter tends to be important in synchronous circuitry like digital state machines where the error-free operation of the circuitry is limited by the shortest possible clock period, and the performance of the circuitry is limited by the average clock period. Hence, synchronous circuitry benefits from minimizing period jitter, so that the shortest clock period approaches the average clock period.
Cycle-to-cycle jitter is the difference in length/duration of any two adjacent clock periods. Accordingly, it can be thought of as the discrete-time derivative of period jitter. It can be important for some types of clock generation circuitry used in microprocessors and RAM interfaces.
Since they have different generation mechanisms, different circuit effects, and different measurement methodology, it is useful to quantify them separately.
In telecommunications, the unit used for the above types of jitter is usually the "Unit Interval" (abbreviated "UI") which quantifies the jitter in terms of a fraction of the ideal period of a bit. This unit is useful because it scales with clock frequency and thus allows relatively slow interconnects such as T1 to be compared to higher-speed internet backbone links such as OC-192. Absolute units such as "picoseconds" are more common in microprocessor applications. Units of "degrees" and "radians" are also used.
If jitter has a Gaussian distribution, it is usually quantified using the standard deviation of this distribution (aka. "RMS"). Often, jitter distribution is significantly non-Gaussian. This can occur if the jitter is caused by external sources such as power supply noise. In these cases, "peak-to-peak" measurements are more useful. Many efforts have been made to meaningfully quantify distributions that are neither Gaussian nor have meaningful peaks (which is the case in all real jitter). All have shortcomings but most tend to be good enough for the purposes of engineering work. Note that typically, the reference point for jitter is defined such that the "mean" jitter is 0.
In networking, in particular IP networks such as the Internet, jitter can refer to the variation (statistical dispersion) in the delay of the packets.
Types.
Random jitter.
Random Jitter, also called Gaussian jitter, is unpredictable electronic timing noise. Random jitter typically follows a Gaussian distribution or Normal distribution. It is believed to follow this pattern because most noise or jitter in an electrical circuit is caused by thermal noise, which has a Gaussian distribution. Another reason for random jitter to have a distribution like this is due to the central limit theorem. The central limit theorem states that composite effect of many uncorrelated noise sources, regardless of the distributions, approaches a Gaussian distribution.
One of the main differences between random and deterministic jitter is that deterministic jitter is bounded and random jitter is unbounded.
Deterministic jitter.
Deterministic jitter is a type of clock timing jitter or data signal jitter that is predictable and reproducible. The peak-to-peak value of this jitter is bounded, and the bounds can easily be observed and predicted. Deterministic jitter can either be correlated to the data stream (data-dependent jitter) or uncorrelated to the data stream (bounded uncorrelated jitter). Examples of data-dependent jitter are duty-cycle dependent jitter (also known as duty-cycle distortion) and intersymbol interference.
Deterministic jitter (or DJ) has a known non-Gaussian probability distribution. 
Total jitter.
Total jitter ("T") is the combination of random jitter ("R") and deterministic jitter ("D"):
in which the value of "n" is based on the bit error rate (BER) required of the link.
A common bit error rate used in communication standards such as Ethernet is 10−12.
Testing.
Testing for jitter and its measurement is of growing importance to electronics engineers because of increased clock frequencies in digital electronic circuitry to achieve higher device performance. Higher clock frequencies have commensurately smaller eye openings, and thus impose tighter tolerances on jitter. For example, modern computer motherboards have serial bus architectures with eye openings of 160 picoseconds or less. This is extremely small compared to parallel bus architectures with equivalent performance, which may have eye openings on the order of 1000 picoseconds.
Testing of device performance for jitter tolerance often involves the injection of jitter into electronic components with specialized test equipment.
Jitter is measured and evaluated in various ways depending on the type of circuitry under test. For example, jitter in serial bus architectures is measured by means of eye diagrams, according to industry accepted standards. A less direct approach—in which analog waveforms are digitized and the resulting data stream analyzed—is employed when measuring pixel jitter in frame grabbers. In all cases, the goal of jitter measurement is to verify that the jitter will not disrupt normal operation of the circuitry.
There are standards for jitter measurement in serial bus architectures. The standards cover jitter tolerance, jitter transfer function and jitter generation, with the required values for these attributes varying among different applications. Where applicable, compliant systems are required to conform to these standards.
Mitigation.
Anti-jitter circuits.
Anti-jitter circuits (AJCs) are a class of electronic circuits designed to reduce the level of jitter in a regular pulse signal. AJCs operate by re-timing the output pulses so they align more closely to an idealised pulse signal. They are widely used in clock and data recovery circuits in digital communications, as well as for data sampling systems such as the analog-to-digital converter and digital-to-analog converter. Examples of anti-jitter circuits include phase-locked loop and delay-locked loop. Inside digital to analog converters jitter causes unwanted high-frequency distortions. In this case it can be suppressed with high fidelity clock signal usage.
Jitter buffers.
Jitter buffers or de-jitter buffers are used to counter jitter introduced by queuing in packet switched networks so that a continuous playout of audio (or video) transmitted over the network can be ensured. The maximum jitter that can be countered by a de-jitter buffer is equal to the buffering delay introduced before starting the play-out of the mediastream. In the context of packet-switched networks, the term "packet delay variation" is often preferred over "jitter".
Some systems use sophisticated delay-optimal de-jitter buffers that are capable of adapting the buffering delay to changing network jitter characteristics. These are known as adaptive de-jitter buffers and the adaptation logic is based on the jitter estimates computed from the arrival characteristics of the media packets. Adaptive de-jittering involves introducing discontinuities in the media play-out, which may appear offensive to the listener or viewer. Adaptive de-jittering is usually carried out for audio play-outs that feature a VAD/DTX encoded audio, that allows the lengths of the silence periods to be adjusted, thus minimizing the perceptual impact of the adaptation.
Dejitterizer.
A dejitterizer is a device that reduces jitter in a digital signal. A dejitterizer usually consists of an elastic buffer in which the signal is temporarily stored and then retransmitted at a rate based on the average rate of the incoming signal. A dejitterizer is usually ineffective in dealing with low-frequency jitter, such as waiting-time jitter.
Filtering.
A filter can be designed to minimize the effect of sampling jitter. For more information, see the paper by S. Ahmed and T. Chen entitled, "Minimizing the effects of sampling jitters in wireless sensors networks".
Video and image jitter.
Video or image jitter occurs when the horizontal lines of video image frames are randomly displaced due to the corruption of synchronization signals or electromagnetic interference during video transmission. Model based dejittering study has been carried out under the framework of digital image/video restoration.

</doc>
<doc id="41297" url="http://en.wikipedia.org/wiki?curid=41297" title="Joint multichannel trunking and switching system">
Joint multichannel trunking and switching system

The Joint multichannel trunking and switching system is that composite multichannel trunking and switching system formed from assets of the Services, the Defense Communications System, other available systems, and/or assets controlled by the Joint Chiefs of Staff to provide an operationally responsive, survivable communication system, preferably in a mobile/transportable/recoverable configuration, for the joint force commander in an area of operations. 

</doc>
<doc id="41298" url="http://en.wikipedia.org/wiki?curid=41298" title="Journal">
Journal

A journal (through French from Latin "diurnalis", daily) has several related meanings:
The word "journalist", for one whose business is writing for the public press and nowadays also other media, has been in use since the end of the 17th century.
Public journal.
A public journal is a record of day-by-day events in a parliament or congress. It is also called minutes or records.
Business and accounting.
The term "journal" is also used in business: 

</doc>
<doc id="41299" url="http://en.wikipedia.org/wiki?curid=41299" title="Justify">
Justify

Justify may refer to:
In music:

</doc>
<doc id="41300" url="http://en.wikipedia.org/wiki?curid=41300" title="Kendall effect">
Kendall effect

In telecommunications the Kendall effect is a spurious pattern or other distortion in a facsimile.
It is caused by unwanted modulation products which arise from the transmission of the carrier signal, and appear in the form of a rectified baseband that interferes with the lower sideband of the carrier. 
The Kendall effect occurs principally when the single-sideband width is greater than half of the facsimile carrier frequency.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41303" url="http://en.wikipedia.org/wiki?curid=41303" title="K-factor">
K-factor

K-factor or K factor may refer to:
In telecommunications:

</doc>
<doc id="41304" url="http://en.wikipedia.org/wiki?curid=41304" title="Knife-edge effect">
Knife-edge effect

In electromagnetic wave propagation, the knife-edge effect or edge diffraction is a redirection by diffraction of a portion of the incident radiation that strikes a well-defined obstacle such as a mountain range or the edge of a building.
The knife-edge effect is explained by Huygens–Fresnel principle, which states that a well-defined obstruction to an electromagnetic wave acts as a secondary source, and creates a new wavefront. This new wavefront propagates into the geometric shadow area of the obstacle.
The knife-edge effect is an outgrowth of the half-plane problem, originally solved by Arnold Sommerfeld using a plane wave spectrum formulation. A generalization of the halfplane problem is the wedge problem, solvable as a boundary value problem in cylindrical coordinates. The solution in cylindrical coordinates was then extended to the optical regime by Joseph B. Keller, who introduced the notion of diffraction coefficients through his geometrical theory of diffraction (GTD). Pathak and Kouyoumjian extended the (singular) Keller coefficients via the uniform theory of diffraction (UTD).
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41305" url="http://en.wikipedia.org/wiki?curid=41305" title="Label (disambiguation)">
Label (disambiguation)

A label is any kind of tag attached to something so as to identify the object or its content.
Label may also refer to:

</doc>
<doc id="41306" url="http://en.wikipedia.org/wiki?curid=41306" title="Lambert's cosine law">
Lambert's cosine law

In optics, Lambert's cosine law says that the radiant intensity or luminous intensity observed from an ideal diffusely reflecting surface or ideal diffuse radiator is directly proportional to the cosine of the angle "θ" between the direction of the incident light and the surface normal. The law is also known as the cosine emission law or Lambert's emission law. It is named after Johann Heinrich Lambert, from his "Photometria", published in 1760.
A surface which obeys Lambert's law is said to be "Lambertian", and exhibits Lambertian reflectance. Such a surface has the same radiance when viewed from any angle. This means, for example, that to the human eye it has the same apparent brightness (or luminance). It has the same radiance because, although the emitted power from a given area element is reduced by the cosine of the emission angle, the apparent size (solid angle) of the observed area, as seen by a viewer, is decreased by a corresponding amount. Therefore, its radiance (power per unit solid angle per unit projected source area) is the same.
Lambertian scatterers and radiators.
When an area element is radiating as a result of being illuminated by an external source, the irradiance (energy or photons/time/area) landing on that area element will be proportional to the cosine of the angle between the illuminating source and the normal. A Lambertian scatterer will then scatter this light according to the same cosine law as a Lambertian emitter. This means that although the radiance of the surface depends on the angle from the normal to the illuminating source, it will not depend on the angle from the normal to the observer. For example, if the moon were a Lambertian scatterer, one would expect to see its scattered brightness appreciably diminish towards the terminator due to the increased angle at which sunlight hit the surface. The fact that it does not diminish illustrates that the moon is not a Lambertian scatterer, and in fact tends to scatter more light into the oblique angles than would a Lambertian scatterer.
The emission of a Lambertian radiator does not depend upon the amount of incident radiation, but rather from radiation originating in the emitting body itself. For example, if the sun were a Lambertian radiator, one would expect to see a constant brightness across the entire solar disc. The fact that the sun exhibits limb darkening in the visible region illustrates that it is not a Lambertian radiator. A black body is an example of a Lambertian radiator.
Details of equal brightness effect.
The situation for a Lambertian surface (emitting or scattering) is illustrated in Figures 1 and 2. For conceptual clarity we will think in terms of photons rather than energy or luminous energy. The wedges in the circle each represent an equal angle "dΩ", and for a Lambertian surface, the number of photons per second emitted into each wedge is proportional to the area of the wedge.
It can be seen that the length of each wedge is the product of the diameter of the circle and cos("θ"). It can also be seen that the maximum rate of photon emission per unit solid angle is along the normal and diminishes to zero for "θ" = 90°. In mathematical terms, the radiance along the normal is "I" photons/(s·cm2·sr) and the number of photons per second emitted into the vertical wedge is "I" "dΩ" "dA". The number of photons per second emitted into the wedge at angle "θ" is "I" cos("θ") "dΩ" "dA".
Figure 2 represents what an observer sees. The observer directly above the area element will be seeing the scene through an aperture of area "dA"0 and the area element "dA" will subtend a (solid) angle of "dΩ"0. We can assume without loss of generality that the aperture happens to subtend solid angle "dΩ" when "viewed" from the emitting area element. This normal observer will then be recording "I" "dΩ" "dA" photons per second and so will be measuring a radiance of
The observer at angle "θ" to the normal will be seeing the scene through the same aperture of area "dA"0 and the area element "dA" will subtend a (solid) angle of "dΩ"0 cos("θ"). This observer will be recording "I" cos("θ") "dΩ" "dA" photons per second, and so will be measuring a radiance of
which is the same as the normal observer.
Relating peak luminous intensity and luminous flux.
In general, the luminous intensity of a point on a surface varies by direction; for a Lambertian surface, that distribution is defined by the cosine law, with peak luminous intensity in the normal direction. Thus when the Lambertian assumption holds, we can calculate the total luminous flux, formula_3, from the peak luminous intensity, formula_4, by integrating the cosine law:
and so
where formula_9 is the determinant of the Jacobian matrix for the unit sphere, and realizing that formula_4 is luminous flux per steradian. Similarly, the peak intensity will be formula_11 of the total radiated luminous flux. For Lambertian surfaces, the same factor of formula_12 relates luminance to luminous emittance, radiant intensity to radiant flux, and radiance to radiant emittance. Radians and steradians are, of course, dimensionless and so "rad" and "sr" are included only for clarity.
Example: A surface with a luminance of say 100 cd/m2 (= 100 nits, typical PC monitor) will, if it is a perfect Lambert emitter, have a luminous emittance of 314 lm/m2. If its area is 0.1 m2 (~19" monitor) then the total light emitted, or luminous flux, would thus be 31.4 lm.
Uses.
Lambert's cosine law in its reversed form (Lambertian reflection) implies that the apparent brightness of a Lambertian surface is proportional to the cosine of the angle between the surface normal and the direction of the incident light.
This phenomenon is, among others, used when creating mouldings, with the effect of creating light- and dark-shaded stripes on a structure or object without having to change the material or apply pigment. The contrast of dark and light areas gives definition to the object. Mouldings are strips of material with various cross-sections used to cover transitions between surfaces or for decoration.

</doc>
<doc id="41308" url="http://en.wikipedia.org/wiki?curid=41308" title="Launch angle">
Launch angle

In fiber optic telecommunications, the launch angle has the following meanings: 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41309" url="http://en.wikipedia.org/wiki?curid=41309" title="Launch numerical aperture">
Launch numerical aperture

In telecommunications, launch numerical aperture (LNA) is the numerical aperture of an optical system used to couple (launch) power into an optical fiber. 
LNA may differ from the stated NA of a final focusing element if, for example, that element is underfilled or the focus is other than that for which the element is specified. 
LNA is one of the parameters that determine the initial distribution of power among the modes of an optical fiber.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41310" url="http://en.wikipedia.org/wiki?curid=41310" title="Layer">
Layer

Layer may refer to:

</doc>
<doc id="41311" url="http://en.wikipedia.org/wiki?curid=41311" title="Layered system">
Layered system

In telecommunication, a layered system is a system in which components are grouped, "i.e.", layered, in a hierarchical arrangement, such that lower layers provide functions and services that support the functions and services of higher layers. 
Systems of ever-increasing complexity and capability can be built by adding or changing the layers to improve overall system capability while using the components that are still in place.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41312" url="http://en.wikipedia.org/wiki?curid=41312" title="Leaky mode">
Leaky mode

A leaky mode or tunneling mode in an optical fiber or other waveguide is a mode having an electric field that decays monotonically for a finite distance in the transverse direction but becomes oscillatory everywhere beyond that finite distance. Such a mode gradually "leaks" out of the waveguide as it travels down it, producing attenuation even if the waveguide is perfect in every respect. In order for a leaky mode to be definable as a mode, the relative amplitude of the oscillatory part (the leakage rate) must be sufficiently small that the mode substantially maintains its shape as it decays.
Leaky modes correspond to leaky rays in the terminology of geometric optics.
The propagation of light through optical fibre can take place via meridional rays or skew rays. These skew rays suffer only partial reflection while meridional rays are completely guided. Thus the modes allowing propagation of skew rays are called leaky modes. 
Some optical power is lost into clad due to these modes.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41314" url="http://en.wikipedia.org/wiki?curid=41314" title="Limiting">
Limiting

Limiting: Any process by which a specified characteristic (usually amplitude) of the output of a device is prevented from exceeding a predetermined value. 
Limiting can refer to non-linear clipping, in which a signal is passed through normally but "sheared off" when it would normally exceed a certain threshold. It can also refer to a type of variable-gain audio level compression, in which the gain of an amplifier is changed very quickly to prevent the signal from going over a certain amplitude.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41316" url="http://en.wikipedia.org/wiki?curid=41316" title="Linear polarization">
Linear polarization

In electrodynamics, linear polarization or plane polarization of electromagnetic radiation is a confinement of the electric field vector or magnetic field vector to a given plane along the direction of propagation. See polarization for more information.
The orientation of a linearly polarized electromagnetic wave is defined by the direction of the electric field vector. For example, if the electric field vector is vertical (alternately up and down as the wave travels) the radiation is said to be vertically polarized.
Mathematical description of linear polarization.
The classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is (cgs units)
for the magnetic field, where k is the wavenumber,
is the angular frequency of the wave, and formula_4 is the speed of light.
Here
is the amplitude of the field and
is the Jones vector in the x-y plane.
The wave is linearly polarized when the phase angles formula_7 are equal,
This represents a wave polarized at an angle formula_9 with respect to the x axis. In that case, the Jones vector can be written
The state vectors for linear polarization in x or y are special cases of this state vector.
If unit vectors are defined such that
and
then the polarization state can written in the "x-y basis" as
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41317" url="http://en.wikipedia.org/wiki?curid=41317" title="Line code">
Line code

In telecommunication, a line code (also called digital baseband modulation or digital baseband transmission method) is a code chosen for use within a communications system for baseband transmission purposes. Line coding is often used for digital data transport.
Line coding.
Line coding consists of representing the digital signal to be transported by an amplitude- and time-discrete signal that is optimally tuned for the specific properties of the physical channel (and of the receiving equipment). The waveform pattern of voltage or current used to represent the 1s and 0s of a digital data on a transmission link is called "line encoding". The common types of line encoding are unipolar, polar, bipolar, and Manchester encoding.
For reliable clock recovery at the receiver, one usually imposes a maximum run length constraint on the generated channel sequence, i.e., the maximum number of consecutive ones or zeros is bounded to a reasonable number. A clock period is recovered by observing transitions in the received sequence, so that a maximum run length guarantees such clock recovery, while sequences without such a constraint could seriously hamper the detection quality.
After line coding, the signal is put through a "physical channel", either a "transmission medium" or "data storage medium".
Sometimes the characteristics of two very different-seeming channels are similar enough that the same line code is used for them. The most common physical channels are:
Unfortunately, most long-distance communication channels cannot transport a DC component. The DC component is also called the disparity, the bias, or the DC coefficient. The simplest possible line code, called unipolar because it has an unbounded DC component, gives too many errors on such systems.
Most line codes eliminate the DC component – such codes are called DC-balanced, zero-DC, DC-free, zero-bias, DC equalized, etc.
There are three ways of eliminating the DC component:
Unfortunately, several long-distance communication channels have polarity ambiguity.
There are three ways of providing unambiguous reception of "0" bits or "1" bits over such channels:
Line coding should make it possible for the receiver to synchronize itself to the phase of the received signal. If the synchronization is not ideal, then the signal to be decoded will not have optimal differences (in amplitude) between the various digits or symbols used in the line code. This will increase the error probability in the received data.
It is also preferred for the line code to have a structure that will enable error detection. Note that the line-coded signal and a signal produced at a terminal may differ, thus requiring translation.
A line code will typically reflect technical requirements of the transmission medium, such as optical fiber or shielded twisted pair. These requirements are unique for each medium, because each one has different behavior related to interference, distortion, capacitance and loss of amplitude.
Common line codes.
Optical line codes:

</doc>
<doc id="41319" url="http://en.wikipedia.org/wiki?curid=41319" title="Link">
Link

Link usually refers to:
It can also refer to:

</doc>
<doc id="41320" url="http://en.wikipedia.org/wiki?curid=41320" title="Link level">
Link level

Link level: In the hierarchical structure of a primary or secondary station, the conceptual level of control or data processing logic that controls the data link. 
"Note:" Link-level functions provide an interface between the station high-level logic and the data link. Link-level functions include (a) transmit bit injection and receive bit extraction, (b) address and control field interpretation, (c) command response generation, transmission and interpretation, and (d) frame check sequence computation and interpretation.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41321" url="http://en.wikipedia.org/wiki?curid=41321" title="Link quality analysis">
Link quality analysis

In adaptive high-frequency (HF) radio, link quality analysis (LQA) is the overall process by which measurements of signal quality are made, assessed, and analyzed.
In LQA, signal quality is determined by measuring, assessing, and analyzing link parameters, such as bit error ratio (BER), and the levels of the ratio of signal-plus-noise-plus-distortion to noise-plus-distortion (SINAD). Measurements are stored at—and exchanged between—stations, for use in making decisions about link establishment.
For adaptive HF radio, LQA is automatically performed and is usually based on analyses of pseudo-BERs and SINAD readings.
References.
</dl>

</doc>
<doc id="41323" url="http://en.wikipedia.org/wiki?curid=41323" title="Load">
Load

 
Load may refer to:

</doc>
<doc id="41325" url="http://en.wikipedia.org/wiki?curid=41325" title="Loading characteristic">
Loading characteristic

Loading characteristic: In multichannel telephone systems, a plot, for the busy hour, of the equivalent mean power and the peak power as a function of the number of voice channels. 
The equivalent power of a multichannel signal referred to the zero transmission level point is a function of the number of channels and has for its basis a specified voice channel mean power.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41326" url="http://en.wikipedia.org/wiki?curid=41326" title="Loading coil">
Loading coil

A loading coil or load coil is an inductor that is inserted into an electronic circuit to increase its inductance. A loading coil is not a transformer to provide coupling to another other circuit. The term originated in the 19th century for inductors used to prevent signal distortion in long-distance telegraph transmission cables. The term is also used for inductors in radio antennas, or between the antenna and its feedline, to make an electrically short antenna resonant at its operating frequency.
Loading coils are historically also known as Pupin coils after Mihajlo Pupin, especially when used for the Heaviside condition and the process of inserting them is sometimes called "pupinization".
The concept of loading coils was discovered by Oliver Heaviside in studying the problem of slow signalling speed of the first transatlantic telegraph cable in the 1860s. He concluded additional inductance was required to prevent amplitude and time delay distortion of the transmitted signal. The mathematical condition for distortion-free transmission is known as the Heaviside condition. Previous telegraph lines were overland or shorter, hence had less delay and the need for extra inductance was not as great. Submarine communications cables are particularly subject to the problem, but early 20th century installations using balanced pairs were often continuously loaded with iron wire or tape rather than discretely with loading coils, which avoided the sealing problem.
Applications.
Telephone lines.
A common application of loading coils is to improve the voice-frequency amplitude response characteristics of the twisted balanced pairs in a telephone cable. Because twisted pair is a balanced format, half the loading coil must be inserted in each leg of the pair to maintain the balance. It is common for both these windings to be formed on the same core. This increases the flux linkages, without which the number of turns on the coil would need to be increased.
Loading coils inserted periodically in series with a pair of wires reduce the attenuation at the higher voice frequencies up to the cutoff frequency of the low-pass filter formed by the inductance of the coils (plus the distributed inductance of the wires) and the distributed capacitance between the wires. Above the cutoff frequency, attenuation increases rapidly. The shorter the distance between the coils, the higher the cut-off frequency.
It should be emphasised that the cutoff effect is an artifact of using lumped inductors. With loading methods using continuous distributed inductance there is no cutoff.
Without loading coils, the line response is dominated by the resistance and capacitance of the line with the attenuation gently increasing with frequency. With loading coils of exactly the right inductance, neither capacitance nor inductance dominate: the response is flat, waveforms are undistorted and the characteristic impedance is resistive up to the cutoff frequency. The coincidental formation of an audio frequency filter is also beneficial in that noise is reduced.
DSL.
When loading coils are in place, signal attenuation remains low for signals within the passband of the transmission line but increases rapidly for frequencies above the audio cutoff frequency. Thus, if the pair is subsequently reused to support applications that require higher frequencies (such as analog or digital carrier systems or DSL), any loading coils that were present on the line must be removed or replaced with ones which are transparent to DSL. Using coils with parallel capacitors will form a filter with the topology of an m-derived filter and a band of frequencies above the cut-off will also be passed.
If the coils are not removed, and the subscriber is an extended distance (e.g. over 4 miles or 6.4 km) from the Central Office, DSL can not be supported. This sometimes happens in dense, growing areas such as Southern California in the late 1990s and early 21st century.
Carrier systems.
American early and middle 20th century telephone cables had load coils at intervals of a mile (1.61 km), usually in coil cases holding many. The coils had to be removed to pass higher frequencies, but the coil cases provided convenient places for repeaters of digital T-carrier systems, which could then transmit a 1.5 Mbit/s signal that distance. Due to narrower streets and higher cost of copper, European cables had thinner wires and used closer spacing. Intervals of a kilometer allowed European systems to carry 2 Mbit/s.
Radio antenna.
Another type of loading coil is used in radio antennas. Monopole and dipole radio antennas are designed to act as resonators for radio waves; the power from the transmitter, applied to the antenna through the antenna's transmission line, excites standing waves of voltage and current in the antenna element. To be resonant, the antenna must have a physical length of one quarter of the wavelength of the radio waves used (or a multiple of that length). At resonance the antenna acts electrically as a pure resistance, absorbing all the power applied to it from the transmitter.
In many cases for practical reasons it is necessary to make the antenna shorter than the resonant length. An antenna shorter than a quarter wavelength presents capacitive reactance to the transmission line. Some of the applied power is reflected back into the transmission line and travels back toward the transmitter. This causes standing waves on the transmission line (a standing wave ratio (SWR) greater than one) which waste energy, and can even overheat the transmitter.
So to make an electrically short antenna resonant, an inductor called a loading coil is inserted in series with the antenna. The inductive reactance of the coil is equal and opposite to, and cancels, the capacitive reactance of the antenna, so the loaded antenna presents a pure resistance to the transmission line, preventing energy from being reflected.
The loading coil is usually inserted at the base of the antenna, between it and the transmission line ("base loading"), but sometimes it is inserted in the center of the antenna element itself ("center loading").
Campbell equation.
The Campbell equation is a relationship due to George Ashley Campbell for predicting the propagation constant of a loaded line. It is stated as;
A more engineer friendly rule of thumb is that the approximate requirement for spacing loading coils is ten coils per wavelength of the maximum frequency being transmitted. This approximation can be arrived at by treating the loaded line as a constant k filter and applying image filter theory to it. From basic image filter theory the angular cutoff frequency and the characteristic impedance of a low-pass constant k filter are given by;
From these basic equations the necessary loading coil inductance and coil spacing can be found;
Expressing this in terms of number of coils per cutoff wavelength yields;
The phenomenon of cutoff whereby frequencies above the cutoff frequency are not transmitted is an undesirable side effect of loading coils (although it proved highly useful in the development of filters). Cutoff is avoided by the use of continuous loading since it arises from the lumped nature of the loading coils.
History.
Oliver Heaviside.
The origin of the loading coil can be found in the work of Oliver Heaviside on the theory of transmission lines. Heaviside (1881) represented the line as a network of infinitesimally small circuit elements. By applying his operational calculus to the analysis of this network he discovered (1887) what has become known as the Heaviside condition. This is the condition that must be fulfilled in order for a transmission line to be free from distortion. The Heaviside condition is that the series impedance, Z, must be proportional to the shunt admittance, Y, at all frequencies. In terms of the primary line coefficients the condition is;
Heaviside was aware that this condition was not met in the practical telegraph cables in use in his day. In general, a real cable would have,
This is mainly due to the low value of leakage through the cable insulator, which is even more pronounced in modern cables which have better insulators than in Heaviside's day. In order to meet the condition, the choices are therefore to try to increase G or L or to decrease R or C. Decreasing R requires larger conductors. Copper was already in use in telegraph cables and this is the very best conductor available short of using silver. Decreasing R means using more copper and a more expensive cable. Decreasing C would also mean a larger cable (although not necessarily more copper). Increasing G is highly undesirable, while it would reduce distortion, it would at the same time increase the signal loss. Heaviside considered, but rejected, this possibility which left him with the strategy of increasing L as the way to reduce distortion.
Heaviside immediately (1887) proposed several methods of increasing the inductance, including spacing the conductors further apart and loading the insulator with iron dust. Finally, Heaviside made the proposal (1893) to use discrete inductors at intervals along the line. However, he never succeeded in persuading the British GPO to take up the idea. Brittain attributes this to Heaviside's failure to provide engineering details on the size and spacing of the coils for particular cable parameters. Heaviside's eccentric character and setting himself apart from the establishment may also have played a part in their ignoring of him.
John Stone.
John S. Stone worked for the American Telephone & Telegraph Company (AT&T) and was the first to attempt to apply Heaviside's ideas to real telecommunications. Stone's idea (1896) was to use a bimetallic iron-copper cable which he had patented. This cable of Stone's would increase the line inductance due to the iron content and had the potential to meet the Heaviside condition. However, Stone left the company in 1899 and the idea was never implemented. Stone's cable was an example of continuous loading, a principle that was eventually put into practice is other forms, see for instance Krarup cable later in this article.
George Campbell.
George Campbell was another AT&T engineer working in their Boston facility. Campbell was tasked with continuing the investigation into Stone's bimetallic cable, but soon abandoned it in favour of the loading coil. His was an independent discovery, Campbell was aware of Heaviside's work in discovering the Heaviside condition, but unaware of Heaviside's suggestion of using loading coils to enable a line to meet it. The motivation for the change of direction was Campbell's limited budget.
Campbell was struggling to set up a practical demonstration over a real telephone route with the budget he had been allocated. After considering that his artificial line simulators used lumped components rather than the distributed quantities found in a real line, he wondered if he could not insert the inductance with lumped components instead of using Stone's distributed line. When his calculations showed that the manholes on telephone routes were sufficiently close together to be able to insert the loading coils without the expense of either having to dig up the route or lay in new cables he changed to this new plan. The very first demonstration of loading coils on a telephone cable was on a 46-mile length of the so-called Pittsburgh cable (the test was actually in Boston, the cable had previously been used for testing in Pittsburgh) on 6 September 1899 carried out by Campbell himself and his assistant. The first telephone cable using loaded lines put into public service was between Jamaica Plain and West Newton in Boston on 18 May 1900.
Campbell's work on loading coils provided the theoretical basis for his subsequent work on filters which proved to be so important for frequency-division multiplexing. The cut-off phenomena of loading coils, an undesirable side-effect, can be exploited to produce a desirable filter frequency response.
Michael Pupin.
Michael Pupin, inventor and Serbian immigrant to the USA, also played a part in the story of loading coils. Pupin filed a rival patent to the one of Campbell's. This patent of Pupin's dates from 1899. There is an earlier patent (1894, filed December 1893) which is sometimes cited as Pupin's loading coil patent but is, in fact, something different. The confusion is easy to understand, Pupin himself claims that he first thought of the idea of loading coils while climbing a mountain in 1894, although there is nothing from him published at that time.
Pupin's 1894 patent "loads" the line with capacitors rather than inductors, a scheme that has been criticised as being theoretically flawed and never put into practice. To add to the confusion, one variant of the capacitor scheme proposed by Pupin does indeed have coils. However, these are not intended to compensate the line in any way. They are there merely to restore DC continuity to the line so that it may be tested with regular equipment. Pupin states that the inductance is to be so large that it will block all AC signals above 50 Hz. Consequently, only the capacitor is adding any significant impedance to the line and "the coils will not exercise any material influence on the results before noted".
Legal battle.
Heaviside never patented his idea; indeed, he took no commercial advantage of any of his work. Despite the legal disputes surrounding this invention, it is unquestionable that Campbell was the first to actually construct a telephone circuit using loading coils. There also can be little doubt that Heaviside was the first to publish and many would dispute Pupin's priority.
AT&T fought a legal battle with Pupin over his claim. Pupin was first to patent but Campbell had already conducted practical demonstrations before Pupin had even filed his patent (December 1899). Campbell's delay in filing was due to the slow internal machinations of AT&T.
However, AT&T foolishly deleted from Campbell's proposed patent application all the tables and graphs detailing the exact value of inductance that would be required before the patent was submitted. Since Pupin's patent contained a (less accurate) formula, AT&T was open to claims of incomplete disclosure. Fearing that there was a risk that the battle would end with the invention being declared unpatentable due to Heaviside's prior publication, they decided to desist from the challenge and buy an option on Pupin's patent for a yearly fee so that AT&T would control both patents. By January 1901 Pupin had been paid $200,000 ($13 million in 2011) and by 1917, when the AT&T monopoly ended and payments ceased, he had received a total of $455,000 ($25 million in 2011).
Benefit to AT&T.
The invention was of enormous value to AT&T. Telephone cables could now be used to twice the distance previously possible, or alternatively, a cable of half the previous quality (and cost) could be used over the same distance. When considering whether to allow Campbell to go ahead with the demonstration, their engineers had estimated that they stood to save $700,000 in new installation costs in New York and New Jersey alone. It has been estimated that AT&T saved $100 million in the first quarter of the 20th century. Heaviside, who began it all, came away with nothing. He was offered a token payment but would not accept, wanting the credit for his work. He remarked ironically that if his prior publication had been admitted it would "interfere . . . with the flow of dollars in the proper direction . . .".
Krarup cable.
Loading coils were not without their problems. In heavy submarine cables, loading coils were difficult to lay. Discontinuities where the coils were installed caused stresses in the cable during laying. Without great care, the cable might part and would be difficult to repair. A second problem was that the material science of the time had difficulties sealing the joint between coil and cable against ingress of seawater. When this occurred the cable was ruined.
A Danish engineer, Carl Emil Krarup, invented a form of continuously loaded cable which solved these problems and the cable is named for him. Krarup cable has iron wires continuously wound around the central copper conductor with adjacent turns in contact with each other. This cable was the first use of continuous loading on any telecommunication cable. In 1902, Krarup both wrote his paper on this subject and saw the installation of the first cable between Helsingør (Denmark) and Helsingborg (Sweden).
Permalloy cable.
Even though the Krarup cable added inductance to the line, this was insufficient to meet the Heaviside condition. AT&T searched for a better material with higher magnetic permeability. In 1914, Gustav Elmen discovered permalloy, a magnetic nickel-iron annealed alloy. In c. 1915, Oliver E. Buckley, H. D. Arnold, and Elmen, all at Bell Labs, greatly improved transmission speeds by suggesting a method of constructing submarine communications cable using permalloy tape wrapped around the copper conductors.
The cable was tested in a trial in Bermuda in 1923. The first permalloy cable placed in service connected New York City and Horta (Azores) in September 1924. Permalloy cable enabled signalling speed on submarine telegraph cables to be increased to 400 words/min at a time when 40 words/min was considered good. The first transatlantic cable achieved only two words/min.
Mu-metal cable.
Mu-metal has similar magnetic properties to permalloy but the addition of copper to the alloy increases the ductility and allows the metal to be drawn into wire. Mu-metal cable is easier to construct than permalloy cable, the mu-metal being wound around the core copper conductor in much the same way as the iron wire in Krarup cable. A further advantage with mu-metal cable is that the construction lends itself to a variable loading profile whereby the loading is tapered towards the ends.
Mu-metal was invented in 1923 by The Telegraph Construction and Maintenance Company Ltd., London, who made the cable, initially, for the Western Union Telegraph Co. Western Union were in competition with AT&T and the Western Electric Company who were using permalloy. The patent for permalloy was held by Western Electric which prevented Western Union from using it.
Patch loading.
Continuous loading of cables is expensive and hence is only done when absolutely necessary. Lumped loading with coils is cheaper but has the disadvantages of difficult seals and a definite cutoff frequency. A compromise scheme is patch loading whereby the cable is continuously loaded in repeated sections. The intervening sections are left unloaded.
Current practice.
Loaded cable is no longer a useful technology for submarine communication cables, having first been superseded by co-axial cable using electrically powered in-line repeaters and then by fibre-optic cable. Manufacture of loaded cable declined in the 1930s and was then superseded by other technologies post-war. Loading coils can still be found in some telephone landlines today but new installations would use more modern technology.

</doc>
<doc id="41327" url="http://en.wikipedia.org/wiki?curid=41327" title="Lobe">
Lobe

Lobe may refer to:

</doc>
<doc id="41328" url="http://en.wikipedia.org/wiki?curid=41328" title="Local access and transport area">
Local access and transport area

 
Local access and transport area (LATA) is a term used in U.S. telecommunications regulation. It represents a geographical area of the United States under the terms of the Modification of Final Judgment (MFJ) entered by the United States District Court for the District of Columbia in Civil Action number 82-0192 or any other geographic area designated as a LATA in the National Exchange Carrier Association, Inc. Tariff FCC No. 4. that precipitated the breakup of the original AT&T into the "Baby Bells" or created since that time for wireline regulation.
Generally, a LATA represents an area within which a divested Regional Bell Operating Company (RBOC) is permitted to offer exchange telecommunications and exchange access services. Under the terms of the MFJ, the RBOCs are generally prohibited from providing services that originate in one LATA and terminate in another.
LATA boundaries tend to be drawn around markets, and not necessarily along existing state or area code borders. Some LATAs cross over state boundaries, such as those for the New York metropolitan area and Greenwich, Connecticut; Chicago, Illinois; Portland, Oregon; and areas between Maryland, Virginia, and West Virginia. Area codes and LATAs do not necessarily share boundaries; many LATAs exist in multiple area codes, and many area codes exist in multiple LATAs.
Originally, the LATAs were grouped into regions within which one particular RBOC was allowed to provide services. The LATAs in each of these regions are numbered beginning with the same digit. Generally the LATAs were associated with carriers or other indications in the following manner:
In addition to this list, two local carriers were made independent: Cincinnati Bell in the Cincinnati area, and SNET (a former unit of AT&T, sold to Frontier) in Connecticut. These were assigned LATAs in the 9xx range.
Since the breakup of the original AT&T in 1984, however, some amount of deregulation, as well as a number of phone company mergers, have blurred the significance of these regions. A number of new LATAs have been formed within these regions since their inception, most beginning with the digit 9.
LATAs contribute to an often confusing aspect of long distance telephone service. Due to the various and overlapping regulatory limitations and inter-business arrangements, phone companies typically provide differing types of “long distance” service, each with potentially different rates:
Given the complexity of the legal and financial issues involved in each distinction, many long distance companies tend to not explain the details of these different rates, which can lead to billing questions from surprised customers.
Local carriers have various alternative terms for LATAs such as “Service Area” by Pacific Bell in California, or “Regional Calling Area” by Verizon in Maryland.
In order to facilitate the sharing of Telcordia telephone routing databases between countries, LATAs were later defined for the provinces of Canada, the other countries and territories of the North American Numbering Plan, and Mexico. Aside from U.S. territories, LATAs have no regulatory purpose in these areas. In 2000, the Canadian Radio-television and Telecommunications Commission eliminated all Canadian provincial LATAs in favor of a single LATA for Canada (888).
No LATAs exist with a second digit of 0 or 1, which distinguished them from traditional area codes.
List of LATAs.
US state LATAs.
The city or place name given with some LATAs is the name given to identify the LATA, not the limit of its boundary. Generally this is the most significant metropolitan area in the LATA. In some cases a LATA is named after the largest phone exchange in the LATA that was historically served by an RBOC. For example, the largest city in the Pahrump LATA in Nevada is Las Vegas. Since Las Vegas was not historically served by an RBOC, the LATA is named after the smaller town of Pahrump, which was historically served by Nevada Bell (now AT&T Inc.). Also, listing under a state does not necessarily limit the LATA's territory to that state; there may be overlaps as well as enclaves. Areas that include notable portions of other states are explained, but not all LATA state overlaps may be detailed.
LATA boundaries are not always solidly defined. Inter-carrier agreements, change proposals to the Federal Communications Commission (FCC), and new wiring developments into rural areas can and do often alter the effective borders between LATAs. Many sources on LATA boundary information conflict with each other at detailed levels. Telcordia data may provide the most up-to-date details of LATA inclusions.
Canada.
As LATAs exist for US regulatory purposes, where they serve as a demarcation between intra-LATA calls (handled by regional Bell operating companies) and inter-LATA calls (handled by interstate long distance carriers such as AT&T), they have no legal significance in Canada.
As of 2000, all of Canada (except for non-geographic numbers) is identified as LATA 888.
The use of this LATA set to identify individual provinces is therefore deprecated:
Local interconnection region.
Canada does define local interconnection regions (LIR's), which determine where points of interconnection (POI) must be provided by competing local exchange and mobile carriers to provide local number portability. A Canadian LIR is geographically smaller than a US LATA, typically comparable in size to a small city's flat-rate local calling area or to an entire large regional municipality. In areas where a small-city Digital Multiplex System controls a group of remote switching centres, one for each surrounding village, the local interconnect region normally includes each exchange in the city plus all downstream remotes of those exchanges. In a Toronto-sized city, the LIR will include only the city itself.
While the LIRs resemble local calling areas in geographic size, there are some key differences:
One example: The tiny unincorporated village of Beebe Plain, divided by the Quebec-Vermont border, is served by +1-819-876 Rock Island, Quebec, Canada (a remote station controlled from Magog) and +1-802-873 Derby Line, Vermont, USA (a remote station controlled from St. Johnsbury). Magog and St. Johnsbury are both a long-distance call from anywhere in Beebe Plain, even though Canadian subscribers can place local calls to Sherbrooke, US subscribers can locally call Newport and an international call within the village is local. An LIR assignment which follows network topology places the Canadian remote station in Magog's LIR, not Sherbrooke's LIR.

</doc>
<doc id="41329" url="http://en.wikipedia.org/wiki?curid=41329" title="Local battery">
Local battery

 

</doc>
<doc id="41330" url="http://en.wikipedia.org/wiki?curid=41330" title="Local call">
Local call

In telephony, the term local call has the following meanings:
Typically, local calls have shorter numbers than long distance calls, as the area code may not be required. However, this is not true in parts of the United States and Canada that are subject to overlay plans or many countries in Europe that require closed dialing plans.
Toll free (e.g. "800" numbers in the United States) are not necessarily local calls; despite being free to the caller, any charge due for the distance of the connection is charged to the "called" party.
Commercial users who make or accept many long distance calls to or from a particular distant place may make them as local calls by use of a foreign exchange service. Such an "FX" line also allows people in the distant place to call by using a telephone number local to them.

</doc>
<doc id="41332" url="http://en.wikipedia.org/wiki?curid=41332" title="Log-periodic antenna">
Log-periodic antenna

A log-periodic antenna (LP), also known as a log-periodic array or aerial, is a multi-element, directional, narrow-beam antenna that operates over a broad band of frequencies. This type of antenna is defined as having its structure prescribed by opening angles. A particular form of the log-periodic design, the log-periodic dipole array or LPDA, is often used in television antennas that work in the VHF band. LPDA antennas look somewhat similar to the Yagi antenna, but are very different designs. LPDA and Yagis are often combined in television antennas that cover both VHF and UHF. The log periodic antenna was invented by Dwight E. Isbell and Raymond DuHamel at the University of Illinois in 1958.
Basic concept.
The LPDA normally consists of a series of dipoles known as "elements" positioned along a support boom lying along the antenna axis. The elements are spaced at intervals following a logarithmic function of the frequency, known as "d" or "sigma". The length of the elements correspond to resonance at different frequencies within the antenna's overall bandwidth. This leads to a series of ever-shorter dipoles towards the "front" of the antenna. The relationship between the lengths is a function known as "tau". The ever-decreasing lengths makes the LPDA look, when viewed from the top, like a triangle or arrow with the tip pointed in the direction of the peak radiation pattern. "Sigma" and "tau" are the key design elements of the LPDA design.
Every element in the LPDA design is "active", that is, connected electrically to the feedline along with the other elements, though at any one frequency most of the elements draw little current from it. Each successive element is connected in "opposite" phase to the active connection running as a transmission line along the boom. For that reason, that transmission line can often be seen zig-zagging across the support boom holding the elements. One common design ploy is to use two booms that also acts as the transmission line, mounting the dipoles on the alternate booms. Other forms of the log-periodic design replace the dipoles with the transmission line itself, forming the log-periodic zig-zag antenna. Many other forms using the transmission wire as the active element also exist.
The Yagi and the LPDA designs look very similar at first glance, as both consist of a number of dipole elements spaced out along a support boom. The Yagi, however, has only a single dipole connected to the transmission line, usually the second one from the back of the array. The other dipoles on the boom are passive, with their two sides shorted, acting as "directors" or "reflectors" depending on their slightly different lengths and position relative to the "driven element". The difference between the LPDA and Yagi becomes obvious when examining their electrical connections. Another clear difference is the length of the dipoles; LPDA designs have "much" shorter dipoles towards the front of the antenna, forming a triangular shape as seen from the top, whereas the difference in lengths of Yagi elements is less noticeable. Another visible difference is the spacing between the elements, which can be rather constant in the Yagi but becomes exponentially wider along the LPDA. Although both directional, the LPDA is intended to achieve a very wide bandwidth, whereas the Yagi has a very narrow bandwidth but achieves greater gain.
It should be strictly noted that the "log-periodic shape" does not provide with broadband property for antennas. The broadband property of log-periodic antennas comes from its self-similarity. Y. Mushiake found, for what he termed "the simplest self-complementary planar antenna," a driving point impedance of η0/2=188.4Ω at frequencies well within its bandwidth limits.
History.
The log periodic antenna was invented by Dwight E. Isbell, Raymond DuHamel and variants by Paul Mayes. The University of Illinois at Urbana-Champaign had patented the Isbell and Mayes-Carrel antennas and licensed the design as a package exclusively to JFD electronics in New York. Channel Master and Blonder-Tongue ignored the patents and produced a wide range of antennas based on this design. Lawsuits regarding the antenna patent which the UI Foundation lost, evolved into the Blonder-Tongue Doctrine. This precedent governs patent litigation.
Short wave broadcast antennas.
The log periodic is commonly used in high power short wave broadcasting where it is desired to invest in only in a single antenna to cover transmissions over multiple bands. The log-periodic zig-zag design with up to 16 zig zag sections has been used. These large antennas are typically designed to cover 6 to 26 MHz but even larger ones have been built which operate as low as 2 MHz. Power ratings are available up to 500 KW. The antenna is supported by two taller and two shorter masts and is fed from the small end. The antenna shown here would have about 14 dBi gain. An antenna array consisting of two such antennas, one above the other and driven in phase has a gain of up to 17 dBi. Being log-periodic, the antenna's main characteristics (radiation pattern, gain, driving point impedance) are almost constant over its entire frequency range, with the match to a 300 ohm feed line achieving a standing wave ratio of better than 2:1 over that range.

</doc>
<doc id="41333" url="http://en.wikipedia.org/wiki?curid=41333" title="Long-haul communications">
Long-haul communications

In telecommunication, the term long-haul communications has the following meanings: 
1. In public switched networks, pertaining to circuits that span large distances, such as the circuits in inter-LATA, interstate, and international communications. See also Long line (telecommunications) 
2. In the military community, communications among users on a national or worldwide basis. 
"Note 1:" Compared to tactical communications, long-haul communications are characterized by (a) higher levels of users, such as the National Command Authority, (b) more stringent performance requirements, such as higher quality circuits, (c) longer distances between users, including world wide distances, (d) higher traffic volumes and densities, (e) larger switches and trunk cross sections, and (f) fixed and recoverable assets. 
"Note 2:" "Long-haul communications" usually pertains to the U.S. Defense Communications System. 
See also.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41334" url="http://en.wikipedia.org/wiki?curid=41334" title="Longitudinal redundancy check">
Longitudinal redundancy check

In telecommunication, a longitudinal redundancy check (LRC) or horizontal redundancy check is a form of redundancy check that is applied independently to each of a parallel group of bit streams. The data must be divided into transmission blocks, to which the additional check data is added.
The term usually applies to a single parity bit per bit stream,
calculated independently of all the other bit streams (BIP-8). although it could also be used to refer to a larger Hamming code.
This "extra" LRC word at the end of a block of data is very similar to checksum and CRC.
Optimal Rectangular Code.
While simple longitudinal parity can only detect errors, it can be combined with additional error control coding, such as a transverse redundancy check, to correct errors.
The transverse redundancy check is stored on a dedicated "parity track".
Whenever any single bit error occurs in a transmission block of data,
such two dimensional parity checking or "two-coordinate parity checking"
enables the receiver to use the TRC to detect which byte the error occurred in, and the LRC to detect exactly which track the error occurred in, to discover exactly which bit is in error, and then correct that bit by flipping it.
Pseudocode.
International standard ISO 1155 states that a longitudinal redundancy check for a sequence of bytes may be computed in software by the following algorithm:
 Set LRC = 0
 For each byte b in the buffer
 do
 Set LRC = (LRC + b) AND 0xFF
 end do
 Set LRC = (((LRC XOR 0xFF) + 1) AND 0xFF)
which can be expressed as "the 8-bit two's-complement value of the sum of all bytes modulo 28."
Many protocols use an XOR-based longitudinal redundancy check byte,
(often called block check character or BCC),
including
the serial line internet protocol (SLIP),
the IEC 62056-21 standard for electrical meter reading,
smart cards as defined in ISO/IEC 7816, and
the ACCESS.bus protocol.
An 8-bit LRC such as this is equivalent to a cyclic redundancy check using the polynomial "x"8+1, but the independence of the bit streams is less clear when looked at in that way.

</doc>
<doc id="41336" url="http://en.wikipedia.org/wiki?curid=41336" title="Long-term stability">
Long-term stability

The long-term stability of an oscillator is the degree of uniformity of frequency over time, when the frequency is measured under identical environmental conditions, such as supply voltage, load, and temperature. Long-term frequency changes are caused by changes in the oscillator elements that determine frequency, such as crystal drift, inductance changes, and capacitance changes. 
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41338" url="http://en.wikipedia.org/wiki?curid=41338" title="Loop gain">
Loop gain

In electronics and control system theory, loop gain is the sum of the gain, expressed as a ratio or in decibels, around a feedback loop. Feedback loops are widely used in electronics in amplifiers and oscillators, and more generally in both electronic and nonelectronic industrial control systems to control industrial plant and equipment. The concept is also used in biology. In a feedback loop, the output of a device, process or plant is sampled and applied to alter the input, to better control the output. The loop gain, along with the related concept of loop phase shift, determines the behavior of the device, and particularly whether the output is stable, or unstable, which can result in oscillation. The importance of loop gain as a parameter for characterizing electronic feedback amplifiers was first recognized by Heinrich Barkhausen in 1921,was developed further by Hendrik Wade Bode and Harry Nyquist at Bell Labs in the 1930s.
A block diagram of an electronic amplifier with negative feedback is shown at right. The input signal is applied to the amplifier with gain "A" and amplified. The output of the amplifier is applied to a feedback network with gain "β", and subtracted from the input to the amplifier. The loop gain is calculated by imagining the feedback loop is broken at some point, and calculating the net gain if a signal is applied. In the diagram shown, the loop gain is the product of the gains of the amplifier and the feedback network, "−Aβ". The minus sign is because the feedback signal is subtracted from the input. 
The gains "A" and "β", and therefore the loop gain, generally vary with the frequency of the input signal, and so are usually expressed as functions of the angular frequency "ω" in radians per second. 
In telecommunications, loop gain can be either of: 
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41339" url="http://en.wikipedia.org/wiki?curid=41339" title="Low-performance equipment">
Low-performance equipment

In telecommunication, the term low-performance equipment has the following meanings:
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41340" url="http://en.wikipedia.org/wiki?curid=41340" title="LPD433">
LPD433

LPD433 (low power device 433 MHz) is a part of ITU region 1 ISM band which allows for licence free communication devices, using frequencies in the UHF band from 433.075 MHz to 434.775 MHz, within a band traditionally reserved for amateur radio operations at higher RF power levels in most nations woldwide.
LPD hand-held radios are authorized for license-free voice communications use in most of Europe using analog frequency modulation (FM) as part of short range device regulations, with 25 kHz channel spacing, for a total of 69 channels. In some countries, LPD devices may only be used with the integral and non-removable antenna with a maximum legal power output of 10 mW.
Voice communication in LPD band was introduced to reduce the burden on the eight PMR446 channels over shorter ranges (less than 1 km) . In some EU countries voice is not allowed over LPD.
LPD is also used by wireless instruments and digital devices such as car keylocks.
Usage by country.
I.T.U. Region 1 (Europe)
U.K.
In the UK, LPD433 equipment that meets the respective Ofcom Interface Requirement can be used for model control, analogue/digitised voice and remote keyless entry systems. There is significant scope for interference however, both on frequency and on adjacent frequencies, as the band is far from free. The frequencies from 430 to 440 MHz are allocated on a secondary basis to licensed radio amateurs who are allowed to use up to 40 W (16 dBW) between 430 and 432 MHz and 400 W (26 dBW) between 432 and 440 MHz. Channels 1 to 14 are UK Amateur repeater outputs and channels 62 to 69 are UK Amateur repeater inputs. This band is shared on a secondary basis for both licensed and licence exempt users, with the primary user being the Ministry of Defence.
Ofcom, together with the R.S.G.B. Emerging Technology Co-ordination Committee have produced guidelines to help mitigate the side effects of interference to an extent.
Other European countries
European remote keyless entry systems often use the 433 MHz band, although, as in the UK, these frequencies are within the 70-centimeter band allocated to amateur radio, and interference results. Germany's radio control enthusiasts that hold "amateurfunk" ham radio licenses already have use of frequencies from channel 03 through 67 on the above chart for radio control of any form of model (air or ground-based), all with odd channel numbers (03, 05, etc. up to ch. 67) as read on the chart, with each sanctioned frequency having 50 kHz of bandwidth separation between each adjacent channel.
I.T.U. Region 2 (America)
In ITU region 2 (the Americas), the frequencies that LPD433 uses are also within the 70-centimeter band allocated to amateur radio. In the United States LPD433 radios can only be used under FCC amateur regulations by properly licenced amateur radio operators.

</doc>
<doc id="41341" url="http://en.wikipedia.org/wiki?curid=41341" title="Machine-readable medium">
Machine-readable medium

In telecommunications and computing a machine-readable medium (automated data medium) is a medium capable of storing data in a format readable by a mechanical device (rather than human readable).
Examples of machine-readable media include magnetic media such as magnetic disks, cards, tapes, and drums, punched cards and paper tapes, optical disks, barcodes and magnetic ink characters.
Common machine-readable technologies include magnetic recording, processing waveforms, and barcodes. Optical character recognition (OCR) can be used to enable machines to read information available to humans. Any information retrievable by any form of energy can be machine-readable. Examples include:
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41342" url="http://en.wikipedia.org/wiki?curid=41342" title="Magneto-ionic double refraction">
Magneto-ionic double refraction

In telecommunication, magneto-ionic double refraction is the combined effect of the Earth's magnetic field and atmospheric ionization, whereby a linearly polarized wave entering the ionosphere is split into two components called the orinary wave and extraordinary wave.
The component waves follow different paths, experience different attenuations, have different phase velocities, and, in general, are elliptically polarized in opposite senses.The critical frequency of the extraordinary wave is always greater then the critical frequency of the ordinary wave (i.e the wave in absence of the magnetic field) by the amount approximately equal to .5 times of gyro frequency .The amplitude of extraordinary wave is dependent on the earth magnetic field at that particular point . Beside splitting ,the polarization of the incident radio wave is also effected by this phenomena because the electron that were earlier in simple harmonic motion only are now in spiral motion too due to the magnetic field.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41343" url="http://en.wikipedia.org/wiki?curid=41343" title="Magneto-optic effect">
Magneto-optic effect

A magneto-optic effect is any one of a number of phenomena in which an electromagnetic wave propagates through a medium that has been altered by the presence of a quasistatic magnetic field. In such a material, which is also called gyrotropic or gyromagnetic, left- and right-rotating elliptical polarizations can propagate at different speeds, leading to a number of important phenomena. When light is transmitted through a layer of magneto-optic material, the result is called the Faraday effect: the plane of polarization can be rotated, forming a Faraday rotator. The results of reflection from a magneto-optic material are known as the magneto-optic Kerr effect (not to be confused with the nonlinear Kerr effect).
In general, magneto-optic effects break time reversal symmetry locally (i.e. when only the propagation of light, and not the source of the magnetic field, is considered) as well as Lorentz reciprocity, which is a necessary condition to construct devices such as optical isolators (through which light passes in one direction but not the other).
Two gyrotropic materials with reversed rotation directions of the two principal polarizations, corresponding to complex-conjugate ε tensors for lossless media, are called optical isomers.
Gyrotropic permittivity.
In particular, in a magneto-optic material the presence of a magnetic field (either externally applied or because the material itself is ferromagnetic) can cause a change in the permittivity tensor ε of the material. The ε becomes anisotropic, a 3×3 matrix, with complex off-diagonal components, depending of course on the frequency ω of incident light. If the absorption losses can be neglected, ε is a Hermitian matrix. The resulting principal axes become complex as well, corresponding to elliptically-polarized light where left- and right-rotating polarizations can travel at different speeds (analogous to birefringence).
More specifically, for the case where absorption losses can be neglected, the most general form of Hermitian ε is:
or equivalently the relationship between the displacement field D and the electric field E is:
where formula_3 is a real symmetric matrix and formula_4 is a real pseudovector called the gyration vector, whose magnitude is generally small compared to the eigenvalues of formula_3. The direction of g is called the axis of gyration of the material. To first order, g is proportional to the applied magnetic field:
where formula_7 is the magneto-optical susceptibility (a scalar in isotropic media, but more generally a tensor). If this susceptibility itself depends upon the electric field, one can obtain a nonlinear optical effect of magneto-optical parametric generation (somewhat analogous to a Pockels effect whose strength is controlled by the applied magnetic field).
The simplest case to analyze is the one in which g is a principal axis (eigenvector) of formula_3, and the other two eigenvalues of formula_3 are identical. Then, if we let g lie in the "z" direction for simplicity, the ε tensor simplifies to the form:
Most commonly, one considers light propagating in the "z" direction (parallel to g). In this case the solutions are elliptically polarized electromagnetic waves with phase velocities formula_11 (where μ is the magnetic permeability). This difference in phase velocities leads to the Faraday effect.
For light propagating purely perpendicular to the axis of gyration, the properties are known as the Cotton-Mouton effect and used for a Circulator.
Kerr Rotation and Kerr Ellipticity.
Kerr Rotation and Kerr Ellipticity are changes in the polarization of incident light which comes in contact with a gyromagnetic material. Kerr Rotation is a rotation in the angle of transmitted light, and Kerr Ellipticity is the ratio of the major to minor axis of the ellipse traced out by elliptically polarized light on the plane through which it propagates. Changes in the orientation of polarized incident light can be quantified using these two properties.
According to classical physics, the speed of light varies with the permittivity of a material:
formula_12
where formula_13 is the velocity of light through the material, formula_14 is the material permittivity, and formula_15 is the material permeability. Because the permittivity is anisotropic, polarized light of different orientations will travel at different speeds.
This can be better understood if we consider a wave of light that is circularly polarized (seen to the right). If this wave interacts with a material at which the horizontal component (green sinusoid) travels at a different speed than the vertical component (blue sinusoid), the two components will fall out of the 90 degree phase difference (required for circular polarization) changing the Kerr Ellipticity.
A change in Kerr Rotation is most easily recognized in linearly polarized light, which can be separated into two Circularly polarized components: Left-Handed Circular Polarized (LCP) light and Right-Handed Circular Polarized (RCP) light. The anisotropy of the Magneto Optic material permittivity causes a difference in the speed of LCP and RCP light, which will cause a change in the angle of polarized light. Materials that exhibit this property are known as Birefringent.
From this rotation, we can calculate the difference in orthogonal velocity components, find the anisotropic permittivity, find the gyration vector, and calculate the applied magnetic field formula_16.
Wavelength parallel measurement of magneto-optical effect.
Because the MO activity is usually very small, normally less than 1°,in conventional systems, the monochromator produces quasi-monochromatic light in a narrow wavelength window since the amplitude of modulation is wavelength dependent. Therefore, to measure the spectroscopic MO activity, a large number of measurements over the full spectra is required to obtain satisfactory wavelength resolution and thus is very time consuming. To obtain the spectroscopic information of the MO activity, light wavelength is varied by the monochromator. Therefore, these methods cost huge amount of time though provide high sensitivity to small MO activities. Fast spectroscopic characterization of the MO activity is thus desirable. Can we use white light source and perform wavelength-parallel measurement such as that in the state-of-art ellipsometry for the characterization of refractive index?
It is well established that when this linear polarized light passes through another polarizer, also called analyzer, the transmitted light intensity could be varied depending on their relative angle θ governed by a cos2("θ") law. Based on this simple idea, now researcher has developed a fast spectroscopic MO system, they can get full spectral range MO activity in a single magnetic field scan. The system requires only stable continuous spectral light source, two polarizers, an achromatic quarter-wave plate and a spectrometer.
It is low cost and flexible for application in full spectral range from UV to IR, or even in THz applications.This new system would booster the exploration to the MO characteristics of a large variety of materials in the full spectral range. The minimum resolvable angle depends on the full scale signal of light source and is limited by the instability and dark noise of the spectrometer. A minimum resolvable angle of 0.004° has been demonstrated in the their configuration.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41344" url="http://en.wikipedia.org/wiki?curid=41344" title="Main distribution frame">
Main distribution frame

In telephony, a main distribution frame (MDF or main frame) is a signal distribution frame for connecting equipment (inside plant) to cables and subscriber carrier equipment (outside plant). The MDF is a termination point within the local telephone exchange where exchange equipment and terminations of local loops are connected by jumper wires at the MDF. All cable copper pairs supplying services through user telephone lines are terminated at the MDF and distributed through the MDF to equipment within the local exchange e.g. repeaters and DSLAM. Cables to intermediate distribution frames (IDF) terminate at the MDF. Trunk cables may terminate on the same MDF or on a separate trunk main distribution frame (TMDF).
Like other distribution frames the MDF provides flexibility in assigning facilities, at lower cost and higher capacity than a patch panel.
The most common kind of large MDF is a long steel rack accessible from both sides. On one side, termination blocks are arranged horizontally at the front of rack shelves. Jumpers lie on the shelves and go through an insulated steel hoop to run vertically to other termination blocks that are arranged vertically. There is a hoop or ring at the intersection of each level and each vertical. Installing a jumper historically required two workers, one on either side of the MDF. The shelves are shallow enough to allow the rings to be within arm's reach, but the workers prefer to hang the jumper on a hook on a pole so their partner can pull it through the ring. A fanning strip at the back of each termination block prevents the wires from covering each other's terminals. With disciplined administration the MDF can hold over a hundred thousand jumpers, with dozens changed every day, for decades without tangling.
Before 1960, MDF jumpers were generally soldered. This was reliable but slow and expensive. Wire wrap was introduced in the 1960s, and punch blocks in the 1970s.
Each jumper is a twisted pair. Middle 20th century jumper wires in the USA were 24 AWG single strand copper, with a soft polyethylene inner jacket and a cotton wrapper, impregnated to make it slightly brittle and easy to remove neatly. Late 20th century ones had a single, thicker coating of polyethylene cross-linked to provide a suitable degree of brittleness.
Some urban telephone exchange MDFs are two stories high so they don't have to be more than a city block long. A few are three stories. Access to the upper levels can be either by a travelling ladder attached to the MDF, or by mezzanine walkways at a suitable height. By British custom the cables to the outside world are terminated on the horizontal side, and the indoors equipment on the vertical side. American usage is the opposite.
Smaller MDFs, and some modern large ones, are single sided so one worker can install, remove or change a jumper. COSMOS and other computerized Operations Support Systems help by assigning terminals close to one another, so most jumpers need not be long and shelves on either type of MDF do not become congested. This database keeps track of all terminals and jumpers. In the early and middle 20th century these records were kept as pencil entries in ledger books. The later database method saves much labor by permitting old jumpers to be reused for new lines.
The adoption of distributed switching in the late 20th century diminished the need for large, active, central MDFs.
The MDF usually holds telephone exchange protective devices including heat coils, and functions as a test point between a line and the exchange equipment. 
Sometimes the MDF is combined with other kinds of distribution frame in a CDF
The MDF in a private branch exchange performs functions similar to those performed by the MDF in a central office.
In order to automate the manual jumpering the Automated Main Distribution Frame (AMDF)
becomes an important role.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41345" url="http://en.wikipedia.org/wiki?curid=41345" title="Main lobe">
Main lobe

In a radio antenna's radiation pattern, the main lobe, or main beam is the lobe containing the maximum power. This is the lobe that exhibits the greatest field strength.
The radiation pattern of most antennas shows a pattern of ""lobes" at various angles, directions where the radiated signal strength reaches a maximum, separated by "nulls", angles at which the radiation falls to zero. In a directional antenna in which the objective is to emit the radio waves in one direction, the lobe in that direction is designed to be bigger (have higher field strength) than the others; this is the main lobe. The other lobes are called "sidelobes", and usually represent unwanted radiation in undesired directions. The sidelobe in the opposite direction from the main lobe is called the "backlobe"". 
The radiation pattern referred to above is usually the horizontal radiation pattern, which is plotted as a function of azimuth about the antenna, although the vertical radiation pattern may also have a main lobe. The beamwidth of the antenna is the width of the main lobe, usually specified by the "half power beam width", the angle encompassed between the points on the side of the lobe where the power has fallen to half (-3 dB) of its maximum value.
The concepts of main lobe and sidelobes also apply to acoustics and optics, and are used to describe the radiation pattern of optical systems like telescopes, and acoustic transducers like microphones and loudspeakers. 
See also.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41347" url="http://en.wikipedia.org/wiki?curid=41347" title="Maintainability">
Maintainability

In engineering, maintainability is the ease with which a product can be maintained in order to:
In some cases, maintainability involves a system of continuous improvement - learning from the past in order to improve the ability to maintain systems, or improve reliability of systems based on maintenance experience.
In telecommunication and several other engineering fields, the term maintainability has the following meanings:
Software engineering.
In software engineering, these activities are known as software maintenance (cf. ISO/IEC 9126).
The maintainability index is calculated with certain formulae from lines-of-code measures, McCabe measures and Halstead complexity measures.
The measurement and track maintainability are intended to help reduce or reverse a system's tendency toward "code entropy" or degraded integrity, and to indicate when it becomes cheaper and/or less risky to rewrite the code than it is to change it.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41348" url="http://en.wikipedia.org/wiki?curid=41348" title="Maintenance">
Maintenance

Maintenance may refer to:
Technical maintenance.
Technical maintenance is intended to maintain or improve the health of some asset. It forms an integral part of any Asset health management strategy.

</doc>
<doc id="41349" url="http://en.wikipedia.org/wiki?curid=41349" title="Managed object">
Managed object

In telecommunication, the term managed object has the following meanings: 
1. In a network, an abstract representation of network resources that are managed. With "representation", we mean not the actual device that is managed, but also the device driver, that communicates with the device. An example of a printer as a managed object is the window that shows information about the printer, such as the location, printer status, printing progress, paper choice, and printing margins. 
The database, where all managed objects are stored, is called Management Information Base. In contrast with a CI, a managed object is "dynamic" and communicates with other network recourses that are managed. 
"Note:" A managed object may represent a physical entity, a network service, or an abstraction of a resource that exists independently of its use in management.
2. In telecommunications management, a resource within the telecommunications environment that may be managed through the use of operation, administration, maintenance, and provisioning (OAMP) application protocols. 
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41350" url="http://en.wikipedia.org/wiki?curid=41350" title="Manchester code">
Manchester code

In telecommunication and data storage, Manchester coding (also known as phase encoding, or PE) is a line code in which the encoding of each data bit has at least one transition and occupies the same time. It therefore has no DC component, and is self-clocking, which means that it may be inductively or capacitively coupled, and that a clock signal can be recovered from the encoded data. As a result, electrical connections using a Manchester code are easily galvanically isolated using a network isolator—a simple one-to-one isolation transformer.
Background.
The name comes from its development at the University of Manchester, where the coding was used to store data on the magnetic drum of the Manchester Mark 1 computer.
Manchester coding is widely used (e.g., in 10BASE-T Ethernet (IEEE 802.3); consumer IR protocols; see also RFID or near field communication). There are more complex codes, such as 8B/10B encoding, that use less bandwidth to achieve the same data rate but may be less tolerant of frequency errors and jitter in the transmitter and receiver reference clocks.
Features.
Manchester code ensures frequent line voltage transitions, directly proportional to the clock rate; this helps clock recovery.
The DC component of the encoded signal is not dependent on the data and therefore carries no information, allowing the signal to be conveyed conveniently by media (e.g., Ethernet) which usually do not convey a DC component.
Description.
Extracting the original data from the received encoded bit (from Manchester as per 802.3):
Summary:
Manchester code always has a transition at the middle of each bit period and may (depending on the information to be transmitted) have a transition at the start of the period also. The direction of the mid-bit transition indicates the data. Transitions at the period boundaries do not carry information. They exist only to place the signal in the correct state to allow the mid-bit transition. The existence of guaranteed transitions allows the signal to be self-clocking, and also allows the receiver to align correctly; the receiver can identify if it is misaligned by half a bit period, as there will no longer always be a transition during each bit period. The price of these benefits is a doubling of the bandwidth requirement compared to simpler NRZ coding schemes (or see also NRZI).
Manchester encoding as phase-shift keying.
Manchester encoding is a special case of binary phase-shift keying (BPSK), where the data controls the phase of a square wave carrier whose frequency is the data rate. Such a signal is easy to generate.
Conventions for representation of data.
There are two opposing conventions for the representations of data.
The first of these was first published by G. E. Thomas in 1949 and is followed by numerous authors (e.g., Andy Tanenbaum). It specifies that for a 0 bit the signal levels will be low-high (assuming an amplitude physical encoding of the data) - with a low level in the first half of the bit period, and a high level in the second half. For a 1 bit the signal levels will be high-low.
The second convention is also followed by numerous authors (e.g., William Stallings ) as well as by IEEE 802.4 (token bus) and lower speed versions of IEEE 802.3 (Ethernet) standards. It states that a logic 0 is represented by a high-low signal sequence and a logic 1 is represented by a low-high signal sequence.
If a Manchester encoded signal is inverted in communication, it is transformed from one convention to the other. This ambiguity can be overcome by using differential Manchester encoding.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41351" url="http://en.wikipedia.org/wiki?curid=41351" title="Mandrel wrapping">
Mandrel wrapping

In multimode fiber optics, mandrel wrapping is a technique used to preferentially attenuate high-order mode power of a propagating optical signal. Consequently, if the fibre is propagating substantial energy in affected modes, the modal distribution will be changed.
A cylindrical rod wrap consists of a specified number turns of fiber on a mandrel of specified size, depending on the fibre characteristics and the desired modal distribution. It has application in optical transmission performance tests, to create a defined mode power distribution or to prevent multimode propagation in single mode fibre. If the launch fibre is fully filled ahead of the mandrel wrap, the higher-order modes will be stripped off, leaving only lower-order modes. If the launch fibre is underfilled, for example as a consequence of being energized by a laser diode or edge-emitting LED, there will be no effect on the mode power distribution or loss measurements.
In multimode fibre, mandrel wrapping is used to eliminate the effect of "transient loss", the tendency of high order modes to experience higher loss than lower order modes. Numerical addition (in decibels) of the measured loss of multiple fibre segments and/or components overestimates the loss of the concatenated set if each segment or component has been measured with a full mode power distribution.
In single mode optical fibre measurements, it is used to enforce true single mode propagation at wavelengths near or below the theoretical cutoff wavelength, at which substantial power can exist in a higher order mode group. In this use, it is commonly termed a High Order Mode Filter (HOMF).
Ultimately, the effect of mandrel wrapping on optical measurements depends on the propagating mode power distribution. An additional loss mechanism has no effect unless power is present in the affected modes. 
Principle of operation.
The effect of physically bending an optical fibre around a cylindrical form is to slightly modify the effective refractive index in the curved region, which locally reduces the effective mode volume of the fibre. This causes optical power in the highest order modes to become unguided, or so weakly guided as to be released into an unbound state, absorbed by the fiber coating or completely ejected from the fibre. The practical effect of mandrel wrapping is to attenuate optical power propagating in the highest order modes. Lower order modes are unaffected, experiencing neither increased loss nor conversion into other modes (mode mixing). 
Determination of appropriate mandrel wrap conditions.
The mandrel diameter and number of turns are chosen to eliminate certain modes in a reproducible way. It is empirically observed that more than 5 full 360 degree wraps creates little additional loss, so 3 to 5 turns are commonly specified. The mandrel diameter affects how far into the mode volume the modal unbinding occurs. Experimentally, one plots the transmitted power from a wrapped fibre into which a uniform modal power distribution has been excited, as a function of mandrel diameter, maintaining a constant number of turns. This reveals step-like reductions in transmitted power as the diameter decreases, where each step is the point at which the mandrel is beginning to affect the next-lower mode group. For best measurement reproducibility, one would select a diameter that is not near such a transition, although this may not be possible if measurements must be performed over a range of wavelengths. Total mode volume in a fiber is a function of wavelength, so the mandrel diameter at which the mode group transitions occur will change with wavelength.
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41352" url="http://en.wikipedia.org/wiki?curid=41352" title="Margin">
Margin

Margin may refer to:

</doc>
<doc id="41353" url="http://en.wikipedia.org/wiki?curid=41353" title="Maritime broadcast communications net">
Maritime broadcast communications net

In telecommunication, a maritime broadcast communications net is a communications net that is used for international distress calling, including international lifeboat, lifecraft, and survival-craft high frequency (HF); aeronautical emergency very high frequency (VHF); survival ultra high frequency (UHF); international calling and safety very high frequency (VHF); combined scene-of-search-and-rescue; and other similar and related purposes. 
"Note:" Basic international distress calling is performed at either medium frequency (MF) or at high frequency (HF).
See also.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41354" url="http://en.wikipedia.org/wiki?curid=41354" title="Master frequency generator">
Master frequency generator

A master frequency generator or master electronic oscillator, in frequency-division multiplexing (FDM), is a piece of equipment used to provide system end-to-end carrier frequency synchronization and frequency accuracy of tones. 
The following types of oscillators are used in the Defense Communications System FDM systems:

</doc>
<doc id="41355" url="http://en.wikipedia.org/wiki?curid=41355" title="Master station">
Master station

In telecommunication, a master station is a station that controls or coordinates the activities of other stations in the system.
Examples:
Operation modes.
In data transmission, a master station can be set to not wait for a reply from a slave station after transmitting each message or transmission block. In this case the station is said to be in "continuous operation".
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41357" url="http://en.wikipedia.org/wiki?curid=41357" title="Material scattering">
Material scattering

Material scattering of an electromagnetic wave is scattering that is attributable to the intrinsic properties of the material through which the wave is propagating. Ionospheric scattering and Rayleigh scattering are examples of material scattering. In an optical fiber, material scattering is caused by micro-inhomogeneities in the refractive indices of the materials used to fabricate the fiber, including the dopants used to modify the refractive index profile.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41358" url="http://en.wikipedia.org/wiki?curid=41358" title="Maximal-ratio combining">
Maximal-ratio combining

In telecommunications, maximal-ratio combining (MRC) is a method of diversity combining in which:
It is also known as ratio-squared combining and predetection combining. Maximal-ratio combining is the optimum combiner for independent AWGN channels.
MRC can restore a signal to its original shape.
Example: Least Squares estimate in the case of Rx diversity.
We consider an example of which the receiver is endowed with N antennas. In this case, the received vector formula_1 is
where formula_2 is noise vector formula_3. Following the ML detection criterion the detection procedure may be written as
where formula_4 is the least square solution to the above model.
The least square solution in this case is also known as maximal-ratio-combining (MRC). In the case of N antennas the LS can be written as
which means that the signal from each antenna is rotated and weighted according to the phase and strength of the channel, such that the signals from all antennas are combined to yield the maximal ratio between signal and noise terms.
See also.
Diversity combining
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41359" url="http://en.wikipedia.org/wiki?curid=41359" title="Maximum usable frequency">
Maximum usable frequency

In radio transmission maximum usable frequency (MUF) is the highest radio frequency that can be used for transmission between two points via reflection from the ionosphere ( skywave or "skip" propagation) at a specified time, independent of transmitter power. This index is especially useful in regard to shortwave transmissions. 
In shortwave radio communication, a major mode of long distance propagation is for the radio waves to reflect off the ionized layers of the atmosphere and return diagonally back to Earth. In this way radio waves can travel beyond the horizon, around the curve of the Earth. However the refractive index of the ionosphere decreases with increasing frequency, so there is an upper limit to the frequency which can be used. Above this frequency the radio waves are not reflected by the ionosphere but are transmitted through it into space. 
The ionization of the atmosphere varies with time of day and season as well as with solar conditions, so the upper frequency limit for skywave communication varies on an hourly basis. MUF is a median frequency, defined as the highest frequency at which skywave communication is possible 50% of the days in a month, as opposed to the lowest usable high frequency (LUF) which is the frequency at which communication is possible 90% of the days, and the Frequency of optimum transmission (FOT).
Typically the MUF is a predicted number. Given the maximum observed frequency (MOF) for a mode on each day of the month at a given hour, the MUF is the highest frequency for which an ionospheric communications path is predicted on 50% of the days of the month. 
On a given day, communications may or may not succeed at the MUF. Commonly, the optimal operating frequency for a given path is estimated at 80 to 90% of the MUF. As a rule of thumb the MUF is approximately 3 times the critical frequency.
formula_1
where the critical frequency is the highest frequency reflected for a signal propagating directly upward.
Sources.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41363" url="http://en.wikipedia.org/wiki?curid=41363" title="Mean time between outages">
Mean time between outages

In a system the mean time between outages (MTBO) is the mean time between equipment failures that result in loss of system continuity or unacceptable degradation. 
The MTBO is calculated by the equation,
formula_1
where MTBF is the nonredundant mean time between failures and FFAS is the fraction of failures for which the failed equipment is automatically bypassed. 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41364" url="http://en.wikipedia.org/wiki?curid=41364" title="Mechanically induced modulation">
Mechanically induced modulation

Mechanically induced modulation is an optical signal modulation induced by mechanical means. 
An example of deleterious mechanically induced modulation is speckle noise created in a multimode fiber by an imperfect splice or imperfectly mated connectors. Mechanical disturbance of the fiber ahead of the joint will introduce changes in the modal structure, resulting in variations of joint loss. This is a subset of many mechanisms that can lead to modal noise in an optical system.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41365" url="http://en.wikipedia.org/wiki?curid=41365" title="Mediation function">
Mediation function

In telecommunications network management, a mediation function is a function that routes or acts on information passing between network elements and network operations. 
Examples of mediation functions are communications control, protocol conversion, data handling, communications of primitives, processing that includes decision-making, and data storage. 
Mediation functions can be shared among network elements, mediation devices, and network operation centers.
Sources.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41366" url="http://en.wikipedia.org/wiki?curid=41366" title="Medium-power talker">
Medium-power talker

In telecommunication, a medium-power talker is a hypothetical talker, within a log-normal distribution of talkers, whose volume lies at the medium power of all talkers determining the volume distribution at the point of interest. 
When the distribution follows a log-normal curve (values expressed in decibels), the mean and standard deviation can be used to compute the medium-power talker. The talker volume distribution follows a log-normal curve and the medium-power talker is uniquely determined by the average talker volume. The medium-power talker volume, "V", is given by "V" = "V" o + 0.115σ2, where "V" o is the average of the talker volume distribution in volume units (vu), and σ2 is the variance of the distribution.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41367" url="http://en.wikipedia.org/wiki?curid=41367" title="Message">
Message

A message is a discrete unit of communication intended by the source for consumption by some recipient or group of recipients. A message may be delivered by various means, including courier, telegraphy, carrier pigeon and electronic bus.
A message can be the content of a broadcast. An interactive
exchange of messages forms a conversation.
One example of a message is a communiqué (pronounced ), which is a brief report or statement released by a public agency.
Role in human communication.
In communication between humans, messages can be verbal or nonverbal:
In computer science.
There are two main senses of the word "message" in computing: messages between the human users of computer systems that are delivered by those computer systems, and messages passed between programs or between components of a single program, for their own purposes.

</doc>
<doc id="41368" url="http://en.wikipedia.org/wiki?curid=41368" title="Message format">
Message format

In telecommunication, a message format is a predetermined or prescribed spatial or time-sequential arrangement of the parts of a message that is recorded in or on a data storage medium. 
At one time, messages prepared for electrical transmission were composed on a printed blank form with spaces for each part of the message and for administrative entries.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41369" url="http://en.wikipedia.org/wiki?curid=41369" title="Micro-mainframe link">
Micro-mainframe link

In telecommunication, a micro-mainframe link is a physical or logical connection established between a remote microprocessor and mainframe host computer for the express purpose of uploading, downloading, or viewing interactive data and databases on-line in real time. 
"Note:" A micro-mainframe link usually requires terminal emulation software on the microcomputer.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41371" url="http://en.wikipedia.org/wiki?curid=41371" title="Mixer">
Mixer

Mixer may refer to:
An electronics device:
A mechanical device:
A synonym for an occupation:
Other:

</doc>
<doc id="41372" url="http://en.wikipedia.org/wiki?curid=41372" title="Sprague–Grundy theorem">
Sprague–Grundy theorem

In combinatorial game theory, the Sprague–Grundy theorem states that every impartial game under the normal play convention is equivalent to a nimber. The Grundy value or nim-value of an impartial game is then defined as the unique nimber that the game is equivalent to. In the case of a game whose positions (or summands of positions) are indexed by the natural numbers (for example the possible heap sizes in nim-like games), the sequence of nimbers for successive heap sizes is called the nim-sequence of the game.
The theorem was discovered independently by R. P. Sprague (1935) and P. M. Carmelo Grundy (1939).
Definitions.
For the purposes of the Sprague–Grundy theorem, a "game" is a two-player game of perfect information satisfying the "ending condition" (all games come to an end: there are no infinite lines of play) and the "normal play condition" (a player who cannot move loses).
An "impartial game" is one such as nim, in which each player has exactly the same available moves as the other player in any position. Note that games such as tic-tac-toe, checkers, and chess are "not" impartial games. In the case of checkers and chess, for example, players can only move their own pieces, not their opponent's pieces. And in tic-tac-toe, one player puts down X's, while the other puts down O's. Positions in impartial games fall into two "outcome classes": either the next player (the one whose turn it is) wins (an "N-position") or the previous player wins (a "P-position").
In this proof, a position is identified with the set of positions that can be reached in one move (these positions are called "options"). For example, the position formula_1 is a P-position under normal play, because the current player has no moves and therefore loses. The position formula_2, in contrast, is an N-position; the current player has only one option, and that option is a losing position for their opponent.
A "nimber" is a special position denoted formula_3 for some ordinal formula_4. formula_5 is formula_1, the P-position given as an example earlier. The other nimbers are defined inductively by formula_7; in particular, formula_8 (the example N-position from above), formula_9 (a choice between the two example positions), etc. formula_3 therefore corresponds to a heap of formula_4 counters in the game of nim, hence the name.
Two positions formula_12 and formula_13 can be "added" to make a new position formula_14 in a combined game where the current player can choose either to move in formula_12 or in formula_13. Explicit computation of the set formula_14 is by repeated application of the rule formula_18, which incidentally indicates that position addition is both commutative and associative as expected.
Two positions formula_12 and formula_20 are defined to be "equivalent" if for every position formula_13, the position formula_14 is in the same outcome class as formula_23. Such an equivalence is written formula_24.
First Lemma.
As an intermediate step to proving the main theorem, we show that, for every position formula_12 and every P-position formula_26, the equivalence formula_27 holds. By the above definition of equivalence, this amounts to showing that formula_14 and formula_29 share an outcome class for all formula_13.
Suppose that formula_14 is a P-position. Then the previous player has a winning strategy for formula_29: respond to moves in formula_26 according to their winning strategy for formula_26 (which exists by virtue of formula_26 being a P-position), and respond to moves in formula_14 according to their winning strategy for formula_14 (which exists for analogous reason). So formula_29 must also be a P-position.
On the other hand, if formula_14 is an N-position, then the next player has a winning strategy: choose a P-position from among the formula_14 options, putting their opponent in the case above. Thus, in this case, formula_29 must be a N-position, just like formula_14.
As these are the only two cases, the lemma holds.
Second Lemma.
As a further step, we show that formula_24 if and only if formula_44 is a P-position.
In the forward direction, suppose that formula_24. Applying the definition of equivalence with formula_46, we find that formula_47 (which is equal to formula_44 by commutativity of addition) is in the same outcome class as formula_49. But formula_49 must be a P-position: for every move made in one copy of formula_12, the previous player can respond with the same move in the other copy, and so always make the last move.
In the reverse direction, we apply the first lemma with formula_52 to formula_12 to get formula_54 and with formula_55 to formula_20 to deduce formula_57. By associativity and commutativity, the right-hand sides of these results are equal. Furthermore, formula_58 is an equivalence relation because equality is an equivalence relation on outcome classes. Via the transitivity of formula_58, we can conclude that formula_24.
Proof.
We prove that all positions are equivalent to a nimber by structural induction. The more specific result, that the given game's initial position must be equivalent to a nimber, shows that the game is itself equivalent to a nimber.
Consider a position formula_61. By the induction hypothesis, all of the options are equivalent to nimbers, say formula_62. So let formula_63. We will show that formula_64, where formula_65 is the mex (minimum exclusion) of the numbers formula_66, that is, the smallest non-negative integer not equal to some formula_67.
The first thing we need to note is that formula_68, by way of the second lemma. If formula_69 is zero, the claim is trivially true. Otherwise, consider formula_44. If the next player makes a move to formula_71 in formula_12, then the previous player can move to formula_73 in formula_20, and conversely if the next player makes a move in formula_20. After this, the position is a P-position by the lemma's forward implication. Therefore, formula_44 is a P-position, and, citing the lemma's reverse implication, formula_68.
Now let us show that formula_78 is a P-position, which, using the second lemma once again, means that formula_79. We do so by giving an explicit strategy for the previous player.
Suppose that formula_20 and formula_81 are empty. Then formula_78 is the null set, clearly a P-position.
Or consider the case that the next player moves in the component formula_81 to the option formula_84 where formula_85. Because formula_65 was the "minimum" excluded number, the previous player can move in formula_20 to formula_84. And, as shown before, any position plus itself is a P-position.
Finally, suppose instead that the next player moves in the component formula_20 to the option formula_73. If formula_91 then the previous player moves in formula_81 to formula_73; otherwise, if formula_94, the previous player moves in formula_73 to formula_81; in either case the result is a position plus itself. (It is not possible that formula_97 because formula_65 was defined to be different from all the formula_67.)
In summary, we have formula_24 and formula_79. By transitivity, we conclude that formula_64, as desired.
Development.
The Sprague–Grundy theorem has been developed into the field of combinatorial game theory, notably by E. R. Berlekamp, John Horton Conway and others. The field is presented in the books "Winning Ways for your Mathematical Plays" and "On Numbers and Games".

</doc>
<doc id="41373" url="http://en.wikipedia.org/wiki?curid=41373" title="Mode field diameter">
Mode field diameter

In fiber optics, the mode field diameter (MFD) is an expression of distribution of the irradiance, i.e., the optical power per unit area, across the end face of a single-mode fiber. 
For a Gaussian intensity (i.e., power density, W/m2) distribution in a single-mode optical fiber, the mode field diameter is that at which the electric and magnetic field strengths are reduced to 1/e of their maximum values, i.e., the diameter at which power density is reduced to 1/e2 of the maximum power density, because the power density is proportional to the square of the field strength.
See also.
Beam diameter
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41374" url="http://en.wikipedia.org/wiki?curid=41374" title="Mode partition noise">
Mode partition noise

Mode partition noise: In an optical communication link, is phase jitter of the signal caused by the combined effects of mode hopping in the optical source and intramodal distortion in the fiber. 
Mode hopping causes random wavelength changes which in turn affect the group velocity, "i.e.", the propagation time. Over a long length of fiber, the cumulative effect is to create jitter, "i.e." mode partition noise. The variation of group velocity creates the mode partition noise.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41375" url="http://en.wikipedia.org/wiki?curid=41375" title="Mode scrambler">
Mode scrambler

In telecommunications, a mode scrambler mode mixer is a device for inducing mode coupling in an optical fiber, or a device that, itself, exhibits a uniform output intensity profile independent of the input mode volume or modal excitation condition. Mode scramblers are used to provide a modal distribution that is independent of the optical source for purposes of laboratory, manufacturing, or field measurements or tests. Mode scramblers are primarily used to improve reproducibility of multimode fiber bandwidth measurements.
If multimode fiber bandwidth is measured using a laser diode directly coupled to its input, the resulting measurement can vary by as much as an order of magnitude. This measurement variability is due to the combination of differences in laser output characteristics (emitted mode power distribution) and the differential mode delay of the fiber. Differential mode delay is the difference in the time delays amongst the fiber's propagating modes caused by imperfections or nonideality of the fiber refractive index profile.
The primary purpose of a mode scrambler is to create a uniform, overfilled launch condition that can be easily reproduced on multiple measurement systems, so that measurement systems have essentially the same launch conditions and can measure approximately the same bandwidth despite having different laser sources. These were used for this purpose in the first U.S. NIST round-robins on multimode fiber. The overfilled launch (OFL) was created to reduce measurement variability, and improve concatenation estimates for multimode fibers, used at that time for telecom 'long haul' (e.g., 7–10 km 850 nm or 20–30 km 1300 nm) systems.
When the telecom industry converted to near-exclusive use of single-mode fiber ca. 1984, multimode fiber was re-purposed for use in LANs, such as Fiber Distributed Date Interface (FDDI), then under development. The output modal power distribution of a mode scrambler is similar to the surface-emitters used in those first LAN transmitters, but this was fortuitous coincidence. On average, but not in every case, the OFL bandwidth measured using a mode scrambler is lower than that produced by excitation of a partial mode volume (restricted mode launch or RML), such as occurs with directly coupled laser diodes.
There are two common types of mode scramblers: the "Step-Graded-Step" (S-G-S) and the "step index with bends". The S-G-S mode scrambler is actually an assembly, a fusion-spliced concatenation of a step-index profile, a graded-index profile and another step-index profile fiber. Typically, each segment is approximately 1 meter long, and may use segments of unconventional size to produce the distribution required according to core size of fiber to be tested. Unconventional fiber size was not an issue, as they were developed by fiber manufacturers, but some test equipment has difficulty complying with revised qualification standards, and now use "Step Index with Bends" mode scramblers, which can be adjusted to purpose. Step Index with Bend mode scramblers are created simply by routing a specially designed step-index multimode fiber through a series of small radius bends, or by compressing fiber against surfaces with specific roughness. The implementations are simple, but generally less reproducible, and require care to avoid over-stressing the fiber.
A mode scrambler can be characterized and qualified by measuring its near-field and far-field distributions, as well as by measuring one of these distributions while restricting the other. Guidelines for constructing a mode scrambler and qualifying its output can be found in the ANSI/TIA/EIA-455-54 fiber optic test procedure (FOTP).

</doc>
<doc id="41376" url="http://en.wikipedia.org/wiki?curid=41376" title="Mode volume">
Mode volume

In fiber optics, mode volume is the number of bound modes that an optical fiber is capable of supporting. 
The mode volume "M" is approximately given by formula_1 and formula_2, respectively for step-index and power-law index profile fibers, where "g" is the profile parameter, and "V" is the normalized frequency, "which must be greater than 5 for this approximation to be valid".

</doc>
<doc id="41377" url="http://en.wikipedia.org/wiki?curid=41377" title="Modification of Final Judgment">
Modification of Final Judgment

In United States telecommunication law, Modification of Final Judgment (MFJ) is the August 1982 agreement approved by the court (consent decree) settling "United States v. AT&T", a landmark antitrust suit, originally filed on January 14, 1949 and modifying the previous Final Judgment of January 24, 1956. Entered into between the United States Department of Justice and the American Telephone & Telegraph Company (AT&T) the MFJ, after modification and upon approval of the United States District Court for the District of Columbia, required the Bell System divestiture of the Bell Operating Companies from AT&T.
The exact term for the MFJ is given under the general definition of local access and transport area, a term used in U.S. telecommunications regulation for geographic areas in which local telephone companies are allowed to provide service, the term being used is 
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41378" url="http://en.wikipedia.org/wiki?curid=41378" title="Modified AMI code">
Modified AMI code

Modified AMI codes are Alternate Mark Inversion (AMI) line codes in which bipolar violations may be deliberately inserted to maintain system synchronization. There are several types of modified AMI codes, used in various T-carrier and E-carrier systems.
Overview.
The clock rate of an incoming T-carrier is extracted from its bipolar line code. Each signal transition provides an opportunity for the receiver to see the transmitter's clock. The AMI code guarantees that transitions are always present before and after each mark (1 bit), but are missing between adjacent spaces (0 bits). To prevent loss of synchronization when a long string of zeros is present in the payload, deliberate bipolar violations are inserted into the line code, to create a sufficient number of transitions to maintain synchronization; this is a form of run length limited coding. The receive terminal equipment recognizes the bipolar violations and removes from the user data the marks attributable to the bipolar violations.
T-carrier was originally developed for voice applications. When voice signals are digitized for transmission via T-carrier, the data stream always includes ample 1 bits to maintain synchronization. (To help this, the μ-law algorithm for digitizing voice signals encodes silence as a continuous stream of 1 bits.) However, when used for the transmission of digital data, the conventional AMI line code may fail to have sufficient marks to permit recovery of the incoming clock, and synchronization is lost. This happens when there are too many consecutive zeros in the user data being transported. 
The exact pattern of bipolar violations that is transmitted in any given case depends on the line rate ("i.e.", the level of the line code in the T-carrier hierarchy) and the polarity of the last valid mark in the user data prior to the unacceptably long string of zeros. It would not be useful to have a violation immediately following a mark, as that would not produce a transition. For this reason, all modified AMI codes include a space (0 bit) before each violation mark.
In the descriptions below, "B" denotes a balancing mark with the opposite polarity to that of the preceding mark, while "V" denotes a bipolar violation mark, which has the same polarity as the preceding mark.
In order to preserve AMI coding's desirable absence of DC bias, the number of positive marks must equal the number of negative marks. This happens automatically for balancing (B) marks, but the line code must ensure that positive and negative violation marks balance each other.
Zero code suppression.
The first technique used to ensure a minimum density of marks was zero code suppression a form of bit stuffing, which set the least significant bit of each 8-bit byte transmitted to a 1. (This bit was already unavailable due to robbed-bit signaling.) This avoided the need to modify the AMI code in any way, but limited available data rates to 56,000 bits per second per DS0 voice channel. Also, the low minimum density of ones (12.5%) sometimes led to increased clock slippage on the span.
Increased demand for bandwidth, and compatibility with the G.703 and ISDN PRI standards which called for 64,000 bits per second, led to this system being superseded by B8ZS.
B8ZS (North American T1).
Commonly used in the North American T1 (Digital Signal 1) 1.544 Mbit/s line code, bipolar with eight-zero substitution (B8ZS) replaces each string of 8 consecutive zeros with the special pattern "000VB0VB". Depending on the polarity of the preceding mark, that could be 000+−0−+ OR 000−+0+−.
B6ZS (North American T2).
At the North American T2 rate (6.312 Mbit/s), bipolar violations are inserted if 6 or more consecutive zeros occur. This line code is called bipolar with six-zero substitution (B6ZS), and replaces 6 consecutive zeros with the pattern "0VB0VB". Depending on the polarity of the preceding mark, that could be 0+−0−+ OR 0−+0+−.
HDB3 (European E-carrier).
Used in all levels of the European E-carrier system, the high density bipolar of order 3 (HDB3) code replaces any instance of 4 consecutive 0 bits with one of the patterns "000V" or "B00V". The choice is made to ensure that consecutive violations are of differing polarity; i.e., separated by an odd number of normal + or − marks.
These rules are applied on the code as it is being built from the original string. Everytime there are 4 consecutive zeros in the code they will be replaced by either 000−, 000+, +00+ or −00−. To determine which pattern to use, one must count the number of pluses (+) and the number of minuses (−) since the last violation bit V, then subtract one from the other. If the result is an odd number then 000− or 000+ is used. If the result is an even number then +00+ or −00− is used. To determine which polarity to use, one must look at the pulse preceding the four zeros. If 000V form must be used then V simply copies the polarity of last pulse, if B00V form must be used then B and V chosen will have the opposite polarity of the last pulse.
Summary of HDB3 encoding rules
Examples.
The pattern of bits 100001102 encoded in HDB3 is +000V−+0 (the corresponding encoding using AMI is +0000−+0.
The pattern of bits 10100000110000110000002 encoded in HDB3 is +0−B00V0+−B00V+−B00V00 which is: +0−+00+0+−+00++−+00+00 (the corresponding encoding using AMI is +0−00000+−0000+−000000)
The pattern of bits 10100001000011000011100001111000010100002 encoded in HDB3 is +0−B00V−000V+−B00V−+−000V+−+−B00V−0+B00V which is: 
+0−+00+−000−+−+00+−+−000−+−+−+00+−0+−00− (the corresponding encoding using AMI is +0−0000+0000−+0000−+−0000+−+−0000+0−0000)
B3ZS (North American T3).
At the North American T3 rate (44.736 Mbit/s), bipolar violations are inserted if 3 or more consecutive zeros occur. This line code is called bipolar with three-zero substitution (B3ZS), and is very similar to HDB3. Each run of 3 consecutive zeros is replaced by "00V" or "B0V". The choice is made to ensure that consecutive violations are of differing polarity, i.e. separated by an odd number of normal B marks.
See also.
Other line codes that have 3 states:
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41382" url="http://en.wikipedia.org/wiki?curid=41382" title="Μ-law algorithm">
Μ-law algorithm

The µ-law algorithm (sometimes written "mu-law", often approximated as "u-law") is a companding algorithm, primarily used in 8-bit PCM digital telecommunication systems in North America and Japan. Companding algorithms reduce the dynamic range of an audio signal. In analog systems, this can increase the signal-to-noise ratio (SNR) achieved during transmission; in the digital domain, it can reduce the quantization error (hence increasing signal to quantization noise ratio). These SNR increases can be traded instead for reduced bandwidth for equivalent SNR.
It is similar to the A-law algorithm used in regions where digital telecommunication signals are carried on E-1 circuits, e.g. Europe.
Algorithm types.
There are two forms of this algorithm—an analog version, and a quantized digital version.
Continuous.
For a given input x, the equation for μ-law encoding is
where "μ" = 255 (8 bits) in the North American and Japanese standards. It is important to note that the range of this function is −1 to 1.
μ-law expansion is then given by the inverse equation:
The equations are culled from .
Discrete.
This is defined in ITU-T Recommendation G.711.
G.711 is unclear about how to code the values at the limit of a range (e.g. whether +31 codes to 0xEF or 0xF0).
However, G.191 provides example C code for a μ-law encoder which gives the following encoding. Note the difference between the positive and negative ranges, e.g. the negative range corresponding to +30 to +1 is −31 to −2. This is accounted for by the use of 1's complement (simple bit inversion) rather than 2's complement to convert a negative value to a positive value during encoding.
Implementation.
There are three ways of implementing a μ-law algorithm:
Usage justification.
This encoding is used because speech has a wide dynamic range. In the analog world, when mixed with a relatively constant background noise source, the finer detail is lost. Given that the precision of the detail is compromised anyway, and assuming that the signal is to be perceived as audio by a human, one can take advantage of the fact that the perceived acoustic intensity level or loudness is logarithmic by compressing the signal using a logarithmic-response operational amplifier (Weber-Fechner law). In telecommunications circuits, most of the noise is injected on the lines, thus after the compressor, the intended signal is perceived as significantly louder than the static, compared to an un-compressed source. This became a common solution, and thus, prior to common digital usage, the μ-law specification was developed to define an inter-compatible standard.
In digital systems this pre-existing algorithm had the effect of significantly reducing the number of bits needed to encode recognizable human voice. Using μ-law, a sample could be effectively encoded in as few as 8 bits, a sample size that conveniently matched the symbol size of most standard computers.
μ-law encoding effectively reduced the dynamic range of the signal, thereby increasing the coding efficiency while biasing the signal in a way that results in a signal-to-distortion ratio that is greater than that obtained by linear encoding for a given number of bits. This is an early form of perceptual audio encoding.
The μ-law algorithm is also used in the .au format, which dates back at least to the SPARCstation 1 by Sun Microsystems as the native method used by the /dev/audio interface, widely used as a de facto standard for sound on Unix systems. The au format is also used in various common audio APIs such as the classes in the sun.audio Java package in Java 1.1 and in some C# methods.
This plot illustrates how μ-law concentrates sampling in the smaller (softer) values. The abscissa represents the byte values 0-255 and the vertical axis is the 16-bit linear decoded value of μ-law encoding.
Comparison with A-law.
The µ-law algorithm provides a slightly larger dynamic range than the A-law at the cost of worse proportional distortion for small signals. By convention, A-law is used for an international connection if at least one country uses it.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41383" url="http://en.wikipedia.org/wiki?curid=41383" title="Multicast address">
Multicast address

A multicast address is a logical identifier for a group of hosts in a computer network, that are available to process datagrams or frames intended to be multicast for a designated network service. Multicast addressing can be used in the Link Layer (Layer 2 in the OSI model), such as Ethernet multicast, and at the Internet Layer (Layer 3 for OSI) for Internet Protocol Version 4 (IPv4) or Version 6 (IPv6) multicast.
IPv4.
IPv4 multicast addresses are defined by the leading address bits of "1110", originating from the classful network design of the early Internet when this group of addresses was designated as "Class D". The Classless Inter-Domain Routing (CIDR) prefix of this group is codice_1. The group includes the addresses from 224.0.0.0 to 239.255.255.255. Address assignments from within this range are specified in RFC 5771, an Internet Engineering Task Force (IETF) "Best Current Practice" document (BCP 51).
The following table is a list of notable well-known IPv4 addresses that are reserved for IP multicasting and that are registered with the Internet Assigned Numbers Authority (IANA).
Local subnetwork.
Addresses in the range of 224.0.0.0 to 224.0.0.255 are individually assigned by IANA and designated for multicasting on the local subnetwork only. For example, the Routing Information Protocol (RIPv2) uses 224.0.0.9, Open Shortest Path First (OSPF) uses 224.0.0.5 and 224.0.0.6, and Zeroconf mDNS uses 224.0.0.251. Routers must not forward these messages outside the subnet in which they originate.
Internetwork control block.
Addresses in the range 224.0.1.0 to 224.0.1.255 are individually assigned by IANA and designated the Internetwork Control Block. This block of addresses is used for traffic that must be routed through the public Internet, such as for applications of the Network Time Protocol (224.0.1.1).
AD-HOC block.
Addresses in the ranges 224.0.2.0 to 224.0.255.255, 224.3.0.0 to 224.4.255.255 and 233.252.0.0 to 233.255.255.255 are individually assigned by IANA and designated the AD-HOC block. These addresses are globally routed and are used for applications that don't fit either of the previously described purposes.
Source-specific multicast.
The 232.0.0.0/8 (IPv4) and FF3x::/32 (IPv6) block is reserved for use by source-specific multicast.
GLOP addressing.
The 233.0.0.0/8 range was originally assigned by RFC 2770 as an experimental, public statically assigned multicast address space for publishers and Internet service providers that wished to source content on the Internet. The allocation method is termed GLOP addressing and provides implementers a block of 255 addresses that is determined by their 16-bit autonomous system number (ASN) allocation. In a nutshell, the middle two octets of this block are formed from assigned ASNs, giving any operator assigned an ASN 256 globally unique multicast group addresses. The method is not applicable to the newer 32-bit extension AS numbers. RFC 3180, superseding RFC 2770, envisioned the use of the range for many-to-many multicast applications. This block has been one of the most successful multicast addressing schemes. Unfortunately, with only 256 multicast addresses available to each autonomous system, GLOP is not adequate for large-scale broadcasters.
Unicast-Prefix-Based IPv4 Multicast addresses.
The 234.0.0.0/8 range is assigned by RFC 6034 as a range of global IPv4 multicast address space provided to each organization that has /24 or larger globally routed unicast address space allocated; one multicast address is reserved per /24 of unicast space. A resulting advantage over GLOP is that the mechanisms in IPv4 and IPv6 become more similar.
Administratively Scoped IPv4 Multicast addresses.
The 239.0.0.0/8 range is assigned by RFC 2365 for private use within an organization. From the RFC, packets destined to administratively scoped IPv4 multicast addresses do not cross administratively defined organizational boundaries, and administratively scoped IPv4 multicast addresses are locally assigned and do not have to be globally unique. The RFC also discusses structuring the 239.0.0.0/8 range to be loosely similar to the scoped IPv6 multicast address range described in RFC 1884.
IPv6.
Multicast addresses in IPv6 have the prefix ff00::/8. IPv6 multicast addresses are generally formed from four bit groups, illustrated as follows:
The "prefix" holds the binary value 11111111 for any multicast address.
Currently, 3 of the 4 flag bits in the "flags" field are defined; the most-significant flag bit is reserved for future use. The other three flags are known as "R", "P" and "T".
Similar to unicast addresses, the prefix of IPv6 multicast addresses specifies their scope, however, the set of possible scopes is different. The 4-bit "sc" (or scope) field (bits 12 to 15) is used to indicate where the address is valid and unique.
The service is identified in the 112-bit "Group ID" field. For example, if ff02::101 refers to all Network Time Protocol (NTP) servers on the local network segment, then ff08::101 refers to all NTP servers in an organization's networks.
The "Group ID" field may be further divided for special multicast address types.
The following table is a partial list of well-known IPv6 multicast addresses that are registered with the Internet Assigned Numbers Authority (IANA).
Ethernet.
Ethernet frames with a value of 1 in the least-significant bit of the first octet of the destination address are treated as multicast frames and are flooded to all points on the network. While frames with ones in all bits of the destination address (codice_2) are sometimes referred to as broadcasts, Ethernet network equipment generally does not distinguish between multicast and broadcast frames. Modern Ethernet controllers filter received packets to reduce CPU load, by looking up the hash of a multicast destination address in a table, initialized by software, which controls whether a multicast packet is dropped or fully received.
802.11.
802.11 wireless networks use the same 01:00:5E:xx:xx:xx and 33:33:xx:xx:xx:xx MAC addresses for multicasting as Ethernet.

</doc>
<doc id="41385" url="http://en.wikipedia.org/wiki?curid=41385" title="Multipath propagation">
Multipath propagation

In wireless telecommunications, multipath is the propagation phenomenon that results in radio signals reaching the receiving antenna by two or more paths. Causes of multipath include atmospheric ducting, ionospheric reflection and refraction, and reflection from water bodies and terrestrial objects such as mountains and buildings.
The effects of multipath include constructive and destructive interference, and phase shifting of the signal. Destructive interference causes fading. Where the magnitudes of the signals arriving by the various paths have a distribution known as the Rayleigh distribution, this is known as Rayleigh fading. Where one component (often, but not necessarily, a line of sight component) dominates, a Rician distribution provides a more accurate model, and this is known as Rician fading.
Examples.
In facsimile and (analog) television transmission, multipath causes jitter and ghosting, seen as a faded duplicate image to the right of the main image. Ghosts occur when transmissions bounce off a mountain or other large object, while also arriving at the antenna by a shorter, direct route, with the receiver picking up two signals separated by a delay.
In radar processing, multipath causes ghost targets to appear, deceiving the radar receiver. These ghosts are particularly bothersome since they move and behave like the normal targets (which they echo), and so the receiver has difficulty in isolating the correct target echo. These problems can be overcome by incorporating a ground map of the radar's surroundings and eliminating all echoes which appear to originate below ground or above a certain height.
In digital radio communications (such as GSM) multipath can cause errors and affect the quality of communications. The errors are due to intersymbol interference (ISI). Equalisers are often used to correct the ISI. Alternatively, techniques such as orthogonal frequency division modulation and rake receivers may be used.
In a Global Positioning System receiver, Multipath Effect can cause a stationary receiver's output to indicate as if it were randomly jumping about or creeping. When the unit is moving the jumping or creeping is hidden, but it still degrades the displayed accuracy.
In wired media.
Multipath propagation may also happen in wired media, especially where impedance mismatch causes signal reflection. A well-known example is power line communication.
High-speed power line communication systems usually employ multi-carrier modulations (such as OFDM or Wavelet OFDM) to avoid the intersymbol interference that multipath propagation would cause.
The ITU-T G.hn standard provides a way to create a high-speed (up to 1 Gigabit/s) local area network using existing home wiring (power lines, phone lines and coaxial cables). G.hn uses OFDM with a cyclic prefix to avoid ISI. Because multipath propagation behaves differently in each kind of wire, G.hn uses different OFDM parameters (OFDM symbol duration, Guard Interval duration) for each media.
Mathematical modeling.
The mathematical model of the multipath can be presented using the method of the impulse response used for studying linear systems.
Suppose you want to transmit a single, ideal Dirac pulse of electromagnetic power at time 0, i.e.
At the receiver, due to the presence of the multiple electromagnetic paths, more than one pulse will be received (we suppose here that the channel has infinite bandwidth, thus the pulse shape is not modified at all), and each one of them will arrive at different times. In fact, since the electromagnetic signals travel at the speed of light, and since every path has a geometrical length possibly different from that of the other ones, there are different air travelling times (consider that, in free space, the light takes 3 μs to cross a 1 km span). Thus, the received signal will be expressed by
where formula_3 is the number of received impulses (equivalent to the number of electromagnetic paths, and possibly very large), formula_4 is the time delay of the generic formula_5 impulse, and formula_6 represent the complex amplitude (i.e., magnitude and phase) of the generic received pulse. As a consequence, formula_7 also represents the impulse response function formula_8 of the equivalent multipath model.
More in general, in presence of time variation of the geometrical reflection conditions, this impulse response is time varying, and as such we have
Very often, just one parameter is used to denote the severity of multipath conditions: it is called the multipath time, formula_12, and it is defined as the time delay existing between the first and the last received impulses
In practical conditions and measurement, the multipath time is computed by considering as last impulse the first one which allows to receive a determined amount of the total transmitted power (scaled by the atmospheric and propagation losses), e.g. 99%.
Keeping our aim at linear, time invariant systems, we can also characterize the multipath phenomenon by the channel transfer function formula_14, which is defined as the continuous time Fourier transform of the impulse response formula_8
where the last right-hand term of the previous equation is easily obtained by remembering that the Fourier transform of a Dirac pulse is a complex exponential function, an eigenfunction of every linear system.
The obtained channel transfer characteristic has a typical appearance of a sequence of peaks and valleys (also called "notches"); it can be shown that, on average, the distance (in Hz) between two consecutive valleys (or two consecutive peaks), is roughly inversely proportional to the multipath time. The so-called coherence bandwidth is thus defined as
For example, with a multipath time of 3 μs (corresponding to a 1 km of added on-air travel for the last received impulse), there is a coherence bandwidth of about 330 kHz.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41387" url="http://en.wikipedia.org/wiki?curid=41387" title="Multiple homing">
Multiple homing

In telecommunication, the term multiple homing has the following meanings: 
1. In telephone systems, the connection of a terminal facility so that it can be served by one or several switching centers. Multiple homing may use a single directory number. 
2. In telephone systems, the connection of a terminal facility to more than one switching center by separate access lines. Separate directory numbers are applicable to each switching center accessed. 
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41388" url="http://en.wikipedia.org/wiki?curid=41388" title="Multiplex baseband">
Multiplex baseband

In telecommunication, the term multiplex baseband has the following meanings: 
For example, the output of a group multiplexer consists of a band of frequencies from 60 kHz to 108 kHz. This is the group-level baseband that results from combining 12 voice-frequency input channels, having a bandwidth of 4 kHz each, including guard bands. In turn, 5 groups are multiplexed into a super group having a baseband of 312 kHz to 552 kHz. This baseband, however, does not represent a group-level baseband. Ten super groups are in turn multiplexed into one master group, the output of which is a baseband that may be used to modulate a microwave-frequency carrier.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41389" url="http://en.wikipedia.org/wiki?curid=41389" title="Multiplexing">
Multiplexing

In telecommunications and computer networks, multiplexing (sometimes contracted to muxing) is a method by which multiple analog message signals or digital data streams are combined into one signal over a shared medium. The aim is to share an expensive resource. For example, in telecommunications, several telephone calls may be carried using one wire. Multiplexing originated in telegraphy in the 1870s, and is now widely applied in communications. In telephony, George Owen Squier is credited with the development of telephone carrier multiplexing in 1910.
The multiplexed signal is transmitted over a communication channel, which may be a physical transmission medium (e.g. a cable). The multiplexing divides the capacity of the low-level communication channel into several high-level logical channels, one for each message signal or data stream to be transferred. A reverse process, known as demultiplexing, can extract the original channels on the receiver side.
A device that performs the multiplexing is called a multiplexer (MUX), and a device that performs the reverse process is called a demultiplexer (DEMUX or DMX).
Inverse multiplexing (IMUX) has the opposite aim as multiplexing, namely to break one data stream into several streams, transfer them simultaneously over several communication channels, and recreate the original data stream.
Types of multiplexing.
Multiple variable bit rate digital bit streams may be transferred efficiently over a single fixed bandwidth channel by means of statistical multiplexing. This is an asynchronous mode time-domain multiplexing which is a form of time-division multiplexing.
Digital bit streams can be transferred over an analog channel by means of code-division multiplexing techniques such as frequency-hopping spread spectrum (FHSS) and direct-sequence spread spectrum (DSSS).
In wireless communications, multiplexing can also be accomplished through alternating polarization (horizontal/vertical or clockwise/counterclockwise) on each adjacent channel and satellite, or through phased multi-antenna array combined with a multiple-input multiple-output communications (MIMO) scheme.
Space-division multiplexing.
In wired communication, space-division multiplexing simply implies different point-to-point wires for different channels. Examples include an analogue stereo audio cable, with one pair of wires for the left channel and another for the right channel, and a multipair telephone cable. Another example is a switched star network such as the analog telephone access network (although inside the telephone exchange or between the exchanges, other multiplexing techniques are typically employed) or a switched Ethernet network. A third example is a mesh network. Wired space-division multiplexing is typically not considered as multiplexing.
In wireless communication, space-division multiplexing is achieved by multiple antenna elements forming a phased array antenna. Examples are multiple-input and multiple-output (MIMO), single-input and multiple-output (SIMO) and multiple-input and single-output (MISO) multiplexing. For example, an IEEE 802.11n wireless router with "k" number of antennas makes it in principle possible to communicate with "k" multiplexed channels, each with a peak bit rate of 54 Mbit/s, thus increasing the total peak bit rate with a factor "k". Different antennas would give different multi-path propagation (echo) signatures, making it possible for digital signal processing techniques to separate different signals from each other. These techniques may also be utilized for space diversity (improved robustness to fading) or beamforming (improved selectivity) rather than multiplexing. 
Frequency-division multiplexing.
Frequency-division multiplexing (FDM) is inherently an analog technology. FDM achieves the combining of several signals into one medium by sending signals in several distinct frequency ranges over a single medium.
One of FDM's most common applications is the old traditional radio and television broadcasting from terrestrial, mobile or satellite stations, using the natural atmosphere of Earth, or the cable television. Only one cable reaches a customer's residential area, but the service provider can send multiple television channels or signals simultaneously over that cable to all subscribers without interference. Receivers must tune to the appropriate frequency (channel) to access the desired signal.
A variant technology, called wavelength-division multiplexing (WDM) is used in optical communications.
Time-division multiplexing.
Time-division multiplexing (TDM) is a digital (or in rare cases, analog) technology which uses time, instead of space or frequency, to separate the different data streams. TDM involves sequencing groups of a few bits or bytes from each individual input stream, one after the other, and in such a way that they can be associated with the appropriate receiver. If done sufficiently quickly, the receiving devices will not detect that some of the circuit time was used to serve another logical communication path.
Consider an application requiring four terminals at an airport to reach a central computer. Each terminal communicated at 2400 baud, so rather than acquire four individual circuits to carry such a low-speed transmission, the airline has installed a pair of multiplexers. A pair of 9600 baud modems and one dedicated analog communications circuit from the airport ticket desk back to the airline data center are also installed. Some modern web proxy servers (e.g. polipo) use TDM in HTTP pipelining of multiple HTTP transactions onto the same TCP/IP connection.
Carrier sense multiple access and multidrop communication methods are similar to time-division multiplexing in that multiple data streams are separated by time on the same medium, but because the signals have separate origins instead of being combined into a single signal, are best viewed as channel access methods, rather than a form of multiplexing.
Polarization-division multiplexing.
Polarization-division multiplexing uses the polarization of electromagnetic radiation to separate orthogonal channels. It is in practical use in both radio and optical communications, particularly in 100 Gbit/s per channel fiber optic transmission systems.
Orbital angular momentum multiplexing.
Orbital angular momentum multiplexing is a relatively new and experimental technique for multiplexing multiple channels of signals carried using electromagnetic radiation over a single path. It can potentially be used in addition to other physical multiplexing methods to greatly expand the transmission capacity of such systems. s of 2012[ [update]] it is still in its early research phase, with small-scale laboratory demonstrations of bandwidths of up to 2.5 Tbit/s over a single light path.
Code-division multiplexing.
Code division multiplexing (CDM) or spread spectrum is a class of techniques where several channels simultaneously share the same frequency spectrum, and this spectral bandwidth is much higher than the bit rate or symbol rate. One form is frequency hopping, another is direct sequence spread spectrum. In the latter case, each channel transmits its bits as a coded channel-specific sequence of pulses called chips. Number of chips per bit, or chips per symbol, is the spreading factor. This coded transmission typically is accomplished by transmitting a unique time-dependent series of short pulses, which are placed within chip times within the larger bit time. All channels, each with a different code, can be transmitted on the same fiber or radio channel or other medium, and asynchronously demultiplexed. Advantages over conventional techniques are that variable bandwidth is possible (just as in statistical multiplexing), that the wide bandwidth allows poor signal-to-noise ratio according to Shannon-Hartley theorem, and that multi-path propagation in wireless communication can be combated by rake receivers.
Another important application of CDMA is the Global Positioning System (GPS).
Relation to multiple access.
A multiplexing technique may be further extended into a multiple access method or channel access method, for example TDM into time division multiple access (TDMA) and statistical multiplexing into carrier sense multiple access (CSMA). A multiple access method makes it possible for several transmitters connected to the same physical medium to share its capacity.
Multiplexing is provided by the Physical Layer of the OSI model, while multiple access also involves a media access control protocol, which is part of the Data Link Layer.
The Transport layer in the OSI model as well as TCP/IP model provides statistical multiplexing of several application layer data flows to/from the same computer.
Code Division Multiplexing (CDM) is a technique in which each channel transmits its bits as a coded channel-specific sequence of pulses. This coded transmission typically is accomplished by transmitting a unique time-dependent series of short pulses, which are placed within chip times within the larger bit time. All channels, each with a different code, can be transmitted on the same fiber and asynchronously demultiplxed. Other widely used multiple access techniques are Time Division Multiple Access (TDMA) and Frequency Division Multiple Access (FDMA).
Code Division Multiplex techniques are used as an access technology, namely Code Division Multiple Access (CDMA), in Universal Mobile Telecommunications System (UMTS) standard for the third generation (3G) mobile communication identified by the ITU.
Application areas.
Telegraphy.
The earliest communication technology using electrical wires, and therefore sharing an interest in the economies afforded by multiplexing, was the electric telegraph. Early experiments allowed two separate messages to travel in opposite directions simultaneously, first using an electric battery at both ends, then at only one end.
Telephony.
In telephony, a customer's telephone line now typically ends at the remote concentrator box, where it is multiplexed along with other telephone lines for that neighborhood or other similar area. The multiplexed signal is then carried to the central switching office on significantly fewer wires and for much further distances than a customer's line can practically go. This is likewise also true for digital subscriber lines (DSL).
Fiber in the loop (FITL) is a common method of multiplexing, which uses optical fiber as the backbone. It not only connects POTS phone lines with the rest of the PSTN, but also replaces DSL by connecting directly to Ethernet wired into the home. Asynchronous Transfer Mode is often the communications protocol used.
Because all the phone (and data) lines have been clumped together, none of them can be accessed except through a demultiplexer. Where such demultiplexers are uncommon, this provides for more-secure communications, though the connections are not typically encrypted. 
Cable TV has long carried multiplexed television channels, and late in the 20th century began offering the same services as telephone companies. IPTV also depends on multiplexing.
Video processing.
In video editing and processing systems, multiplexing refers to the process of interleaving audio and video into one coherent MPEG transport stream (time-division multiplexing).
In digital video, such a transport stream is normally a feature of a container format which may include metadata and other information, such as subtitles. The audio and video streams may have variable bit rate. Software that produces such a transport stream and/or container is commonly called a statistical multiplexor or muxer. A demuxer is software that extracts or otherwise makes available for separate processing the components of such a stream or container.
Digital broadcasting.
In digital television and digital radio systems, several variable bit-rate data streams are multiplexed together to a fixed bitrate transport stream by means of statistical multiplexing. This makes it possible to transfer several video and audio channels simultaneously over the same frequency channel, together with various services.
In the digital television systems, this may involve several standard definition television (SDTV) programmes (particularly on DVB-T, DVB-S2, ISDB and ATSC-C), or one HDTV, possibly with a single SDTV companion channel over one 6 to 8 MHz-wide TV channel. The device that accomplishes this is called a statistical multiplexer. In several of these systems, the multiplexing results in an MPEG transport stream. The newer DVB standards DVB-S2 and DVB-T2 has the capacity to carry several HDTV channels in one multiplex. Even the original DVB standards can carry more HDTV channels in a multiplex if the most advanced MPEG-4 compressions hardware is used.
On communications satellites which carry broadcast television networks and radio networks, this is known as multiple channel per carrier or MCPC. Where multiplexing is not practical (such as where there are different sources using a single transponder), single channel per carrier mode is used.
Signal multiplexing of satellite TV and radio channels is typically carried out in a central signal playout and uplink centre, such as SES Platform Services in Germany, which provides playout, digital archiving, encryption, and satellite uplinks, as well as multiplexing, for hundreds of digital TV and radio channels.
In digital radio, both the Digital Audio Broadcasting (DAB) Eureka 147 system of digital audio broadcasting and the in-band on-channel HD Radio, FMeXtra, and Digital Radio Mondiale systems can multiplex channels. This is essentially required with DAB-type transmissions (where a multiplex is called a DAB ensemble), but is entirely optional with IBOC systems.
Analog broadcasting.
In FM broadcasting and other analog radio media, multiplexing is a term commonly given to the process of adding subcarriers to the audio signal before it enters the transmitter, where modulation occurs. (In fact, the stereo multiplex signal can be generated using time-division multiplexing, by switching between the two (left channel and right channel) input signals at an ultrasonic rate (the subcarrier), and then filtering out the higher harmonics.) Multiplexing in this sense is sometimes known as MPX, which in turn is also an old term for stereophonic FM, seen on stereo systems since the 1960s.
Other meanings.
In spectroscopy the term is used in a related sense to indicate that the experiment is performed with a mixture of frequencies at once and their respective response unravelled afterwards using the Fourier transform principle.
In computer programming, it may refer to using a single in-memory resource (such as a file handle) to handle multiple external resources (such as on-disk files).
Some electrical multiplexing techniques do not require a physical "multiplexer" device, they refer to a "keyboard matrix" or "Charlieplexing" design style:
References.
</dl>

</doc>
<doc id="41391" url="http://en.wikipedia.org/wiki?curid=41391" title="Narrative traffic">
Narrative traffic

Narrative traffic is data communications consisting of plain or encrypted messages written in a natural language and transmitted in accordance with standard formats and procedures. 
Examples of narrative traffic include:
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41392" url="http://en.wikipedia.org/wiki?curid=41392" title="Narrowband modem">
Narrowband modem

In telecommunication, a narrowband modem is a modem whose modulated output signal has an essential frequency spectrum that is limited to that which can be wholly contained within, and faithfully transmitted through, a voice channel with a nominal 4 kHz bandwidth. 
"Note:" High frequency (HF) modems are limited to operation over a voice channel with a nominal 3 kHz bandwidth.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41394" url="http://en.wikipedia.org/wiki?curid=41394" title="National Communications System">
National Communications System

The National Communications System (NCS) was an office within the United States Department of Homeland Security charged with enabling national security and emergency preparedness communications (NS/EP telecommunications) using the national telecommunications system. The NCS was disbanded by Executive Order 13618 on July 6, 2012.
Background and history.
The genesis of the NCS began in 1962 after the Cuban missile crisis when communications problems among the United States, the Union of Soviet Socialist Republics, the North Atlantic Treaty Organization, and foreign heads of state threatened to complicate the crisis further. After the crisis, President John F. Kennedy ordered an investigation of national security communications, and the National Security Council (NSC) formed an interdepartmental committee to examine the communications networks and institute changes. This interdepartmental committee recommended the formation of a single unified communications system to serve the President, Department of Defense, diplomatic and intelligence activities, and civilian leaders. Consequently, in order to provide better communications support to critical Government functions during emergencies, President Kennedy established the National Communications System by a Presidential Memorandum on August 21, 1963. The NCS mandate included linking, improving, and extending the communications facilities and components of various Federal agencies, focusing on interconnectivity and survivability.
On April 3, 1984, President Ronald Reagan signed Executive Order (E.O.) 12472 which broadened the NCS' national security and emergency preparedness (NS/EP) capabilities and superseded President Kennedy's original 1963 memorandum. The NCS expanded from its original six members to an interagency group of 23 Federal departments and agencies, and began coordinating and planning NS/EP telecommunications to support crises and disasters.
With the addition of the Office of the Director of National Intelligence (ODNI) on September 30, 2007, the NCS membership currently stands at 24 members.
Each NCS member organization is represented on the NCS through the Committee of Principals (COP) -- and its subordinate Council of Representatives (COR). The COP, formed as a result of Executive Order 12472, provides advice and recommendations to the NCS and the National Security Council through the President's Critical Infrastructure Protection Board on NS/EP telecommunications and its ties to other critical infrastructures. The NCS also participates in joint industry-Government planning through its work with the President's National Security Telecommunications Advisory Committee (NSTAC), with the NCS's National Coordinating Center for Telecommunications (NCC) and the NCC's subordinate Information Sharing and Analysis Center (ISAC).
After nearly 40 years with the Secretary of Defense serving as its Executive Agent, President George W. Bush transferred the National Communications System to the Department of Homeland Security (DHS). The NCS was one of 22 Federal agencies transferred to the Department on March 1, 2003, in accordance with Executive Order 13286. A revised Executive Order 12472 reflects the changes of E.O. 13286. On November 15, 2005, the NCS became part of the Department's Directorate for Preparedness after nearly two years under the Information Analysis and Infrastructure Protection Directorate. In March 2007 the NCS became an entity of the National Protection and Programs Directorate. The DHS Under Secretary for National Protection and Programs Directorate served as the NCS Manager.
On July 6, 2012, President Barack Obama signed Executive Order 13618, which revoked Executive Order 12472, thus eliminating the NCS. A ceremony to retire the colors of the NCS and to celebrate the legacy of the organization was held on August 30, 2012 in Arlington, VA.
Services.
In fulfillment of their mission to enable emergency communications, the NCS has created a number of different services.
References.
</dl>
External links.
</dl>
Category:Government agencies disestablished in 2012

</doc>
<doc id="41396" url="http://en.wikipedia.org/wiki?curid=41396" title="National Information Infrastructure">
National Information Infrastructure

The National Information Infrastructure (NII) was the product of the High Performance Computing Act of 1991. It was a telecommunications policy buzzword, which was popularized during the Clinton Administration under the leadership of Vice-President Al Gore. It was a proposed, advanced, seamless web of public and private communications networks, interactive services, interoperable computer hardware and software, computers, databases, and consumer electronics to put vast amounts of information at users' fingertips. 
NII was to have included more than just the physical facilities (more than the cameras, scanners, keyboards, telephones, fax machines, computers, switches, compact disks, video and audio tape, cable, wire, satellites, optical fiber transmission lines, microwave nets, switches, televisions, monitors, and printers) used to transmit, store, process, and display voice, data, and images; it was also to encompass a wide range of interactive functions, user-tailored services, and multimedia databases that were interconnected in a technology-neutral manner that will favor no one industry over any other.

</doc>
<doc id="41400" url="http://en.wikipedia.org/wiki?curid=41400" title="Negative-acknowledge character">
Negative-acknowledge character

In the ASCII code, the NAK character is 21 (decimal), or ^U (CTRL-U). EBCDIC uses 0x3D. Unicode also defines a visible representation at U+2415(␕).

</doc>
<doc id="41402" url="http://en.wikipedia.org/wiki?curid=41402" title="Neper">
Neper

The neper (unit symbol Np) is a logarithmic unit for ratios of measurements of physical field and power quantities, such as gain and loss of electronic signals. The unit's name is derived from the name of John Napier, the inventor of logarithms. As is the case for the decibel and bel, the neper is unit of the International System of Quantities (ISQ), but not part of the International System of Units (SI), but it is accepted for use alongside the SI.
Definition.
Like the decibel, the neper is a unit in a logarithmic scale. While the bel uses the decadic (base-10) logarithm to compute ratios, the neper uses the natural logarithm, based on Euler's number ("e" ≈ 2.71828). The value of a ratio in nepers is given by
where formula_2 and formula_3 are the values of interest, and "ln" is the natural logarithm.
In the ISQ, the neper is defined as 1 Np = 1.
Units.
The neper is defined in terms of ratios of field quantities (for example, voltage or current amplitudes in electrical circuits, or pressure in acoustics), whereas the decibel was originally defined in terms of power ratios. A power ratio 10 log "r" dB is equivalent to a field-quantity ratio 20 log "r" dB, since power is proportional to the square (Joule's laws) of the amplitude. Hence the neper and dB are related via:
and
The decibel and the neper have a fixed ratio to each other. The (voltage) level ratio is
Like the decibel, the neper is a dimensionless unit. The International Telecommunication Union (ITU) recognizes both units.
Applications.
The neper is a natural linear unit of relative difference, meaning in nepers (logarithmic units), relative differences add, rather than multiply. This property is shared with logarithmic units in other bases, such as the bel.
Particular to the neper however is that the derived unit of "centineper" is infinitesimally equal to percentage difference – hence approximately equal for very small differences – since the derivative of the natural log (at 1) is 1; this is not shared with other logarithmic units, which introduce a scaling factor due to the derivative not being unity. The centineper can thus be used as a linear replacement for percentage differences. The linear approximation for small percentage differences,
is widely used, particularly in finance—see for example the Fisher equation. However, it is only approximate, with error increasing for large percentage changes. Measured instead in centinepers, these linear approximations can be replaced with exact equalities, and applicable to any magnitude change, by defining the following centineper quantity for any change formula_8
formula_9

</doc>
<doc id="41403" url="http://en.wikipedia.org/wiki?curid=41403" title="Net gain (telecommunications)">
Net gain (telecommunications)

In telecommunications, net gain is the overall gain of a transmission circuit. Net gain is measured by applying a test signal at an appropriate power level at the input port of a circuit and measuring the power delivered at the output port. The net gain in dB is calculated by taking 10 times the common logarithm of the ratio of the output power to the input power.
The net gain expressed in dB may be positive or negative. If the net gain expressed in dB is negative, it is also called the "net loss". If the net gain is expressed as a ratio, and the ratio is less than unity, a net loss is indicated.
The test signal must be chosen so that its power level is within the usual operating range of the circuit being tested.

</doc>
<doc id="41404" url="http://en.wikipedia.org/wiki?curid=41404" title="Net operation">
Net operation

In telecommunication, net operation is the operation of an organization of stations capable of direct communication on a common channel or frequency. 
"Note:" Net operations (a) allow participants to conduct ordered conferences among participants who usually have common information needs or related functions to perform, (b) are characterized by adherence to standard formats and procedures, and (c) are responsive to a common supervisory station, called the ""net control station"," which permits access to the net and maintains net operational discipline.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41406" url="http://en.wikipedia.org/wiki?curid=41406" title="Network architecture">
Network architecture

Network architecture is the design of a communications network. It is a framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as data formats used in its operation.
In telecommunication, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated.
The network architecture of the Internet is predominantly expressed by its use of the Internet Protocol Suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links.
OSI network model.
The Open Systems Interconnection model (OSI model) is a product of the Open Systems Interconnection effort at the International Organization for Standardisation (ISO) . It is a way of sub-dividing a communications system into smaller parts called layers. A layer is a collection of similar functions that provide services to the layer above it and receives services from the layer below it. On each layer, an instance provides services to the instances at the layer above and requests service from the layer below.
Physical layer.
The physical layer defines the electrical and physical specifications for devices. In particular, it defines the relationship between a device and a transmission medium, such as a copper or optical cable. This includes the layout of pins, voltages, cable specifications, hubs, repeaters, network adapters, host bus adapters (HBA used in storage area networks) and more. Its main task is the transmission of a stream of bits over a communication channel.
Data-linking layer.
The data link layer provides the functional and procedural means to transfer data between network entities and to detect and possibly correct errors that may occur in the physical layer. Originally, this layer was intended for point-to-point and point-to-multipoint media, characteristic of wide area media in the telephone system. Local area network architecture, which included broadcast-capable multiaccess media, was developed independently of the ISO work in IEEE Project 802. IEEE work assumed sublayering and management functions not required for WAN use. In modern practice, only error detection, not flow control using sliding window, is present in data link protocols such as Point-to-Point Protocol (PPP), and, on local area networks, the IEEE 802.2 LLC layer is not used for most protocols on the Ethernet, and on other local area networks, its flow control and acknowledgment mechanisms are rarely used. Sliding-window flow control and acknowledgment is used at the transport layer by protocols such as TCP, but is still used in niches where X.25 offers performance advantages. Simply, its main job is to create and recognize the frame boundary. This can be done by attaching special bit patterns to the beginning and the end of the frame. The input data is broken up into frames.
Network layer.
The network layer provides the functional and procedural means of transferring variable length data sequences from a source host on one network to a destination host on a different network, while maintaining the quality of service requested by the transport layer (in contrast to the data link layer which connects hosts within the same network). The network layer performs network routing functions, and might also perform fragmentation and reassembly, and report delivery errors. Routers operate at this layer—sending data throughout the extended network and making the Internet possible. This is a logical addressing scheme; values are chosen by the network engineer. The addressing scheme is not hierarchical. It controls the operation of the subnet and determine the routing strategies between IMP and ensures that all the packs are correctly received at the destination in the proper order.
Transport layer.
The transport layer provides transparent transfer of data between end users, providing reliable data transfer services to the upper layers. The transport layer controls the reliability of a given link through flow control, segmentation/desegmentation, and error control. Some protocols are state and connection oriented. This means that the transport layer can keep track of the segments and retransmit those that fail. The transport layer also provides the acknowledgement of the successful data transmission and sends the next data if no errors occurred. Some transport layer protocols (such as TCP, but not UDP) support virtual circuits that provide connection-oriented communication over an underlying packet-oriented datagram network, where it assures the delivery of packets in the order in which they were sent and that they are free of errors. The datagram transportation deliver the packets randomly and broadcast it to multiple nodes.
The transport layer multiplexes several streams on to one physical channel. The transport headers indicate which message belongs to which connection.
Session layer.
This layer provides a user interface to the network where the user negotiates to establish a connection. The user must provide the remote address to be contacted. The operation of setting up a session between two processes is known as "binding". In some protocols, it is merged with the transport layer. Its main work is to transfer data from the other application to this application so this application is mainly used for transferred layer.
Presentation layer.
The presentation layer establishes context between entities on the application layer, in which the higher-layer entities may use different syntax and semantics if the presentation service provides a mapping between them. If a mapping is available, presentation service data units are encapsulated into session protocol data units, and passed down the stack. This layer provides independence from data representation (e.g. encryption) by translating between application and network formats. The presentation layer transforms data into the form that the application accepts. This layer formats and encrypts data to be sent across a network. It is sometimes called the syntax layer. The original presentation structure used the basic encoding rules of Abstract Syntax Notation One (ASSN), with capabilities such as converting an BODICE-coded text file to an ASCII-coded file, or serialization of objects and other data structures from and to XML.
Application layer.
The application layer is the OSI layer closest to the end user, which means that both the OSI application layer and the user interact directly with the software application. This layer interacts with software applications that implement a communicating component. Such application programs fall outside the scope of the OSI model. Application layer functions typically include identifying communication partners, determining resource availability, and synchronizing communication. When identifying communication partners, the application layer determines the identity and availability of communication partners for an application with data to transmit.
Distributed computing.
In distinct usage in distributed computing, the term "network architecture" often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a "network". For example, the applications architecture of the public switched telephone network (PSTN) has been termed the Advanced Intelligent Network. There are any number of specific classifications but all lie on a continuum between the dumb network (e.g. Internet) and the intelligent computer network (e.g. the telephone network). Other networks contain various elements of these two classical types to make them suitable for various types of applications. Recently the context aware network, which is a synthesis of two, has gained much interest with its ability to combine the best elements of both.
A popular example of such usage of the term in distributed applications, as well as PVCs (permanent virtual circuits), is the organization of nodes in peer-to-peer (P2P) services and networks. P2P networks usually implement overlay networks running over an underlying physical or logical network. These overlay network may implement certain organizational structures of the nodes according to several distinct models, the network architecture of the system.
Network architecture is a broad plan that specifies everything necessary for two application programs on different networks on an Internet to be able to work together effectively.

</doc>
<doc id="41407" url="http://en.wikipedia.org/wiki?curid=41407" title="Network engineering">
Network engineering

In telecommunications, network engineering may refer to:
See also.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41408" url="http://en.wikipedia.org/wiki?curid=41408" title="Network interface">
Network interface

In computing, a network interface is a system's (software and/or hardware) interface between two pieces of equipment or protocol layers in a computer network.
A network interface will usually have some form of network address. This may consist of a node Id and a port number or may be a unique node Id in its own right.
Network interfaces provide standardized functions such as passing messages, connecting and disconnecting, etc.

</doc>
<doc id="41409" url="http://en.wikipedia.org/wiki?curid=41409" title="Network interface device">
Network interface device

In telecommunications, a Network Interface Device (NID) is a device that serves as the demarcation point between the carrier's local loop and the customer's premises wiring. Outdoor telephone NIDs also provide the subscriber with access to the station wiring and serve as a convenient test point for verification of loop integrity and of the subscriber’s inside wiring. 
Generically, an NID may also be called a Network Interface Unit (NIU), Telephone Network Interface (TNI), System Network Interface (SNI), or telephone network box. Australia's National Broadband Network uses the term Network Termination Device or NTD. A smartjack is a type of NID with capabilities beyond simple electrical connection, such as diagnostics. An Optical Network Terminal (ONT) is a type of NID used with fiber-to-the-premises applications.
Wiring termination.
The simplest NIDs are essentially just a specialized set of wiring terminals. These will typically take the form of a small, weather-proof box, mounted on the outside of the building. The telephone line from the telephone company will enter the NID and be connected to one side. The customer connects their wiring to the other side. A single NID enclosure may contain termination for a single line or multiple lines.
In its role as the demarcation point, the NID separates the telephone company's equipment from the customer's wiring and equipment. The telephone company owns the NID itself, and all wiring up to it. Anything past the NID is the customer's responsibility. To facilitate this, there is typically a test jack inside the NID. Accessing the test jack disconnects the customer premises wiring from the network and allows the customer to plug a "known good" telephone into the jack to isolate trouble. If the telephone works at the test jack, the problem is the customer's wiring, and the customer is responsible for repair. If the telephone does not work, the line is faulty and the telephone company is responsible for repair.
Most NIDs also include "circuit protectors", which are surge protectors for a telephone line. They protect customer wiring, equipment, and personnel from any transient energy on the line, such as from a lightning strike to a telephone pole.
Simple NIDs contain no "intelligence" or "logic"; they are "dumb" devices. They have no capabilities beyond wiring termination, circuit protection, and providing a place to connect test equipment.
Smartjack.
Several types of NIDs provide more than just a terminal for the connection of wiring. Such NIDs are colloquially called "smartjacks" or "Intelligent Network Interface Devices" (INIDs) as an indication of their built-in "intelligence", as opposed to a simple NID, which is just a wiring device. Smartjacks are typically used for more complicated types of telecommunications service, such as T1 lines. Plain old telephone service lines generally cannot be equipped with smartjacks.
Despite the name, most smartjacks are much more than a simple telephone jack. One common form for a smartjack is a circuit board with a face plate on one edge, mounted in an enclosure.
A smartjack may provide signal conversion, converting codes and protocols (e.g. framing types) to the type needed by the customer equipment. It may buffer and/or regenerate the signal, to compensate for signal degradation from line transmission, similar to what a repeater does.
Smartjacks also typically provide diagnostic capabilities. A very common capability provided by a smartjack is loopback, such that the signal from the telephone company is transmitted back to the telephone company. This allows the telephone company to test the line from the central office, without the need to have test equipment at the customer site. The telephone company usually has the ability to remotely activate loopback, without even needing personnel at the customer site. When looped back, the customer equipment is disconnected from the line.
Additional smartjack diagnostic capabilities include alarm indication signal, which reports trouble at one end of the line to the far end. This helps the telephone company know if trouble is present in the line, the smartjack, or customer equipment. Indicator lights to show configuration, status, and alarms are also common.
Smartjacks typically derive their operating power from the telephone line, rather than relying on premises electrical power, although this is not a universal rule.
Optical network terminals.
In fiber-to-the-premises systems, the signal is transmitted to the customer premises using fiber optic technologies. Unlike many conventional telephone technologies, this does not provide power for premises equipment, nor is it suitable for direct connection to customer equipment. An ONT (optical network terminal) is used to terminate the fiber optic line, demultiplex the signal into its component parts (voice telephone, television, and Internet), and provide power to customer telephones. As the ONT must derive its power from the customer premises electrical supply, many ONTs have the option for a battery backup, to maintain service in the event of a power outage.
Environmental Conditions.
According to Telcordia , requirements for telecommunications NIDs vary based on three categories of environmental conditions:
Service providers must decide which condition best suits their application.

</doc>
<doc id="41410" url="http://en.wikipedia.org/wiki?curid=41410" title="Network management">
Network management

In computer networks, network management is the operation, administration, maintenance, and provisioning (OAMP) of networked systems. Network management is essential to command and control practices and is generally carried out of a network operations center.
A common way of characterizing network management functions is FCAPS—Fault, Configuration, Accounting, Performance and Security.
Functions that are performed as part of network management accordingly include controlling, planning, allocating, deploying, coordinating, and monitoring the resources of a network, network planning, frequency allocation, predetermined traffic routing to support load balancing, cryptographic key distribution authorization, configuration management, fault management, security management, performance management, bandwidth management, Route analytics and accounting management.
Data for network management is collected through several mechanisms, including agents installed on infrastructure, synthetic monitoring that simulates transactions, logs of activity, sniffers and real user monitoring. In the past network management mainly consisted of monitoring whether devices were up or down; today performance management has become a crucial part of the IT team's role which brings about a host of challenges—especially for global organizations.
Network management does not include user terminal equipment.
Technologies.
A small number of accessory methods exist to support network and network device management. Access methods include the SNMP, command-line interface, custom XML, CMIP, Windows Management Instrumentation (WMI), Transaction Language 1, CORBA, NETCONF, and the Java Management Extensions (JMX). Internet service providers (ISP) use a technology known as deep packet inspection in order to regulate network congestion and lessen Internet bottlenecks.
Schemas include the WBEM, the Common Information Model, and MTOSI amongst others.
In the United States, Medical Service Providers provide a niche marketing utility for managed service providers as HIPAA legislation consistently increases demands for knowledgeable providers. Medical Service Providers are liable for the protection of their clients' confidential information, including in an electronic realm. This liability creates a significant need for managed service providers who can provide secure infrastructure for transportation of medical data.

</doc>
<doc id="41411" url="http://en.wikipedia.org/wiki?curid=41411" title="Network operating system">
Network operating system

Network operating system refers, to software that implements an operating system of some kind that is oriented to computer networking.
For example, one that runs on a server and enables the server to manage data, users, groups, security, applications, and other networking functions. The network operating system is designed to allow shared file and printer access among multiple computers in a network, typically a local area network (LAN), a private network or to other networks.
Network operating systems can be embedded in a router or hardware firewall that operates the functions in the network layer (layer 3) of the OSI model.
Peer-to-Peer.
In a peer-to-peer network operating system users are allowed to share resources and files located on their computers and access shared resources from others. This system is not based with having a file server or centralized management source. A peer-to-peer network sets all connected computers equal; they all share the same abilities to use resources available on the network.
Advantages
Disadvantages
Client/Server.
Network operating systems can be based on a client/server architecture in which a server enables multiple clients to share resources.
Client/server network operating systems allow the network to centralize functions and applications in one or more dedicated file servers. The server is the center of the system, allowing access to resources and instituting security. The network operating system provides the mechanism to integrate all the components on a network to allow multiple users to simultaneously share the same resources regardless of physical location.
Advantages
Disadvantages
Security Issues Involved in using a Client/Server Network.
In a client/server network security issues may evolve at three different locations: the client, the network, and the server. All three points need to be monitored for unauthorized activity and need to be secured against hackers or eavesdroppers.
The Client
The client is the end user of the network and needs to be secured the most. The client end usually exposes data through the screen of the computer. Client connections to server should be secured through passwords and upon leaving their workstations clients should make sure that their connection to the server is securely cut off in order to make sure that no hackers or intruders are able to reach the server data. Not only securing the workstations connection to the server is important but also securing the files on the workstation (client) is important as it ensures that no hackers are able to reach the system. Another possibility is that of introducing a virus or running unauthorized software on the client workstation thus threatening the entire information bank at the server (Exforsys Inc., 2007).
The users themselves could also be a security threat if they purposely leave their IDs logged in or use easy IDs and passwords to enable hacking. Users may also be sharing their passwords in order to give the hackers access to confidential data (Wilson, Lin, & Craske, 1999). 
This can be overcome by giving passwords to each client and regularly asking clients to change their passwords. Also passwords should be checked for guessability and for their strength and uniqueness.
The Network
The network allows transmission of data from the clients to the server. There are several points on the network where a hacker could eavesdrop or steal important packets of information. These packets may contain important confidential data such as passwords or company details.
It is important that these networks are secured properly to keep unauthorized professionals away from all the data stored on the server. This can be done by encrypting important data being sent on the network. However, encryption may not be the only possible way of protecting networks as hackers can work their way around encryption. Another method could be conducting security audits regularly and ensuring identification and authorization of individuals at all points along the network. This should discourage potential hackers (Wilson, Lin, & Craske, 1999).
Making the entire environment difficult to impersonate also makes sure that the clients are reaching the true files and applications on the server and that the server is providing information to authorized personnel only.
The Server
The server can be secured by placing all the data in a secure, centralized location that is protected through permitting access to authorized personnel only. Virus protection should also be available on server computers as neal vast amounts of data can be infected. Regular upgrades should be provided to the servers as the software and the applications need to be updated. 
Even the entire body of data on a server could be encrypted in order to make sure that reaching the data would require excessive time and effort (Wilson, Neal, & Craske, 1999).

</doc>
<doc id="41413" url="http://en.wikipedia.org/wiki?curid=41413" title="Network topology">
Network topology

Network topology is the arrangement of the various elements (links, nodes, etc.) of a computer network. Essentially, it is the topological structure of a network and may be depicted physically or logically. "Physical topology" is the placement of the various components of a network, including device location and cable installation, while "logical topology" illustrates how data flows within a network, regardless of its physical design. Distances between nodes, physical interconnections, transmission rates, or signal types may differ between two networks, yet their topologies may be identical.
An example is a local area network (LAN): Any given node in the LAN has one or more physical links to other devices in the network; graphically mapping these links results in a geometric shape that can be used to describe the physical topology of the network. Conversely, mapping the data flow between the components determines the logical topology of the network.
Topology.
There are two basic categories of network topologies: physical topologies and logical topologies.
The cabling layout used to link devices is the physical topology of the network. This refers to the layout of cabling, the locations of nodes, and the interconnections between the nodes and the cabling. The physical topology of a network is determined by the capabilities of the network access devices and media, the level of control or fault tolerance desired, and the cost associated with cabling or telecommunications circuits.
The logical topology in contrast, is the way that the signals act on the network media, or the way that the data passes through the network from one device to the next without regard to the physical interconnection of the devices. A network's logical topology is not necessarily the same as its physical topology. For example, the original twisted pair Ethernet using repeater hubs was a logical bus topology with a physical star topology layout. Token Ring is a logical ring topology, but is wired as a physical star from the Media Access Unit.
The logical classification of network topologies generally follows the same classifications as those in the physical classifications of network topologies but describes the path that the "data" takes between nodes being used as opposed to the actual "physical" connections between nodes. The logical topologies are generally determined by network protocols as opposed to being determined by the physical layout of cables, wires, and network devices or by the flow of the electrical signals, although in many cases the paths that the electrical signals take between nodes may closely match the logical flow of data, hence the convention of using the terms "logical topology" and "signal topology" interchangeably.
Logical topologies are often closely associated with Media Access Control methods and protocols. Logical topologies are able to be dynamically reconfigured by special types of equipment such as routers and switches.
The study of network topology recognizes eight basic topologies: point-to-point, bus, star, ring or circular, mesh, tree, hybrid, or daisy chain.
Point-to-point.
The simplest topology with a permanent link between two endpoints. Switched point-to-point topologies are the basic model of conventional telephony. The value of a permanent point-to-point network is unimpeded communications between the two endpoints. The value of an on-demand point-to-point connection is proportional to the number of potential pairs of subscribers and has been expressed as Metcalfe's Law.
Mesh.
The value of fully meshed networks is proportional to the exponent of the number of subscribers, assuming that communicating groups of any two endpoints, up to and including all the endpoints, is approximated by Reed's Law.
Tree.
A tree topology is essentially a combination of bus topology and star topology. The nodes of bus topology are replaced with standalone star topology networks. This results in both disadvantages of bus topology and advantages of star topology.
For example, if the connection between two groups of networks is broken down due to breaking of the connection on the central linear core, then those two groups cannot communicate, much like nodes of a bus topology. However, the star topology nodes will effectively communicate with each other.
It has a root node, intermediate nodes, and ultimate nodes. This structure is arranged in a hierarchical form and any intermediate node can have any number of the child nodes.
But the tree topology is practically impossible to construct, because the node in the network is nothing, but the computing device can have maximum one or two connections, so we cannot attach more than 2 child nodes to the computing device (or parent node). There are many sub structures under tree topology, but the most convenient is B-tree topology whereby finding errors is relatively easy.
Hybrid.
Hybrid networks use a combination of any two or more topologies, in such a way that the resulting network does not exhibit one of the standard topologies (e.g., bus, star, ring, etc.). For example a tree network connected to a tree network is still a tree network topology. A hybrid topology is always produced when two different basic network topologies are connected. Two common examples for Hybrid network are: "star ring network" and "star bus network"
While grid and torus networks have found popularity in high-performance computing applications, some systems have used genetic algorithms to design custom networks that have the fewest possible hops in between different nodes. Some of the resulting layouts are nearly incomprehensible, although they function quite well.
A Snowflake topology is really a "Star of Stars" network, so it exhibits characteristics of a hybrid network topology but is not composed of two different basic network topologies being connected.
Daisy chain.
Except for star-based networks, the easiest way to add more computers into a network is by daisy-chaining, or connecting each computer in series to the next. If a message is intended for a computer partway down the line, each system bounces it along in sequence until it reaches the destination. A daisy-chained network can take two basic forms: linear and ring.
Centralization.
The star topology reduces the probability of a network failure by connecting all of the peripheral nodes (computers, etc.) to a central node. When the physical star topology is applied to a logical bus network such as Ethernet, this central node (traditionally a hub) rebroadcasts all transmissions received from any peripheral node to all peripheral nodes on the network, sometimes including the originating node. All peripheral nodes may thus communicate with all others by transmitting to, and receiving from, the central node only. The failure of a transmission line linking any peripheral node to the central node will result in the isolation of that peripheral node from all others, but the remaining peripheral nodes will be unaffected. However, the disadvantage is that the failure of the central node will cause the failure of all of the peripheral nodes.
If the central node is "passive", the originating node must be able to tolerate the reception of an echo of its own transmission, delayed by the two-way round trip transmission time (i.e. to and from the central node) plus any delay generated in the central node. An "active" star network has an active central node that usually has the means to prevent echo-related problems.
A tree topology (a.k.a. hierarchical topology) can be viewed as a collection of star networks arranged in a hierarchy. This tree has individual peripheral nodes (e.g. leaves) which are required to transmit to and receive from one other node only and are not required to act as repeaters or regenerators. Unlike the star network, the functionality of the central node may be distributed.
As in the conventional star network, individual nodes may thus still be isolated from the network by a single-point failure of a transmission path to the node. If a link connecting a leaf fails, that leaf is isolated; if a connection to a non-leaf node fails, an entire section of the network becomes isolated from the rest.
To alleviate the amount of network traffic that comes from broadcasting all signals to all nodes, more advanced central nodes were developed that are able to keep track of the identities of the nodes that are connected to the network. These network switches will "learn" the layout of the network by "listening" on each port during normal data transmission, examining the data packets and recording the address/identifier of each connected node and which port it is connected to in a lookup table held in memory. This lookup table then allows future transmissions to be forwarded to the intended destination only.
Decentralization.
In a mesh topology (i.e., a partially connected mesh topology), there are at least two nodes with two or more paths between them to provide redundant paths to be used in case the link providing one of the paths fails. This decentralization is often used to compensate for the single-point-failure disadvantage that is present when using a single device as a central node (e.g., in star and tree networks). A special kind of mesh, limiting the number of hops between two nodes, is a hypercube. The number of arbitrary forks in mesh networks makes them more difficult to design and implement, but their decentralized nature makes them very useful. In 2012 the IEEE published the Shortest path bridging protocol to ease configuration tasks and allows all paths to be active which increases bandwidth and redundancy between all devices.
This is similar in some ways to a grid network, where a linear or ring topology is used to connect systems in multiple directions. A multidimensional ring has a toroidal topology, for instance.
A fully connected network, complete topology, or full mesh topology is a network topology in which there is a direct link between all pairs of nodes. In a fully connected network with n nodes, there are n(n-1)/2 direct links. Networks designed with this topology are usually very expensive to set up, but provide a high degree of reliability due to the multiple paths for data that are provided by the large number of redundant links between nodes. This topology is mostly seen in military applications.

</doc>
<doc id="41414" url="http://en.wikipedia.org/wiki?curid=41414" title="Neutral direct-current telegraph system">
Neutral direct-current telegraph system

In telecommunication, a neutral direct-current telegraph system ("single-current system", "single-current transmission system", "single-Morse system") is a telegraph system in which (a) current flows during marking intervals and no current flows during spacing intervals for the transmission of signals over a line, and (b) the direction of current flow is immaterial.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41415" url="http://en.wikipedia.org/wiki?curid=41415" title="Noise">
Noise

Noise means any unwanted sound. Sounds, particularly loud ones, that disturb people or make it difficult to hear wanted sounds, are noise. For example, conversations of other people may be called noise by people not involved in any of them; any unwanted sound such as domesticated dogs barking, neighbours playing loud music, portable mechanical saws, road traffic sounds, or a distant aircraft in quiet countryside, is called noise.
Acoustic noise can be anything from quiet but annoying to loud and harmful. At one extreme users of public transport sometimes complain about the faint and tinny sounds emanating from the headphones or earbuds of somebody listening to a portable audio player; at the other the sound of very loud music, a jet engine at close quarters, etc. can cause irreversible hearing damage. At intermediate levels there are a range of deleterious health effects from noise. This "intolerable corruption of human space" can be called noise pollution. A claim made by Luigi Russolo in his article, "The Joys of Noise" is that noise has become so prominent that pure sound no longer exists.
Roland Barthes also observes that noise can be perceived either physiologically or psychologically. We perceive noise physiologically when we "hear" it. On the other hand when we "listen" to a noise we are doing this psychologically. When we perceive a physiological noise we subconsciously feel the vibrations of the noise (sound) waves with our particles in our physical body whereas psychological noise refers to noise that is perceived when our conscious awareness shifts its attention to that noise rather than letting it filter through our subconscious where it goes unnoticed.
Sound intensity follows an inverse square law with distance from the source; doubling the distance from a noise source reduces its intensity by a factor of four, or 6 dB.
Regulation of acoustic noise.
Noise regulation includes statutes or guidelines relating to sound transmission established by national, state or provincial and municipal levels of government. After a watershed passage of the U.S. Noise Control Act of 1972[1], the program was abandoned at the federal level, under President Ronald Reagan, in 1981 and the issue was left to local and state governments. Although the UK and Japan enacted national laws in 1960 and 1967 respectively, these laws were not at all comprehensive or fully enforceable as to address (a) generally rising ambient noise (b) enforceable numerical source limits on aircraft and motor vehicles or (c) comprehensive directives to local government.
Underwater noise is one of 11 Descriptors of Good Environmental Status according to the EU's Marine Strategy Framework Directive.
Recording and reproduction noise.
In audio, recording, and broadcast systems "audio noise" refers to the residual low level sound (usually hiss and hum) that is heard in quiet periods of programme. This is also known as white noise according to the Merriam Webster Definition. There is a similar phenomena to "white noise" which emanates not only from audio recording equipment but from everything and more particularly, musical instruments (whether they are acoustic or electric). These noises are "impurities." When an instrument plays a pitch, even the most beautiful sounding instruments, there is noise (consisting of impurities) projected. Henry Cowell claims that technological advancements have brought machines closer to diminishing these unwanted noises, but have not been completely successful thus far.
In audio engineering it can also refer to the unwanted residual electronic noise signal that gives rise to acoustic noise heard as hiss. This signal noise is commonly measured using A-weighting or ITU-R 468 weighting.

</doc>
<doc id="41416" url="http://en.wikipedia.org/wiki?curid=41416" title="Noise-equivalent power">
Noise-equivalent power

Noise-equivalent power (NEP) is a measure of the sensitivity of a photodetector or detector system. It is defined as the signal power that gives a signal-to-noise ratio of one in a one hertz output bandwidth. An output bandwidth of one hertz is equivalent to half a second of integration time. The units of NEP are watts per square root hertz. The NEP is equal to the noise spectral density (expressed in units of formula_1 or formula_2) divided by the responsivity (expressed in units of formula_3 or formula_4, respectively). 
A smaller NEP corresponds to a more sensitive detector. For example, a detector with an NEP of formula_5 can detect a signal power of one picowatt with a signal-to-noise ratio (SNR) of one after one half second of averaging. The SNR improves as the square root of the averaging time, and hence the SNR in this example can be improved to 10 by averaging for 50 seconds.
If the NEP refers to the signal power absorbed in the detector, it is known as the electrical NEP. If instead it refers to the signal power incident on the detector system, it is called the optical NEP. The optical NEP is equal to the electrical NEP divided by the optical coupling efficiency of the detector system. 

</doc>
<doc id="41417" url="http://en.wikipedia.org/wiki?curid=41417" title="Noise figure">
Noise figure

Noise figure (NF) and noise factor ("F") are measures of degradation of the signal-to-noise ratio (SNR), caused by components in a radio frequency (RF) signal chain. It is a number by which the performance of an amplifier or a radio receiver can be specified, with lower values indicating better performance.
The noise factor is defined as the ratio of the output noise power of a device to the portion thereof attributable to thermal noise in the input termination at standard noise temperature "T"0 (usually 290 K). The noise factor is thus the ratio of actual output noise to that which would remain if the device itself did not introduce noise, or the ratio of input SNR to output SNR.
The noise "figure" is simply the noise "factor" expressed in decibels (dB).
General.
The noise figure is the difference in decibels (dB) between the noise output of the actual receiver to the noise output of an “ideal” receiver with the same overall gain and bandwidth when the receivers are connected to matched sources at the standard noise temperature "T"0 (usually 290 K). The noise power from a simple load is equal to "k T B", where "k" is Boltzmann's constant, "T" is the absolute temperature of the load (for example a resistor), and "B" is the measurement bandwidth. 
This makes the noise figure a useful figure of merit for terrestrial systems where the antenna effective temperature is usually near the standard 290 K. In this case, one receiver with a noise figure say 2 dB better than another, will have an output signal to noise ratio that is about 2 dB better than the other. However, in the case of satellite communications systems, where the receiver antenna is pointed out into cold space, the antenna effective temperature is often colder than 290 K. In these cases a 2 dB improvement in receiver noise figure will result in more than a 2 dB improvement in the output signal to noise ratio. For this reason, the related figure of "effective noise temperature" is therefore often used instead of the noise figure for characterizing satellite-communication receivers and low noise amplifiers.
In heterodyne systems, output noise power includes spurious contributions from image-frequency transformation, but the portion attributable to thermal noise in the input termination at standard noise temperature includes only that which appears in the output via the principal frequency transformation of the system and excludes that which appears via the image frequency transformation.
Definition.
The noise factor "F" of a system is defined as:
where SNRin and SNRout are the input and output signal-to-noise ratios, respectively. The SNR quantities are power ratios. 
The noise figure NF is defined as the noise factor in dB:
where SNRin, dB and SNRout, dB are in decibels (dB).
These formulae are only valid when the input termination is at standard noise temperature "T"0, although in practice small differences in temperature do not significantly affect the values.
The noise factor of a device is related to its noise temperature "T"e:
Attenuators have a noise factor "F" equal to their attenuation ratio "L" when their physical temperature equals "T"0. More generally, for an attenuator at a physical temperature "T", the noise temperature is formula_4, giving a noise factor of:
If several devices are cascaded, the total noise factor can be found with Friis' Formula:
where "F""n" is the noise factor for the "n"-th device and "G"n is the power gain (linear, not in dB) of the "n"-th device. The first amplifier in a chain has the most significant effect on the total noise figure than any
other amplifier in the chain. The lower noise figure amplifier should usually go first in a line of
amplifiers (assuming all else is equal).
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41419" url="http://en.wikipedia.org/wiki?curid=41419" title="Noise power">
Noise power

In telecommunication, the term noise power has the following meanings: 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41420" url="http://en.wikipedia.org/wiki?curid=41420" title="Noise temperature">
Noise temperature

In electronics, noise temperature is one way of expressing the level of available noise power introduced by a component or source. The power spectral density of the noise is expressed in terms of the temperature (in kelvins) that would produce that level of Johnson–Nyquist noise, thus:
where:
Thus the noise temperature is proportional to the power spectral density of the noise, formula_6. That is the power that would be absorbed from the component or source by a matched load. Noise temperature is generally a function of frequency, unlike that of an ideal resistor which is simply equal to the actual temperature of the resistor at all frequencies.
Noise voltage and current.
A noisy component may be modelled as a noiseless component in series with a noisy voltage source producing a voltage of "vn", or as a noiseless component in parallel with a noisy current source producing a current of "in". This equivalent voltage or current corresponds to the above power spectral density formula_7, and would have a mean squared amplitude over a bandwidth "B" of:
where "R" is the resistive part of the component's impedance or "G" is the conductance (real part) of the component's admittance. Speaking of noise temperature therefore offers a fair comparison between components having different impedances rather than specifying the noise voltage and qualifying that number by mentioning the component's resistance. It is also more accessible than speaking of the noise's power spectral density (in watts per hertz) since it is expressed as an ordinary temperature which can be compared to the noise level of an ideal resistor at room temperature (290 K).
Note that one can only speak of the noise temperature of a component or source whose impedance has a substantial (and measurable) resistive component. Thus it doesn't make sense to talk about the noise temperature of a capacitor or of a voltage source. The noise temperature of an amplifier refers to the noise that would be added at the amplifier's "input" (relative to the input impedance of the amplifier) in order to account for the added noise observed following amplification.
Application to communication systems.
A communications system is typically made up of a transmitter, a communications channel, and a receiver. The communications channel may consist of a combination of different physical media, resulting in an electrical signal presented to the receiver. Whatever physical media a channel consists of, the transmitted signal will be attenuated and corrupted with additive noise.
The additive noise in a receiving system can be of thermal origin (thermal noise) or can be from other noise-generating processes. Most noise processes will have a white spectrum, at least over the bandwidth of interest, identical to that of thermal noise. Since they are indistinguishable, the contributions of all noise sources can be lumped together and regarded as a level of thermal noise. The noise power spectral density generated by all these sources (formula_10) can be described by assigning to the noise a temperature formula_11 as defined above:
In a wireless communications receiver, the equivalent input noise temperature formula_13 would equal the sum of two noise temperatures:
The antenna noise temperature formula_15 gives the noise power seen at the output of the antenna. The noise temperature of the receiver circuitry formula_16 represents noise generated by noisy components inside the receiver.
Note that formula_17 refers not to the noise at the output of the receiver after amplification, but the equivalent "input" noise power. In other words, the output of the receiver reflects that of a noiseless amplifier whose input had a noise level not of formula_18 but of formula_13. Thus the figure of merit of a communications system is not the noise level at the speaker of a radio, for instance, since that depends on the setting of the receiver's gain. Rather we ask how much noise the receiver "added" to the original noise level before its gain was applied. That additional noise level is formula_20. If a signal is present, then the decrease in signal to noise ratio incurred using the receiver system with a noise temperature of formula_16 is proportional to formula_22.
Noise figure.
One use of noise temperature is in the definition of a system's noise factor or noise figure. The noise factor specifies the increase in noise power (referred to the input of an amplifier) due to a component or system when its input noise temperature is formula_23.
formula_23 is customarily taken to be room temperature, 290 K. 
The noise factor (a linear term) is more often expressed as the "noise figure" (in decibels) using the conversion:
The noise figure can also be seen as the decrease in signal to noise ratio (SNR) caused by passing a signal through a system if the original signal had a noise temperature of 290 K. This is a common way of expressing the noise contributed by a radio frequency amplifier regardless of the amplifier's gain. For instance, assume an amplifier has a noise temperature 870 K and thus a noise figure of 6 dB. If that amplifier is used to amplify a source having a noise temperature of about room temperature (290 K), as many sources do, then the insertion of that amplifier would reduce the SNR of a signal by 6 dB. This simple relationship is frequently applicable where the source's noise is of thermal origin since a passive transducer will often have a noise temperature similar to 290 K.
However in many cases the input source's noise temperature is much higher, such as an antenna at lower frequencies where atmospheric noise dominates. Then there will be little degradation of the SNR. On the other hand a good satellite dish looking through the atmosphere into space (so that it sees a much lower noise temperature) would have the SNR of a signal degraded by "more" than 6 dB. In those cases a reference to the amplifier's noise temperature itself, rather than the noise figure defined according to room temperature, is more appropriate.
Noise temperature of an amplifier chain.
The noise temperature of an amplifier is commonly measured using the Y-factor method. If there are multiple amplifiers in cascade, the noise temperature of the cascade can be calculated using the Friis equation:
where
Therefore the amplifier chain can be modelled as a black box having a gain of formula_34 and a noise figure given by formula_35. In the usual case where the gains of the amplifier's stages are much greater than one, then it can be seen that the noise temperatures of the earlier stages have a much greater influence on the resulting noise temperature than those later in the chain. One can appreciate that the noise introduced by the first stage, for instance, is amplified by all of the stages whereas the noise introduced by later stages undergoes lesser amplification. Another way of looking at it is that the signal applied to a later stage already has a high noise level, due to amplification of noise by the previous stages, so that the noise contribution of that stage to that already amplified signal is of less significance.
This explains why the quality of a preamplifier or RF amplifier is of particular importance in an amplifier chain. In most cases only the noise figure of the first stage need be considered. However one must check that the noise figure of the second stage is not so high (or that the gain of the first stage is so low) that there is SNR degradation due to the second stage anyway. That will be a concern if the noise figure of the first stage plus that stage's gain (in decibels) is not much greater than the noise figure of the second stage.
One corollary of the Friis equation is that an attenuator prior to the first amplifier will degrade the noise figure due to the amplifier. For instance, if stage 1 represents a 6 dB attenuator so that formula_36, then formula_37. Effectively the noise temperature of the amplifier formula_38 has been quadrupled, in addition to the (smaller) contribution due to the attenuator itself formula_39 (usually room temperature if the attenuator is composed of resistors). An antenna with poor efficiency is an example of this principle, where formula_40 would represent the antenna's efficiency.

</doc>
<doc id="41421" url="http://en.wikipedia.org/wiki?curid=41421" title="Noise weighting">
Noise weighting

A noise weighting is a specific amplitude-vs.-frequency characteristic that is designed to allow subjectively valid measurement of noise. It emphasises the parts of the spectrum that are most important.
Usually, noise means audible noise, in audio systems, broadcast systems or telephone circuits. In this case the weighting is sometimes referred to as Psophometric weighting, though this term is best avoided because, although strictly a general term, the word Psophometric is sometimes assumed to refer to a particular weighting used in telecommunications.
A major use of noise weighting is in the measurement of residual noise in audio equipment, usually present as hiss or hum in quiet moments of programme material. The purpose of weighting here is to emphasise the parts of the audible spectrum that our ears perceive most readily, and attenuate the parts that contribute less to our perception of loudness, in order to get a measured figure that correlates well with subjective effect. 
The ITU-R 468 noise weighting was devised specifically for this purpose, and is widely used in broadcasting, especially in the UK and Europe. A-weighting is also used, especially in the USA, though this is only really valid for the measurement of tones, not noise, and is widely incorporated into sound level meters. 
In telecommunication, noise weightings are used by agencies concerned with public telephone service, and various standard curves are based on the characteristics of specific commercial telephone instruments, representing successive stages of technological development. The coding of commercial apparatus appears in the nomenclature of certain weightings. The same weighting nomenclature and units are used in military versions of commercial noise measuring sets.
Telecommunication measurements are made in lines terminated either by the measuring set or the an instrument of the relevant class.

</doc>
<doc id="41425" url="http://en.wikipedia.org/wiki?curid=41425" title="Non-return-to-zero">
Non-return-to-zero

In telecommunication, a non-return-to-zero (NRZ) line code is a binary code in which 1s are represented by one significant condition (usually a positive voltage) and 0s are represented by some other significant condition (usually a negative voltage), with no other neutral or rest condition. The pulses have more energy than a return-to-zero (RZ) code. Unlike RZ, NRZ does not have a rest state.
NRZ is not inherently a self-clocking signal, thus some additional synchronization technique (for example a run length limited constraint, or a parallel synchronization signal) must be used for avoiding bit slip.
For a given data signaling rate, i.e., bit rate, the NRZ code requires only half the baseband bandwidth required by the Manchester code (the passband bandwidth is the same). When used to represent data in an asynchronous communication scheme, the absence of a neutral state requires other mechanisms for bit synchronization when a separate clock signal is not available.
NRZ-Level itself is not a synchronous system but rather an encoding that can be used in either a synchronous or asynchronous transmission environment, that is, with or without an explicit clock signal involved. Because of this, it is not strictly necessary to discuss how the NRZ-Level encoding acts "on a clock edge" or "during a clock cycle" since all transitions happen in the given amount of time representing the actual or implied integral clock cycle. The real question is that of sampling—the high or low state will be received correctly provided the transmission line has stabilized for that bit when the physical line level is sampled at the receiving end.
However, it is helpful to see NRZ transitions as happening on the trailing (falling) clock edge in order to compare NRZ-Level to other encoding methods, such as the mentioned Manchester code, which requires clock edge information (is the XOR of the clock and NRZ, actually) see the difference between NRZ-Mark and NRZ-Inverted.
Unipolar non-return-to-zero level.
"One" is represented by one physical level (such as a DC bias on the transmission line), while "zero" is represented by another level (usually a negative voltage).
In clock language, "one" transitions or remains high on the trailing clock edge of the previous bit and "zero" transitions or remains low on the trailing clock edge of the previous bit, or just the opposite. This allows for long series without change, which makes synchronization difficult. One solution is to not send bytes without transitions. Disadvantages of an on-off keying are the waste of power due to the transmitted DC level and the power spectrum of the transmitted signal does not approach zero at zero frequency. See RLL
Bipolar non-return-to-zero level.
"One" is represented by one physical level (usually a positive voltage), while "zero" is represented by another level (usually a negative voltage). In clock language, in bipolar NRZ-Level the voltage "swings" from positive to negative on the trailing edge of the previous bit clock cycle.
An example of this is RS-232, where "one" is −12 V to −5 V and "zero" is +5 V to +12 V.
Non-return-to-zero space.
"One" is represented by no change in physical level, while "zero" is represented by a change in physical level. In clock language, the level transitions on the trailing clock edge of the previous bit to represent a "zero."
This "change-on-zero" is used by High-Level Data Link Control and USB. They both avoid long periods of no transitions (even when the data contains long sequences of 1 bits) by using zero-bit insertion. HDLC transmitters insert a 0 bit after five contiguous 1 bits (except when transmitting the frame delimiter '01111110'). USB transmitters insert a 0 bit after six consecutive 1 bits. The receiver at the far end uses every transition — both from 0 bits in the data and these extra non-data 0 bits — to maintain clock synchronization. The receiver otherwise ignores these non-data 0 bits.
Non-return-to-zero inverted.
Non return to zero, inverted (NRZI) is a method of mapping a binary signal to a physical signal for transmission over some transmission media. The two level NRZI signal has a transition at a clock boundary if the bit being transmitted is a logical 1, and does not have a transition if the bit being transmitted is a logical 0.
"One" is represented by a transition of the physical level, while "zero" has no transition. Also, NRZI might take the opposite convention, as in Universal Serial Bus (USB) signalling, when in Mode 1, in which a transition occurs when signaling zero, and a steady level when signaling a one. The transition occurs on the leading edge of the clock for the given bit. This distinguishes NRZI from NRZ-Mark.
However, even NRZI can have long series of zeros (or ones if transitioning on "zero"), and thus clock recovery can be difficult unless some form of run length limited (RLL) coding is used in addition to NRZI. Magnetic disk and tape storage devices generally use fixed-rate RLL codes, while USB uses bit stuffing, which inserts an additional 0 bit after 6 consecutive 1 bits, thus forcing a transition. While bit stuffing is efficient, it results in a variable data rate because it takes slightly longer to send a long string of 1 bits than it does to send a long string of 0 bits.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41426" url="http://en.wikipedia.org/wiki?curid=41426" title="Normalized frequency">
Normalized frequency

Normalized frequency can refer to:

</doc>
<doc id="41427" url="http://en.wikipedia.org/wiki?curid=41427" title="NS/EP telecommunications">
NS/EP telecommunications

NS/EP telecommunications is an abbreviation for "National Security or Emergency Preparedness telecommunications" of the United States. Telecommunications services that are used to maintain a state of readiness or to respond to and manage any event or crisis (local, national, or international) that causes or could cause injury or harm to the population, damage to or loss of property, or degrade or threaten the national security or emergency preparedness posture of the United States.
NS/EP telecommunications are managed and controlled by the National Communications System using Telecommunications Service Priority through both the Government Emergency Telecommunications Service and Wireless Priority Service.

</doc>
<doc id="41428" url="http://en.wikipedia.org/wiki?curid=41428" title="N-entity">
N-entity

In telecommunication, a "n"-entity is an active element in the "n"-th layer of the Open Systems Interconnection--Reference Model (OSI-RM) that (a) interacts directly with elements, "i.e.", entities, of the layer immediately above or below the "n"-th layer, (b) is defined by a unique set of rules, "i.e.", syntax, and information formats, including data and control formats, and (c) performs a defined set of functions. 
The "n" refers to any one of the 7 layers of the OSI-RM. 
In an existing layered open system, the "n" may refer to any given layer in the system. 
Layers are conventionally numbered from the lowest, "i.e.", the physical layer, to the highest, so that the ("n" + 1)-th layer is above the "n"-th layer and the ("n" − 1)-th layer is below.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41432" url="http://en.wikipedia.org/wiki?curid=41432" title="Numerical aperture">
Numerical aperture

In optics, the numerical aperture (NA) of an optical system is a dimensionless number that characterizes the range of angles over which the system can accept or emit light. By incorporating index of refraction in its definition, NA has the property that it is constant for a beam as it goes from one material to another provided there is no optical power at the interface. The exact definition of the term varies slightly between different areas of optics. Numerical aperture is commonly used in microscopy to describe the acceptance cone of an objective (and hence its light-gathering ability and resolution), and in fiber optics, in which it describes the range of angles within which light that is incident on the fiber will be transmitted along it. 
General optics.
In most areas of optics, and especially in microscopy, the numerical aperture of an optical system such as an objective lens is defined by
where "n" is the index of refraction of the medium in which the lens is working (1.00 for air, 1.33 for pure water, and typically 1.52 for immersion oil; see also list of refractive indices), and "θ" is the half-angle of the maximum cone of light that can enter or exit the lens. In general, this is the angle of the real marginal ray in the system. Because the index of refraction is included, the NA of a pencil of rays is an invariant as a pencil of rays passes from one material to another through a flat surface. This is easily shown by rearranging Snell's law to find that formula_2 is constant across an interface.
In air, the angular aperture of the lens is approximately twice this value (within the paraxial approximation). The NA is generally measured with respect to a particular object or image point and will vary as that point is moved. In microscopy, NA generally refers to object-space NA unless otherwise noted.
In microscopy, NA is important because it indicates the resolving power of a lens. The size of the finest detail that can be resolved is proportional to λ/2NA, where λ is the wavelength of the light. A lens with a larger numerical aperture will be able to visualize finer details than a lens with a smaller numerical aperture. Assuming quality (diffraction limited) optics, lenses with larger numerical apertures collect more light and will generally provide a brighter image, but will provide shallower depth of field.
Numerical aperture is used to define the "pit size" in optical disc formats.
Increasing the magnification and the numerical aperture of the objective reduces the working distance, i.e. the distance between front lens and specimen.
Numerical aperture versus f-number.
Numerical aperture is not typically used in photography. Instead, the angular aperture of a lens (or an imaging mirror) is expressed by the f-number, written f/# or formula_3, which is defined as the ratio of the focal length formula_4 to the diameter of the entrance pupil formula_5:
This ratio is related to the image-space numerical aperture when the lens is focused at infinity. Based on the diagram at the right, the image-space numerical aperture of the lens is:
The approximation holds when the numerical aperture is small, but it turns out that for well-corrected optical systems such as camera lenses, a more detailed analysis shows that formula_3 is almost exactly equal to formula_11 even at large numerical apertures. As Rudolf Kingslake explains, "It is a common error to suppose that the ratio [formula_12 ] is actually equal to formula_13, and not formula_14 ... The tangent would, of course, be correct if the principal planes were really plane. However, the complete theory of the Abbe sine condition shows that if a lens is corrected for coma and spherical aberration, as all good photographic objectives must be, the second principal plane becomes a portion of a sphere of radius "f" centered about the focal point, ..." In this sense, the traditional thin-lens definition and illustration of f-number is misleading, and defining it in terms of numerical aperture may be more meaningful.
Working (effective) f-number.
The f-number describes the light-gathering ability of the lens in the case where the marginal rays on the object side are parallel to the axis of the lens. This case is commonly encountered in photography, where objects being photographed are often far from the camera. When the object is not distant from the lens, however, the image is no longer formed in the lens's focal plane, and the f-number no longer accurately describes the light-gathering ability of the lens or the image-side numerical aperture. In this case, the numerical aperture is related to what is sometimes called the "working f-number" or "effective f-number." A practical example of this is, that when focusing closer, with e.g. a macro lens, the lens' effective aperture becomes smaller, from e.g. f/22 to f/45, thus affecting the exposure.
The working f-number is defined by modifying the relation above, taking into account the magnification from object to image:
where formula_16 is the working f-number, formula_17 is the lens's magnification for an object a particular distance away, and the NA is defined in terms of the angle of the marginal ray as before. The magnification here is typically negative; in photography, the factor is sometimes written as 1 + "m", where "m" represents the absolute value of the magnification; in either case, the correction factor is 1 or greater.
The two equalities in the equation above are each taken by various authors as the definition of working f-number, as the cited sources illustrate. They are not necessarily both exact, but are often treated as if they are. The actual situation is more complicated — as Allen R. Greenleaf explains, "Illuminance varies inversely as the square of the distance between the exit pupil of the lens and the position of the plate or film. Because the position of the exit pupil usually is unknown to the user of a lens, the rear conjugate focal distance is used instead; the resultant theoretical error so introduced is insignificant with most types of photographic lenses."
Conversely, the object-side numerical aperture is related to the f-number by way of the magnification (tending to zero for a distant object):
Laser physics.
In laser physics, the numerical aperture is defined slightly differently. Laser beams spread out as they propagate, but slowly. Far away from the narrowest part of the beam, the spread is roughly linear with distance—the laser beam forms a cone of light in the "far field". The relation used to define the NA of the laser beam is the same as that used for an optical system,
but "θ" is defined differently. Laser beams typically do not have sharp edges like the cone of light that passes through the aperture of a lens does. Instead, the irradiance falls off gradually away from the center of the beam. It is very common for the beam to have a Gaussian profile. Laser physicists typically choose to make "θ" the "divergence" of the beam: the far-field angle between the propagation direction and the distance from the beam axis for which the irradiance drops to 1/e2 times the wavefront total irradiance. The NA of a Gaussian laser beam is then related to its minimum spot size by
where λ0 is the vacuum wavelength of the light, and 2"w"0 is the diameter of the beam at its narrowest spot, measured between the 1/e2 irradiance points ("Full width at e−2 maximum of the intensity"). This means that a laser beam that is focused to a small spot will spread out quickly as it moves away from the focus, while a large-diameter laser beam can stay roughly the same size over a very long distance. See also: Gaussian beam width.
Fiber optics.
A multi-mode optical fiber will only propagate light that enters the fiber within a certain cone, known as the acceptance cone of the fiber. The half-angle of this cone is called the acceptance angle, "θ"max. For step-index multimode fiber in a given medium, the acceptance angle is determined only by the indices of refraction of the core, the cladding, and the medium:
where "n" is the refractive index of the medium, "n"core is the refractive index of the fiber core, and "n"clad is the refractive index of the cladding. While the core will accept light at higher angles, those rays will not totally reflect off the core–cladding interface, and so will not be transmitted to the other end of the fiber.
When a light ray is incident from a medium of refractive index n to the core of index "n"core at the maximum acceptance angle, Snell's law at the medium–core interface gives
From the geometry of the above figure we have:
where formula_24is the critical angle for total internal reflection.
Substituting cos θ"c" for sin θ"r" in Snell's law we get:
By squaring both sides 
Solving, we find the formula stated above:
This has the same form as the numerical aperture in other optical systems, so it has become common to "define" the NA of any type of fiber to be
where "n"core is the refractive index along the central axis of the fiber. Note that when this definition is used, the connection between the NA and the acceptance angle of the fiber becomes only an approximation. In particular, manufacturers often quote "NA" for single-mode fiber based on this formula, even though the acceptance angle for single-mode fiber is quite different and cannot be determined from the indices of refraction alone.
The number of bound modes, the mode volume, is related to the normalized frequency and thus to the NA.
In multimode fibers, the term "equilibrium numerical aperture" is sometimes used. This refers to the numerical aperture with respect to the extreme exit angle of a ray emerging from a fiber in which equilibrium mode distribution has been established.

</doc>
<doc id="41435" url="http://en.wikipedia.org/wiki?curid=41435" title="Nyquist rate">
Nyquist rate

In signal processing, the Nyquist rate, named after Harry Nyquist, is twice the bandwidth of a bandlimited function or a bandlimited channel. This term means two different things under two different circumstances: 
Nyquist rate relative to sampling.
When a continuous function, x(t), is sampled at a constant rate, fs (samples/second), there is always an unlimited number of other continuous functions that fit the same set of samples. But only one of them is bandlimited to ½ fs (hertz), which means that its Fourier transform, X(f), is 0 for all |f| ≥ ½ fs (see Sampling theorem). The mathematical algorithms that are typically used to recreate a continuous function from samples create arbitrarily good approximations to this theoretical, but infinitely long, function. It follows that if the original function, x(t), is bandlimited to ½ fs, which is called the "Nyquist criterion", then it is the one unique function the interpolation algorithms are approximating. In terms of a function's own bandwidth (B), as depicted above, the Nyquist criterion is often stated as fs > 2B.  And 2B is called the Nyquist rate for functions with bandwidth B. When the Nyquist criterion is not met (B > ½ fs), a condition called aliasing occurs, which results in some inevitable differences between x(t) and a reconstructed function that has less bandwidth. In most cases, the differences are viewed as distortion.
Intentional aliasing.
Figure 1 depicts a type of function called baseband or lowpass, because its positive-frequency range of significant energy is [0, "B"). When instead, the frequency range is ("A", "A"+"B"), for some "A" > "B", it is called bandpass, and a common desire (for various reasons) is to convert it to baseband. One way to do that is frequency-mixing (heterodyne) the bandpass function down to the frequency range (0, "B"). One of the possible reasons is to reduce the Nyquist rate for more efficient storage. And it turns out that one can directly achieve the same result by sampling the bandpass function at a sub-Nyquist sample-rate that is the smallest integer-sub-multiple of frequency "A" that meets the baseband Nyquist criterion:  fs > 2"B". For a more general discussion, see bandpass sampling.
Nyquist rate relative to signaling.
Long before Harry Nyquist had his name associated with sampling, the term "Nyquist rate" was used differently, with a meaning closer to what Nyquist actually studied. Quoting Harold S. Black's 1953 book "Modulation Theory," in the section Nyquist Interval of the opening chapter "Historical Background:"
According to the OED, Black's statement regarding 2"B" may be the origin of the term "Nyquist rate".
Nyquist's famous 1928 paper was a study on how many pulses (code elements) could be transmitted per second, and recovered, through a channel of limited bandwidth. "Signaling at the Nyquist rate" meant putting as many code pulses through a telegraph channel as its bandwidth would allow. Shannon used Nyquist's approach when he proved the sampling theorem in 1948, but Nyquist did not work on sampling per se.
Black's later chapter on "The Sampling Principle" does give Nyquist some of the credit for some relevant math:

</doc>
<doc id="41437" url="http://en.wikipedia.org/wiki?curid=41437" title="Off-axis optical system">
Off-axis optical system

An off-axis optical system is an optical system in which the optical axis of the aperture is not coincident with the mechanical center of the aperture. Note: The principal applications of off-axis optical systems are to avoid obstruction of the primary aperture by secondary optical elements, instrument packages, or sensors, and to provide ready access to instrument packages or sensors at the focus. The engineering tradeoff of an off-axis optical system is an increase in image aberrations.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41438" url="http://en.wikipedia.org/wiki?curid=41438" title="Off-hook">
Off-hook

In telephony, the term off-hook has the following meanings:
On an ordinary two-wire telephone line, off-hook status is communicated to the telephone exchange by a resistance short across the pair. When an off-hook condition persists without dialing, for example because the handset has fallen off or the cable has been flooded, it is treated as a "permanent loop" or permanent signal.
The act of "going off-hook" is also referred to as "seizing" the line or channel.

</doc>
<doc id="41440" url="http://en.wikipedia.org/wiki?curid=41440" title="Online and offline">
Online and offline

The terms "online" and "offline" have specific meanings in regard to computer technology and telecommunications in which "online" indicates a state of connectivity, while "offline" indicates a disconnected state. Common vernacular extended from their computing and telecommunication meanings and refers specifically to an internet connection. Lastly, in the area of human interaction and conversation, discussions taking place during a business meeting are "online", while issues that do not concern all participants of the meeting should be "taken offline" — continued outside of the meeting.
Definitions.
In computer technology and telecommunication, online and offline are defined by Federal Standard 1037C. They are states or conditions of a "device or equipment" or of a "functional unit". To be considered online, one of the following may apply to a system: it is under the direct control of another device; it is under the direct control of the system with which it is associated; or it is available for immediate use on demand by the system without human intervention.
In contrast, a device that is offline meets none of these criteria (e.g., its main power source is disconnected or turned off, or it is off-power).
The Oxford dictionary defines "online" as "controlled by or connected to a computer" and as an activity or service which is "available on or performed using the Internet or other computer network". The term is utilized within terms such as these: "online identity", "online predator", "online gambling", "online shopping", "online banking", and "online learning". The online context is given to other words by the prefixes "cyber" and "e", as in the words "cyberspace", "cybercrime", "email", and "ecommerce".
Antecedents.
During the 19th century, the term "on line" was commonly used in both the railroad and telegraph industries. For railroads, a signal box would send a message down the line (track), via a telegraph line (cable), indicating the track's status: "Train on line" or "Line clear". Telegraph linemen would refer to sending current through a line as "direct on line" or "battery on line"; or they may refer to a problem with the circuit as being "on line", as opposed to the power source or end-point equipment.
Examples.
Offline email.
One example of a common use of these concepts with email is a mail user agent (MUA) that can be instructed to be in either online or offline states. One such MUA is Microsoft Outlook. When online it will attempt to connect to mail servers (to check for new mail at regular intervals, for example), and when offline it will not attempt to make any such connection. The online or offline state of the MUA does not necessarily reflect the connection status between the computer on which it is running and the Internet. That is, the computer itself may be online—connected to Internet via a cable modem or other means—while Outlook is kept offline by the user, so that it makes no attempt to send or to receive messages. Similarly, a computer may be configured to employ a dial-up connection on demand (as when an application such as Outlook attempts to make connection to a server), but the user may not wish for Outlook to trigger that call whenever it is configured to check for mail.
Offline media playing.
Another example of the use of these concepts is digital audio technology. A tape recorder, digital audio editor, or other device that is online is one whose clock is under the control of the clock of a synchronization master device. When the sync master commences playback, the online device automatically synchronizes itself to the master and commences playing from the same point in the recording. A device that is offline uses no external clock reference and relies upon its own internal clock. When a large number of devices are connected to a sync master it is often convenient, if one wants to hear just the output of one single device, to take it offline because, if the device is played back online, all synchronized devices have to locate the playback point and wait for each other device to be in synchronization. (For related discussion, see MIDI timecode, word sync, and recording system synchronization.)
Offline browsing.
A third example of a common use of these concepts is a web browser that can be instructed to be in either online or offline states. The browser attempts to fetch pages from servers while only in the online state. In the offline state, users can perform offline browsing, where pages can be browsed using local copies of those pages that have previously been downloaded while in the online state. This can be useful when the computer is offline and connection to the Internet is impossible or undesirable. The pages are downloaded either implicitly into the web browser's own cache as a result of prior online browsing by the user or explicitly by a browser configured to keep local copies of certain web pages, which are updated when the browser is in the online state, either by checking that the local copies are up-to-date at regular intervals or by checking that the local copies are up-to-date whenever the browser is switched to the online state. One such web browser capable of being explicitly configured to download pages for offline browsing is Internet Explorer. When pages are added to the Favourites list, they can be marked to be "available for offline browsing". Internet Explorer will download to local copies both the marked page and, optionally, all of the pages that it links to. In Internet Explorer version 6, the level of direct and indirect links, the maximum amount of local disc space allowed to be consumed, and the schedule on which local copies are checked to see whether they are up-to-date, are configurable for each individual Favourites entry.
For communities that lack adequate Internet connectivity—such as developing countries, rural areas, and prisons—offline information stores such as the eGranary Digital Library (a collection of approximately thirty million educational resources from more than two thousand web sites and hundreds of CD-ROMs) provide offline access to information. Numerous organizations have developed, or are developing, flash memory chips with collections of educational materials for offline use in smartphones, tablets, and laptops.
Offline storage.
Likewise, offline storage is computer data storage that is not "available for immediate use on demand by the system without human intervention." Additionally, an otherwise online system that is powered down may be considered offline.
Offline messages.
With the growing communication tools and media, the words offline and online are used very frequently. If a person is active over a messaging tool and is able to accept the messages it is termed as online message and if the person is not available and the message is left to view when the person is back, it is termed as offline message. In the same context, the person's availability is termed as online and non availability is termed as offline
Generalizations.
Online and offline distinctions have been generalized from computing and telecommunication into the field of human interpersonal relationships. The distinction between what is considered online and what is considered offline has become a subject of study in the field of sociology.
The distinction between online and offline is conventionally seen as the distinction between computer-mediated communication and face-to-face communication (e.g., face time), respectively. Online is virtuality or cyberspace, and offline is reality (i.e., Real life or meatspace). Slater states that this distinction is "obviously far too simple". To support his argument that the distinctions in relationships are more complex than a simple dichotomy of online versus offline, he observes that some people draw no distinction between an online relationship, such as indulging in cybersex, and an offline relationship, such as being pen pals. He argues that even the telephone can be regarded as an online experience in some circumstances, and that the blurring of the distinctions between the uses of various technologies (such as PDA versus mobile phone, Internet television versus Internet, and telephone versus Voice over Internet Protocol) has made it "impossible to use the term "online" meaningfully in the sense that was employed by the first generation of Internet research".
Slater asserts that there are legal and regulatory pressures to reduce the distinction between online and offline, with a "general tendency to assimilate online to offline and erase the distinction," stressing, however, that this does not mean that online relationships are being reduced to "pre-existing" offline relationships. He conjectures that greater legal status may be assigned to online relationships (pointing out that contractual relationships, such as business transactions, online are already seen as just as "real" as their offline counterparts), although he states it to be hard to imagine courts awarding palimony to people who have had a purely online sexual relationship. He also conjectures that an online/offline distinction may be seen by people as "rather quaint and not quite comprehensible" within 10 years.
This distinction between "online" and "offline" is sometimes inverted, with online concepts being used to define and to explain offline activities, rather than (as per the conventions of the desktop metaphor with its desktops, trash cans, folders, and so forth) the other way around. Several cartoons appearing in "The New Yorker" have satirized this. One includes Saint Peter asking for a username and a password before admitting a man into Heaven. Another illustrates "the off-line store" where "All items are actual size!", shoppers may "Take it home as soon as you pay for it!", and "Merchandise may be handled prior to purchase!"
References.
</dl>

</doc>
<doc id="41441" url="http://en.wikipedia.org/wiki?curid=41441" title="Off-the-air">
Off-the-air

In telecommunication, the term off-air or off-the-air has the following meanings:
"Note": The carrier wave may continue unmodulated or it may be modulated by another signal source, such as a subcarrier.
Also, off-the-air may be synonymous with over-the-air or from-the-air, as in picking up a terrestrial broadcast TV station off-the-air instead of from cable TV. As this usage may be confusing due to the term's other meanings, the terms "over the air" or "on the air" are more standard in the broadcasting industry.

</doc>
<doc id="41442" url="http://en.wikipedia.org/wiki?curid=41442" title="One-way trunk">
One-way trunk

In telecommunication, a one-way trunk is a trunk between two switching centers, over which traffic may be originated from one preassigned location only. 
"Note 1:" The traffic may consist of two-way communications; the expression "one way" refers only to the origin of the demand for a connection. 
"Note 2:" At the originating end, the one-way trunk is known as an "outgoing trunk" ; at the other end, it is known as an ""incoming trunk"." 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).
See also: Telephone signaling interface

</doc>
<doc id="41443" url="http://en.wikipedia.org/wiki?curid=41443" title="On-hook">
On-hook

In telephony, the term on-hook has the following meanings: 
The act of "going on-hook" is also referred to as "releasing the line" or "channel", and may initiate the process of clearing.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41445" url="http://en.wikipedia.org/wiki?curid=41445" title="On-premises wiring">
On-premises wiring

On-premises wiring (customer premises wiring) is customer-owned telecommunication transmission or distribution lines. The transmission lines may be metallic (copper) or optical fiber, and may be installed within or between buildings.
Premises wiring may consist of horizontal wiring, vertical wiring, and backbone cabling. It may extend from the point-of-entry to user work areas. Any type of communications or data wiring is considered premises wiring, including telephone, computer/data, intercom, closed-circuit television.
Premises networks are wired worldwide, across every industry, in both small and large scale applications. Any type or number of topologies may be used -- star, bus, ring, etc.
Ownership.
The ownership of on-premises wiring varies between jurisdictions: It depends on the location of the demarcation point. The location determines ownership and responsibility for maintenance and repair.
In the US and Canada, most premises wiring is owned by the customer. There generally is a demarcation point "as close to the poles" as possible. For many installations, this is a network interface device mounted on the outside of the building. In some cases, it is a minimum-point-of-entry (MPOE) location inside the building.
In the UK, the demarcation point is the wall jack, and hence most of the on-premises wiring is the property of the telephone company.

</doc>
<doc id="41448" url="http://en.wikipedia.org/wiki?curid=41448" title="Open network architecture">
Open network architecture

In telecommunications, and in the context of Federal Communications Commission's (FCC) Computer Inquiry III, Open network architecture (ONA) is the overall design of a communication carrier's basic network facilities and services to permit all users of the basic network to interconnect to specific basic network functions and interfaces on an unbundled, equal-access basis.
The ONA concept consists of three integral components:

</doc>
<doc id="41449" url="http://en.wikipedia.org/wiki?curid=41449" title="Open systems architecture">
Open systems architecture

Open systems architecture, in telecommunication, is a standard that describes the layered hierarchical structure, configuration, or model of a communications or distributed data processing system that:
Open systems architecture may be implemented using the OSI Model as a guide while designing the system to meet performance requirements.

</doc>
<doc id="41453" url="http://en.wikipedia.org/wiki?curid=41453" title="Operation">
Operation

Operation or Operations may refer to:

</doc>
<doc id="41455" url="http://en.wikipedia.org/wiki?curid=41455" title="Optical attenuator">
Optical attenuator

An optical attenuator , or fiber optic attenuator, is a device used to reduce the power level of an optical signal, either in free space or in an optical fiber. The basic types of optical attenuators are fixed, step-wise variable, and continuously variable.
Applications of Optical Attenuators.
Optical attenuators are commonly used in fiber optic communications, either to test power level margins by temporarily adding a calibrated amount of signal loss, or installed permanently to properly match transmitter and receiver levels. Sharp bends stress optic fibers and can cause losses. If a received signal is too strong a temporary fix is to wrap the cable around a pencil until the desired level of attenuation is achieved. However, such arrangements are unreliable, since the stressed fiber tends to break over time.
How Optical Attenuators Work?
The power reduction are done by such means as absorption, reflection, diffusion, scattering, deflection, diffraction, and dispersion, etc. Optical attenuators usually works by absorbing the light, like sunglasses absorb the extra light energy. They typically have a working wavelength range in which they absorb the light energy equally. They should not reflect the light since that could cause unwanted back reflection in the fiber system. Or by scattering the light such as an air gap. Another type of attenuator utilizes a length of high-loss optical fiber, that operates upon its input optical signal power level in such a way that its output signal power level is less than the input level. 
Fixed Optical Attenuators.
Fixed optical attenuators used in fiber optic systems may use a variety of principles for their functioning. Preferred attenuators use either doped fibers, or mis-aligned splices,or total power since both of these are reliable and inexpensive.
"Inline" style attenuators are incorporated into patch cables. The alternative "build out" style attenuator is a small male-female adapter that can be added onto other cables.
Non-preferred attenuators often use gap loss or reflective principles. Such devices can be sensitive to: modal distribution, wavelength, contamination, vibration, temperature, damage due to power bursts, may cause back reflections, may cause signal dispersion etc.
Loopback Attenuators.
Loopback fiber optic attenuator is designed for testing, engineering and the burn-in stage of boards or other equipment. Available in SC/UPC, SC/APC, LC/UPC, LC/APC, MTRJ, MPO for singlemode application.900um fiber cable inside of the black shell for LC and SC type.
No black shell for MTRJ and MPO type.
Built-in Variable Optical Attenuators.
Built-in variable optical attenuators may be either manually or electrically controlled. A manual device is useful for one-time set up of a system, and is a near-equivalent to a fixed attenuator, and may be referred to as an "adjustable attenuator". In contrast, an electrically controlled attenuator can provide adaptive power optimization.
Attributes of merit for electrically controlled devices, include speed of response and avoiding degradation of the transmitted signal. Dynamic range is usually quite restricted, and power feedback may mean that long term stability is a relatively minor issue. Speed of response is a particularly major issue in dynamically reconfigurable systems, where a delay of one millionth of a second can result in the loss of large amounts of transmitted data. Typical technologies employed for high speed response include LCD, or Lithium niobate devices. There is a class of built-in attenuators that is technically indistinguishable from test attenuators, except they are packaged for rack mounting, and have no test display.
Variable Optical Test Attenuators.
Variable optical test attenuators generally use a variable neutral density filter. Despite relatively high cost, this arrangement has the advantages of being stable, wavelength insensitive, mode insensitive, and offering a large dynamic range. Other schemes such as LCD, variable air gap etc. have been tried over the years, but with limited success.
They may be either manually or motor controlled. Motor control give regular users a distinct productivity advantage, since commonly used test sequences can be run automatically. 
Attenuator instrument calibration is a major issue. The user typically would like an absolute port to port calibration. Also, calibration should usually be at a number of wavelengths and power levels, since the device is not always linear. However a number of instruments do not in fact offer these basic features, presumably in an attempt to reduce cost. The most accurate variable attenuator instruments have thousands of calibration points, resulting in excellent overall accuracy in use.
Types of Optical Attenuators.
Optical attenuators can take a number of different forms and are typically classified as fixed or variable attenuators. What's more, they can be classified as LC, SC, ST, FC, MU, E2000 etc. according to the different types of connectors.
Test automation.
Test sequences that use variable attenuators, can be very time consuming. Therefore, automation is likely to achieve useful benefits. Both bench and handheld style devices are available that offer such features.

</doc>
<doc id="41456" url="http://en.wikipedia.org/wiki?curid=41456" title="Optical axis">
Optical axis

An optical axis is a line along which there is some degree of rotational symmetry in an optical system such as a camera lens or microscope. 
The optical axis is an imaginary line that defines the path along which light propagates through the system, up to first approximation. For a system composed of simple lenses and mirrors, the axis passes through the center of curvature of each surface, and coincides with the axis of rotational symmetry. The optical axis is often coincident with the system's mechanical axis, but not always, as in the case of off-axis optical systems.
For an optical fiber, the optical axis is along the center of the fiber core, and is also known as the "fiber axis".

</doc>
<doc id="41458" url="http://en.wikipedia.org/wiki?curid=41458" title="Optical disc">
Optical disc

In computing and optical disc recording technologies, an optical disc (OD) is a flat, usually circular disc which encodes binary data (bits) in the form of pits (binary value of 0 or off, due to lack of reflection when read) and lands (binary value of 1 or on, due to a reflection when read) on a special material (often aluminium ) on one of its flat surfaces. The encoding material sits atop a thicker substrate (usually polycarbonate) which makes up the bulk of the disc and forms a dust defocusing layer. The encoding pattern follows a continuous, spiral path covering the entire disc surface and extending from the innermost track to the outermost track. The data is stored on the disc with a laser or stamping machine, and can be accessed when the data path is illuminated with a laser diode in an optical disc drive which spins the disc at speeds of about 200 to 4,000 RPM or more, depending on the drive type, disc format, and the distance of the read head from the center of the disc (inner tracks are read at a higher disc speed). Most optical discs exhibit a characteristic iridescence as a result of the diffraction grating formed by its grooves. This side of the disc contains the actual data and is typically coated with a transparent material, usually lacquer. The reverse side of an optical disc usually has a printed label, sometimes made of paper but often printed or stamped onto the disc itself. Unlike the 3½-inch floppy disk, most optical discs do not have an integrated protective casing and are therefore susceptible to data transfer problems due to scratches, fingerprints, and other environmental problems.
Optical discs are usually between 7.6 and 30 cm (3 to 12 in) in diameter, with 12 cm (4.75 in) being the most common size. A typical disc is about 1.2 mm (0.05 in) thick, while the track pitch (distance from the center of one track to the center of the next) is typically 1.6 µm.
An optical disc is designed to support one of three recording types: read-only (e.g.: CD and CD-ROM), recordable (write-once, e.g. CD-R), or re-recordable (rewritable, e.g. CD-RW). Write-once optical discs commonly have an organic dye recording layer between the substrate and the reflective layer. Rewritable discs typically contain an alloy recording layer composed of a phase change material, most often AgInSbTe, an alloy of silver, indium, antimony, and tellurium.
Optical discs are most commonly used for storing music (e.g. for use in a CD player), video (e.g. for use in a Blu-ray player), or data and programs for personal computers (PC). The Optical Storage Technology Association (OSTA) promotes standardized optical storage formats. Although optical discs are more durable than earlier audio-visual and data storage formats, they are susceptible to environmental and daily-use damage. Libraries and archives enact optical media preservation procedures to ensure continued usability in the computer's optical disc drive or corresponding disc player.
For computer data backup and physical data transfer, optical discs such as CDs and DVDs are gradually being replaced with faster, smaller solid-state devices, especially the USB flash drive. This trend is expected to continue as USB flash drives continue to increase in capacity and drop in price. Additionally, music purchased or shared over the Internet has significantly reduced the number of audio CDs sold annually.
History.
 The optical disc was invented in 1958. In 1961 and 1969, David Paul Gregg registered a patent for the analog optical disc for video recording. This form of optical disc was a very early form of the DVD U.S. Patent . It is of special interest that U.S. Patent , filed 1989, issued 1990, generated royalty income for Pioneer Corporation's DVA until 2007 —then encompassing the CD, DVD, and Blu-ray systems. In the early 1960s, the Music Corporation of America bought Gregg's patents and his company, Gauss Electrophysics.
American inventor James T. Russell has been credited with inventing the first system to record a digital signal on an optical transparent foil which is lit from behind by a high-power halogen lamp. Russell's patent application was first filed in 1966 and he was granted a patent in 1970. Following litigation, Sony and Philips licensed Russell's patents (then held by a Canadian company, Optical Recording Corp.) in the 1980s.
Both Gregg's and Russell's disc are floppy media read in transparent mode, which impose serious drawbacks. In the Netherlands in 1969, Philips Research physicist, Pieter Kramer invented an optical videodisc in reflective mode with a protective layer read by a focused laser beam U.S. Patent , filed 1972, issued 1991. Kramer's physical format is used in all optical discs. In 1975, Philips and MCA began to work together, and in 1978, commercially much too late, they presented their long-awaited Laserdisc in Atlanta. MCA delivered the discs and Philips the players. However, the presentation was a commercial failure, and the cooperation ended.
In Japan and the U.S., Pioneer succeeded with the videodisc until the advent of the DVD. In 1979, Philips and Sony, in consortium, successfully developed the audio compact disc.
In the mid-1990s, a consortium of manufacturers developed the second generation of the optical disc, the DVD.
Magnetic disks found limited applications in storing the data in large amount. So,there was the need of finding some more data storing techniques. As a result, it was found that by using optical means large data storing devices can be made which in turn gave rise to the optical discs.The very first application of this kind was the Compact Disc(CD) which was used in audio systems.
Sony and Philips developed the first generation of the CDs in the mid 1980s with the complete specifications for these devices. With the help of this kind of technology the possibility of representing the analog signal into digital signal was exploited to great level. For this purpose the 16 bit samples of the analog signal were taken at the rate of 44,100 samples per second. This sample rate was based on the Nyquist rate of 40,000 samples per second required to capture the audible frequency range to 20 kHz without aliasing, with an additional tolerance to allow the use of less-than-perfect analog audio pre-filters to remove any higher frequencies. The first version of the standard allowed up to 75 minutes of music which required 650Mb of storage.
The third generation optical disc was developed in 2000–2006, and was introduced as Blu-ray Disc. First movies on Blu-ray Discs were released in June 2006. Blu-ray eventually prevailed in a high definition optical disc format war over a competing format, the HD DVD. A standard Blu-ray disc can hold about 25 GB of data, a DVD about 4.7 GB, and a CD about 700 MB.
First-generation.
Initially, optical discs were used to store music and computer software. The Laserdisc format stored analog video signals for the distribution of home video, but commercially lost to the VHS videocassette format, due mainly to its high cost and non-re-recordability; other first-generation disc formats were designed only to store digital data and were not initially capable of use as a digital video medium.
Most first-generation disc devices had an infrared laser reading head. The minimum size of the laser spot is proportional to the wavelength of the laser, so wavelength is a limiting factor upon the amount of information that can be stored in a given physical area on the disc. The infrared range is beyond the long-wavelength end of the visible light spectrum, so it supports less density than shorter-wavelength visible light. One example of high-density data storage capacity, achieved with an infrared laser, is 700 MB of net user data for a 12 cm compact disc.
Other factors that affect data storage density include: the existence of multiple layers of data on the disc, the method of rotation (Constant linear velocity (CLV), Constant angular velocity (CAV), or zoned-CAV), the composition of lands and pits, and how much margin is unused is at the center and the edge of the disc.
Second-generation.
Second-generation optical discs were for storing great amounts of data, including broadcast-quality digital video. Such discs usually are read with a visible-light laser (usually red); the shorter wavelength and greater numerical aperture allow a narrower light beam, permitting smaller pits and lands in the disc. In the DVD format, this allows 4.7 GB storage on a standard 12 cm, single-sided, single-layer disc; alternatively, smaller media, such as the DataPlay format, can have capacity comparable to that of the larger, standard compact 12 cm disc.
Third-generation.
Third-generation optical discs are in development, meant for distributing high-definition video and support greater data storage capacities, accomplished with short-wavelength visible-light lasers and greater numerical apertures. Blu-ray Disc and HD DVD uses blue-violet lasers and focusing optics of greater aperture, for use with discs with smaller pits and lands, thereby greater data storage capacity per layer.
In practice, the effective multimedia presentation capacity is improved with enhanced video data compression codecs such as H.264/MPEG-4 AVC and VC-1.
Fourth-generation.
The following formats go beyond the current third-generation discs and have the potential to hold more than one terabyte (1 TB) of data:
Overview of optical types.
1)Prototypes and theoretical Values.
2)Years from (known) start of development till end of sales or development.
Recordable and writable optical discs.
There are numerous formats of optical direct to disk recording devices on the market, all of which are based on using a laser to change the reflectivity of the digital recording medium in order to duplicate the effects of the pits and lands created when a commercial optical disc is pressed.
Formats such as CD-R and DVD-R are "Write once read many", while CD-RW and DVD-RW are rewritable, more like a magnetic recording hard disk drive (HDD).
Media technologies vary, M-DISC uses a different recording technique & media versus DVD-R and BD-R.

</doc>
<doc id="41459" url="http://en.wikipedia.org/wiki?curid=41459" title="Normandie-Niemen">
Normandie-Niemen

The Normandie-Niemen Regiment (Russian: Нормандия-Неман) is a fighter squadron, later regiment (of three squadrons) of the French Air Force. It served on the Eastern Front of the European Theatre of World War II with the 1st Air Army. The regiment is notable for being one of only two air combat units from an Allied western European country to participate on the Eastern Front during World War II, the other being the British No. 151 Wing RAF, and the only one to fight together with the Soviets until the end of the war in Europe.
The unit originated in mid-1943 during World War II. Initially the "groupe" comprised a group of French fighter pilots sent to aid Soviet forces on the Eastern Front at the suggestion of Charles de Gaulle, leader of the Free French Forces, who felt it important that French servicemen serve on all fronts in the war.
The unit was the GC3 ("Groupe de Chasse 3" or 3rd Fighter Group) in the Free French Air Force, first commanded by Jean Tulasne. It fought in three campaigns on behalf of the Soviet Union between 22 March 1943, and 9 May 1945, during which time it destroyed 273 enemy aircraft and received numerous orders, citations and decorations from both France and the Soviet Union, including the French "Légion d’Honneur" and the Soviet Order of the Red Banner. Joseph Stalin awarded the unit the name Niemen for its participation in the Battle of the Niemen River (1944).
s of 2005[ [update]] the unit, known as squadron , flied Dassault Mirage F1 CT planes. The squadron was briefly administratively dissolved in June 2010 for reorganization and recreated in 2011 in view of its transition to Dassault Rafale planes. It was officially reactivated on June 25, 2012 as .
Operational history.
Six months after the Germans invaded the USSR in June 1941, talks aimed at closer co-operation between Free France and the Soviet Union resulted in setting up a special squadron with an initial core of 12 fighter pilots and 47 ground staff for service on the Russo-German front. De Gaulle officially promulgated the "Groupe de Chasse" GC 3 "Normandie" on 1 September 1942, with "Commandant" Pouliquen in command. Mechanics, pilots and hardware travelled by rail and air via Tehran (Iran) to Baku (now[ [update]] the capital of Azerbaijan). They completed a period of training on the Yakovlev Yak-7 by the end of January 1943, when "Commandant" Jean Tulasne took command of the "groupe". The unit became operational on 22 March 1943.
The first campaign of GC 3, equipped with the Yakovlev Yak-1 fighter, lasted until 5 October, and saw combat between Polotniani-Zavod and Sloboda/Monostirtchina. From an initial aerial victory over a Focke-Wulf Fw 190 on 5 April their tally rose dramatically and the squadron became the focus of Soviet propaganda, so much so that "Generalfeldmarschall" Wilhelm Keitel decreed that any French pilot captured would be executed.
Tulasne was killed in combat on 17 July, and "Commandant" Pierre Pouyade took command. On 11 October de Gaulle accorded the "groupe" the title of "Compagnon de la Libération". By the time GC 3 relocated to Tula on 6 November 1943, only six pilots remained from the original "groupe", which had accumulated 72 aerial victories since becoming operational. In their first year on the front they claimed 86 kills (77 confirmed, 9 'probables') and 16 enemy aircraft damaged, for the loss of 25 Yak fighters. 
In 1944 the "groupe" was expanded to become a "régiment", with a fourth "escadrille" joining its ranks. After completing training on the more advanced Yakovlev Yak-9D fighter at Tula, the expanded regiment rejoined front line operations for its second campaign. This took place around Doubrovka (in Russia) and Gross-Kalweitchen (in East Prussia, Germany) until 27 November 1944. During this campaign Joseph Stalin ordered the regiment to style itself "Normandie-Niemen" in recognition of its participation in the battles to liberate the river of the same name. On 16 October, the first day of a new offensive against East Prussia, the regiment’s pilots claimed 29 enemy aircraft destroyed without loss. By the following month the regiment found itself based in German territory. By the end of the year, Pouyade was released from command of the regiment and he, along with other veteran pilots, returned to France. He was replaced by Commandant Louis Delfino. By the end of 1944 201 kills have been claimed. 
14 January 1945 saw the "Normandie-Niemen" start its third campaign (from Dopenen to Heiligenbeil), concentrating in the East Prussian part of the German "Reich", until the formal announcement of victory in the east on 9 May the day after V-E Day in Europe. The USSR expressed its gratitude to the regiment by offering 37 of the unit’s Yak-3 fighters as a gift to France. The pilots returned to a heroes' welcome in Paris on 20 June 1945.
At the end of the war, the regiment had claimed 273 enemy aircraft shot down, 37 probables, and lost 87 aircraft and 52 pilots in return. Some 5,240 sorties were flown and the unit took part in 869 dogfights. The unit also destroyed 27 trains, 22 locomotives, two E-boats, 132 trucks, and 24 staff cars. Forty-two of the squadron's pilots were killed and 30 reached ace status.
Four of its pilots, Marcel Albert, Marcel Lefèvre, Jacques André and Roland de La Poype, became Heroes of the Soviet Union.
Its battle honours included such names such as Bryansk, Orel, Ielnia, Smolensk, Königsberg (later renamed Kaliningrad by the Soviets), and Pillau. It received the following decorations: from France, the "Légion d'Honneur", the "Croix de la Libération", the "Médaille Militaire", the "Croix de guerre" with six "palmes"; from the USSR, it received the Order of the Red Banner and the Order of Alexander Nevsky, with eleven citations between the two orders. 
The squadron's last Yak-3 fighter is on static display at the Le Bourget Air and Space Museum.
Popular culture.
The 1960 Franco-Russian film "Normandie-Niemen" directed by Jean Dréville and Damir Viatich-Berejnykh, relates the arrival in Russia of the first twenty pilots for intensive training and the formation of the squadron.
In the Yuri Bondarev 1970-1971 "Liberation" film dramatization of the course of the war from the Battle of Kursk to the Battle of Berlin, the "Normandie-Niemen" makes an appearance. Pierre Pouyade is portrayed by Italian actor Erno Bertoli.
Character Lieutenant Duroc (Patrick Chauvel) accounts his battles as Normandie-Niemen Free French fighter in Pierre Schoendoerffer's 1992 movie "Dien Bien Phu".

</doc>
<doc id="41460" url="http://en.wikipedia.org/wiki?curid=41460" title="Optical isolator">
Optical isolator

An optical isolator, or optical diode, is an optical component which allows the transmission of light in only one direction. It is typically used to prevent unwanted feedback into an optical oscillator, such as a laser cavity. The operation of the device depends on the Faraday effect (which in turn is produced by magneto-optic effect), which is used in the main component, the Faraday rotator.
Theory.
The main component of the optical isolator is the Faraday rotator. The magnetic field, formula_1, applied to the Faraday rotator causes a rotation in the polarization of the light due to the Faraday effect. The angle of rotation, formula_2, is given by,
where, formula_4 is the Verdet constant of the material (amorphous or crystalline; solid, liquid, or gaseous) of which the rotator is made, and formula_5 is the length of the rotator. This is shown in Figure 2. Specifically for an optical isolator, the values are chosen to give a rotation of 45°.
It has been shown that a crucial requirement for any kind of optical isolator (not only the Faraday isolator) is some kind of non-reciprocal optics 
Polarization dependent isolator.
The polarization dependent isolator, or Faraday isolator, is made of three parts, an input polarizer (polarized vertically), a Faraday rotator, and an output polarizer, called an analyser (polarized at 45°).
Light traveling in the forward direction becomes polarized vertically by the input polarizer. The Faraday rotator will rotate the polarization by 45°. The analyser then enables the light to be transmitted through the isolator.
Light traveling in the backward direction becomes polarized at 45° by the analyser. The Faraday rotator will again rotate the polarization by 45°. This means the light is polarized horizontally (the rotation is sensitive to direction of propagation). Since the polarizer is vertically aligned, the light will be extinguished.
Figure 2 shows a Faraday rotator with an input polarizer, and an output analyser. For a polarization dependent isolator, the angle between the polarizer and the analyser, formula_2, is set to 45°. The Faraday rotator is chosen to give a 45° rotation.
Polarization dependent isolators are typically used in free space optical systems. This is because the polarization of the source is typically maintained by the system. In optical fibre systems, the polarization direction is typically dispersed in non polarization maintaining systems. Hence the angle of polarization will lead to a loss.
Polarization independent isolator.
The polarization independent isolator is made of three parts, an input birefringent wedge (with its ordinary polarization direction vertical and its extraordinary polarization direction horizontal), a Faraday rotator, and an output birefringent wedge (with its ordinary polarization direction at 45°, and its extraordinary polarization direction at −45°).
Light traveling in the forward direction is split by the input birefringent wedge into its vertical (0°) and horizontal (90°) components, called the ordinary ray (o-ray) and the extraordinary ray (e-ray) respectively. The Faraday rotator rotates both the o-ray and e-ray by 45°. This means the o-ray is now at 45°, and the e-ray is at −45°. The output birefringent wedge then recombines the two components.
Light traveling in the backward direction is separated into the o-ray at 45, and the e-ray at −45° by the birefringent wedge. The Faraday Rotator again rotates both the rays by 45°. Now the o-ray is at 90°, and the e-ray is at 0°. Instead of being focused by the second birefringent wedge, the rays diverge.
Typically collimators are used on either side of the isolator. In the transmitted direction the beam is split and then combined and focused into the output collimator. In the isolated direction the beam is split, and then diverged, so it does not focus at the collimator.
Figure 3 shows the propagation of light through a polarization independent isolator. The forward travelling light is shown in blue, and the backward propagating light is shown in red. The rays were traced using an ordinary refractive index of 2, and an extraordinary refractive index of 3. The wedge angle is 7°.
The Faraday rotator.
The most important optical element in an isolator is the Faraday rotator. The characteristics that one looks for in a Faraday rotator optic include a high Verdet constant, low absorption coefficient, low non-linear refractive index and high damage threshold. Also, to prevent self-focusing and other thermal related effects, the optic should be as short as possible. The two most commonly used materials for the 700–1100 nm range are terbium doped borosilicate glass and terbium gallium garnet crystal (TGG). For long distance fibre communication, typically at 1310 nm or 1550 nm, yttrium iron garnet crystals are used (YIG). Commercial YIG based Faraday isolators reach isolations higher than 30 dB.
Optical isolators are different from 1/4 wave plate based isolators because the Faraday rotator provides non-reciprocal rotation while maintaining linear polarization. That is, the polarization rotation due to the Faraday rotator is always in the same relative direction. So in the forward direction, the rotation is positive 45°. In the reverse direction, the rotation is −45°. This is due to the change in the relative magnetic field direction, positive one way, negative the other. This then adds to a total of 90° when the light travels in the forward direction and then the negative direction. This allows the higher isolation to be achieved.
Optical isolators and thermodynamics.
It might seem at first glance that a device that allows light to flow in only one direction would violate Kirchhoff's law and the second law of thermodynamics, by allowing light energy to flow from a cold object to a hot object and blocking it in the other direction, but the violation is avoided because the isolator must absorb (not reflect) the light from the hot object and will eventually reradiate it to the cold one. Attempts to re-route the photons back to their source unavoidably involve creating a route by which other photons can travel from the hot body to the cold one, avoiding the paradox.

</doc>
<doc id="41461" url="http://en.wikipedia.org/wiki?curid=41461" title="Optical path length">
Optical path length

In optics, optical path length (OPL) or optical distance is the product of the geometric length of the path light follows through the system, and the index of refraction of the medium through which it propagates. A difference in optical path length between two paths is often called the optical path difference (OPD). Optical path length is important because it determines the phase of the light and governs interference and diffraction of light as it propagates.
Optical path difference (OPD).
"Optical path difference" corresponds to the phase shift which happens between two previously coherent sources when passed through different mediums. For example a wave passed through glass will appear to travel a greater distance than an identical wave in air. This is because the source in the glass will have experienced a greater number of wavelengths due to the higher refractive index of the glass.
The OPD can be calculated from the following equation:
where "d"1 and "d"2 are the distances of the ray passing through medium 1 or 2, "n"1 is the greater refractive index (e.g., glass) and "n"2 is the smaller refractive index (e.g., air).
Details.
In a medium of constant refractive index, "n", the OPL for a path of physical length "d" is just
If the refractive index varies along the path, the OPL is given by
where "n"("s") is the local refractive index as a function of distance, "s", along the path "C".
An electromagnetic wave that travels a path of given optical path length arrives with the same phase shift as if it had traveled a path of that "physical" length in a vacuum. Thus, if a wave is traveling through several different media, then the optical path length of each medium can be added to find the total optical path length. The optical path difference between the paths taken by two identical waves can then be used to find the phase change. Finally, using the phase change, the interference between the two waves can be calculated.
Fermat's principle states that the path light takes between two points is the path that has the minimum optical path length.

</doc>
<doc id="41462" url="http://en.wikipedia.org/wiki?curid=41462" title="Optical power budget">
Optical power budget

The optical power budget in a fiber-optic communication link is the allocation of available optical power (launched into a given fiber by a given source) among various loss-producing mechanisms such as launch coupling loss, fiber attenuation, splice losses, and connector losses, in order to ensure that adequate signal strength (optical power) is available at the receiver. In optical power budget attenuation is specified in decibels (dB) and optical power in dBms. 
The amount of optical power launched into a given fiber by a given transmitter depends on the nature of its active optical source (LED or laser diode) and the type of fiber, including such parameters as core diameter and numerical aperture. Manufacturers sometimes specify an optical power budget only for a fiber that is optimum for their equipment—or specify only that their equipment will operate over a given distance, without mentioning the fiber characteristics. The user must first ascertain, from the manufacturer or by testing, the transmission losses for the type of fiber to be used, and the required signal strength for a given level of performance. 
In addition to transmission loss, including those of any splices and connectors, allowance should be made for at least several dB of optical power margin losses, to compensate for component aging and to allow for future splices in the event of a severed cable.
Definitions:
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41463" url="http://en.wikipedia.org/wiki?curid=41463" title="Optical power margin">
Optical power margin

In an optical communications link, the optical power margin is the difference between the optical power that is launched by a given transmitter into the fiber, less transmission losses from all causes, and the minimum optical power that is required by the receiver for a specified level of performance. An optical power margin is typically measured using a calibrated light source and an optical power meter.
The optical power margin is usually expressed in decibels (dB). At least several dB of optical power margin should be included in the optical power budget. The amount of optical power launched into a given fiber by a given transmitter depends on the nature of its active optical source (LED or laser diode) and the type of fiber, including such parameters as core diameter and numerical aperture. 
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41464" url="http://en.wikipedia.org/wiki?curid=41464" title="Visible spectrum">
Visible spectrum

The visible spectrum is the portion of the electromagnetic spectrum that is visible to the human eye. Electromagnetic radiation in this range of wavelengths is called visible light or simply light. A typical human eye will respond to wavelengths from about 390 to 700 nm. In terms of frequency, this corresponds to a band in the vicinity of 430–790 THz. 
The spectrum does not, however, contain all the colors that the human eyes and brain can distinguish. Unsaturated colors such as pink, or purple variations such as magenta, are absent, for example, because they can be made only by a mix of multiple wavelengths. Colors containing only one wavelength are also called pure colors or spectral colors.
Visible wavelengths pass through the "optical window", the region of the electromagnetic spectrum that allows wavelengths to pass largely unattenuated through the Earth's atmosphere. An example of this phenomenon is that clean air scatters blue light more than red wavelengths, and so the midday sky appears blue. The optical window is also referred to as the "visible window" because it overlaps the human visible response spectrum. The near infrared (NIR) window lies just out of the human vision, as well as the Medium Wavelength IR (MWIR) window, and the Long Wavelength or Far Infrared (LWIR or FIR) window, although other animals may experience them.
History.
In the 13th century, Roger Bacon theorized that rainbows were produced by a similar process to the passage of light through glass or crystal.
In the 17th century, Isaac Newton discovered that prisms could disassemble and reassemble white light, and described the phenomenon in his book "Opticks". He was the first to use the word "spectrum" (Latin for "appearance" or "apparition") in this sense in print in 1671 in describing his experiments in optics. Newton observed that, when a narrow beam of sunlight strikes the face of a glass prism at an angle, some is reflected and some of the beam passes into and through the glass, emerging as different-colored bands. Newton hypothesized light to be made up of "corpuscles" (particles) of different colors, with the different colors of light moving at different speeds in transparent matter, red light moving more quickly than violet in glass. The result is that red light bends (refracted) less sharply than violet as it passes through the prism, creating a spectrum of colors.
 Newton divided the spectrum into seven named colors: red, orange, yellow, green, blue, indigo, and violet. He chose seven colors out of a belief, derived from the ancient Greek sophists, of there being a connection between the colors, the musical notes, the known objects in the solar system, and the days of the week. The human eye is relatively insensitive to indigo's frequencies, and some otherwise-well-sighted people cannot distinguish indigo from blue and violet. For this reason, some later commentators, including Isaac Asimov, have suggested that indigo should not be regarded as a color in its own right but merely as a shade of blue or violet. However, the evidence indicates that what Newton meant by "indigo" and "blue" does not correspond to the modern meanings of those color words. Comparing Newton's observation of prismatic colors to a color image of the visible light spectrum shows that "indigo" corresponds to what is today called blue, whereas "blue" corresponds to cyan.
In the 18th century, Goethe wrote about optical spectra in his "Theory of Colours". Goethe used the word "spectrum" ("Spektrum") to designate a ghostly optical afterimage, as did Schopenhauer in "On Vision and Colors". Goethe argued that the continuous spectrum was a compound phenomenon. Where Newton narrowed the beam of light to isolate the phenomenon, Goethe observed that a wider aperture produces not a spectrum but rather reddish-yellow and blue-cyan edges with white between them. The spectrum appears only when these edges are close enough to overlap.
In the early 19th century, the concept of the visible spectrum became more definite, as light outside the visible range was discovered and characterized by William Herschel (infrared) and Johann Wilhelm Ritter (ultraviolet), Thomas Young, Thomas Johann Seebeck, and others.
Young was the first to measure the wavelengths of different colors of light, in 1802.
The connection between the visible spectrum and color vision was explored by Thomas Young and Hermann von Helmholtz in the early 19th century. Their theory of color vision correctly proposed that the eye uses three distinct receptors to perceive color.
Animal color vision.
"See also: Physiology of color perception"
Many species can see light with frequencies outside the human "visible spectrum". Bees and many other insects can detect ultraviolet light, which helps them find nectar in flowers. Plant species that depend on insect pollination may owe reproductive success to their appearance in ultraviolet light rather than how colorful they appear to humans. Birds, too, can see into the ultraviolet (300–400 nm), and some have sex-dependent markings on their plumage that are visible only in the ultraviolet range. Many animals that can see into the ultraviolet range, however, cannot see red light or any other reddish wavelengths. Bees' visible spectrum ends at about 590 nm, just before the orange wavelengths start. Birds, however, can see some red wavelengths, although not as far into the light spectrum as humans. The popular belief that the common goldfish is the only animal that can see both infrared and ultraviolet light is incorrect, because goldfish cannot see infrared light.
Spectral colors.
Colors that can be produced by visible light of a narrow band of wavelengths (monochromatic light) are called pure spectral colors. The various color ranges indicated in the diagram on the right are an approximation: The spectrum is continuous, with no clear boundaries between one color and the next.
Spectroscopy.
Spectroscopy is the study of objects based on the spectrum of color they emit, absorb or reflect. Spectroscopy is an important investigative tool in astronomy, where scientists use it to analyze the properties of distant objects. Typically, astronomical spectroscopy uses high-dispersion diffraction gratings to observe spectra at very high spectral resolutions. Helium was first detected by analysis of the spectrum of the sun. Chemical elements can be detected in astronomical objects by emission lines and absorption lines.
The shifting of spectral lines can be used to measure the Doppler shift (red shift or blue shift) of distant objects.
Color display spectrum.
Color displays (e.g. computer monitors and televisions) cannot reproduce "all" colors discernible by a human eye. Colors outside the color gamut of the device, such as most spectral colors, can only be approximated. For color-accurate reproduction, a spectrum can be projected onto a uniform gray field. The resulting mixed colors can have all their R,G,B coordinates non-negative, and so can be reproduced without distortion. This accurately simulates looking at a spectrum on a gray background.#redirect 

</doc>
<doc id="41465" url="http://en.wikipedia.org/wiki?curid=41465" title="Optical switch">
Optical switch

In telecommunication, an optical switch is a switch that enables signals in optical fibers or integrated optical circuits (IOCs) to be selectively switched from one circuit to another.
Terminology.
The word applies on several levels. In commercial terms (such as "the telecom optical switch market size") it refers to any piece of circuit switching equipment between fibers. The majority of installed systems in this category actually use electronic switching between fiber transponders. Systems that perform this function by routing light beams are often referred to as "photonic" switches, independent of how the light itself is switched. Away from telecom, an optical switch is the unit that actually switches light between fibers, and a photonic switch is one that does this by exploiting nonlinear material properties to steer light (i.e., to switch wavelengths or signals within a given fiber).
Hence a certain portion of the optical switch market is made up of photonic switches. These will contain within them an optical switch, which will, in some cases, be a photonic switch.
Operation.
An optical switch may operate by mechanical means, such as physically shifting an optical fiber to drive one or more alternative fibers, or by electro-optic effects, magneto-optic effects, or other methods. Slow optical switches, such as those using moving fibers, may be used for alternate routing of an optical switch transmission path, such as routing around a fault. Fast optical switches, such as those using electro-optic or magneto-optic effects, may be used to perform logic operations; also included in this category are semiconductor optical amplifiers, which are optoelectronic devices that can be used as optical switches and be integrated with discrete or integrated microelectronic circuits.
Functionality.
The functionality of any switch can be described in terms of the connections it can establish. As stated in Telcordia 
a connection is the association between two ports on a switch and is indicated as a
pair of port identifiers ("i", "j" ), where "i" and "j" are two ports between which the
connection is established. A connection identifies the transmission path between
two ports. An optical signal can be applied to either one of the connected ports.
However, the nature of the signal emerging at the other port depends on the optical
switch and the state of the connection. A connection can be in the "on" state or the
"off" state. A connection is said to be in the "on" state if an optical signal applied to
one port emerges at the other port with essentially zero loss in optical energy. A
connection is said to be in the "off" state if essentially zero optical energy emerges
at the other port.
Connections established in optical switches can be unidirectional or bidirectional. A unidirectional connection only allows optical signal transmission in one direction between the connected ports. A bidirectional connection allows optical signal transmission in both directions over the connection. Connections in passive and transparent optical switches are bidirectional, i.e., if a connection ("i", "j" ) is set up, optical transmission is possible from "i" to "j" and from "j" to "i".
A device is optically “transparent” if the optical signal launched at the input remains optical throughout its transmission path in the device and appears as an optical signal at the output. Optically transparent devices operate over a range of wavelengths called the passband.
A passive optical switch does not have optical gain elements. An active optical switch has optical gain elements. An all-optical switch is a transparent optical switch in which the actuating signal is also optical. Thus, in an all-optical switch, an optical signal is used to switch the path another optical signal takes through the switch.
Performance.
Various parameters are defined and specified to quantify the performance of optical switches. The steady state performance of an optical switch (or optical switching matrix) is measured by its ability to effectively transmit optical power from an input port to any one of N output ports over the “on” state transmission path, and its ability to effectively isolate input power sources from all non-active ports over the “off” state transmission paths. Other key optical performance parameters include transmission efficiency over a range of wavelengths, the ability to minimize input optical power reflected back into the input fiber, transmission balance, and bidirectional transmission. The optical switch (or switching matrix) transient behavior is another important characteristic that is specified by its speed of response to control stimulation via the time interval it takes to either transmit or block the optical signal on any given output port.
Two rates can be associated with switches: the switching rate and the signal transmission rate. The switching rate is the rate at which a switch changes states. The signal transmission rate is the modulation rate of information passing through a switch. The signal transmission rate is usually much greater than the switching rate. (If the switching rate approaches or exceeds the transmission rate, then the switch can be called an optical modulator.)
A switch’s ability to sustain its steady state and transient performance specifications under stressful environmental conditions and over time is also an important characteristic.
Applications.
Optical switching technology is driven by the need to provide flexibility in optical network connectivity. Prime applications are optical protection, test systems, and remotely reconfigurable add-drop multiplexers. Possible future applications include remote optical provisioning and restoration.
Current switching applications include passive protection switching for service restoration following a disruption, such as a fiber cut. One common application for switches is in Remote Fiber Test Systems (RFTSs) that can monitor and locate a fault on a fiber transmission line. An emerging application of optical switches is optical cross-connection. Optical cross-connects utilize optical switching fabrics to establish an interconnection between multiple optical inputs and outputs.
Patents.
A 2011 search on “optical switch” yielded some 8,000 patents, roughly categorized as follows:

</doc>
<doc id="41466" url="http://en.wikipedia.org/wiki?curid=41466" title="Optical time-domain reflectometer">
Optical time-domain reflectometer

An optical time-domain reflectometer (OTDR) is an optoelectronic instrument used to characterize an optical fiber. An OTDR is the optical equivalent of an electronic time domain reflectometer. It injects a series of optical pulses into the fiber under test and extracts, from the same end of the fiber, light that is scattered (Rayleigh backscatter) or reflected back from points along the fiber. The scattered or reflected light that is gathered back is used to characterize the optical fiber. This is equivalent to the way that an electronic time-domain reflectometer measures reflections caused by changes in the impedance of the cable under test. The strength of the return pulses is measured and integrated as a function of time, and plotted as a function of fiber length.
Reliability and quality of OTDR equipment.
The reliability and quality of an OTDR is based on its accuracy, measurement range, ability to resolve and measure closely spaced events, measurement speed, and ability to perform satisfactorily under various environmental extremes and after various types of physical abuse. The instrument is also judged on the basis of its cost, features provided, size, weight, and ease of use.
Some of the terms often used in specifying the quality of an OTDR are as follows:
Industry requirements for the reliability and quality of OTDRs are specified in the Generic Requirements for Optical Time Domain Reflectometer (OTDR) Type Equipment.
Types of OTDR-like test equipment.
The common types of OTDR-like test equipment are:
OTDR Data Format.
In the late 1990s, OTDR industry representatives and the OTDR user community developed a unique data format to store and analyze OTDR fiber data. This data was based on the specifications in GR-196, Generic Requirements for Optical Time Domain Reflectometer (OTDR) Type Equipment. The goal was for the data format to be truly universal, in that it was intended to be implemented by all OTDR manufacturers. OTDR suppliers developed the software to implement the data format. As they proceeded, they identified inconsistencies in the format, along with areas of misunderstanding among users.
From 1997 to 2000, a group of OTDR supplier software specialists attempted to resolve problems and inconsistencies in what was then called the “Bellcore” OTDR Data Format. This group, called the OTDR Data Format Users Group (ODFUG), made progress. Since then, many OTDR developers continued to work with other developers to solve individual interaction problems and enable cross use between manufacturers.
In 2011, Telcordia decided to compile industry comments on this data format into one document entitled Optical Time Domain Reflectometer (OTDR) Data Format. This Special Report (SR) summarizes the state of the Bellcore OTDR Data Format, renaming it as the Telcordia OTDR Data Format.
The data format is intended for all OTDR-related equipment designed to save trace data and analysis information. Initial implementations require standalone software to be provided by the OTDR supplier to convert existing OTDR trace files to the SR-4731 data format and to convert files from this universal format to a format that is usable by their older OTDRs. This file conversion software can be developed by the hardware supplier, the end user, or a third party. This software also provides backward compatibility of the OTDR data format with existing equipment.
The SR-4731 format describes binary data. While text information is contained in several fields, most numbers are represented as either 16-bit (2-byte) or 32-bit (4-byte) signed or unsigned integers stored as binary images. Byte ordering in this file format is explicitly low-byte ordering, as is common on Intel® processor-based machines. String fields are terminated with a zero byte “\0”. OTDR waveform data are represented as short, unsigned integer data uniformly spaced in time, in units of decibels (dB) times 1000, referenced to the maximum power level. The maximum power level is set to zero, and all waveform data points are assumed to be zero or negative (the sign bit is implied), so that the minimum power level in this format is -65.535 dB, and the minimum resolution between power level steps is 0.001 dB. In some cases, this will not provide sufficient power range to represent all waveform points. For this reason, the use of a scale factor has been introduced to expand the data point power range.
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41472" url="http://en.wikipedia.org/wiki?curid=41472" title="Outside plant">
Outside plant

In telecommunication, the term outside plant has the following meanings:
The CATV industry divides its fixed assets between head end or inside plant, and outside plant. The electrical power industry also uses the term outside plant to refer to electric power distribution systems.
Context.
Network connections between devices such as computers, printers, and phones require a physical infrastructure to carry and process signals. Typically, this infrastructure will consist of:
The portion of this infrastructure contained within a building is the inside plant, and the portion of this infrastructure connecting buildings or facilities is the outside plant. Where these two plants meet in a given structure is the demarcation point.
Outside plant cabling, whether copper or fiber, is generally installed as aerial cable between poles, in an underground conduit system, or by direct burial.
Hardware associated with the outside plant must be either protected from the elements (for example, distribution frames are generally protected by a street side cabinet) or constructed with materials suitable for exposure to the elements. Installation of the outside plant elements often require construction of significant physical infrastructure, such as underground vaults. In older large installations, cabling is sometimes protected by air pressure systems designed to prevent water infiltration. While this is not a modern approach, the cost of replacement of the older cabling with sealed cabling is often prohibitively expensive. The cabling used in the outside plant must also be protected from electrical disturbances caused by lightning or voltage surges due to electrical shorts or induction.
Example: copper access network.
In civilian telecommunications, the copper access network (also known as the local loop) providing basic telephone or DSL services typically consists of the following elements:
Active equipment (such as a POTS or DSL line circuit) can then be connected to the line in order to provide service, but this is not considered part of outside plant.
Protecting equipment in the outside plant.
The environment can play a large role in the quality and lifespan of equipment used in the outside plant. It is critical that environmental testing criteria as well as design and performance requirements be defined for this type of equipment.
There are generally four operating environments or classes covering all outside plant (OSP) applications, including wireless facilities.
Electronic equipment located in one or more of these environmental class locations is designed to withstand various environmental operating conditions resulting from climatic conditions that may include rain, snow, sleet, high winds, ice, salt spray, and sand storms. Since outside temperatures can possibly range from −40°C (−40°F) to 46°C (115°F), with varying degrees of solar loading, along with humidity levels ranging from below 10% up to 100%, significant environmental stresses within the enclosure or facility can be produced.
Telcordia , contains the most recent industry data regarding each Class described above. It also discusses what is currently happening in ATIS and Underwriters Laboratories (UL).
The document also includes 
Handholes and other below-ground splice vaults.
Handholes and other below-ground splice vaults house telecommunications components used in an Outside Plant (OSP) environment.
Handholes are plastic or polymer concrete structures set below ground with their lids flush to the surrounding soil, turf, footpath, or road surface. They can be used to house and protect copper, coaxial, and optical fiber telephone cable splices and distribution elements. They safeguard and provide convenient access to cable termination and branch points, provide flexibility and access for installation operations (e.g., pulling or blowing cables), provide mechanical and environmental protection for splices, allow access for craftsperson work activities, and discourage access by unauthorized persons. 
Handholes and other below-ground splice vaults are deployed in a variety of environments. The major distinctions in these environments focus on the strength and frequency of vehicular and foot traffic loading. There are four basic application environments:
Handhole-type products deployed in any environment are subjected to the following types of traffic loading: Vertical Cover Load, Vertical Sidewall Load, Lateral Sidewall Load, and Long-Term Lateral Sidewall Load.
Telcordia contains detailed industry requirements for handholes, and includes specific loading requirements for the defined application environments. It provides explicit correlations to other standards such as ANSI/SCTE-77, AASHTO specifications, and ASTM C857.
Corrosion resistance.
Corrosion in outside plant telecommunications network components is caused by exposure to the effects of temperature, humidity, electrical power, and contaminants. Corrosion resistance criteria for these network components are based on the environments to which they are exposed.
Outside plant environments can be above-ground, underground, buried, or underwater. Industry requirements document Telcordia defines these environments and provides corrosion resistance criteria for the telecommunications equipment in each. It also includes references to various associated ASTM Standards.
Above-ground plant.
Above-ground plant includes all the telecommunications equipment physically located on or above the ground. This includes enclosures such as huts, cabinets, and pedestals, and the equipment mounted therein. It also includes pole-mounted equipment and cases, and pole-line hardware. 
Above-ground plant can be exposed to extreme temperatures, and to humidity that varies with the seasons and with daily temperature changes. When humidity condenses on the surfaces of outdoor apparatus or equipment, the corrosivity of the moisture layer can be increased by industrial pollutants that render the condensate moisture corrosive. In sea coastal areas, wind-borne, salt-laden water droplets can deposit on exposed components. 
Near large cultivated areas, where fertilizers are applied by airplanes, the wind may carry nitrates, phosphates, and ammonium compounds to settle on metallic components of the above-ground telephone plant. Similarly, in residential areas, lawn fertilizers and herbicides can cause corrosion. In regions with snow, the salts used to melt snow and ice on roadways can accelerate corrosion. Under extreme conditions, pedestals and cabinets may be flooded with water that contains mud and corrosive salts. Corrosion of these flooded components may be accelerated by the presence of dc voltages used to power the networks. Secretions from insects can also accelerate corrosion. Finally, chewing by rodents may expose metallic components, normally protected by a polymer or paint coating, to a corrosive environment.
Underground plant.
Underground plant includes all the telecommunications equipment installed in underground structures such as utility holes, Controlled Environment Vaults (CEVs), and ducts, along with associated hardware. Underground plant can be exposed to waters containing water soluble salts of the native soil. Utility holes often show evidence of corrosion of support hardware and bonding ribbons that is caused by sulfate-reducing bacteria. The environment in utility holes and ducts can be made corrosive by man-made chemicals such as industrial effluent, fertilizers, and de-icing salts. Protective plastic coatings and cable jackets can rapidly deteriorate from leaking steam pipes present in many urban areas and from gasoline leaking from underground storage tanks.
The most aggressive contributor to corrosion of underground plant is dc stray current from electrified rail transportation systems, cathodic protection rectifiers, or welding and mining operations. Although such dc currents are mostly dealt with “after the fact” using protective systems (e.g., low resistance bonds, reverse current switches, cathodic protection), some of the protection has to be included at the manufacturing stage. This protection may include insulating covers on cable shields, or nonmetallic components or coatings for apparatus.
Buried plant.
Buried plant consists of telecommunications equipment such as cables, splice closures, lower parts of pedestals, and grounding systems directly buried in the soil. Buried plant can be exposed to the same corrosive environment as underground plant. In addition, attack by gophers can expose underlying components to corrosion attack.
Underwater plant.
Underwater plant includes all telecommunications equipment located beneath the surface of a body of water. This includes cables and repeaters. The water can range from relatively pure, to brackish, to badly contaminated with industrial effluent.

</doc>
<doc id="41473" url="http://en.wikipedia.org/wiki?curid=41473" title="Ovality">
Ovality

In telecommunications and fiber optics, ovality or noncircularity is the degree of deviation from perfect circularity of the cross section of the core or cladding of the fiber.
The cross-sections of the core and cladding are assumed to a first approximation to be elliptical. Quantitatively, the ovality of either the core or cladding is expressed as formula_1, where "a" is the length of the major axis and "b" is the length of the minor axis. The dimensionless quantity so obtained may be multiplied by 100 to express ovality as a percentage. Alternatively, ovality of the core or cladding may be specified by a tolerance field consisting of two concentric circles, within which the cross section boundaries must lie.
In measurements, ovality is the amount of out-of-roundness of a hole or cylindrical part in the typical form of an oval.
In chemistry.
In computational chemistry, especially in QSAR studies, ovality refers to, a measure of how the shape of a molecule approaches a sphere (at one extreme) or a cigar shape (at the other). 
Ovality is described by a ratio of volume to area:
formula_2
where:
The ovality of the He atom is 1.0 and that of HC24H (12 triple bonds) is ~1.7.

</doc>
<doc id="41474" url="http://en.wikipedia.org/wiki?curid=41474" title="Overfill">
Overfill

In telecommunications, overfill is the condition that prevails when the numerical aperture or the beam diameter of an optical source, such as a laser, light-emitting diode, or optical fiber, exceeds that of the driven element, e.g. an optical fiber core. In optical communications testing, overfill in both numerical aperture and mean diameter (core diameter or spot size) is usually required.
In polygonal mirror scanners, an overfilled type is one which uses each mirror facet at least in one dimension completely.

</doc>
<doc id="41475" url="http://en.wikipedia.org/wiki?curid=41475" title="Overflow">
Overflow

Overflow may refer to:

</doc>
<doc id="41476" url="http://en.wikipedia.org/wiki?curid=41476" title="Overhead information">
Overhead information

Overhead information is digital information transferred across the functional interface between a user and a telecommunications system, or between functional units within a telecommunications system, for the purpose of directing or controlling the transfer of user information or the detection and correction of errors. 
Overhead information originated by the user is not considered to be system overhead information. Overhead information generated within the communications system and not delivered to the user is system overhead information. Thus, the user throughput is reduced by both overheads while system throughput is reduced only by system overhead.

</doc>
<doc id="41477" url="http://en.wikipedia.org/wiki?curid=41477" title="Overmodulation">
Overmodulation

Overmodulation is the condition that prevails in telecommunication when the instantaneous level of the modulating signal exceeds the value necessary to produce 100% modulation of the carrier. In the sense of this definition, it is almost always considered a fault condition. In layman's terms, the signal is going "off the scale". Overmodulation results in spurious emissions by the modulated carrier, and distortion of the recovered modulating signal. This means that the envelope of the output waveform is distorted. 
Although overmodulation is sometimes considered permissible, it should not occur in practice; a distorted waveform envelope will result in a distorted output signal of the receiving medium.

</doc>
<doc id="41478" url="http://en.wikipedia.org/wiki?curid=41478" title="Override">
Override

Override may refer to:

</doc>
<doc id="41479" url="http://en.wikipedia.org/wiki?curid=41479" title="Overshoot">
Overshoot

Overshoot may refer to:

</doc>
<doc id="41480" url="http://en.wikipedia.org/wiki?curid=41480" title="Overtone">
Overtone

An overtone is any frequency higher than the fundamental frequency of a sound. Using the model of Fourier analysis, the fundamental and the overtones together are called partials. Harmonics, or more precisely, harmonic partials, are partials whose frequencies are integer multiples of the fundamental (including the fundamental which is 1 times itself). These overlapping terms are variously used when discussing the acoustic behavior of musical instruments. (See etymology below.) The model of Fourier analysis provides for the inclusion of inharmonic partials, which are partials whose frequencies are not whole-number ratios of the fundamental (such as 1.1 or 2.14179).
When a resonant system such as a blown pipe or plucked string is excited, a number of overtones may be produced along with the fundamental tone. In simple cases, such as for most musical instruments, the frequencies of these tones are the same as (or close to) the harmonics. Examples of exceptions include the circular drum, – a timpani whose first overtone is about 1.6 times its fundamental resonance frequency, gongs and cymbals, and brass instruments. The human vocal tract is able to produce highly variable amplitudes of the overtones, called formants, which define different vowels.
Explanation.
Most oscillators, from a guitar string to a flute (or even the hydrogen atom or a periodic variable star) will naturally vibrate at a series of distinct frequencies known as normal modes. The lowest normal mode frequency is known as the fundamental frequency, while the higher frequencies are called overtones. Often, when an oscillator is excited by, for example, plucking a guitar string, it will oscillate at several of its modal frequencies at the same time. So when a note is played, this gives the sensation of hearing other frequencies (overtones) above the lowest frequency (the fundamental).
Timbre is the quality that gives the listener the ability to distinguish between the sound of different instruments. The timbre of an instrument is determined by which overtones it emphasizes. That is to say, the relative volumes of these overtones to each other determines the specific "flavor" or "color" of sound of that family of instruments. The intensity of each of these overtones is rarely constant for the duration of a note. Over time, different overtones may decay at different rates, causing the relative intensity of each overtone to rise or fall independent of the overall volume of the sound. A carefully trained ear can hear these changes even in a single note. This is why the timbre of a note may be perceived differently when played staccato or legato.
A driven non-linear oscillator, such as the vocal folds, a blown wind instrument, or a bowed violin string (but not a struck guitar string or bell) will oscillate in a periodic, non-sinusoidal manner. This generates the impression of sound at integer multiple frequencies of the fundamental known as harmonics, or more precisely, harmonic partials. For most string instruments and other long and thin instruments such as a bassoon, the first few overtones are quite close to integer multiples of the fundamental frequency, producing an approximation to a harmonic series. Thus, in music, overtones are often called harmonics. Depending upon how the string is plucked or bowed, different overtones can be emphasized.
However, some overtones in some instruments may not be of a close integer multiplication of the fundamental frequency, thus causing a small dissonance. "High quality" instruments are usually built in such a manner that their individual notes do not create disharmonious overtones. In fact, the flared end of a brass instrument is not to make the instrument sound louder, but to correct for tube length “end effects” that would otherwise make the overtones significantly different from integer harmonics. This is illustrated by the following:
Consider a guitar string. Its idealized 1st overtone would be exactly twice its fundamental if its length were shortened by ½, perhaps by lightly pressing a guitar string at the 12th fret; however, if a vibrating string is examined, it will be seen that the string does not vibrate flush to the bridge and nut, but it instead has a small “dead length” of string at each end. This dead length actually varies from string to string, being more pronounced with thicker and/or stiffer strings. This means that halving the physical string length does not halve the actual string vibration length, and, hence, the overtones will not be exact multiples of a fundamental frequency. The effect is so pronounced that properly set up guitars will angle the bridge such that the thinner strings will progressively have a length up to few millimeters shorter than the thicker strings. Not doing so would result in inharmonious chords made up of two or more strings. Similar considerations apply to tube instruments.
Musical usage term.
An overtone is a partial (a "partial wave" or "constituent frequency") that can be either a harmonic partial (a harmonic) other than the fundamental, or an inharmonic partial. A harmonic frequency is an integer multiple of the fundamental frequency. An inharmonic frequency is a non-integer multiple of a fundamental frequency.
An example of harmonic overtones: (absolute harmony)
Some musical instruments produce overtones that are slightly sharper or flatter than true harmonics. The sharpness or flatness of their overtones is one of the elements that contributes to their unique sound. Due to phase inconsistencies between the fundamental and the partial harmonic, this also has the effect of making their waveforms not perfectly periodic.
Musical instruments that can create notes of any desired duration and definite pitch have harmonic partials.
A tuning fork, provided it is sounded with a mallet (or equivalent) that is reasonably soft, has a tone that consists very nearly of the fundamental, alone; it has a sinusoidal waveform. Nevertheless, music consisting of pure sinusoids was found to be unsatisfactory in the early 20th century.
Etymology.
In Hermann von Helmholtz's classic "On The Sensations Of Tone" he used the German "Obertöne" which was actually a contraction of "Oberpartialtöne", or in English: "upper partial tones". According to Alexander Ellis (in pages 24–25 of his definitive English translation of Helmholtz), the similarity of German "ober" to English "over" caused a Prof. Tyndall to mistranslate Helmholtz' term, thus creating "overtone". Ellis disparages the term "overtone" for its awkward implications. Because "overtone" makes the upper partials seem like such a distinct phenomena, it leads to the mathematical problem described above where the first overtone is the second partial. Also, unlike discussion of "partials", the word "overtone" has connotations that have led people to wonder about the presence of "undertones" (a term sometimes confused with "difference tones" but also used in speculation about a hypothetical "undertone series").
"Overtones" in barbershop music.
In barbershop music, the word "overtone" is often used in a related but particular manner. It refers to a psychoacoustic effect in which a listener hears an audible pitch that is higher than, and different from, the fundamentals of the four pitches being sung by the quartet. The barbershop singer's "overtone" is created by the interactions of the upper partial tones in each singer's note (and by sum and difference frequencies created by nonlinear interactions within the ear). Similar effects can be found in other "a cappella" polyphonic music such as the music of the Republic of Georgia and the Sardinian "cantu a tenore".
String instruments.
String instruments can also produce multiphonic tones when strings are divided in two pieces. The most developed instrument for playing multiphonic tones is the Sitar in which there are sympathetic strings which help to bring out the overtones while one is playing. The most well-known technique on a guitar is playing flageolet tones. The Ancient Chinese instrument the Guqin contains a scale based on the knotted positions of overtones. Also the Vietnamese Đàn bầu functions on flageolet tones. Other multiphonic extended techniques used are prepared piano, prepared guitar and 3rd bridge.
Overtone singing.
Overtone singing, also called harmonic singing, occurs when the singer amplifies voluntarily two overtones in the sequence available given the fundamental tone he/she is singing. Overtone singing is a traditional form of singing in many parts of the Himalayas and Altay; Tibetans, Mongols and Tuvans are known for their overtone singing. In these contexts it is often referred to as throat singing, though it should not be confused with Inuit throat singing, which is produced by different means.
Jaw harp.
A similar technique is used for playing the jaw harp: the performer amplifies the instrument's overtones by changing the shape, and therefore the resonance, of their vocal tract.
Free-reed aerophone.
Likewise, when playing a harmonica or pitch pipe, one may alter the shape of their mouth to amplify specific overtones.

</doc>
<doc id="41481" url="http://en.wikipedia.org/wiki?curid=41481" title="Packet-switching node">
Packet-switching node

Packet-switching node: In a packet-switching network, a node that contains data switches and equipment for controlling, formatting, transmitting, routing, and receiving data packets. 
"Note:" In the Defense Data Network (DDN), a packet-switching node is usually configured to support up to thirty-two X.25 56 kbit/s host connections, as many as six 56 kbit/s interswitch trunk (IST) lines to other packet-switching nodes, and at least one Terminal Access Controller (TAC).
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41482" url="http://en.wikipedia.org/wiki?curid=41482" title="Paired disparity code">
Paired disparity code

In telecommunication, a paired disparity code is a line code in which at least one of the data characters are represented by two codewords of opposite disparity that are used in sequence so as to minimize the total disparity of a longer sequence of digits.
A particular codeword of any line code can either have no disparity (the average weight of the codeword is zero), negative disparity (the average weight of the codeword is negative), or positive disparity (the average weight of the codeword is positive).
In a paired disparity code, every codeword that averages to a negative level (negative disparity) is paired with some other codeword that averages to a positive level (positive disparity).
In a system that uses a paired disparity code, the transmitter must keep track of the running DC buildup -- the running disparity -- and always pick the codeword that pushes the DC level back towards zero. The receiver is designed so that either codeword of the pair decodes to the same data bits.
Most line codes use either a paired disparity code or a constant-weight code.
The simplest paired disparity code is alternate mark inversion signal.
Other paired disparity codes include 8B10B, , the modified AMI codes, coded mark inversion, and 4B3T.
The digits may be represented by disparate physical quantities, such as two different frequencies, phases, voltage levels, magnetic polarities, or electrical polarities, each one of the pair representing a 0 or a 1.

</doc>
<doc id="41486" url="http://en.wikipedia.org/wiki?curid=41486" title="Title 47 CFR Part 68">
Title 47 CFR Part 68

Title 47 CFR Part 68 is a section of the Code of Federal Regulations of the United States that regulate the direct electrical connection of telecommunications equipment and customer premises wiring with the public switched telephone network, certain private line services, and connection of private branch exchange (PBX) equipment to certain telecommunication interfaces.
Scope.
Part 68 rules provide the technical and procedural standards under which direct electrical connection of customer-provided telephone equipment, systems, and protective apparatus may be made to the nationwide network without causing harm and without a requirement for protective circuit arrangements in the service-provider networks.
The equivalent European regulation is called TBR21.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41487" url="http://en.wikipedia.org/wiki?curid=41487" title="Party line">
Party line

Party line may refer to:

</doc>
<doc id="41488" url="http://en.wikipedia.org/wiki?curid=41488" title="Passband">
Passband

A passband is the range of frequencies or wavelengths that can pass through a filter.
A bandpass-filtered signal (that is, a signal with energy only in a passband), is known as a bandpass signal contrary to a baseband signal.
Filters.
In telecommunications, optics, and acoustics, a passband (a band-pass filtered signal) is the portion of the frequency spectrum that is transmitted (with minimum relative loss or maximum relative gain) by some filtering device. In other words, it is a "band" of frequencies which "pass"es through some filter or set of filters.
The accompanying figure shows a schematic of a waveform being filtered by a bandpass filter consisting of a highpass and a lowpass filter.
Radio receivers generally include a tunable band-pass filter with a passband that is wide enough to accommodate the bandwidth of the radio signal transmitted by a single station.
Digital transmission.
There are two main categories of digital communication transmission methods: baseband and passband.
Details.
In general, there is an inverse relationship between the width of a filter's passband and the time required for the filter to respond to new inputs. Broad passbands yield faster response. This is a consequence of the mathematics of Fourier analysis.
The limiting frequencies of a passband are defined as those at which the relative intensity or power decreases to a specified fraction of the maximum intensity or power. This decrease in power is often specified to be the half-power points, "i.e.", 3 dB below the maximum power.
The difference between the limiting frequencies is called the bandwidth, and is expressed in hertz (in the optical regime, in nanometers or micrometers of differential wavelength).
The related term "bandpass" is an adjective that describes a type of filter or filtering process; it is frequently confused with "passband", which refers to the actual portion of affected spectrum. The two words are both compound words that follow the English rules of formation: the primary meaning is the latter part of the compound, while the modifier is the first part. Hence, one may correctly say 'A dual bandpass filter has two passbands'.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41490" url="http://en.wikipedia.org/wiki?curid=41490" title="Password length parameter">
Password length parameter

In telecommunication, a password length parameter is a basic parameter the value of which affects password strength against brute force attack and so is a contributor to computer security. 
One use of the password length parameters is in the expression formula_1, where formula_2 is the probability that a password can be guessed in its lifetime, formula_3 is the maximum lifetime a password can be used to log into a system, formula_4 is the number of guesses per unit of time, and formula_5 is the number of unique algorithm-generated passwords (the 'password space'). 
The degree of password security is determined by the probability that a password can be guessed in its lifetime.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41492" url="http://en.wikipedia.org/wiki?curid=41492" title="Path loss">
Path loss

Path loss (or path attenuation) is the reduction in power density (attenuation) of an electromagnetic wave as it propagates through space. Path loss is a major component in the analysis and design of the link budget of a telecommunication system.
This term is commonly used in wireless communications and propagation. Path loss may be due to many effects, such as free-space loss, refraction, diffraction, reflection, aperture-medium coupling loss, and absorption. Path loss is also influenced by terrain contours, environment (urban or rural, vegetation and foliage), propagation medium (dry or moist air), the distance between the transmitter and the receiver, and the height and location of antennas.
Causes.
Path loss normally includes "propagation losses" caused by the natural expansion of the radio wave front in free space (which usually takes the shape of an ever-increasing sphere), "absorption losses" (sometimes called penetration losses), when the signal passes through media not transparent to electromagnetic waves, "diffraction losses" when part of the radiowave front is obstructed by an opaque obstacle, and losses caused by other phenomena.
The signal radiated by a transmitter may also travel along many and different paths to a receiver simultaneously; this effect is called multipath. Multipath waves combine at the receiver antenna, resulting in a received signal that may vary widely, depending on the distribution of the intensity and relative propagation time of the waves and bandwidth of the transmitted signal. The total power of interfering waves in a Rayleigh fading scenario vary quickly as a function of space (which is known as "small scale fading"). Small-scale fading refers to the rapid changes in radio signal amplitude in a short period of time or travel distance.
Loss exponent.
In the study of wireless communications, path loss can be represented by the path loss exponent, whose value is normally in the range of 2 to 4 (where 2 is for propagation in free space, 4 is for relatively lossy environments and for the case of full specular reflection from the earth surface—the so-called Flat Earth model). In some environments, such as buildings, stadiums and other indoor environments, the path loss exponent can reach values in the range of 4 to 6. On the other hand, a tunnel may act as a waveguide, resulting in a path loss exponent less than 2.
Path loss is usually expressed in dB. In its simplest form, the path loss can be calculated using the formula
where formula_2 is the path loss in decibels, formula_3 is the path loss exponent, formula_4 is the distance between the transmitter and the receiver, usually measured in meters, and formula_5 is a constant which accounts for system losses.
Radio engineer formula.
Radio and antenna engineers use the following simplified formula (also known as the Friis transmission equation) for the path loss between two isotropic antennas in free space:
Path loss in dB: 
where formula_2 is the path loss in decibels, formula_8 is the wavelength and formula_4 is the transmitter-receiver distance in the same units as the wavelength.
Prediction.
Calculation of the path loss is usually called "prediction". Exact prediction is possible only for simpler cases, such as the above-mentioned "free space" propagation or the "flat-earth model". For practical cases the path loss is calculated using a variety of approximations.
"Statistical" methods (also called "stochastic" or "empirical") are based on measured and averaged losses along typical classes of radio links. Among the most commonly used such methods are Okumura-Hata, the COST Hata model, W.C.Y.Lee, etc. These are also known as "radio wave propagation models" and are typically used in the design of cellular networks and PLMN. For wireless communications in the VHF and UHF frequency band (the bands used by walkie-talkies, police, taxis and cellular phones), one of the most commonly used methods is that of Okumura-Hata as refined by the COST 231 project. Other well-known models are those of Walfisch-Ikegami, W.C.Y. Lee, and Erceg. For FM radio and TV broadcasting the path loss is most commonly predicted using the ITU model as described in P.1546 (former P.370) recommendation.
Deterministic methods based on the physical laws of wave propagation are also used; ray tracing is one such method. These methods are expected to produce more accurate and reliable predictions of the path loss than the empirical methods; however, they are significantly more expensive in computational effort and depend on the detailed and accurate description of all objects in the propagation space, such as buildings, roofs, windows, doors, and walls. For these reasons they are used predominantly for short propagation paths. Among the most commonly used methods in the design of radio equipment such as antennas and feeds is the finite-difference time-domain method.
The path loss in other frequency bands (MW, SW, Microwave) is predicted with similar methods, though the concrete algorithms and formulas may be very different from those for VHF/UHF. Reliable prediction of the path loss in the SW/HF band is particularly difficult, and its accuracy is comparable to weather predictions.
Easy approximations for calculating the path loss over distances significantly shorter than the distance to the radio horizon:
Examples.
In cellular networks, such as UMTS and GSM, which operate in the UHF band, the value of the path loss in built-up areas can reach 110–140 dB for the first kilometer of the link between the BTS and the mobile. The path loss for the first ten kilometers may be 150–190 dB ("Note": These values are very approximate and are given here only as an illustration of the range in which the numbers used to express the path loss values "can eventually be", these are not definitive or binding figures—the path loss may be very different for the same distance along two different paths and it can be different even along the same path if measured at different times.)
In the radio wave environment for mobile services the mobile antenna is close to the ground. LOS propagation models are highly modified. The signal path from the BTS antenna normally elevated above the roof tops is refracted down into the local physical environment (hills, trees, houses) and the LOS signal seldom reaches the antenna. The environment will produce several deflections of the direct signal onto the antenna, where typically 2-5 deflected signal components will be vectorially added.
These refraction and deflection processes cause loss of signal strength, which changes when the mobile antenna moves (Raleigh fading), causing instantaneous variations of up to 20 dB. The network is therefore designed to provide an excess of signal strength compared to LOS of 8-25 dB depending on the nature of the physical environment, and another 10 dB to overcome the fading due to movement.

</doc>
<doc id="41493" url="http://en.wikipedia.org/wiki?curid=41493" title="Path profile">
Path profile

In telecommunication, a path profile is a graphic representation of the physical features of a propagation path in the vertical plane containing both endpoints of the path, showing the surface of the Earth and including trees, buildings, and other features that may obstruct the radio signal. 
Profiles are drawn either with an effective Earth radius simulated by a parabolic arc--in which case the ray paths are drawn as straight lines--or with a ""flat Earth"--" in which case the ray paths are drawn as parabolic arcs.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41494" url="http://en.wikipedia.org/wiki?curid=41494" title="Path quality analysis">
Path quality analysis

Path quality analysis: In a communications path, an analysis that (a) includes the overall evaluation of the component quality measures, the individual link quality measures, and the aggregate path quality measures, and (b) is performed by evaluating communications parameters, such as bit error ratio, signal-plus-noise-plus-distortion to noise-plus-distortion ratio, and spectral distortion.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41495" url="http://en.wikipedia.org/wiki?curid=41495" title="Payload">
Payload

Payload is the carrying capacity of an aircraft or launch vehicle, usually measured in terms of weight. Depending on the nature of the flight or mission, the payload of a vehicle may include cargo, passengers, flight crew, munitions, scientific instruments or experiments, or other equipment. Extra fuel, when optionally carried, is also considered part of the payload. In a commercial context (i.e., an airline or air freight carrier), payload may refer only to revenue-generating cargo or paying passengers.
For a rocket, the payload can be a satellite, space probe, or spacecraft carrying humans, animals, or cargo. For a ballistic missile, the payload is one or more warheads and related systems; the total weight of these systems is referred to as the throw-weight.
The fraction of payload to the total liftoff weight of the air or spacecraft is known as the "payload fraction". When the weight of the payload and fuel are considered together, it is known as the "useful load fraction". In spacecraft, "mass fraction" is normally used, which is the ratio of payload to everything else, including the rocket structure.
Relationship of range and payload.
There is a natural trade-off between the payload and the range of an aircraft. A payload range diagram (also known as the "elbow chart") illustrates the trade-off.
The top horizontal line represents the maximum payload. It is limited structurally by maximum zero-fuel weight (MZFW) of the aircraft. Maximum payload is the difference between maximum zero-fuel weight and operational empty weight (OEW). Moving left-to-right along the line shows the constant maximum payload as the range increases. More fuel needs to be added for more range.
The vertical line represents the range at which the combined weight of the aircraft, maximum payload and needed fuel reaches the maximum take-off weight (MTOW) of the aircraft. If the range is increased beyond that point, payload has to be sacrificed for fuel.
The maximum take-off weight is limited by a combination of the maximum net power of the engines and the lift/drag ratio of the wings. The diagonal line after the range-at-maximum-payload point shows how reducing the payload allows increasing the fuel (and range) when taking off with the maximum take-off weight.
The second kink in the curve represents the point at which the maximum fuel capacity is reached. Flying further than that point means that the payload has to be reduced further, for an even lesser increase in range. The absolute range is thus the range at which an aircraft can fly with maximum possible fuel without carrying any payload.
Examples.
Examples of payload capacity:
Structural capacity.
For aircraft, the weight of fuel in wing tanks does not contribute as significantly to the bending moment of the wing as does weight in the fuselage. So even when the airplane has been loaded with its maximum payload that the wings can support, it can still carry a significant amount of fuel.
Payload constraints.
Launch and transport system differ not only on the payload that can be carried but also in the stresses and other factors placed on the payload. The payload must not only be lifted to its target, it must also arrive safely, whether elsewhere on the surface of the Earth or a specific orbit. To ensure this the payload, such as a warhead or satellite, is designed to withstand certain amounts of various types of "punishment" on the way to its destination. Most rocket payloads are fitted within a payload fairing to protect them against dynamic pressure of high-velocity travel through the atmosphere, and to improve the overall aerodynamics of the launch vehicle). Most aircraft payloads are carried within the fuselage for similar reasons. Outsize cargo may require a fuselage with unusual proportions, such as the Super Guppy.
The various constraints placed on the launch system can be roughly categorized into those that cause physical damage to the payload and those that can damage its electronic or chemical makeup. Examples of physical damage include extreme accelerations over short time scales caused by atmospheric buffeting or oscillations, extreme accelerations over longer time scales caused by rocket thrust and gravity, and sudden changes in the magnitude or direction of the acceleration caused by how quick engines are throttled and shut down, etc. Electrical, chemical, or biological payloads can be damage by extreme temperatures (hot or cold), rapid changes in temperature or pressure, contact with fast moving air streams causing ionization, and radiation exposure from cosmic rays, the van Allen belt, or solar wind.

</doc>
<doc id="41496" url="http://en.wikipedia.org/wiki?curid=41496" title="Pseudo bit error ratio">
Pseudo bit error ratio

Pseudo bit error ratio (PBER) in adaptive high-frequency (HF) radio, is a bit error ratio derived by a majority decoder that processes redundant transmissions. 
"Note:" In adaptive HF radio automatic link establishment, PBER is determined by the extent of error correction, such as by using the fraction of non-unanimous votes in the 2-of-3 majority decoder. 
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41497" url="http://en.wikipedia.org/wiki?curid=41497" title="PCS switching center">
PCS switching center

PCS switching center: In personal communications service, a facility that (a) supports access-independent call control/service control, and connection control (switching) functions, and (b) is responsible for interconnection of access and network systems to support end-to-end services. 
"Note 1:" The PCS switching center represents a collection of one or more network elements. 
"Note 2:" The term "center" does not imply a physical location.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41498" url="http://en.wikipedia.org/wiki?curid=41498" title="Greater Poland Voivodeship">
Greater Poland Voivodeship

Greater Poland Voivodeship (in Polish, "Województwo Wielkopolskie" ), also known as Wielkopolska Voivodeship or Wielkopolska Province, is a voivodeship, or province, in west-central Poland. It was created on 1 January 1999 out of the former Poznań, Kalisz, Konin, Piła and Leszno Voivodeships, pursuant to the Polish local government reforms adopted in 1998. The province is named after the region called Greater Poland or "Wielkopolska" . The modern province includes most of this historic region, except for some south-western parts.
Greater Poland Voivodeship is second in area and third in population among Poland's sixteen voivodeships, with an area of 29826 km2 and a population of close to 3.4 million. Its capital city is Poznań; other important cities include Kalisz, Konin, Piła, Ostrów Wielkopolski and Gniezno (an early capital of Poland). It is bordered by seven other voivodeships: West Pomeranian to the northwest, Pomeranian to the north, Kuyavian-Pomeranian to the north-east, Łódź to the south-east, Opole to the south, Lower Silesian to the southwest and Lubusz to the west.
The city of Poznań has international twinning arrangements with the English county of Nottinghamshire.
History.
Greater Poland, sometimes called the "cradle of Poland," formed the heart of the 10th-century early Polish state. Poznań and Gniezno were early centers of royal power, but following the region's devastation by pagan rebellion in the 1030s, and an invasion by Bretislaus I of Bohemia in 1038, the capital was moved by Casimir the Restorer from Gniezno to Kraków.
In the testament of Bolesław III Krzywousty, which initiated the period of fragmentation of Poland (1138–1320), the western part of Greater Poland (including Poznań) was granted to Mieszko III the Old. The eastern part, with Gniezno and Kalisz, was part of the Duchy of Kraków, granted to Władysław II. However for most of the period the two parts were under a single ruler, and were known as the Duchy of Greater Poland (although at times there were separately ruled duchies of Poznań, Gniezno, Kalisz and Ujście). The region came under the control of Władysław I the Elbow-High in 1314, and thus became part of the reunited Poland of which Władyslaw was crowned king in 1320.
In the reunited kingdom, and later in the Polish–Lithuanian Commonwealth, the country came to be divided into administrative units called voivodeships. In the case of the Greater Poland region these were Poznań Voivodeship and Kalisz Voivodeship. The Commonwealth also had larger subdivisions known as "prowincja", one of which was named Greater Poland. However, this "prowincja" covered a larger area than the Greater Poland region itself, also taking in Masovia and Royal Prussia. (This division of Crown Poland into two entities called Greater and Lesser Poland had its roots in the Statutes of Casimir the Great of 1346–1362, where the laws of "Greater Poland" – the northern part of the country – were codified in the Piotrków statute, with those of "Lesser Poland" in the separate Wiślica statute.)
In 1768 a new Gniezno Voivodeship was formed out of the northern part of Kalisz Voivodeship. However more far-reaching changes would come with the Partitions of Poland. In the first partition (1772), northern parts of Greater Poland along the Noteć (German "Netze") were taken over by Prussia, becoming the Netze District. In the second partition (1793) the whole of Greater Poland was absorbed by Prussia, becoming part of the province of South Prussia. It remained so in spite of the first Greater Poland Uprising (1794), part of the unsuccessful Kościuszko Uprising directed chiefly against Russia.
More successful was the Greater Poland Uprising of 1806, which led to the region's becoming part of the Napoleonic Duchy of Warsaw (forming the Poznań Department and parts of the Kalisz and Bydgoszcz Departments). However, following the Congress of Vienna in 1815, Greater Poland was again partitioned, with the western part (including Poznań) going to Prussia. The eastern part joined the Russian-controlled Kingdom of Poland, where it formed the Kalisz Voivodeship until 1837, then the Kalisz Governorate (merged into the Warsaw Governorate between 1844 and 1867).
Within the Prussian empire, western Greater Poland became the Grand Duchy of Posen (Poznań), which theoretically held some autonomy. Following an unrealized uprising in 1846, and the more substantial but still unsuccessful uprising of 1848 (during the Spring of Nations), the Grand Duchy was replaced by the Province of Posen. The authorities made efforts to Germanize the region, particularly after the founding of Germany in 1871, and from 1886 onwards the Prussian Settlement Commission was active in increasing German land ownership in formerly Polish areas.
Following the end of World War I, the Greater Poland Uprising (1918–1919) ensured that most of the region became part of the newly independent Polish state, forming most of Poznań Voivodeship (1921–1939). Northern and some western parts of Greater Poland remained in Germany, where they formed much of the province of Posen-West Prussia (1922–1938), whose capital was Schneidemühl (Piła).
Following the German invasion of 1939, Greater Poland was incorporated into Nazi Germany, becoming the province called Reichsgau Posen, later Reichsgau Wartheland ("Warthe" being the German name for the Warta river). The Polish population was oppressed, with many former officials and others considered potential enemies by the Nazis being imprisoned or executed, including at the notorious Fort VII concentration camp in Poznań. Poznań was declared a stronghold city "(Festung)" in the closing stages of the war, being taken by the Red Army in the Battle of Poznań, which ended on 22 February 1945.
After the war, Greater Poland was fully within the Polish People's Republic, as Poznań Voivodeship. With the reforms of 1975 this was divided into smaller provinces (the voivodeships of Kalisz, Konin, Leszno and Piła, and a smaller Poznań Voivodeship). The present-day Greater Poland Voivodeship, again with Poznań as its capital, was created in 1999.
Cities and towns.
The voivodeship contains 109 cities and towns. These are listed below in descending order of population (according to official figures for 2006 ):
Geography.
Topography.
The relief of Greater Poland, geological conditions and soil have been shaped by two glaciations:
The highest elevation is Greater Kobyla Mountain (284 m) in the Ostrzeszowski Hills, the lowest area is located in the valley of the Warta River at the mouth of its tributary the Noteć (21 m) in the north-western part of the region. Agriculturally fertile soils account for around 60% of the province's area, while 20%, the rest of the non-forested or urban areas, is mostly wetland soil (muck-peat and alluvial soils).
An area of approximately 800 thousand hectares is covered by forests, this represents around 25.8% of the total surface area of the region.
In the lake districts of the northern and central parts of the province there are about 800 lakes; 58% of which cover an area of at least 10 hectares and 8%, with an area exceeding 100 hectares. The largest reservoir is the natural Greater Powidzkie Lake (1036 ha) in the Gniezno Lake District.
Wielkopolska Region lies within the basin of the Oder River, 88% of the province's surface water drains into the Warta river basin, and the remaining 12% is drained by a multitude of other river systems, including the Barycz, Ladislaus Trench and Obrzycy waterways. The quality of river waters is generally poor, but their condition is gradually improving and should soon be classed as 'clean'.
Geology.
The main mineral energy resources in Greater Poland are lignite, natural gas, oil and peat.
Brown coal deposits are currently mined in the Konin area, and form the basis for the province's power industry (the Pątnów-Adams-Konin coal-fired power stations account for more than 10% of the national electricity production). The region also has significant quantities of peat deposits; it is calculated that there are ca. 886 thousand hectares of land covered with an average thickness of 1.5 m of peat. An abundance of raw materials used in the production of numerous medicines was recently discovered in the muds of Błażejewo, Oderbank and Mechnacz. In addition, very large deposits of brown coal have been discovered in the vicinity of Kościan, these however are not currently being extracted and probably never will be extracted, due to the expense that would be incurred in adapting the site to build a coal mine and the need to resettle thousands of people.
Rock salt is mined intensively at a salt mine in Kłodawa (this mine alone accounts for about 20% of domestic production). 
Throughout the province there are significant deposits of aggregates, gypsum, ceramic materials, and lacustrine chalk. In Kościan the largest and most modern, a natural gas production site is in operation. It supplies raw material for Kościańska Zieme, and Zielona Gora CHP. It is estimated that at the rate local gas reserves are being exploited, the reserves in Kościan will be enough for about 20 years of operation, thus practically allowing for local independence against the effects of gas crises.
Climate.
Wielkopolska is influenced by oceanic air masses that affect the mildness of the climate. The farther east one travels the more distinctly continental the climate becomes. The area is situated in the Silesian Greater Poland agro-climatic region where the average annual temperature is about 8.2 °C, and in the north drops to around 7.6 °C. It is slightly warmer in the south and west where the average temperature is usually about 8.5 °C. The number of days with snow can reach up to 57 days in and around the Kalisz district.
The growing season is one of the longest in Poland. On the province's southern plains this season constitutes around 228 days, while north of Gniezno and Szamotuły this gradually declines to 216 days.
Precipitation ranges from 500 to 550 mm. Despite this the region is still faced with a deficit in rainfall, particularly in the eastern part of the province (around Słupcy, Kazimierz Biskupi, Kleczew) where sometimes experience only 450 mm of rainfall per year, this threatens steppization of the region. Throughout the province there is typically a prevailing westerly wind.
Transportation.
Greater Poland is a major transport hub within Poland, a great deal of traffic from Russia and other states of the former Soviet Union pass through Poznań and Konin to reach Germany and other EU member states. To the south runs the international route from Gdańsk via Poznań and Leszno to Prague and then to the south of Europe. There is also a major highway being built in the province, the A2 motorway, which when completed will run from the western border of Poland with Germany, through Poznań to Warsaw and then onwards via Belarus to Moscow.
The main railway hubs located in Greater Poland are Poznań, Piła and Ostrów Wielkopolski. PKP Intercity operate a number of trains a day between Warsaw and Berlin which provide a fast connection for the two cities also to Poznań. This route was the first in Poland, adapted for use by the European high-speed transportation system.
In the near future the government expects to construct a high-speed rail line in the shape of a Y connecting Kalisz and Poznań from Łódź, Warsaw and Wrocław.
Poznań is the port of arrival for most international travellers as it is plays host to Ławica International Airport which has recently seen the second highest passenger growth rate in the country.
Politics.
The Greater Poland voivodeship's government is headed by the province's voivode "(governor)" who is appointed by the Polish Prime Minister. The voivode is then assisted in performing his duties by the voivodeship's marshal, who is the appointed speaker for the voivodeship's executive and is elected by the sejmik "(provincial assembly)". The current voivode of Greater Poland is Piotr Florek, whilst the present marshal is Marek Woźniak.
The Sejmik of Greater Poland consists of 39 members.
Administrative division.
Greater Poland Voivodeship is divided into 35 counties (powiats): 4 city counties and 31 land counties. These are further divided into 226 gminas.
The counties are listed in the following table (ordering within categories is by decreasing population).
Protected areas.
Protected areas in Greater Poland Voivodeship include two National Parks and 12 Landscape Parks. These are listed below.

</doc>
<doc id="41500" url="http://en.wikipedia.org/wiki?curid=41500" title="Penetration">
Penetration

Penetration may refer to:
In popular culture

</doc>
<doc id="41501" url="http://en.wikipedia.org/wiki?curid=41501" title="Performance management">
Performance management

Performance management (PM) includes activities which ensure that goals are consistently being met in an effective and efficient manner. Performance management can focus on the performance of an organization, a department, employee, or even the processes to build a product or service, as well as many other areas.
PM is also known as a process by which organizations align their resources, systems and employees to strategic objectives and priorities.
Application.
This is used most often in the workplace, can apply wherever people interact — schools, churches, community meetings, sports teams, health setting, governmental agencies,social events and even political settings - anywhere in the world people interact with their environments to produce desired effects. Armstrong and Baron (1998) defined it as a “strategic and integrated approach to increase the effectiveness of companies by improving the performance of the people who work in them and by developing the capabilities of teams and individual contributors.”
It may be possible to get all employees to reconcile personal goals with organizational goals and increase productivity and profitability of an organization using this process. It can be applied by organizations or a single department or section inside an organization, as well as an individual person. The performance process is appropriately named the self-propelled performance process (SPPP).
First, a commitment analysis must be done where a job mission statement is drawn up for each job. The job mission statement is a job definition in terms of purpose, customers, product and scope. The aim with this analysis is to determine the continuous key objectives and performance standards for each job position.
Following the commitment analysis is the work analysis of a particular job in terms of the reporting structure and job description. If a job description is not available, then a systems analysis can be done to draw up a job description. The aim with this analysis is to determine the continuous critical objectives and performance standards for each job.
Benefits.
Managing employee or system performance and aligning their objectives facilitates the effective delivery of strategic and operational goals. Some proponents argue that there is a clear and immediate correlation between using performance management programs or software and improved business and organizational results. In the public sector, the effects of performance management systems have differed from positive to negative, suggesting that differences in the characteristics of performance management systems and the contexts into which they are implemented play an important role to the success or failure of performance management.
For employee performance management, using integrated software, rather than a spreadsheet based recording system, may deliver a significant return on investment through a range of direct and indirect sales benefits, operational efficiency benefits and by unlocking the latent potential in every employees work day (i.e. the time they spend not actually doing their job). Benefits may include:
Organizational development.
In organizational development (OD), "performance" can be thought of as Actual Results vs Desired Results. Any discrepancy, where Actual is less than Desired, could constitute the performance improvement zone. Performance management and improvement can be thought of as a cycle:
A performance problem is any gap between Desired Results and Actual Results. Performance improvement is any effort targeted at closing the gap between Actual Results and Desired Results.
Other organizational development definitions are slightly different. The U.S. Office of Personnel Management (OPM) indicates that Performance Management consists of a system or process whereby:
Performance Management in Companies.
Many people equate performance management with performance appraisal. This is a common misconception. Performance management is the term used to refer to activities, tools, processes, and programs that companies create or apply to manage the performance of individual employees, teams, departments, and other organizational units within their organizational influence. In contrast, performance appraisal refers to the act of appraising or evaluating performance during a given performance period to determine how well an employee, a vendor or an organizational unit has performed relative to agreed objectives or goals, and this is only one of many important activities within the overall concept of performance management.
At the workplace, performance management is implemented by employees with supervisory roles. Normally, the goal of managing performance is to allow individual employees to find out how well they had performed relative to performance targets or key performance indicators during a specific performance period from their supervisors and managers. 
Organizations and companies typically manage employee performance over a formal 12-month period (otherwise known as the formal company performance period).
The results of performance management exercises are used:

</doc>
<doc id="41502" url="http://en.wikipedia.org/wiki?curid=41502" title="Performance measurement period">
Performance measurement period

In telecommunication, performance measurement period is the period during which performance parameters are measured. 
A performance measurement period is determined by required confidence limits and may vary as a function of the observed parameter values. User time is divided into consecutive performance measurement periods to enable measurement of user information transfer reliability.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41503" url="http://en.wikipedia.org/wiki?curid=41503" title="Periscope antenna">
Periscope antenna

In telecommunication, a periscope antenna is an antenna configuration in which the transmitting antenna is oriented to produce a vertical radiation pattern, and a flat or off-axis parabolic reflector, mounted above the transmitting antenna, is used to direct the beam in a horizontal path toward the receiving antenna. 
A periscope antenna facilitates increased terrain clearance without long transmission lines, while permitting the active equipment to be located at or near ground level for ease of maintenance.

</doc>
<doc id="41506" url="http://en.wikipedia.org/wiki?curid=41506" title="Personal mobility">
Personal mobility

In Universal Personal Telecommunications (UPT), personal mobility is the ability of a user to access telecommunication services at any UPT terminal on the basis of a personal identifier, and the capability of the network to provide those services in accord with the user's service profile. 
Personal mobility involves the network's capability to locate the terminal associated with the user for the purposes of addressing, routing, and charging the user for calls. "Access" is intended to convey the concepts of both originating and terminating services. Management of the service profile by the user is not part of personal mobility. The personal mobility aspects of personal communications are based on the UPT number.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41507" url="http://en.wikipedia.org/wiki?curid=41507" title="Phantom circuit">
Phantom circuit

In telecommunication and electrical engineering, a phantom circuit is an electrical circuit derived from suitably arranged wires with one or more conductive paths being a circuit in itself and at the same time acting as one conductor of another circuit. 
Phantom group.
A phantom group is composed of three circuits that are derived from two single-channel circuits to form a "phantom circuit". Here the phantom circuit is a third circuit derived from two suitably arranged pairs of wires, called side circuits, with each pair of wires being a circuit in itself and at the same time acting as one conductor of the third circuit. The "side circuits" within phantom circuits can be coupled to their respective voltage drops by center-tapped transformers, usually called "repeating coils". The center taps are on the line side of the side circuits. Current from the phantom circuit is split evenly by the center taps. This cancels crosstalk from the phantom circuit to the side circuits. 
Phantom working increased the number of circuits on long distance routes in the early 20th century without putting up more wires. Phantoming declined with the adoption of carrier systems. 
It is theoretically possible to create a phantom circuit from two other phantom circuits and so on up in a pyramid with a maximum 2n-1 circuits being derived from n original circuits. However, more than one level of phantoming is usually impractical. Isolation between the phantom circuit and the side circuits relies on accurate balance of the line and transformers. Imperfect balance results in crosstalk between the phantom and side circuits and this effect accumulates as each level of phantoms is added. Even small levels of crosstalk are unacceptable on analogue telecommunications circuits since speech crosstalk is still intelligible down to quite low levels.
Phantom microphone powering.
Condenser microphones have impedance converter (current amplifier) circuitry that requires powering; in addition, the capsule of any non-electret, non-RF condenser microphone requires a polarizing voltage to be applied. Since the mid- to late 1960s most balanced, professional condenser microphones for recording and broadcast have used phantom powering. It can be provided by outboard AC or battery supplies, but nowadays is most often built in to the mixing console, recorder or microphone preamplifier to which the microphones are connected.
By far the most common circuit uses +48 VDC fed through a matched pair of 6.8 kOhm resistors for each input channel. This arrangement has been standardized by the IEC and ISO, along with a less-commonly-used arrangement with +12 VDC and 680 Ohm feed resistors.
As a practical matter, phantom powering allows the same two-conductor shielded cables to be used for both dynamic microphones and condenser microphones, while being harmless to balanced microphones that aren't designed to consume it, since the circuit balance prevents any substantial DC from flowing through the output circuit of those microphones.
DC phantom.
Simple DC signalling can be achieved on a telecommunications line in a similar way to phantom powering of microphones. A switch connected to the transformer centre-tap at one end of the line can operate a similarly connected relay at the other end. The return path is through the ground connection. This arrangement can be used for remotely controlling equipment.
Carrier circuit phantoms.
From the 1950s to around the 1980s, using phantoms on star-quad trunk carrier circuits was a popular method of deriving a high quality broadcast audio circuit. The multiplexed FDM telecommunications carrier system usually did not use the baseband of the cable because it was inconvenient to separate low frequencies with filters. On the other hand, a one-way audio phantom could be formed from the two pairs (go and return signals) making up the star-quad cable.
Unloaded phantom.
Unloaded phantom is a phantom configuration of loaded lines (a circuit fitted with loading coils). The idea here is not to create additional circuits. Rather, the purpose is to cancel or greatly reduce the effect of the loading coils fitted to a line. The reason for doing this is that loaded lines have a definite cut-off frequency and it may be desired to equalise the line to a frequency which is higher than this, for example to make a circuit suitable for use by a broadcaster. Ideally, the loading would be removed or reduced for a permanent connection, but this is not feasible for temporary arrangements such as a requirement for outside broadcast. Instead, two circuits in a phantom configuration can be used to greatly reduce the inductance being inserted by the loading coils, and hence the loading effect.
It works because the loading coils used on balanced lines have two windings, one for each leg of the circuit. They are both wound on a common core and the windings are so arranged that the magnetic flux induced by both of them is in the same direction. Both windings induce an emf in each other as well as their own self-induction. This effect greatly increases the inductance of the coil and hence its loading effectiveness. By contrast, when the circuit is in the phantom configuration the currents in the two wires of each pair are in the same direction and the magnetic flux is being cancelled. This has precisely the opposite effect and the inductance is greatly reduced.
This configuration is most commonly used on the two pairs of a star-quad cable. It is not so successful with other pairs of wires. The difference in the path of the two pairs can easily destroy the balance and results in crosstalk and interference.
This configuration can also be called "bunched pairs". However, "bunched pairs" can also refer to the straightforward connection of two lines in parallel which is not a phantom circuit and will not reduce the loading.

</doc>
<doc id="41508" url="http://en.wikipedia.org/wiki?curid=41508" title="Phase angle">
Phase angle

In the context of phasors, phase angle refers to the angular component of the complex number representation of the function. The notation formula_1  for a vector with magnitude (or "amplitude") "A" and phase angle θ, is called angle notation. This notation is frequently used to represent an electrical impedance. In this case the phase angle is the phase difference between the voltage applied to the impedance and the current driven through it.
In the context of periodic phenomena, such as a wave, "phase angle" is synonymous with phase.

</doc>
<doc id="41509" url="http://en.wikipedia.org/wiki?curid=41509" title="Phased array">
Phased array

In antenna theory, a phased array is an array of antennas in which the relative phases of the respective signals feeding the antennas are set in such a way that the effective radiation pattern of the array is reinforced in a desired direction and suppressed in undesired directions. The phase relationships among the antennas may be fixed, as is usual in a tower array, or may be adjustable, as for beam steering.
History.
Phased array transmission was originally shown in 1905 by Nobel laureate Karl Ferdinand Braun who demonstrated enhanced transmission of radio waves in one direction. During World War II, Nobel laureate Luis Alvarez used phased array transmission in a rapidly steerable radar system for "ground-controlled approach", a system to aid in the landing of aircraft. At the same time, the GEMA in Germany built the PESA
Mammut 1. It was later adapted for radio astronomy leading to Nobel Prizes for Physics for Antony Hewish and Martin Ryle after several large phased arrays were developed at the University of Cambridge. This design is also used for radar, and is generalized in interferometric radio antennas. In 2007, DARPA researchers announced a 16 element phased array radar antenna which was also integrated with all the necessary circuits on a single silicon chip and operated at 30–50 GHz.
The relative amplitudes of—and constructive and destructive interference effects among—the signals radiated by the individual antennas determine the effective radiation pattern of the array. A phased array may be used to point a fixed radiation pattern, or to scan rapidly in azimuth or elevation. Simultaneous electrical scanning in both azimuth and elevation was first demonstrated in a phased array antenna at Hughes Aircraft Company, Culver City, California in 1957.
Phased arrays are used in optical communication as a wavelength-selective splitter. 
For information about active as well as passive phased array radars, see also active electronically scanned array.
Broadcasting.
In broadcast engineering, it is required that phased arrays be used by many AM broadcast radio stations to enhance signal strength and therefore coverage in the city of license, while minimizing interference to other areas. Due to the differences between daytime and nighttime ionospheric propagation at mediumwave frequencies, it is common for AM broadcast stations to change between day (groundwave) and night (skywave) radiation patterns by switching the phase and power levels supplied to the individual antenna elements (mast radiators) daily at sunrise and sunset. For shortwave broadcasts many stations use arrays of horizontal dipoles. A common arrangement uses 16 dipoles in a 4x4 array. Usually this is in front of a wire grid reflector. The phasing is often switchable to allow Beam steering in azimuth and sometimes elevation. 
More modest phased array longwire antenna systems may be employed by private radio enthusiasts to receive longwave, mediumwave (AM) and shortwave radio broadcasts from great distances.
On VHF, phased arrays are used extensively for FM broadcasting. These greatly increase the antenna gain, magnifying the emitted RF energy toward the horizon, which in turn greatly increases a station's broadcast range. In these situations, the distance to each element from the transmitter is identical, or is one (or other integer) wavelength apart. Phasing the array such that the lower elements are slightly delayed (by making the distance to them longer) causes a downward beam tilt, which is very useful if the antenna is quite high on a radio tower.
Other phasing adjustments can increase the downward radiation in the far field without tilting the main lobe, creating null fill to compensate for extremely high mountaintop locations, or decrease it in the near field, to prevent excessive exposure to those workers or even nearby homeowners on the ground. The latter effect is also achieved by half-wave spacing – inserting additional elements halfway between existing elements with full-wave spacing. This phasing achieves roughly the same horizontal gain as the full-wave spacing; that is, a five-element full-wave-spaced array equals a nine- or ten-element half-wave-spaced array.
Radar.
Phased array radar systems are also used by warships of many navies. Because of the rapidity with which the beam can be steered, phased array radars allow a warship to use one radar system for surface detection and tracking (finding ships), air detection and tracking (finding aircraft and missiles) and missile uplink capabilities. Before using these systems, each surface-to-air missile in flight required a dedicated fire-control radar, which meant that ships could only engage a small number of simultaneous targets. Phased array systems can be used to control missiles during the mid-course phase of the missile's flight. During the terminal portion of the flight, continuous-wave fire control directors provide the final guidance to the target. Because the radar beam is electronically steered, phased array systems can direct radar beams fast enough to maintain a fire control quality track on many targets simultaneously while also controlling several in-flight missiles. 
The AN/SPY-1 phased array radar, part of the Aegis combat system deployed on modern U.S. cruisers and destroyers, "is able to perform search, track and missile guidance functions simultaneously with a capability of over 100 targets." Likewise, the Thales Herakles phased array multi-function radar used in service with France and Singapore has a track capacity of 200 targets and is able to achieve automatic target detection, confirmation and track initiation in a single scan, while simultaneously providing mid-course guidance updates to the MBDA Aster missiles launched from the ship. The German Navy and the Royal Dutch Navy have developed the Active Phased Array Radar System (APAR). The MIM-104 Patriot and other ground-based antiaircraft systems use phased array radar for similar benefits. 
Phased arrays are used in naval sonar, in active (transmit and receive) and passive (receive only) and hull-mounted and towed array sonar.
Space probe communication.
The MESSENGER spacecraft was a space probe mission to the planet Mercury (2011–2015
). This was the first deep-space mission to use a phased-array antenna for communications. The radiating elements are linearly-polarized, slotted waveguides. The antenna, which uses the X band, used 26 radiative elements and can gracefully degrade. 
Weather research usage.
The National Severe Storms Laboratory has been using a SPY-1A phased array antenna, provided by the US Navy, for weather research at its Norman, Oklahoma facility since April 23, 2003. It is hoped that research will lead to a better understanding of thunderstorms and tornadoes, eventually leading to increased warning times and enhanced prediction of tornadoes. Current project participants include the National Severe Storms Laboratory and National Weather Service Radar Operations Center, Lockheed Martin, United States Navy, University of Oklahoma School of Meteorology, School of Electrical and Computer Engineering, and Atmospheric Radar Research Center, Oklahoma State Regents for Higher Education, the Federal Aviation Administration, and . The project includes research and development, future technology transfer and potential deployment of the system throughout the United States. It is expected to take 10 to 15 years to complete and initial construction was approximately $25 million.
Optics.
Within the visible or infrared spectrum of electromagnetic waves it is possible to construct optical phased arrays. They are used in wavelength multiplexers and filters for telecommunication purposes, laser beam steering, and holography. Synthetic array heterodyne detection is an efficient method for multiplexing an entire phased array onto a single element photodetector.
Radio-frequency identification (RFID).
By 2014, phased array antennas were integrated into RFID systems to increase the area of coverage of a single system by 100% to 76200 sqm while still using traditional passive UHF tags.
Human-machine interfaces (HMI).
A phased array of acoustic transducers, denominated airborne ultrasound tactile display (AUTD), was developed at the University of Tokyo's Shinoda Lab to induce tactile feedback. This system was demonstrated to enable a user to interactively manipulate virtual holographic objects.
Mathematical perspective and formulas.
A phased array is an example of "N"-slit diffraction. It may also be viewed as the coherent addition of "N" line sources. Since each individual antenna acts as a slit, emitting radio waves, their diffraction pattern can be calculated by adding the phase shift φ to the fringing term.
We will begin from the "N"-slit diffraction pattern derived on the diffraction formalism page.
Now, adding a φ term to the formula_2 fringe effect in the second term yields:
Taking the square of the wave function gives us the intensity of the wave.
Now space the emitters a distance formula_6 apart. This distance is chosen for simplicity of calculation but can be adjusted as any scalar fraction of the wavelength.
As sine achieves its maximum at formula_8, we set the numerator of the second term = 1.
Thus as "N" gets large, the term will be dominated by the formula_12 term. As sine can oscillate between −1 and 1, we can see that setting formula_13 will send the maximum energy on an angle given by 
Additionally, we can see that if we wish to adjust the angle at which the maximum energy is emitted, we need only to adjust the phase shift φ between successive antennas. Indeed the phase shift corresponds to the negative angle of maximum signal.
A similar calculation will show that the denominator is minimized by the same factor.
Different types of phased arrays.
There are two main types of beamformers. These are time domain beamformers and frequency domain beamformers.
A graduated attenuation window is sometimes applied across the face of the array to improve side-lobe suppression performance, in addition to the phase shift.
Time domain beamformer works by introducing time delays. The basic operation is called "delay and sum". It delays the incoming signal from each array element by a certain amount of time, and then adds them together. The most common kind of time domain beam former is serpentine waveguide. Active phase array uses individual delay lines that are switched on and off. Yttrium iron garnet phase shifters vary the phase delay using the strength of a magnetic field.
There are two different types of frequency domain beamformers.
The first type separates the different frequency components that are present in the received signal into multiple frequency bins (using either an Discrete Fourier transform (DFT) or a filterbank). When different delay and sum beamformers are applied to each frequency bin, the result is that the main lobe simultaneously points in multiple different directions at each of the different frequencies. This can be an advantage for communication links, and is used with the SPS-48 radar.
The other type of frequency domain beamformer makes use of Spatial Frequency. Discrete samples are taken from each of the individual array elements. The samples are processed using a Discrete Fourier Transform (DFT). The DFT introduces multiple different discrete phase shifts during processing. The outputs of the DFT are individual channels that correspond with evenly spaced beams formed simultaneously. A 1-dimensional DFT produces a fan of different beams. A 2-dimensional DFT produces beams with a pineapple configuration.
These techniques are used to create two kinds of phase array.
There are two further sub-categories that modify the kind of dynamic array or fixed array.
Dynamic Phased Array.
Each array element incorporates an adjustable phase shifter that are collectively used to move the beam with respect to the array face.
Dynamic phase array require no physical movement to aim the beam. The beam is moved electronically. This can produce antenna motion fast enough to use a small pencil-beam to simultaneously track multiple targets while searching for new targets using just one radar set (track while search).
The position of mechanically steered antennas can be predicted, which can be used to create electronic countermeasures that interfere with radar operation. The flexibility resulting from phase array operation allows beams to be aimed at random locations, which eliminates this vulnerability. This is also desirable for military applications.
Fixed Phase Array.
Fixed phase array antennas are typically used to create an antenna with a more desirable form factor than the conventional parabolic reflector or cassegrain reflector. Fixed phased array incorporate fixed phase shifters. For example, most commercial FM Radio and TV antenna towers use a collinear antenna array, which is a fixed phased array of dipole elements.
In radar applications, this kind of phase array is physically moved during the track and scan process. There are two configurations.
The SPS-48 radar uses multiple transmit frequencies with a serpentine delay line along the left side of the array to produce vertical fan of stacked beams. Each frequency experiences a different phase shift as it propagates down the serpentine delay line, which forms different beams. A filter bank is used to split apart the individual receive beams. The antenna is mechanically rotated.
Semi-active radar homing uses monopulse radar that relies on a fixed phase array to produce multiple adjacent beams that measure angle errors. This form factor is suitable for gimbal mounting in missile seekers.
Active Phase Array.
Active phased arrays (AESA) elements incorporate transmit amplification with phase shift in each antenna element (or group of elements). Each element also includes receive pre-amplification. The phase shifter setting is the same for transmit and receive.
Active phase array do not require phase reset after the end of the transmit pulse, which is compatible with Doppler radar and Pulse-Doppler radar.
Passive Phase Array.
Passive phased arrays typically use large amplifiers that produce all of the microwave transmit signal for the antenna. Phase shifters typically consist of waveguide elements that contain phase shifters controlled by magnetic field, voltage gradient, or equivalent technology.
The phase shift process used with passive phase array typically puts the receive beam and transmit beam into diagonally opposite quadrants. The sign of the phase shift must be inverted after the transmit pulse is finished and before the receive period begins to place the receive beam into the same location as the transmit beam. That requires a phase impulse that degrades sub-clutter visibility performance on Doppler radar and Pulse-Doppler radar. As an example, Yttrium iron garnet phase shifters must be changed after transmit pulse quench and before receiver processing starts to align transmit and receive beams. That impulse introduces FM noise that degrades clutter performance.
Passive phase array is used with AEGIS.

</doc>
<doc id="41510" url="http://en.wikipedia.org/wiki?curid=41510" title="Phase distortion">
Phase distortion

In signal processing, phase distortion or phase-frequency distortion is distortion that occurs when (a) a filter's phase response is not linear over the frequency range of interest, that is, the phase shift introduced by a circuit or device is not directly proportional to frequency, or (b) the zero-frequency intercept of the phase-frequency characteristic is not 0 or an integral multiple of 2π radians.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41511" url="http://en.wikipedia.org/wiki?curid=41511" title="Phase inversion">
Phase inversion

Phase inversion is the swapping of the two poles of an alternating current source. A phase inversion is neither a time shift nor a phase shift, but simply a swap of plus and minus.
For example, in a push-pull power amplifier using vacuum tubes, the signal is most often "split" by a phase splitter (aka phase inverter) stage which produces two signals, one "in phase", and the other "out of phase", that is, phase inverted. These two signals then drive the two halves of the first push-pull stage, which may be either the output stage (in which case the phase splitter will be in between the driver stage if there is one and the output stage) or the driver stage. The other common arrangements for driving a push-pull stage are by using an isolation transformer to produce the split signals, or by using the in-phase half of the first push-pull stage to drive the other half. A common circuit using this last technique is the long-tailed pair, often seen in television sets and oscilloscopes. 
In solid state electronics all of these techniques can be used, and phase inversion can also be produced by the use of NPN/PNP complementary circuitry, which has no corresponding technique in vacuum tube designs. 
Phase inversion may occur with a random or periodic, symmetrical or non-symmetrical waveform, although it is usually produced by the inversion of a symmetrical periodic signal, resulting in a change in sign. A symmetrical periodic signal represented by "f" ("t" ) = "A" ej"ωt", after phase inversion, becomes "f" 1("t" ) = Aej(ω"t" +π), where "t" is time, "A" is the magnitude of the vector, ω is angular frequency (ω = 2π"f" ), where "f" is the frequency and π ≈3.1416 and e ≈ 2.7183. The algebraic sum of "f" ("t" ) and "f" 1("t" ) will always be zero.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41512" url="http://en.wikipedia.org/wiki?curid=41512" title="Voivodeships of Poland">
Voivodeships of Poland

A województwo ; plural: "województwa") is the highest-level administrative subdivision of Poland, corresponding to a "province" in many other countries. The term "województwo" has been in use since the 14th century, and is commonly translated in English as "province". The word "województwo" is also rendered as "voivodeship" or a variant spelling.
The Polish local government reforms adopted in 1998, which went into effect on 1 January 1999, created sixteen new voivodeships. These replaced the 49 former voivodeships that had existed from 1 July 1975.
Today's voivodeships are mostly named after historical and geographical regions, while those prior to 1998 generally took their names from the cities on which they were centered. The new units range in area from under 10000 km2 (Opole Voivodeship) to over 35000 km2 (Masovian Voivodeship), and in population from one million (Lubusz Voivodeship) to over five million (Masovian Voivodeship).
Administrative authority at voivodeship level is shared between a government-appointed governor called a voivode (Polish "wojewoda"), an elected assembly called a sejmik, and an executive chosen by that assembly. The leader of that executive is called the "marszałek województwa" (voivodeship marshal). Voivodeships are further divided into powiats (counties) and gminas (communes or municipalities): see Administrative divisions of Poland.
Voivodeships since 1999.
Administrative powers.
Competences and powers at voivodeship level are shared between the voivode (governor), the sejmik (regional assembly) and the executive. In most cases these institutions are all based in one city, but in Kuyavian-Pomeranian and Lubusz Voivodeship the voivode's offices are in a different city from those of the executive and the sejmik. Voivodeship capitals are listed in the table below.
The voivode is appointed by the Prime Minister and is the regional representative of the central government. The voivode acts as the head of central government institutions at regional level (such as the police and fire services, passport offices, and various inspectorates), manages central government property in the region, oversees the functioning of local government, coordinates actions in the field of public safety and environment protection, and exercises special powers in emergencies. The voivode's offices collectively are known as the "urząd wojewódzki".
The sejmik is elected every four years, at the same time as the local authorities at powiat and gmina level. It passes bylaws, including the voivodeship's development strategies and budget. It also elects the "marszałek" and other members of the executive, and holds them to account.
The executive ("zarząd województwa"), headed by the "marszałek", drafts the budget and development strategies, implements the resolutions of the sejmik, manages the voivodeship's property, and deals with many aspects of regional policy, including management of European Union funding. Its offices collectively are known as the "urząd marszałkowski".
Former voivodeships.
Poland's voivodeships 1975–1998.
Administrative division of Poland between 1979 and 1998 included 49 voivodeships upheld after the establishment of the Third Polish Republic in 1989 for another decade. This reorganization of administrative division of Poland was mainly a result of local government reform acts of 1973–1975. In place of the three-level administrative division (voivodeship, county, commune), a new two-level administrative division was introduced (49 small voivodeships, and communes). The three smallest voivodeships – Warsaw, Kraków and Łódź – had the special status of municipal voivodeship; the city president (mayor) was also provincial governor.
Poland's voivodeships 1945–1975.
After World War II, the new administrative division of the country within the new national borders was based on the prewar one and included 14 (+2) voivodeships, then 17 (+5). The voivodeships in the east that had not been annexed by the Soviet Union had their borders left almost unchanged. The newly acquired territories in the west and north were organized into the new voivodeships of Szczecin, Wrocław and Olsztyn, and partly joined to Gdańsk, Katowice and Poznań voivodeships. Two cities were granted voivodeship status: Warsaw and Łódź.
In 1950, new voivodeships were created: Koszalin (previously part of Szczecin), Opole (previously part of Katowice), and Zielona Góra (previously part of Poznań, Wrocław and Szczecin voivodeships).
In 1957, three more cities were granted voivodeship status: Wrocław, Kraków and Poznań.
Poland's voivodeships 1921–1939.
The administrative division of Poland in the interwar period included 16 voivodeships and Warsaw (with voivodeship rights).
They were very similar to the current voivodeships.
Congress Poland 1816–1837.
From 1816 to 1837 there were 8 voivodeships in Congress Poland.
Etymology and use of "voivodeship".
Some English-language sources, in historic contexts, speak of "palatinates" rather than "voivodeships"; the former term traces back to the Latin "palatinus" ("palatine"). More commonly used now is "voivodeship", a loanword-calque hybrid formed on the Polish "województwo". Other sources refer instead to "provinces" (Polish singular: ""prowincja"), though in pre-1795 contexts this may be confusing because the cognate Polish "prowincyja" (as it was then spelled) was idiosyncratically applied, until the last of the three Partitions of the Polish–Lithuanian Commonwealth, in 1795, to each of the three main Regions (Greater Poland, Lesser Poland, and Lithuania) of the Polish–Lithuanian Commonwealth, each of those Regions in turn comprising a number of "województwa" (plural of "województwo").
The Polish "województwo", designating a second-tier Polish or Polish–Lithuanian administrative unit, derives from "wojewoda"" (etymologically, a "war leader" or "leader of warriors", but now simply the governor of a "województwo") and the suffix ""-stwo"" (a "state or condition").
The English "voivodeship", which is a hybrid of the loanword "voivode" and "-ship" (the latter a suffix, likewise meaning a "state or condition", that calques the Polish "-stwo"), has never been much used and is absent from many dictionaries. According to the "Oxford English Dictionary", it first appeared in 1792, spelled "woiwodship", in the sense of "the district or province governed by a voivode." The word subsequently also appeared in 1886 in the sense of "the office or dignity of a voivode."
An official Polish body, the Commission on Standardization of Geographic Names outside the Republic of Poland, recommends the spelling "voivodship", without the "e". This is consistently reflected in publications and in the international arena, e.g., at the United Nations.

</doc>
<doc id="41515" url="http://en.wikipedia.org/wiki?curid=41515" title="Cod">
Cod

Cod is the common name for the genus "Gadus" of demersal fishes, belonging to the family Gadidae. Cod is also used as part of the common name for a number of other fish species, and there are species suggested to belong to genus "Gadus" that are not called cod (the Alaska pollock).
The two most important species of cod are the Atlantic cod ("Gadus morhua"), which lives in the colder waters and deeper sea regions throughout the North Atlantic, and the Pacific cod ("Gadus macrocephalus"), found in both eastern and western regions of the northern Pacific. "Gadus morhua" was named by Linnaeus in 1758. (However, "G. morhua callarias", a low salinity, non-migratory race restricted to parts of the Baltic, was originally described as "Gadus callarias" by Linnaeus.)
Cod is popular as a food with a mild flavour and a dense, flaky white flesh. Cod livers are processed to make cod liver oil, an important source of vitamin A, vitamin D, vitamin E and omega-3 fatty acids (EPA and DHA). Young Atlantic cod or haddock prepared in strips for cooking is called scrod. In the United Kingdom, Atlantic cod is one of the most common ingredients in fish and chips, along with haddock and plaice.
Species.
At various times in the past, taxonomists included many species in the genus "Gadus". Most of these are now either classified in other genera, or have been recognized as simply forms of one of three species. All these species have a number of common names, most of them ending with the word "cod", whereas other species, as closely related, have other common names (such as pollock, haddock, etc.). On the other hand, many other, unrelated species also have common names ending with cod. The usage often changes with different localities and at different times.
Cod in the genus "Gadus".
There are three species in the "Gadus" genus currently called cod:
Related species.
"Cod" forms part of the common name of many other fish no longer classified in the genus "Gadus". Many are members of the family Gadidae; others are members of three related families within the order Gadiformes whose names include the word "cod": the morid cods, Moridae (100 or so species); the eel cods, Muraenolepididae (four species); and the Eucla cod, Euclichthyidae (one species). The tadpole cod family (Ranicipitidae) has now been placed in Gadidae.
Gadiformes include:
Some fish have common names derived from "cod", such as codling, codlet or tomcod. ("Codling" is also used as a name for a young cod.)
Other species.
Some fish commonly known as cod are unrelated to "Gadus". Part of this name confusion is market-driven. Severely shrunken Atlantic cod stocks have led to the marketing of cod replacements using culinary names of the form ""x" cod", according to culinary rather than phyletic similarity. The common names for the following species have become well established; note that all inhabit the Southern Hemisphere.
Fish of the order Perciformes that are commonly called "cod" include:
Almost all coral cod, reef cod or rock cod are also in order Perciformes. Most are better known as groupers, and belong to the family Serranidae. Others belong to the Nototheniidiae. Two exceptions are the Australasian red rock cod, which belongs to a different order (see below), and the fish known simply as the rock cod and as soft cod in New Zealand, "Lotella rhacina", which as noted above actually is related to the true cod (it is a morid cod).
From the order Scorpaeniformes:
The tadpole cod family, Ranicipitidae, and the Eucla cod family, Euclichthyidae, were formerly classified in the order Ophidiiformes, but are now grouped with the Gadiformes.
Marketed as cod.
Some fish that do not have "cod" in their names are sometimes sold as cod. Haddock and whiting belong in the same family, the Gadidae, as cod.
Characteristics.
Cods of the genus "Gadus" have three rounded dorsal and two anal fins. The pelvic fins are small, with the first ray extended, and are set under the gill cover (i.e. the throat region), in front of the pectoral fins. The upper jaw extends over the lower jaw, which has a well-developed chin barbel. The eyes are medium-sized, approximately the same as the length of the chin barbel. Cod have a distinct white lateral line running from the gill slit above the pectoral fin, to the base of the caudal or tail fin. The back tends to be a greenish to sandy brown, and shows extensive mottling, especially towards the lighter sides and white belly. Dark brown colouration of the back and sides is not uncommon, especially for individuals that have resided in rocky inshore regions.
The Atlantic cod can change colour at certain water depths. It has two distinct colour phases: gray-green and reddish brown. Its average weight is 5 -, but specimens weighing up to 100 kg have been recorded. Pacific cod are smaller than Atlantic cod and are darker in colour.
Distribution.
Atlantic cod ("Gadus morhua") live in the colder waters and deeper sea regions throughout the North Atlantic. Pacific cod ("Gadus macrocephalus") is found in both eastern and western regions of the Pacific.
Atlantic cod divide into several stocks, including the Arcto-Norwegian, North Sea, Faroe, Iceland, East Greenland, West Greenland, Newfoundland, and Labrador stocks. There seems to be little interchange between the stocks, although migrations to their individual breeding grounds may involve distances of 200 mior more.
Atlantic cod occupy varied habitat, favouring rough ground, especially inshore, and are demersal in depths between 20 and, 80 m on average, although not uncommonly to depths of 600 m. Off the Norwegian and New England coasts and on the Grand Banks of Newfoundland, cod congregate at certain seasons in water of 30 - depth. Cod are gregarious and form schools, although shoaling tends to be a feature of the spawning season.
Life cycle.
Spawning of northeastern Atlantic cod occurs between January and April (March and April are the peak months), at a depth of 200 m in specific spawning grounds at water temperatures between 4 and. Around the UK, the major spawning grounds are in the middle to southern North Sea, the start of the Bristol Channel (north of Newquay), the Irish Channel (both east and west of the Isle of Man), around Stornoway, and east of Helmsdale.
Prespawning courtship involves fin displays and male grunting, which leads to pairing. The male inverts himself beneath the female, and the pair swim in circles while spawning. The eggs are planktonic and hatch between eight and 23 days, with larva reaching 4 mm in length. This planktonic phase lasts some ten weeks, enabling the young cod to increase its body weight by 40-fold, and growing to about 2 cm. The young cod then move to the seabed and change their diet to small benthic crustaceans, such as isopods and small crabs. They increase in size to 8 cm in the first six months, 14 - by the end of their first year, and to 25 - by the end of the second. Growth tends to be less at higher latitudes. Cod reach maturity at about 50 cm at about 3 to 4 years of age.
Ecology.
Adult cod are active hunters, feeding on sand eels, whiting, haddock, small cod, squid, crabs, lobsters, mussels, worms, mackerel, and molluscs.
In the Baltic Sea the most important species are Atlantic cod, herring, and sprat. Many studies that analyze the stomach contents of these fish indicate that cod is the top predator, preying on the herring and sprat. Sprat form particularly high concentrations in the Bornholm Basin in the southern Baltic Sea. Although cod feed primarily on adult sprat, sprat tend to prey on the cod eggs and larvae.
Cod and related species are plagued by parasites. For example the cod worm, "Lernaeocera branchialis", starts life as a copepod, a small free-swimming crustacean larva. The first host used by cod worm is a flatfish or lumpsucker, which they capture with grasping hooks at the front of their body. They penetrate the lumpsucker with a thin filament which they use to suck its blood. The nourished cod worms then mate on the lumpsucker. The female worm, with her now fertilized eggs, then finds a cod, or a cod-like fish such as a haddock or whiting. There the worm clings to the gills while it metamorphoses into a plump, sinusoidal, wormlike body, with a coiled mass of egg strings at the rear. The front part of the worms body penetrates the body of the cod until it enters the rear bulb of the host's heart. There, firmly rooted in the cod's circulatory system, the front part of the parasite develops like the branches of a tree, reaching into the main artery. In this way, the worm extracts nutrients from the cod's blood, remaining safely tucked beneath the cod's gill cover until it releases a new generation of offspring into the water. 
Fisheries.
The 2006 northwest Atlantic cod quota is 23,000 tons, representing half the available stocks, while the northeast Atlantic quota is 473,000 tons. Pacific cod is currently enjoying strong global demand. The 2006 total allowable catch (TAC) for the Gulf of Alaska and Aleutian Islands was 260,000 tons.
Aquaculture.
Farming of Atlantic cod has received a significant amount of interest due to the overall trend of increasing cod prices alongside reduced wild catches. However, progress in creating large scale farming of cod has been slow, mainly due to bottlenecks in the larval production stage, where survival and growth are often unpredictable. It has been suggested that this bottleneck may be overcome by ensuring cod larvae are fed diets with similar nutritional content as the copepods they feed on in the wild Recent examples have shown that increasing dietary levels of minerals such as selenium, iodine and zinc may improve survival and/or biomarkers for health in aquaculture reared cod larvae.
As food.
Cod is popular as a food with a mild flavor and a dense, flaky white flesh. Cod livers are processed to make cod liver oil, an important source of vitamin A, vitamin D, vitamin E and omega-3 fatty acids (EPA and DHA).
Young Atlantic cod or haddock prepared in strips for cooking is called scrod. In the United Kingdom, Atlantic cod is one of the most common ingredients in fish and chips, along with haddock and plaice. Cod's soft liver can be tinned (canned) and eaten. Cod is mainly consumed in Portugal, Spain, Italy and Brazil. Cod flesh is white, moist, and flaky when cooked.
USDA data: Pacific cod; Atlantic cod
Management.
Following the early 1990s collapse of Canadian stocks, the Canadian Department of Fisheries and Oceans (DFO) banned fishing for northern (that is, cod to the north and east of the island of Newfoundland, in NAFO areas JKL) cod in 1992, which caused great economic hardship in Newfoundland and Labrador. The collapse was blamed on cold water, or seals, and it had even been suggested the cod were really still there; only rarely was overfishing acknowledged, or management's role in that.
The DFO partly lifted its ban in 1997, although the International Council for the Exploration of the Sea noted the poor recovery of Canadian cod stocks. In general, depleted populations of cod and other gadids appear to recover poorly when fishing pressure is reduced or stopped.
In 1998, the Committee on the Status of Endangered Wildlife in Canada (COSEWIC) listed the Atlantic cod as "vulnerable", a category subsequently rebranded as "special concern", though not as an endangered species. Dr. Kim Bell, who drafted the report for COSEWIC, subsequently stated the original report in fact "had" advised endangered status, but political pressure by the DFO within COSEWIC had resulted in a decision of "vulnerable".
In 2000, WWF placed cod on the endangered species list. The WWF issued a report stating the global cod catch had suffered a 70% drop over the last 30 years, and if this trend continued, the world’s cod stocks would disappear in 15 years. Åsmund Bjordal, director of the Norwegian Institute of Marine Research disputed the WWF's claim, noting the healthy Barents Sea cod population. Cod (known in Norway as skrei) is among Norway's most important fishery exports and the Barents Sea is Norway's most important cod fishery. In 2015, the Norwegian Seafood Council invited Crown Prince Haakon to take part in opening the year’s cod fishing season at Senja.
In 2003, under the new legislative framework of the Species At Risk Act [SARA], COSEWIC placed the Newfoundland and Labrador fisheries cod on the endangered species list and Fisheries Minister Robert Thibault announced an indefinite closure in the Gulf of St. Lawrence and off Newfoundland's northeast coast. In a 2004 report, the WWF agreed the Barents Sea cod fishery appeared to be healthy, but that the situation may not last due to illegal fishing, industrial development, and high quotas. In 2005, the WWF—Canada accused both foreign and Canadian fishing vessels of deliberate, large-scale violations of the restrictions on the Grand Banks, in the form of bycatch. WWF also claimed poor enforcement by NAFO, an intergovernmental organization with a mandate to provide scientific fishery advice and management in the northwestern Atlantic.
In 2006, the Norwegian Institute of Marine Research considered coastal cod (but not the North East Arctic cod) endangered, but has since reversed this assessment. In 2010, Greenpeace International added the Atlantic cod to its seafood red list. "The Greenpeace International seafood red list is a list of fish that are commonly sold in supermarkets around the world, and which have a very high risk of being sourced from unsustainable fisheries." According to Seafood Watch, cod is currently on the list of fish consumers should avoid. In the book , it is claimed cod is an example of how unsustainable fishing is destroying ocean ecosystems.
In a letter to "Nature" in 2011, a group of Canadian scientists reported that cod in the Scotian Shelf ecosystem off Canada are showing signs of recovery.
History.
Cod has been an important economic commodity in international markets since the Viking period (around 800 AD). Norwegians traveled with dried cod and soon a dried cod market developed in southern Europe. This market has lasted for more than 1,000 years, enduring the Black Death, wars and other crises, and is still an important Norwegian fish trade. The Portuguese began fishing cod in the 15th century. Clipfish is widely enjoyed in Portugal. The Basques played an important role in the cod trade, and allegedly found the Canadian fishing banks before Columbus' discovery of America. The North American east coast developed in part due to the vast cod stocks. Many cities in the New England area are located near cod fishing grounds. The fish was so important to the history and development of Massachusetts, the state's House of Representatives hung a wood carving of a codfish, known as the Sacred Cod of Massachusetts, in its chambers.
Apart from the long history, cod differ from most fish because the fishing grounds are far from population centers. The large cod fisheries along the coast of North Norway (and in particular close to the Lofoten islands) have been developed almost uniquely for export, depending on sea transport of stockfish over large distances. Since the introduction of salt, dried and salted cod (clipfish or 'klippfisk' in Norwegian) has also been exported. By the end of the 14th century, the Hanseatic League dominated trade operations and sea transport, with Bergen as the most important port.
William Pitt the Elder, criticizing the Treaty of Paris in Parliament, claimed cod was "British gold"; and that it was folly to restore Newfoundland fishing rights to the French.
In the 17th and 18th centuries in the New World, especially in Massachusetts and Newfoundland, cod became a major commodity, creating trade networks and cross-cultural exchanges. In 1733, Britain tried to gain control over trade between New England and the British Caribbean by imposing the Molasses Act, which they believed would eliminate the trade by making it unprofitable. The cod trade grew instead, because the "French were eager to work with the New Englanders in a lucrative contraband arrangement". In addition to increasing trade, the New England settlers organized into a "codfish aristocracy". The colonists rose up against Britain's "tariff on an import".
In the 20th century, Iceland re-emerged as a fishing power and entered the Cod Wars. In the late 20th and early 21st centuries, fishing off the European and American coasts severely depleted stocks and become a major political issue. The necessity of restricting catches to allow stocks to recover upset the fishing industry and politicians reluctant to hurt employment.

</doc>
<doc id="41519" url="http://en.wikipedia.org/wiki?curid=41519" title="Photic zone">
Photic zone

The photic zone, euphotic zone (Greek for "well lit": εὖ "well" + φῶς "light"), or sunlight zone is the depth of the water in a lake or ocean that is exposed to such intensity of sunlight which designates compensation point, i.e. the intensity of light at which the rate of carbon dioxide uptake, or equivalently, the rate of oxygen production, is equal to the rate of carbon dioxide production, equivalently to the rate of oxygen consumption, reducing thus the net carbon dioxide assimilation to zero. 
It extends from the surface down to a depth where light intensity falls to one percent of that at the surface, called the euphotic depth. Accordingly, its thickness depends on the extent of light attenuation in the water column. Typical euphotic depths vary from only a few centimetres in highly turbid eutrophic lakes, to around 200 metres in the open ocean. It also varies with seasonal changes in turbidity.
Since the photic zone is where almost all of the photosynthesis occurs, the depth of the photic zone is generally proportional to the level of primary production that occurs in that area of the ocean. About 90% of all marine life lives in the photic zone. A small amount of primary production is generated deep in the abyssal zone around the hydrothermal vents which exist along some mid-oceanic ridges.
The zone which extends from the base of the euphotic zone to about 200 metres is sometimes called the disphotic zone. While there is some light, it is insufficient for photosynthesis, or at least insufficient for photosynthesis at a rate greater than respiration. The euphotic zone together with the disphotic zone coincides with the epipelagic zone. The bottommost zone, below the euphotic zone, is called the aphotic zone. Most deep ocean waters belong to this zone. 
The transparency of the water, which determines the depth of the photic zone, is measured simply with a Secchi disk. It may also be measured with a photometer lowered into the water.

</doc>
<doc id="41520" url="http://en.wikipedia.org/wiki?curid=41520" title="Masovian Voivodeship">
Masovian Voivodeship

Masovian Voivodeship or Mazovia Province (Polish: "województwo mazowieckie" ), is the largest and most populous of the sixteen Polish provinces, or voivodeships, created in 1999. It occupies 35579 km2 of east-central Poland, and has 5,324,500 million inhabitants. Its principal cities are Warsaw (1.729 million) in the centre of the Warsaw metropolitan area, Radom (226,000) in the south, Płock (127,000) in the west, Siedlce (77,000) in the east, and Ostrołęka (55,000) in the north. The capital of the voivodeship is the national capital, Warsaw.
The province was created on January 1, 1999, out of the former Warsaw, Płock, Ciechanów, Ostrołęka, Siedlce and Radom Voivodeships, pursuant to the Polish local government reforms adopted in 1998. The province's name recalls the traditional name of the region, "Mazowsze" (sometimes rendered in English as "Masovia"), with which it is roughly coterminous. However, southern part of the voivodeship, with Radom, historically belongs to Małopolska (Lesser Poland), while Łomża and its surroundings, even though historically part of Masovia, now is part of Podlaskie Voivodeship.
It is bordered by six other voivodeships: Warmian-Masurian to the north, Podlaskie to the north-east, Lublin to the south-east, Świętokrzyskie to the south, Łódź to the south-west, and Kuyavian-Pomeranian to the north-west.
Administrative division.
Masovian Voivodeship is divided into 42 counties ("powiats"): 5 city counties ("miasto na prawach powiatu") and 37 "land counties" ("powiat ziemski"). These are subdivided into 314 gminas, which include 85 "urban gminas".
Cities and towns.
The voivodeship contains 85 cities and towns. These are listed below in descending order of population (according to official figures for 2006):
Protected areas.
Protected areas in Masovian Voivodeship include one National Park and nine Landscape Parks. These are listed below.
Historical.
Masovian Voivodeship (1526–1795).
Masovia Voivodeship, 1526–1795 (Polish: "Województwo Mazowieckie") was an administrative region of the Kingdom of Poland, and of the Polish-Lithuanian Commonwealth, from the 15th century until the partitions of the Polish-Lithuanian Commonwealth (1795). Together with Płock and Rawa Voivodeships, it formed the province (prowincja) of Masovia.
Masovian Voivodeship (1816–1837).
Masovian Voivodeship was one of the voivodeships of Congress Poland. It was formed from Warsaw Department, and transformed into Masovia Governorate.
Transport.
There are three main road routes that pass through the voivodship: Cork-Berlin-Poznań-Warszawa-Minsk-Moscow-Omsk, Prague-Wrocław-Warsaw-Białystok-Helsinki and Pskov-Gdańsk-Warsaw-Kraków-Budapest.
Currently there are only small stretches of autostrada in the area. However, the A2 autostrada, upon its completion, will be the first autostrada to connect the region, and therefore the capital city, with the rest of Europe. The autostrada will pass directly through the voivodship from east to west connecting it with Belarus and Germany.
The railroad system is based on Koleje Mazowieckie and PKP Intercity.
The main international airport in the region is Warsaw Frederic Chopin Airport.
Economy.
Masovian Voivodeship is the wealthiest in Poland. It produces 22% of Polish GDP, and GDP per capita is 160% of country average.

</doc>
<doc id="41522" url="http://en.wikipedia.org/wiki?curid=41522" title="Northanger Abbey">
Northanger Abbey

Northanger Abbey was the first of Jane Austen's novels to be completed for publication, though she had previously made a start on "Sense and Sensibility" and "Pride and Prejudice". According to Cassandra Austen's "Memorandum", "Susan" (as it was first called) was written circa 1798–99. It was revised by Austen for the press in 1803, and sold in the same year for £10 to a London bookseller, Crosby & Co., who decided against publishing. In the spring of 1816, the bookseller was content to sell it back to the novelist's brother, Henry Austen, for the exact sum—£10—that he had paid for it at the beginning, not knowing that the writer was by then the author of four popular novels. 
The novel was further revised by Austen in 1816/17, with the intention of having it published. Among other changes, the lead character's name was changed from Susan to Catherine, and Austen retitled the book "Catherine" as a result.
Austen died in July 1817. "Northanger Abbey" (as the novel was now called) was brought out posthumously in late December 1817 (1818 given on the title page), as the first two volumes of a four-volume set that also featured another previously unpublished Austen novel, "Persuasion". Neither novel was published under the title Jane Austen had given it; the title "Northanger Abbey" is presumed to have been the invention of Henry Austen, who had arranged for the book's publication.
Plot summary.
Seventeen-year-old Catherine Morland is one of ten children of a country clergyman. Although a tomboy in her childhood, by the age of 17 she is "in training for a heroine" and is excessively fond of reading Gothic novels, among which Ann Radcliffe's "Mysteries of Udolpho" is a favourite.
Catherine is invited by the Allens, her wealthier neighbours in Fullerton, to accompany them to visit the town of Bath and partake in the winter season of balls, theatre and other social delights. Although initially the excitement of Bath is dampened by her lack of acquaintances, she is soon introduced to a clever young gentleman, Henry Tilney, with whom she dances and converses. Much to Catherine's disappointment, Henry does not reappear in the subsequent week and, not knowing whether or not he has left Bath for good, she wonders if she will ever see him again. Through Mrs Allen's old school-friend Mrs Thorpe, she meets her daughter Isabella, a vivacious and flirtatious young woman, and the two quickly become friends. Mrs Thorpe's son John is also a friend of Catherine's older brother, James, at Oxford where they are both students.
James and John arrive unexpectedly in Bath. While Isabella and James spend time together, Catherine becomes acquainted with John, a vain and crude young gentleman who incessantly tells fantastical stories about himself. Henry Tilney then returns to Bath, accompanied by his younger sister Eleanor, who is a sweet, elegant, and respectable young lady. Catherine also meets their father, the imposing General Tilney.
The Thorpes are not very happy about Catherine's friendship with the Tilneys, as they (correctly as it happens) perceive Henry as a rival for Catherine's affections. Catherine tries to maintain her friendships with both the Thorpes and the Tilneys, though John Thorpe continuously tries to sabotage her relationship with the Tilneys. This leads to several misunderstandings, which upset Catherine and put her in the awkward position of having to explain herself to the Tilneys.
Isabella and James become engaged. James's father approves of the match and offers his son a country parson's living of a modest sum, 400 pounds annually, which he may have in two and a half years. The couple must therefore wait until that time to marry. Isabella is dissatisfied, having believed that the Morlands were quite wealthy, but she pretends to Catherine that she is merely dissatisfied that they must wait so long. James departs to purchase a ring, and John accompanies him after coyly suggesting marriage to the oblivious Catherine. Isabella immediately begins to flirt with Captain Tilney, Henry's older brother. Innocent Catherine cannot understand her friend's behaviour, but Henry understands all too well, as he knows his brother's character and habits. The flirtation continues even when James returns, much to the latter's embarrassment and distress.
The Tilneys invite Catherine to stay with them for a few weeks at their home, Northanger Abbey. Catherine, in accordance with her novel reading, expects the abbey to be exotic and frightening. Henry teases her about this, as it turns out that Northanger Abbey is pleasant and decidedly not Gothic. However, the house includes a mysterious suite of rooms that no one ever enters; Catherine learns that they were Mrs Tilney's, who died nine years earlier. Catherine decides that, since General Tilney does not now seem to be affected by the loss of his wife, he may have murdered her or even imprisoned her in her chamber.
Catherine persuades Eleanor to show her Mrs Tilney's rooms, but General Tilney suddenly appears. Catherine flees, sure that she will be punished. Later, Catherine sneaks back to Mrs Tilney's rooms, to discover that her over-active imagination has once again led her astray, as nothing is strange or distressing in the rooms at all. Unfortunately, Henry joins her in the corridor and questions why she is there. He guesses her surmises and inferences, and informs her that his father loved his wife in his own way and was truly upset by her death. "What have you been judging from? Remember the country and the age in which we live. Remember that we are English, that we are Christians. Consult your own understanding, your own sense of the probable, your own observation of what is passing around you. Does our education prepare us for such atrocities? Do our laws connive at them? ... Dearest Miss Morland, what ideas have you been admitting?" She leaves, crying, fearing that she has lost Henry's regard entirely.
Realizing how foolish she has been, Catherine comes to believe that, though novels may be delightful, their content does not relate to everyday life. Henry lets her get over her shameful thoughts and actions in her own time and does not mention them to her again.
Soon after this adventure, James writes to inform her that he has broken off his engagement to Isabella and that she has become engaged instead to Captain Tilney. Henry and Eleanor Tilney are shocked but rather sceptical that their brother has actually become engaged to Isabella Thorpe. Catherine is terribly disappointed, realising what a dishonest person Isabella is. A subsequent letter from Isabella herself confirms the Tilney siblings' doubts about the engagement and shows that Frederick Tilney was merely flirting with Isabella. The General goes off to London, and the atmosphere at Northanger Abbey immediately becomes lighter and pleasanter for his absence. Catherine passes several enjoyable days with Henry and Eleanor until, in Henry's absence, he returns abruptly, in a temper. Eleanor tells Catherine that the family has an engagement that prevents Catherine from staying any longer and that she must go home early the next morning, in a shocking, inhospitable move that forces Catherine to undertake the 70 mi journey alone.
At home, Catherine is listless and unhappy. Her parents, unaware of her trials of the heart, try to bring her up to her usual spirits, with little effect. Two days after she returns home, however, Henry pays a sudden unexpected visit and explains what happened. General Tilney (on the misinformation of John Thorpe) had believed her to be exceedingly rich and therefore a proper match for Henry. In London, General Tilney ran into Thorpe again, who, angry at Catherine's refusal of his half-made proposal of marriage, said instead that she was nearly destitute. Enraged, General Tilney returned home to evict Catherine. When Henry returned to Northanger from Woodston, his father informed him of what had occurred and forbade him to think of Catherine again. When Henry learns how she had been treated, he breaks with his father and tells Catherine he still wants to marry her despite his father's disapproval. Catherine is delighted.
Eventually, General Tilney acquiesces, because Eleanor has become engaged to a wealthy and titled man; and he discovers that the Morlands, while not extremely rich, are far from destitute.
Characters.
Catherine Morland: A 17-year-old girl who loves reading Gothic novels. Something of a tomboy in her childhood, her looks are described by the narrator as "pleasing, and, when in good looks, pretty." Catherine lacks experience and sees her life as if she were a heroine in a Gothic novel. She sees the best in people, and to begin with always seems ignorant of other people's malign intentions. She is the devoted sister of James Morland. She is good-natured and frank and often makes insightful comments on the inconsistencies and insincerities of people around her, usually to Henry Tilney, and thus is unintentionally sarcastic and funny. (He is delighted when she says, "I cannot speak well enough to be unintelligible.") She is also seen as a humble and modest character, becoming exceedingly happy when she receives the smallest compliment. Catherine's character grows throughout the novel, as she gradually becomes a real heroine, learning from her mistakes when she is exposed to the outside world in Bath. She sometimes makes the mistake of applying Gothic novels to real life situations; for example, later in the novel she begins to suspect General Tilney of having murdered his deceased wife. Catherine soon learns that Gothic novels are really just fiction and do not always correspond with reality.
James Morland: Catherine's older brother who is in school at the beginning of the story. Assumed to be of moderate wealth, he becomes the love interest of Isabella Thorpe, the younger sister to his friend and Catherine's admirer John Thorpe.
Henry Tilney: A 26 year-old well-read clergyman, the younger son of the wealthy Tilney family. He is Catherine's romantic interest throughout the novel, and during the course of the plot he comes to return her feelings. He is sarcastic, intuitive, fairly handsome and clever, given to witticisms and light flirtations (which Catherine is not always able to understand or reciprocate in kind), but he also has a sympathetic nature (he is a good brother to Eleanor), which leads him to take a liking to Catherine's naïve straightforward sincerity.
John Thorpe: An arrogant and extremely boastful young man who certainly appears distasteful to the likes of Catherine. He is Isabella's brother and he has shown many signs of feelings towards Catherine Morland.
Isabella Thorpe: A manipulative and self-serving young woman on a quest to obtain a well-off husband; at the time, marriage was the accepted way for young women of a certain class to become "established" with a household of their own (as opposed to becoming a dependent spinster), and Isabella lacks most assets (such as wealth or family connections to bring to a marriage) that would make her a "catch" on the "marriage market". Upon her arrival in Bath she is without acquaintance, leading her to immediately form a quick friendship with Catherine Morland. Additionally, when she learns that Catherine is the sister to James Morland (whom Isabella suspects to be worth more financially than he is in reality), she goes to every length to ensure a connection between the two families.
General Tilney: A stern and rigid retired general with an obsessive nature, General Tilney is the sole surviving parent to his three children Frederick, Henry, and Eleanor.
Eleanor Tilney: Henry's sister, she plays little part in Bath, but takes on more importance in Northanger Abbey. A convenient chaperon for Catherine and Henry's times together. Obedient daughter, warm friend, sweet sister, but lonely under her father's tyranny.
Frederick Tilney: Henry's older brother (the presumed heir to the Northanger estate), very handsome and fashionable, an officer in the army who enjoys pursuing flirtations with pretty girls who are willing to offer him some encouragement (though without any ultimate serious intent on his part).
Mr. Allen: A kindly man, with some slight resemblance to Mr. Bennet of "Pride and Prejudice".
Mrs. Allen: Somewhat vacuous, she sees everything in terms of her obsession with clothing and fashion, and has a tendency to utter repetitions of remarks made by others in place of original conversation.
Major themes.
In addition, Catherine Morland realises she is not to rely upon others, such as Isabella, who are negatively influential on her, but to be single-minded and independent. It is only through bad experiences that Catherine really begins to mature and grow up.
Allusions/references to other works.
Several Gothic novels are mentioned in the book, including most importantly "The Mysteries of Udolpho" and "The Italian" by Ann Radcliffe. Austen also satirises "Clermont", a Gothic novel by Regina Maria Roche. This last is included in a list of seven somewhat obscure Gothic works, known as the 'Northanger horrid novels' as recommended by Isabella Thorpe to Catherine Morland:
Though these lurid titles were assumed by some to be Austen's own invention, Montague Summers and Michael Sadleir discovered that they really did exist and have since been republished.
Jane Austen, who referred to Fanny Burney as "the first of English novelists," in "Northanger Abbey" refers to her inspiring novels:
John Thorpe, who knows little about literature, tells Catherine that he likes "The Monk" (an over-the-top tale of lurid Gothic horror):
Literary significance and relationship.
"Northanger Abbey" is fundamentally a parody of Gothic fiction. Austen turns the conventions of eighteenth-century novels on their head, by making her heroine a plain and undistinguished girl from a middle-class family, allowing the heroine to fall in love with the hero before he has a serious thought of her, and exposing the heroine's romantic fears and curiosities as groundless. Austen biographer Claire Tomalin speculates that Austen may have begun this book, which is more explicitly comic than her other works and contains many literary allusions that her parents and siblings would have enjoyed, as a family entertainment—a piece of lighthearted parody to be read aloud by the fireside. Moreover, as Joan Aiken writes,
Austen addresses the reader directly in parts, particularly at the end of Chapter 5, where she gives a lengthy opinion of the value of novels, and the contemporary social prejudice against them in favour of drier historical works and newspapers. In discussions featuring Isabella, the Thorpe sisters, Eleanor, and Henry, and by Catherine perusing the library of the General, and her mother's books on instructions on behaviours, the reader gains further insights into Austen's various perspectives on novels in contrast with other popular literature of the time (especially the Gothic novel). Eleanor even praises history books, and while Catherine points out the obvious fiction of the speeches given to important historical characters through, Eleanor enjoys them for what they are.
The directness with which Austen addresses the reader, especially at the end of the story, gives a unique insight into Austen's thoughts at the time, which is particularly important due to her letters having been burned at her request by her sister upon her death.
Adaptations.
Literature.
In 2012, it was announced that HarperCollins had hired Scottish crime writer Val McDermid to adapt "Northanger Abbey" for a modern audience, as a suspenseful teen thriller. McDermid said of the project, "At its heart it's a teen novel, and a satire – that's something which fits really well with contemporary fiction. And you can really feel a shiver of fear moving through it. I will be keeping the suspense – I know how to keep the reader on the edge of their seat. I think Jane Austen builds suspense well in a couple of places, but she squanders it, and she gets to the endgame too quickly. So I will be working on those things."
Historical discoveries.
The book contains an early reference to baseball. ("...Catherine, who had by nature nothing heroic about her, should prefer cricket, baseball, riding on horseback, and running about the country...").
References to "Northanger Abbey".
A passage from the novel appears as the preface of Ian McEwan's "Atonement", thus likening the naive mistakes of Austen's Catherine Morland to those of his own character Briony Tallis, who is in a similar position: both characters have very over-active imaginations, which lead to misconceptions that cause distress in the lives of people around them. Both treat their own lives like those of heroines in fantastical works of fiction, with Miss Morland likening herself to a character in a Gothic novel and young Briony Tallis writing her own melodramatic stories and plays with central characters such as "spontaneous Arabella" based on herself.
Richard Adams quotes a portion of the novel's last sentence for the epigraph to Chapter 50 in his "Watership Down"; the reference to the General is felicitous, as the villain in "Watership Down" is also a General.

</doc>
<doc id="41523" url="http://en.wikipedia.org/wiki?curid=41523" title="Bath, Somerset">
Bath, Somerset

Bath is a city in Somerset, South West England, 97 mi west of London and 12 mi south-east of Bristol. In 2011, its population was 88,859. It became part of Avon in 1974; since Avon's abolition in 1996, it has been the principal centre of Bath and North East Somerset. The city, in the valley of the River Avon, became a World Heritage Site in 1987.
The city became a spa with the Latin name Aquae Sulis ("the waters of Sulis") "c." AD 60 when the Romans built baths and a temple in the valley of the River Avon, although oral tradition suggests that the hot springs were known before then. Bath Abbey was founded in the 7th century becoming a religious centre and the building was rebuilt in the 12th and 16th centuries. In the 17th century claims were made for the curative properties of the water from the springs and Bath became popular as a spa town during the Georgian era, leaving a heritage of Georgian architecture crafted from Bath stone, including the Royal Crescent, Circus, Pump Room and Assembly Rooms where Beau Nash presided over the city's social life from 1705 until his death in 1761. Many of the streets and squares were laid out by John Wood, the Elder and in the 18th century the city became fashionable and the population grew. Jane Austen lived in Bath in the early 19th century. Further building was undertaken in the 19th century and following the Bath Blitz in World War II.
Manufacturing has been in decline in the city, but it has strong software, publishing and service-oriented industries. The city's theatres, museums and other cultural and sporting venues have helped to make it a major centre for tourism with more than one million staying visitors and 3.8 million day visitors to the city each year. There are several museums including the Museum of Bath Architecture, Victoria Art Gallery, Museum of East Asian Art, and the Holburne Museum. The city has two universities; the University of Bath and Bath Spa University with Bath College providing further education. Sporting clubs include Bath Rugby and Bath City F.C. while TeamBath is the umbrella name for all of the University of Bath sports teams.
History.
Iron Age and Roman.
The hills in the locality such as Bathampton Down saw human activity from the Mesolithic period. Several Bronze Age round barrows were opened by John Skinner in the 18th century. Bathampton Camp may have been an Iron Age hill fort or stock enclosure. A long barrow site believed to be from the Beaker people was flattened to make way for RAF Charmy Down.
Archaeological evidence shows that the site of the Roman baths' main spring may have been treated as a shrine by the Britons, and was dedicated to the goddess Sulis, whom the Romans identified with Minerva; the name Sulis continued to be used after the Roman invasion, appearing in the town's Roman name, "Aquae Sulis" (literally, "the waters of Sulis"). Messages to her scratched onto metal, known as curse tablets, have been recovered from the sacred spring by archaeologists. The tablets were written in Latin, and cursed people by whom the writers felt they had been wronged. For example, if a citizen had his clothes stolen at the baths, he might write a curse, naming the suspects, on a tablet to be read by the goddess.
A temple was constructed in 60–70 AD and a bathing complex was built up over the next 300 years. Engineers drove oak piles into the mud to provide a stable foundation, and surrounded the spring with an irregular stone chamber lined with lead. In the 2nd century, the spring was enclosed within a wooden barrel-vaulted structure, that housed the caldarium (hot bath), tepidarium (warm bath), and frigidarium (cold bath).
The city was later given defensive walls, probably in the 3rd century. After the failure of Roman authority in the first decade of the 5th century, the baths fell into disrepair and were eventually lost as a result of silting.
In March 2012 a hoard of 30,000 silver Roman coins, one of the largest discovered in Britain, was unearthed in an archaeological dig. The coins, believed to date from the 3rd century, were found about 450 feet from the Roman baths.
Post-Roman and Medieval.
Bath may have been the site of the Battle of Badon (c. 500 AD), in which King Arthur is said to have defeated the Anglo-Saxons. The city fell to the West Saxons in 577 after the Battle of Deorham; the Anglo-Saxon poem "The Ruin" may describe the appearance of the Roman site about this time. A monastery was founded at an early date – reputedly by Saint David, although more probably in 675 by Osric, King of the Hwicce, perhaps using the walled area as its precinct. Nennius, a 9th century historian, mentions a "Hot Lake" in the land of the Hwicce along the River Severn, and adds "It is surrounded by a wall, made of brick and stone, and men may go there to bathe at any time, and every man can have the kind of bath he likes. If he wants, it will be a cold bath; and if he wants a hot bath, it will be hot". Bede described hot baths in the geographical introduction to the "Ecclesiastical History" in terms very similar to those of Nennius. King Offa of Mercia gained control of the monastery in 781 and rebuilt the church, which was dedicated to St. Peter.
By the 9th century the old Roman street pattern was lost and Bath was a royal possession. King Alfred laid out the town afresh, leaving its south-eastern quadrant as the abbey precinct. In the Burghal Hidage Bath is described as having walls of 1375 yd and was allocated 1000 men for defence. During the reign of Edward the Elder coins were minted in Bath based on a design from the Winchester mint but with 'BAD' on the obverse relating to the Anglo-Saxon name for the town, Baðum, Baðan or Baðon, meaning "at the baths," and this was the source of the present name. Edgar of England was crowned king of England in Bath Abbey in 973.
William Rufus granted the city to a royal physician, John of Tours, who became Bishop of Wells and Abbot of Bath, following the sacking of the town during the Rebellion of 1088. It was papal policy for bishops to move to more urban seats, and he translated his own from Wells to Bath. He planned and began a much larger church as his cathedral, to which was attached a priory, with the bishop's palace beside it. New baths were built around the three springs. Later bishops returned the episcopal seat to Wells, while retaining the name Bath in the title, Bishop of Bath and Wells. St John's Hospital was founded around 1180, by Bishop Reginald Fitz Jocelin and is among the oldest almshouses in England. The 'hospital of the baths' was built beside the hot springs of the Cross Bath, for their health giving properties and to provide shelter for the poor infirm.
Administrative systems fell within the hundreds. The Bath Hundred had various names including the Hundred of Le Buri. The Bath Foreign Hundred or Forinsecum covered the area outside the city and was later combined into the Bath Forum Hundred. Wealthy merchants had no status within the hundred courts and formed guilds to gain influence. They built the first guildhall probably in the 13th century. Around 1200 the first mayor was appointed.
Early Modern.
By the 15th century, Bath's abbey church was badly dilapidated and Oliver King, Bishop of Bath and Wells, decided to rebuild it on a smaller scale in 1500. The new church was completed just a few years before Bath Priory was dissolved in 1539 by Henry VIII. The abbey church became derelict before being restored as the city's parish church in the Elizabethan era, when the city experienced a revival as a spa. The baths were improved and the city began to attract the aristocracy. A Royal charter granted by Queen Elizabeth I in 1590 confirmed city status.
During the English Civil War, the city was garrisoned for Charles I. Seven thousand pounds was spent on fortifications but on the appearance of parliamentary forces, the gates were thrown open and the city surrendered. It became a significant post for the New Model Army under William Waller. It was retaken by royalists following the Battle of Lansdowne fought on the northern outskirts of the city on 5 July 1643. Thomas Guidott, a student of chemistry and medicine at Wadham College, Oxford, set up a practice in the town in 1668. He was interested in the curative properties of the waters and he wrote "A discourse of Bathe, and the hot waters there. Also, Some Enquiries into the Nature of the water" in 1676. It brought the health-giving properties of the hot mineral waters to the attention of the country and the aristocracy arrived to partake in them.
Several areas of the city were developed in the Stuart period, and more building took place during Georgian times in response to the increasing number of visitors who required accommodation. Architects John Wood the elder and his son laid out the new quarters in streets and squares, the identical façades of which gave an impression of palatial scale and classical decorum. Much of the creamy gold Bath stone, a type of limestone, used for construction in the city was obtained from the Combe Down and Bathampton Down Mines, owned by Ralph Allen (1694–1764). Allen, to advertise the quality of his quarried limestone, commissioned the elder John Wood to build a country house on his Prior Park estate between the city and the mines. Allen was responsible for improving and expanding the postal service in western England, for which he held the contract for more than forty years. Although not fond of politics, Allen was a civic-minded man and member of Bath Corporation for many years. He was elected mayor for a single term in 1742.
In the early 18th century, Bath acquired its first purpose-built theatre, the Old Orchard Street Theatre. It was rebuilt as the Theatre Royal, the along with the Grand Pump Room attached to the Roman Baths and assembly rooms. Master of ceremonies Beau Nash, who presided over the city's social life from 1705 until his death in 1761, drew up a code of behaviour for public entertainments.
Late Modern.
The population of the city was 40,020 at the 1801 census, making it one of the largest cities in Britain. William Thomas Beckford bought a house in Lansdown Crescent in 1822, and subsequently two adjacent houses to form his residence. Having acquired all the land between his home and the top of Lansdown Hill, he created a garden more than 1/2 mi in length and built Beckford's Tower at the top.
Emperor Haile Selassie of Ethiopia spent the four years in exile, from 1936 to 1940, at Fairfield House in Bath. During World War II, between the evening of 25 April and the early morning of 27 April 1942, Bath suffered three air raids in reprisal for RAF raids on the German cities of Lübeck and Rostock, part of the Luftwaffe campaign popularly known as the Baedeker Blitz. During the Bath Blitz, more than 400 people were killed, and more than 19,000 buildings damaged or destroyed. Houses in the Royal Crescent, Circus and Paragon were burnt out along with the Assembly Rooms. A 500 kg high explosive bomb landed on the east side of Queen Square, resulting in houses on the south side being damaged, and the Francis Hotel losing 24 m of its frontage. The buildings have all been restored, although there are still signs of the bombing.
A postwar review of inadequate housing led to the clearance and redevelopment of areas of the city in a postwar style, often at variance with the local Georgian style. In the 1950s the nearby villages of Combe Down, Twerton and Weston were incorporated into the city to enable the development of housing, much of it council housing. In the 1970s and 1980s it was recognised that conservation of historic buildings was inadequate, leading to more care and reuse of buildings and open spaces. In 1987 the city was selected by UNESCO as a World Heritage Site, recognising its international cultural significance.
Since 2000, major developments have included the Bath Spa, SouthGate shopping centre and the residential Western Riverside project.
Government.
Historically part of the county of Somerset, Bath was made a county borough in 1889 independent of the newly created administrative Somerset County Council. Bath became part of Avon when the non-metropolitan county was created in 1974. Since the abolition of Avon in 1996, Bath has been the centre of the unitary authority of Bath and North East Somerset (B&NES). Bath remains in the ceremonial county of Somerset, although not within the administrative non-metropolitan county.
Because Bath is unparished, there is no longer a city council or parish council. The City of Bath's ceremonial functions, including the mayoralty – which can be traced back to 1230 – and control of the coat of arms, are maintained by "the charter trustees of the City of Bath". The coat of arms includes two silver strips representing the River Avon and the hot springs. The sword of St. Paul is a link to Bath Abbey. The supporters, a lion and a bear, stand on a bed of acorns, a link to Bladud, the subject of the Legend of Bath. The knight's helmet indicates a municipality and the crown is that of King Edgar.
Before the Reform Act 1832 Bath elected two members to the unreformed House of Commons. Bath is now a single parliamentary constituency, represented by Conservative Ben Howlett, who replaced the retiring Liberal Democrat Don Foster at the 2015 general election. Foster's election was a notable result of the 1992 general election, as Chris Patten, the previous Member (and Cabinet Minister) played a major part, as Chairman of the Conservative Party, in re-electing the government of John Major, but failed to defend his marginal seat.
The sixteen electoral wards of the Bath and North East Somerset unitary authority within Bath (they are co-terminus with the unparished area) are the central Abbey, Kingsmead and Walcot wards, and the more outlying wards of Bathwick, Combe Down, Lambridge, Lansdown, Lyncombe, Newbridge, Odd Down, Oldfield, Southdown, Twerton, Westmoreland, Weston and Widcombe.
Geography.
Physical geography.
Bath is in the Avon Valley near the southern edge of the Cotswolds, a range of limestone hills designated as an Area of Outstanding Natural Beauty. The hills that surround and make up the city have a maximum altitude of 781 ft on the Lansdown plateau. Bath has an area of 11 mi2.
The floodplain of the Avon, on which the city centre is built, has an altitude of about 59 ft above sea level. The river, once an unnavigable series of braided streams broken up by swamps and ponds, has been managed by weirs into a single channel. Periodic flooding, which shortened the life of many buildings in the lowest part of the city, was normal until major flood control works were completed in the 1970s.
Water bubbling up from the ground, as geothermal springs, fell as rain on the Mendip Hills. It percolates through limestone aquifers to a depth of between c. 9000 to where geothermal energy raises its temperature to between 64 and 96 °C (c. 147–205 °F). Under pressure, the heated water rises to the surface along fissures and faults in the limestone. The process is similar to an artificial one known as enhanced geothermal system which makes use of the high pressures and temperatures in the Earth's crust. Hot water at a temperature of 46 °C rises here at the rate of 1170000 l daily, from a geological fault (the Pennyquick fault). In 1983, a new spa-water bore-hole was sunk, providing a clean and safe supply for drinking in the Pump Room. There is no universal definition to distinguish a hot spring from a geothermal spring, although by several definitions, the Bath springs can be considered the only hot springs in the UK. Three of the springs feed the thermal baths.
Climate.
Along with the rest of South West England, Bath has a temperate climate which is generally wetter and milder than the rest of the country. The annual mean temperature is approximately 10 °C. Seasonal temperature variation is less extreme than most of the United Kingdom because of the adjacent sea temperatures. The summer months of July and August are the warmest with mean daily maxima of approximately 21 °C. In winter mean minimum temperatures of 1 or are common. In the summer the Azores high pressure affects the south-west of England, however convective cloud sometimes forms inland, reducing the number of hours of sunshine. Annual sunshine rates are slightly less than the regional average of 1,600 hours. In December 1998 there were 20 days without sun recorded at Yeovilton. Most of the rainfall in the south-west is caused by Atlantic depressions or by convection. Most of the rainfall in autumn and winter is caused by the Atlantic depressions, which is when they are most active. In summer, a large proportion of the rainfall is caused by sun heating the ground leading to convection and to showers and thunderstorms. Average rainfall is around 700 mm. About 8–15 days of snowfall is typical. November to March have the highest mean wind speeds, and June to August have the lightest winds. The predominant wind direction is from the south-west.
Demography.
The 2011 census recorded a population of 88,859 for the city of Bath. (This figure represents the combined populations of the 16 wards that are co-terminus with the unparished area that covers the city.) An Office for National Statistics estimate of the population (for mid-2010) for the urban area was put at 97,311.
According to the 2001 census, Bath, together with North East Somerset, which includes areas around Bath as far as the Chew Valley, had a population of 169,040, with an average age of 39.9 (the national average being 38.6). Demography shows according to the same statistics, the district is overwhelmingly populated by people of a white background at 97.2% – significantly higher than the national average of 90.9%. Other ethnic groups in the district, in order of population size, are multiracial at 1%, Asian at 0.5% and black at 0.5% (the national averages are 1.3%, 4.6% and 2.1%, respectively).
The district is largely Christian at 71%, with no other religion reaching more than 0.5%. These figures generally compare with the national averages, though the non-religious, at 19.5%, are significantly more prevalent than the national 14.8%. 7.4% of the population describe themselves as "not healthy" in the last 12 months, compared with a national average of 9.2%; nationally 18.2% of people describe themselves as having a long-term illness, in Bath it is 15.8%.
An inhabitant of Bath is known as a Bathonian.
Economy.
Industry.
Bath once had an important manufacturing sector, led by companies such Stothert and Pitt. Nowadays manufacturing is in decline in the city, but it boasts strong software, publishing and service-oriented industries, being home to companies such as Future Publishing and London & Country mortgage brokers. The city's attraction to tourists has also led to a significant number of jobs in tourism-related industries. Important economic sectors in Bath include education and health (30,000 jobs), retail, tourism and leisure (14,000 jobs) and business and professional services (10,000 jobs). Its main employers are the National Health Service, the two universities and the Bath and North East Somerset Council, as well as the Ministry of Defence, although a number of MOD offices formerly in Bath have now moved to Bristol. Growing employment sectors include information and communication technologies and creative and cultural industries where Bath is one of the recognised national centres for publishing, with the magazine and digital publisher Future Publishing employing around 650 people. Others include Buro Happold (400) and IPL Information Processing Limited (250). The city contains over 400 retail shops, 50% being run by independent specialist retailers, and around 100 restaurants and cafes which are primarily supported by tourism.
Tourism.
One of Bath's principal industries is tourism, with more than one million staying visitors and 3.8 million day visitors to the city on an annual basis. The visits mainly fall into the categories of heritage tourism and cultural tourism, aided by the city's selection in 1987 as a World Heritage Site, recognising its international cultural significance.
All significant stages of the history of England are represented within the city, from the Roman Baths (including their significant Celtic presence), to Bath Abbey and the Royal Crescent, to Thermae Bath Spa in the 2000s. The size of the tourist industry is reflected in the almost 300 places of accommodation – including over 80 hotels, and over 180 bed and breakfasts – many of which are located in Georgian buildings. Two of the hotels have 'five-star' ratings. There are also two campsites located on the western edge of the city. The city also contains about 100 restaurants, and a similar number of pubs and bars. Several companies offer open top bus tours around the city, as well as tours on foot and on the river. Since 2006, with the opening of Thermae Bath Spa, the city has attempted to recapture its historical position as the only town in the United Kingdom offering visitors the opportunity to bathe in naturally heated spring waters.
In the 2010 Google Street View Best Streets Awards, the Royal Crescent took the second place in the "Britain's Most Picturesque Street" award, first place being given to The Shambles in York. Milsom Street was also awarded "Britain's Best Fashion Street" in the 11,000 strong vote.
Architecture.
There are many Roman archaeological sites throughout the central area of the city, but the baths themselves are about 6 m below the present city street level. Around the hot springs, Roman foundations, pillar bases, and baths can still be seen, however all the stonework above the level of the baths is from more recent periods.
Bath Abbey was a Norman church built on earlier foundations, although the present building dates from the early 16th century and shows a late Perpendicular style with flying buttresses and crocketed pinnacles decorating a crenellated and pierced parapet. The choir and transepts have a fan vault by Robert and William Vertue. The nave was given a matching vault in the 19th century. The building is lit by 52 windows.
Most buildings in Bath are made from the local, golden-coloured Bath Stone, and many date from the 18th and 19th century. The dominant style of architecture in Central Bath is Georgian; this evolved from the Palladian revival style which became popular in the early 18th century. Many of the prominent architects of the day were employed in the development of the city. The original purpose of much of Bath's architecture is concealed by the honey-coloured classical façades; in an era before the advent of the luxury hotel, these apparently elegant residences were frequently purpose-built lodging houses, where visitors could hire a room, a floor, or (according to their means) an entire house for the duration of their visit, and be waited on by the house's communal servants. The masons Reeves of Bath were prominent in the city from the 1770s to 1860s.
The Circus consists of three long, curved terraces designed by the elder John Wood to form a circular space or theatre intended for civic functions and games. The games give a clue to the design, the inspiration behind which was the Colosseum in Rome. Like the Colosseum, the three façades have a different order of architecture on each floor: Doric on the ground level, then Ionic on the piano nobile and finishing with Corinthian on the upper floor, the style of the building thus becoming progressively more ornate as it rises. Wood never lived to see his unique example of town planning completed, as he died five days after personally laying the foundation stone on 18 May 1754.
The most spectacular of Bath's terraces is the Royal Crescent, built between 1767 and 1774 and designed by the younger John Wood. But all is not what it seems; while Wood designed the great curved façade of what appears to be about 30 houses with Ionic columns on a rusticated ground floor, that was the extent of his input. Each purchaser bought a certain length of the façade, and then employed their own architect to build a house to their own specifications behind it; hence what appears to be two houses is sometimes one. This system of town planning is betrayed at the rear of the crescent: while the front is completely uniform and symmetrical, the rear is a mixture of differing roof heights, juxtapositions and fenestration. This "Queen Anne fronts and Mary-Anne backs" architecture occurs repeatedly in Bath. Other fine terraces elsewhere in the city include Lansdown Crescent and Somerset Place on the northern hill.
Around 1770 the neoclassical architect Robert Adam designed Pulteney Bridge, using as the prototype for the three-arched bridge spanning the Avon an original, but unused, design by Andrea Palladio for the Rialto Bridge in Venice. Thus, Pulteney Bridge became not just a means of crossing the river, but also a shopping arcade. Along with the Rialto Bridge and the Ponte Vecchio in Florence, which it resembles, it is one of the very few surviving bridges in Europe to serve this dual purpose. It has been substantially altered since it was built. The bridge was named after Frances and William Pulteney, the owners of the Bathwick estate for which the bridge provided a link to the rest of Bath. The Georgian streets in the vicinity of the river tended to be built high above the original ground level to avoid flooding, with the carriageways supported on vaults extending in front of the houses. This can be seen in the multi-storey cellars around Laura Place South of Pulteney Bridge, in the colonnades below Grand Parade, and in the grated coal holes in the pavement of North Parade. In some parts of the city, such as George Street, and London Road near Cleveland Bridge, the developers of the opposite side of the road did not match this pattern, leaving raised pavements with the ends of the vaults exposed to a lower street below.
The heart of the Georgian city was the Pump Room, which, together with its associated Lower Assembly Rooms, was designed by Thomas Baldwin, a local builder responsible for many other buildings in the city, including the terraces in Argyle Street, and the Guildhall. Baldwin rose rapidly, becoming a leader in Bath's architectural history. In 1776 he was made the chief City Surveyor, and in 1780 became Bath City Architect. Great Pulteney Street, where he eventually lived, is another of his works: this wide boulevard, constructed circa 1789 and over 1000 ft long and 100 ft wide, is lined on both sides by Georgian terraces.
In the 1960s and early 1970s some parts of Bath were unsympathetically redeveloped, resulting in the loss of some 18th- and 19th century buildings. This process was largely halted by a popular campaign which drew strength from the publication of Adam Fergusson's "The Sack of Bath". Controversy has revived perodically, most recently with the demolition of the 1930s Churchill House, a neo-Georgian municipal building originally housing the Electricity Board, to make way for a new bus station. This is part of the Southgate redevelopment in which an ill-favoured 1960s shopping precinct, bus station and multi-story car park were demolished and replaced by a new area of mock-Georgian shopping streets. As a result of this and other changes, notably plans for abandoned industrial land along the Avon, the city's status as a World Heritage Site was reviewed by UNESCO in 2009. The decision was made let Bath keep its status, but UNESCO has asked to be consulted on future phases of the Riverside development, saying that the density and volume of buildings in the second and third phases of the development need to be reconsidered.
It also demands that Bath do more to attract world-class architecture in new developments.
A panoramic view of the Royal Crescent
Culture.
Bath became the centre of fashionable life in England during the 18th century when its Old Orchard Street Theatre and architectural developments such as Lansdown Crescent, the Royal Crescent, The Circus and Pulteney Bridge were built.
Bath's five theatres – Theatre Royal, Ustinov Studio, the egg, the Rondo Theatre, and the Mission Theatre – attract internationally renowned companies and directors and an annual season by Sir Peter Hall. The city has a long-standing musical tradition; Bath Abbey, home to the Klais Organ and the largest concert venue in the city, stages about 20 concerts and 26 organ recitals each year. Another concert venue, the 1,700-seat Art Deco Forum, originated as a cinema. The city holds the annual Bath International Music Festival and Mozartfest, the annual Bath Literature Festival (and its counterpart for children), the Bath Film Festival, the Bath Fringe Festival, the Bath Beer Festival and the Bath Chilli Festival. The Bach Festivals occur at two and a half-year intervals. An annual Bard of Bath competition aims to find the best poet, singer or storyteller.
The city is home to the Victoria Art Gallery, the Museum of East Asian Art, and Holburne Museum, numerous commercial art galleries and antique shops, as well as numerous museums, among them Bath Postal Museum, the Fashion Museum, the Jane Austen Centre, the Herschel Museum of Astronomy and the Roman Baths. The Bath Royal Literary and Scientific Institution (BRLSI) in Queen Square was founded in 1824 from the Society for the encouragement of Agriculture, Planting, Manufactures, Commerce and the Fine Arts founded in 1777. In September 1864, BRLSI hosted the 34th annual meeting of the British Science Association which was attended by explorers David Livingstone, Sir Richard Francis Burton, and John Hanning Speke.
The history of the city is displayed at the Museum of Bath Architecture which is housed in a building which was built in 1765 as the Trinity Presbyterian Church. It was also known as the Countess of Huntingdon's Chapel, as she lived in the attached house from 1707 to 1791.
Bath in the arts.
During the 18th century Thomas Gainsborough and Sir Thomas Lawrence lived and worked in Bath. John Maggs, a painter best known for coaching scenes, was born and lived in Bath with his artistic family.
Jane Austen lived here from 1801 with her father, mother and sister Cassandra, and the family resided at four different addresses until 1806. Jane Austen never liked the city, and wrote to Cassandra, "It will be two years tomorrow since we left Bath for Clifton, with what happy feelings of escape." Bath has honoured her name with the Jane Austen Centre and a city walk. Austen's "Northanger Abbey" and "Persuasion" are set in the city and describe taking the waters, social life, and music recitals.
William Friese-Greene experimented with celluloid and motion pictures in his studio in the 1870s, developing some of the earliest movie camera technology. He is credited as being the inventor of cinematography.
Taking the waters is described in Charles Dickens' novel "The Pickwick Papers" in which Pickwick's servant, Sam Weller, comments that the water has "a very strong flavour o' warm flat irons". The Royal Crescent is the venue for a chase between two characters, Dowler and Winkle. Moyra Caldecott's novel The Waters of Sul is set in Roman Bath in 72 AD, and "The Regency Detective", by David Lassman & Terence James, revolves around the exploits of Jack Swann during the early 1800s. Richard Brinsley Sheridan's play "The Rivals" takes place in the city, as does Roald Dahl's chilling short story, "The Landlady".
Many films and television programmes have been filmed using its architecture as the backdrop including: the 2004 film of Thackeray's "Vanity Fair", "The Duchess" (2008), "The Elusive Pimpernel" (1950) and "The Titfield Thunderbolt" (1953).
In August 2003 The Three Tenors sang at a concert to mark the opening of the Thermae Bath Spa, a new hot water spa in the city centre but delays to the project meant the spa opened three years later on 7 August 2006.
In 2008, 104 decorated pigs were displayed around the city in a public art event called "King Bladud's Pigs in Bath". It celebrated the city, its origins and artists. Decorated pig sculptures were displayed throughout the summer and were auctioned to raise funds for Two Tunnels Greenway.
In 2012, Pulteney Weir was used as a replacement location during post production of the film adaptation of Les Misérables. Stunt shots were filmed in October 2012 after footage acquired during the main filming period was found to have errors.
Parks.
Royal Victoria Park, a short walk from the city centre was opened in 1830 by the 11-year-old Princess Victoria, and was the first park to carry her name. The public park is overlooked by the Royal Crescent and covers 23 ha. It has a skatepark, tennis courts, bowling green, a putting green and a 12- and 18-hole golf course, a pond, open-air concerts, an annual funfair at Easter, and a children's play area. Much of its area is lawn; a notable feature is a ha-ha that segregates it from the Royal Crescent, while giving the impression from the Crescent of uninterrupted grassland across the park to Royal Avenue. It has a "Green Flag Award", the national standard for parks and green spaces in England and Wales, and is registered by English Heritage as of National Historic Importance. The 3.84 ha botanical gardens were formed in 1887 and contain one of the finest collections of plants on limestone in the West Country. The replica of a Roman Temple was used at the British Empire Exhibition at Wembley in 1924. In 1987 the gardens were extended to include the Great Dell, a disused quarry that contains a collection of conifers.
Other parks include: Alexandra Park on a hill overlooking the city; Parade Gardens, along the river near the abbey in the city centre; Sydney Gardens, an 18th century pleasure-garden; Henrietta Park; Hedgemead Park; and Alice Park. Jane Austen wrote that "It would be pleasant to be near the Sydney Gardens. We could go into the Labyrinth every day." Alexandra, Alice and Henrietta parks were built into the growing city among the housing developments. There is a linear park following the old Somerset and Dorset Joint Railway line. Cleveland Pools were built around 1815 close to the River Avon, now the oldest surviving public outdoor lido in England, and plans have been submitted for its restoration.
Bath and Queen Victoria.
Victoria Art Gallery and Royal Victoria Park are named after Queen Victoria who wrote in her journal "The people are really too kind to me.". This feeling seemed to have been reciprocated by the people of Bath: "Lord James O'Brien brought a drawing of the intended pillar which the people of Bath are so kind as to erect in commemoration of my 18th birthday.".
Food.
Several foods have an association with the city. "Sally Lunn buns" (a type of teacake) have long been baked in Bath. They were first mentioned by name in verses printed in the Bath Chronicle, in 1772. At that time they were eaten hot at public breakfasts in Spring Gardens. They can be eaten with sweet or savoury toppings and are sometimes confused with "Bath buns" which are smaller, round, very sweet and very rich. They were associated with the city following The Great Exhibition. Bath buns were originally topped with crushed comfits created by dipping caraway seeds repeatedly in boiling sugar; but today seeds are added to a 'London Bath Bun' (a reference to the bun's promotion and sale at the Great Exhibition). The seeds may be replaced by crushed sugar granules or 'nibs'.
Bath has lent its name to one other distinctive recipe – "Bath Olivers" – dry baked biscuit invented by Dr William Oliver, physician to the Mineral Water Hospital in 1740. Oliver was an anti-obesity campaigner and author of a "Practical Essay on the Use and Abuse of warm Bathing in Gluty Cases". In more recent years, Oliver's efforts have been traduced by the introduction of a version of the biscuit with a plain chocolate coating. "Bath Chaps", the salted and smoked cheek and jawbones of the pig, takes its name from the city. It is available from a stall in the daily covered market. Bath Ales brewery is located in Warmley and Abbey Ales are brewed in the city.
Twinning.
Bath is twinned with five other cities and has an historic partnership agreement with Manly, New South Wales, Australia.
Education.
Bath has two universities. The University of Bath was established in 1966. The university was named University of the Year by "The Sunday Times" (2011) and includes courses for the physical sciences, engineering, mathematics, architecture, management and technology.
Bath Spa University was first granted degree-awarding powers in 1992 as a university college, before being granted university status in August 2005. It delivers Postgraduate Certificate in Education courses. It has schools in the following subject areas: Art and Design, Education, English and Creative Studies, Historical and Cultural Studies, Music and the Performing Arts, Science and the Environment and Social Sciences.
The city contains one further education college, Bath College, and is also home to Norland College, a provider of childcare training and education.
Sport.
Bath Rugby is a rugby union team in the Aviva Premiership league. It plays in black, blue and white kit at the Recreation Ground in the city, where it has been since the late 19th century, following its establishment in 1865. The team's first major honour was winning the John Player Cup, now sponsored as the LV Cup and also known as the Anglo-Welsh Cup, four years consecutively from 1984 until 1987. The team then led the Courage league in six seasons in eight years between 1988–89 and 1995–96, during which time it also won the renamed Pilkington Cup in 1989, 1990, 1992, 1994, 1995 and 1996. It finally won the Heineken Cup in the 1997–98 season, and topped the Zürich Premiership (now Aviva Premiership) in 2003–04. The team's squad includes several members who also play, or have played in the English national team including: Lee Mears, Rob Webber, Dave Attwood, Nick Abendanon and Matt Banahan. Colston's School, Bristol has had a large input in the team over the past decade, providing several current 1st XV squad members. The former England Rugby Team Manager and former Scotland national coach Andy Robinson used to play for Bath Rugby team and was captain and later coach. Both of Robinson's predecessors, Clive Woodward and Jack Rowell, were also former Bath coaches and managers as well as his successor Brian Ashton.
Bath City F.C. is the major football team. Bath City gained promotion to the Conference Premier from the Conference South in 2010. Bath City F.C. play their games at Twerton Park. Until 2009 Team Bath F.C. operated as an affiliate to the University Athletics programme. In 2002, Team Bath became the first university team to enter the FA Cup in 120 years, and advanced through four qualifying rounds to the first round proper. The university's team was established in 1999, while the city team has existed since before 1908 (when it entered the Western League). However in 2009, the Football Conference ruled that Team Bath would not be eligible to gain promotion to a National division, nor were they allowed to participate in Football Association cup competitions. This ruling led to the decision by the club to fold at the end of the 2008–09 Conference South competition. In their final season, Team Bath F.C. finished 11th in the league.
Bath City narrowly missed out on election to The Football League in 1978. Bath also has Non-League football club Odd Down F.C. who play at Lew Hill Memorial Ground.
Many cricket clubs are based in the city, including Bath Cricket Club, who are based at the North Parade Ground and play in the West of England Premier League. Cricket is also played on the Recreation Ground, just across from where the Rugby is played. The Recreation Ground is also home to Bath Croquet Club, which was re-formed in 1976 and is affiliated with the South West Federation of Croquet Clubs.
The Bath Half Marathon is run annually through the city streets, with over 10,000 runners.
TeamBath is the umbrella name for all of the University of Bath sports teams, including the aforementioned football club. Other sports for which TeamBath is noted are athletics, badminton, basketball, bob skeleton, bobsleigh, hockey, judo, modern pentathlon, netball, rugby union, swimming, tennis, triathlon and volleyball. The City of Bath Triathlon takes place annually at the university.
Transport.
Bath is approximately 13 mi south-east of the larger city and port of Bristol, to which it is linked by the A4 road, and is a similar distance south of the M4 motorway. In an attempt to reduce the level of car use park and ride schemes have been introduced, with sites at Odd Down, Lansdown and Newbridge. Paradoxically a very large increase in city centre parking was provided under the new shopping centre which necessarily introduces more car traffic. In addition a bus gate scheme in Northgate aims to reduce private car use in the city centre. National Express operates coach services from Bath Bus Station to a number of cities. Internally, Bath has a network of bus routes run by FirstGroup, with services to surrounding towns and cities. Wessex Bath and the Faresaver Bus company also operate numerous services to surrounding towns. The Bath Bus Company runs open top double-decker bus tours around the city.
The city is connected to Bristol and the sea by the River Avon, navigable via locks by small boats. The river was connected to the River Thames and London by the Kennet and Avon Canal in 1810 via Bath Locks; this waterway – closed for many years, but restored in the last years of the 20th century – is now popular with narrowboat users. Bath is on National Cycle Route 4, with one of Britain's first cycleways, the Bristol and Bath Railway Path, to the west, and an eastern route toward London on the canal towpath. Bath is about 20 mi from Bristol Airport.
Bath is served by the Bath Spa railway station (designed by Isambard Kingdom Brunel), which has regular connections to London Paddington, Bristol Temple Meads, Cardiff Central, Cheltenham, Exeter, Plymouth and Penzance (see Great Western Main Line), and also Westbury, Warminster, Salisbury, Southampton, Portsmouth and Brighton (see Wessex Main Line). Services are provided by First Great Western. There is a suburban station on the main line, Oldfield Park, which has a limited commuter service to Bristol as well as other destinations. Green Park Station was once the terminus of the Midland Railway, and junction for the Somerset and Dorset Joint Railway, whose line, always steam hauled, went under Bear Flat through the Combe Down Tunnel and climbed over the Mendips to serve many towns and villages on its 71 mi run to Bournemouth. This example of an English rural line was closed by Beeching in March 1966. Its Bath station building, now restored, houses shops, small businesses, the Saturday Bath Farmers Market and parking for a supermarket, while the route of the Somerset and Dorset within Bath has been reused for the Two Tunnels Greenway, a shared use path that extends National Cycle Route 24 into the city.
The Bath Tramways Company was introduced in the late 19th century opening on 24 December 1880. The gauge cars were horse-drawn along a route from London Road to the Bath Spa railway station, but the system closed in 1902. It was replaced by electric tram cars on a greatly expanded gauge system that opened in 1904. This eventually extended to 18 mi with routes to Combe Down, Oldfield Park, Twerton, Newton St Loe, Weston and Bathford. There was a fleet of 40 cars, all but 6 being double deck. The first line to close was replaced by a bus service in 1938, and the last went on 6 May 1939. In 2005 an detailed plan was created and presented to the Council to re-introduce trams to Bath (http://www.bathtram.org/) however but this did not go ahead reportedly due to the focus by the Council on the govt supported Busway planned to run from the Newbridge park and ride into the town centre. Part of the justification for the proposed tram re-introduction plan was the pollution fro, vehicles within the city at twice the legal levels, and the heavy traffic congestion due to high car usage.
A transportation study (the Bristol/Bath to South Coast Study) was published in 2004, after being initiated by the Government Office for the South West and Bath and North East Somerset Council. It was undertaken by WSP Global as a result of the de-trunking in 1999 of the A36/A46 trunk road network from Bath to Southampton.
Media.
Bath's local newspaper is the "Bath Chronicle", owned by Local World. Published since 1760, the "Chronicle" was a daily newspaper until mid-September 2007, when it became a weekly.
The BBC Somerset website has featured coverage of news and events within Bath since 2003.
For television, Bath is served by the BBC West studios based in Bristol, and by ITV West (formerly HTV) with studios similarly in Bristol.
Radio stations broadcasting to the city include The Breeze on 107.9FM and Heart West Country (formerly GWR FM) as well as The University of Bath's University Radio Bath, a student-focused radio station available on campus and also online, and Classic Gold 1260 a networked commercial radio station with local programmes.
Bath is sometimes covered by Bristol's local media, including "Bristol Live Magazine".

</doc>
<doc id="41524" url="http://en.wikipedia.org/wiki?curid=41524" title="Renaissance architecture">
Renaissance architecture

Renaissance architecture is the architecture of the period between the early 15th and early 17th centuries in different regions of Europe, demonstrating a conscious revival and development of certain elements of ancient Greek and Roman thought and material culture. Stylistically, Renaissance architecture followed Gothic architecture and was succeeded by Baroque architecture. Developed first in Florence, with Filippo Brunelleschi as one of its innovators, the Renaissance style quickly spread to other Italian cities. The style was carried to France, Germany, England, Russia and other parts of Europe at different dates and with varying degrees of impact.
Renaissance style places emphasis on symmetry, proportion, geometry and the regularity of parts as they are demonstrated in the architecture of classical antiquity and in particular ancient Roman architecture, of which many examples remained. Orderly arrangements of columns, pilasters and lintels, as well as the use of semicircular arches, hemispherical domes, niches and aedicules replaced the more complex proportional systems and irregular profiles of medieval buildings.
Historiography.
The word "Renaissance" derived from the term "la rinascita", which means rebirth, first appeared in Giorgio Vasari's "Vite de' più eccellenti architetti, pittori, et scultori Italiani" The Lives of the Artists, 1550–60.
Although the term Renaissance was used first by the French historian Jules Michelet, it was given its more lasting definition from the Swiss historian Jacob Burckhardt, whose book, "Die Kultur der Renaissance in Italien" 1860,The Civilization of the Renaissance in Italy, 1860, English translation, by SGC Middlemore, in 2 vols., London, 1878) was influential in the development of the modern interpretation of the Italian Renaissance. The folio of measured drawings "Édifices de Rome moderne; ou, Recueil des palais, maisons, églises, couvents et autres monuments" (The Buildings of Modern Rome), first published in 1840 by Paul Letarouilly, also played an important part in the revival of interest in this period.Erwin Panofsky, "Renaissance and Renascences in Western Art", (New York: Harper and Row, 1960) The Renaissance style was recognized by contemporaries in the term "all'antica", or "in the ancient manner" (of the Romans).
Development in Italy – influences.
Italy of the 15th century, and the city of Florence in particular, was home to the Renaissance. It is in Florence that the new architectural style had its beginning, not slowly evolving in the way that Gothic grew out of Romanesque, but consciously brought to being by particular architects who sought to revive the order of a past "Golden Age". The scholarly approach to the architecture of the ancient coincided with the general revival of learning. A number of factors were influential in bringing this about.
Architectural.
Italian architects had always preferred forms that were clearly defined and structural members that expressed their purpose. Many Tuscan Romanesque buildings demonstrate these characteristics, as seen in the Florence Baptistery and Pisa Cathedral.
Italy had never fully adopted the Gothic style of architecture. Apart from the Cathedral of Milan, (influenced by French Rayonnant Gothic), few Italian churches show the emphasis on vertically, the clustered shafts, ornate tracery and complex ribbed vaulting that characterise Gothic in other parts of Europe.
The presence, particularly in Rome, of ancient architectural remains showing the ordered Classical style provided an inspiration to artists at a time when philosophy was also turning towards the Classical.
Political.
In the 15th century, Florence, Venice and Naples extended their power through much of the area that surrounded them, making the movement of artists possible. This enabled Florence to have significant artistic influence in Milan, and through Milan, France.
In 1377, the return of the Pope from the Avignon Papacy and the re-establishment of the Papal court in Rome, brought wealth and importance to that city, as well as a renewal in the importance of the Pope in Italy, which was further strengthened by the Council of Constance in 1417. Successive Popes, especially Julius II, 1503–13, sought to extend the Pope’s temporal power throughout Italy.
Commercial.
In the early Renaissance, Venice controlled sea trade over goods from the East. The large towns of Northern Italy were prosperous through trade with the rest of Europe, Genoa providing a seaport for the goods of France and Spain; Milan and Turin being centers of overland trade, and maintaining substantial metalworking industries.
Trade brought wool from England to Florence, ideally located on the river for the production of fine cloth, the industry on which its wealth was founded. By dominating Pisa, Florence gained a seaport, and also maintained dominance of Genoa.
In this commercial climate, one family in particular turned their attention from trade to the lucrative business of money-lending. The Medici became the chief bankers to the princes of Europe, becoming virtually princes themselves as they did so, by reason of both wealth and influence.
Along the trade routes, and thus offered some protection by commercial interest, moved not only goods but also artists, scientists and philosophers.
Religious.
The return of the Pope Gregory XI from Avignon in September 1377 and the resultant new emphasis on Rome as the center of Christian spirituality, brought about a boom in the building of churches in Rome such as had not taken place for nearly a thousand years. This commenced in the mid 15th century and gained momentum in the 16th century, reaching its peak in the Baroque period. The construction of the Sistine Chapel with its uniquely important decorations and the entire rebuilding of St Peter's, one of Christendom's most significant churches, were part of this process.
In wealthy republican Florence, the impetus for church-building was more civic than spiritual. The unfinished state of the enormous cathedral dedicated to the Blessed Virgin Mary did no honour to the city under her patronage. However, as the technology and finance were found to complete it, the rising dome did credit not only to the Blessed Virgin, its architect and the Church but also the Signoria, the Guilds and the sectors of the city from which the manpower to construct it was drawn. The dome inspired further religious works in Florence.
Philosophic.
The development of printed books, the rediscovery of ancient writings, the expanding of political and trade contacts and the exploration of the world all increased knowledge and the desire for education.
The reading of philosophies that were not based on Christian theology led to the development of Humanism through which it was clear that while God had established and maintained order in the Universe, it was the role of Man to establish and maintain order in Society.
Civil.
Through Humanism, civic pride and the promotion of civil peace and order were seen as the marks of citizenship. This led to the building of structures such as Brunelleschi's Hospital of the Innocents with its elegant colonnade forming a link between the charitable building and the public square, and the Laurentian Library where the collection of books established by the Medici family could be consulted by scholars.
Some major ecclesiastical building works were also commissioned, not by the church, but by guilds representing the wealth and power of the city. Brunelleschi’s dome at Florence Cathedral, more than any other building, belonged to the populace because the construction of each of the eight segments was achieved by a different sector of the city.
Patronage.
As in the Platonic academy of Athens, it was seen by those of Humanist understanding that those people who had the benefit of wealth and education ought to promote the pursuit of learning and the creation of that which was beautiful. To this end, wealthy families—the Medici of Florence, the Gonzaga of Mantua, the Farnese in Rome, the Sforzas in Milan—gathered around them people of learning and ability, promoting the skills and creating employment for the most talented artists and architects of their day.
Architectural theory.
During the Renaissance, architecture became not only a question of practice, but also a matter for theoretical discussion. Printing played a large role in the dissemination of ideas.
Principal phases.
Historians often divide the Renaissance in Italy into three phases. Whereas art historians might talk of an "Early Renaissance" period, in which they include developments in 14th-century painting and sculpture, this is usually not the case in architectural history. The bleak economic conditions of the late 14th century did not produce buildings that are considered to be part of the Renaissance. As a result, the word "Renaissance" among architectural historians usually applies to the period 1400 to ca. 1525, or later in the case of non-Italian Renaissances.
Historians often use the following designations:
Quattrocento.
In the "Quattrocento", concepts of architectural order were explored and rules were formulated. (See- Characteristics of Renaissance Architecture, below.) The study of classical antiquity led in particular to the adoption of Classical detail and ornamentation.
Space, as an element of architecture, was utilised differently from the way it had been in the Middle Ages. Space was organised by proportional logic, its form and rhythm subject to geometry, rather than being created by intuition as in Medieval buildings. The prime example of this is the Basilica di San Lorenzo in Florence by Filippo Brunelleschi (1377–1446).
High Renaissance.
During the "High Renaissance", concepts derived from classical antiquity were developed and used with greater surety. The most representative architect is Bramante (1444–1514) who expanded the applicability of classical architecture to contemporary buildings. His San Pietro in Montorio (1503) was directly inspired by circular Roman temples. He was, however, hardly a slave to the classical forms and it was his style that was to dominate Italian architecture in the 16th century.
Mannerism.
During the "Mannerist" period, architects experimented with using architectural forms to emphasize solid and spatial relationships. The Renaissance ideal of harmony gave way to freer and more imaginative rhythms. The best known architect associated with the Mannerist style was Michelangelo (1475–1564), who is credited with inventing the giant order, a large pilaster that stretches from the bottom to the top of a façade. He used this in his design for the Campidoglio in Rome.
Prior to the 20th century, the term "Mannerism" had negative connotations, but it is now used to describe the historical period in more general non-judgemental terms.
From Renaissance to Baroque.
As the new style of architecture spread out from Italy, most other European countries developed a sort of proto-Renaissance style, before the construction of fully formulated Renaissance buildings. Each country in turn then grafted its own architectural traditions to the new style, so that Renaissance buildings across Europe are diversified by region.
Within Italy the evolution of Renaissance architecture into Mannerism, with widely diverging tendencies in the work of Michelangelo and Giulio Romano and Andrea Palladio, led to the Baroque style in which the same architectural vocabulary was used for very different rhetoric.
Outside Italy, Baroque architecture was more widespread and fully developed than the Renaissance style, with significant buildings as far afield as Mexico and the Philippines.
Characteristics.
The obvious distinguishing features of Classical Roman architecture were adopted by Renaissance architects. However, the forms and purposes of buildings had changed over time, as had the structure of cities. Among the earliest buildings of the reborn Classicism were churches of a type that the Romans had never constructed. Neither were there models for the type of large city dwellings required by wealthy merchants of the 15th century. Conversely, there was no call for enormous sporting fixtures and public bath houses such as the Romans had built. The ancient orders were analysed and reconstructed to serve new purposes.
Plan.
The plans of Renaissance buildings have a square, symmetrical appearance in which proportions are usually based on a module. Within a church, the module is often the width of an aisle. The need to integrate the design of the plan with the façade was introduced as an issue in the work of Filippo Brunelleschi, but he was never able to carry this aspect of his work into fruition. The first building to demonstrate this was St. Andrea in Mantua by Alberti. The development of the plan in secular architecture was to take place in the 16th century and culminated with the work of Palladio.
Façade.
Façades are symmetrical around their vertical axis. Church façades are generally surmounted by a pediment and organised by a system of pilasters, arches and entablatures. The columns and windows show a progression towards the centre. One of the first true Renaissance façades was the Cathedral of Pienza (1459–62), which has been attributed to the Florentine architect Bernardo Gambarelli (known as Rossellino) with Alberti perhaps having some responsibility in its design as well.
Domestic buildings are often surmounted by a cornice. There is a regular repetition of openings on each floor, and the centrally placed door is marked by a feature such as a balcony, or rusticated surround. An early and much copied prototype was the façade for the Palazzo Rucellai (1446 and 1451) in Florence with its three registers of pilasters
Columns and pilasters.
The Roman orders of columns are used:- Tuscan, Doric, Ionic, Corinthian and Composite. The orders can either be structural, supporting an arcade or architrave, or purely decorative, set against a wall in the form of pilasters. During the Renaissance, architects aimed to use columns, pilasters, and entablatures as an integrated system. One of the first buildings to use pilasters as an integrated system was in the Old Sacristy (1421–1440) by Brunelleschi.
Arches.
Arches are semi-circular or (in the Mannerist style) segmental. Arches are often used in arcades, supported on piers or columns with capitals. There may be a section of entablature between the capital and the springing of the arch. Alberti was one of the first to use the arch on a monumental scale at the St. Andrea in Mantua.
Vaults.
Vaults do not have ribs. They are semi-circular or segmental and on a square plan, unlike the Gothic vault which is frequently rectangular. The barrel vault is returned to architectural vocabulary as at the St. Andrea in Mantua.
Domes.
The dome is used frequently, both as a very large structural feature that is visible from the exterior, and also as a means of roofing smaller spaces where they are only visible internally. After the success of the dome in Brunelleschi’s design for the Basilica di Santa Maria del Fiore and its use in Bramante’s plan for St. Peter's Basilica (1506) in Rome, the dome became an indispensable element in church architecture and later even for secular architecture, such as Palladio's Villa Rotonda.
Ceilings.
Roofs are fitted with flat or coffered ceilings. They are not left open as in Medieval architecture. They are frequently painted or decorated.
Doors.
Doors usually have square lintels. They may be set within an arch or surmounted by a triangular or segmental pediment.
Openings that do not have doors are usually arched and frequently have a large or decorative keystone.
Windows.
Windows may be paired and set within a semi-circular arch. They may have square lintels and triangular or segmental pediments, which are often used alternately. Emblematic in this respect is the Palazzo Farnese in Rome, begun in 1517.
In the Mannerist period the “Palladian” arch was employed, using a motif of a high semi-circular topped opening flanked with two lower square-topped openings. Windows are used to bring light into the building and in domestic architecture, to give views. Stained glass, although sometimes present, is not a feature.
Walls.
External walls are generally constructed of brick, rendered, or faced with stone in highly finished ashlar masonry, laid in straight courses. The corners of buildings are often emphasised by rusticated quoins. Basements and ground floors were often rusticated, as at the Palazzo Medici Riccardi (1444–1460) in Florence. Internal walls are smoothly plastered and surfaced with lime wash. For more formal spaces, internal surfaces are decorated with frescoes.
Details.
Courses, mouldings and all decorative details are carved with great precision. Studying and mastering the details of the ancient Romans was one of the important aspects of Renaissance theory. The different orders each required different sets of details. Some architects were stricter in their use of classical details than others, but there was also a good deal of innovation in solving problems, especially at corners. Mouldings stand out around doors and windows rather than being recessed, as in Gothic Architecture. Sculptured figures may be set in niches or placed on plinths. They are not integral to the building as in Medieval architecture.
Development in Italy – Early Renaissance.
The leading architects of the Early Renaissance or Quattrocento were Brunelleschi, Michelozzo and Alberti.
Brunelleschi.
The person generally credited with bringing about the Renaissance view of architecture is Filippo Brunelleschi, (1377–1446). The underlying feature of the work of Brunelleschi was "order".
In the early 15th century, Brunelleschi began to look at the world to see what the rules were that governed one's way of seeing. He observed that the way one sees regular structures such as the Baptistery of Florence and the tiled pavement surrounding it follows a mathematical order—linear perspective.
The buildings remaining among the ruins of ancient Rome appeared to respect a simple mathematical order in the way that Gothic buildings did not. One incontrovertible rule governed all Ancient Roman architecture—a semi-circular arch is exactly twice as wide as it is high. A fixed proportion with implications of such magnitude occurred nowhere in Gothic architecture. A Gothic pointed arch could be extended upwards or flattened to any proportion that suited the location. Arches of differing angles frequently occurred within the same structure. No set rules of proportion applied.
From the observation of the architecture of Rome came a desire for symmetry and careful proportion in which the form and composition of the building as a whole and all its subsidiary details have fixed relationships, each section in proportion to the next, and the architectural features serving to define exactly what those rules of proportion are. Brunelleschi gained the support of a number of wealthy Florentine patrons, including the Silk Guild and Cosimo de' Medici.
Florence Cathedral.
Brunelleschi's first major architectural commission was for the enormous brick dome which covers the central space of Florence's cathedral, designed by Arnolfo di Cambio in the 14th century but left unroofed. While often described as the first building of the Renaissance, Brunelleschi's daring design utilizes the pointed Gothic arch and Gothic ribs that were apparently planned by Arnolfio. It seems certain, however, that while stylistically Gothic, in keeping with the building it surmounts, the dome is in fact structurally influenced by the great dome of Ancient Rome, which Brunelleschi could hardly have ignored in seeking a solution. This is the dome of the Pantheon, a circular temple, now a church.
Inside the Pantheon's single-shell concrete dome is coffering which greatly decreases the weight. The vertical partitions of the coffering effectively serve as ribs, although this feature does not dominate visually. At the apex of the Pantheon's dome is an opening, 8 meters across. Brunelleschi was aware that a dome of enormous proportion could in fact be engineered without a keystone. The dome in Florence is supported by the eight large ribs and sixteen more internal ones holding a brick shell, with the bricks arranged in a herringbone manner. Although the techniques employed are different, in practice both domes comprise a thick network of ribs supporting very much lighter and thinner infilling. And both have a large opening at the top.
San Lorenzo.
The new architectural philosophy of the Renaissance is best demonstrated in the churches of San Lorenzo, and Santo Spirito in Florence. Designed by Brunelleschi in about 1425 and 1428 respectively, both have the shape of the Latin cross. Each has a modular plan, each portion being a multiple of the square bay of the aisle. This same formula controlled also the vertical dimensions. In the case of Santo Spirito, which is entirely regular in plan, transepts and chancel are identical, while the nave is an extended version of these. In 1434 Brunelleschi designed the first Renaissance centrally planned building, Santa Maria degli Angeli of Florence. It is composed of a central octagon surrounded by a circuit of eight smaller chapels. From this date onwards numerous churches were built in variations of these designs.
Michelozzo.
Michelozzo Michelozzi (1396–1472), was another architect under patronage of the Medici family, his most famous work being the Palazzo Medici Riccardi, which he was commissioned to design for Cosimo de' Medici in 1444. A decade later he built the Villa Medici at Fiesole. Among his other works for Cosimo are the library at the Convent of San Marco, Florence. He went into exile in Venice for a time with his patron. He was one of the first architects to work in the Renaissance style outside Italy, building a palace at Dubrovnik.
The Palazzo Medici Riccardi is Classical in the details of its pedimented windows and recessed doors, but, unlike the works of Brunelleschi and Alberti, there are no "orders" of columns in evidence. Instead, Michelozzo has respected the Florentine liking for rusticated stone. He has seemingly created three orders out of the three defined rusticated levels, the whole being surmounted by an enormous Roman-style cornice which juts out over the street by 2.5 meters.
Alberti.
Leon Battista Alberti, born in Genoa (1402–1472), was an important Humanist theoretician and designer whose book on architecture "De re Aedificatoria" was to have lasting effect. An aspect of Humanism was an emphasis of the anatomy of nature, in particular the human form, a science first studied by the Ancient Greeks. Humanism made man the measure of things. Alberti perceived the architect as a person with great social responsibilities.
He designed a number of buildings, but unlike Brunelleschi, he did not see himself as a builder in a practical sense and so left the supervision of the work to others. Miraculously, one of his greatest designs, that of the Church of Sant'Andrea in Mantua, was brought to completion with its character essentially intact. Not so the church of San Francesco in Rimini, a rebuilding of a Gothic structure, which, like Sant'Andrea, was to have a façade reminiscent of a Roman triumphal arch. This was left sadly incomplete.
Sant'Andrea is an extremely dynamic building both without and within. Its triumphal façade is marked by extreme contrasts. The projection of the order of pilasters that define the architectural elements, but are essentially non-functional, is very shallow. This contrasts with the gaping deeply recessed arch which makes a huge portico before the main door. The size of this arch is in direct contrast to the two low square-topped openings that frame it. The light and shade play dramatically over the surface of the building because of the shallowness of its mouldings and the depth of its porch. In the interior Alberti has dispensed with the traditional nave and aisles. Instead there is a slow and majestic progression of alternating tall arches and low square doorways, repeating the "triumphal arch" motif of the façade.
Two of Alberti’s best known buildings are in Florence, the Palazzo Rucellai and at Santa Maria Novella. For the palace, Alberti applied the classical orders of columns to the façade on the three levels, 1446–51. At Santa Maria Novella he was commissioned to finish the decoration of the façade. He completed the design in 1456 but the work was not finished until 1470.
The lower section of the building had Gothic niches and typical polychrome marble decoration. There was a large ocular window in the end of the nave which had to be taken into account. Alberti simply respected what was already in place, and the Florentine tradition for polychrome that was well established at the Baptistery of San Giovanni, the most revered building in the city. The decoration, being mainly polychrome marble, is mostly very flat in nature, but a sort of order is established by the regular compartments and the circular motifs which repeat the shape of the round window. For the first time, Alberti linked the lower roofs of the aisles to nave using two large scrolls. These were to become a standard Renaissance device for solving the problem of different roof heights and bridge the space between horizontal and vertical surfaces.
Spread of the Renaissance in Italy.
In the 15th century the courts of certain other Italian states became centres for spreading of Renaissance philosophy, art and architecture.
In Mantua at the court of the Gonzaga, Alberti designed two churches, the Basilica of Sant'Andrea and San Sebastiano.
Urbino was an important centre with the ancient ducal palace being extended for Federico da Montefeltro in the mid 15th century. The duke employed Luciano Laurana from Dalmatia, renowned for his expertise at fortification. The design incorporates much of the earlier medieval building and includes an unusual turreted three-storeyed façade. Laurana was assisted by Francesco di Giorgio Martini. Later parts of the building are clearly Florentine in style, particularly the inner courtyard, but it is not known who the designer was.
Ferrara, under the Este, was expanded in the late fifteenth century, with several new palaces being built such as the Palazzo dei Diamanti and Palazzo Schifanoia for Borso d'Este. In Milan, under the Visconti, the Certosa di Pavia was completed, and then later under the Sforza, the Castello Sforzesco was built.
In Venice, San Zaccaria received its Renaissance façade at the hands of Antonio Gambello and Mauro Codussi, begun in the 1480s. Giovanni Maria Falconetto, the Veronese architect-sculptor, introduced Renaissance architecture to Padua with the Loggia Cornaro in the garden of Alvise Cornaro.
In southern Italy, Renaissance masters were called to Naples by Alfonso V of Aragon after his conquest of the Kingdom of Naples. The most notable examples of Renaissance architecture in that city are the Cappella Caracciolo, attributed to Bramante, and the Palazzo Orsini di Gravina, built by Gabriele d'Angelo between 1513 and 1549.
High Renaissance.
In the late 15th century and early 16th century, architects such as Bramante, Antonio da Sangallo the Younger and others showed a mastery of the revived style and ability to apply it to buildings such as churches and city palazzo which were quite different from the structures of ancient times. The style became more decorated and ornamental, statuary, domes and cupolas becoming very evident.
The architectural period is known as the "High Renaissance" and coincides with the age of Leonardo, Michelangelo and Raphael.
Bramante.
Donato Bramante, (1444–1514), was born in Urbino and turned from painting to architecture, finding his first important patronage under Ludovico Sforza, Duke of Milan, for whom he produced a number of buildings over 20 years. After the fall of Milan to the French in 1499, Bramante travelled to Rome where he achieved great success under papal patronage.
Bramante’s finest architectural achievement in Milan is his addition of crossing and choir to the abbey church of Santa Maria delle Grazie (Milan). This is a brick structure, the form of which owes much to the Northern Italian tradition of square domed baptisteries. The new building is almost centrally planned, except that, because of the site, the chancel extends further than the transept arms. The hemispherical dome, of approximately 20 metres across, rises up hidden inside an octagonal drum pierced at the upper level with arched classical openings. The whole exterior has delineated details decorated with the local terracotta ornamentation.
In Rome Bramante created what has been described as "a perfect architectural gem", the Tempietto in the Cloister of San Pietro in Montorio. This small circular temple marks the spot where St Peter was martyred and is thus the most sacred site in Rome. The building adapts the style apparent in the remains of the Temple of Vesta, the most sacred site of Ancient Rome. It is enclosed by and in spatial contrast with the cloister which surrounds it. As approached from the cloister, as in the picture above, it is seen framed by an arch and columns, the shape of which are echoed in its free-standing form.
Bramante went on to work at the Vatican where he designed the impressive Cortili of St. Damaso and of the Belvedere. In 1506 Bramante’s design for Pope Julius II’s rebuilding of St. Peter’s Basilica was selected, and the foundation stone laid. After Bramante’s death and many changes of plan, Michelangelo, as chief architect, reverted to something closer to Bramante’s original proposal. See below- Michelangelo.
Sangallo.
Antonio da Sangallo the Younger, (1485–1546), was one of a family of military engineers. His uncle, Giuliano da Sangallo was one of those who submitted a plan for the rebuilding of St Peter’s and was briefly a co-director of the project, with Raphael.
Antonio da Sangallo also submitted a plan for St Peter’s and became the chief architect after the death of Raphael, to be succeeded himself by Michelangelo.
His fame does not rest upon his association with St Peter’s but in his building of the Farnese Palace, “the grandest palace of this period”, started in 1530. The impression of grandness lies in part in its sheer size, (56 m long by 29.5 meters high) and in its lofty location overlooking a broad piazza. It is also a building of beautiful proportion, unusual for such a large and luxurious house of the date in having been built principally of stuccoed brick, rather than of stone. Against the smooth pink-washed walls the stone quoins of the corners, the massive rusticated portal and the stately repetition of finely detailed windows give a powerful effect, setting a new standard of elegance in palace-building. The upper of the three equally sized floors was added by Michelangelo. It is probably just as well that this impressive building is of brick; the travertine for its architectural details came not from a quarry, but from the Colosseum.
Raphael.
Raphael, (1483–1520), Urbino, trained under Perugino in Perugia before moving to Florence, was for a time the chief architect for St. Peter’s, working in conjunction with Antonio Sangallo. He also designed a number of buildings, most of which were finished by others. His single most influential work is the Palazzo Pandolfini in Florence with its two stories of strongly articulated windows of a "tabernacle" type, each set around with ordered pilasters, cornice and alternate arched and triangular pediments.
Mannerism.
Mannerism in architecture was marked by widely diverging tendencies in the work of Michelangelo, Giulio Romano, Baldassare Peruzzi and Andrea Palladio, that led to the Baroque style in which the same architectural vocabulary was used for very different rhetoric.
Peruzzi.
Baldassare Peruzzi, (1481–1536), was an architect born in Siena, but working in Rome, whose work bridges the High Renaissance and the Mannerist.
His Villa Farnesina of 1509 is a very regular monumental cube of two equal stories, the bays being strongly articulated by orders of pilasters. The building is unusual for its frescoed walls.
Peruzzi’s most famous work is the Palazzo Massimo alle Colonne in Rome. The unusual features of this building are that its façade curves gently around a curving street. It has in its ground floor a dark central portico running parallel to the street, but as a semi enclosed space, rather than an open loggia. Above this rise three undifferentiated floors, the upper two with identical small horizontal windows in thin flat frames which contrast strangely with the deep porch, which has served, from the time of its construction, as a refuge to the city’s poor.
Giulio Romano.
Giulio Romano (1499–1546), was a pupil of Raphael, assisting him on various works for the Vatican. Romano was also a highly inventive designer, working for Federico II Gonzaga at Mantua on the Palazzo Te, (1524–1534), a project which combined his skills as architect, sculptor and painter. In this work, incorporating garden grottoes and extensive frescoes, he uses illusionistic effects, surprising combinations of architectural form and texture, and the frequent use of features that seem somewhat disproportionate or out of alignment. The total effect is eerie and disturbing. Ilan Rachum cites Romano as "“one of the first promoters of Mannerism”".
Michelangelo.
Michelangelo Buonarroti (1475–1564) was one of the creative giants whose achievements mark the High Renaissance. He excelled in each of the fields of painting, sculpture and architecture and his achievements brought about significant changes in each area. His architectural fame lies chiefly in two buildings: the interiors of the Laurentian Library and its lobby at the monastery of San Lorenzo in Florence, and St Peter's Basilica in Rome.
St Peter's was "the greatest creation of the Renaissance", and a great number of architects contributed their skills to it. But at its completion, there was more of Michelangelo’s design than of any other architect, before or after him.
St Peter's.
The plan that was accepted at the laying of the foundation stone in 1506 was that by Bramante. Various changes in plan occurred in the series of architects that succeeded him, but Michelangelo, when he took over the project in 1546, reverted to Bramante’s Greek-cross plan and redesigned the piers, the walls and the dome, giving the lower weight-bearing members massive proportions and eliminating the encircling aisles from the chancel and identical transept arms. Helen Gardner says: "Michelangelo, with a few strokes of the pen, converted its snowflake complexity into a massive, cohesive unity."
Michelangelo’s dome was a masterpiece of design using two masonry shells, one within the other and crowned by a massive lantern supported, as at Florence, on ribs. For the exterior of the building he designed a giant order which defines every external bay, the whole lot being held together by a wide cornice which runs unbroken like a rippling ribbon around the entire building.
There is a wooden model of the dome, showing its outer shell as hemispherical. When Michelangelo died in 1564, the building had reached the height of the drum. The architect who succeeded Michelangelo was Giacomo della Porta. The dome, as built, has a much steeper projection than the dome of the model. It is generally presumed that it was della Porta who made this change to the design, to lessen the outward thrust. But, in fact it is unknown who it was that made this change, and it equally possible, and a stylistic likelihood that the person who decided upon the more dynamic outline was Michelangelo himself, at some time during the years that he supervised the project.
Laurentian Library.
Michelangelo was at his most Mannerist in the design of the vestibule of the Laurentian Library, also built by him to house the Medici collection of books at the convent of San Lorenzo in Florence, the same San Lorenzo’s at which Brunelleschi had recast church architecture into a Classical mold and established clear formula for the use of Classical orders and their various components.
Michelangelo takes all Brunelleschi’s components and bends them to his will. The Library is upstairs. It is a long low building with an ornate wooden ceiling, a matching floor and crowded with corrals finished by his successors to Michelangelo’s design. But it is a light room, the natural lighting streaming through a long row of windows that appear positively crammed between the order of pilasters that march along the wall. The vestibule, on the other hand, is tall, taller than it is wide and is crowded by a large staircase that pours out of the library in what Pevsner refers to as a “flow of lava”, and bursts in three directions when it meets the balustrade of the landing. It is an intimidating staircase, made all the more so because the rise of the stairs at the center is steeper than at the two sides, fitting only eight steps into the space of nine.
The space is crowded and it is to be expected that the wall spaces would be divided by pilasters of low projection. But Michelangelo has chosen to use paired columns, which, instead of standing out boldly from the wall, he has sunk deep into recesses within the wall itself. In San Lorenzo's church nearby, Brunelleschi used little scrolling console brackets to break the strongly horizontal line of the course above the arcade. Michelangelo has borrowed Brunelleschi’s motifs and stood each pair of sunken columns on a pair of twin console brackets. Pevsner says the "“Laurenziana… reveals Mannerism in its most sublime architectural form”".
Giacomo della Porta.
Giacomo della Porta, (c.1533–1602), was famous as the architect who made the dome of St Peter’s Basilica a reality. The change in outline between the dome as it appears in the model and the dome as it was built, has brought about speculation as to whether the changes originated with della Porta or with Michelangelo himself.
Della Porta spent nearly all his working life in Rome, designing villas, palazzi and churches in the Mannerist style. One of his most famous works is the façade of the Church of the Gesù, a project that he inherited from his teacher Jacopo Barozzi da Vignola. Most characteristics of the original design are maintained, subtly transformed to give more weight to the central section, where della Porta uses, among other motifs, a low triangular pediment overlaid on a segmental one above the main door. The upper storey and its pediment give the impression of compressing the lower one. The center section, like that of Sant'Andrea at Mantua, is based on the Triumphal Arch, but has two clear horizontal divisions like Santa Maria Novella. See Alberti above. The problem of linking the aisles to the nave is solved using Alberti’s scrolls, in contrast to Vignola’s solution which provided much smaller brackets and four statues to stand above the paired pilasters, visually weighing down the corners of the building. The influence of the design may be seen in Baroque churches throughout Europe.
Andrea Palladio.
Andrea Palladio, (1508–80), "the most influential architect of the whole Renaissance"', was, as a stonemason, introduced to Humanism by the poet Giangiorgio Trissino. His first major architectural commission was the rebuilding of the Basilica Palladiana at Vicenza, in the Veneto where he was to work most of his life.
Palladio was to transform the architectural style of both palaces and churches by taking a different perspective on the notion of Classicism. While the architects of Florence and Rome looked to structures like the Colosseum and the Arch of Constantine to provide formulae, Palladio looked to classical temples with their simple peristyle form. When he used the “triumphal arch” motif of a large arched opening with lower square-topped opening on either side, he invariably applied it on a small scale, such as windows, rather than on a large scale as Alberti used it at Sant’Andrea’s. This Ancient Roman motif is often referred to as the "Palladian Arch".
The best known of Palladio’s domestic buildings is Villa Capra, otherwise known as "la Rotonda", a centrally planned house with a domed central hall and four identical façades, each with a temple-like portico like that of the Pantheon in Rome. At the Villa Cornaro, the projecting portico of the north façade and recessed loggia of the garden façade are of two ordered stories, the upper forming a balcony.
Like Alberti, della Porta and others, in the designing of a church façade, Palladio was confronted by the problem of visually linking the aisles to the nave while maintaining and defining the structure of the building. Palladio’s solution was entirely different from that employed by della Porta. At the church of San Giorgio Maggiore in Venice he overlays a tall temple, its columns raised on high plinths, over another low wide temple façade, its columns rising from the basements and its narrow lintel and pilasters appearing behind the giant order of the central nave.
Progression from Early Renaissance through to Baroque.
In Italy, there appears to be a seamless progression from Early Renaissance architecture through the High Renaissance and Mannerist to the Baroque style. Pevsner comments about the vestibule of the Laurentian Library that it "has often been said that the motifs of the walls show Michelangelo as the father of the Baroque".
While continuity may be the case in Italy, it was not necessarily the case elsewhere. The adoption of the Renaissance style of architecture was slower in some areas than in others, as may be seen in England, for example. Indeed, as Pope Julius II was having the ancient Basilica of St. Peter’s demolished to make way for the new, Henry VII of England was adding a glorious new chapel in the Perpendicular Gothic style to Westminster Abbey.
Likewise, the style that was to become known as Baroque evolved in Italy in the early 17th century, at about the time that the first fully Renaissance buildings were constructed at Greenwich and Whitehall in England, after a prolonged period of experimentation with Classical motifs applied to local architectural forms, or conversely, the adoption of Renaissance structural forms in the broadest sense with an absence of the formulae that governed their use. While the English were just discovering what the rules of Classicism were, the Italians were experimenting with methods of breaking them. In England, following the Restoration of the Monarchy in 1660, the architectural climate changed, and taste moved in the direction of the Baroque. Rather than evolving, as it did in Italy, it arrived fully fledged.
In a similar way, in many parts of Europe that had few purely classical and ordered buildings like Brunelleschi’s Santo Spirito and Michelozzo’s Medici Riccardi Palace, Baroque architecture appeared almost unheralded, on the heels of a sort of Proto-Renaissance local style. The spread of the Baroque and its replacement of traditional and more conservative Renaissance architecture was particularly apparent in the building of churches as part of the Counter Reformation.
Spread in Europe.
The 16th century saw the economic and political ascendancy of France and Spain, and then later of Holland, England, Germany and Russia. The result was that these places began to import the Renaissance style as indicators of their new cultural position. This also meant that it was not until about 1500 and later that signs of Renaissance architectural style began to appear outside Italy.
Though Italian architects were highly sought after, such as Sebastiano Serlio in France, Aristotile Fioravanti in Russia, and Francesco Fiorentino in Poland, soon, non-Italians were studying Italian architecture and translating it into their own idiom. These included Philibert de l'Orme (1510–1570) in France, Juan Bautista de Toledo (died: 1567) in Spain, Inigo Jones (1573–1652) in England and Elias Holl (1573–1646) in Germany.
Books or ornament prints with engraved illustrations demonstrating plans and ornament were very important in spreading Renaissance styles in Northern Europe, with among the most important authors being Androuet du Cerceau in France, and Hans Vredeman de Vries in the Netherlands, and Wendel Dietterlin, author of "Architectura" (1593–94) in Germany.
Croatia.
In the 15th century, Croatia was divided into three states – the northern and central part of Croatia and Slavonia were in union with the Kingdom of Hungary, while Dalmatia, with the exception of independent Dubrovnik, was under the rule of the Venetian Republic. The Cathedral of St.James in Šibenik, was begun in 1441 in the Gothic style by Giorgio da Sebenico "(Juraj Dalmatinac)". Its unusual construction does not use mortar, the stone blocks, pilasters and ribs being bonded with joints and slots in the way that was usual in wooden constructions. In 1477 the work was unfinished, and continued under Niccolò di Giovanni Fiorentino who respected the mode of construction and the plan of the former architect, but continued the work which includes the upper windows, the vaults and the dome, in the Renaissance style. The combination of a high barrel vault with lower half-barrel vaults over the aisles the gives the façade its distinctive trefoil shape, the first of this type in the region. The cathedral was listed as a UNESCO World Heritage List in 2001.
Kingdom of Hungary.
One of the earliest places to be influenced by the Renaissance style of architecture was the Kingdom of Hungary. The style appeared following the marriage of King Matthias Corvinus and Beatrice of Naples in 1476. Many Italian artists, craftsmen and masons arrived at Buda with the new queen. Important remains of the Early Renaissance summer palace of King Matthias can be found in Visegrád. The Ottoman conquest of Hungary after 1526 cut short the development of Renaissance architecture in the country and destroyed its most famous examples. Today, the only completely preserved work of Hungarian Renaissance architecture is the Bakócz Chapel (commissioned by the Hungarian cardinal Tamás Bakócz), now part of the Esztergom Basilica.
Russia.
Prince Ivan III introduced Renaissance architecture to Russia by inviting a number of architects from Italy, who brought new construction techniques and some Renaissance style elements with them, while in general following the traditional designs of the Russian architecture. In 1475 the Bolognese architect Aristotele Fioravanti came to rebuild the Cathedral of the Dormition in the Moscow Kremlin, damaged in an earthquake. Fioravanti was given the 12th-century Vladimir Cathedral as a model, and produced a design combining traditional Russian style with a Renaissance sense of spaciousness, proportion and symmetry.
In 1485 Ivan III commissioned the building of a royal Terem Palace within the Kremlin, with Aloisio da Milano being the architect of the first three floors. Aloisio da Milano, as well as the other Italian architects, also greatly contributed to the construction of the Kremlin walls and towers. The small banqueting hall of the Russian Tsars, called the Palace of Facets because of its facetted upper story, is the work of two Italians, Marco Ruffo and Pietro Solario, and shows a more Italian style. In 1505, an Italian known in Russia as Aleviz Novyi built 12 churches for Ivan III, including the Cathedral of the Archangel, a building remarkable for the successful blending of Russian tradition, Orthodox requirements and Renaissance style.
Poland.
Polish Renaissance architecture is divided into three periods:
The First period (1500–50), is the so-called "Italian". Most of Renaissance buildings were building of this time were by Italian architects, mainly from Florence including Francesco Fiorentino and Bartolomeo Berrecci (Wawel Courtyard, Sigismund's Chapel).
In the Second period (1550–1600), Renaissance architecture became more common, with the beginnings of Mannerist and under the influence of the Netherlands, particularly in Pomerania. Buildings include the New Cloth Hall in Kraków and city halls in Tarnów, Sandomierz, Chełm (demolished) and most famously in Poznań.
In the Third period (1600–50), the rising power of Jesuits and Counter Reformation gave impetus to the development of Mannerist architecture and Baroque.
France.
During the early years of the 16th century the French were involved in wars in northern Italy, bringing back to France not just the Renaissance art treasures as their war booty, but also stylistic ideas. In the Loire Valley a wave of building was carried and many Renaissance chateaux appeared at this time, the earliest example being the Château d'Amboise (c. 1495) in which Leonardo da Vinci spent his last years. The style became dominant under Francis I (See Châteaux of the Loire Valley).
Netherlands/Flanders.
As in painting, Renaissance architecture took some time to reach the Netherlands and did not entirely supplant the Gothic elements. An architect directly influenced by the Italian masters was Cornelis Floris de Vriendt, who designed the city hall of Antwerpen, finished in 1564. The style sometimes known as "Antwerp Mannerism", keeping a similar overall structure to late-Gothic buildings, but with larger windows and much florid decoration and detailing in Renaissance styles, was widely influential across Northern Europe, for example in Elizabethan architecture, and is part of the wider movement of Northern Mannerism.
In the early 17th century Dutch Republic, Hendrick de Keyser played an important role in developing the Amsterdam Renaissance style, which has local characteristics including the prevalence of tall narrow town-houses, the "trapgevel" or Dutch gable and the employment of decorative triangular pediments over doors and windows in which the apex rises much more steeply than in most other Renaissance architecture, but in keeping with the profile of the gable. Carved stone details are often of low profile, in strapwork resembling leatherwork, a stylistic feature originating in the School of Fontainebleau. This feature was exported to England.
Germany.
The Renaissance in Germany was inspired first by German philosophers and artists such as Albrecht Dürer and Johannes Reuchlin who visited Italy. Important early examples of this period are especially the Landshut Residence, the Castle in Heidelberg, Johannisburg Palace in Aschaffenburg, the City Hall and Fugger Houses in Augsburg and St. Michael in Munich. A particular form of Renaissance architecture in Germany is the Weser Renaissance, with prominent examples such as the City Hall of Bremen and the Juleum in Helmstedt.
In July 1567 the city council of Cologne approved a design in the Renaissance style by Wilhelm Vernukken for a two storied loggia for Cologne City Hall. St Michael in Munich is the largest Renaissance church north of the Alps. It was built by Duke William V of Bavaria between 1583 and 1597 as a spiritual center for the Counter Reformation and was inspired by the Church of il Gesù in Rome. The architect is unknown. Many examples of Brick Renaissance buildings can be found in Hanseatic old towns, such as Stralsund, Wismar, Lübeck, Lüneburg, Friedrichstadt and Stade. Notable German Renaissance architects include Friedrich Sustris, Benedikt Rejt, Abraham van den Blocke, Elias Holl and Hans Krumpper.
England.
Renaissance architecture arrived in England during the reign of Elizabeth I, having first spread through the Low countries where among other features it acquired versions of the Dutch gable, and Flemish strapwork in geometric designs adorning the walls. The new style tended to manifest itself in large square tall houses such as Longleat House.
The first great exponent of Italian Renaissance architecture in England was Inigo Jones (1573–1652), who had studied architecture in Italy where the influence of Palladio was very strong. Jones returned to England full of enthusiasm for the new movement and immediately began to design such buildings as the Queen's House at Greenwich in 1616 and the Banqueting House at Whitehall three years later. These works, with their clean lines, and symmetry were revolutionary in a country still enamoured with mullion windows, crenellations and turrets.
Spain.
In Spain, Renaissance began to be grafted to Gothic forms in the last decades of the 15th century. The new style is called Plateresque, because of the extremely decorated façade, that brought to the mind the decorative motifs of the intricately detailed work of silversmiths, the "Plateros". Classical orders and candelabra motifs ("a candelieri") combined freely into symmetrical wholes.
From the mid-sixteenth century, under such architects as Pedro Machuca, Juan Bautista de Toledo and Juan de Herrera there was a closer adherence to the art of ancient Rome, sometimes anticipating Mannerism, examples of which include the palace of Charles V in Granada and the Escorial.
Portugal.
As in Spain, the adoption of the Renaissance style in Portugal was gradual. The so-called Manueline style (c. 1490–1535) married Renaissance elements to Gothic structures with the superficial application of exuberant ornament similar to the Isabelline Gothic of Spain. Examples of Manueline include the Belém Tower, a defensive building of Gothic form decorated with Renaissance-style loggias, and the Jerónimos Monastery, with Renaissance ornaments decorating portals, columns and cloisters.
The first "pure" Renaissance structures appear under King John III, like the Chapel of Nossa Senhora da Conceição in Tomar (1532–40), the "Porta Especiosa" of Coimbra Cathedral and the Graça Church at Évora (c. 1530–1540), as well as the cloisters of the Cathedral of Viseu (c. 1528–1534) and Convent of Christ in Tomar (John III Cloisters, 1557–1591). The Lisbon buildings of São Roque Church (1565–87) and the Mannerist Monastery of São Vicente de Fora (1582–1629), strongly influenced religious architecture in both Portugal and its colonies in the next centuries.
Scandinavia.
The Renaissance architecture that found its way to Scandinavia was influenced by the Flemish architecture, and included high gables and a castle air as demonstrated in the architecture of Frederiksborg Palace. Consequently much of the Neo-Renaissance to be found in the Scandinavian countries is derived from this source.
In Denmark, Renaissance architecture thrived during the reigns of Frederick II and especially Christian IV. Inspired by the French castles of the times, Flemish architects designed masterpieces such as Kronborg Castle in Helsingør and Frederiksborg Palace in Hillerod. Frederiksborg Palace (1602–1620) in Hillerod is the largest Renaissance palace in Scandinavia.
Elsewhere, in Sweden, with Gustav Vasa's seizure of power and the onset of the Protestant reformation, church construction and aristocratic building projects came to a near standstill. During this time period, severaö magnificent so-called Vasa castles appeared. They were erected at strategic locations to control the country as well as to accommodate the travelling royal court. Gripsholm Castle, Kalmar Castle and Vadstena Castle are known for their fusion of medieval elements with Renaissance architecture.
The architecture of Norway was influenced partly by the occurrence of the plague during the Renaissance era. After the Black Death, monumental construction in Norway came to a standstill. There are few examples of Renaissance architecture in Norway, the most prominent being renovations to the medieval Rosenkrantz Tower in Bergen, Barony Rosendal in Hardanger, and the contemporary Austrat manor near Trondheim, and parts of Akershus Fortress.
There is little evidence of Renaissance influence in Finnish architecture.
Baltic States.
The Renaissance arrived late in what is today Estonia, Latvia and Lithuania, the so-called Baltic States, and did not make a great imprint architecturally. It was a politically tumultuous time, marked by the decline of the State of the Teutonic Order and the Livonian War.
In Estonia, artistic influences came from Dutch, Swedish and Polish sources. The House of the Blackheads in Tallinn with a façade designed by Arent Passer, is the only truly Renaissance building in the country that has survived more or less intact. Significantly for these troubled times, the only other examples are purely military buildings, such as the "Fat Margaret" cannon tower, also in Tallinn.
Latvian Renaissance architecture was influenced by Polish-Lithuanian and Dutch style, with Mannerism following from Gothic without intermediaries. St. John's Church in the Latvian capital of Riga is example of an earlier Gothic church which was reconstructed in 1587–89 by the Dutch architect Gert Freze (Joris Phraeze). The prime example of Renaissance architecture in Latvia is the heavily decorated House of the Blackheads, rebuilt from an earlier Medieval structure into its present Mannerist forms as late as 1619–25 by the architects A. and L. Jansen. It was destroyed during World War II and rebuilt during the 1990s.
Lithuania meanwhile formed one half of the large Polish-Lithuanian commonwealth. Renaissance influences grew stronger during the reign of the Grand Dukes of Lithuania Sigismund I the Old and Sigismund II Augustus. The Palace of the Grand Dukes of Lithuania (destroyed in 1801, a copy built in 2002–2009) show Italian influences. Several architects of Italian origin were active in the country, including Bernardino Zanobi de Gianotis, Giovanni Cini and Giovanni Maria Mosca.
Legacy.
During the 19th century there was a conscious revival of the style in Renaissance Revival architecture, that paralleled the Gothic Revival. Whereas the Gothic style was perceived by architectural theorists as being the most appropriate style for Church building, the Renaissance palazzo was a good model for urban secular buildings requiring an appearance of dignity and reliability such as banks, gentlemen's clubs and apartment blocks. Buildings that sought to impress, such as the Paris Opera, were often of a more Mannerist or Baroque style. Architects of factories, office blocks and department stores continued to use the Renaissance palazzo form into the 20th century, in Mediterranean Revival Style architecture with an Italian Renaissance emphasis.
Many of the concepts and forms of Renaissance architecture can be traced through subsequent architectural movements—from Renaissance to High-Renaissance, to Mannerism, to Baroque (or Rococo), to Neo-Classicism, and to Eclecticism. While Renaissance style and motifs were largely purged from Modernism, they have been reasserted in some Postmodern architecture. The influence of Renaissance architecture can still be seen in many of the modern styles and rules of architecture today.
Bibliography.
</dl>
Reading.
</dl>
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="41525" url="http://en.wikipedia.org/wiki?curid=41525" title="Amadeus">
Amadeus

Amadeus is a play by Peter Shaffer, which gives a highly fictionalized account of the lives of the composers Wolfgang Amadeus Mozart and Antonio Salieri. First performed in 1979, "Amadeus" was inspired by a short 1830 play by Alexander Pushkin called "Mozart and Salieri" (which was also used as the libretto for an opera of the same name by Nikolai Rimsky-Korsakov in 1897).
In the play, significant use is made of the music of Mozart, Salieri and other composers of the period. The premieres of Mozart's operas "The Abduction from the Seraglio", "The Marriage of Figaro," "Don Giovanni", and "The Magic Flute" are each the setting for key scenes of the play.
"Amadeus" won the 1981 Tony Award for Best Play. It was adapted by Shaffer for the 1984 Academy Award winning film of the same name.
Plot.
Since the original run, Shaffer has extensively revised his play, including changes to plot details; the following is common to all revisions.
At the opening of the tale, Salieri is an old man, having long outlived his fame. Speaking directly to the audience, he claims to have used poison to assassinate Mozart, and promises to explain himself. The action then flashes back to the eighteenth century, at a time when Salieri has not met Mozart in person, but has heard of him and his music. He adores Mozart's compositions, and is thrilled at the chance to meet Mozart in person, during a salon at which some of Mozart's compositions will be played. When he finally does catch sight of Mozart, however, he is deeply disappointed to find that Mozart himself lacks the grace and charm of his compositions: When Salieri first meets him, Mozart is crawling around on his hands and knees, engaging in profane talk with his future bride Constanze Weber.
Salieri cannot reconcile Mozart's boorish behaviour with the genius that God has inexplicably bestowed upon him. Indeed, Salieri, who has been a devout Catholic all his life, cannot believe that God would choose Mozart over him for such a gift. Salieri renounces God and vows to do everything in his power to destroy Mozart as a way of getting back at his Creator.
Throughout much of the rest of the play, Salieri masquerades as Mozart's ally to his face while doing his utmost to destroy his reputation and any success his compositions may have. On more than one occasion it is only the direct intervention of the Emperor himself that allows Mozart to continue (interventions which Salieri opposes, and then is all too happy to take credit for when Mozart assumes it was he who intervened). Salieri also humiliates Mozart's wife when she comes to Salieri for aid, and smears Mozart's character with the Emperor and the court. A major theme in "Amadeus" is Mozart's repeated attempts to win over the aristocratic "public" with increasingly brilliant compositions, which are always frustrated either by Salieri or by the aristocracy's own inability to appreciate Mozart's genius.
The play ends with Salieri attempting suicide in a last attempt to be remembered, leaving a confession of having murdered Mozart with arsenic. He survives, however, and his confession is met with disbelief, leaving him to wallow once again in mediocrity.
Background and production.
Historical accuracy.
Shaffer used artistic license in his portrayals of both Mozart and Salieri. Documentary evidence suggests that there was some antipathy between the two men, but the idea that Salieri was the instigator of Mozart's demise is not taken seriously by scholars of the men's lives and careers. While historically there may have been actual rivalry and tension between Mozart and Salieri, there is also evidence that they enjoyed a relationship marked by mutual respect. As an example, Salieri later tutored Mozart's son Franz in music. He also conducted some of Mozart's works, both in Mozart's lifetime and afterwards.
Writer David Cairns called "Amadeus" "myth-mongering" and argued against Shaffer's alleged portrait of Mozart as "two contradictory beings, sublime artist and fool", positing instead that Mozart was "fundamentally well-integrated". Cairns also rejects the "romantic legend" that Mozart always wrote out perfect manuscripts of works already completely composed in his head, citing major and prolonged revisions to several manuscripts (see: Mozart's compositional method).
Notable productions.
"Amadeus" was first presented at the Royal National Theatre, London in 1979, directed by Sir Peter Hall and starring Paul Scofield as Salieri, Simon Callow as Mozart, and Felicity Kendal as Constanze. (Callow later appeared in the film version in a different role.) It was later transferred in modified form to the West End, starring Frank Finlay as Salieri. The cast also included Andrew Cruickshank (Rosenberg), Basil Henson (von Strack), Philip Locke (Greybig), John Normington (Joseph II) and Nicholas Selby (van Swieten).
The play premiered on Broadway in 1980 with Ian McKellen as Salieri, Tim Curry as Mozart, and Jane Seymour as Constanze. It ran for 1,181 performances and was nominated for seven Tony Awards (best actor for both McKellen and Curry, best director for Peter Hall, best play, best costume design, lighting, and set design for John Bury), of which it won five (including a best actor Tony for McKellen). During the run of the play McKellen was replaced by John Wood, Frank Langella, David Dukes, David Birney, John Horton, and Daniel Davis. Curry was replaced by Peter Firth, Peter Crook, Dennis Boutsikaris, John Pankow, Mark Hamill, and John Thomas Waite. Also playing Constanze were Amy Irving, Suzanne Lederer, Michele Farr, Caris Corfman and Maureen Moore.
Adam Redfield and Terry Finn appeared as Mozart and Constanze, respectively, in the 1984 Virginia Stage Company production. Performed at the Wells Theatre in Norfolk, the drama was directed by Charles Towers.
The play was revived in 1999 at the Music Box Theatre, New York City, directed again by Peter Hall and ran for 173 performances (December 15, 1999 - May 14, 2000), receiving Tony Award nominations for Best Revival and Best Actor in a Play (David Suchet, who played Salieri). Also in the cast were Michael Sheen as Mozart, Cindy Katz as Constanze and David McCallum as Joseph II.
In July 2006, the Los Angeles Philharmonic presented a production of portions from the latest revision of the play at the Hollywood Bowl. Neil Patrick Harris starred as Mozart, Kimberly Williams-Paisley as Constanze Mozart, and Michael York as Salieri. Leonard Slatkin conducted the Philharmonic Orchestra.
Rupert Everett played Salieri in a production at the newly refurbished Chichester Festival Theatre from July 12, 2014 - August 2, 2014. The cast also featured Joshua McGuire as Mozart, Jessie Buckley as Constanze and John Standing as Count Orsini-Rosenberg. Simon Jones played Joseph II. Peter Shaffer attended the play himself at the closing performance.
Film and other adaptations.
The 1984 film adaptation won an Academy Award for Best Picture. In total, the film won eight Academy Awards. It starred F. Murray Abraham as Salieri (winning the Oscar for Best Actor for this role), Tom Hulce as Mozart, and Elizabeth Berridge as Constanze. The play was thoroughly reworked by Shaffer and the film's director, Miloš Forman with scenes and characters not found in the play. While the focus of the play is primarily on Salieri, the film goes further into developing the characters of both composers.
In 1983, BBC Radio 3 broadcast the play directed by Sir Peter Hall and starring the original cast of his National Theatre production. The cast included:
This radio production was re-broadcast on 2 January 2011 as part of Radio 3's "Genius of Mozart" season.
To celebrate Mozart's 250th birthday in 2006, BBC Radio 2 broadcast an adaptation by Neville Teller of Shaffer's play in eight fifteen-minute episodes directed by Peter Leslie Wilde and narrated by F. Murray Abraham as Salieri (re-broadcast 24 May – 2 June 2010 on BBC Radio 7).

</doc>
<doc id="41527" url="http://en.wikipedia.org/wiki?curid=41527" title="Contrapposto">
Contrapposto

Contrapposto is an Italian term that means counterpose. It is used in the visual arts to describe a human figure standing with most of its weight on one foot so that its shoulders and arms twist off-axis from the hips and legs. This gives the figure a more dynamic, or alternatively relaxed appearance. It can also be used to refer to multiple figures which are in counter-pose (or opposite pose) to one another. It can further encompass the tension as a figure changes from resting on a given leg to walking or running upon it (so-called "ponderation"). The leg that carries the weight of the body is known as the "engaged" leg, the relaxed leg is known as the "free" leg. Contrapposto is less emphasized than the more sinuous S Curve.
Contrapposto was an extremely important sculptural development for it is the first time in Western art that the human body is used to express a psychological disposition. The balanced, harmonious pose of the Kritios Boy suggests a calm and relaxed state of mind, an evenness of temperament that is part of the ideal of man represented. From this point onwards Greek sculptors went on to explore how the body could convey the whole range of human experience, culminating in the desperate anguish and pathos of Laocoön and his Sons (1st century AD) in the Hellenistic period.
History.
Classical.
The first known statue to use contrapposto is Kritios Boy, c. 480 BC, so called because it was once attributed to the sculptor Kritios. It is possible, even likely, that earlier Bronze statues had used the technique, but if they did, they have not survived and Kenneth Clark called the statue "the first beautiful nude in art". The statue is a Greek marble original and not a Roman copy.
Prior to the introduction of contrapposto, the statues that dominated ancient Greece were the archaic kouros (male) and the kore (female). Contrapposto has been used since the dawn of classical western sculpture. According to the "canon" of the Classical Greek Sculptor Polykleitos in the 4th century BC, it is one of the most important characteristics of his figurative works and those of his successors, Lysippos, Skopas, etc. The Polykletian statues -- for example, Discophoros ("discus-bearer") and Doryphoros ("spear-bearer") -- are idealized athletic young men with the divine sense, and captured in contrapposto. In these works, the pelvis is no longer axial with the vertical statue as in the archaic style of earlier Greek sculpture before "Kritios Boy".
Contrapposto can be clearly seen in the Roman copies of the statues of Hermes and Heracles. A famous example is the marble statue of Hermes with the infant Dionysus in Olympia by Praxiteles. It can also be seen in the Roman copies of Polyclitus' amazon.
Renaissance.
Classical contrapposto was revived in the Renaissance by the Italian artists Donatello and Leonardo da Vinci, followed by Michelangelo, Raphael and other artists of the High Renaissance. One of the major achievements of the Italian Renaissance was the re-discovery of contrapposto, although in Mannerism it became greatly over-used.
Modern times.
The technique continues to be widely employed in sculpture.

</doc>
<doc id="41528" url="http://en.wikipedia.org/wiki?curid=41528" title="Forrest Gump">
Forrest Gump

Forrest Gump is a 1994 American epic romantic-comedy-drama film based on the 1986 novel of the same name by Winston Groom. The film was directed by Robert Zemeckis and stars Tom Hanks, Robin Wright, Gary Sinise, Mykelti Williamson, and Sally Field. The story depicts several decades in the life of Forrest Gump, a slow-witted and naïve, but good-hearted and athletically prodigious, man from Alabama who witnesses, and in some cases influences, some of the defining events of the latter half of the 20th century in the United States; more specifically, the period between Forrest's birth in 1944 and 1982. The film differs substantially from Winston Groom's novel on which it was based, including Gump's personality and several events that were depicted.
Principal photography took place in late 1993, mainly in Georgia, North Carolina, and South Carolina. Extensive visual effects were used to incorporate the protagonist into archived footage and to develop other scenes. A comprehensive soundtrack was featured in the film, using music intended to pinpoint specific time periods portrayed on screen. Its commercial release made it a top-selling soundtrack, selling over twelve million copies worldwide.
Released in the United States on July 6, 1994, "Forrest Gump" became a commercial success as the top grossing film in North America released in that year, being the first major success for Paramount Pictures since the studio's sale to Viacom, earning over $677 million worldwide during its theatrical run. In 1995 it won the Academy Awards for Best Picture, Best Director for Robert Zemeckis, Best Actor for Tom Hanks, Best Adapted Screenplay for Eric Roth, Best Visual Effects, and Best Film Editing. It also garnered multiple other awards and nominations, including Golden Globes, People's Choice Awards, and Young Artist Awards, among others. Since the film's release varying interpretations have been made of the film's protagonist and its political symbolism. In 1996, a themed restaurant, Bubba Gump Shrimp Company, opened based on the film and has since expanded to multiple locations worldwide. The scene of Gump running across the country is often referred to when real-life people attempt the feat. In 2011, the Library of Congress selected "Forrest Gump" for preservation in the United States National Film Registry as being "culturally, historically, or aesthetically significant". 
Plot.
While waiting at a bus stop in 1981, Forrest Gump (Tom Hanks) begins telling his life story to strangers who sit next to him on the bench. His story begins with his being named for a relative, Nathan Bedford Forrest, and proceeds to the leg braces he had to wear as a child in the 1950s, which resulted in other children bullying him. He lives with his mother (Sally Field), who tells him that "stupid is as stupid does." His mother runs a rooming house and Forrest teaches one of their guests, a young Elvis Presley (Peter Dobson), a hip-swinging dance. On a bus for his first day of school, Forrest meets Jenny (Robin Wright), with whom he immediately falls in love, and they become best friends. One day, while fleeing from bullies, Forrest's leg braces break apart and he discovers that he can run very fast. Despite his below-average intelligence, his speed earns him an athletic scholarship to the University of Alabama. While in college, he witnesses George Wallace's Stand in the Schoolhouse Door, is named an All-American football player, and meets President John F. Kennedy.
After graduating, Forrest enlists in the United States Army, where he befriends former shrimp fisherman Benjamin Buford "Bubba" Blue (Mykelti Williamson), and they agree to go into the shrimping business together once they end their service. They are sent to Vietnam, and while on patrol their platoon is ambushed. Forrest saves four of the men in his platoon, including platoon leader First Lieutenant Dan Taylor (Gary Sinise), but Bubba is killed. Forrest himself is wounded and receives the Medal of Honor from President Lyndon B. Johnson. While recovering from his injuries, Forrest meets Lieutenant Dan, who has had both of his legs amputated due to his injuries. He is furious at Forrest for leaving him a "cripple" and cheating him out of his destiny to die in battle.
Forrest discovers an aptitude for ping pong and begins playing for the U.S. Army team, eventually competing against Chinese teams on a goodwill tour. After his return from China, he appears on the The Dick Cavett Show with John Lennon, which after describing his experience in China as best as he can, inspires Lennon to write the song "Imagine". He visits the White House again and meets President Richard Nixon, who provides him a room at the Watergate hotel, where Forrest inadvertently helps expose the Watergate scandal. He again encounters Lieutenant Dan, now an embittered drunk living on welfare. Dan is scornful of Forrest's plans to enter the shrimping business and mockingly promises to be Forrest's first mate if he ever succeeds. The two go back to Dan's hotel room with two prostitutes, but Forrest rejects one of the prostitute's advances. When the other one calls Forrest "stupid," Dan surprisingly defends Forrest, demands that they never call him stupid, and kicks the prostitutes out of his hotel room.
Forrest is discharged from the military as a Sergeant and uses money from a ping pong endorsement to buy a shrimping boat, fulfilling his wartime promise to Bubba. Lieutenant Dan keeps his own promise and joins Forrest as first mate. They initially have little luck, but after Hurricane Carmen wrecks every other shrimping boat in the region, the Bubba Gump Shrimp Company becomes a huge success. Forrest returns home to care for his ailing mother, who dies soon afterwards. He leaves the company in the hands of Dan, who invests the proceeds of the company in shares of "some kind of fruit company", making them both wealthy.
Jenny returns to visit Forrest and stays with him. He proposes but she turns him down. They have sexual intercourse, but she quietly leaves the next morning. Distraught at discovering this, Forrest decides to go for a run, which turns into a three-year coast-to-coast marathon. Forrest becomes a celebrity, attracting a band of followers. One day he stops his marathon suddenly and returns home, where he receives a letter from Jenny asking to meet.
This brings Forrest to the bus stop where he began telling his story at the start of the film. During his reunion with Jenny, Forrest discovers they have a young son, also named Forrest (Haley Joel Osment). Jenny reveals that she is suffering from an unspecified viral illness, presumably HIV/AIDS. She proposes and he accepts, and they return to Alabama with Forrest Jr. and marry. At his wedding, he meets Lieutenant Dan, who now has titanium alloy prosthetic legs and can walk, as well as his fiancee.
Eventually, Jenny dies of her illness. Forrest waits with Forrest Jr. for the bus to pick him up for his first day of school, and watches his feather bookmark float off in the wind.
Production.
Script.
"The writer, Eric Roth, departed substantially from the book. We flipped the two elements of the book, making the love story primary and the fantastic adventures secondary. Also, the book was cynical and colder than the movie. In the movie, Gump is a completely decent character, always true to his word. He has no agenda and no opinion about anything except Jenny, his mother and God."
—director Robert Zemeckis
The film is based on the 1986 novel by Winston Groom. Both center on the character of Forrest Gump. However, the film primarily focuses on the first eleven chapters of the novel, before skipping ahead to the end of the novel with the founding of Bubba Gump Shrimp Co. and the meeting with Forrest, Jr. In addition to skipping some parts of the novel, the film adds several aspects to Gump's life that do not occur in the novel, such as his needing leg braces as a child and his run across the United States.
Gump's core character and personality are also changed from the novel; among other things his film character is less of an autistic savant—in the novel, while playing football at the university, he fails craft and gym, but receives a perfect score in an advanced physics class he is enrolled in by his coach to satisfy his college requirements. The novel also features Gump as an astronaut, a professional wrestler, and a chess player.
Two directors were offered the opportunity to direct the film before Robert Zemeckis was selected. Terry Gilliam turned down the offer to direct. Barry Sonnenfeld was attached to the film, but left to direct "Addams Family Values".
Filming.
Filming began in August 1993 and ended in December of that year. Although most of the film is set in Alabama, filming took place mainly in and around Beaufort, South Carolina, as well as parts of coastal Virginia and North Carolina, including a running shot on the Blue Ridge Parkway. Downtown portions of the fictional town of Greenbow were filmed in Varnville, South Carolina. The scene of Forrest running through Vietnam while under fire was filmed on Fripp Island, South Carolina. Additional filming took place on the Biltmore Estate in Asheville, North Carolina and along the Blue Ridge Parkway near Boone, North Carolina. The most notable place was Grandfather Mountain where a part of the road is named "Forrest Gump Curve". The Gump family home set was built along the Combahee River near Yemassee, South Carolina and the nearby land was used to film Curran's home as well as some of the Vietnam scenes. Over 20 palmetto trees were planted to improve the Vietnam scenes. Forrest Gump narrated his life's story in Chippewa Square in Savannah, Georgia as he sat at a bus stop bench. There were other scenes filmed in and around the Savannah area as well, including a running shot on the Richard V. Woods Memorial Bridge in Beaufort while he was being interviewed by the press, and on West Bay Street in Savannah. Most of the college campus scenes were filmed in Los Angeles at the University of Southern California. The lighthouse that Forrest runs across to reach the Atlantic Ocean the first time is the Marshall Point Lighthouse in Port Clyde, Maine.
Visual effects.
Ken Ralston and his team at Industrial Light & Magic were responsible for the film's visual effects. Using CGI techniques, it was possible to depict Gump meeting deceased personages and shaking their hands. Hanks was first shot against a blue screen along with reference markers so that he could line up with the archive footage. To record the voices of the historical figures, voice doubles were hired and special effects were used to alter lip-syncing for the new dialogue. Archival footage was used and with the help of such techniques as chroma key, image warping, morphing, and rotoscoping, Hanks was integrated into it.
In one Vietnam War scene, Gump carries Bubba away from an incoming napalm attack. To create the effect, stunt actors were initially used for compositing purposes. Then, Hanks and Williamson were filmed, with Williamson supported by a cable wire as Hanks ran with him. The explosion was then filmed, and the actors were digitally added to appear just in front of the explosions. The jet fighters and napalm canisters were also added by CGI.
The CGI removal of actor Gary Sinise's legs, after his character had them amputated, was achieved by wrapping his legs with a blue fabric, which later facilitated the work of the "roto-paint" team to paint out his legs from every single frame. At one point, while hoisting himself into his wheelchair, his legs are used for support.
The scene where Forrest spots Jenny at a peace rally at the Lincoln Memorial and Reflecting Pool in Washington, D.C., required visual effects to create the large crowd of people. Over two days of filming, approximately 1,500 extras were used. At each successive take, the extras were rearranged and moved into a different quadrant away from the camera. With the help of computers, the extras were multiplied to create a crowd of several hundred thousand people.
Release.
Critical reception.
The film received generally positive reviews. The review aggregator website Rotten Tomatoes reported that 72% of critics gave the film a positive review based on a sample of 79 reviews. At the website Metacritic the film earned a rating of 82/100 based on 19 reviews by mainstream critics. 
The story was commended by several critics. Roger Ebert of the "Chicago Sun-Times" wrote, "I've never met anyone like Forrest Gump in a movie before, and for that matter I've never seen a movie quite like 'Forrest Gump.' Any attempt to describe him will risk making the movie seem more conventional than it is, but let me try. It's a comedy, I guess. Or maybe a drama. Or a dream. The screenplay by Eric Roth has the complexity of modern fiction...The performance is a breathtaking balancing act between comedy and sadness, in a story rich in big laughs and quiet truths...What a magical movie." Todd McCarthy of "Variety" wrote that the film "has been very well worked out on all levels, and manages the difficult feat of being an intimate, even delicate tale played with an appealingly light touch against an epic backdrop." The film did receive notable pans from several major reviewers. Anthony Lane of "The New Yorker" called the film "Warm, wise, and wearisome as hell." Owen Gleiberman of "Entertainment Weekly" said that the film "reduces the tumult of the last few decades to a virtual-reality theme park: a baby-boomer version of Disney's America."
Critics had mixed views on the main character. Gump has been compared with various characters and people including Huckleberry Finn, Bill Clinton, and Ronald Reagan. Peter Chomo writes that Gump acts as a "social mediator and as an agent of redemption in divided times". Peter Travers of "Rolling Stone" called Gump "everything we admire in the American character – honest, brave, loyal." "The New York Times" reviewer Janet Maslin called Gump a "hollow man" who is "self-congratulatory in his blissful ignorance, warmly embraced as the embodiment of absolutely nothing." Marc Vincenti of "Palo Alto Weekly" called the character "a pitiful stooge taking the pie of life in the face, thoughtfully licking his fingers." Bruce Kawin and Gerald Mast's textbook on film history notes that Forrest Gump's dimness was a metaphor for glamorized nostalgia in that he represented a blank slate by which the Baby Boomer generation projected their memories of those events.
The film is commonly seen as a polarizing one for audiences, with "Entertainment Weekly" writing in 2004, "Nearly a decade after it earned gazillions and swept the Oscars, Robert Zemeckis's ode to 20th-century America still represents one of cinema's most clearly drawn lines in the sand. One half of folks see it as an artificial piece of pop melodrama, while everyone else raves that it's sweet as a box of chocolates."
Box office performance.
Produced on a budget of $55 million, "Forrest Gump" opened in 1,595 theaters in its first weekend of domestic release, earning $24,450,602. Motion picture business consultant and screenwriter Jeffrey Hilton suggested to producer Wendy Finerman to double the P&A (film marketing budget) based on his viewing of an early print of the film. The budget was immediately increased, per his advice. The film placed first in the weekend's box office, narrowly beating "The Lion King", which was in its fourth week of release. For the first ten weeks of its release, the film held the number one position at the box office. The film remained in theaters for 42 weeks, earning $329.7 million in the United States and Canada, making it the fourth-highest grossing film at that time (behind only "E.T. the Extra-Terrestrial", "", and "Jurassic Park").
The film took 66 days to surpass $250 million and was the fastest grossing Paramount film to pass $100 million, $200 million, and $300 million in box office receipts (at the time of its release). The film had gross receipts of $329,694,499 in the U.S. and Canada and $347,693,217 in international markets for a total of $677,387,716 worldwide. Even with such revenue, the film was known as a "successful failure"—due to distributors' and exhibitors' high fees, Paramount's "losses" clocked in at $62 million, leaving executives realizing the necessity of better deals. This has, however, also been associated with Hollywood accounting, where expenses are inflated in order to minimize profit sharing. It is Robert Zemeckis' highest-grossing film to date.
Home media.
"Forrest Gump" was first released on VHS tape on April 27, 1995, as a two-disc Laserdisc set on April 28, 1995, (including the "Through the Eyes of Forrest" special feature), before being released in a two-disc DVD set on August 28, 2001. Special features included director and producer commentaries, production featurettes, and screen tests. The film was released on Blu-ray disc in November 2009.
Accolades.
The film won the 67th Academy Awards for the Best Picture, Best Actor in a Leading Role, Best Director, Best Visual Effects, Best Adapted Screenplay, and Best Film Editing. The film was nominated for seven Golden Globe Awards, winning three of them: Best Actor – Motion Picture Drama, Best Director – Motion Picture, and Best Motion Picture – Drama. The film was also nominated for six Saturn Awards and won two for Best Fantasy Film and Best Supporting Actor (Film).
In addition to the film's multiple awards and nominations, it has also been recognized by the American Film Institute on several of its lists. The film ranks 37th on "100 Years...100 Cheers", 71st on "100 Years...100 Movies", and 76th on "100 Years...100 Movies (10th Anniversary Edition)". In addition, the quote "Mama always said life was like a box of chocolates. You never know what you're gonna get," was ranked 40th on "100 Years...100 Movie Quotes". The film also ranked at number 240 on "Empire"‍ '​s list of the 500 Greatest Movies of All Time.
In December 2011, "Forrest Gump" was selected for preservation in the Library of Congress' National Film Registry. The Registry said that the film was "honored for its technological innovations (the digital insertion of Gump seamlessly into vintage archival footage), its resonance within the culture that has elevated Gump (and what he represents in terms of American innocence) to the status of folk hero, and its attempt to engage both playfully and seriously with contentious aspects of the era's traumatic history."
American Film Institute Lists
Author controversy.
Winston Groom was paid $350,000 for the screenplay rights to his novel "Forrest Gump" and was contracted for a 3 percent share of the film's net profits. However, Paramount and the film's producers did not pay him, using Hollywood accounting to posit that the blockbuster film lost money. Tom Hanks, by contrast, contracted for the film's gross receipts instead of a salary, and he and director Zemeckis each received $40 million. Additionally, Groom was not mentioned once in any of the film's six Oscar-winner speeches.
Groom's dispute with Paramount was later effectively resolved after Groom declared he was satisfied with Paramount's explanation of their accounting, this coinciding with Groom receiving a 7-figure contract with Paramount for film rights to another of his books, titled 'Gump & Co.'
Symbolism.
Feather.
"I don't want to sound like a bad version of 'the child within'. But the childlike innocence of Forrest Gump is what we all once had. It's an emotional journey. You laugh and cry. It does what movies are supposed to do: make you feel alive."
—producer Wendy Finerman
Various interpretations have been suggested for the feather present at the opening and conclusion of the film. Sarah Lyall of "The New York Times" noted several suggestions that were made about the feather: "Does the white feather symbolize the unbearable lightness of being? Forrest Gump's impaired intellect? The randomness of experience?" Hanks interpreted the feather as: "Our destiny is only defined by how we deal with the chance elements to our life and that's kind of the embodiment of the feather as it comes in. Here is this thing that can land anywhere and that it lands at your feet. It has theological implications that are really huge." Sally Field compared the feather to fate, saying: "It blows in the wind and just touches down here or there. Was it planned or was it just perchance?" Visual effects supervisor Ken Ralston compared the feather to an abstract painting: "It can mean so many things to so many different people."
The feather is stored in a book titled "Curious George", Forrest's favorite book, which his mother read to him, connecting the scene's present time with his childhood in the 1940s. The placement of the feather in the book is directly on a picture of the monkey walking on a tightrope. Whether that was intentional or not, it is very symbolic. The feather also has a correlation with Jenny's constant obsession with "becoming a bird and flying far far away" due to the abuse (sexual and physical) she endured from her father. She goes as far in the film as to ask Forrest "if [she] jumped off the bridge, could [she] fly?"
Political interpretations.
In Tom Hanks' words, "The film is non-political and thus non-judgmental." Nevertheless, in 1994, CNN's "Crossfire" debated whether the film promoted conservative values or was an indictment of the counterculture movement of the 1960s. Thomas Byers, in a "Modern Fiction Studies" article, called the film "an aggressively conservative film".
"...all over the political map, people have been calling Forrest their own. But, "Forrest Gump" isn't about politics or conservative values. It's about humanity, it's about respect, tolerance and unconditional love."
—producer Steve Tisch
It has been noted that while Gump follows a very conservative lifestyle, Curran's life is full of countercultural embrace, complete with drug usage, promiscuity, and antiwar rallies, and that their eventual marriage might be a kind of reconciliation. Jennifer Hyland Wang argued in a "Cinema Journal" article that Curran's death to an unnamed virus "...symbolizes the death of liberal America and the death of the protests that defined a decade [1960s]." She also notes that the film's screenwriter, Eric Roth, when developing the screenplay from the novel, had "...transferred all of Gump's flaws and most of the excesses committed by Americans in the 1960s and 1970s to her [Curran]."
Other commentators believe that the film forecast the 1994 Republican Revolution and used the image of Forrest Gump to promote movement leader Newt Gingrich's traditional, conservative values. Jennifer Hyland Wang observes that the film idealizes the 1950s, as evidenced by the lack of "whites only" signs in Gump's southern childhood, and "revisions" the 1960s as a period of social conflict and confusion. She argues that this sharp contrast between the decades criticizes the counterculture values and reaffirms conservatism. As viewed by political scientist Joe Paskett, this film is "one of the best films of all time". Wang argued that the film was used by Republican politicians to illustrate a "traditional version of recent history" to gear voters towards their ideology for the congressional elections. In addition, presidential candidate Bob Dole cited the film's message in influencing his campaign due to its "...message that has made [the film] one of Hollywood's all-time greatest box office hits: no matter how great the adversity, the American Dream is within everybody's reach."
In 1995, "National Review" included "Forrest Gump" in its list of the "Best 100 Conservative Movies" of all time. Then, in 2009, the magazine ranked the film number four on its 25 Best Conservative Movies of the Last 25 Years list. "Tom Hanks plays the title character, an amiable dunce who is far too smart to embrace the lethal values of the 1960s. The love of his life, wonderfully played by Robin Wright Penn, chooses a different path; she becomes a drug-addled hippie, with disastrous results."
James Burton, a communication arts professor at Salisbury University, argued that conservatives claimed "Forrest Gump" as their own due less to the content of the film and more to the historical and cultural context of 1994. Burton claimed that the film's content and advertising campaign were affected by the cultural climate of the 1990s, which emphasized family values and "American values"—values epitomized in the successful book "Hollywood vs. America". He claimed that this climate influenced the apolitical nature of the film, which allowed for many different political interpretations.
Burton points out that many conservative critics and magazines (John Simon, James Bowman, the "World Report") initially either criticized the film or praised it only for its non-political elements. Only after the popularity of the film was well-established did conservatives embrace the film as an affirmation of traditional values. Burton implies that the liberal-left could have prevented the conservatives from claiming rights to the film, had it chosen to vocalize elements of the film such as its criticism of military values. Instead, the liberal-left focused on what the film omitted, such as the feminist and civil rights movements.
Some commentators see the conservative readings of "Forrest Gump" as indicants of the death of irony in American culture. Vivian Sobchack notes that the film's humor and irony relies on the assumption of the audience's historical (self-) consciousness.
Soundtrack.
The 32-song soundtrack from the film was released on July 6, 1994. With the exception of a lengthy suite from Alan Silvestri's score, all the songs are previously released; the soundtrack includes songs from Elvis Presley, Fleetwood Mac, Creedence Clearwater Revival, Aretha Franklin, Lynyrd Skynyrd, Three Dog Night, The Byrds, The Doors, The Mamas & the Papas, The Doobie Brothers, Simon & Garfunkel, Bob Seger, and Buffalo Springfield among others. Music producer Joel Sill reflected on compiling the soundtrack: "We wanted to have very recognizable material that would pinpoint time periods, yet we didn't want to interfere with what was happening cinematically." The two-disc album has a variety of music from the 1950s–1980s performed by American artists. According to Sills, this was due to Zemeckis' request, "All the material in there is American. Bob (Zemeckis) felt strongly about it. He felt that Forrest wouldn't buy anything but American."
The soundtrack reached a peak of number 2 on the "Billboard" album chart. The soundtrack went on to sell twelve million copies, and is one of the top selling albums in the United States. The score for the film was composed and conducted by Alan Silvestri and released on August 2, 1994.
Possible sequel.
The screenplay for the sequel was written by Eric Roth in 2001. It is based on the original novel's sequel, "Gump and Co." that was written by Winston Groom in 1995. Roth's script begins with Forrest sitting on a bench waiting for his son to return from school. After the September 11 attacks, Roth, Zemeckis, and Hanks decided the story was no longer "relevant." In March 2007, however, it was reported that Paramount producers took another look at the screenplay.
On the very first page of the sequel novel, Forrest Gump tells readers "Don't never let nobody make a movie of your life's story," though "Whether they get it right or wrong, it doesn't matter." The first chapter of the book suggests that the real-life events surrounding the film have been incorporated into Forrest's storyline, and that Forrest got a lot of media attention as a result of the film. During the course of the sequel novel, Gump runs into Tom Hanks and at the end of the novel is the film's release, including Gump going on "The David Letterman Show" and attending the Academy Awards.

</doc>
<doc id="41531" url="http://en.wikipedia.org/wiki?curid=41531" title="Stanislaw Ulam">
Stanislaw Ulam

Stanisław Marcin Ulam (pronounced ; 13 April 1909 – 13 May 1984) was a Polish-American mathematician. He participated in America's Manhattan Project, originated the Teller–Ulam design of thermonuclear weapons, invented the Monte Carlo method of computation, and suggested nuclear pulse propulsion. In pure and applied mathematics, he proved some theorems and proposed several conjectures.
Born into a wealthy Polish Jewish family, Ulam studied mathematics at the Lwów Polytechnic Institute, where he earned his D.Sc. in 1933 under the supervision of Kazimierz Kuratowski. In 1935, John von Neumann, whom Ulam had met in Warsaw, invited him to come to the Institute for Advanced Study in Princeton, New Jersey, for a few months. From 1936 to 1939, he spent summers in Poland and academic years at Harvard University in Cambridge, Massachusetts, where he worked to establish important results regarding ergodic theory. On 20 August 1939, he sailed for America for the last time with his 17-year-old brother Adam Ulam. He became an assistant professor at the University of Wisconsin–Madison in 1940, and a United States citizen in 1941.
In October 1943, he received an invitation from Hans Bethe to join the Manhattan Project at the secret Los Alamos Laboratory in New Mexico. There, he worked on the hydrodynamic calculations to predict the behavior of the explosive lenses that were needed by an implosion-type weapon. He was assigned to Edward Teller's group, where he worked on Teller's "Super" bomb for Teller and Enrico Fermi. After the war he left to become an associate professor at the University of Southern California, but returned to Los Alamos in 1946 to work on thermonuclear weapons. With the aid of a cadre of female "computers", including his wife Françoise Aron Ulam, he found that Teller's "Super" design was unworkable. In January 1951, Ulam and Teller came up with the Teller–Ulam design, which is the basis for all thermonuclear weapons.
Ulam considered the problem of nuclear propulsion of rockets, which was pursued by Project Rover, and proposed, as an alternative to Rover's nuclear thermal rocket, to harness small nuclear explosions for propulsion, which became Project Orion. With Fermi and John Pasta, Ulam studied the Fermi–Pasta–Ulam problem, which became the inspiration for the field of non-linear science. He is probably best known for realising that electronic computers made it practical to apply statistical methods to functions without known solutions, and as computers have developed, the Monte Carlo method has become a common and standard approach to many problems.
Poland.
Ulam was born in Lemberg, Galicia, on 13 April 1909. At this time, Galicia was in the Kingdom of Galicia and Lodomeria of the Austro-Hungarian Empire, known to Poles as the Austrian partition. In 1918, it became part of the newly restored Poland, the Second Polish Republic, and the city took its Polish name again, Lwów.
The Ulams were a wealthy Polish Jewish family of bankers, industrialists, and other professionals. Ulam's immediate family was "well-to-do but hardly rich". His father, Józef Ulam, was born in Lwów and was a lawyer, and his mother, Anna (née Auerbach), was born in Stryj. His uncle, Michał Ulam, was an architect, building contractor, and lumber industrialist. From 1916 until 1918, Józef's family lived temporarily in Vienna. After they returned, Lwów became the epicenter of the Polish–Ukrainian War, during which the city experienced a Ukrainian siege.
In 1919, Ulam entered Lwów Gymnasium Nr. VII, from which he graduated in 1927. He then studied mathematics at the Lwów Polytechnic Institute. Under the supervision of Kazimierz Kuratowski, he received his Master of Arts degree in 1932, and became a Doctor of Science in 1933. At the age of 20, in 1929, he published his first paper "Concerning Function of Sets" in the journal "Fundamenta Mathematicae". From 1931 until 1935, he traveled to and studied in Wilno (Vilnius), Vienna, Zurich, Paris, and Cambridge, England, where he met G. H. Hardy and Subrahmanyan Chandrasekhar.
Along with Stanisław Mazur, Mark Kac, Włodzimierz Stożek, Kuratowski, and others, Ulam was a member of the Lwów School of Mathematics. Its founders were Hugo Steinhaus and Stefan Banach, who were professors at the University of Lwów. Mathematicians of this "school" met for long hours at the Scottish Café, where the problems they discussed were collected in the Scottish Book, a thick notebook provided by Banach's wife. Ulam was a major contributor to the book. Of the 193 problems recorded between 1935 and 1941, he contributed 40 problems as a single author, another 11 with Banach and Mazur, and an additional 15 with others. In 1957, he received from Steinhaus a copy of the book, which had survived the war, and translated it into English. In 1981, Ulam's friend R. Daniel Maudlin published an expanded and annotated version.
Coming to America.
In 1935, John von Neumann, whom Ulam had met in Warsaw, invited him to come to the Institute for Advanced Study in Princeton, New Jersey, for a few months. In December of that year, Ulam sailed to America. At Princeton, he went to lectures and seminars, where he heard Oswald Veblen, James Alexander, and Albert Einstein. During a tea party at von Neumann's house, he encountered G. D. Birkhoff, who suggested that he apply for a position with the Harvard Society of Fellows. Following up on Birkhoff's suggestion, Ulam spent summers in Poland and academic years at Harvard University in Cambridge, Massachusetts from 1936 to 1939, where he worked with John C. Oxtoby to establish results regarding ergodic theory. These appeared in Annals of Mathematics in 1941.
On 20 August 1939, in Gdynia, Józef Ulam, along with his brother Szymon, put his two sons, Stanislaw and 17 year old Adam, on a ship headed for America. Two weeks later, the . Within two months, the Germans completed their occupation of western Poland, and the Soviets and occupied eastern Poland. Within two years, Józef Ulam and the rest of his family were victims of the Holocaust, Hugo Steinhaus was in hiding, Kazimierz Kuratowski was lecturing at the underground university in Warsaw, Włodzimierz Stożek and his two sons had been killed in the massacre of Lwów professors, and the last problem had been recorded in the Scottish Book. Stefan Banach survived the Nazi occupation by feeding lice at Rudolf Weigl's typhus research institute. In 1963, Adam Ulam, who had become an eminent kremlinologist at Harvard, received a letter from George Volsky, who hid in Józef Ulam's house after deserting from the Polish army. This reminiscence gave a chilling account of Lwów's chaotic scenes in late 1939. In later life Ulam described himself as "an agnostic. Sometimes I muse deeply on the forces that are for me invisible. When I am almost close to the idea of God, I feel immediately estranged by the horrors of this world, which he seems to tolerate".
In 1940, after being recommended by Birkhoff, Ulam became an assistant professor at the University of Wisconsin–Madison. Here, he became an United States citizen in 1941. That year, he married Françoise Aron. She had been a French exchange student at Mount Holyoke College, whom he met in Cambridge. They had one daughter, Claire. In Madison, Ulam met his friend and colleague C. J. Everett, with whom he would collaborate on a number of papers.
Manhattan Project.
In early 1943, Ulam asked von Neumann to find him a war job. In October, he received an invitation to join an unidentified project near Santa Fe, New Mexico. The letter was signed by Hans Bethe, who had been appointed as leader of the theoretical division of Los Alamos National Laboratory by Robert Oppenheimer, its scientific director. Knowing nothing of the area, he borrowed a New Mexico guide book. On the checkout card, he found the names of his Wisconsin colleagues, Joan Hinton, David Frisch, and Joseph McKibben, all of whom had mysteriously disappeared. This was Ulam's introduction to the Manhattan Project, which was America's wartime effort to create the atomic bomb.
Hydrodynamical calculations of implosion.
A few weeks after Ulam reached Los Alamos in February 1944, the project experienced a crisis. In April, Emilio Segrè discovered that plutonium made in reactors would not work in a gun-type plutonium weapon like the "Thin Man", which was being developed in parallel with a uranium weapon, the "Little Boy" that was dropped on Hiroshima. This problem threatened to waste an enormous investment in new reactors at the Hanford site and to make slow separation of uranium isotopes the only way to prepare fissile material suitable for use in bombs. To respond, Oppenheimer implemented, in August, a sweeping reorganization of the laboratory to focus on development of an implosion-type weapon and appointed George Kistiakowsky head of the implosion department. He was a professor at Harvard and an expert on precise use of explosives.
The basic concept of implosion is to use chemical explosives to crush a chunk of fissile material into a critical mass, where neutron multiplication leads to a nuclear chain reaction, releasing a large amount of energy. Cylindrical implosive configurations had been studied by Seth Neddermeyer, but von Neumann, who had experience with shaped charges used in armor piercing ammunition, was a vocal advocate of spherical implosion driven by explosive lenses. He realized that the symmetry and speed with which implosion compressed the plutonium were critical issues, and enlisted Ulam to help design lens configurations that would provide nearly spherical implosion. Within an implosion, because of enormous pressures and high temperatures, solid materials behave much like fluids. This meant that hydrodynamical calculations were needed to predict and minimize asymmetries that would spoil a nuclear detonation. Of these calculations, Ulam said:The hydrodynamical problem was simply stated, but very difficult to calculate – not only in detail, but even in order of magnitude. In this discussion, I stressed pure pragmatism and the necessity to get a heuristic survey of the problem by simple-minded brute force, rather than by massive numerical work.
Nevertheless, with the primitive facilities available at the time, Ulam and von Neumann did carry out numerical computations that led to a satisfactory design. This motivated their advocacy of a powerful computational capability at Los Alamos, which began during the war years, continued through the cold war, and still exists. Otto Frisch remembered Ulam as "a brilliant Polish topologist with a charming French wife. At once he told me that he was a pure mathematician who had sunk so low that his latest paper actually contained numbers with decimal points!"
Statistics of branching and multiplicative processes.
Even the inherent statistical fluctuations of neutron multiplication within a chain reaction have implications with regard to implosion speed and symmetry. In November 1944, David Hawkins and Ulam addressed this problem in a report entitled "Theory of Multiplicative Processes". This report, which invokes probability-generating functions, is also an early entry in the extensive literature on statistics of branching and multiplicative processes. In 1948, its scope was extended by Ulam and Everett.
Early in the Manhattan project, Enrico Fermi's attention was focused on the use of reactors to produce plutonium. In September 1944, he arrived at Los Alamos, shortly after breathing life into the first Hanford reactor, which had been poisoned by a xenon isotope. Soon after Fermi's arrival, Teller's "Super" bomb group, of which Ulam was a part, was transferred to a new division headed by Fermi. Fermi and Ulam formed a relationship that became very fruitful after the war.
Post war Los Alamos.
In September 1945, Ulam left Los Alamos to become an associate professor at the University of Southern California in Los Angeles. In January 1946, he suffered an acute attack of encephalitis, which put his life in danger, but which was alleviated by emergency brain surgery. During his recuperation, many friends visited, including Nicholas Metropolis from Los Alamos and the famous mathematician Paul Erdős, who remarked: "Stan, you are just like before." This was encouraging, because Ulam was concerned about the state of his mental faculties, for he had lost the ability to speak during the crisis. Another friend, Gian-Carlo Rota, asserted in a 1987 article that the attack changed Ulam's personality; afterwards, he turned from rigorous pure mathematics to more speculative conjectures concerning the application of mathematics to physics and biology. This assertion was not accepted by Françoise Aron Ulam.
By late April 1946, Ulam had recovered enough to attend a secret conference at Los Alamos to discuss thermonuclear weapons. Those in attendance included Ulam, von Neumann, Metropolis, Teller, Stan Frankel, and others. Throughout his participation in the Manhattan Project, Teller's efforts had been directed toward the development of a "super" weapon based on nuclear fusion, rather than toward development of a practical fission bomb. After extensive discussion, the participants reached a consensus that his ideas were worthy of further exploration. A few weeks later, Ulam received an offer of a position at Los Alamos from Metropolis and Robert D. Richtmyer, the new head of its theoretical division, at a higher salary, and the Ulams returned to Los Alamos.
Monte Carlo method.
Late in the war, under the sponsorship of von Neumann, Frankel and Metropolis began to carry out calculations on the first general-purpose electronic computer, the ENIAC. Shortly after returning to Los Alamos, Ulam participated in a review of results from these calculations. Earlier, while playing solitaire during his recovery from surgery, Ulam had thought about playing hundreds of games to estimate statistically the probability of a successful outcome. With ENIAC in mind, he realized that the availability of computers made such statistical methods very practical. John von Neumann immediately saw the significance of this insight. In March 1947 he proposed a statistical approach to the problem of neutron diffusion in fissionable material. Because Ulam had often mentioned his uncle, Michał Ulam, "who just had to go to Monte Carlo" to gamble, Metropolis dubbed the statistical approach "The Monte Carlo method". Metropolis and Ulam published the first unclassified paper on the Monte Carlo method in 1949.
Fermi, learning of Ulam's breakthrough, devised an analog computer known as the Monte Carlo trolley, later dubbed the FERMIAC. The device performed a mechanical simulation of random diffusion of neutrons. As computers improved in speed and programmability, these methods became more useful. In particular, many Monte Carlo calculations carried out on modern massively parallel supercomputers are embarrassingly parallel applications, whose results can be very accurate.
Teller–Ulam design.
On 29 August 1949, the Soviet Union tested its first fission bomb, the RDS-1. Created under the supervision of Lavrentiy Beria, who sought to duplicate the American effort, this weapon was nearly identical to Fat Man, for its design was based on information provided by spies Klaus Fuchs, Theodore Hall, and David Greenglass. In response, on 31 January 1950, President Harry S. Truman announced a crash program to develop a fusion bomb.
To advocate an aggressive development program, Ernest Lawrence and Luis Alvarez came to Los Alamos, where they conferred with Norris Bradbury, the laboratory director, and with George Gamow, Edward Teller, and Ulam. Soon, these three became members of a short-lived committee appointed by Bradbury to study the problem, with Teller as chairman. At this time, research on the use of a fission weapon to create a fusion reaction had been ongoing since 1942, but the design was still essentially the one originally proposed by Teller. His concept was to put tritium and/or deuterium in close proximity to a fission bomb, with the hope that the heat and intense flux of neutrons released when the bomb exploded, would ignite a self-sustaining fusion reaction. Reactions of these isotopes of hydrogen are of interest because the energy per unit mass of fuel released by their fusion is much larger than that from fission of heavy nuclei.
Because the results of calculations based on Teller's concept were discouraging, many scientists believed it could not lead to a successful weapon, while others had moral and economic grounds for not proceeding. Consequently, several senior people of the Manhattan Project opposed development, including Bethe and Oppenheimer. To clarify the situation, Ulam and von Neumann resolved to do new calculations to determine whether Teller's approach was feasible. To carry out these studies, von Neumann decided to use electronic computers: ENIAC at Aberdeen, a new computer, MANIAC, at Princeton, and its twin, which was under construction at Los Alamos. Ulam enlisted Everett to follow a completely different approach, one guided by physical intuition. Françoise Ulam was one of a cadre of women "computers" who carried out laborious and extensive computations of thermonuclear scenarios on mechanical calculators, supplemented and confirmed by Everett's slide rule. Ulam and Fermi collaborated on further analysis of these scenarios. The results showed that, in workable configurations, a thermonuclear reaction would not ignite, and if ignited, it would not be self-sustaining. Ulam had used his expertise in Combinatorics to analyze the chain reaction in deuterium, which was much more complicated than the ones in uranium and plutonium, and he concluded that no self-sustaining chain reaction would take place at the (low) densities that Teller was considering. In late 1950, these conclusions were confirmed by von Neumann's results.
In January 1951, Ulam had another idea: to channel the mechanical shock of a nuclear explosion so as to compress the fusion fuel. On the recommendation of his wife, Ulam discussed this idea with Bradbury and Mark before he told Teller about it. Almost immediately, Teller saw its merit, but noted that soft X-rays from the fission bomb would compress the thermonuclear fuel more strongly than mechanical shock and suggested ways to enhance this effect. On 9 March 1951, Teller and Ulam submitted a joint report describing these innovations. A few weeks later, Teller suggested placing a fissile rod or cylinder at the center of the fusion fuel. The detonation of this "spark plug" would help to initiate and enhance the fusion reaction. The design based on these ideas, called staged radiation implosion, has become the standard way to build thermonuclear weapons. It is often described as the "Teller–Ulam design".
In September 1951, after a series of differences with Bradbury and other scientists, Teller resigned from Los Alamos, and returned to the University of Chicago. At about the same time, Ulam went on leave as a visiting professor at Harvard for a semester. Although Teller and Ulam submitted a joint report on their design and jointly applied for a patent on it, they soon became involved in a dispute over who deserved credit. After the war, Bethe returned to Cornell University, but he was deeply involved in the development of thermonuclear weapons as a consultant. In 1954, he wrote an article on the history of the H-bomb, which presents his opinion that both men contributed very significantly to the breakthrough. This balanced view is shared by others who were involved, including Mark and Fermi, but Teller persistently attempted to downplay Ulam's role. "After the H-bomb was made," Bethe recalled, "reporters started to call Teller the father of the H-bomb. For the sake of history, I think it is more precise to say that Ulam is the father, because he provided the seed, and Teller is the mother, because he remained with the child. As for me, I guess I am the midwife."
With the basic fusion reactions confirmed, and with a feasible design in hand, there was nothing to prevent Los Alamos from testing a thermonuclear device. On 1 November 1952, the first thermonuclear explosion occurred when Ivy Mike was detonated on Enewetak Atoll, within the US Pacific Proving Grounds. This device, which used liquid deuterium as its fusion fuel, was immense and utterly unusable as a weapon. Nevertheless, its success validated the Teller–Ulam design, and stimulated intensive development of practical weapons.
Fermi–Pasta–Ulam problem.
When Ulam returned to Los Alamos, his attention turned away from weapon design and toward the use of computers to investigate problems in physics and mathematics. With John Pasta, who helped Metropolis to bring MANIAC on line in March 1952, he explored these ideas in a report "Heuristic Studies in Problems of Mathematical Physics on High Speed Computing Machines", which was submitted on 9 June 1953. It treated several problems that cannot be addressed within the framework of traditional analytic methods: billowing of fluids, rotational motion in gravitating systems, magnetic lines of force, and hydrodynamic instabilities.
Soon, Pasta and Ulam became experienced with electronic computation on MANIAC, and by this time, Enrico Fermi had settled into a routine of spending academic years at the University of Chicago and summers at Los Alamos. During these summer visits, Pasta and Ulam joined him to study a variation of the classic problem of a string of masses held together by springs that exert forces linearly proportional to their displacement from equilibrium. Fermi proposed to add to this force a nonlinear component, which could be chosen to be proportional to either the square or cube of the displacement, or to a more complicated "broken linear" function. This addition is the key element of the Fermi–Pasta–Ulam problem, which is often designated by the abbreviation FPU.
A classical spring system can be described in terms of vibrational modes, which are analogous to the harmonics that occur on a stretched violin string. If the system starts in a particular mode, vibrations in other modes do not develop. With the nonlinear component, Fermi expected energy in one mode to transfer gradually to other modes, and eventually, to be distributed equally among all modes. This is roughly what began to happen shortly after the system was initialized with all its energy in the lowest mode, but much later, essentially all the energy periodically reappeared in the lowest mode. This behavior is very different from the expected equipartition of energy. It remained mysterious until 1965, when Kruskal and Zabusky showed that, after appropriate mathematical transformations, the system can be described by the Korteweg–de Vries equation, which is the prototype of nonlinear partial differential equations that have soliton solutions. This means that FPU behavior can be understood in terms of solitons.
Nuclear propulsion.
Starting in 1955, Ulam and Frederick Reines considered nuclear propulsion of aircraft and rockets. This is an attractive possibility, because the nuclear energy per unit mass of fuel is a million times greater than that available from chemicals. From 1955 to 1972, their ideas were pursued during Project Rover, which explored the use of nuclear reactors to power rockets. In response to a question by Senator John O. Pastore at a congressional committee hearing on "Outer Space Propulsion by Nuclear Energy", on January 22, 1958, Ulam replied that "the future as a whole of mankind is to some extent involved inexorably now with going outside the globe."
Ulam and C. J. Everett also proposed, in contrast to Rover's continuous heating of rocket exhaust, to harness small nuclear explosions for propulsion. Project Orion was a study of this idea. It began in 1958 and ended in 1965, after the Partial Nuclear Test Ban Treaty of 1963 banned nuclear weapons tests in the atmosphere and in space. Work on this project was spearheaded by physicist Freeman Dyson, who commented on the decision to end Orion in his article, "Death of a Project".
Bradbury appointed Ulam and John H. Manley as research advisors to the laboratory director in 1957. These newly created positions were on the same administrative level as division leaders, and Ulam held his until he retired from Los Alamos. In this capacity, he was able to influence and guide programs in many divisions: theoretical, physics, chemistry, metallurgy, weapons, health, Rover, and others.
In addition to these activities, Ulam continued to publish technical reports and research papers. One of these introduced the Fermi–Ulam model, an extension of Fermi's theory of the acceleration of cosmic rays. Another, with Paul Stein and Mary Tsingou, titled "Quadratic Transformations", was an early investigation of chaos theory and is considered the first published use of the phrase "chaotic behavior".
Return to academia.
During his years at Los Alamos, Ulam was a visiting professor at Harvard from 1951 to 1952, MIT from 1956 to 1957, the University of California, San Diego, in 1963, and the University of Colorado at Boulder from 1961 to 1962 and 1965 to 1967. In 1967, the last of these positions became permanent, when Ulam was appointed as professor and Chairman of the Department of Mathematics at Boulder, Colorado. He kept a residence in Santa Fe, New Mexico, which made it convenient to spend summers at Los Alamos as a consultant.
In Colorado, where he rejoined his friends Gamow, Richtmyer, and Hawkins, Ulam's research interests turned toward biology. In 1968, recognizing this emphasis, the University of Colorado School of Medicine appointed Ulam as Professor of Biomathematics, and he held this position until his death. With his Los Alamos colleague Robert Schrandt he published a report, "Some Elementary Attempts at Numerical Modeling of Problems Concerning Rates of Evolutionary Processes", which applied his earlier ideas on branching processes to biological inheritance. Another, report, with William Beyer, Temple F. Smith, and M. L. Stein, titled "Metrics in Biology", introduced new ideas about biometric distances.
When he retired from Colorado in 1975, Ulam had begun to spend winter semesters at the University of Florida, where he was a graduate research professor. Except for sabbaticals at the University of California, Davis from 1982 to 1983, and at Rockefeller University from 1980 to 1984, this pattern of spending summers in Colorado and Los Alamos and winters in Florida continued until Ulam died of an apparent heart attack in Santa Fe on 13 May 1984.
Paul Erdős noted that "he died suddenly of heart failure, without fear or pain, while he could still prove and conjecture." In 1987, Françoise Ulam deposited his papers with the American Philosophical Society Library in Philadelphia. She continued to live in Santa Fe until she died on 30 April 2011, at the age of 93. Both Françoise and her husband are buried with her French family in Montmartre Cemetery in Paris.
Impact and legacy.
From the publication of his first paper as a student in 1929 until his death, Ulam was constantly writing on mathematics. The list of Ulam's publications includes more than 150 papers. Topics represented by a significant number of papers are: set theory (including measurable cardinals and abstract measures), topology, transformation theory, ergodic theory, group theory, projective algebra, number theory, combinatorics, and graph theory. In March 2009, the Mathematical Reviews database contained 697 papers with the name "Ulam".
Notable results of this work are:
With his pivotal role in the development of thermonuclear weapons, Stanislaw Ulam changed the world. According to Françoise Ulam: "Stan would reassure me that, barring accidents, the H-bomb rendered nuclear war impossible." In 1980, Ulam and his wife appeared in the television documentary "The Day After Trinity".
The Monte Carlo method has become a ubiquitous and standard approach to computation, and the method has been applied to a vast number of scientific problems. In addition to problems in physics and mathematics, the method has been applied to finance, social science, environmental risk assessment, linguistics, radiation therapy, and sports.
The Fermi–Pasta–Ulam problem is credited not only as "the birth of experimental mathematics", but also as inspiration for the vast field of Nonlinear Science. In his Lilienfeld Prize lecture, David K. Campbell noted this relationship and described how FPU gave rise to ideas in chaos, solitons, and dynamical systems. In 1980, Donald Kerr, laboratory director at Los Alamos, with the strong support of Ulam and Mark Kac, founded the Center for Nonlinear Studies (CNLS). In 1985, CNLS initiated the "Stanislaw M. Ulam Distinguished Scholar" program, which provides an annual award that enables a noted scientist to spend a year carrying out research at Los Alamos.
The fiftieth anniversary of the original FPU paper was the subject of the March 2005 issue of the journal Chaos, and the topic of the 25th Annual International Conference of CNLS. The University of Southern Mississippi and the University of Florida supported the "Ulam Quarterly", which was active from 1992 to 1996, and which was one of the first online mathematical journals. Florida's Department of Mathematics has sponsored, since 1998, the annual "Ulam Colloquium Lecture", and in March 2009, the "Ulam Centennial Conference".
Ulam's work on non-Euclidean distance metrics in the context of molecular biology made a significant contribution to sequence analysis and his contributions in theoretical biology are considered watersheds in the development of cellular automata theory, population biology, pattern recognition, and biometrics generally. Colleagues noted that some of his greatest contributions were in clearly identifying problems to be solved and general techniques for solving them.
In 1987, Los Alamos issued a special issue of its "Science" publication, which summarized his accomplishments, and which appeared, in 1989, as the book "From Cardinals to Chaos". Similarly, in 1990, the University of California Press issued a compilation of mathematical reports by Ulam and his Los Alamos collaborators: "Analogies Between Analogies". During his career, Ulam was awarded honorary degrees by the Universities of New Mexico, Wisconsin, and Pittsburgh.

</doc>
<doc id="41533" url="http://en.wikipedia.org/wiki?curid=41533" title="György Dalos">
György Dalos

György Dalos (born September 23, 1943) is a Hungarian Jewish writer and historian. He is best known for his novel "1985", and "The Guest from the Future: Anna Akhmatova and Isaiah Berlin".
Life.
Dalos was born in Budapest and spent his childhood with his grandparents, as his father had died in 1945 in a work camp, where he had been sent to as a Jew during World War II. From 1962 to 1967, he studied history at the Lomonossov University in Moscow. He then returned to his native town Budapest to work as a museologist. In 1968, Dalos was accused of "Maoist activities" and was handed seven months prison on probation and a Berufsverbot (professional disqualification) and a publication ban; due to that, he worked as a translator. In 1977, he was among the founders of the opposition movement against the Communist regime of Hungary. In 1988/89 he was co-editor of the East German underground opposition paper "Ostkreuz". From 1995 to 1999, Dalos was head of the Institute for Hungarian Culture in Berlin. Since 2009 he is member of the International Council of Austrian Service Abroad.
Dalos lived in Vienna from 1987 to 1995. Since 1995, he has lived in Berlin as a freelance publisher and editor.
Work.
Articles

</doc>
<doc id="41534" url="http://en.wikipedia.org/wiki?curid=41534" title="Eldred v. Ashcroft">
Eldred v. Ashcroft

Eldred v. Ashcroft, (2003) was decision by the Supreme Court of the United States upholding the constitutionality of the 1998 Sonny Bono Copyright Term Extension Act (CTEA).
Background.
The Sonny Bono Copyright Term Extension Act (or CTEA) extended existing copyright terms by an additional 20 years from the terms set by the Copyright Act of 1976. The law affected both new and existing works (making it both a "prospective" extension as well as a "retroactive" one). Specifically, for works published before January 1, 1978 and still in copyright on October 27, 1998, the term was extended to 95 years. For works authored by "individuals" on or after January 1, 1978 (including new works), the copyright term was extended to equal the life of the author plus 70 years. For works authored by joint authors, the copyright term was extended to the life of the last surviving author plus 70 years. In the case of works-for-hire, anonymous or pseudonymous works, the term was set at 95 years from the date of first publication, or 120 years from creation.
The practical result of this was to prevent a number of works from entering the public domain in 1998 and following years, as would have occurred under the Copyright Law of 1976. Materials which the plaintiffs had worked with and were ready to republish were now unavailable due to copyright restrictions.
The lead petitioner, Eric Eldred, is an Internet publisher. Eldred was joined by a group of commercial and non-commercial interests who relied on the public domain for their work. These included Dover Publications, a commercial publisher of paperback books; Luck's Music Library, Inc., and Edwin F. Kalmus & Co., Inc., publishers of orchestral sheet music; and a large number of "amici" including the Free Software Foundation, the American Association of Law Libraries, the Bureau of National Affairs, and the College Art Association.
Supporting the law were the U.S. government, represented by the Attorney General in an "ex officio" capacity (originally Janet Reno, later replaced by John Ashcroft), along with a set of "amici" including the Motion Picture Association of America, the Recording Industry Association of America, ASCAP and Broadcast Music Incorporated.
District court.
The original complaint was filed in the United States District Court for the District of Columbia on January 11, 1999. The plaintiffs' argument was threefold:
In response, the government argued that Congress does indeed have the latitude to retroactively extend terms, so long as the individual extensions are also for "limited times," as required by the Constitution. As an argument for this position, they referred to the Copyright Act of 1790, the first Federal copyright legislation, which applied Federal protection to existing works. Furthermore, they argued, neither the First Amendment nor the doctrine of public trust is applicable to copyright cases.
On October 28, 1999, Judge June Green issued a brief opinion rejecting all three of the petitioners' arguments. On the first count, she wrote that Congress had the power to extend terms as it wished, as long as the terms themselves were of limited duration. On the second count, she rejected the notion of First Amendment scrutiny in copyright cases, based on her interpretation of "Harper and Row Publishers, Inc., v. Nation Enterprises", an earlier Supreme Court decision. On the third count, she rejected the notion that public trust doctrine was applicable to copyright law.
Court of Appeals.
The plaintiffs appealed the decision of the district court to the United States Court of Appeals for the District of Columbia Circuit, filing their initial brief on May 22, 2000, and arguing the case on October 5 of the same year in front of a three-judge panel. Arguments were similar to those made in the district court, except for those regarding the public trust doctrine, which were not included in the appeal.
Instead, the plaintiffs extended their argument on the copyright clause to note that the clause requires Congress to "promote the Progress of Science and useful Arts," and argued that retroactive extensions do not directly serve this purpose in the standard "quid pro quo" previously required by the courts.
The case was decided on February 16, 2001. The appeals court upheld the decision of the district court in a 2-1 opinion. In his dissent, Judge David Sentelle agreed with the plaintiffs that CTEA was indeed unconstitutional based on the "limited Times" requirement. Supreme Court precedent, he argued, held that one must be able to discern an "outer limit" to a limited power; in the case of retrospective copyright extensions, Congress could continue to extend copyright terms indefinitely through a set of limited extensions, thus rendering the "limited times" requirement meaningless.
Following this ruling, plaintiffs petitioned for a rehearing "en banc" (in front of the full panel of nine judges). This petition was rejected, 7–2, with Judges Sentelle and David Tatel dissenting.
Supreme Court.
On October 11, 2001, the plaintiffs filed a petition for certiorari to the Supreme Court of the United States. On February 19, 2002, the Court granted Certiorari, agreeing to hear the case.
Oral arguments were presented on October 9, 2002. Lead counsel for the plaintiff was Lawrence Lessig; the government's case was argued by Solicitor General Theodore Olson.
Lessig refocused the Plaintiffs' brief to emphasize the Copyright clause restriction, as well as the First Amendment argument from the Appeals case. The decision to emphasize the Copyright clause argument was based on both the minority opinion of Judge Sentelle in the appeals court, and on several recent Supreme Court decisions authored by Chief Justice William Rehnquist: "United States v. Lopez" and "United States v. Morrison".
In both of those decisions, Rehnquist, along with four of the Court's more conservative justices, held Congressional legislation unconstitutional, because said legislation exceeded the limits of the Constitution's Commerce clause. This profound reversal of precedent, Lessig argued, could not be limited to only one of the enumerated powers. If the court felt that it had the power to review legislation under the Commerce clause, Lessig argued, then the Copyright clause deserved similar treatment, or at very least a "principled reason" must be stated for according such treatment to only one of the enumerated powers.
On January 15, 2003, the Court held the CTEA constitutional by a 7–2 decision. The majority opinion, written by Justice Ginsburg, relied heavily on the Copyright Acts of 1790, 1831, 1909, and 1976 as precedent for retroactive extensions. One of the arguments supporting the act was the life expectancy has significantly increased among the human population since the 18th century, and therefore copyright law needed extending as well. However, the major argument for the act that carried over into the case was that the Constitution specified that Congress only needed to set time limits for copyright, the length of which was left to their discretion. Thus, as long as the limit is not "forever," any limit set by Congress can be deemed constitutional.
A key factor in the CTEA’s passage was a 1993 European Union (EU) directive instructing EU members to establish a baseline copyright term of life plus 70 years and to deny this longer term to the works of any non-EU country whose laws did not secure the same extended term. By extending the baseline United States copyright term, Congress sought to ensure that American authors would receive the same copyright protection in Europe as their European counterparts.
The Supreme Court declined to address Lessig's contention that "Lopez" and "Morrison" offered precedent for enforcing the Copyright clause, and instead reiterated the lower court's reasoning that a retroactive term extension can satisfy the "limited times" provision in the copyright clause, as long as the extension itself is limited instead of perpetual. Furthermore, the Court refused to apply the proportionality standards of the Fourteenth Amendment or the free-speech standards in the First Amendment to limit Congress's ability to confer copyrights for limited terms.
Justice Breyer dissented, arguing that the CTEA amounted to a grant of perpetual copyright that undermined public interests. While the constitution grants Congress power to extend copyright terms in order to "promote the progress of science and useful arts," CTEA granted precedent to continually renew copyright terms making them virtually perpetual. Justice Breyer argued in his dissent that it is highly unlikely any artist will be more inclined to produce work knowing their great-grandchildren will receive royalties. With regard to retroactive copyright extension, he viewed it foolish to apply the government's argument that income received from royalties allows artists to produce more work saying, "How will extension help today’s Noah Webster create new works 50 years after his death?".
In a separate dissenting opinion, Justice Stevens also challenged the virtue of an individual reward, analyzing it from the perspective of patent law. He argued that the focus on compensation results only in “frustrating the legitimate members of the public who want to make use of it (a completed invention) in a free market.” Further, the compelling need to encourage creation is proportionally diminished once a work is already created. Yet while a formula pairing commercial viability to duration of protection may be said to produce more economically efficient results in respect of high technology inventions with shorter shelf-lives, the same perhaps cannot be said for certain forms of copyrighted works, for which the present value of expenditures relating to creation depend less on scientific equipment and research and development programmes and more on unquantifiable creativity.
Lessig expressed surprise that no decision was authored by Chief Justice Rehnquist or by any of the other four justices who supported the "Lopez" or "Morrison" decisions. Lessig later expressed regret that he based his argument on precedent rather than attempting to demonstrate that the weakening of the public domain would cause harm to the economic health of the country.
External links.
 Works related to at Wikisource

</doc>
<doc id="41535" url="http://en.wikipedia.org/wiki?curid=41535" title="Bix Beiderbecke">
Bix Beiderbecke

Leon "Bix" Beiderbecke (March 10, 1903 – August 6, 1931) was an American jazz cornetist, jazz pianist, and composer.
With Louis Armstrong and Muggsy Spanier, Beiderbecke was one of the most influential jazz soloists of the 1920s. His turns on "Singin' the Blues" and "I'm Coming, Virginia" (both 1927), in particular, demonstrated an unusual purity of tone and a gift for improvisation. With these two recordings, especially, he helped to invent the jazz ballad style and hinted at what, in the 1950s, would become cool jazz. "In a Mist" (1927), one of a handful of his piano compositions and one of only two he recorded, mixed classical (Impressionist) influences with jazz syncopation.
A native of Davenport, Iowa, Beiderbecke taught himself to play cornet largely by ear, leading him to adopt a non-standard fingering some critics have connected to his original sound. He first recorded with Midwestern jazz ensembles, The Wolverines and The Bucktown Five in 1924, after which he played briefly for the Detroit-based Jean Goldkette Orchestra before joining Frankie "Tram" Trumbauer for an extended gig at the Arcadia Ballroom in St. Louis. Beiderbecke and Trumbauer joined Goldkette in 1926. The band toured widely and famously played a set opposite Fletcher Henderson at the Roseland Ballroom in New York City in October 1926. He made his greatest recordings in 1927 (see above). In 1928, Trumbauer and Beiderbecke left Detroit to join the best-known and most prestigious dance orchestra in the country: the New-York-based Paul Whiteman Orchestra.
Beiderbecke's most influential recordings date from his time with Goldkette and Whiteman, although they were generally recorded under his own name or Trumbauer's. The Whiteman period also marked a precipitous decline in Beiderbecke's health, brought on by the demand of the bandleader's relentless touring and recording schedule in combination with Beiderbecke's persistent alcoholism. A few stints in rehabilitation centers, as well as the support of Whiteman and the Beiderbecke family in Davenport, did not check Beiderbecke's decline in health. He left the Whiteman band in 1930 and the following summer died in his Queens apartment at the age of 28.
His death, in turn, gave rise to one of the original legends of jazz. In magazine articles, musicians' memoirs, novels, and Hollywood films, Beiderbecke has been reincarnated as a Romantic hero, the "Young Man with a Horn". His life has been portrayed as a battle against such common obstacles to art as family and commerce, while his death has been seen as a martyrdom for the sake of art. The musician-critic Benny Green sarcastically called Beiderbecke "jazz's Number One Saint," while Ralph Berton compared him to Jesus. Beiderbecke remains the subject of scholarly controversy regarding his true name, the cause of his death, and the importance of his contributions to jazz.
Early life.
Bix Beiderbecke was born on March 10, 1903, in Davenport, Iowa, the son of Bismark Herman and Agatha Jane (Hilton) Beiderbecke. There is disagreement over whether Beiderbecke was christened Leon Bismark (and nicknamed "Bix") or Leon Bix. His father was nicknamed "Bix", as, for a time, was his older brother, Charles Burnette "Burnie" Beiderbecke. Burnie Beiderbecke claimed that the boy was named Leon Bix and subsequent biographers have reproduced birth certificates to that effect. However, more recent research—which takes into account church and school records in addition to the will of a relative—has suggested that he was originally named Leon Bismark. Regardless, his parents called him Bix, which seems to have been his preference. In a letter to his mother when he was nine years old, Beiderbecke signed off, "frome your Leon Bix Beiderbecke not Bismark Remeber ["sic"]".
Beiderbecke's father, the son of German immigrants, was a well-to-do coal and lumber merchant, named after the Iron Chancellor of his native Germany. Beiderbecke's mother was the daughter of a Mississippi riverboat captain. She played the organ at Davenport's First Presbyterian Church, and encouraged young Bix's interest in the piano. Bix Beiderbecke was the youngest of three children. His brother, Burnie, was born in 1895, and his sister, Mary Louise, in 1898. Bix began playing piano at age two or three. His sister recalls that he stood on the floor and played it with his hands over his head. Five years later, he was the subject of an admiring article in the "Davenport Daily Democrat" that proclaimed: "Seven-year-old boy musical wonder! Little Bickie Beiderbecke plays any selection he hears."
At age ten, his older brother Burnie recalled that he stopped coming home for supper, instead hurrying down to the riverfront and slipping aboard one or another of the excursion boats to play the Calliope. A friend remembered that the plots of the silent matinees Bix and his friends watched on Saturdays didn't interest him much, but as soon as the lights came on he would rush home to see if he could duplicate the melodies the accompanist had played during the action.
When his brother Burnie returned to Davenport at the end of 1918 after serving stateside during World War I, he brought with him a Victrola phonograph and several records, including "Tiger Rag" and "Skeleton Jangle" by the Original Dixieland Jazz Band. From these records, Bix first learned to love hot jazz; he taught himself to play cornet by listening to Nick LaRocca's horn lines. Beiderbecke also listened to jazz music off the riverboats that docked in downtown Davenport. Louis Armstrong and the drummer Baby Dodds claimed to have met Beiderbecke when their New-Orleans-based excursion boat stopped in Davenport. Historians disagree over whether that is true.
Beiderbecke attended Davenport High School from 1919 to 1921. During this time, he sat in and played professionally with various bands, including those of Wilbur Hatch, Floyd Bean and Carlisle Evans. In the spring of 1920 he performed for the school's Vaudeville Night, singing in a vocal quintet called the Black Jazz Babies and playing his horn. He also performed, at the invitation of his friend Fritz Putzier, in Neal Buckley's Novelty Orchestra. The group was hired for a gig in December 1920, but a complaint was lodged with the American Federation of Musicians, Local 67, that the boys did not have union cards. In an audition before a union executive, Beiderbecke was forced to sight read and failed. He did not earn his card.
On April 22, 1921, a month after he turned 18, Beiderbecke was arrested by two Davenport police officers on a charge brought by the father of a young girl. According to biographer Jean Pierre Lion, "Bix was accused of having taken this man's five-year-old daughter into a garage and committing on her an act qualified by the police report as 'lewd and lascivious.'" Although Beiderbecke was briefly taken into custody and held on a $1,500 bond, the charge was dropped after the girl was not made available to testify. According to an affidavit submitted by her father, this was because "of the child's age and the harm that would result to her in going over this case." It is not clear from the father's affidavit if the girl had identified Beiderbecke. Until recently, biographers have largely ignored this incident in Beiderbecke's life, and Lion was the first, in 2005, to print the police blotter and affidavit associated with the arrest. He dismissed the seriousness of the charge, but speculated that the arrest nevertheless might have led Beiderbecke to "feel abandoned and ashamed: he saw himself as suspect of perversion." Beiderbecke fans and scholars continue to argue over this incident's relevance and importance.
Beiderbecke's parents enrolled him in the exclusive Lake Forest Academy, north of Chicago in Lake Forest, Illinois. While historians have traditionally suggested that his parents sent him to Lake Forest to discourage his interest in jazz, others have begun to doubt this version of events, believing that he may have been sent away in response to his arrest. Regardless, Mr. and Mrs. Beiderbecke apparently felt that a boarding school would provide their son with both the necessary faculty attention and discipline to improve his academic performance. His interests, however, remained limited to music and sports. In pursuit of the former, Beiderbecke took the train into Chicago to catch the hot jazz bands at clubs and speakeasies, including the infamous Friar's Inn, where he listened to and sometimes sat in with the New Orleans Rhythm Kings. He also traveled to the predominantly African-American South Side to listen to what he called "real" jazz musicians. "Don't think I'm getting hard, Burnie," he wrote to his brother, "but I'd go to hell to hear a good band." On campus, he helped organize the Cy-Bix Orchestra with drummer Walter "Cy" Welge and almost immediately got into trouble with the Lake Forest headmaster for performing indecorously at a school dance.
Beiderbecke often failed to return to his dormitory before curfew, and sometimes stayed off-campus the next day. In the early morning hours of May 20, he was caught on the fire escape to his dormitory, attempting to climb back into his room. The faculty voted to expel him the next day, due both to his academic failings and his extracurricular activities, which included drinking. The headmaster informed Beiderbecke's parents by letter that following his expulsion school officials confirmed that Beiderbecke "was drinking himself and was responsible, in part at least, in having liquor brought into the School." Soon after, Beiderbecke began pursuing a career in music.
He returned to Davenport briefly in the summer of 1922, then moved to Chicago to join the Cascades Band, working that summer on Lake Michigan excursion boats. He gigged around Chicago until the fall of 1923, at times returning to Davenport to work for his father.
Career.
Wolverines.
Beiderbecke joined the Wolverine Orchestra late in 1923, and the seven-man group first played a speakeasy called the Stockton Club near Hamilton, Ohio. Specializing in hot jazz and recoiling from so-called sweet music, the band took its name from one of its most frequent numbers, Jelly Roll Morton's "Wolverine Blues." During this time, Beiderbecke also took piano lessons from a young woman who introduced him to the works of Eastwood Lane. Lane's piano suites and orchestral arrangements were both self-consciously American and influenced by the French Impressionists, and it is said to have greatly influenced Beiderbecke's style, especially on "In a Mist." A subsequent gig at Doyle's Dance Academy in Cincinnati became the occasion for a series of band and individual photographs that resulted in the most famous image of Beiderbecke—sitting fresh-faced, his hair perfectly combed, his horn resting on his right knee.
On February 18, 1924, the Wolverines first recorded at Gennett Records in Richmond, Indiana. Their two sides that day included "Fidgety Feet", written by Nick LaRocca and Larry Shields from the Original Dixieland Jazz Band, and "Jazz Me Blues." Beiderbecke's solo on the latter suggested something new and significant in jazz, according to biographers Richard M. Sudhalter and Philip R. Evans:
Both qualities—complementary or "correlated" phrasing and cultivation of the vocal, "singing" middle-range of the cornet—are on display in Bix's "Jazz Me Blues" solo, along with an already discernible inclination for unusual accidentals and inner chordal voices. It is a pioneer record, introducing a musician of great originality with a pace-setting band. And it astonished even the Wolverines themselves.
The Wolverines recorded 15 sides for Gennett Records between February and October 1924. The titles revealed a tough and well-formed cornet talent. His lip had toughened from earlier, more tentative years; on nine of the Wolverines' recorded titles he proceeds commandingly from lead to opening solo without any need for a respite from playing.
Beiderbecke made his first recordings 21 months before Armstrong recorded as a leader with the Hot Five. Beiderbecke's style was very different from that of Louis Armstrong according to "The Oxford Companion to Jazz":
Where Armstrong's playing was bravura, regularly optimistic, and openly emotional, Beiderbecke's conveyed a range of intellectual alternatives. Where Armstrong, at the head of an ensemble, played it hard, straight, and true, Beiderbecke, like a shadowboxer, invented his own way of phrasing "around the lead." Where Armstrong's superior strength delighted in the sheer power of what a cornet could produce, Beiderbecke's cool approach invited rather than commanded you to listen.
Where Armstrong emphasized showmanship and virtuosity, Beiderbecke emphasized melody, even when improvising, and—different from Armstrong and contrary to how the Bix Beiderbecke of legend would be portrayed—he rarely strayed into the upper reaches of the register. Paul Mares of the New Orleans Rhythm Kings insisted that Beiderbecke's chief influence was the New Orleans cornetist Emmett Hardy, who died in 1925 at the age of 23. Indeed, Beiderbecke had met Hardy and the clarinetist Leon Roppolo in Davenport in 1921 when the two joined a local band and played in town for three months. Beiderbecke apparently spent time with them, but the degree to which Hardy's style influenced Beiderbecke's is difficult to know because Hardy never recorded. In some respects, Beiderbecke's playing was "sui generis", but he nevertheless listened to and studied the music around him: from Armstrong and Joe "King" Oliver to the Original Dixieland Jazz Band and the New Orleans Rhythm Kings to Claude Debussy and Maurice Ravel.
Soon, he was listening to Hoagy Carmichael, too. A law student and aspiring pianist and songwriter, Carmichael invited the Wolverines to Bloomington, Indiana, late in April 1924. Beiderbecke had met Carmichael a couple of times before and the two became friends. On May 6, 1924, the Wolverines recorded a tune Carmichael had written especially for Beiderbecke and his colleagues: "Riverboat Shuffle".
Beiderbecke left the Wolverines in October 1924 for a spot with Jean Goldkette in Detroit, but the job didn't last long. Goldkette recorded for the Victor Talking Machine Company, whose musical director, Eddie King, objected to Beiderbecke's hot-jazz style of soloing; it wasn't copacetic with the commercial obligations that came with the band's recording contract. King also was frustrated by the cornetist's inability to deftly sight read. After a few weeks, Beiderbecke was bounced from the Goldkette band, but soon arranged a recording session back in Richmond with some of its members. On January 26, 1925, Bix and His Rhythm Jugglers set two tunes to wax: "Toddlin' Blues", another number by LaRocca and Shields, and Beiderbecke's own composition, "Davenport Blues". Beiderbecke biographer Lion has complained that the second number was marred by the alcohol consumed by the musicians. In subsequent years, "Davenport Blues" has been recorded by musicians from Bunny Berigan to Ry Cooder to Geoff Muldaur.
The following month, Beiderbecke enrolled at the University of Iowa in Iowa City, Iowa. His stint in academia was even briefer than his time in Detroit, however. When he attempted to pack his course schedule with music, his guidance counselor forced him instead to take religion, ethics, physical education, and military training. It was an institutional blunder that Benny Green described as being, in retrospect, "comical," "fatuous," and "a parody." Beiderbecke promptly began to skip classes, and after he participated in a drunken bar fight, he was expelled. That summer he played with his friends Don Murray and Howdy Quicksell at a lake resort in Michigan. The band was run by Goldkette, and it put Beiderbecke in touch with another musician he had met before: the C-melody saxophone player Frankie Trumbauer. The two hit it off, both personally and musically, despite Trumbauer having been warned by other musicians: "Look out, he's trouble. He drinks and you'll have a hard time handling him." They were inseparable for much of the rest of Beiderbecke's career, with Trumbauer acting as a father figure to Beiderbecke. When Trumbauer organized a band for an extended run at the Arcadia Ballroom in St. Louis, Beiderbecke joined him. There he also played alongside the clarinetist Pee Wee Russell, who praised Beiderbecke's ability to drive the band. "He more or less made you play whether you wanted to or not," Russell said. "If you had any talent at all he made you play better."
Goldkette.
In the spring of 1926, Trumbauer closed up shop in St. Louis and, with Beiderbecke, moved to Detroit, this time to play with Goldkette's headline ensemble. They played the summer at Hudson Lake, a resort in northern Indiana, and split the next year between touring, recording, and performing at Detroit's Graystone Ballroom. In October 1926, Goldkette's "Famous Fourteen", as they came to be called, opened at the Roseland Ballroom in New York City opposite the Fletcher Henderson Orchestra, one of the East Coast's outstanding African American big bands. The Roseland promoted a "Battle of the Bands" in the local press and, on October 12, after a night of furious playing, Goldkette's men were declared the winners. "We [...] were amazed, angry, morose, and bewildered," Rex Stewart, Fletcher's lead trumpeter, said of listening to Beiderbecke and his colleagues play. He called the experience "most humiliating".
Although the band recorded numerous sides for Victor during this period, none of them showcases Beiderbecke's most famous solos. Much of Goldkette's money was made through these records, but they were subject—as Eddie King had well understood—to the forces of the commercial market. As a result, their sound was often "sweeter" than what many of the hot jazz musicians would have preferred. In addition to their sessions with Goldkette, Beiderbecke and his friends recorded under their own names for the Okeh label. For instance, on February 4, 1927, Frank Trumbauer and His Orchestra recorded "Trumbology", "Clarinet Marmalade", and "Singin' the Blues", all three of which featured some of Beiderbecke's best work. Again with Trumbauer, Beiderbecke re-recorded Carmichael's "Riverboat Shuffle" in May and delivered two of his best known solos a few days later on "I'm Coming, Virginia" and "Way Down Yonder in New Orleans". Beiderbecke earned co-writing credit with Trumbauer on "For No Reason at All in C", recorded under the name Tram, Bix and Eddie (in their Three Piece Band). Beiderbecke switched between cornet and piano on that number, and then in September played only piano for his recording of "In A Mist". This was perhaps the most fruitful year of his short career.
Under financial pressure, Goldkette folded his premier band in September in New York. Paul Whiteman hoped to snatch up Goldkette's best musicians for his traveling orchestra, but Beiderbecke, Trumbauer, Murray, Bill Rank, Eddie Lang, Joe Venuti, Chauncey Morehouse, and Frank Signorelli instead joined the bass saxophone player Adrian Rollini at the Club New Yorker. When that job ended sooner than expected, in October 1927, Beiderbecke and Trumbauer signed on with Whiteman. They joined his orchestra in Indianapolis on October 27.
Whiteman.
The Paul Whiteman Orchestra was the most popular and highest paid band of the day. In spite of Whiteman's nickname, "The King of Jazz", his was not a jazz ensemble, but a popular music outfit that played bits of jazz and classical music according to the demands of its record-buying and concert-going audience. Whiteman was perhaps best known for having premiered George Gershwin's "Rhapsody in Blue" in New York in 1924, and the orchestrator of that piece, Ferde Grofé, continued to be an important part of the band in 1928. At three hundred pounds, Whiteman was huge both physically and culturally—"a man flabby, virile, quick, coarse, untidy and sleek, with a hard core of shrewdness in an envelope of sentimentalism," according to a 1926 "New Yorker" profile. And many Beiderbecke partisans have turned Whiteman into a villain in the years since.
Benny Green, in particular, derided Whiteman for being a mere "mediocre vaudeville act," and suggesting that "today we only tolerate the horrors of Whiteman's recordings at all in the hope that here and there a Bixian fragment will redeem the mess." Richard Sudhalter has responded by suggesting that Beiderbecke saw Whiteman as an opportunity to pursue musical ambitions that did not stop at jazz:
Colleagues have testified that, far from feeling bound or stifled by the Whiteman orchestra, as Green and others have suggested, Bix often felt a sense of exhilaration. It was like attending a music school, learning and broadening: formal music, especially the synthesis of the American vernacular idiom with a more classical orientation, so much sought-after in the 1920s, were calling out to him.
The education that Beiderbecke did not receive from the University of Iowa, in other words, he sought through Whiteman. In the meantime, Beiderbecke played on four number-one records in 1928, all under the Whiteman name: "Together", "Ramona", "My Angel", and "Ol' Man River", which featured Bing Crosby on vocals. This accomplishment says less about the jazz excellence of these records than it does about the tastes of the largely white, record-buying public to which Whiteman (and Goldkette before him) catered.
For Beiderbecke, the downside of being with Whiteman was the relentless touring and recording schedule, exacerbated by Beiderbecke's alcoholism. On November 30, 1928, in Cleveland, Beiderbecke suffered what Lion terms "a severe nervous crisis" and Sudhalter and Evans suggest "was in all probability an acute attack of delirium tremens," presumably triggered by Beiderbecke's attempt to curb his alcohol intake. "He cracked up, that's all," trombonist Bill Rank said. "Just went to pieces; broke up a roomful of furniture in the hotel."
In February 1929, Beiderbecke returned home to Davenport to convalesce and was hailed by the local press as "the world's hottest cornetist." He then spent the summer with Whiteman's band in Hollywood in preparation for the shooting of a new talking picture, "The King of Jazz". Production delays prevented any real work from being done on the film, leaving Beiderbecke and his pals plenty of time to drink heavily. By September, he was back in Davenport, where his parents helped him to seek treatment. He spent a month, from October 14 until November 18, at the Keeley Institute in Dwight, Illinois.
While he was away, Whiteman famously kept a chair empty in Beiderbecke's honor. But when he returned to New York at the end of January 1930, the renowned soloist did not rejoin Whiteman and performed only sparingly. On his last recording session, in New York, on September 15, 1930, Beiderbecke played on the original recording of Hoagy Carmichael's new song, "Georgia on My Mind", with Carmichael doing the vocal, Eddie Lang on guitar, Joe Venuti on violin, Jimmy Dorsey on clarinet and alto saxophone, Jack Teagarden on trombone, and Bud Freeman on tenor saxophone. The song would go on to become a jazz and popular music standard. In 2014, the 1930 recording of "Georgia on My Mind" was inducted into the Grammy Hall of Fame.
Two years earlier, Beiderbecke had influenced another Carmichael standard, "Star Dust". A Beiderbecke riff caught in Carmichael's head and became the tune's chorus. Bing Crosby, who sang with Whiteman, also cited Beiderbecke as an important influence. "Bix and all the rest would play and exchange ideas on the piano," he said.
With all the noise [of a New York pub] going on, I don't know how they heard themselves, but they did. I didn't contribute anything, but I listened and learned [...] I was now being influenced by these musicians, particularly horn men. I could hum and sing all of the jazz choruses from the recordings made by Bix, Phil Napoleon, and the rest.
Following the Wall Street Crash of 1929, the once-booming music industry contracted and work became more difficult to find. For a while, Beiderbecke's only income came from a radio show booked by Whiteman, "The Camel Pleasure Hour". However, during a live broadcast on October 8, 1930, Beiderbecke's seemingly limitless gift for improvisation finally failed him: "He stood up to take his solo, but his mind went blank and nothing happened," recalled a fellow musician, Frankie Cush. Whiteman finally let Beiderbecke go. The cornetist spent the rest of the year at home in Davenport and then, in February 1931, he returned to New York one last time.
Death.
Beiderbecke died in his apartment, No. 1G, 43-30 46th Street, in Sunnyside, Queens, on Thursday, August 6, 1931. The week had been quite hot, making sleep difficult, and late into the evenings, Beiderbecke had played piano, both to the annoyance and to the delight of his neighbors. On the evening of August 6, at about 9.30 pm, his rental agent, George Kraslow, heard noises coming from across the hallway. "His hysterical shouts brought me to his apartment on the run," Kraslow told Philip Evans in 1959.
He pulled me in and pointed to the bed. His whole body was trembling violently. He was screaming there were two Mexicans hiding under his bed with long daggers. To humor him, I looked under the bed and when I rose to assure him there was no one hiding there, he staggered and fell, a dead weight, in my arms. I ran across the hall and called in a woman doctor, Dr. Haberski, to examine him. She pronounced him dead.
Historians have disagreed over the identity of the doctor who pronounced Beiderbecke dead. The official cause of death, meanwhile, was lobar pneumonia, with scholars continuing to debate the extent to which his alcoholism was also a factor. Beiderbecke's mother and brother took the train to New York and brought his body home to Davenport. He was buried there on August 11 in the family plot at Oakdale Cemetery.
Legend and legacy.
At the time of his death Beiderbecke was little known except among fellow musicians, and for several years critics paid little attention to his music. As Jean Pierre Lion has pointed out, "The only serious and analytical obituary to have been published in the months" after his death was by a Frenchman, Hugues Panassié. The notice appeared in October 1931 and began with a bit of hyperbole and an incorrect fact, two hallmarks of much of the subsequent writing about Beiderbecke: "The announcement of Bix Beiderbecke's death plunged all jazz musicians into despair. We first believed it was a false alarm, as we had heard so often before about Bix. Unfortunately, precise information has been forthcoming, and we even know the day—August 7—when he passed away."
The "New Republic" critic Otis Ferguson wrote two short articles for the magazine, "Young Man with a Horn" (July 29, 1936) and "Young Man with a Horn Again" (November 18, 1940), that worked to revive interest not only in Beiderbecke's music but also in his biography. Beiderbecke "lived very briefly [...] in what might be called the servants' entrance to art," Ferguson wrote. "His story is a good story, quite humble and right." The romantic notion of the short-lived, doomed jazz genius can be traced back at least as far as Beiderbecke, and lived on in Charlie Parker, Billie Holiday and many more.
Ferguson's sense of what was "right" became the basis for the Beiderbecke Romantic legend, which has traditionally emphasized the musician's Iowa roots, his often careless dress, his difficulty sight reading, the purity of his tone, his drinking, and his early death. These themes were repeated by Beiderbecke's friends in various memoirs, including "The Stardust Road" (1946) and "Sometimes I Wonder" (1965) by Hoagy Carmichael, "Really the Blues" (1946) by Mezz Mezzrow, and "We Called It Music" (1947) by Eddie Condon. Beiderbecke was portrayed as a tragic genius along the lines of Ludwig van Beethoven. "For his talent there were no conservatories to get stuffy in, no high-trumpet didoes to be learned doggedly, note-perfect as written," Ferguson wrote, "because in his chosen form the only writing of any account was traced in the close shouting air of Royal Gardens, Grand Pavilions, honkeytonks, etc." He was "this big overgrown kid, who looked like he'd been snatched out of a cradle in the cornfields," Mezzrow wrote. "The guy didn't have an enemy in the world," recalled Beiderbecke's friend Russ Morgan, "[b]ut he was "out of this world" most of the time." According to Ralph Berton, he was "as usual gazing off into his private astronomy," but his cornet, Condon famously quipped, sounded "like a girl saying yes."
In 1938, Dorothy Baker borrowed the titles of her friend Otis Ferguson's two articles and published the novel "Young Man with a Horn". Her story of the doomed trumpet player Rick Martin was inspired, she wrote, by "the music, but not the life" of Beiderbecke, but the image of Martin quickly became the image of Beiderbecke: His story is about "the gap between the man's musical ability and his ability to fit it to his own life." In 1950, Michael Curtiz directed the film "Young Man with a Horn", starring Kirk Douglas, Lauren Bacall, and Doris Day. In this version, in which Hoagy Carmichael also plays a role, the Rick Martin character lives.
In "Blackboard Jungle", a 1955 film starring Glenn Ford and Sidney Poitier, Beiderbecke's music is briefly featured, but as a symbol of cultural conservatism in a nation on the cusp of the rock and roll revolution.
In 1971, on the 40th anniversary of Beiderbecke's death, the Bix Beiderbecke Memorial Jazz Festival was founded in Davenport, Iowa, to honor the musician. In 1974, Sudhalter and Evans published their biography, "Bix: Man and Legend", which was nominated for a National Book Award. In 1977, the Beiderbecke childhood home at 1934 Grand Avenue in Davenport was added to the National Register of Historic Places.
Beiderbecke's music was featured in three British comedy drama television series, all written by Alan Plater: "The Beiderbecke Affair" (1984), "The Beiderbecke Tapes" (1987), and "The Beiderbecke Connection" (1988). In 1991, the Italian director Pupi Avati released "Bix: An Interpretation of a Legend". Filmed partially in the Beiderbecke home, which Avati had purchased and renovated, "Bix" was screened at the Cannes Film Festival.
At the beginning of the 21st century, Beiderbecke's music continues to reside mostly out of the mainstream and some of the facts of his life are still debated, but scholars largely agree—due in part to the influence of Sudhalter and Evans—that he was an important innovator in early jazz; jazz cornetists, including Sudhalter (before his death in 2008), and Tom Pletcher, closely emulate his style. In 2003, to mark the hundredth anniversary of his birth, the Greater Astoria Historical Society and other community organizations, spearheaded by Paul Maringelli and The Bix Beiderbecke Sunnyside Memorial Committee, erected a plaque in Beiderbecke's honor at the apartment building in which he died in Queens. That same year, Frederick Turner published his novel "1929", which followed the facts of Beiderbecke's life fairly closely, focusing on his summer in Hollywood and featuring appearances by Al Capone and Clara Bow. The critic and musician Digby Fairweather sums up Beiderbecke's musical legacy, arguing that "with Louis Armstrong, Bix Beiderbecke was the most striking of jazz's cornet (and of course, trumpet) fathers; a player who first captivated his 1920s generation and after his premature death, founded a dynasty of distinguished followers beginning with Jimmy McPartland and moving on down from there."
Music.
Style and influence.
Bix Beiderbecke and Louis Armstrong were among jazz's first soloists. In New Orleans, jazz had been ensemble playing, with the various instruments weaving their parts into a single and coherent aural tapestry. There had been soloists, to be sure, with the clarinetist Sidney Bechet the best known among them, but these players "lacked the technical resources and, even more, the creative depth to make the solo the compelling centerpiece of jazz music." That changed in 1924 when Beiderbecke and Armstrong began to make their most important records. According to the critic Terry Teachout, they are "the two most influential figures in the early history of jazz" and "the twin lines of descent from which most of today's jazz can be traced."
Beiderbecke's cornet style is often described by contrasting it with Armstrong's markedly different approach. Armstrong was a virtuoso on his instrument, and his solos often took advantage of that fact. Beiderbecke was largely, although not completely, self-taught, and the constraints imposed by that fact were evident in his music. While Armstrong often soared into the upper register, Beiderbecke stayed in the middle range, more interested in exploring the melody and harmonies than in dazzling the audience. Armstrong often emphasized the performance aspect of his playing, while Beiderbecke tended to stare at his feet while playing, uninterested in personally engaging his listeners. Armstrong was deeply influenced by the blues, while Beiderbecke was influenced as much by modernist composers such as Debussy and Ravel as by his fellow jazzmen.
Beiderbecke's most famous solo was on "Singin' the Blues", recorded February 4, 1927. It has been hailed as an important example of the "jazz ballad style"—"a slow or medium-tempo piece played gently and sweetly, but not cloyingly, with no loss of muscle." The tune's laid-back emotions hinted at what would become, in the 1950s, the cool jazz style, personified by Chet Baker and Bill Evans. More than that, though, "Singin' the Blues" has been noted for the way its improvisations feel less improvised than composed, with each phrase building on the last in a logical fashion. Benny Green describes the solo's effect on practiced ears:
When a musician hears Bix's solo on 'Singing the Blues', he becomes aware after two bars that the soloist knows exactly what he is doing and that he has an exquisite sense of discord and resolution. He knows also that this player is endowed with the rarest jazz gift of all, a sense of form which lends to an improvised performance a coherence which no amount of teaching can produce. The listening musician, whatever his generation or his style, recognizes Bix as a modern, modernism being not a style but an attitude.
Like Green, who made particular mention of Beiderbecke's "amount of teaching," the jazz historian Ted Gioia also has emphasized Beiderbecke's lack of formal instruction, suggesting that it caused him to adopt "an unusual, dry embouchure" and "unconventional fingerings," which he retained for the rest of his life. Gioia points to "a characteristic streak of obstinacy" in Beiderbecke that provokes "this chronic disregard of the tried-and-true." He argues that this stubbornness was behind Beiderbecke's decision not to switch from cornet to trumpet when many other musicians, including Armstrong, did so. In addition, Gioia highlights Beiderbecke's precise timing, relaxed delivery, and pure tone, which contrasted with "the dirty, rough-edged sound" of King Oliver and his protégé Armstrong, whose playing was often more energetic and whose style held more sway early in the 1920s than Beiderbecke's. Gioia further wonders whether the many hyperbolic and quasi-poetic descriptions of Beiderbecke's style—most notably Condon's "like a girl saying yes"—may indicate that Beiderbecke's sound was muddled on recordings.
Eddie Condon, Hoagy Carmichael, and Mezz Mezzrow, all of whom hyperbolically raved about his playing, also saw Beiderbecke play live or performed alongside him. Condon, for instance, wrote of being amazed by Beiderbecke's piano playing: "All my life I had been listening to music [...] But I had never heard anything remotely like what Beiderbecke played. For the first time I realized music isn't all the same, it had become an entirely new set of sounds" "I tried to explain Bix to the gang," Carmichael wrote, but "[i]t was no good, like the telling of a vivid, personal dream [...] the emotion couldn't be transmitted."
Mezzrow described Beiderbecke's tone as being "pickled in alcohol [...] I have never heard a tone like he got before or since. He played mostly open horn, every note full, big, rich and round, standing out like a pearl, loud but never irritating or jangling, with a powerful drive that few white musicians had in those days."
Some critics have highlighted "Jazz Me Blues", recorded with the Wolverines on February 18, 1924, as being particularly important to understanding Beiderbecke's style. Although it was one of his earliest recordings, the hallmarks of his playing were evident. "The overall impression we get from this solo, as in all of Bix at his best," writes the trumpeter Randy Sandke, "is that every note is spontaneous yet inevitable." Richard Hadlock describes Beiderbecke's contribution to "Jazz Me Blues" as "an ordered solo that seems more inspired by clarinetists Larry Shields of the ODJB and Leon Roppolo of the NORK than by other trumpet players." He goes on to suggest that clarinetists, by virtue of their not being tied to the melody as much as cornetists and trumpet players, could explore harmonies.
"Jazz Me Blues" was also important because it introduced what has been called the "correlated chorus", a method of improvising that Beiderbecke's Davenport friend Esten Spurrier attributed to both Beiderbecke and Armstrong. "Louis departed greatly from all cornet players in his ability to compose a close-knit individual 32 measures with all phrases compatible with each other", Spurrier told the biographers Sudhalter and Evans, "so Bix and I always credited Louis as being the father of the correlated chorus: play two measures, then two related, making four measures, on which you played another four measures related to the first four, and so on ad infinitum to the end of the chorus. So the secret was simple—a series of related phrases."
Beiderbecke plays piano on his recordings "Big Boy" (October 8, 1924), "For No Reason at All in C" (May 13, 1927), "Wringin' and Twistin'" (September 17, 1927)—all with ensembles—and his only solo recorded work, "In a Mist" (September 8, 1927). Critic Frank Murphy argues that many of the same characteristics that mark Beiderbecke on the cornet mark him on the keyboard: the uncharacteristic fingering, the emphasis on inventive harmonies, and the correlated choruses. Those inventive harmonies, on both cornet and piano, eventually helped point the way to bebop, which abandoned melody almost entirely.
Compositions.
Bix Beiderbecke wrote or co-wrote six instrumental compositions during his career:
"Candlelights", "Flashes", and "In the Dark" are piano compositions transcribed with the help of Bill Challis but never recorded by Beiderbecke. Two additional compositions were attributed to him by two other jazz composers: "Betcha I Getcha", attributed to Beiderbecke as a co-composer by Joe Venuti, the composer of the song, and "Cloudy", attributed to Beiderbecke by composer Charlie Davis as a composition from circa 1924.
Grammy Hall of Fame.
Bix Beiderbecke was posthumously inducted into the Grammy Hall of Fame, which is a special Grammy award established in 1973 to honor recordings that are at least 25 years old and that have "qualitative or historical significance."
See also.
In BBC Worldwide's 2014 series Intruders, Amy Whelan (played by Mira Sorvino) attempts to resurrect her lover from a past life, Bix, into a waiter (played by Trieu Tran), using the mouthpiece of his trumpet as a trigger.

</doc>
<doc id="41536" url="http://en.wikipedia.org/wiki?curid=41536" title="Duke Ellington">
Duke Ellington

Edward Kennedy "Duke" Ellington (April 29, 1899 – May 24, 1974) was an American composer, pianist and bandleader of jazz orchestras. He led his orchestra from 1923 until his death, his career spanning over 50 years.
Born in Washington, D.C., Ellington was based in New York City from the mid-1920s onward, and gained a national profile through his orchestra's appearances at the Cotton Club in Harlem. In the 1930s, his orchestra toured in Europe. Though widely considered to have been a pivotal figure in the history of jazz, Ellington embraced the phrase "beyond category" as a "liberating principle", and referred to his music as part of the more general category of "American Music", rather than to a musical genre such as "jazz".
Some of the musicians who were members of Ellington's orchestra, such as saxophonist Johnny Hodges, are considered to be among the best players in jazz. Ellington melded them into the best-known orchestral unit in the history of jazz. Some members stayed with the orchestra for several decades. A master at writing miniatures for the three-minute 78 rpm recording format, Ellington often composed specifically to feature the style and skills of his individual musicians, such as "Jeep's Blues" for Hodges, and "Concerto for Cootie" for trumpeter Cootie Williams, which later became "Do Nothing Till You Hear from Me" with Bob Russell's lyrics.
Often collaborating with others, Ellington wrote more than one thousand compositions; his extensive body of work is the largest recorded personal jazz legacy, with many of his extant works having become standards. Ellington also recorded songs written by his bandsmen, for example Juan Tizol's "Caravan", and "Perdido", which brought a Spanish tinge to big-band jazz.
After 1941, Ellington collaborated with composer-arranger-pianist Billy Strayhorn, whom he called his "writing and arranging companion". With Strayhorn, he composed many extended compositions, or "suites", as well as additional short pieces. Following an appearance at the Newport Jazz Festival, Rhode Island, in July 1956, Ellington and his orchestra enjoyed a major career revival and embarked on world tours. Ellington recorded for most American record companies of his era; performed in several films, scoring several; and composed stage musicals.
Due to his inventive use of the orchestra, or big band, and thanks to his eloquence and charisma, Ellington is generally considered to have elevated the perception of jazz to an art form on a par with other traditional musical genres. His reputation continued to rise after his death, and he was awarded a special Pulitzer Prize for music in 1999.
Gunther Schuller wrote in 1989Ellington composed incessantly to the very last days of his life. Music was indeed his mistress; it was his total life and his commitment to it was incomparable and unalterable. In jazz he was a giant among giants. And in twentieth century music, he may yet one day be recognized as one of the half-dozen greatest masters of our time.
Early life.
Edward Kennedy "Duke" Ellington was born on April 29, 1899, to James Edward Ellington and Daisy (Kennedy) Ellington in Washington, DC. Both his parents were pianists. Daisy primarily played parlor songs and J.E. preferred operatic arias. They lived with his maternal grandparents at 2129 Ida Place (now Ward Place), NW in the West End neighborhood of Washington, D.C. Duke's father was born in Lincolnton, North Carolina, on April 15, 1879, and moved to Washington, D.C. in 1886 with his parents. Daisy Kennedy was born in Washington, D.C. on January 4, 1879, the daughter of a former American slave . James Ellington made blueprints for the United States Navy. When Ellington was a child, his family showed racial pride and support in their home, as did many other families. African Americans in D.C. worked to protect their children from the era's Jim Crow laws.
At the age of seven, Ellington began taking piano lessons from Marietta Clinkscales. Daisy surrounded her son with dignified women to reinforce his manners and teach him to live elegantly. Ellington’s childhood friends noticed that "his casual, offhand manner, his easy grace, and his dapper dress gave him the bearing of a young nobleman", and began calling him "Duke." Ellington credited his "chum" Edgar McEntree for the nickname. "I think he felt that in order for me to be eligible for his constant companionship, I should have a title. So he called me Duke."
Though Ellington took piano lessons, he was more interested in baseball. "President Roosevelt (Teddy) would come by on his horse sometimes, and stop and watch us play", he recalled. Ellington went to Armstrong Technical High School in Washington, D.C. He got his first job selling peanuts at Washington Senators baseball games.
In the summer of 1914, while working as a soda jerk at the Poodle Dog Café, Ellington wrote his first composition, "Soda Fountain Rag" (also known as the "Poodle Dog Rag"). He created the piece by ear, as he had not yet learned to read and write music. "I would play the 'Soda Fountain Rag' as a one-step, two-step, waltz, tango, and fox trot", Ellington recalled. "Listeners never knew it was the same piece. I was established as having my own repertoire." In his autobiography, "Music is my Mistress" (1973), Ellington wrote that he missed more lessons than he attended, feeling at the time that playing the piano was not his talent. 
Ellington started sneaking into Frank Holiday's Poolroom at the age of fourteen. Hearing the poolroom pianists play ignited Ellington's love for the instrument, and he began to take his piano studies seriously. Among the many piano players he listened to were Doc Perry, Lester Dishman, Louis Brown, Turner Layton, Gertie Wells, Clarence Bowser, Sticky Mack, Blind Johnny, Cliff Jackson, Claude Hopkins, Phil Wurd, Caroline Thornton, Luckey Roberts, Eubie Blake, Joe Rochester, and Harvey Brooks.
Ellington began listening to, watching, and imitating ragtime pianists, not only in Washington, D.C., but in Philadelphia and Atlantic City, where he vacationed with his mother during the summer months. Dunbar High School music teacher Henry Lee Grant gave him private lessons in harmony. With the additional guidance of Washington pianist and band leader Oliver "Doc" Perry, Ellington learned to read sheet music, project a professional style, and improve his technique. Ellington was also inspired by his first encounters with stride pianists James P. Johnson and Luckey Roberts. Later in New York he took advice from Will Marion Cook, Fats Waller, and Sidney Bechet. Ellington started to play gigs in cafés and clubs in and around Washington, D.C. His attachment to music was so strong that in 1916 he turned down an art scholarship to the Pratt Institute in Brooklyn. Three months before graduating he dropped out of Armstrong Manual Training School, where he was studying commercial art.
Working as a freelance sign-painter from 1917, Ellington began assembling groups to play for dances. In 1919 he met drummer Sonny Greer from New Jersey, who encouraged Ellington's ambition to become a professional musician. Ellington built his music business through his day job: when a customer asked him to make a sign for a dance or party, he would ask if they had musical entertainment; if not, Ellington would offer to play for the occasion. He also had a messenger job with the U.S. Navy and State departments, where he made a wide range of contacts. Ellington moved out of his parents' home and bought his own as he became a successful pianist. At first, he played in other ensembles, and in late 1917 formed his first group, "The Duke's Serenaders" ("Colored Syncopators", his telephone directory advertising proclaimed). He was also the group's booking agent. His first play date was at the True Reformer's Hall, where he took home 75 cents.
Ellington played throughout the Washington, D.C. area and into Virginia for private society balls and embassy parties. The band included childhood friend Otto Hardwick, who started on string bass, then moved to C-melody sax and finally settled on alto saxophone; Arthur Whetsol on trumpet; Elmer Snowden on banjo; and Sonny Greer on drums. The band thrived, performing for both African-American and white audiences, a rarity in the segregated society of the time.
Music career.
Early career.
When his drummer Sonny Greer was invited to join the Wilber Sweatman Orchestra in New York City, Ellington made the fateful decision to leave behind his successful career in Washington, D.C., and move to Harlem, ultimately becoming part of the Harlem Renaissance. New dance crazes such as the Charleston emerged in Harlem, as well as African-American musical theater, including Eubie Blake's "Shuffle Along". After the young musicians left the Sweatman Orchestra to strike out on their own, they found an emerging jazz scene that was highly competitive and hard to crack. They hustled pool by day and played whatever gigs they could find. The young band met stride pianist Willie "The Lion" Smith, who introduced them to the scene and gave them some money. They played at rent-house parties for income. After a few months, the young musicians returned to Washington, D.C., feeling discouraged.
In June 1923, a gig in Atlantic City, New Jersey, led to a play date at the prestigious Exclusive Club in Harlem. This was followed in September 1923 by a move to the Hollywood Club – 49th and Broadway – and a four-year engagement, which gave Ellington a solid artistic base. He was known to play the bugle at the end of each performance. The group was initially called Elmer Snowden and his Black Sox Orchestra and had seven members, including trumpeter James "Bubber" Miley. They renamed themselves "The Washingtonians". Snowden left the group in early 1924 and Ellington took over as bandleader. After a fire, the club was re-opened as the Club Kentucky (often referred to as the "Kentucky Club").
Ellington made eight records in 1924, receiving composing credit on three including "Choo Choo". In 1925, Ellington contributed four songs to "Chocolate Kiddies" starring Lottie Gee and Adelaide Hall, an all-African-American revue which introduced European audiences to African-American styles and performers. Duke Ellington and his Kentucky Club Orchestra grew to a group of ten players; they developed their own sound by displaying the non-traditional expression of Ellington’s arrangements, the street rhythms of Harlem, and the exotic-sounding trombone growls and wah-wahs, high-squealing trumpets, and sultry saxophone blues licks of the band members. For a short time soprano saxophonist Sidney Bechet played with them, imparting his propulsive swing and superior musicianship to the young band members.
Cotton Club engagement.
In October 1926, Ellington made a career-advancing agreement with agent-publisher Irving Mills, giving Mills a 45% interest in Ellington's future. Mills had an eye for new talent and early on published compositions by Hoagy Carmichael, Dorothy Fields, and Harold Arlen. After recording a handful of acoustic titles during 1924-1926, Ellington's signing with Mills allowed him to record prolifically, although sometimes he recorded different versions of the same tune. Mills often took a co-composer credit. From the beginning of their relationship, Mills arranged recording sessions on nearly every label including Brunswick, Victor, Columbia, OKeh, Pathê (and its Perfect label), the ARC/Plaza group of labels (Oriole, Domino, Jewel, Banner) and their dime-store labels (Cameo, Lincoln, Romeo), Hit of the Week, and Columbia's cheaper labels (Harmony, Diva, Velvet Tone, Clarion) labels which gave Ellington popular recognition. On OKeh, his records were usually issued as "The Harlem Footwarmers", while the Brunswick's were usually issued as "The Jungle Band". "Whoopee Makers" and the "Ten Black Berries" were other pseudonyms.
In September 1927, King Oliver turned down a regular booking for his group as the house band at Harlem's Cotton Club; the offer passed to Ellington after Jimmy McHugh suggested him and Mills arranged an audition. Ellington had to increase from a six to eleven-piece group to meet the requirements of the Cotton Club's management for the audition, and the engagement finally began on December 4. With a weekly radio broadcast, the Cotton Club's exclusively white and wealthy clientele poured in nightly to see them. At the Cotton Club, Ellington's group performed all the music for the revues, which mixed comedy, dance numbers, vaudeville, burlesque, music, and illegal alcohol. The musical numbers were composed by Jimmy McHugh and the lyrics by Dorothy Fields (later Harold Arlen and Ted Koehler), with some Ellington originals mixed in. (Here he moved in with a dancer, his second wife Mildred Dixon.) Weekly radio broadcasts from the club gave Ellington national exposure, while Ellington also recorded Fields-JMcHugh and Fats Waller-Andy Razaf songs.
Although trumpeter Bubber Miley was a member of the orchestra for only a short period, he had a major influence on Ellington's sound. As an early exponent of growl trumpet, Miley changed the "sweet" dance band sound of the group to one that was hotter, which contemporaries termed "jungle" style. In October 1927, Ellington and his Orchestra recorded several compositions with Adelaide Hall. One side in particular, "Creole Love Call", became a worldwide sensation and gave both Ellington and Hall their first hit record. Miley had composed most of "Creole Love Call" and "Black and Tan Fantasy". An alcoholic, Miley had to leave the band before they gained wider fame. He died in 1932 at the age of 29, but he was an important influence on Cootie Williams, who replaced him.
In 1929, the Cotton Club Orchestra appeared on stage for several months in Florenz Ziegfeld's "Show Girl", along with vaudeville stars Jimmy Durante, Eddie Foy, Jr., Ruby Keeler, and with music and lyrics by George Gershwin and Gus Kahn. Will Vodery, Ziegfeld’s musical supervisor, recommended Ellington for the show, and, according to John Hasse's "Beyond Category: The Life and Genius of Duke Ellington," "Perhaps during the run of Show Girl, Ellington received what he later termed ' valuable lessons in orchestration from Will Vodery.' In his 1946 biography, "Duke Ellington", Barry Ulanov wrote:
From Vodery, as he (Ellington) says himself, he drew his chromatic convictions, his uses of the tones ordinarily extraneous to the diatonic scale, with the consequent alteration of the harmonic character of his music, its broadening, The deepening of his resources. It has become customary to ascribe the classical influences upon Duke – Delius, Debussy and Ravel – to direct contact with their music. Actually his serious appreciation of those and other modern composers, came after his meeting with Vodery.
Ellington's film work began with "Black and Tan" (1929), a nineteen-minute all-African-American RKO short in which he played the hero "Duke". He also appeared in the Amos 'n' Andy film "Check and Double Check" released in 1930. That year, Ellington and his Orchestra connected with a whole different audience in a concert with Maurice Chevalier and they also performed at the Roseland Ballroom, "America's foremost ballroom". Australian-born composer Percy Grainger was an early admirer and supporter. He wrote "The three greatest composers who ever lived are Bach, Delius and Duke Ellington. Unfortunately Bach is dead, Delius is very ill but we are happy to have with us today The Duke". Ellington's first period at the Cotton Club concluded in 1931.
The early 1930s.
Ellington led the orchestra by conducting from the keyboard using piano cues and visual gestures; very rarely did he conduct using a baton. As a bandleader, Ellington was not a strict disciplinarian; he maintained control of his orchestra with a combination of charm, humor, flattery, and astute psychology. A complex, private person, he revealed his feelings to only his closest intimates and effectively used his public persona to deflect attention away from himself .
Ellington signed exclusively to Brunswick in 1932 and stayed with them through late 1936 (albeit with a short-lived 1933-34 switch to Victor when Irving Mills temporarily moved him and his other acts from Brunswick).
As the Depression worsened, the recording industry was in crisis, dropping over 90% of its artists by 1933. Ivie Anderson was hired as their featured vocalist in 1931. She is the vocalist on "It Don't Mean a Thing (If It Ain't Got That Swing)" (1932) among other recordings. Sonny Greer had been providing occasional vocals and continued to do in a cross-talk feature with Anderson. Radio exposure helped maintain popularity as Ellington and his orchestra began to tour. The other records of this era include: "Mood Indigo" (1930), "Sophisticated Lady" (1933), "Solitude" (1934), and "In a Sentimental Mood" (1935)
While the band's United States audience remained mainly African-American in this period, the Ellington orchestra had a huge following overseas, exemplified by the success of their trip to England in 1933 and their 1934 visit to the European mainland. The English visit saw Ellington win praise from members of the "serious" music community, including composer Constant Lambert, which gave a boost to Ellington's interest in composing longer works.
Those longer pieces had already begun to appear. He had composed and recorded Creole Rhapsody as early as 1931 (issued as both sides of a 12" record for Victor and both sides of a 10" record for Brunswick), and a tribute to his mother, "Reminiscing in Tempo", took four 10" record sides to record in 1935 after her death in that year. "Symphony in Black" (also 1935), a short film, featured his extended piece 'A Rhapsody of Negro Life'. It introduced Billie Holiday, and won an Academy Award as the best musical short subject. Ellington and his Orchestra also appeared in the features "Murder at the Vanities" and "Belle of the Nineties" (both 1934),
For agent Mills the attention was a publicity triumph, as Ellington was now internationally known. On the band's tour through the segregated South in 1934, they avoided some of the traveling difficulties of African-Americans by touring in private railcars. These provided easy accommodations, dining, and storage for equipment while avoiding the indignities of segregated facilities.
Competition was intensifying though, as swing bands like Benny Goodman's, began to receive popular attention. Swing dancing became a youth phenomenon, particularly with white college audiences, and "danceability" drove record sales and bookings. Jukeboxes proliferated nationwide, spreading the gospel of "swing." Ellington's band could certainly swing, but their strengths were mood, nuance, and richness of composition, hence his statement "jazz is music, swing is business".
The later 1930s.
From 1936, Ellington began to make recordings of smaller groups (sextets, octets, and nonets) drawn from his then-15-man orchestra and he composed pieces intended to feature a specific instrumentalist, as with "Jeep's Blues" for Johnny Hodges, "Yearning for Love" for Lawrence Brown, "Trumpet in Spades" for Rex Stewart, "Echoes of Harlem" for Cootie Williams and "Clarinet Lament" for Barney Bigard. In 1937, Ellington returned to the Cotton Club which had relocated to the mid-town Theater District. In the summer of that year, his father died, and due to many expenses, Ellington's finances were tight, although his situation improved the following year.
After leaving agent Irving Mills, he signed on with the William Morris Agency. Mills though continued to record Ellington. After only a year, his Master and Variety labels, the small groups had recorded for the latter, collapsed in late 1937, Mills placed Ellington back on Brunswick and those small group units on Vocalion through to 1940. Well known sides continued to be recorded, "Caravan" in 1937, and "I Let a Song Go Out of My Heart" the following year.
Billy Strayhorn, originally hired as a lyricist, began his association with Ellington in 1939. Nicknamed "Swee' Pea" for his mild manner, Strayhorn soon became a vital member of the Ellington organization. Ellington showed great fondness for Strayhorn and never failed to speak glowingly of the man and their collaborative working relationship, "my right arm, my left arm, all the eyes in the back of my head, my brain waves in his head, and his in mine". Strayhorn, with his training in classical music, not only contributed his original lyrics and music, but also arranged and polished many of Ellington's works, becoming a second Ellington or "Duke's doppelganger". It was not uncommon for Strayhorn to fill in for Duke, whether in conducting or rehearsing the band, playing the piano, on stage, and in the recording studio. The 1930s ended with a very successful European tour just as World War II loomed in Europe.
Ellington in the early to mid-1940s.
Some of the musicians who joined Ellington at this time created a sensation in their own right. The short-lived Jimmy Blanton transformed the use of double bass in jazz, allowing it to function as a solo rather than a rhythm instrument alone. Terminal illness forced him to leave by late 1941 after only about two years. Ben Webster, the Orchestra's first regular tenor saxophonist, whose main tenure with Ellington spanned 1939 to 1943, started a rivalry with Johnny Hodges as the Orchestra's foremost voice in the sax section.
Trumpeter Ray Nance joined, replacing Cootie Williams who had "defected", contemporary wags claimed, to Benny Goodman. Additionally, Nance added violin to the instrumental colors Ellington had at his disposal. Recordings exist of Nance's first concert date on November 7, 1940, at Fargo, North Dakota. Privately made by Jack Towers and Dick Burris, these recordings were first legitimately issued in 1978 as "Duke Ellington at Fargo, 1940 Live"; they are among the earliest of innumerable live performances which survive. Nance was also an occasional vocalist, although Herb Jeffries was the main male vocalist in this era (until 1943) while Al Hibbler (who replaced Jeffries in 1943) continued until 1951. Ivie Anderson left in 1942 after eleven years: the longest term of any of Ellington's vocalists.
Once again recording for Victor (from 1940), with the small groups recording for their Bluebird label, three-minute masterpieces on 78 rpm record sides continued to flow from Ellington, Billy Strayhorn, Ellington's son Mercer Ellington, and members of the Orchestra. "Cotton Tail", "Main Stem", "Harlem Airshaft", "Jack the Bear", and dozens of others date from this period. Strayhorn's "Take the "A" Train" a hit in 1941, became the band's theme, replacing "East St. Louis Toodle-Oo". Ellington and his associates wrote for an orchestra of distinctive voices who displayed tremendous creativity. Mary Lou Williams, working as a staff arranger, would briefly join Ellington a few years later.
Ellington's long-term aim though was to extend the jazz form from that three-minute limit, of which he was an acknowledged master. While he had composed and recorded some extended pieces before, such works now became a regular feature of Ellington's output. In this, he was helped by Strayhorn, who had enjoyed a more thorough training in the forms associated with classical music than Ellington. The first of these, "Black, Brown, and Beige" (1943), was dedicated to telling the story of African-Americans, and the place of slavery and the church in their history. Ellington debuted "Black, Brown and Beige" in Carnegie Hall on January 23, 1943, beginning an annual series of concerts there over the next four years. While some jazz musicians had played at Carnegie Hall before, none had performed anything as elaborate as Ellington’s work. Unfortunately, starting a regular pattern, Ellington's longer works were generally not well received.
A partial exception was "Jump for Joy", a full-length musical based on themes of African-American identity, debuted on July 10, 1941 at the Mayan Theater in Los Angeles. Hollywood luminaries like actors John Garfield and Mickey Rooney invested in the production, and Charlie Chaplin and Orson Welles offered to direct. At one performance though, Garfield insisted Herb Jeffries, who was light skinned, should wear make-up. Ellington objected in the interval, and compared Jeffries to Al Jolson. The change was reverted, and the singer later commented that the audience must have thought he was an entirely different character in the second half of the show.
Although it had sold-out performances, and received positive reviews, it ran for only 122 performances until September 29, 1941, with a brief revival in November of that year. Its subject matter did not make it appealing to Broadway; Ellington had unfulfilled plans to take it there. Despite this disappointment, a Broadway production of Ellington's "Beggar's Holiday", his sole book musical, premiered on December 23, 1946 under the direction of Nicholas Ray.
The settlement of the first recording ban of 1942–43, leading to an increase in royalties paid to musicians, had a serious effect on the financial viability of the big bands, including Ellington's Orchestra. His income as a songwriter ultimately subsidized it. Although he always spent lavishly and drew a respectable income from the Orchestra's operations, the band's income often just covered expenses.
Early post-war years.
World War II brought about a swift end to the big band era as musicians went off to serve in the military and travel restrictions made touring difficult. When the war ended, the focus of popular music shifted towards crooners such as Frank Sinatra and Jo Stafford, so Ellington's wordless vocal feature "Transblucency" (1946) with Kay Davis was not going to have a similar reach. With inflation setting in after 1945, the cost of hiring big bands went up and club owners preferred smaller jazz groups who played in new styles such as bebop.
Ellington continued on his own course through these tectonic shifts. While Count Basie was forced to disband his whole ensemble and work as an octet for a time, Ellington was able to tour most of Western Europe between 6 April and 30 June 1950, with the orchestra playing 74 dates over 77 days. During the tour, according to Sonny Greer, the newer works were not performed, though Ellington's extended composition, "Harlem" (1950) was in the process of being completed at this time. Ellington later presented its score to music-loving President Harry Truman. Also during his time in Europe, Ellington would compose the music for a stage production by Orson Welles. Titled "Time Runs" in Paris and "An Evening With Orson Welles" in Frankfurt, the variety show also featured a newly discovered Eartha Kitt, who performed Ellington's original song "Hungry Little Trouble" as Helen of Troy.
In 1951, Ellington suffered a significant loss of personnel: Sonny Greer, Lawrence Brown, and most importantly Johnny Hodges left to pursue other ventures, although only Greer was a permanent departee. Drummer Louie Bellson replaced Greer, and his "Skin Deep" was a hit for Ellington. Tenor player Paul Gonsalves had joined in December 1950 after periods with Count Basie and Dizzy Gillespie and stayed for the rest of his life, while Clark Terry joined in November 1951.
During the early 50s, Ellington's career was at a low point with his style being generally seen as outmoded, but his reputation did not suffer as badly as some artists. André Previn said in 1952: "You know, Stan Kenton can stand in front of a thousand fiddles and a thousand brass and make a dramatic gesture and every studio arranger can nod his head and say, ‘‘Oh, yes, that’s done like this.’’ But Duke merely lifts his finger, three horns make a sound, and I don’t know what it is!" However by 1955, after three years of recording for Capitol, Ellington lacked a regular recording affiliation.
Career revival.
Ellington's appearance at the Newport Jazz Festival on July 7, 1956 returned him to wider prominence and introduced him to a new generation of fans. The feature "Diminuendo and Crescendo in Blue" comprised two tunes that had been in the band's book since 1937 but largely forgotten until Ellington, who had abruptly ended the band's scheduled set because of the late arrival of four key players, called the two tunes as the time was approaching midnight. Announcing that the two pieces would be separated by an "interlude" played by tenor saxophonist Paul Gonsalves, Ellington proceeded to lead the band through the two pieces, with Gonsalves' 27-chorus marathon solo whipping the crowd into a frenzy, leading the Maestro to play way beyond the curfew time despite urgent pleas from Festive organizer George Wein to bring the program to an end.
The concert made international headlines, led to one of only five"Time" magazine cover stories dedicated to a jazz musician (Louis Armstrong, Thelonious Monk, Dave Brubeck, and Wynton Marsalis are the others) and resulted in an album produced by George Avakian that would become the best-selling long-playing recording of Ellington's career.
Ironically though, much of the music on the vinyl LP was, in effect, "simulated", with only about 40% actually from the concert itself. According to Avakian, Ellington was dissatisfied with aspects of the performance and felt the musicians had been under rehearsed. The band assembled the next day to re-record several of the numbers with the addition of artificial crowd noise, none of which was disclosed to purchasers of the album. Not until 1999 was the concert recording properly released for the first time. The revived attention brought about by the Newport appearance should not have surprised anyone, Johnny Hodges had returned the previous year, and Ellington's collaboration with Strayhorn had been renewed around the same time, under terms more amenable to the younger man.
The original "Ellington at Newport" album was the first release in a new recording contract with Columbia Records which yielded several years of recording stability, mainly under producer Irving Townsend, who coaxed both commercial and artistic productions from Ellington.
In 1957, CBS (Columbia Records' parent corporation) aired a live television production of "A Drum Is a Woman", an allegorical suite which received mixed reviews. His hope that television would provide a significant new outlet for his type of jazz was not fulfilled. Tastes and trends had moved on without him. Festival appearances at the new Monterey Jazz Festival and elsewhere provided venues for live exposure, and a European tour in 1958 was well received. "Such Sweet Thunder" (1957), based on Shakespeare's plays and characters, and "The Queen's Suite" (1958), dedicated to Britain's Queen Elizabeth II, were products of the renewed impetus which the Newport appearance helped to create, although the latter work was not commercially issued at the time. The late 1950s also saw Ella Fitzgerald record her "Duke Ellington Songbook" (Verve) with Ellington and his orchestra—a recognition that Ellington's songs had now become part of the cultural canon known as the 'Great American Songbook'.
Ellington at this time (with Strayhorn) began to work directly on scoring for film soundtracks, in particular "Anatomy of a Murder" (1959), with James Stewart, in which Ellington appeared fronting a roadhouse combo, and "Paris Blues" (1961), which featured Paul Newman and Sidney Poitier as jazz musicians. "Detroit Free Press" music critic Mark Stryker concludes that the work of Billy Strayhorn and Ellington in "Anatomy of a Murder", a trial court drama film directed by Otto Preminger, is "indispensable, [although] . . . too sketchy to rank in the top echelon among Ellington-Strayhorn masterpiece suites like "Such Sweet Thunder" and "The Far East Suite", but its most inspired moments are their equal."
Film historians have recognized the soundtrack "as a landmark – the first significant Hollywood film music by African Americans comprising non-diegetic music, that is, music whose source is not visible or implied by action in the film, like an on-screen band." The score avoided the cultural stereotypes which previously characterized jazz scores and rejected a strict adherence to visuals in ways that presaged the New Wave cinema of the ’60s". Ellington and Strayhorn, always looking for new musical territory, produced suites for John Steinbeck's novel "Sweet Thursday", Tchaikovsky's "Nutcracker Suite" and Edvard Grieg's "Peer Gynt".
In the early 1960s, Ellington embraced recording with artists who had been friendly rivals in the past, or were younger musicians who focused on later styles. The Ellington and Count Basie orchestras recorded together. During a period when he was between recording contracts, he made records with Louis Armstrong (Roulette), Coleman Hawkins, John Coltrane (both for Impulse) and participated in a session with Charles Mingus and Max Roach which produced the "Money Jungle" (United Artists) album. He signed to Frank Sinatra's new Reprise label, but the association with the label was short-lived.
Musicians who had previously worked with Ellington returned to the Orchestra as members: Lawrence Brown in 1960 and Cootie Williams in 1962."The writing and playing of music is a matter of intent... You can't just throw a paint brush against the wall and call whatever happens art. My music fits the tonal personality of the player. I think too strongly in terms of altering my music to fit the performer to be impressed by accidental music. You can't take doodling seriously." He was now performing all over the world; a significant part of each year was spent on overseas tours. As a consequence, he formed new working relationships with artists from around the world, including the Swedish vocalist Alice Babs, and the South African musicians Dollar Brand and Sathima Bea Benjamin ("A Morning in Paris", 1963/1997).
Ellington wrote an original score for director Michael Langham's production of Shakespeare's "Timon of Athens" at the Stratford Festival in Ontario, Canada which opened on July 29, 1963. Langham has used it for several subsequent productions, including a much later adaptation by Stanley Silverman which expands the score with some of Ellington's best-known works.
Last years.
Ellington was a Pulitzer Prize for Music nominee in 1965 but another nominee was selected. Then 66 years old, he said: "Fate is being kind to me. Fate doesn't want me to be famous too young." In 1999 he was posthumously awarded a special Pulitzer Prize (not the Music prize), "commemorating the centennial year of his birth, in recognition of his musical genius, which evoked aesthetically the principles of democracy through the medium of jazz and thus made an indelible contribution to art and culture."
In September 1965, he premiered the first of his Sacred Concerts. He created a jazz Christian liturgy. Although the work received mixed reviews, Ellington was proud of the composition and performed it dozens of times. This concert was followed by two others of the same type in 1968 and 1973, known as the Second and Third Sacred Concerts. These generated controversy in what was already a tumultuous time in the United States. Many saw the Sacred Music suites as an attempt to reinforce commercial support for organized religion, though Ellington simply said it was "the most important thing I've done". The Steinway piano upon which the Sacred Concerts were composed is part of the collection of the Smithsonian's National Museum of American History. Like Haydn and Mozart, Ellington conducted his orchestra from the piano – he always played the keyboard parts when the Sacred Concerts were performed.
Despite his advancing age (he turned 65 in the spring of 1964), Ellington showed no sign of slowing down as he continued to make vital and innovative recordings, including "The Far East Suite" (1966), "New Orleans Suite" (1970), "Latin American Suite" (1972) and "The Afro-Eurasian Eclipse" (1971), much of it inspired by his world tours. It was during this time that he recorded his only album with Frank Sinatra, entitled "Francis A. & Edward K." (1967).
Although he made two more stage appearances before his death, Ellington performed what is considered his final "full" concert in a ballroom at Northern Illinois University on March 20, 1974.
The last three shows Ellington and his orchestra performed were one on March 21, 1973 at Purdue University's Hall of Music and two on March 22, 1973 at the Sturges-Young Auditorium in Sturgis, Michigan.
Personal life.
Ellington married his high school sweetheart, Edna Thompson (d. 1967), on July 2, 1918, when he was 19. The next spring, on March 11, 1919, Edna gave birth to their only son, Mercer Kennedy Ellington.
Ellington was joined in New York City by his wife and son in the late twenties, but the couple soon permanently separated. According to her obituary in "Jet" magazine, she was "homesick for Washington" and returned (she died in 1967). In 1928, Ellington became the companion of Mildred Dixon, who traveled with him, managed Tempo Music, inspired songs at the peak of his career, and reared his son Mercer. 
In 1938 he left his family (his son was then 19) and moved in with Beatrice "Evie" Ellis, a Cotton Club employee. Their relationship, though stormy, continued after Ellington met and formed a relationship with Fernanda de Castro Monte in the early 1960s. Ellington supported both women for the rest of his life.
Ellington's sister Ruth (1915–2004) later ran Tempo Music, his music publishing company. Ruth's second husband was the bass-baritone McHenry Boatwright, whom she met when he sang at her brother's funeral. 
As an adult, son Mercer Ellington (d. 1996) played trumpet and piano, and led his own band. He also worked as his father's business manager, eventually taking full control of the band after Duke's death. He was an important archivist of his father's musical life.
Ellington died on May 24, 1974 of complications from lung cancer and pneumonia, a few weeks after his 75th birthday. His last words were, "Music is how I live, why I live and how I will be remembered." At his funeral, attended by over 12,000 people at the Cathedral of St. John the Divine, Ella Fitzgerald summed up the occasion, "It's a very sad day. A genius has passed." He was interred in the Woodlawn Cemetery, The Bronx, New York City.
Following Duke's death, his son Mercer took over leadership of the orchestra, continuing until his own death in 1996. Like the Count Basie Orchestra, this group continued to release albums long after Duke Ellington's death. "Digital Duke", credited to The Duke Ellington Orchestra, won the 1988 Grammy Award for Best Large Jazz Ensemble Album. Mercer Ellington had been handling all administrative aspects of his father's business for several decades. Mercer's children continue a connection with their grandfather's work.
Legacy.
Memorials.
Numerous memorials have been dedicated to Duke Ellington, in cities from New York and Washington, D.C. to Los Angeles.
In Ellington's birthplace, Washington, D.C., the Duke Ellington School of the Arts educates talented students, who are considering careers in the arts, by providing intensive arts instruction and strong academic programs that prepare students for post-secondary education and professional careers. Originally built in 1935, the Calvert Street Bridge was renamed the Duke Ellington Bridge in 1974.
In 1989, a bronze plaque was attached to the newly named Duke Ellington Building at 2121 Ward Place, NW. In 2012, the new owner of the building commissioned a mural by Aniekan Udofia that appears above the lettering "Duke Ellington".
In 2010 the triangular park, across the street from Duke Ellington's birth site, at the intersection of New Hampshire and M Streets, NW was named the Duke Ellington Park. 
Ellington's residence at 2728 Sherman Avenue, NW, during the years 1919-1922, is marked by a bronze plaque.
On February 24, 2009, the United States Mint launched a new coin featuring Duke Ellington, making him the first African American to appear by himself on a circulating U.S. coin. Ellington appears on the reverse ("tails") side of the District of Columbia quarter. The coin is part of the U.S. Mint's program honoring the District and the U.S. territories and celebrates Ellington's birthplace in the District of Columbia. Ellington is depicted on the quarter seated at a piano, sheet music in hand, along with the inscription "Justice for All", which is the District's motto.
Ellington lived for years in a townhouse on the corner of Manhattan's Riverside Drive and West 106th Street. After his death, West 106th Street was officially renamed Duke Ellington Boulevard. A large memorial to Ellington, created by sculptor Robert Graham, was dedicated in 1997 in New York's Central Park, near Fifth Avenue and 110th Street, an intersection named Duke Ellington Circle.
A statue of Ellington at a piano is featured at the entrance to UCLA's Schoenberg Hall. According to "UCLA Magazine":
When UCLA students were entranced by Duke Ellington's provocative tunes at a Culver City club in 1937, they asked the budding musical great to play a free concert in Royce Hall. 'I've been waiting for someone to ask us!' Ellington exclaimed.
On the day of the concert, Ellington accidentally mixed up the venues and drove to USC instead. He eventually arrived at the UCLA campus and, to apologize for his tardiness, played to the packed crowd for more than four hours. And so, "Sir Duke" and his group played the first-ever jazz performance in a concert venue.
The Essentially Ellington High School Jazz Band Competition and Festival is a nationally renowned annual competition for prestigious high school bands. Started in 1996 at Jazz at Lincoln Center, the festival is named after Ellington because of the large focus that the festival places on his works.
Tributes.
Martin Williams said: "Duke Ellington lived long enough to hear himself named among our best composers. And since his death in 1974, it has become not at all uncommon to see him named, along with Charles Ives, as the greatest composer we have produced, regardless of category."
In the opinion of Bob Blumenthal of "The Boston Globe" in 1999: "[i]n the century since his birth, there has been no greater composer, American or otherwise, than Edward Kennedy Ellington."
In 2002, scholar Molefi Kete Asante listed Duke Ellington on his list of 100 Greatest African Americans.
While his compositions are now the staple of the repertoire of music conservatories, they have been revisited by artists and musicians around the world both as a source of inspiration and a bedrock of their own performing careers.
There are hundreds of albums dedicated to the music of Duke Ellington and Billy Strayhorn by artists famous and obscure. The more notable artists include Sonny Stitt, Thelonious Monk, Dizzy Gillespie, Billy Eckstine, Tony Bennett, Oscar Peterson, Toshiko Akiyoshi, Joe Pass, Milt Jackson, Earl Hines, Rahsaan Roland Kirk, McCoy Tyner, André Previn, World Saxophone Quartet, Ben Webster, Zoot Sims, Kenny Burrell, Lambert, Hendricks and Ross, Martial Solal, Clark Terry and Randy Weston.
"Sophisticated Ladies", an award-winning 1981 musical revue, incorporated many tunes from Ellington's repertoire. A second Broadway musical interpolating Ellington's music, "Play On!", debuted in 1997.
Awards.
Grammy Awards.
Ellington earned 12 Grammy awards from 1959 to 2000, three of which were posthumous.
Grammy Hall of Fame.
Recordings of Duke Ellington were inducted into the Grammy Hall of Fame, which is a special Grammy award established in 1973 to honor recordings that are at least twenty-five years old, and that have "qualitative or historical significance".

</doc>
<doc id="41538" url="http://en.wikipedia.org/wiki?curid=41538" title="Rahsaan Roland Kirk">
Rahsaan Roland Kirk

Rahsaan Roland Kirk (August 7, 1935 – December 5, 1977) was an American jazz multi-instrumentalist who played tenor saxophone, flute and many other instruments. He was renowned for his onstage vitality, during which virtuoso improvisation was accompanied by comic banter, political ranting, and the ability to play several instruments simultaneously.
Biography.
Kirk was born Ronald Theodore Kirk in Columbus, Ohio, where he lived in a neighborhood known as Flytown. He felt compelled by a dream to transpose two letters in his first name to make "Roland." He became blind at an early age as a result of poor medical treatment. In 1970, Kirk added "Rahsaan" to his name after hearing it in a dream.
Preferring to lead his own bands, Kirk rarely performed as a sideman, although he did record with arranger Quincy Jones and drummer Roy Haynes and had notable stints with bassist Charles Mingus. One of his best-known recorded performances is the lead flute and solo on Jones' "Soul Bossa Nova", a 1964 hit song repopularized in the "Austin Powers" films (Jones 1964; McLeod et al. 1997).
His playing was generally rooted in soul jazz or hard bop, but Kirk's knowledge of jazz history allowed him to draw from many elements of the music's past, from ragtime to swing and free jazz. Kirk also absorbed classical influences, and his artistry reflected elements of pop music by composers such as Smokey Robinson and Burt Bacharach, as well as Duke Ellington, John Coltrane and other jazz musicians. The live album "Bright Moments" (1973) is an example of one of his shows. His main instrument was the tenor saxophone, supplemented by other saxes, and contrasted with the lighter sound of the flute. At times he would play a number of these horns at once, harmonizing with himself, or sustain a note for lengthy durations by using circular breathing, or play the rare, seldom heard nose flute. A number of his instruments were exotic or homemade, but even while playing two or three saxophones at once, the music was intricate, powerful jazz with a strong feel for the blues.
Kirk was politically outspoken. During his concerts, between songs he often talked about topical issues, including black history and the civil rights movement. His monologues were often laced with satire and absurdist humor. According to comedian Jay Leno, when Leno toured with Kirk as Kirk's opening act, Kirk would introduce him by saying, "I want to introduce a young brother who knows the black experience and knows all about the white devils ... Please welcome Jay Leno!"
In 1975, Kirk suffered a major stroke which led to partial paralysis of one side of his body. However, he continued to perform and record, modifying his instruments to enable him to play with one arm. At a live performance at Ronnie Scott's Jazz Club in London he even managed to play two instruments, and carried on to tour internationally and even appear on television.
He died from a second stroke in 1977 after performing in the Frangipani Room of the Indiana University Student Union in Bloomington, Indiana.
Instruments and technique.
Kirk played and collected a number of musical instruments, mainly various saxophones, clarinets and flutes. His main instruments were tenor saxophone and two obscure saxophones: the stritch (a straight alto sax lacking the instrument's characteristic upturned bell) and a manzello (a modified saxello soprano sax, with a larger, upturned bell). Kirk modified these instruments himself to accommodate his simultaneous playing technique.
He typically appeared on stage with all three horns hanging around his neck, as well as a variety of other instruments, including flutes and whistles, and often kept a gong within reach. Kirk also played clarinet, harmonica, English horn, and recorders, and was a competent trumpeter. He often had unique approaches, using a saxophone mouthpiece on a trumpet or playing nose flute. He additionally used many non-musical devices, such as alarm clocks, sirens, or a section of common garden hose (dubbed "the black mystery pipes"). His studio recordings also used tape-manipulated "musique concrète" and primitive electronic sounds (before such things became commonplace).
 · 
Rahsaan simultaneously playing flute and singing, punctuated with a siren whistle. ()
Rahsaan playing black mystery pipes. ()
Rahsaan simultaneously playing multiple saxophones. ()
Kirk was also an influential flautist, employing several techniques that he developed himself. One technique was to sing or hum into the flute at the same time as playing. Another was to play the standard transverse flute at the same time as a nose flute.
Some have suggested that Kirk's unique onstage appearance and simultaneous multi-instrumentalism were gimmicks to the point of linking these to his blindness , but Kirk's playing and performance tended to silence these ideas. He used the multiple horns to play true chords, essentially functioning as a one-man saxophone section. Kirk insisted that he was only trying to emulate the sounds he heard in his head.
Kirk was a major exponent of circular breathing. Using this technique, he was not only able to sustain a single note for an extended period; he could also play sixteenth-note runs of almost unlimited length, and at high speeds. His circular breathing ability enabled him to record "Concerto For Saxophone" on the "Prepare Thyself to Deal With a Miracle" LP in one continuous take of about 20 minutes' playing with no discernible "break" for inhaling. His long-time producer at Atlantic Jazz, Joel Dorn, believed he should have received credit in "The Guinness Book of World Records" for such feats (he was capable of playing continuously "without taking a breath" for far longer than exhibited on that LP), but this never happened.
"The Case of the 3 Sided Dream in Audio Color" was a unique album in jazz and popular music recorded annals. It was a two-LP set, with Side 4 apparently "blank", the label not indicating any content. However, once word of "the secret message" got around among Rahsaan's fans, one would find that about 12 minutes into Side 4 appeared the first of two telephone answering machine messages recorded by Kirk, the second following soon thereafter (but separated by more blank grooves). The surprise impact of these segments appearing on "blank" Side 4 was lost on the CD reissue of this album.
He gleaned information on what was happening in the world via audio media like radio and the sounds coming from TV sets. His later recordings often incorporated his spoken commentaries on current events, including Richard Nixon's involvement in the Watergate scandal. The "3-Sided Dream" album was a "concept album" which incorporated of "found" or environmental sounds and tape loops, tapes being played backwards, etc. Snippets of Billie Holiday singing are also heard briefly. The album even confronts the rise of influence of computers in society, as Rahsaan threatens to pull the plug on the machine trying to tell him what to do.
In the album "Other Folks' Music" the spoken words of Paul Robeson, another outspoken black artist, can be briefly heard.
Discography.
As sideman.
With Charles Mingus
With Roy Haynes
With Tubby Hayes
With Quincy Jones
With Tommy Peltier
With Jaki Byard
With Les McCann

</doc>
<doc id="41544" url="http://en.wikipedia.org/wiki?curid=41544" title="John McLoughlin">
John McLoughlin

Dr. John McLoughlin, baptized Jean-Baptiste McLoughlin, (October 19, 1784 – September 3, 1857) was a Chief Factor and Superintendent of the Columbia District of the Hudson's Bay Company at Fort Vancouver from 1824 to 1845. He was later known as the "Father of Oregon" for his role in assisting the American cause in the Oregon Country in the Pacific Northwest. In the late 1840s his general store in Oregon City was famous as the last stop on the Oregon Trail.
Childhood and early career.
McLoughlin was born in Rivière-du-Loup, Quebec, of Irish (his grandfather came from Sharagower in the Inishowen peninsular of County Donegal), Scottish, and French Canadian descent. He lived with his great uncle, Colonel William Fraser, for a while as a child. Though baptized Roman Catholic, he was raised Anglican and in his later life he returned to the Roman Catholic faith. In 1798, he began to study medicine with Sir James Fisher of Quebec. After studying for 4½ years he was granted a license to practice medicine on April 30, 1803. He was hired as a physician at Fort William, Ontario (now Thunder Bay), a fur-gathering post of the North West Company on Lake Superior; there he became a trader and mastered several Indian languages.
In 1814 he became a partner in the company. In 1816 McLoughlin was arrested for the murder of Robert Semple, the governor of the Red River Colony, after the Battle of Seven Oaks, though it is often claimed he stood in proxy for some Indians who were blamed. He was tried on October 30, 1818, and the charges were dismissed. McLoughlin was instrumental in the negotiations leading to the North West Company's 1821 merger with the Hudson's Bay Company. He was promoted to head the Lac la Pluie district temporarily shortly after the merger.
The Columbia District.
In 1824 the Hudson's Bay Company appointed McLoughlin, already a Chief Factor, Superintendent of the Columbia District (roughly parallel to what Americans know as the Oregon Country), with Peter Skene Ogden appointed to assist him. At the time, the region was under joint occupation of both the United States and Britain pursuant to the Treaty of 1818. Upon his arrival, he determined that the headquarters of the company at Fort Astoria (now Astoria, Oregon) at the mouth of the Columbia River was unfit. The York Factory Express trade route evolved from an earlier express brigade used by the North West Company between Fort George (originally Fort Astoria founded in 1811 by John Jacob Astor's American Fur Company), at the mouth of the Columbia River, to Fort William on Lake Superior.
In the 1821 merger with the North West Company, the Hudson's Bay Company gained control of NWC trading posts west of the Rocky Mountains, with headquarters at Fort George (formerly Astoria). George Simpson, Governor of Hudson's Bay Company, visited the Columbia District in 1824-25, journeying from York Factory. He investigated a quicker route than previously used, following the Saskatchewan River and crossing the mountains at Athabasca Pass. This route was thereafter followed by the York Factory Express brigades.
McLoughlin built Fort Vancouver (now Vancouver, Washington) as a replacement for Fort George, on the north side of the Columbia across from the mouth of the Willamette River, at a site chosen by Sir George Simpson. The post was opened for business on March 19, 1825. From his Columbia Department headquarters in Fort Vancouver he supervised trade and kept peace with the Indians, inaugurated salmon and timber trade with Mexican controlled California and Hawaii, and supplied Russian America with produce.
By 1825 there were usually two brigades, each setting out from opposite ends of the route, Fort Vancouver in the Columbia District on the lower Columbia River and the other from York Factory on Hudson Bay, in spring and passing each other in the middle of the continent. Each brigade consisted of about forty to seventy five men and two to five specially made boats and travelled at breakneck speed (for the time). Indians along the way were often paid in trade goods to help them portage around falls and unnavigable rapids. An 1839 report cites the travel time as three months and ten days—almost 26 miles (40 km) per day on average. These men carried supplies in and furs out by boat, horseback and as back packs for the forts and trading posts along the route. They also carried status reports for supplies needed, furs traded etc. from Dr. John McLoughlin head of the Oregon Country HBC operations, and the other fort managers along the route. 
Fort Vancouver became the center of activity in the Pacific Northwest. Every year ships would come from London to drop off supplies and trade goods in exchange for the furs. It was the nexus for the fur trade on the Pacific Coast; its influence reached from the Rocky Mountains to the Hawaiian Islands, and from Russian Alaska into Mexican-controlled California. From Fort Vancouver, at its pinnacle, McLoughlin watched over 34 outposts, 24 ports, six ships, and 600 employees. Under McLoughlin's management, the Columbia Department remained highly profitable, in part due to the ongoing high demand for beaver hats in Europe.
In 1828, McLoughlin was in charge at Fort Vancouver when American explorer Jedediah Smith and three other survivors arrived following the massacre of fifteen members of his exploring party by Umpqua people, who lived to the south in Oregon.
McLoughlin's appearance, 6 foot 4 inches (193 cm) tall with long, prematurely white hair, brought him respect; but he was also generally known for his fair treatment of the people with whom he dealt, whether they were British subjects, U.S. citizens, or of indigenous origin. At the time, the wives of many Hudson's Bay field employees were indigenous, including McLoughlin's wife Marguerite; who was metis, the daughter of an aboriginal woman and one of the original partners of the North West Company, Jean-Etienne Wadin. She was the widow of Alexander McKay, a trader killed in the "Tonquin" incident. See Jonathan Thorn. Her son Thomas McKay became McLoughlin's stepson.
When three Japanese fishermen, among them Otokichi, were shipwrecked on the Olympic Peninsula in 1834, McLoughlin, envisioning an opportunity to use them to open trade with Japan, sent the trio to London on the "Eagle" to try to convince the Crown of his plan. They reached London in 1835, probably the first Japanese to do so since the 16th century Christopher and Cosmas. The British Government finally did not show interest, and the castaways were sent to Macau so that they could be returned to Japan, but that was not possible as Japan was closed to most outsiders at the time.
Relations with American settlers.
In 1821 the British Parliament imposed the laws of Upper Canada on British subjects in Columbia District, and gave the authority to enforce those laws to the Hudson's Bay Company. John McLoughlin, as Superintendent of Fort Vancouver, applied the law to British subjects, kept peace with the natives and sought to maintain law and order over American settlers as well.
In the early 1840s, with the arrival of the first wagon trains via the Oregon Trail, McLoughlin disobeyed company orders and extended substantial aid to the American settlers. Relations between Britain and the United States had become very strained, and many expected war to break out any time. McLoughlin's aid probably prevented an armed attack on his outpost by the numerous American settlers. The settlers understood that his motives were not purely altruistic, and some resented the assistance, working against him for the rest of his life.
The Hudson Bay Company officially discouraged settlement because it interfered with the lucrative fur trade. The company belatedly realized that the increasing numbers of American settlers in the area would result in Columbia District becoming part of U.S. territory. In 1841, Hudson Bay Company Governor George Simpson ordered Alexander Ross to organize a party of Red River settlers to emigrate and occupy the land for Britain. James Sinclair took over leadership from Ross, and when the expedition of almost 200 men women and children reached Fort Vancouver later that year most were settled on Hudson's Bay farms at Nisqually and Cowlitz and some settled south of the Columbia River In the Willamette Valley
As tensions mounted in the Oregon boundary dispute; Simpson, realizing that border might ultimately be as far north as the 49th parallel, ordered McLoughlin to relocate their regional headquarters to Vancouver Island. McLoughlin, in turn, directed James Douglas to construct Fort Camosun (now Victoria, British Columbia, Canada) in 1843. But McLoughlin, whose life was increasingly connected to the Willamette River Valley, refused to move there.
McLoughlin was involved with the debate over the future of the Oregon Country. He advocated an independent nation that would be free of the United States during debates at the Oregon Lyceum in 1842 through his lawyer. This view won support at first and a resolution adopted, but was later moved away from in favor of a resolution by George Abernethy of the Methodist Mission to wait on forming an independent country.
In 1843 American settlers established their own government, called the Provisional Government of Oregon. A legislative committee drafted a code of laws known as the Organic Law. It included the creation of an executive committee of three, a judiciary, militia, land laws, and four counties. There was vagueness and confusion over the nature of the 1843 Organic Law, in particular whether it was a constitutional or statutory. In 1844 a new legislative committee decided to consider it statutory. The 1845 Organic Law made additional changes, including allowing the participation of British subjects in the government. Although the Oregon Treaty of 1846 settled the boundaries of US jurisdiction upon all lands south of the 49th parallel, the Provisional Government continued to function until 1849, when the first governor of Oregon Territory arrived.
Later life in the Oregon Territory.
After resigning from the Hudson's Bay Company in 1846, McLoughlin moved his family back south to Oregon City in the Willamette Valley. The Oregon Treaty had been ratified by that time, and the region, now known as the Oregon Territory, was part of the United States. The valley was the destination of choice for settlers streaming in over the Oregon Trail. At his Oregon City store he sold food and farming tools to settlers. In 1847, McLoughlin was given the Knighthood of St. Gregory, bestowed on him by Pope Gregory XVI. He became a U.S. citizen in 1849. McLoughlin's opponents succeeded in inserting a clause forfeiting his land claim in the Donation Land Claim Act of 1850 by Samuel R. Thurston. Although it was never enforced, it embittered the elderly McLoughlin. He served as mayor of Oregon City in 1851, winning 44 of 66 votes. He died of natural causes in 1857. His grave is now located beside his home overlooking downtown Oregon City.
Legacy.
In 1953, the state of Oregon donated to the National Statuary Hall Collection a bronze statue of McLoughlin, which is currently displayed at the Capitol Visitor Center. The title "Father of Oregon" was officially bestowed on him by the Oregon Legislative Assembly in 1957, on the centennial of his death. Many public works in Oregon are named after him, including:
McLoughlin's former residence in Oregon City, now known as the McLoughlin House, is today a museum; it is part of the Fort Vancouver National Historic Site.
John McLoughlin lost one son to a violent death. John McLoughlin, Jr. had been appointed the second Clerk in Charge at Fort Stikine, only to die in April 1842 at the hands of one of the fort employees, Urbain Heroux, who was charged with his murder but acquitted for lack of evidence, which added to the grievances John Sr. held against the Company. In 2015, Goose Lane Editions published "The Bastard of Fort Stikine" by author Debra Komar. The book is both a historical look at McLoughlin, Jr.'s violent death and a true crime investigation into the circumstances of his death.

</doc>
<doc id="41545" url="http://en.wikipedia.org/wiki?curid=41545" title="Avogadro constant">
Avogadro constant

In chemistry and physics, the Avogadro constant (symbols: "L", "N"A) is the number of constituent particles, usually atoms or molecules, that are contained in the amount of substance given by one mole. Thus it is the proportionality factor that relates the molar mass of a material to its mass. It has the dimension of reciprocal amount of substance. Avogadro's constant has the value in the International System of Units (SI).
Previous definitions of chemical quantity involved Avogadro's number, a historical term closely related to the Avogadro constant but defined differently: Avogadro's number was initially defined by Jean Baptiste Perrin as the number of atoms in one gram-molecule of atomic hydrogen, meaning (in modern terminology) one gram of (atomic) hydrogen. It was later redefined as the number of atoms in 12 grams of the isotope carbon-12 (12C) and still later generalized to relate amounts of a substance to their molecular weight. For instance, to a first approximation, 1 gram of hydrogen element (H), which has a mass number of 1 (atomic number 1), has hydrogen atoms. Similarly, 12 grams of 12C, with the mass number of 12 (atomic number 6), has the same number of carbon atoms, . Avogadro's number is a dimensionless quantity and has the numerical value of the Avogadro constant given in base units.
The Avogadro constant is fundamental to understanding both the makeup of molecules and their interactions and combinations. For instance, since one atom of oxygen will combine with two atoms of hydrogen to create one molecule of water (H2O), one can similarly see that one mole of oxygen ( of O atoms) will combine with two moles of hydrogen (2 × of H atoms) to make one mole of H2O.
"Mole" and "moles" are frequently abbreviated as "mol" in chemical and mathematic notation.
Revisions in the base set of SI units necessitated redefinitions of the concepts of chemical quantity and so Avogadro's number, and its definition, was deprecated in favor of the Avogadro constant and its definition. Changes in the SI units are proposed that will precisely fix the value of the constant to exactly when it is expressed in the unit mol−1 (see New SI definitions, in which an "X" at the end of a number means one or more final digits yet to be agreed upon).
History.
The Avogadro constant is named after the early 19th century Italian scientist Amedeo Avogadro who, in 1811, first proposed that the volume of a gas (at a given pressure and temperature) is proportional to the number of atoms or molecules regardless of the nature of the gas. The French physicist Jean Perrin in 1909 proposed naming the constant in honor of Avogadro. Perrin won the 1926 Nobel Prize in Physics, largely for his work in determining the Avogadro constant by several different methods.
The value of the Avogadro constant was first indicated by Johann Josef Loschmidt who in 1865 estimated the average diameter of the molecules in air by a method that is equivalent to calculating the number of particles in a given volume of gas. This latter value, the number density formula_1 of particles in an ideal gas, is now called the Loschmidt constant in his honor, and is related to the Avogadro constant, "N"A, by
where "p"0 is the pressure, "R" is the gas constant and "T"0 is the absolute temperature. The connection with Loschmidt is the root of the symbol "L" sometimes used for the Avogadro constant, and German language literature may refer to both constants by the same name, distinguished only by the units of measurement.
Accurate determinations of Avogadro's number require the measurement of a single quantity on both the atomic and macroscopic scales using the same unit of measurement. This became possible for the first time when American physicist Robert Millikan measured the charge on an electron in 1910. The electric charge per mole of electrons is a constant called the Faraday constant and had been known since 1834 when Michael Faraday published his works on electrolysis. By dividing the charge on a mole of electrons by the charge on a single electron the value of Avogadro's number is obtained. Since 1910, newer calculations have more accurately determined the values for the Faraday constant and the elementary charge. ("See below")
Perrin originally proposed the name Avogadro's number ("N") to refer to the number of molecules in one gram-molecule of oxygen (exactly of oxygen, according to the definitions of the period), and this term is still widely used, especially in introductory works. The change in name to "Avogadro constant" ("N"A) came with the introduction of the mole as a base unit in the International System of Units (SI) in 1971, which recognized amount of substance as an independent dimension of measurement. With this recognition, the Avogadro constant was no longer a pure number, but had a unit of measurement, the reciprocal mole (mol−1).
While it is rare to use units of amount of substance other than the mole, the Avogadro constant can also be expressed in units such as the pound mole (lb-mol) and the ounce mole (oz-mol).
General role in science.
Avogadro's constant is a scaling factor between macroscopic and microscopic (atomic scale) observations of nature. As such, it provides the relation between other physical constants and properties. For example, it establishes a relationship between the gas constant "R" and the Boltzmann constant "k"B,
and the Faraday constant "F" and the elementary charge "e",
The Avogadro constant also enters into the definition of the unified atomic mass unit, u,
where "M"u is the molar mass constant.
Measurement.
Coulometry.
The earliest accurate method to measure the value of the Avogadro constant was based on coulometry. The principle is to measure the Faraday constant, "F", which is the electric charge carried by one mole of electrons, and to divide by the elementary charge, "e", to obtain the Avogadro constant.
The classic experiment is that of Bower and Davis at NIST, and relies on dissolving silver metal away from the anode of an electrolysis cell, while passing a constant electric current "I" for a known time "t". If "m" is the mass of silver lost from the anode and "A"r the atomic weight of silver, then the Faraday constant is given by:
The NIST scientists devised a method to compensate for silver lost from the anode by mechanical causes, and conducted an isotope analysis of the silver used to determine its atomic weight. Their value for the conventional Faraday constant is "F"90 = , which corresponds to a value for the Avogadro constant of : both values have a relative standard uncertainty of .
Electron mass measurement.
The Committee on Data for Science and Technology (CODATA) publishes values for physical constants for international use. It determines the Avogadro constant from the ratio of the molar mass of the electron "A"r("e")"M"u to the rest mass of the electron "m"e:
The relative atomic mass of the electron, "A"r("e"), is a directly-measured quantity, and the molar mass constant, "M"u, is a defined constant in the SI. The electron rest mass, however, is calculated from other measured constants:
As may be observed in the table of 2006 CODATA values below, the main limiting factor in the precision of the Avogadro constant is the uncertainty in the value of the Planck constant, as all the other constants that contribute to the calculation are known more precisely.
X-ray crystal density (XRCD) methods.
A modern method to determine the Avogadro constant is the use of X-ray crystallography. Silicon single crystals may be produced today in commercial facilities with extremely high purity and with few lattice defects. This method defines the Avogadro constant as the ratio of the molar volume, "V"m, to the atomic volume "V"atom:
The unit cell of silicon has a cubic packing arrangement of 8 atoms, and the unit cell volume may be measured by determining a single unit cell parameter, the length of one of the sides of the cube, "a".
In practice, measurements are carried out on a distance known as "d"220(Si), which is the distance between the planes denoted by the Miller indices {220}, and is equal to "a"/√8. The 2006 CODATA value for "d"220(Si) is , a relative uncertainty of , corresponding to a unit cell volume of .
The isotope proportional composition of the sample used must be measured and taken into account. Silicon occurs in three stable isotopes (28Si, 29Si, 30Si), and the natural variation in their proportions is greater than other uncertainties in the measurements. The atomic weight "A"r for the sample crystal can be calculated, as the relative atomic masses of the three nuclides are known with great accuracy. This, together with the measured density "ρ" of the sample, allows the molar volume "V"m to be determined:
where "M"u is the molar mass constant. The 2006 CODATA value for the molar volume of silicon is 12.058 8349(11) cm3mol−1, with a relative standard uncertainty of .
As of the 2006 CODATA recommended values, the relative uncertainty in determinations of the Avogadro constant by the X-ray crystal density method is , about two and a half times higher than that of the electron mass method.
International Avogadro Coordination.
The International Avogadro Coordination (IAC), often simply called the "Avogadro project", is a collaboration begun in the early 1990s between various national metrology institutes to measure the Avogadro constant by the X-ray crystal density method to a relative uncertainty of 2×10−8 or less. The project is part of the efforts to redefine the kilogram in terms of a universal physical constant, rather than the International Prototype Kilogram, and complements the measurements of the Planck constant using watt balances. Under the current definitions of the International System of Units (SI), a measurement of the Avogadro constant is an indirect measurement of the Planck constant:
The measurements use highly polished spheres of silicon with a mass of one kilogram. Spheres are used to simplify the measurement of the size (and hence the density) and to minimize the effect of the oxide coating that inevitably forms on the surface. The first measurements used spheres of silicon with natural isotopic composition, and had a relative uncertainty of 3.1×10−7. These first results were also inconsistent with values of the Planck constant derived from watt balance measurements, although the source of the discrepancy is now believed to be known.
The main residual uncertainty in the early measurements was in the measurement of the isotopic composition of the silicon to calculate the atomic weight so, in 2007, a 4.8-kg single crystal of isotopically-enriched silicon (99.94% 28Si) was grown, and two one-kilogram spheres cut from it. Diameter measurements on the spheres are repeatable to within 0.3 nm, and the uncertainty in the mass is 3 µg. Full results from these determinations were expected in late 2010.
Their paper, published in January 2011, summarized the result of the International Avogadro Coordination and presented a measurement of the Avogadro constant to be mol−1.

</doc>
<doc id="41548" url="http://en.wikipedia.org/wiki?curid=41548" title="Phase-locked loop">
Phase-locked loop

A phase-locked loop or phase lock loop (PLL) is a control system that generates an output signal whose phase is related to the phase of an input signal. While there are several differing types, it is easy to initially visualize as an electronic circuit consisting of a variable frequency oscillator and a phase detector. The oscillator generates a periodic signal. The phase detector compares the phase of that signal with the phase of the input periodic signal and adjusts the oscillator to keep the phases matched. Bringing the output signal back toward the input signal for comparison is called a feedback loop since the output is 'fed back' toward the input forming a loop.
Keeping the input and output phase in lock step also implies keeping the input and output frequencies the same. Consequently, in addition to synchronizing signals, a phase-locked loop can track an input frequency, or it can generate a frequency that is a multiple of the input frequency. These properties are used for computer clock synchronization, demodulation, and frequency synthesis.
Phase-locked loops are widely employed in radio, telecommunications, computers and other electronic applications. They can be used to demodulate a signal, recover a signal from a noisy communication channel, generate a stable frequency at multiples of an input frequency (frequency synthesis), or distribute precisely timed clock pulses in digital logic circuits such as microprocessors. Since a single integrated circuit can provide a complete phase-locked-loop building block, the technique is widely used in modern electronic devices, with output frequencies from a fraction of a hertz up to many gigahertz.
Practical analogies.
Automobile race analogy.
For a practical idea of what is going on, consider an auto race. There are many cars, and the driver of each of them wants to go around the track as fast as possible. Each lap corresponds to a complete cycle, and each car will complete dozens of laps per hour. The number of laps per hour (a speed) corresponds to an angular velocity (i.e. a frequency), but the number of laps (a distance) corresponds to a phase (and the conversion factor is the distance around the track loop). 
During most of the race, each car is on its own and the driver of the car is trying to beat the driver of every other car on the course, and the phase of each car varies freely. 
However, if there is an accident, a pace car comes out to set a safe speed. None of the race cars are permitted to pass the pace car (or the race cars in front of them), but each of the race cars wants to stay as close to the pace car as it can. While it is on the track, the pace car is a reference, and the race cars become phase-locked loops. Each driver will measure the phase difference (a distance in laps) between him and the pace car. If the driver is far away, he will increase his engine speed to close the gap. If he's too close to the pace car, he will slow down. The result is all the race cars lock on to the phase of the pace car. The cars travel around the track in a tight group that is a small fraction of a lap.
Clock analogy.
Phase can be proportional to time, so a phase difference can be a time difference. Clocks are, with varying degrees of accuracy, phase-locked (time-locked) to a master clock.
Left on its own, each clock will mark time at slightly different rates. A wall clock, for example, might be fast by a few seconds per hour compared to the reference clock at NIST. Over time, that time difference would become substantial.
To keep the wall clock in sync with the reference clock, each week the owner compares the time on his wall clock to a more accurate clock (a phase comparison), and he resets his clock. Left alone, the wall clock will continue to diverge from the reference clock at the same few seconds per hour rate.
Some clocks have a timing adjustment (a fast-slow control). When the owner compared his wall clock's time to the reference time, he noticed that his clock was too fast. Consequently, he could turn the timing adjust a small amount to make the clock run a little slower(frequency). If things work out right, his clock will be more accurate. Over a series of weekly adjustments, the wall clock's notion of a second would agree with the reference time (locked both in frequency and phase within the wall clock's stability).
An early electromechanical version of a phase-locked loop was used in 1921 in the Shortt-Synchronome clock.
History.
Spontaneous synchronization of weakly coupled pendulum clocks was noted by the Dutch physicist Christiaan Huygens as early as 1673. Around the turn of the 19th century, Lord Rayleigh observed synchronization of weakly coupled organ pipes and tuning forks. In 1919, W. H. Eccles and J. H. Vincent found that two electronic oscillators that had been tuned to oscillate at slightly different frequencies but that were coupled to a resonant circuit would soon oscillate at the same frequency. Automatic synchronization of electronic oscillators was described in 1923 by Edward Victor Appleton. 
Earliest research towards what became known as the phase-locked loop goes back to 1932, when British researchers developed an alternative to Edwin Armstrong's superheterodyne receiver, the Homodyne or direct-conversion receiver. In the homodyne or synchrodyne system, a local oscillator was tuned to the desired input frequency and multiplied with the input signal. The resulting output signal included the original modulation information. The intent was to develop an alternative receiver circuit that required fewer tuned circuits than the superheterodyne receiver. Since the local oscillator would rapidly drift in frequency, an automatic correction signal was applied to the oscillator, maintaining it in the same phase and frequency as the desired signal. The technique was described in 1932, in a paper by Henri de Bellescize, in the French journal "L'Onde Électrique".
In analog television receivers since at least the late 1930s, phase-locked-loop horizontal and vertical sweep circuits are locked to synchronization pulses in the broadcast signal.
When Signetics introduced a line of monolithic integrated circuits such as the NE565 that were complete phase-locked loop systems on a chip in 1969, applications for the technique multiplied. A few years later RCA introduced the "CD4046" CMOS Micropower Phase-Locked Loop, which became a popular integrated circuit.
Structure and function.
Phase-locked loop mechanisms may be implemented as either analog or digital circuits. Both implementations use the same basic structure.
Both analog and digital PLL circuits include four basic elements:
Variations.
There are several variations of PLLs. Some terms that are used are analog phase-locked loop (APLL) also referred to as a linear phase-locked loop (LPLL), digital phase-locked loop (DPLL), all digital phase-locked loop (ADPLL), and software phase-locked loop (SPLL).
Applications.
Phase-locked loops are widely used for synchronization purposes; in space communications for coherent demodulation and threshold extension, bit synchronization, and symbol synchronization. Phase-locked loops can also be used to demodulate frequency-modulated signals. In radio transmitters, a PLL is used to synthesize new frequencies which are a multiple of a reference frequency, with the same stability as the reference frequency.
Other applications include:
Clock recovery.
Some data streams, especially high-speed serial data streams (such as the raw stream of data from the magnetic head of a disk drive), are sent without an accompanying clock. The receiver generates a clock from an approximate frequency reference, and then phase-aligns to the transitions in the data stream with a PLL. This process is referred to as clock recovery. In order for this scheme to work, the data stream must have a transition frequently enough to correct any drift in the PLL's oscillator. Typically, some sort of line code, such as 8b/10b encoding, is used to put a hard upper bound on the maximum time between transitions.
Deskewing.
If a clock is sent in parallel with data, that clock can be used to sample the data. Because the clock must be received and amplified before it can drive the flip-flops which sample the data, there will be a finite, and process-, temperature-, and voltage-dependent delay between the detected clock edge and the received data window. This delay limits the frequency at which data can be sent. One way of eliminating this delay is to include a deskew PLL on the receive side, so that the clock at each data flip-flop is phase-matched to the received clock. In that type of application, a special form of a PLL called a delay-locked loop (DLL) is frequently used.
Clock generation.
Many electronic systems include processors of various sorts that operate at hundreds of megahertz. Typically, the clocks supplied to these processors come from clock generator PLLs, which multiply a lower-frequency reference clock (usually 50 or 100 MHz) up to the operating frequency of the processor. The multiplication factor can be quite large in cases where the operating frequency is multiple gigahertz and the reference crystal is just tens or hundreds of megahertz.
Spread spectrum.
All electronic systems emit some unwanted radio frequency energy. Various regulatory agencies (such as the FCC in the United States) put limits on the emitted energy and any interference caused by it. The emitted noise generally appears at sharp spectral peaks (usually at the operating frequency of the device, and a few harmonics). A system designer can use a spread-spectrum PLL to reduce interference with high-Q receivers by spreading the energy over a larger portion of the spectrum. For example, by changing the operating frequency up and down by a small amount (about 1%), a device running at hundreds of megahertz can spread its interference evenly over a few megahertz of spectrum, which drastically reduces the amount of noise seen on broadcast FM radio channels, which have a bandwidth of several tens of kilohertz.
Clock distribution.
Typically, the reference clock enters the chip and drives a phase locked loop (PLL), which then drives the system's clock distribution. The clock distribution is usually balanced so that the clock arrives at every endpoint simultaneously. One of those endpoints is the PLL's feedback input. The function of the PLL is to compare the distributed clock to the incoming reference clock, and vary the phase and frequency of its output until the reference and feedback clocks are phase and frequency matched.
PLLs are ubiquitous—they tune clocks in systems several feet across, as well as clocks in small portions of individual chips. Sometimes the reference clock may not actually be a pure clock at all, but rather a data stream with enough transitions that the PLL is able to recover a regular clock from that stream. Sometimes the reference clock is the same frequency as the clock driven through the clock distribution, other times the distributed clock may be some rational multiple of the reference.
Jitter and noise reduction.
One desirable property of all PLLs is that the reference and feedback clock edges be brought into very close alignment. The average difference in time between the phases of the two signals when the PLL has achieved lock is called the static phase offset (also called the steady-state phase error). The variance between these phases is called tracking jitter. Ideally, the static phase offset should be zero, and the tracking jitter should be as low as possible.
Phase noise is another type of jitter observed in PLLs, and is caused by the oscillator itself and by elements used in the oscillator's frequency control circuit. Some technologies are known to perform better than others in this regard. The best digital PLLs are constructed with emitter-coupled logic (ECL) elements, at the expense of high power consumption. To keep phase noise low in PLL circuits, it is best to avoid saturating logic families such as transistor-transistor logic (TTL) or CMOS.
Another desirable property of all PLLs is that the phase and frequency of the generated clock be unaffected by rapid changes in the voltages of the power and ground supply lines, as well as the voltage of the substrate on which the PLL circuits are fabricated. This is called substrate and supply noise rejection. The higher the noise rejection, the better.
To further improve the phase noise of the output, an injection locked oscillator can be employed following the VCO in the PLL.
Frequency synthesis.
In digital wireless communication systems (GSM, CDMA etc.), PLLs are used to provide the local oscillator up-conversion during transmission and down-conversion during reception. In most cellular handsets this function has been largely integrated into a single integrated circuit to reduce the cost and size of the handset. However, due to the high performance required of base station terminals, the transmission and reception circuits are built with discrete components to achieve the levels of performance required. GSM local oscillator modules are typically built with a frequency synthesizer integrated circuit and discrete resonator VCOs.
Block diagram.
A phase detector compares two input signals and produces an error signal which is proportional to their phase difference. The error signal is then low-pass filtered and used to drive a VCO which creates an output phase. The output is fed through an optional divider back to the input of the system, producing a negative feedback loop. If the output phase drifts, the error signal will increase, driving the VCO phase in the opposite direction so as to reduce the error. Thus the output phase is locked to the phase at the other input. This input is called the reference.
Analog phase locked loops are generally built with an analog phase detector, low pass filter and VCO placed in a negative feedback configuration. A digital phase locked loop uses a digital phase detector; it may also have a divider in the feedback path or in the reference path, or both, in order to make the PLL's output signal frequency a rational multiple of the reference frequency. A non-integer multiple of the reference frequency can also be created by replacing the simple divide-by-"N" counter in the feedback path with a programmable pulse swallowing counter. This technique is usually referred to as a fractional-N synthesizer or fractional-N PLL.
The oscillator generates a periodic output signal. Assume that initially the oscillator is at nearly the same frequency as the reference signal. If the phase from the oscillator falls behind that of the reference, the phase detector changes the control voltage of the oscillator so that it speeds up. Likewise, if the phase creeps ahead of the reference, the phase detector changes the control voltage to slow down the oscillator. Since initially the oscillator may be far from the reference frequency, practical phase detectors may also respond to frequency differences, so as to increase the lock-in range of allowable inputs.
Depending on the application, either the output of the controlled oscillator, or the control signal to the oscillator, provides the useful output of the PLL system.
Elements.
Phase detector.
A phase detector (PD) generates a voltage, which represents the phase difference between two signals. In a PLL, the two inputs of the phase detector are the reference input and the feedback from the VCO. The PD output voltage is used to control the VCO such that the phase difference between the two inputs is held constant, making it a negative feedback system. There are several types of phase detectors in the two main categories of analog and digital.
Different types of phase detectors have different performance characteristics.
For instance, the frequency mixer produces harmonics that adds complexity in applications where spectral purity of the VCO signal is important. The resulting unwanted (spurious) sidebands, also called "reference spurs" can dominate the filter requirements and reduce the capture range well below and/or increase the lock time beyond the requirements. In these applications the more complex digital phase detectors are used which do not have as severe a reference spur component on their output. Also, when in lock, the steady-state phase difference at the inputs using this type of phase detector is near 90 degrees. The actual difference is determined by the DC loop gain.
A bang-bang charge pump phase detector must always have a dead band where the phases of inputs are close enough that the detector detects no phase error. For this reason, bang-bang phase detectors are associated with significant minimum peak-to-peak jitter, because of drift within the dead band. However these types, having outputs consisting of very narrow pulses at lock, are very useful for applications requiring very low VCO spurious outputs. The narrow pulses contain very little energy and are easy to filter out of the VCO control voltage. This results in low VCO control line ripple and therefore low FM sidebands on the VCO.
In PLL applications it is frequently required to know when the loop is out of lock. The more complex digital phase-frequency detectors usually have an output that allows a reliable indication of an out of lock condition.
Filter.
The block commonly called the PLL loop filter (usually a low pass filter) generally has two distinct functions.
The primary function is to determine loop dynamics, also called stability. This is how the loop responds to disturbances, such as changes in the reference frequency, changes of the feedback divider, or at startup. Common considerations are the range over which the loop can achieve lock (pull-in range, lock range or capture range), how fast the loop achieves lock (lock time, lock-up time or settling time) and damping behavior. Depending on the application, this may require one or more of the following: a simple proportion (gain or attenuation), an integral (low pass filter) and/or derivative (high pass filter). Loop parameters commonly examined for this are the loop's gain margin and phase margin. Common concepts in control theory including the PID controller are used to design this function.
The second common consideration is limiting the amount of reference frequency energy (ripple) appearing at the phase detector output that is then applied to the VCO control input. This frequency modulates the VCO and produces FM sidebands commonly called "reference spurs". The low pass characteristic of this block can be used to attenuate this energy, but at times a band reject "notch" may also be useful.
The design of this block can be dominated by either of these considerations, or can be a complex process juggling the interactions of the two. Typical trade-offs are: increasing the bandwidth usually degrades the stability or too much damping for better stability will reduce the speed and increase settling time. Often also the phase-noise is affected.
Oscillator.
All phase-locked loops employ an oscillator element with variable frequency capability. This can be an analog VCO either driven by analog circuitry in the case of an APLL or driven digitally through the use of a digital-to-analog converter as is the case for some DPLL designs. Pure digital oscillators such as a numerically controlled oscillator are used in ADPLLs.
Feedback path and optional divider.
PLLs may include a divider between the oscillator and the feedback input to the phase detector to produce a frequency synthesizer. A programmable divider is particularly useful in radio transmitter applications, since a large number of transmit frequencies can be produced from a single stable, accurate, but expensive, quartz crystal–controlled reference oscillator.
Some PLLs also include a divider between the reference clock and the reference input to the phase detector. If the divider in the feedback path divides by formula_1 and the reference input divider divides by formula_2, it allows the PLL to multiply the reference frequency by formula_3. It might seem simpler to just feed the PLL a lower frequency, but in some cases the reference frequency may be constrained by other issues, and then the reference divider is useful.
Frequency multiplication can also be attained by locking the VCO output to the "N"th harmonic of the reference signal. Instead of a simple phase detector, the design uses a harmonic mixer (sampling mixer). The harmonic mixer turns the reference signal into an impulse train that is rich in harmonics. The VCO output is coarse tuned to be close to one of those harmonics. Consequently, the desired harmonic mixer output (representing the difference between the "N" harmonic and the VCO output) falls within the loop filter passband.
It should also be noted that the feedback is not limited to a frequency divider. This element can be other elements such as a frequency multiplier, or a mixer. The multiplier will make the VCO output a sub-multiple (rather than a multiple) of the reference frequency. A mixer can translate the VCO frequency by a fixed offset. It may also be a combination of these. An example being a divider following a mixer; this allows the divider to operate at a much lower frequency than the VCO without a loss in loop gain.
Modeling.
Time domain model.
The equations governing a phase-locked loop with an analog multiplier
as the phase detector and linear filter may be derived as follows.
Let the input to the phase
detector be formula_4 and the output of the VCO is
formula_5 with phases formula_6 and
formula_7. Functions formula_8 and
formula_9 describe waveforms of signals. Then the
output of the phase detector formula_10 is given by
the VCO frequency is usually taken as a function of the VCO input
formula_12 as
where formula_14 is the "sensitivity" of the VCO and is
expressed in Hz / V;
formula_15 is a free-running frequency of VCO.
The loop filter can be described by system of linear differential equations
where formula_10 is an input of the filter,
formula_12 is an output of the filter, formula_19 is
formula_20-by-formula_20 matrix,
formula_22. formula_23 represents an initial state of
the filter. The star symbol is a conjugate transpose.
Hence the following system describes PLL
where formula_25 is an initial phase shift.
Phase domain model.
Consider the input of PLL formula_4 and VCO output
formula_5 are high frequency signals.
Then for any
piecewise differentiable formula_28-periodic functions
formula_8 and formula_9 there is a function
formula_31 such that the output formula_32 of Filter
in phase domain is asymptotically equal ( the difference formula_34 is small with respect to the frequencies) to the output of the Filter in time domain model. 
Here function formula_31 is a phase detector characteristic.
Denote by formula_36 the phase difference
Then the following dynamical system describes PLL behavior
Here formula_39; formula_40 is a frequency of reference oscillator (we assume that formula_15 is constant).
Example.
Consider sinusoidal signals
and simple one-pole RC circuit as a filter. The time-domain model takes the form
PD characteristics for this signals is equal to
Hence the phase domain model takes form
This system of equations is equivalent to the equation of mathematical pendulum
Linearized phase domain model.
Phase locked loops can also be analyzed as control systems by applying the Laplace transform. The loop response can be written as:
Where
The loop characteristics can be controlled by inserting different types of loop filters. The simplest filter is a one-pole RC circuit. The loop transfer function in this case is:
The loop response becomes:
This is the form of a classic harmonic oscillator. The denominator can be related to that of a second order system:
Where
For the one-pole RC filter,
The loop natural frequency is a measure of the response time of the loop, and the damping factor is a measure of the overshoot and ringing. Ideally, the natural frequency should be high and the damping factor should be near 0.707 (critical damping). With a single pole filter, it is not possible to control the loop frequency and damping factor independently. For the case of critical damping,
A slightly more effective filter, the lag-lead filter includes one pole and one zero. This can be realized with two resistors and one capacitor. The transfer function for this filter is
This filter has two time constants
Substituting above yields the following natural frequency and damping factor
The loop filter components can be calculated independently for a given natural frequency and damping factor
Real world loop filter design can be much more complex e.g. using higher order filters to reduce various types or source of phase noise. (See the D Banerjee ref below)
Implementing a digital phase-locked loop in software.
Digital phase locked loops can be implemented in hardware, using integrated circuits such as a CMOS 4046. However, with microcontrollers becoming faster, it may make sense to implement a phase locked loop in software for applications that do not require locking onto signals in the MHz range or faster, such as precisely controlling motor speeds. Software implementation has several advantages including easy customization of the feedback loop including changing the multiplication or division ratio between the signal being tracked and the output oscillator. Furthermore, a software implementation is useful to understand and experiment with. As an example of a phase-locked loop implemented using a phase frequency detector is presented in MATLAB, as this type of phase detector is robust and easy to implement. This example uses integer arithmetic rather than floating point, as such an example is likely more useful in practice.
In this example, an array tracksig is assumed to contain a reference signal to be tracked. The oscillator is implemented by a counter, with the most significant bit of the counter indicating the on/off status of the oscillator. This code simulates the two D-type flip-flops that comprise a phase-frequency comparator. When either the reference or signal has a positive edge, the corresponding flip-flop switches high. Once both reference and signal is high, both flip-flops are reset. Which flip-flop is high determines at that instant whether the reference or signal leads the other. The error signal is the difference between these two flip-flop values. The pole-zero filter is implemented by adding the error signal and its derivative to the filtered error signal. This in turn is integrated to find the oscillator frequency. 
In practice, one would likely insert other operations into the feedback of this phase-locked loop. For example, if the phase locked loop were to implement a frequency multiplier, the oscillator signal could be divided in frequency before it is compared to the reference signal.

</doc>
<doc id="41549" url="http://en.wikipedia.org/wiki?curid=41549" title="Phase noise">
Phase noise

In signal processing, phase noise is the frequency domain representation of rapid, short-term, random fluctuations in the phase of a waveform, caused by time domain instabilities ("jitter"). Generally speaking, radio frequency engineers speak of the phase noise of an oscillator, whereas digital system engineers work with the jitter of a clock.
Definitions.
Historically there have been two conflicting yet widely used definitions for phase noise. Some authors define phase noise to be the spectral density of a signal's phase only, while the other definition refers to the phase spectrum (which pairs up with the amplitude spectrum, see spectral density#Related concepts) resulting from the spectral estimation of the signal itself. Both definitions yield the same result at offset frequencies well removed from the carrier. At close-in offsets however, the two definitions differ.
The IEEE defines phase noise as formula_1 where the "phase instability" formula_2 is the one-sided spectral density of a signal's phase deviation. Although formula_2 is a one-sided function, it represents "the double-sideband spectral density of phase fluctuation". The phase noise expression formula_4 is pronounced "script ell of f".
Background.
An ideal oscillator would generate a pure sine wave. In the frequency domain, this would be represented as a single pair of Dirac delta functions (positive and negative conjugates) at the oscillator's frequency, i.e., all the signal's power is at a single frequency. All real oscillators have phase modulated noise components. The phase noise components spread the power of a signal to adjacent frequencies, resulting in noise sidebands. Oscillator phase noise often includes low frequency flicker noise and may include white noise.
Consider the following noise-free signal:
Phase noise is added to this signal by adding a stochastic process represented by φ to the signal as follows:
Phase noise is a type of cyclostationary noise and is closely related to jitter. A particularly important type of phase noise is that produced by oscillators.
Phase noise (ℒ("f")) is typically expressed in units of dBc/Hz, and it represents the noise power relative to the carrier contained in a 1 Hz bandwidth centered at a certain offsets from the carrier. For example, a certain signal may have a phase noise of -80 dBc/Hz at an offset of 10 kHz and -95 dBc/Hz at an offset of 100 kHz. Phase noise can be measured and expressed as single sideband or double sideband values, but as noted earlier, the IEEE has adopted the definition as one-half of the double sideband PSD.
Jitter conversions.
Phase noise is sometimes also measured and expressed as a power obtained by integrating ℒ("f") over a certain range of offset frequencies. For example, the phase noise may be -40 dBc integrated over the range of 1 kHz to 100 kHz. This Integrated phase noise (expressed in degrees) can be converted to jitter (expressed in seconds) using the following formula:
formula_5
In the absence of 1/f noise in a region where the phase noise displays a –20 dBc/decade slope, the
rms cycle jitter can be related to the phase noise by:
formula_6
Likewise:
formula_7
Measurement.
Phase noise can be measured using a spectrum analyzer if the phase noise of the device under test (DUT) is large with respect to the spectrum analyzer's local oscillator. Care should be taken that observed values are due to the measured signal and not the shape factor of the spectrum analyzer's filters. Spectrum analyzer based measurement can show the phase-noise power over many decades of frequency e.g. 1 Hz to 10 MHz. The slope with offset frequency in various offset frequency regions can provide clues as to the source of the noise, e.g. low frequency flicker noise decreasing at 30 dB per decade (=9 dB per octave).
Phase noise measurement systems are alternatives to spectrum analyzers. These systems may use internal and external references and allow measurement of both residual and additive noise. Additionally, these systems can make low-noise, close-to-the-carrier, measurements.
Spectral purity.
The sinewave output of an ideal oscillator is a single line in the frequency spectrum. Such perfect spectral purity is not achievable in a practical oscillator. Spreading of the spectrum line caused by phase noise must be minimised in the local oscillator for a superheterodyne receiver because it defeats the aim of restricting the receiver frequency range by filters in the IF (intermediate frequency) amplifier.

</doc>
<doc id="41550" url="http://en.wikipedia.org/wiki?curid=41550" title="Phase perturbation">
Phase perturbation

Phase perturbation is the shifting, from whatever cause, in the phase of an electronic signal. The shifting is often quite rapid, and may appear to be random or cyclic. The phase departure in phase perturbation usually is larger, but less rapid, than in phase jitter.
Phase perturbation may be expressed in degrees, with any cyclic component expressed in hertz.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41551" url="http://en.wikipedia.org/wiki?curid=41551" title="Phase-shift keying">
Phase-shift keying

Phase-shift keying (PSK) is a digital modulation scheme that conveys data by changing, or modulating, the phase of a reference signal (the carrier wave).
Any digital modulation scheme uses a finite number of distinct signals to represent digital data. PSK uses a finite number of phases, each assigned a unique pattern of binary digits. Usually, each phase encodes an equal number of bits. Each pattern of bits forms the symbol that is represented by the particular phase. The demodulator, which is designed specifically for the symbol-set used by the modulator, determines the phase of the received signal and maps it back to the symbol it represents, thus recovering the original data. This requires the receiver to be able to compare the phase of the received signal to a reference signal — such a system is termed coherent (and referred to as CPSK).
Alternatively, instead of operating with respect to a constant reference wave, the broadcast can operate with respect to itself. Changes in phase of a single broadcast waveform can be considered the significant items. In this system, the demodulator determines the changes in the phase of the received signal rather than the phase (relative to a reference wave) itself. Since this scheme depends on the difference between successive phases, it is termed differential phase-shift keying (DPSK). DPSK can be significantly simpler to implement than ordinary PSK since there is no need for the demodulator to have a copy of the reference signal to determine the exact phase of the received signal (it is a non-coherent scheme). In exchange, it produces more erroneous demodulation.
Introduction.
There are three major classes of digital modulation techniques used for transmission of digitally represented data:
All convey data by changing some aspect of a base signal, the carrier wave (usually a sinusoid), in response to a data signal. In the case of PSK, the phase is changed to represent the data signal. There are two fundamental ways of utilizing the phase of a signal in this way:
A convenient method to represent PSK schemes is on a constellation diagram. This shows the points in the complex plane where, in this context, the real and imaginary axes are termed the in-phase and quadrature axes respectively due to their 90° separation. Such a representation on perpendicular axes lends itself to straightforward implementation. The amplitude of each point along the in-phase axis is used to modulate a cosine (or sine) wave and the amplitude along the quadrature axis to modulate a sine (or cosine) wave. By convention, in-phase modulates cosine and quadrature modulates sine.
In PSK, the constellation points chosen are usually positioned with uniform angular spacing around a circle. This gives maximum phase-separation between adjacent points and thus the best immunity to corruption. They are positioned on a circle so that they can all be transmitted with the same energy. In this way, the moduli of the complex numbers they represent will be the same and thus so will the amplitudes needed for the cosine and sine waves. Two common examples are "binary phase-shift keying" (BPSK) which uses two phases, and "quadrature phase-shift keying" (QPSK) which uses four phases, although any number of phases may be used. Since the data to be conveyed are usually binary, the PSK scheme is usually designed with the number of constellation points being a power of 2.
Definitions.
For determining error-rates mathematically, some definitions will be needed:
formula_9 will give the probability that a single sample taken from a random process with zero-mean and unit-variance Gaussian probability density function will be greater or equal to formula_10. It is a scaled form of the complementary Gaussian error function:
The error-rates quoted here are those in additive white Gaussian noise (AWGN). These error rates are lower than those computed in fading channels, hence, are a good theoretical benchmark to compare with.
Applications.
Owing to PSK's simplicity, particularly when compared with its competitor quadrature amplitude modulation, it is widely used in existing technologies.
The wireless LAN standard, IEEE 802.11b-1999, uses a variety of different PSKs depending on the data rate required. At the basic rate of 1 Mbit/s, it uses DBPSK (differential BPSK). To provide the extended rate of 2 Mbit/s, DQPSK is used. In reaching 5.5 Mbit/s and the full rate of 11 Mbit/s, QPSK is employed, but has to be coupled with complementary code keying. The higher-speed wireless LAN standard, IEEE 802.11g-2003, has eight data rates: 6, 9, 12, 18, 24, 36, 48 and 54 Mbit/s. The 6 and 9 Mbit/s modes use OFDM modulation where each sub-carrier is BPSK modulated. The 12 and 18 Mbit/s modes use OFDM with QPSK. The fastest four modes use OFDM with forms of quadrature amplitude modulation.
Because of its simplicity BPSK is appropriate for low-cost passive transmitters, and is used in RFID standards such as ISO/IEC 14443 which has been adopted for biometric passports, credit cards such as American Express's ExpressPay, and many other applications.
Bluetooth 2 will use formula_12-DQPSK at its lower rate (2 Mbit/s) and 8-DPSK at its higher rate (3 Mbit/s) when the link between the two devices is sufficiently robust. Bluetooth 1 modulates with Gaussian minimum-shift keying, a binary scheme, so either modulation choice in version 2 will yield a higher data-rate. A similar technology, IEEE 802.15.4 (the wireless standard used by ZigBee) also relies on PSK. IEEE 802.15.4 allows the use of two frequency bands: 868–915 MHz using BPSK and at 2.4 GHz using OQPSK.
Notably absent from these various schemes is 8-PSK. This is because its error-rate performance is close to that of 16-QAM — it is only about 0.5 dB better — but its data rate is only three-quarters that of 16-QAM. Thus 8-PSK is often omitted from standards and, as seen above, schemes tend to 'jump' from QPSK to 16-QAM (8-QAM is possible but difficult to implement).
Included among the exceptions is HughesNet satellite ISP. For example, the model HN7000S modem
(on KU-band satcom) uses 8-PSK modulation.
Binary phase-shift keying (BPSK).
BPSK (also sometimes called PRK, phase reversal keying, or 2PSK) is the simplest form of phase shift keying (PSK). It uses two phases which are separated by 180° and so can also be termed 2-PSK. It does not particularly matter exactly where the constellation points are positioned, and in this figure they are shown on the real axis, at 0° and 180°. This modulation is the most robust of all the PSKs since it takes the highest level of noise or distortion to make the demodulator reach an incorrect decision. It is, however, only able to modulate at 1 bit/symbol (as seen in the figure) and so is unsuitable for high data-rate applications.
In the presence of an arbitrary phase-shift introduced by the communications channel, the demodulator is unable to tell which constellation point is which. As a result, the data is often differentially encoded prior to modulation.
BPSK is functionally equivalent to 2-QAM modulation.
Implementation.
The general form for BPSK follows the equation:
This yields two phases, 0 and π.
In the specific form, binary data is often conveyed with the following signals:
where "f""c" is the frequency of the carrier-wave.
Hence, the signal-space can be represented by the single basis function
where 1 is represented by formula_17 and 0 is represented by formula_18. This assignment is, of course, arbitrary.
This use of this basis function is shown at the end of the next section in a signal timing diagram. The topmost signal is a BPSK-modulated cosine wave that the BPSK modulator would produce. The bit-stream that causes this output is shown above the signal (the other parts of this figure are relevant only to QPSK).
Bit error rate.
The bit error rate (BER) of BPSK in AWGN can be calculated as:
Since there is only one bit per symbol, this is also the symbol error rate.
Quadrature phase-shift keying (QPSK).
Sometimes this is known as "quadriphase PSK", 4-PSK, or 4-QAM. (Although the root concepts of QPSK and 4-QAM are different, the resulting modulated radio waves are exactly the same.) QPSK uses four points on the constellation diagram, equispaced around a circle. With four phases, QPSK can encode two bits per symbol, shown in the diagram with Gray coding to minimize the bit error rate (BER) — sometimes misperceived as twice the BER of BPSK.
The mathematical analysis shows that QPSK can be used either to double the data rate compared with a BPSK system while maintaining the "same" bandwidth of the signal, or to "maintain the data-rate of BPSK" but halving the bandwidth needed. In this latter case, the BER of QPSK is "exactly the same" as the BER of BPSK - and deciding differently is a common confusion when considering or describing QPSK. The transmitted carrier can undergo numbers of phase changes.
Given that radio communication channels are allocated by agencies such as the Federal Communication Commission giving a prescribed (maximum) bandwidth, the advantage of QPSK over BPSK becomes evident: QPSK transmits twice the data rate in a given bandwidth compared to BPSK - at the same BER. The engineering penalty that is paid is that QPSK transmitters and receivers are more complicated than the ones for BPSK. However, with modern electronics technology, the penalty in cost is very moderate.
As with BPSK, there are phase ambiguity problems at the receiving end, and differentially encoded QPSK is often used in practice.
Implementation.
The implementation of QPSK is more general than that of BPSK and also indicates the implementation of higher-order PSK. Writing the symbols in the constellation diagram in terms of the sine and cosine waves used to transmit them:
This yields the four phases π/4, 3π/4, 5π/4 and 7π/4 as needed.
This results in a two-dimensional signal space with unit basis functions
The first basis function is used as the in-phase component of the signal and the second as the quadrature component of the signal.
Hence, the signal constellation consists of the signal-space 4 points
The factors of 1/2 indicate that the total power is split equally between the two carriers.
Comparing these basis functions with that for BPSK shows clearly how QPSK can be viewed as two independent BPSK signals. Note that the signal-space points for BPSK do not need to split the symbol (bit) energy over the two carriers in the scheme shown in the BPSK constellation diagram.
QPSK systems can be implemented in a number of ways. An illustration of the major components of the transmitter and receiver structure are shown below.
Bit error rate.
Although QPSK can be viewed as a quaternary modulation, it is easier to see it as two independently modulated quadrature carriers. With this interpretation, the even (or odd) bits are used to modulate the in-phase component of the carrier, while the odd (or even) bits are used to modulate the quadrature-phase component of the carrier. BPSK is used on both carriers and they can be independently demodulated.
As a result, the probability of bit-error for QPSK is the same as for BPSK:
However, in order to achieve the same bit-error probability as BPSK, QPSK uses twice the power (since two bits are transmitted simultaneously).
The symbol error rate is given by:
If the signal-to-noise ratio is high (as is necessary for practical QPSK systems) the probability of symbol error may be approximated:
The modulated signal is shown below for a short segment of a random binary data-stream. The two carrier waves are a cosine wave and a sine wave, as indicated by the signal-space analysis above. Here, the odd-numbered bits have been assigned to the in-phase component and the even-numbered bits to the quadrature component (taking the first bit as number 1). The total signal — the sum of the two components — is shown at the bottom. Jumps in phase can be seen as the PSK changes the phase on each component at the start of each bit-period. The topmost waveform alone matches the description given for BPSK above.
The binary data that is conveyed by this waveform is: 1 1 0 0 0 1 1 0.
Variants.
Offset QPSK (OQPSK).
"Offset quadrature phase-shift keying" ("OQPSK") is a variant of phase-shift keying modulation using 4 different values of the phase to transmit. It is sometimes called "Staggered quadrature phase-shift keying" ("SQPSK").
Taking four values of the phase (two bits) at a time to construct a QPSK symbol can allow the phase of the signal to jump by as much as 180° at a time. When the signal is low-pass filtered (as is typical in a transmitter), these phase-shifts result in large amplitude fluctuations, an undesirable quality in communication systems. By offsetting the timing of the odd and even bits by one bit-period, or half a symbol-period, the in-phase and quadrature components will never change at the same time. In the constellation diagram shown on the right, it can be seen that this will limit the phase-shift to no more than 90° at a time. This yields much lower amplitude fluctuations than non-offset QPSK and is sometimes preferred in practice.
The picture on the right shows the difference in the behavior of the phase between ordinary QPSK and OQPSK. It can be seen that in the first plot the phase can change by 180° at once, while in OQPSK the changes are never greater than 90°.
The modulated signal is shown below for a short segment of a random binary data-stream. Note the half symbol-period offset between the two component waves. The sudden phase-shifts occur about twice as often as for QPSK (since the signals no longer change together), but they are less severe. In other words, the magnitude of jumps is smaller in OQPSK when compared to QPSK.
"π" /4–QPSK.
This variant of QPSK uses two identical constellations which are rotated by 45° (formula_12 radians, hence the name) with respect to one another. Usually, either the even or odd symbols are used to select points from one of the constellations and the other symbols select points from the other constellation. This also reduces the phase-shifts from a maximum of 180°, but only to a maximum of 135° and so the amplitude fluctuations of formula_12–QPSK are between OQPSK and non-offset QPSK.
One property this modulation scheme possesses is that if the modulated signal is represented in the complex domain, it does not have any paths through the origin. In other words, the signal does not pass through the origin. This lowers the dynamical range of fluctuations in the signal which is desirable when engineering communications signals.
On the other hand, formula_12–QPSK lends itself to easy demodulation and has been adopted for use in, for example, TDMA cellular telephone systems.
The modulated signal is shown below for a short segment of a random binary data-stream. The construction is the same as above for ordinary QPSK. Successive symbols are taken from the two constellations shown in the diagram. Thus, the first symbol (1 1) is taken from the 'blue' constellation and the second symbol (0 0) is taken from the 'green' constellation. Note that magnitudes of the two component waves change as they switch between constellations, but the total signal's magnitude remains constant (constant envelope). The phase-shifts are between those of the two previous timing-diagrams.
SOQPSK.
The license-free shaped-offset QPSK (SOQPSK) is interoperable with Feher-patented QPSK (FQPSK), in the sense that an integrate-and-dump offset QPSK detector produces the same output no matter which kind of transmitter is used.
These modulations carefully shape the I and Q waveforms such that they change very smoothly, and the signal stays constant-amplitude even during signal transitions. (Rather than traveling instantly from one symbol to another, or even linearly, it travels smoothly around the constant-amplitude circle from one symbol to the next.)
The standard description of SOQPSK-TG involves ternary symbols.
DPQPSK.
Dual-polarization quadrature phase shift keying (DPQPSK) or dual-polarization QPSK - involves the polarization multiplexing of two different QPSK signals, thus improving the spectral efficiency by a factor of 2. This is a cost-effective alternative, to utilizing 16-PSK instead of QPSK to double the spectral efficiency.
Higher-order PSK.
Any number of phases may be used to construct a PSK constellation but 8-PSK is usually the highest order PSK constellation deployed. With more than 8 phases, the error-rate becomes too high and there are better, though more complex, modulations available such as quadrature amplitude modulation (QAM). Although any number of phases may be used, the fact that the constellation must usually deal with binary data means that the number of symbols is usually a power of 2 to allow an integer number of bits per symbol.
Bit error rate.
For the general formula_30-PSK there is no simple expression for the symbol-error probability if formula_31. Unfortunately, it can only be obtained from:
where
This may be approximated for high formula_30 and high formula_40 by:
The bit-error probability for formula_30-PSK can only be determined exactly once the bit-mapping is known. However, when Gray coding is used, the most probable error from one symbol to the next produces only a single bit-error and
The graph on the left compares the bit-error rates of BPSK, QPSK (which are the same, as noted above), 8-PSK and 16-PSK. It is seen that higher-order modulations exhibit higher error-rates; in exchange however they deliver a higher raw data-rate.
Bounds on the error rates of various digital modulation schemes can be computed with application of the union bound to the signal constellation.
Differential phase-shift keying (DPSK).
Differential encoding.
Differential phase shift keying (DPSK) is a common form of phase modulation that conveys data by changing the phase of the carrier wave. As mentioned for BPSK and QPSK there is an ambiguity of phase if the constellation is rotated by some effect in the communications channel through which the signal passes. This problem can be overcome by using the data to "change" rather than "set" the phase.
For example, in differentially encoded BPSK a binary '1' may be transmitted by adding 180° to the current phase and a binary '0' by adding 0° to the current phase. 
Another variant of DPSK is Symmetric Differential Phase Shift keying, SDPSK, where encoding would be +90° for a '1' and −90° for a '0'.
In differentially encoded QPSK (DQPSK), the phase-shifts are 0°, 90°, 180°, −90° corresponding to data '00', '01', '11', '10'. This kind of encoding may be demodulated in the same way as for non-differential PSK but the phase ambiguities can be ignored. Thus, each received symbol is demodulated to one of the formula_30 points in the constellation and a comparator then computes the difference in phase between this received signal and the preceding one. The difference encodes the data as described above. 
Symmetric Differential Quadrature Phase Shift Keying (SDQPSK) is like DQPSK, but encoding is symmetric, using phase shift values of −135°, −45°, +45° and +135°.
The modulated signal is shown below for both DBPSK and DQPSK as described above. In the figure, it is assumed that the "signal starts with zero phase", and so there is a phase shift in both signals at formula_45.
Analysis shows that differential encoding approximately doubles the error rate compared to ordinary formula_30-PSK but this may be overcome by only a small increase in formula_40. Furthermore, this analysis (and the graphical results below) are based on a system in which the only corruption is additive white Gaussian noise(AWGN). However, there will also be a physical channel between the transmitter and receiver in the communication system. This channel will, in general, introduce an unknown phase-shift to the PSK signal; in these cases the differential schemes can yield a "better" error-rate than the ordinary schemes which rely on precise phase information.
Demodulation.
For a signal that has been differentially encoded, there is an obvious alternative method of demodulation. Instead of demodulating as usual and ignoring carrier-phase ambiguity, the phase between two successive received symbols is compared and used to determine what the data must have been. When differential encoding is used in this manner, the scheme is known as differential phase-shift keying (DPSK). Note that this is subtly different from just differentially encoded PSK since, upon reception, the received symbols are "not" decoded one-by-one to constellation points but are instead compared directly to one another.
Call the received symbol in the formula_48th timeslot formula_49 and let it have phase formula_50. Assume without loss of generality that the phase of the carrier wave is zero. Denote the AWGN term as formula_51. Then
The decision variable for the formula_53th symbol and the formula_48th symbol is the phase difference between formula_49 and formula_56. That is, if formula_49 is projected onto formula_56, the decision is taken on the phase of the resultant complex number:
where superscript * denotes complex conjugation. In the absence of noise, the phase of this is formula_60, the phase-shift between the two received signals which can be used to determine the data transmitted.
The probability of error for DPSK is difficult to calculate in general, but, in the case of DBPSK it is:
which, when numerically evaluated, is only slightly worse than ordinary BPSK, particularly at higher formula_40 values.
Using DPSK avoids the need for possibly complex carrier-recovery schemes to provide an accurate phase estimate and can be an attractive alternative to ordinary PSK.
In optical communications, the data can be modulated onto the phase of a laser in a differential way. The modulation is a laser which emits a continuous wave, and a Mach-Zehnder modulator which receives electrical binary data. For the case of BPSK for example, the laser transmits the field unchanged for binary '1', and with reverse polarity for '0'. The demodulator consists of a delay line interferometer which delays one bit, so two bits can be compared at one time. In further processing, a photodiode is used to transform the optical field into an electric current, so the information is changed back into its original state.
The bit-error rates of DBPSK and DQPSK are compared to their non-differential counterparts in the graph to the right. The loss for using DBPSK is small enough compared to the complexity reduction that it is often used in communications systems that would otherwise use BPSK. For DQPSK though, the loss in performance compared to ordinary QPSK is larger and the system designer must balance this against the reduction in complexity.
Example: Differentially encoded BPSK.
At the formula_63 time-slot call the bit to be modulated formula_64, the differentially encoded bit formula_65 and the resulting modulated signal formula_66. Assume that the constellation diagram positions the symbols at ±1 (which is BPSK). The differential encoder produces:
where formula_68 indicates binary or modulo-2 addition.
So formula_65 only changes state (from binary '0' to binary '1' or from binary '1' to binary '0') if formula_64 is a binary '1'. Otherwise it remains in its previous state. This is the description of differentially encoded BPSK given above.
The received signal is demodulated to yield formula_71±1 and then the differential decoder reverses the encoding procedure and produces:
Therefore, formula_73 if formula_65 and formula_75 differ and formula_76 if they are the same. Hence, if both formula_65 and formula_75 are "inverted", formula_64 will still be decoded correctly. Thus, the 180° phase ambiguity does not matter.
Differential schemes for other PSK modulations may be devised along similar lines. The waveforms for DPSK are the same as for differentially encoded PSK given above since the only change between the two schemes is at the receiver.
The BER curve for this example is compared to ordinary BPSK on the right. As mentioned above, whilst the error-rate is approximately doubled, the increase needed in formula_40 to overcome this is small. The increase in formula_40 required to overcome differential modulation in coded systems, however, is larger - typically about 3 dB. The performance degradation is a result of noncoherent transmission - in this case it refers to the fact that tracking of the phase is completely ignored.
Channel capacity.
Like all M-ary modulation schemes with M = 2"b" symbols, when given exclusive access to a fixed bandwidth, the channel capacity of any phase shift keying modulation scheme rises to a maximum of "b" bits per symbol as the signal-to-noise ratio increases.
References.
The notation and theoretical results in this article are based on material presented in the following sources:

</doc>
<doc id="41552" url="http://en.wikipedia.org/wiki?curid=41552" title="Phonetic alphabet">
Phonetic alphabet

Phonetic alphabet can mean:

</doc>
<doc id="41553" url="http://en.wikipedia.org/wiki?curid=41553" title="Photocurrent">
Photocurrent

Photocurrent is the electric current through a photosensitive device, such as a photodiode, as the result of exposure to radiant power.
The photocurrent may occur as a result of the photoelectric, photoemissive, or photovoltaic effect.
The photocurrent may be enhanced by internal gain caused by interaction among ions and photons under the influence of applied fields, such as occurs in an avalanche photodiode (APD).
When a suitable radiation is used, the photoelectric current is directly proportional to the intensity of the radiation.

</doc>
<doc id="41555" url="http://en.wikipedia.org/wiki?curid=41555" title="Pilot">
Pilot

Pilot most commonly refers to:
The term may also refer to:

</doc>
<doc id="41556" url="http://en.wikipedia.org/wiki?curid=41556" title="PIN diode">
PIN diode

 
A PIN diode is a diode with a wide, undoped intrinsic semiconductor region between a p-type semiconductor and an n-type semiconductor region. The p-type and n-type regions are typically heavily doped because they are used for ohmic contacts.
The wide intrinsic region is in contrast to an ordinary PN diode. The wide intrinsic region makes the PIN diode an inferior rectifier (one typical function of a diode), but it makes the PIN diode suitable for attenuators, fast switches, photodetectors, and high voltage power electronics applications.
Operation.
A PIN diode operates under what is known as high-level injection. In other words, the intrinsic "i" region is flooded with charge carriers from the "p" and "n" regions. Its function can be likened to filling up a water bucket with a hole on the side. Once the water reaches the hole's level it will begin to pour out. Similarly, the diode will conduct current once the flooded electrons and holes reach an equilibrium point, where the number of electrons is equal to the number of holes in the intrinsic region. When the diode is forward biased, the injected carrier concentration is typically several orders of magnitude higher than the intrinsic level carrier concentration. Due to this high level injection, which in turn is due to the depletion process, the electric field extends deeply (almost the entire length) into the region. This electric field helps in speeding up of the transport of charge carriers from P to N region, which results in faster operation of the diode, making it a suitable device for high frequency operations.
Characteristics.
A PIN diode obeys the standard diode equation for low frequency signals. At higher frequencies, the diode looks like an almost perfect (very linear, even for large signals) resistor. There is a lot of stored charge in the intrinsic region. At low frequencies, the charge can be removed and the diode turns off. At higher frequencies, there is not enough time to remove the charge, so the diode never turns off. The PIN diode has a poor reverse recovery time.
The high-frequency resistance is inversely proportional to the DC bias current through the diode. A PIN diode, suitably biased, therefore acts as a variable resistor. This high-frequency resistance may vary over a wide range (from 0.1 ohm to 10 kΩ in some cases; the useful range is smaller, though).
The wide intrinsic region also means the diode will have a low capacitance when reverse biased.
In a PIN diode, the depletion region exists almost completely within the intrinsic region. This depletion region is much larger than in a PN diode, and almost constant-size, independent of the reverse bias applied to the diode. This increases the volume where electron-hole pairs can be generated by an incident photon. Some photodetector devices, such as PIN photodiodes and phototransistors (in which the base-collector junction is a PIN diode), use a PIN junction in their construction.
The diode design has some design tradeoffs. Increasing the dimensions of the intrinsic region (and its stored charge) allows the diode to look like a resistor at lower frequencies. It adversely affects the time needed to turn off the diode and its shunt capacitance. PIN diodes will be tailored for a particular use.
Applications.
PIN diodes are useful as RF switches, attenuators, photodetectors, and phase shifters.
RF and microwave switches.
Under zero or reverse bias, a PIN diode has a low capacitance. The low capacitance will not pass much of an RF signal. Under a forward bias of 1 mA, a typical PIN diode will have an RF resistance of about 1 ohm, making it a good RF conductor. Consequently, the PIN diode makes a good RF switch.
Although RF relays can be used as switches, they switch very slowly (on the order of 10 milliseconds). A PIN diode switch can switch much more quickly (e.g., 1 microsecond).
The capacitance of an off discrete PIN diode might be 1 pF. At 320 MHz, the reactance of 1 pF is about -j500 ohms. As a series element in a 50 ohm system, the off-state attenuation would be -20 times the base 10 log of the ratio of the load impedance to the sum of load, diode and source impedances, or roughly 20 dB, which may not be adequate. In applications where higher isolation is needed, both shunt and series elements may be used, with the shunt diodes biased in complementary fashion to the series elements. Adding shunt elements effectively reduces the source and load impedances, reducing the impedance ratio and increasing the off-state attenuation. However, in addition to the added complexity, the on-state attenuation is increased due to the series resistance of the on-state blocking element and the capacitance of the off-state shunt elements.
PIN diode switches are used not only for signal selection, but they are also used for component selection. For example, some low phase noise oscillators use PIN diodes to range-switch inductors.
RF and microwave variable attenuators.
By changing the bias current through a PIN diode, it is possible to quickly change the RF resistance.
At high frequencies, the PIN diode appears as a resistor whose resistance is an inverse function of its forward current. Consequently, PIN diode can be used in some variable attenuator designs as amplitude modulators or output leveling circuits.
PIN diodes might be used, for example, as the bridge and shunt resistors in a bridged-T attenuator. Another common approach is to use PIN diodes as terminations connected to the 0 degree and -90 degree ports of a quadrature hybrid. The signal to be attenuated is applied to the input port, and the attenuated result is taken from the isolation port. The advantages of this approach over the bridged-T and pi approaches are (1) complementary PIN diode bias drives are not needed -- the same bias is applied to both diodes -- and (2) the loss in the attenuator equals the return loss of the terminations, which can be varied over a very wide range.
Limiters.
PIN diodes are sometimes used as input protection devices for high frequency test probes. If the input signal is within range, the PIN diode has little impact as a small capacitance. If the signal is large, then the PIN diode starts to conduct and becomes a resistor that shunts most of the signal to ground.
Photodetector and photovoltaic cell.
The PIN photodiode was invented by Jun-ichi Nishizawa and his colleagues in 1950. 
PIN photodiodes are used in fibre optic network cards and switches. As a photodetector, the PIN diode is reverse biased. Under reverse bias, the diode ordinarily does not conduct (save a small dark current or Is leakage). When a photon of sufficient energy enters the depletion region of the diode, it creates an electron, hole pair. The reverse bias field sweeps the carriers out of the region creating a current. Some detectors can use avalanche multiplication.
The same mechanism applies to the PIN structure, or p-i-n junction, of a solar cell. In this case, the advantage of using a PIN structure over conventional semiconductor p–n junction is the better long wavelength response of the former. In case of long wavelength irradiation, photons penetrate deep into the cell. But only those electron-hole pairs generated in and near the depletion region contribute to current generation. The depletion region of a PIN structure extends across the intrinsic region, deep into the device. This wider depletion width enables electron-hole pair generation deep within the device. This increases the quantum efficiency of the cell.
Typically, amorphous silicon thin-film cells use PIN structures. On the other hand, CdTe cells use NIP structure, a variation of the PIN structure. In a NIP structure, an intrinsic CdTe layer is sandwiched by n-doped CdS and p-doped ZnTe. The photons are incident on the n-doped layer unlike a PIN diode.
A PIN photodiode can also detect X-ray and gamma ray photons.
Example diodes.
SFH203 or BPW43 are cheap general purpose PIN diodes in 5 mm clear plastic case with bandwidth over
100 MHz. They are used in RONJA telecommunication systems and other circuitry applications.

</doc>
<doc id="41557" url="http://en.wikipedia.org/wiki?curid=41557" title="Planar array">
Planar array

In telecommunications and radar, a planar array is an antenna in which all of the elements, both active and parasitic, are in one plane. A planar array provides a large aperture and may be used for directional beam control by varying the relative phase of each element. A planar array may be used with a reflecting screen behind the active plane.
A planar array with a reflecting screen is related to a radar absorber. Both are supposed not to reflect incoming radiation of the desired wavelength. Radar absorbers have the advantage that they can use magnetic materials to avoid reflections at the interface to air at least for some frequencies. For antennas to be broadband, the transition from air (vacuum) to the metal of the screen has to be a gradual one. Radar absorbers have the advantage that they do not have to collect the received energy. The antenna needs a 3D tree of twin-leads and chokes to connect a single cable to a large number of micro antennas. Micro dipole antennas are only resonant at a single high frequency, to be broadband the arms of adjoining antennas have to be connected.

</doc>
<doc id="41559" url="http://en.wikipedia.org/wiki?curid=41559" title="Plane wave">
Plane wave

In the physics of wave propagation, a plane wave (also spelled planewave) is a constant-frequency wave whose wavefronts (surfaces of constant phase) are infinite parallel planes of constant peak-to-peak amplitude normal to the phase velocity vector.
It is not possible in practice to have a true plane wave; only a plane wave of infinite extent will propagate as a plane wave. However, many waves are approximately plane waves in a localized region of space. For example, a localized source such as an antenna produces a field that is approximately a plane wave far from the antenna in its far-field region. Similarly, if the length scales are much longer than the wave’s wavelength, as is often the case for light in the field of optics, one can treat the waves as light rays which correspond locally to plane waves.
 
Mathematical formalisms.
Two functions that meet the above criteria of having a constant frequency and constant amplitude are the sine and cosine functions. One of the simplest ways to use such a sinusoid involves defining it along the direction of the x-axis. The equation below, which is illustrated toward the right, uses the cosine function to represent a plane wave travelling in the positive x direction.
In the above equation:
Other formalisms which directly use the wave’s wavelength formula_14, period formula_15, frequency formula_16 and velocity formula_17 are below.
To appreciate the equivalence of the above set of equations note that formula_21 and formula_22
Arbitrary direction.
A more generalized form is used to describe a plane wave traveling in an arbitrary direction. It uses vectors in combination with the vector dot product.
here:
Complex exponential form.
Many choose to use a more mathematically versatile formulation that utilizes the complex number plane. It requires the use of the natural exponent formula_28 and the imaginary number formula_29.
To appreciate this equation’s relationship to the earlier ones, below is this same equation expressed using sines and cosines. Observe that the first term equals the real form of the plane wave just discussed.
The introduced complex form of the plane wave can be simplified by using a complex-valued amplitude formula_33 substitute the real valued amplitude formula_4. <br>
Specifically, since the complex form…
equals
one can absorb the phase factor formula_37 into a complex amplitude by letting formula_38, resulting in the more compact equation
While the complex form has an imaginary component, after the necessary calculations are performed in the complex plane, its real value can be extracted giving a real valued equation representing an actual plane wave.
The main reason one would choose to work with complex exponential form of plane waves is that complex exponentials are often algebraically easier to handle than the trigonometric sines and cosines. Specifically, the angle-addition rules are extremely simple for exponentials. 
Additionally, when using Fourier analysis techniques for waves in a lossy medium, the resulting attenuation is easier to deal with using complex Fourier coefficients. It should be noted however that if a wave is traveling through a lossy medium, the amplitude of the wave is no longer constant, and therefore the wave is strictly speaking no longer a true plane wave.
In quantum mechanics the solutions of the Schrödinger wave equation are by their very nature complex and in the simplest instance take a form identical to the complex plane wave representation above. The imaginary component in that instance however has not been introduced for the purpose of mathematical expediency but is in fact an inherent part of the “wave”.
Applications.
These waves are solutions for a scalar wave equation in a homogeneous medium. For vector wave equations, such as the ones describing electromagnetic radiation or waves in an elastic solid, the solution for a homogeneous medium is similar: the "scalar" amplitude "Ao" is replaced by a constant "vector" Ao. For example, in electromagnetism Ao is typically the vector for the electric field, magnetic field, or vector potential. A transverse wave is one in which the amplitude vector is orthogonal to k, which is the case for electromagnetic waves in an isotropic medium. By contrast, a longitudinal wave is one in which the amplitude vector is parallel to k, such as for acoustic waves in a gas or fluid.
The plane-wave equation works for arbitrary combinations of "ω" and k, but any real physical medium will only allow such waves to propagate for those combinations of "ω" and k that satisfy the dispersion relation of the medium. The dispersion relation is often expressed as a function, "ω"(k). The ratio "ω"/|k| gives the magnitude of the phase velocity and "dω"/"d"k gives the group velocity. For electromagnetism in an isotropic medium with index of refraction "n", the phase velocity is "c"/"n", which equals the group velocity if the index is not frequency-dependent. 
In linear uniform media, a wave solution can be expressed as a superposition of plane waves. This approach is known as the Angular spectrum method. The form of the planewave solution is actually a general consequence of translational symmetry. More generally, for periodic structures having discrete translational symmetry, the solutions take the form of Bloch waves, most famously in crystalline atomic materials but also in photonic crystals and other periodic wave equations. As another generalization, for structures that are only uniform along one direction "x" (such as a waveguide along the "x" direction), the solutions (waveguide modes) are of the form exp["i"("kx"-"ωt")] multiplied by some amplitude function "a"("y","z"). This is a special case of a separable partial differential equation.
Polarized electromagnetic plane waves.
Represented in the first illustration toward the right is a linearly polarized, electromagnetic wave. Because this is a plane wave, each blue vector, indicating the perpendicular displacement from a point on the axis out to the sine wave, represents the magnitude and direction of the electric field for an entire plane that is perpendicular to the axis.
Represented in the second illustration is a circularly polarized, electromagnetic plane wave. Each blue vector indicating the perpendicular displacement from a point on the axis out to the helix, also represents the magnitude and direction of the electric field for an entire plane perpendicular to the axis.
In both illustrations, along the axes is a series of shorter blue vectors which are scaled down versions of the longer blue vectors. These shorter blue vectors are extrapolated out into the block of black vectors which fill a volume of space. Notice that for a given plane, the black vectors are identical, indicating that the magnitude and direction of the electric field is constant along that plane.
In the case of the linearly polarized light, the field strength from plane to plane varies from a maximum in one direction, down to zero, and then back up to a maximum in the opposite direction.
In the case of the circularly polarized light, the field strength remains constant from plane to plane but its direction steadily changes in a rotary type manner.
Not indicated in either illustration is the electric field’s corresponding magnetic field which is proportional in strength to the electric field at each point in space but is at a right angle to it. Illustrations of the magnetic field vectors would be virtually identical to these except all the vectors would be rotated 90 degrees about the axis of propagation so that they were perpendicular to both the direction of propagation and the electric field vector.
The ratio of the amplitudes of the electric and magnetic field components of a plane wave in free space is known as the free-space wave-impedance, equal to 376.730313 ohms.

</doc>
<doc id="41560" url="http://en.wikipedia.org/wiki?curid=41560" title="Plastic-clad silica fiber">
Plastic-clad silica fiber

In telecommunications and fiber optics, a plastic-clad silica fiber or polymer-clad silica fiber (PCS) is an optical fiber that has a silica-based core and a plastic cladding. The cladding of a PCS fiber should not be confused with the polymer overcoat of a conventional all-silica fiber. PCS fibers in general have significantly lower performance characteristics, particularly higher transmission losses and lower bandwidths, than all-glass fibers.
The main applications of plastic-clad silica fiber are industrial, medical or sensing applications where cores that are larger than those used in standard data communications fibers are advantageous.

</doc>
<doc id="41562" url="http://en.wikipedia.org/wiki?curid=41562" title="Ingrid Bergman">
Ingrid Bergman

Ingrid Bergman (29 August 1915 – 29 August 1982) was a Swedish actress who starred in a variety of European and American films. She won three Academy Awards, two Emmy Awards, four Golden Globe Awards and the Tony Award for Best Actress. She is ranked as the fourth greatest female star of American cinema of all time by the American Film Institute. She is best remembered for her roles as Ilsa Lund in "Casablanca" (1942), and as Alicia Huberman in "Notorious" (1946), an Alfred Hitchcock thriller co-starring Cary Grant.
Before becoming a star in American films, she had been a leading actress in Swedish films. Her first introduction to U.S. audiences came with her starring role in the English-language remake of "Intermezzo" in 1939. In the United States, she brought to the screen a "Nordic freshness and vitality", along with exceptional beauty and intelligence, and according to the "St. James Encyclopedia of Popular Culture", she quickly became "the ideal of American womanhood" and one of Hollywood's greatest leading actresses.
After her performance in Victor Fleming's remake of "Dr. Jekyll and Mr. Hyde" in 1941, she was noticed by her future producer David O. Selznick, who called her "the most completely conscientious actress" he had ever worked with. He started her with a one-film role at her insistence, then signed a four-film contract (also at her insistence) rather than the typical seven-year acting contracts typically signed with foreign actors at that time, thereby supporting her continued success. A few of her other starring roles, besides "Casablanca", included "For Whom the Bell Tolls" (1943), "Gaslight" (1944), "The Bells of St. Mary's" (1945), Alfred Hitchcock's "Spellbound" (1945), "Notorious" (1946), and "Under Capricorn" (1949), and the independent production "Joan of Arc" (1948).
In 1950, after a decade of stardom in American films, she starred in the Italian film "Stromboli", which led to a love affair with director Roberto Rossellini while they were both already married. The affair and then marriage with Rossellini created a scandal in the US that forced her to remain in Europe until 1956, when she made a successful Hollywood return in "Anastasia", for which she won her second Academy Award, as well as the forgiveness of her US fans. Many of her personal and film documents can be seen in the Wesleyan University Cinema Archives.
Early years: 1915–38.
Bergman, named after Princess Ingrid of Sweden, was born on 29 August 1915 in Stockholm, to a Swedish father, Justus Bergman, and his German wife, Frieda (née Adler) Bergman. When she was two years of age, her mother died. Her father, who was an artist and photographer, died when she was 13. In the years before he died, he wanted her to become an opera star, and had her take voice lessons for three years. But she always "knew from the beginning that she wanted to be an actress," sometimes wearing her mother's clothes and staging plays in her father's empty studio. Her father documented all her birthdays with a borrowed camera.
After his death, she was sent to live with an aunt, who died of heart disease only six months later. She then moved in with her Aunt Hulda and Uncle Otto, who had five children. Another aunt she visited, Elsa Adler, first told Ingrid, when she was 11, that her mother may have "some Jewish blood," and that her father was aware of that fact long before they married. But her aunt also cautioned her about telling others about her possible ancestry as "there might be some difficult times coming.":294
At 17, in 1932, Bergman was allowed only one chance to become an actress by entering an acting competition with the Royal Dramatic Theatre in Stockholm.:30–31 Bergman recalls her feelings during that competition: As I walked off the stage, I was in mourning, I was at a funeral. My own. It was the death of my creative self. My heart had truly broken... they didn't think I was even worth listening to, or watching.
Her impression was wrong, as she later met one of the judges who described how the others viewed her performance: We loved your security and your impertinence. We loved you and told each other that there was no reason to waste time as there were dozens of other entrants still to come. We didn't need to waste any time with you. We knew you were a natural and great. Your future as an actress was settled.:31–33
As a result she received a scholarship to the state-sponsored Royal Dramatic Theatre School, where Greta Garbo had years earlier earned a similar scholarship. After several months she was given a part in a new play, "Ett Brott" ("A Crime"), written by Sigfrid Siwertz. Chandler notes that this was "totally against procedure" at the school, where girls were expected to complete three years of study before getting such acting roles.:33
During her first summer break, she was also hired by a Swedish film studio, which led to her leaving the Royal Dramatic Theatre after just one year, to work in films full-time. Her first film role after leaving the Royal Dramatic Theatre was a small part in 1935's "Munkbrogreven" (although she had previously been an extra in the 1932 film "Landskamp"). She went on to act in a dozen films in Sweden, including "En kvinnas ansikte," which was later remade as "A Woman's Face" with Joan Crawford, and one film in Germany, "Die vier Gesellen" ("The Four Companions") (1938).
Hollywood period: 1939–49.
"Intermezzo: A Love Story" (1939).
Bergman's first acting role in the United States came when Hollywood producer David O. Selznick brought her to America to star in "Intermezzo: A Love Story", an English language remake of her 1936 Swedish film, "Intermezzo." Unable to speak English and uncertain about her acceptance by the American audience, she expected to complete this one film and return home to Sweden. Her husband, Dr. Petter Lindström, remained in Sweden with their daughter Pia (born 1938).:63 In "Intermezzo", she played the role of a young piano accompanist opposite Leslie Howard as a famous violin virtuoso. She arrived in Los Angeles on 6 May 1939, and stayed at the Selznick home until she could find another residence. According to Selznick's son, Danny, who was a child at the time, his father had a few concerns about Ingrid: "She didn't speak English, she was too tall, her name sounded too German, and her eyebrows were too thick." 
Bergman was soon accepted without having to modify her looks or name, despite some early suggestions by Selznick.:6 "He let her have her way," notes a story in "Life Magazine." Selznick understood her fear of Hollywood make-up artists, who might turn her into someone she wouldn't recognize, and "instructed them to lay off." He was also aware that her natural good looks would compete successfully with Hollywood's "synthetic razzle-dazzle." During the following weeks, while "Intermezzo" was being filmed, Selznick was also filming "Gone with the Wind." In a letter to William Hebert, his publicity director, Selznick described a few of his early impressions of Bergman:
Miss Bergman is the most completely conscientious actress with whom I have ever worked, in that she thinks of absolutely nothing but her work before and during the time she is doing a picture ... She practically never leaves the studio, and even suggested that her dressing room be equipped so that she could live here during the picture. She never for a minute suggests quitting at six o'clock or anything of the kind ... Because of having four stars acting in "Gone with the Wind," our star dressing-room suites were all occupied and we had to assign her a smaller suite. She went into ecstasies over it and said she had never had such a suite in her life ... All of this is completely unaffected and completely unique and I should think would make a grand angle of approach to her publicity ... so that her natural sweetness and consideration and conscientiousness become something of a legend ... and is completely in keeping with the fresh and pure personality and appearance which caused me to sign her.:135–136
"Intermezzo" became an enormous success and as a result Bergman became a star. The film's director, Gregory Ratoff, said "She is sensational," as an actress. This was the "sentiment of the entire set," writes "Life," adding that workmen would go out of their way to do things for her, and the cast and crew "admired the quick, alert concentration she gave to direction and to her lines." Film historian David Thomson notes that this would become "the start of an astonishing impact on Hollywood and America" where her lack of makeup contributed to an "air of nobility." According to "Life," the impression that she left on Hollywood, after she returned to Sweden, was of a tall (5 ft. 9 in.) girl "with light brown hair and blue eyes who was painfully shy but friendly, with a warm, straight, quick smile." Selznick appreciated her uniqueness, and with his wife Irene, they remained important friends throughout her career.:76
"Casablanca" (1942).
After the onset of World War II, Bergman "felt guilty because she had so misjudged the situation in Germany" while she was there filming "Die vier Gesellen" (The Four Companions). According to one of her biographers, Charlotte Chandler (2007), she had at first considered the Nazis only a "temporary aberration, 'too foolish to be taken seriously.' She believed Germany would not start a war." Bergman felt that "The good people there would not permit it." Chandler adds, "Ingrid felt guilty all the rest of her life because when she was in Germany at the end of the war, she had been afraid to go with the others to witness the atrocities of the Nazi extermination camps.":293–295
After completing one last film in Sweden and appearing in three moderately successful films ("Adam Had Four Sons", "Rage in Heaven" and "Dr. Jekyll and Mr. Hyde", all in 1941) in the United States, Bergman co-starred with Humphrey Bogart in the 1942 classic film "Casablanca", which remains her best-known role. In this film, she played the role of Ilsa, the beautiful Norwegian wife of Victor Laszlo, played by Paul Henreid, an "anti-Nazi underground hero" who is in Casablanca, a safe-haven from the Nazis. Bergman did not consider "Casablanca" to be one of her favorite performances. "I made so many films which were more important, but the only one people ever want to talk about is that one with Bogart." In later years she stated, "I feel about "Casablanca" that it has a life of its own. There is something mystical about it. It seems to have filled a need, a need that was there before the film, a need that the film filled.":88
"For Whom the Bell Tolls" (1943).
After "Casablanca", with "Selznick's steady boosting," she played the part of Maria in "For Whom the Bell Tolls" (1943), which was also her first color film. For the role she received her first Academy Award nomination for Best Actress. The film was taken from Ernest Hemingway's novel of the same title. When the book was sold to Paramount Pictures, Hemingway stated that "Miss Bergman, and no one else should play the part." His opinion came from seeing her in her first American role, "Intermezzo," although he hadn't yet met her. A few weeks later, they did meet, and after studying her he said, "You "are" Maria!"
"Gaslight" (1944).
The following year, she won the Academy Award for Best Actress for "Gaslight" (1944), a film in which George Cukor directed her as a "wife driven close to madness" by co-star Charles Boyer. The film, according to Thomson, "was the peak of her Hollywood glory.":77 Bergman next played a nun in "The Bells of St. Mary's" (1945) opposite Bing Crosby, for which she received her third consecutive nomination for Best Actress.
Hitchcock films.
Bergman starred in the Alfred Hitchcock films "Spellbound" (1945), "Notorious" (1946), and "Under Capricorn" (1949). "Under Capricorn," the only one of the three made in color, was a costume drama which has never received the acclaim that the other films that Bergman made with Hitchcock. Bergman was a student of the acting coach Michael Chekhov during the 1940s. Coincidentally, it was for his role in "Spellbound" that Chekhov received his only Academy Award nomination.
"Joan of Arc" (1948).
Bergman received another Best Actress nomination for "Joan of Arc" (1948), an independent film based on the Maxwell Anderson play "Joan of Lorraine", produced by Walter Wanger, and initially released through RKO. Bergman had championed the role since her arrival in Hollywood, which was one of the reasons she had played it on the Broadway stage in Anderson's play. The film was not a big hit with the public, partly because of the scandal of Bergman's affair with Italian film director Roberto Rossellini, which broke while the film was still in theatres. Even worse, it received disastrous reviews, and although nominated for several Academy Awards, did not receive a Best Picture nomination. It was subsequently shorn of 45 minutes. It was not until it was restored to full length in 1998 and released in 2004 on DVD that later audiences could see it as it was intended to be shown.
Between motion pictures, Bergman had appeared in the stage plays "Liliom", "Anna Christie", and "Joan of Lorraine". During a press conference in Washington, D.C. for the promotion of "Joan of Lorraine", she protested against racial segregation after seeing it first hand at the theater she was acting in. This led to a lot of publicity and some hate mail. Bergman went to Alaska during World War II to entertain US Army troops. Soon after the war ended, she also went to Europe for the same purpose, where she was able to see the devastation caused by the war. 
Personal life.
In 1937, at the age of 21, Bergman married dentist Petter Aron Lindström (later to become a neurosurgeon), and they had a daughter, Friedel Pia Lindström (born 20 September 1938). After returning to the United States in 1940, she acted on Broadway before continuing to do films in Hollywood. The following year, her husband arrived from Sweden with daughter Pia. Lindström stayed in Rochester, New York, where he studied medicine and surgery at the University of Rochester. Bergman would travel to New York and stay at their small rented stucco house between films, her visits lasting from a few days to four months.
According to an article in "Life Magazine", the "doctor regards himself as the undisputed head of the family, an idea that Ingrid accepts cheerfully." He insisted she draw the line between her film and personal life, as he has a "professional dislike for being associated with the tinseled glamor of Hollywood." Lindström later moved to San Francisco, California, where he completed his internship at a private hospital, and they continued to spend time together when she could travel between filming.
Bergman returned to Europe after the scandalous publicity surrounding her affair with Italian director Roberto Rossellini during the filming of "Stromboli" in 1950. In the same month the film was released, she gave birth to a boy, Robertino Rossellini (born 2 February 1950). A week after her son was born, she divorced Lindström and married Rossellini in Mexico. On 18 June 1952 she gave birth to the twin daughters Isotta Ingrid Rossellini and Isabella Rossellini. In 1957 she divorced Rossellini. The next year she married Lars Schmidt, a theatrical entrepreneur from a wealthy Swedish shipping family. That marriage lasted nearly two decades, until 1975 when they divorced. 
During her marriage with Lindstrom, Bergman had a brief affair with "Spellbound" costar Gregory Peck. Unlike the affair with Rossellini, that with Peck was kept private until he confessed it to Brad Darrach of "People" in an interview five years after Bergman's death. Peck said, “All I can say is that I had a real love for her (Bergman), and I think that’s where I ought to stop…. I was young. She was young. We were involved for weeks in close and intense work.”
Italian period with Rossellini: 1949–57.
Bergman strongly admired two films by Italian director Roberto Rossellini that she had seen in the United States. In 1949, Bergman wrote to Rossellini, expressing this admiration and suggesting that she make a film with him. This led to her being cast in his film "Stromboli" (1950). During production, Bergman fell in love with Rossellini, and they began an affair. Bergman became pregnant with their son, Renato Roberto Ranaldo Giusto Giuseppe ("Robin") Rossellini (born 2 February 1950).:18
This affair caused a huge scandal in the United States, where it led to Bergman being denounced on the floor of the United States Senate. Ed Sullivan chose not to have her on his show, despite a poll indicating that the public wanted her to appear. However, Steve Allen, whose show was equally popular, did have her on, later explaining "the danger of trying to judge artistic activity through the prism of one's personal life." Spoto notes that Bergman had, by virtue of her roles and screen persona, placed herself "above all that". She had played a nun in "The Bells of St. Mary's" (1945) and a virgin saint in "Joan of Arc" (1948). Bergman later said, "People saw me in "Joan of Arc" and declared me a saint. I'm not. I'm just a woman, another human being.":300
As a result of the scandal, Bergman returned to Italy, leaving her husband and daughter (Pia). She went through a publicized divorce and custody battle for their daughter. Bergman and Rossellini were married on 24 May 1950. In addition to Renato, they had twin daughters (born 18 June 1952): Isabella Rossellini, who became an actress and model, and Isotta Ingrid Rossellini, who became a professor of Italian literature. 
"Stromboli" and "neorealism".
Rossellini completed five films starring Bergman between 1949 and 1955: "Stromboli," "Europa '51," "Viaggio in Italia," "Giovanna d'Arco al rogo," and "La Paura" ("Fear").
Rossellini directed her in a brief segment of his 1953 documentary film, "Siamo donne" (We, the Women), which was devoted to film actresses.:18 His biographer Peter Bondanella notes that problems with communication during their marriage may have inspired his films' central themes of "solitude, grace and spirituality in a world without moral values.":19
Rossellini's use of a Hollywood star in his typically "neorealist" films, in which he normally used non-professional actors, did provoke some negative reactions in certain circles. In Bergman's first film with Rossellini, her character was "defying audience expectations" in that the director preferred to work without a script, forcing Bergman to act "inspired by reality while she worked, a style which Bondanella calls 'a new cinema of psychological introspection'".:98 Bergman was aware of Rossellini's directing style before filming, as the director had earlier written to her explaining that he worked from "a few basic ideas, developing them little by little" as a film progressed.:19
After separating from Rossellini, Bergman starred in Jean Renoir's "Elena and Her Men" ("Elena et les Hommes," 1956), a romantic comedy in which she played a Polish princess caught up in political intrigue. Although the film wasn't a success, it has since come to be regarded as one of her best performances.
Later years: 1957–82.
"Anastasia" (1956).
With her starring role in 1956's "Anastasia", Bergman made a triumphant return to the American screen and won the Academy Award for Best Actress for a second time. The award was accepted for her by her friend Cary Grant.
Bergman made her first post-scandal public appearance in Hollywood in the 1958 Academy Awards, when she was the presenter of the Academy Award for Best Picture. She was given a standing ovation, after being introduced by Cary Grant and walking out onto the stage to present the award. She continued to alternate between performances in American and European films for the rest of her career and also made occasional appearances in television dramas such as a 1959 production of "The Turn of the Screw" for the "Ford Startime" TV series—for which she won the Emmy Award for Outstanding Single Performance by an Actress.
During this time, she performed in several stage plays. She married producer Lars Schmidt, a fellow Swede, on 21 December 1958. This marriage ended in divorce in 1975. Schmidt died on 18 October 2009. After a long hiatus, Bergman made the film "Cactus Flower" in 1969, with Walter Matthau and Goldie Hawn.
In 1972, U.S. Senator Charles H. Percy entered an apology into the "Congressional Record" for the attack made on Bergman 22 years earlier by Edwin C. Johnson.
Bergman was the President of the Jury at the 1973 Cannes Film Festival.
"Murder on the Orient Express" (1974).
Bergman became one of the few actresses ever to receive three Oscars when she won her third (and first in the category of Best Supporting Actress) for her performance in "Murder on the Orient Express" (1974). Director Sidney Lumet offered Bergman the important part of Princess Dragomiroff, with which he felt she could win an Oscar. She insisted on playing the much smaller role of Greta Ohlsson, the old Swedish missionary. Lumet discussed Bergman's role:
Bergman could speak Swedish (her native language), German (her second language, learned from her German mother and in school), English (learned when brought over to the United States), Italian (learned while living in Italy) and French (her third language, learned in school). She acted in each of these languages at various times. Fellow actor John Gielgud, who had acted with her in "Murder on the Orient Express" and who had directed her in the play "The Constant Wife", playfully commented: "She speaks five languages and can't act in any of them."
Although known chiefly as a film star, Bergman strongly admired the great English stage actors and their craft. She had the opportunity to appear in London's West End, working with such stage stars as Michael Redgrave in "A Month in the Country" (1965), Sir John Gielgud in "The Constant Wife" (1973) and Wendy Hiller in "Waters of the Moon" (1977–78). 
"Autumn Sonata" (1978).
In 1978, Bergman played in Ingmar Bergman's "Autumn Sonata" ("Höstsonaten") for which she received her 7th Academy Award nomination. This was her final performance on the big screen. In the film, Bergman plays a celebrity pianist who travels to Norway to visit her neglected daughter, played by Liv Ullmann. The film was shot in Norway.
In 1979, Bergman hosted the AFI's Life Achievement Award Ceremony for Alfred Hitchcock.
"A Woman Called Golda" (1982) – her final role.
In 1982 she was offered the starring role in a television mini-series, "A Woman Called Golda", about the late Israeli prime minister Golda Meir. It was to be her final acting role and she was honored posthumously with a second Emmy Award for Best Actress. Her daughter, Isabella, described Ingrid's surprise at being offered the part and the producer trying to explain to her, "People believe you and trust you, and this is what I want, because Golda Meir had the trust of the people." Isabella adds, "Now "that" was interesting to Mother." She was also persuaded that Golda was a "grand-scale person," one that people would assume was much taller than she actually was. Chandler notes that the role "also had a special significance for her, as during World War II, Ingrid felt guilty because she had so misjudged the situation in Germany.":293
According to Chandler, "Ingrid's rapidly deteriorating health was a more serious problem. Insurance for Ingrid was impossible. Not only did she have cancer, but it was spreading, and if anyone had known how bad it was, no one would have gone on with the project." After viewing the series on TV, Isabella commented,
She never showed herself like that in life. In life, Mum showed courage. She was always a little vulnerable, courageous, but vulnerable. Mother had a sort of presence, like Golda, I was surprised to see it ... When I saw her performance, I saw a mother that I'd never seen before—this woman with balls.:290
Bergman was frequently ill during the filming although she rarely complained or showed it. Four months after the filming was completed, she died. After her death her daughter Pia accepted her Emmy.:296
Death and legacy.
Bergman died in 1982 on her 67th birthday in London, of breast cancer. Her body was cremated at Kensal Green Cemetery, London, and her ashes taken to Sweden. Most of them were scattered in the sea around the islet of Dannholmen off the fishing village of Fjällbacka in Bohuslän, on the west coast of Sweden, where she spent most of the summers from 1958 until her death in 1982. The rest were placed next to her parents' ashes in Norra Begravningsplatsen (Northern Cemetery), Stockholm, Sweden.
According to biographer Donald Spoto, she was "arguably the most international star in the history of entertainment." Acting in five languages, she was seen on stage, screen and television, and won three Academy Awards plus many others. After her American film debut in the 1939 film "Intermezzo: A Love Story", co-starring Leslie Howard, Hollywood saw her as a unique actress who was completely natural in style and without need of makeup. Film critic James Agee wrote that she "not only bears a startling resemblance to an imaginable human being; she really knows how to act, in a blend of poetic grace with quiet realism."
According to film historian David Thomson, she "always strove to be a 'true' woman", and many filmgoers identified with her:
There was a time in the early and mid-1940s when Bergman commanded a kind of love in America that has been hardly ever matched. In turn, it was the strength of that affection that animated the "scandal" when she behaved like an impetuous and ambitious actress instead of a saint.:76
Writing about her first years in Hollywood, "Life" magazine stated that "All Bergman vehicles are blessed," and "they all go speedily and happily, with no temperament from the leading lady." She was "completely pleased" with her early career's management by David O. Selznick, who always found excellent dramatic roles for her to play, and equally satisfied with her salary, once saying, "I am an actress and I am interested in acting, not in making money." "Life" adds that "she has greater versatility than any actress on the American screen ... her roles have demanded an adaptability and sensitiveness of characterization to which few actresses could rise."
She continued her acting career while suffering from cancer for eight years, and won international honors for her final roles. "Her spirit triumphed with remarkable grace and courage," adds Spoto. Director George Cukor once summed up her contributions to the film media when he said to her, "Do you know what I especially love about you, Ingrid, my dear? I can sum it up as your naturalness. The camera loves your beauty, your acting, and your individuality. A star must have individuality. It makes you a great star. A great star.":11
For her contributions to the motion picture industry, Bergman has a star on the Hollywood Walk of Fame at 6759 Hollywood Blvd.
Woody Guthrie wrote the erotic song "Ingrid Bergman", which references Bergman's relationship with Roberto Rosselini on the film "Stromboli". It was never recorded by Guthrie but, when later found in the Woody Guthrie archives, it was set to music, and recorded, by Billy Bragg on the album "Mermaid Avenue".
In March 2015, a picture of Bergman photographed by David Seymour was chosen for the main poster for the 2015 Cannes Film Festival.
Autobiography.
In 1980, Bergman's autobiography was published under the title "Ingrid Bergman: My Story." It was written with the help of Alan Burgess, and in it she discusses her childhood, her early career, her life during her time in Hollywood, the Rossellini scandal, and subsequent events. The book was written after her children warned her that she would only be known through rumors and interviews if she did not tell her own story. It was through this autobiography that her affair with Robert Capa became known.
Awards.
Bergman won three Academy Awards for acting, two for Best Actress and one for Best Supporting Actress. She ranks in equal second place in terms of Oscars won, with Walter Brennan (all three for Best Supporting Actor), Jack Nicholson (two for Best Actor and one for Best Supporting Actor), Meryl Streep (two for Best Actress and one for Best Supporting Actress), and Daniel Day-Lewis (all three for Best Actor). Katharine Hepburn still leads the record with four (all four for Best Actress).

</doc>
<doc id="41563" url="http://en.wikipedia.org/wiki?curid=41563" title="Polarential telegraph system">
Polarential telegraph system

A polarential telegraph system is a direct-current telegraph system employing polar transmission in one direction and a form of differential duplex transmission in the other. 
Two types of polarential systems, known as types A and B, are in use. In half-duplex operation of a type A polarential system, the direct-current balance is independent of line resistance. In half-duplex operation of a type B polarential system, the direct current is substantially independent of the line leakage. Type A is better for cable loops where leakage is negligible but resistance varies with temperature. Type B is considered better for open wire where variable line leakage is frequent.

</doc>
