<doc id="40773" url="http://en.wikipedia.org/wiki?curid=40773" title="Baseband">
Baseband

A signal is baseband if it has a very narrow frequency range, i.e. a spectral magnitude that is nonzero only for frequencies in the vicinity of the origin (termed "f" = 0) and negligible elsewhere. In telecommunications and signal processing, baseband signals are transmitted without modulation, that is, without any shift in the range of frequencies of the signal, and are low frequency - contained within the band of frequencies from close to 0 hertz up to a higher cut-off frequency or maximum bandwidth. Baseband can be synonymous with lowpass or non-modulated, and is differentiated from passband, bandpass, carrier-modulated, intermediate frequency, or radio frequency (RF).
Various uses.
Baseband bandwidth.
A "baseband bandwidth" is equal to the highest frequency of a signal or system, or an upper bound on such frequencies, for example the upper cut-off frequency of a Lowpass filter. By contrast, passband bandwidth is the difference between a highest frequency and a nonzero lowest frequency.
Baseband channel.
A "baseband channel" or "lowpass channel" (or "system", or "network") is a communication channel that can transfer frequencies that are very near zero. Examples are serial cables and local area networks (LANs), as opposed to passband channels such as radio frequency channels and passband filtered wires of the analog telephone network. Frequency division multiplexing (FDM) allows an analog telephone wire to carry a baseband telephone call, concurrently as one or several carrier-modulated telephone calls.
Digital baseband transmission.
Digital baseband transmission, also known as line coding, aims at transferring a digital bit stream over baseband channel, typically an unfiltered wire, contrary to passband transmission, also known as "carrier-modulated" transmission. Passband transmission makes communication possible over a bandpass filtered channel, such as the telephone network local-loop or a band-limited wireless channel.
Baseband transmission in Ethernet.
The word "BASE" in Ethernet physical layer standards, for example 10BASE5, 100BASE-T and 1000BASE-SX, implies baseband digital transmission (i.e. that a line code and an unfiltered wire are used).
Baseband processor.
A baseband processor is a chip in wireless transmission devices such as mobile phones that performs signal processing and implements the device's realtime radio transmission operations.
Baseband signal.
A "baseband signal" or "lowpass signal" is a signal that can include frequencies that are very near zero, by comparison with its highest frequency (for example, a sound waveform can be considered as a baseband signal, whereas a radio signal or any other modulated signal is not).
Equivalent baseband signal.
An "equivalent baseband signal" or "equivalent lowpass signal" is—in analog and digital modulation methods with constant carrier frequency (for example ASK, PSK and QAM, but not FSK)—a complex valued representation of the modulated physical signal (the so-called passband signal or RF signal). The equivalent baseband signal is formula_1 where formula_2 is the inphase signal, formula_3 the quadrature phase signal, and formula_4 the imaginary unit. In a digital modulation method, the formula_2 and formula_3 signals of each modulation symbol are evident from the constellation diagram. The frequency spectrum of this signal includes negative as well as positive frequencies. The physical passband signal corresponds to
Modulation.
A signal at baseband is often used to modulate a higher frequency carrier wave in order that it may be transmitted via radio. Modulation results in shifting the signal up to much higher frequencies (radio frequencies, or RF) than it originally spanned. A key consequence of the usual double-sideband amplitude modulation (AM) is that the range of frequencies the signal spans (its spectral bandwidth) is doubled. Thus, the RF bandwidth of a signal (measured from the lowest frequency as opposed to 0 Hz) is twice its baseband bandwidth. Steps may be taken to reduce this effect, such as single-sideband modulation. Some transmission schemes such as frequency modulation use even more bandwidth.
The figure shows what happens with AM modulation:

</doc>
<doc id="40775" url="http://en.wikipedia.org/wiki?curid=40775" title="Basic exchange telecommunications radio service">
Basic exchange telecommunications radio service

In telecommunication, a basic exchange telecommunications radio service (BETRS) is a commercial service that can extend telephone service to rural areas by replacing the local loop with radio communications. In the BETRS, non-government ultra high frequency (UHF) and very high frequency (VHF) common carrier and the private radio service frequencies are shared.
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40776" url="http://en.wikipedia.org/wiki?curid=40776" title="Basic service">
Basic service

In telecommunications, basic service is: 
1. A pure transmission capability over a communication path that is virtually transparent in terms of its interaction with customer-supplied information. 
2. The offering of transmission capacity between two or more points suitable for a user's transmission needs and subject only to the technical parameters of fidelity and distortion criteria, or other conditioning.

</doc>
<doc id="40777" url="http://en.wikipedia.org/wiki?curid=40777" title="Basic service element">
Basic service element

In telecommunication, a basic service element (BSE) is: 
BSEs constitute optional capabilities to which the customer may subscribe or decline to subscribe.

</doc>
<doc id="40778" url="http://en.wikipedia.org/wiki?curid=40778" title="Basic serving arrangement">
Basic serving arrangement

In telecommunication, the term basic serving arrangement (BSA) has the following meanings: 

</doc>
<doc id="40779" url="http://en.wikipedia.org/wiki?curid=40779" title="BCH code">
BCH code

In coding theory, the BCH codes form a class of cyclic error-correcting codes that are constructed using finite fields. BCH codes were invented in 1959 by French mathematician Alexis Hocquenghem, and independently in 1960 by Raj Bose and D. K. Ray-Chaudhuri. The acronym "BCH" comprises the initials of these inventors' names.
One of the key features of BCH codes is that during code design, there is a precise control over the number of symbol errors correctable by the code. In particular, it is possible to design binary BCH codes that can correct multiple bit errors. Another advantage of BCH codes is the ease with which they can be decoded, namely, via an algebraic method known as syndrome decoding. This simplifies the design of the decoder for these codes, using small low-power electronic hardware.
BCH codes are used in applications such as satellite communications, compact disc players, DVDs, disk drives, solid-state drives and two-dimensional bar codes.
Definition and illustration.
Primitive narrow-sense BCH codes.
Given a prime power q and positive integers m and d with "d" ≤ "q""m" − 1, a primitive narrow-sense BCH code over the finite field GF("q") with code length and distance at least d is constructed by the following method.
Let α be a primitive element of GF("q""m").
For any positive integer i, let "m""i"("x") be the minimal polynomial of α"i" over GF("q").
The generator polynomial of the BCH code is defined as the least common multiple .
It can be seen that "g"("x") is a polynomial with coefficients in GF("q") and divides "x""n" − 1.
Therefore, the polynomial code defined by "g"("x") is a cyclic code.
Example.
Let and (therefore ). We will consider different values of d. There is a primitive root α in GF(16) satisfying
its minimal polynomial over GF(2) is
The minimal polynomials of the first seven powers of α are
The BCH code with formula_6 has generator polynomial
formula_7
It has minimal Hamming distance at least 3 and corrects up to one error. Since the generator polynomial is of degree 4, this code has 11 data bits and 4 checksum bits.
The BCH code with formula_8 has generator polynomial
formula_9
It has minimal Hamming distance at least 5 and corrects up to two errors. Since the generator polynomial is of degree 8, this code has 7 data bits and 8 checksum bits.
The BCH code with formula_10 and higher has generator polynomial
formula_11
This code has minimal Hamming distance 15 and corrects 7 errors. It has 1 data bit and 14 checksum bits. In fact, this code has only two codewords: 000000000000000 and 111111111111111.
General BCH codes.
General BCH codes differ from primitive narrow-sense BCH codes in two respects.
First, the requirement that formula_12 be a primitive element of formula_13 can be relaxed. By relaxing this requirement, the code length changes from formula_14 to formula_15 the order of the element formula_16
Second, the consecutive roots of the generator polynomial may run from formula_17 instead of formula_18
Definition. Fix a finite field formula_19 where formula_20 is a prime power. Choose positive integers formula_21 such that formula_22 formula_23 and formula_24 is the multiplicative order of formula_20 modulo formula_26
As before, let formula_12 be a primitive formula_28th root of unity in formula_29 and let formula_30 be the minimal polynomial over formula_31 of formula_32 for all formula_33
The generator polynomial of the BCH code is defined as the least common multiple formula_34
Note: if formula_35 as in the simplified definition, then formula_36 is automatically 1, and the order of formula_20 modulo formula_28 is automatically formula_39
Therefore, the simplified definition is indeed a special case of the general one.
Special cases.
The generator polynomial formula_42 of a BCH code has coefficients from formula_43
In general, a cyclic code over formula_44 with formula_42 as the generator polynomial is called a BCH code over formula_46
The BCH code over formula_13 with formula_42 as the generator polynomial is called a Reed–Solomon code. In other words, a Reed–Solomon code is a BCH code where the decoder alphabet is the same as the channel alphabet.
Properties.
1. The generator polynomial of a BCH code has degree at most formula_49
Moreover, if formula_50 and formula_51 the generator polynomial has degree at most formula_52
Therefore, the least common multiple of formula_55 of them has degree at most formula_49
Moreover, if formula_57 then formula_58 for all formula_33
Therefore, formula_42 is the least common multiple of at most formula_61 minimal polynomials formula_30 for odd indices formula_63 each of degree at most formula_39
2. A BCH code has minimal Hamming distance at least formula_65
Proof: Suppose that formula_66 is a code word with fewer than formula_67 non-zero terms. Then
Recall that formula_17 are roots of formula_70 hence of formula_71
This implies that formula_72 satisfy the following equations, for formula_73
In matrix form, we have
The determinant of this matrix equals
The matrix formula_77 is seen to be a Vandermonde matrix, and its determinant is
which is non-zero. It therefore follows that formula_79 hence formula_80
3. A BCH code is cyclic.
Proof: A polynomial code of length formula_28 is cyclic if and only if its generator polynomial divides formula_82
Since formula_42 is the minimal polynomial with roots formula_84 it suffices to check that each of formula_17 is a root of formula_82
This follows immediately from the fact that formula_12 is, by definition, an formula_28th root of unity.
Decoding.
There are many algorithms for decoding BCH codes. The most common ones follow this general outline:
During some of these steps, the decoding algorithm may determine that the received vector has too many errors and cannot be corrected. For example, if an appropriate value of "t" is not found, then the correction would fail. In a truncated (not primitive) code, an error location may be out of range. If the received vector has more errors than the code can correct, the decoder may unknowingly produce an apparently valid message that is not the one that was sent.
Calculate the syndromes.
The received vector formula_89 is the sum of the correct codeword formula_90 and an unknown error vector formula_91
The syndrome values are formed by considering formula_89 as a polynomial and evaluating it at formula_93
Thus the syndromes are
for formula_95 to formula_96
Since formula_97 are the zeros of formula_70 of which
formula_99 is a multiple, formula_100
Examining the syndrome values thus isolates the error vector so one can begin to solve for it.
If there is no error, formula_101 for all formula_102
If the syndromes are all zero, then the decoding is done.
Calculate the error location polynomial.
If there are nonzero syndromes, then there are errors. The decoder needs to figure out how many errors and the location of those errors.
If there is a single error, write this as formula_103
where formula_104 is the location of the error and formula_105 is its magnitude. Then the first two syndromes are
so together they allow us to calculate formula_105 and provide some information about formula_104 (completely determining it in the case of Reed–Solomon codes).
If there are two or more errors,
It is not immediately obvious how to begin solving the resulting syndromes for the unknowns formula_111 and formula_112
First step is finding locator polynomial
Two popular algorithms for this task are:
Peterson–Gorenstein–Zierler algorithm.
Peterson's algorithm is the step 2 of the generalized BCH decoding procedure. Peterson's algorithm is used to calculate the error locator polynomial coefficients formula_115 of a polynomial
Now the procedure of the Peterson–Gorenstein–Zierler algorithm. Expect we have at least 2"t" syndromes "s""c"...,"s""c"+2"t"−1.
Let "v" = "t".
 if formula_127
 then
 declare an empty error locator polynomial
 stop Peterson procedure.
 end
 set formula_128
 continue from the beginning of Peterson's decoding by making smaller formula_124
Factor error locator polynomial.
Now that you have the formula_131 polynomial, its roots can be found in the form formula_132 by brute force for example using the Chien search algorithm. The exponential
powers of the primitive element formula_12 will yield the positions where errors occur in the received word; hence the name 'error locator' polynomial.
The zeros of Λ("x") are "α"−"i"1, ..., "α"−"i""v".
Calculate error values.
Once the error locations are known, the next step is to determine the error values at those locations. The error values are then used to correct the received values at those locations to recover the original codeword.
For the case of binary BCH, (with all characters readable) this is trivial; just flip the bits for the received word at these positions, and we have the corrected code word. In the more general case, the error weights formula_134 can be determined by solving the linear system
Forney algorithm.
However, there is a more efficient method known as the Forney algorithm.
Let formula_136
Let formula_137 formula_138 and formula_139
Let formula_140 be the error evaluator polynomial
Let formula_141 where formula_142 denotes here formula_143 rather than multiplying in the field.
Than if syndromes could be explained by an error word, which could be nonzero only on positions formula_144, then error values are
For narrow-sense BCH codes, "c" = 1, so the expression simplifies to:
Explanation of Forney algorithm computation.
It is based on Lagrange interpolation and techniques of generating functions.
Look at formula_147
Let for simplicity formula_148 for formula_149 and formula_150 for formula_151
Then formula_152
We could gain form of polynomial:
We want to compute unknowns formula_156 and we could simplify the context by removing the formula_157 terms. This leads to the error evaluator polynomial
Thanks to formula_159 we have
Look at formula_161
Thanks to formula_121 (the Lagrange interpolation trick) the sum degenerates to only one summand
To get formula_111 we just should get rid of the product. We could compute the product directly from already computed roots formula_165 of formula_166 but we could use simpler form.
As formal derivative formula_167
we get again only one summand in
So finally
This formula is advantageous when one computes the formal derivative of formula_121 form its formula_171 form, gaining
where formula_142 denotes here formula_143 rather than multiplying in the field.
Decoding based on extended Euclidean algorithm.
The process of finding both the polynomial Λ and the error values could be based on the Extended Euclidean algorithm. Correction of unreadable characters could be incorporated to the algorithm easily as well.
Let formula_175 be positions of unreadable characters. One creates polynomial localising these positions formula_176
Set values on unreadable positions to 0 and compute the syndromes.
As we have already defined for the Forney formula let formula_177
Let us run extended Euclidean algorithm for locating least common divisor of polynomials formula_178 and formula_179
The goal is not to find the least common divisor, but a polynomial formula_180 of degree at most formula_181 and polynomials formula_182 such that formula_183
Low degree of formula_180 guarantees, that formula_185 would satisfy extended (by formula_186) defining conditions for formula_187
Defining formula_188 and using formula_189 on the place of formula_131 in the Fourney formula will give us error values.
The main advantage of the algorithm is that it meanwhile computes formula_191 required in the Forney formula.
Explanation of the decoding process.
The goal is to find a codeword which differs from the received word minimally as possible on readable positions.
When expressing the received word as a sum of nearest codeword and error word, we are trying to find error word with minimal number of non-zeros on readable positions.
Syndrom formula_192 restricts error word by condition formula_193
We could write these conditions separately or we could create polynomial formula_194 and compare coefficients near powers formula_195 to formula_196
formula_197
Suppose there is unreadable letter on position formula_198 we could replace set of syndromes formula_199 by set of syndromes formula_200 defined by equation formula_201
Suppose for an error word all restrictions by original set formula_199 of syndromes hold,
than formula_203
New set of syndromes restricts error vector formula_204 the same way the original set of syndromes restricted the error vector formula_205
Note, that except the coordinate formula_198 where formula_207 an formula_208 is zero, iff formula_134 is zero.
For the goal of locating error positions we could change the set of syndromes in the similar way to reflect all unreadable characters.
This shortens the set of syndromes by formula_210
In polynomial formulation, the replacement of syndromes set formula_199 by syndromes set formula_200 leads to formula_213
Therefore formula_214
After replacement of formula_215 by formula_178, one would require equation for coefficients near powers formula_217
One could consider looking for error positions from the point of view of eliminating influence of given positions similarly as for unreadable characters.
If we found formula_218 positions such that eliminating their influence leads to obtaining set of syndromes consisting of all zeros,
than there exists error vector with errors only on these coordinates.
If formula_131 denotes the polynomial eliminating the influence of these coordinates, we obtain formula_220
In Euclidean algorithm, we try to correct at most formula_221 errors (on readable positions), because with bigger error count there could be more codewords in the same distance from the received word.
Therefore, for formula_131 we are looking for, the equation must hold for coefficients near powers starting from formula_223
In Forney formula, formula_131 could be multiplied by a scalar giving the same result.
It could happen that the Euclidean algorithm finds formula_131 of degree higher than formula_226 having number of different roots equal to its degree,
where the Fourney formula would be able to correct errors in all its roots, anyways correcting such many errors could be risky (especially with no other restrictions on received word).
Usually after getting formula_131 of higher degree, we decide not to correct the errors.
Correction could fail in the case formula_131 has roots with higher multiplicity or the number of roots is smaller than its degree.
Fail could be detected as well by Forney formula returning error outside the transmitted alphabet.
Correct the errors.
Using the error values and error location, correct the errors and form a corrected code vector by subtracting error values at error locations.
Decoding examples.
Decoding of binary code without unreadable characters.
Consider a BCH code in GF(24) with formula_229 and formula_230. (This is used in QR codes.) Let the message to be transmitted be [1 1 0 1 1], or in polynomial notation, formula_231
The "checksum" symbols are calculated by dividing formula_232 by formula_42 and taking the remainder, resulting in formula_234 or [ 1 0 0 0 0 1 0 1 0 0 ]. These are appended to the message, so the transmitted codeword is [ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 ].
Now, imagine that there are two bit-errors in the transmission, so the received codeword is [ 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 ]. In polynomial notation:
In order to correct the errors, first calculate the syndromes. Taking formula_236 we have formula_237 formula_238 formula_239 formula_240 formula_241 and formula_242
Next, apply the Peterson procedure by row-reducing the following augmented matrix.
Due to the zero row, "S"3×3 is singular, which is no surprise since only two errors were introduced into the codeword.
However, the upper-left corner of the matrix is identical to ["S"2×2 ], which gives rise to the solution formula_244 formula_245
The resulting error locator polynomial is formula_246 which has zeros at formula_247 and formula_248
The exponents of formula_12 correspond to the error locations.
There is no need to calculate the error values in this example, as the only possible value is 1.
Decoding with unreadable characters.
Suppose the same scenario, but the received word has two unreadable characters [ 1 0 0 ? 1 1 ? 0 0 1 1 0 1 0 0 ].
We replace the unreadable characters by zeros while creating the polynom reflecting their positions formula_250
We compute the syndromes formula_251 formula_252 formula_253
formula_254 formula_255 and formula_256
(Using log notation which is independent on GF(24) isomorphisms.
For computation checking we can use the same representation for addition as was used in previous example.
Hexadecimal description of the powers of formula_12 are consecutively 1,2,4,8,3,6,C,B,5,A,7,E,F,D,9 with the addition based on bitwise xor.)
Let us make syndrome polynomial formula_258
compute
formula_259
Run the extended Euclidean algorithm:
formula_260
formula_261
formula_262
formula_263
formula_264
formula_265
We have reached polynomial of degree at most 3, and as
formula_266
we get
formula_267
Therefore
formula_268
Let formula_269
Don't worry that formula_270
Find by brute force a root of formula_187
The roots are formula_272 and formula_273
(after finding for example formula_274 we can divide formula_121 by corresponding monom formula_276 and the root of resulting monom could be found easily).
Let formula_277
and let formula_278
Let us look for error values using formula formula_279
where formula_165 are roots of formula_281
formula_282
We get formula_283
formula_284
formula_285
formula_286
Fact, that formula_287 should not be surprising.
Corrected code is therefore [ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0].
Decoding with unreadable characters with a small number of errors.
Let us show the algorithm behaviour for the case with small number of errors. Let the received word is [ 1 0 0 ? 1 1 ? 0 0 0 1 0 1 0 0 ].
Again, replace the unreadable characters by zeros while creating the polynom reflecting their positions formula_250
Compute the syndromes
formula_289 formula_290 formula_291
formula_292 formula_293 and formula_294
Create syndrom polynommial
formula_295
and formula_296
Let us run the extended Euclidean algorithm:
formula_297
formula_298
formula_299
formula_300
We have reached polynomial of degree at most 3, and as
we get
Therefore,
Let formula_304 Don't worry that formula_270 The root of formula_131 is formula_307
Let formula_308 and formula_309
Let us look for error values using formula formula_279
where formula_165 are roots of polynomial formula_281
formula_313
We get formula_314
formula_315
formula_316
The fact that formula_317 should not be surprising.
Corrected code is therefore [ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0].

</doc>
<doc id="40780" url="http://en.wikipedia.org/wiki?curid=40780" title="Beam diameter">
Beam diameter

The beam diameter or beam width of an electromagnetic beam is the diameter along any specified line that is perpendicular to the beam axis and intersects it. Since beams typically do not have sharp edges, the diameter can be defined in many different ways. Five definitions of the beam width are in common use: D4σ, 10/90 or 20/80 knife-edge, 1/e2, FWHM, and D86. The beam width can be measured in units of length at a particular plane perpendicular to the beam axis, but it can also refer to the angular width, which is the angle subtended by the beam at the source.
Beam diameter is usually used to characterize electromagnetic beams in the optical regime, and occasionally in the microwave regime, that is, cases in which the aperture from which the beam emerges is very large with respect to the wavelength.
Beam diameter usually refers to a beam of circular cross section, but not necessarily so. A beam may, for example, have an elliptical cross section, in which case the orientation of the beam diameter must be specified, for example with respect to the major or minor axis of the elliptical cross section. The term "beam width" may be preferred in applications where the beam does not have circular symmetry.
Width definitions.
Full width at half maximum.
The simplest way to define the width of a beam is to choose two diametrically opposite points at which the irradiance is a specified fraction of the beam's peak irradiance, and take the distance between them as a measure of the beam's width. An obvious choice for this fraction is ½ (−3 dB), in which case the diameter obtained is the full width of the beam at half its maximum intensity (FWHM). This is also called the "half-power beam width" (HPBW).
1/e2 width.
The 1/e2 width is equal to the distance between the two points on the marginal distribution that are 1/e2 = 0.135 times the maximum value. In many cases, it makes more sense to take the distance between points where the intensity falls to 1/e2 = 0.135 times the maximum value. If there are more than two points that are 1/e2 times the maximum value, then the two points closest to the maximum are chosen. The 1/e2 width is important in the mathematics of Gaussian beams.
The American National Standard Z136.1-2007 for Safe Use of Lasers (p. 6) defines the beam diameter as the distance between diametrically opposed points in that cross-section of a beam where the power per unit area is 1/e (0.368) times that of the peak power per unit area. This is the beam diameter definition that is used for computing the maximum permissible exposure to a laser beam. In addition, the Federal Aviation Administration also uses the 1/e definition for laser safety calculations in FAA Order 7400.2F, "Procedures for Handling Airspace Matters," February 16, 2006, p. 29-1-2.
Measurements of the 1/e2 width only depend on three points on the marginal distribution, unlike D4σ and knife-edge widths that depend on the integral of the marginal distribution. 1/e2 width measurements are noisier than D4σ width measurements. For multimodal marginal distributions (a beam profile with multiple peaks), the 1/e2 width usually does not yield a meaningful value and can grossly underestimate the inherent width of the beam. For multimodal distributions, the D4σ width is a better choice. For an ideal single-mode Gaussian beam, the D4σ, D86 and 1/e2 width measurements would give the same value.
For a Gaussian beam, the relationship between the 1/e2 width and the full width at half maximum is formula_1, where formula_2 is the full width of the beam at 1/e2.
D4σ or second moment width.
The D4σ width of a beam in the horizontal or vertical direction is 4 times σ, where σ is the standard deviation of the horizontal or vertical marginal distribution, respectively. Mathematically, the D4σ beam width in the x-dimension for the beam profile formula_3 is expressed as
where
is the centroid of the beam profile in the x-direction.
When a beam is measured with a laser beam profiler, the wings of the beam profile influence the D4σ value more than the center of the profile since the wings are weighted by the square of its distance, "x"2, from the center of the beam. If the beam does not fill more than a third of the beam profiler’s sensor area, then there will be a significant number of pixels at the edges of the sensor that register a small baseline value (the background value). If the baseline value is large or if it is not subtracted out of the image, then the computed D4σ value will be larger than the actual value because the baseline value near the edges of the sensor are weighted in the D4σ integral by "x"2. Therefore, baseline subtraction is necessary for accurate D4σ measurements. The baseline is easily measured by recording the average value for each pixel when the sensor is not illuminated. The D4σ width, unlike the FWHM and 1/e2 widths, is meaningful for multimodal marginal distributions — that is, beam profiles with multiple peaks — but requires careful subtraction of the baseline for accurate results. The D4σ is the ISO international standard definition for beam width.
Knife-edge width.
Before the advent of the CCD beam profiler, the beam width was estimated using the knife-edge technique: slice a laser beam with a razor and measure the power of the clipped beam as a function of the razor position. The measured curve is the integral of the marginal distribution, and starts at the total beam power and decreases monotonically to zero power. The width of the beam is defined as the distance between the points of the measured curve that are 10% and 90% (or 20% and 80%) of the maximum value. If the baseline value is small or subtracted out, the knife-edge beam width always corresponds to 60%, in the case of 20/80, or 80%, in the case of 10/90, of the total beam power no matter what the beam profile. On the other hand, the D4σ, 1/e2, and FWHM widths encompass fractions of power that are beam-shape dependent. Therefore, the 10/90 or 20/80 knife-edge width is a useful metric when the user wishes to be sure that the width encompasses a fixed fraction of total beam power. Most CCD beam profiler's software can compute the knife-edge width numerically.
D86 width.
The D86 width is defined as the diameter of the circle that is centered at the centroid of the beam profile and contains 86% of the beam power. The solution for D86 is found by computing the area of increasingly larger circles around the centroid until the area contains 0.86 of the total power. Unlike the previous beam width definitions, the D86 width is not derived from marginal distributions. The percentage of 86, rather than 50, 80, or 90, is chosen because a circular Gaussian beam profile integrated down to 1/e2 of its peak value contains 86% of its total power. The D86 width is often used in applications that are concerned with knowing exactly how much power is in a given area. For example, applications of high-energy laser weapons and lidars require precise knowledge of how much transmitted power actually illuminates the target.
ISO11146 beam width for elliptic beams.
The definition given before holds for stigmatic (circular symmetric) beams only. For astigmatic beams however, a more rigorous definition of the beam width has to be used, 
and
This definition also incorporates information about x-y-correlation formula_8, but for circular symmetric beams, both definitions are the same.
Some new symbols appeared within the formulas, which are the first- and second-order moments
the beam power 
and 
Using this general definition, also the beam's azimutal-angle formula_16 can be expressed. It is the angle between the beam's directions of minimum and maximum elongation, known as principal axis, and the laboratory system, being the formula_17- and formula_18-axis of the detector and given by 
Measurement.
International standard ISO 11146-1:2005 specifies methods for measuring beam widths (diameters), divergence angles and beam propagation ratios of laser beams (if the beam is stigmatic) and for general astigmatic beams ISO 11146-2 is applicable. The D4σ beam width is the ISO standard definition and the measurement of the M² beam quality parameter requires the measurement of the D4σ widths.
The other definitions provide complementary information to the D4σ. The D4σ and knife-edge widths are sensitive to the baseline value, whereas the 1/e2 and FWHM widths are not. The fraction of total beam power encompassed by the beam width depends on which definition is used.

</doc>
<doc id="40781" url="http://en.wikipedia.org/wiki?curid=40781" title="Beam divergence">
Beam divergence

The beam divergence of an electromagnetic beam is an angular measure of the increase in beam diameter or radius with distance from the optical aperture or antenna aperture from which the electromagnetic beam emerges. The term is relevant only in the "far field", away from any focus of the beam. Practically speaking, however, the far field can commence physically close to the radiating aperture, depending on aperture diameter and the operating wavelength.
Beam divergence is often used to characterize electromagnetic beams in the optical regime, for cases in which the aperture from which the beam emerges is very large with respect to the wavelength. However, it is also used in the Radio Frequency (RF) regime for cases in which the antenna is operating in the so-called optical region and is likewise very large relative to a wavelength.
Beam divergence usually refers to a beam of circular cross section, but not necessarily so. A beam may, for example, have an elliptical cross section, in which case the orientation of the beam divergence must be specified, for example with respect to the major or minor axis of the elliptical cross section.
The divergence of a beam can be calculated if one knows the beam diameter at two separate points far from any focus ("Di", "Df"), and the distance ("l") between these points. The beam divergence, formula_1, is given by 
If a collimated beam is focused with a lens, the diameter formula_3 of the beam in the rear focal plane of the lens is related to the divergence of the initial beam by 
where "f" is the focal length of the lens.
Like all electromagnetic beams, lasers are subject to divergence, which is measured in milliradians (mrad) or degrees. For many applications, a lower-divergence beam is preferable. Neglecting divergence due to poor beam quality, the divergence of a laser beam is proportional to its wavelength and inversely proportional to the diameter of the beam at its narrowest point. For example, an ultraviolet laser that emits at a wavelength of 308 nm will have a lower divergence than an infrared laser at 808 nm, if both have the same minimum beam diameter. The divergence of good-quality laser beams is modeled using the mathematics of Gaussian beams.
Gaussian laser beams are said to be diffraction limited when their radial beam divergence formula_5 is close to the minimum possible value, which is given by 
where formula_7 is the laser wavelength and formula_8 is the radius of the beam at its narrowest point, which is called the "beam waist". This type of beam divergence is observed from optimized laser cavities. Information on the diffraction-limited divergence of a coherent beam is inherently given by the N-slit interferometric equation.

</doc>
<doc id="40782" url="http://en.wikipedia.org/wiki?curid=40782" title="Beam steering">
Beam steering

Beam steering (also spelled beamsteering or beam-steering) is about changing the direction of the main lobe of a radiation pattern.
In radio systems, beam steering may be accomplished by switching the antenna elements or by changing the relative phases of the RF signals driving the elements.
In acoustics, beam steering is used to direct the audio from loudspeakers to a specific location in the listening area. This is done by changing the magnitude and phase of two or more loudspeakers installed in a column where the combined sound is added and cancelled at the required position. Commercially, this type of loudspeaker arrangement is known as a line array. This technique has been around for many years but since the emergence of modern DSP (Digital Signal Processing) technology there are now many commercially available products on the market. Beam Steering and Directivity Control using DSP was pioneered in the early 1990s by Duran Audio who launched a technology called DDC (Digital Directivity Control).
In optical systems, beam steering may be accomplished by changing the refractive index of the medium through which the beam is transmitted or by the use of mirrors, prisms, lenses, or rotating diffraction gratings. Examples of optical beam steering approaches include mechanical mirror-based gimbals or beam-director units, galvanometer mechanisms that rotate mirrors, Risley prisms, phased-array optics, and microelectromechanical systems (MEMS) using micro-mirrors.
Source: from Federal Standard 1037C

</doc>
<doc id="40783" url="http://en.wikipedia.org/wiki?curid=40783" title="Beamwidth">
Beamwidth

In telecommunication, the term beamwidth has the following meanings:
1. In a radio antenna pattern, the half power beam width is the angle between the half-power (-3 dB) points of the main lobe, when referenced to the peak effective radiated power of the main lobe. See beam diameter.
Beamwidth is usually but not always expressed in degrees and for the horizontal plane.
2. For the optical regime, "see" beam divergence.
See also.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40785" url="http://en.wikipedia.org/wiki?curid=40785" title="Bel">
Bel

Bel can mean:

</doc>
<doc id="40786" url="http://en.wikipedia.org/wiki?curid=40786" title="Bias">
Bias

Bias is an inclination of temperament or outlook to present or hold a partial perspective, often accompanied by a refusal to consider the possible merits of alternative points of view. People may be biased toward or against an individual, a race, a religion, a social class, a political party, or a species. Biased means one-sided, lacking a neutral viewpoint, not having an open mind. Bias can come in many forms and is often considered to be synonymous with prejudice or bigotry.
There can be many forms of bias. Some overlooked aspects of bias, occurring especially with the pedagogical circles of public and private schools—sources that are unrelated to fiduciary or mercantile impoverishment which may be unduly magnified—include teacher bias as well as a general bias against women who are going into STEM research, noted William Van Ornum, former research director of American Mental Health Foundation. Van Ornum is known for taking a multidimensional perspective toward bias, based both on his background as school psychologist and practicing clinical psychologist.
Media bias is the bias of journalists and news producers within the mass media, in the selection of which events and stories are reported and how they are covered. The term "media bias" implies a pervasive or widespread bias contravening the standards of journalism, rather than the perspective of an individual journalist or article. The direction and degree of media bias in various countries is widely disputed.
Practical limitations to media neutrality include the inability of journalists to report all available stories and facts, and the requirement that selected facts be linked into a coherent narrative. Since it is impossible to report everything, selectivity is inevitable. Government influence, including overt and covert censorship, biases the media in some countries. Market forces that result in a biased presentation include the ownership of the news source, concentration of media ownership, the selection of staff, the preferences of an intended audience, and pressure from advertisers.
Unfortunately most discussions of educational bias focus on standardized testing. It is often assumed that "all" bias is education is caused by educational and standardized tests. This has become magnified due to legislation concerning No Child Left Behind. This law is usually falsely attributed to president George Bush, although its antecedents are found in the Clinton administration. Nearly all textbooks focus on test bias.
Taking a broad approach, William Van Ornum of Marist College cautions that the simplistic pattern of conflating all educational bias with standardized testing can lead to a laissez-faire atmosphere were other forms of bias are not confronted. This is harmful to teachers, students, parents, and the general public.
Van Ornum stated: "It is especially important for students in education to understand and learn to confront sources of bias that occur when teachers evaluate students in elementary high school and high school. These forms include but are not limited to: teacher bias against very gifted students; teacher bias toward those who score high on standardized tests ("People like that aren't creative"; "This student just may have had a good day"; teacher bias against a current student because a family member was in a class previously and was either vastly smarter or not as adept at schoolwork"; teacher bias against athletes or teacher bias against non-athletes."
Finally, bias against LGBTQI students within the educational system itself may transcend even the aggregate amount of bias within schools caused, engendered, and brought about when standardized tests are used inappropriately.
Van Ornum suggests a continued empirical and multimodal approach toward bias in educational systems.
Political bias has been a feature of the mass media since its birth with the invention of the printing press. The expense of early printing equipment restricted media production to a limited number of people. Historians have found that publishers often served the interests of powerful social groups.

</doc>
<doc id="40788" url="http://en.wikipedia.org/wiki?curid=40788" title="Bias distortion">
Bias distortion

In telecommunication, the term bias distortion has the following meanings: 
Bias distortion is expressed in percent of the system-specified unit interval.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40789" url="http://en.wikipedia.org/wiki?curid=40789" title="Bilateral synchronization">
Bilateral synchronization

In telecommunication, bilateral synchronization (or bilateral control) is a synchronization control system between exchanges A and B in which the clock at telephone exchange A controls the data received at exchange B and the clock at exchange B controls the data received at exchange A. 
Bilateral synchronization is usually implemented by deriving the timing from the incoming bitstream.
Source: from Federal Standard 1037C in support of MIL-STD-188

</doc>
<doc id="40792" url="http://en.wikipedia.org/wiki?curid=40792" title="Bipolar signal">
Bipolar signal

In telecommunication, a bipolar signal is a signal that may assume either of two polarities, neither of which is zero. 
A bipolar signal may have a two-state non-return-to-zero (NRZ) or a three-state return-to-zero (RZ) binary coding scheme. 
A bipolar signal is usually symmetrical with respect to zero amplitude, "i.e.," the absolute values of the positive and negative signal states are nominally equal.

</doc>
<doc id="40793" url="http://en.wikipedia.org/wiki?curid=40793" title="Bit-count integrity">
Bit-count integrity

In telecommunication, the term bit-count integrity (BCI) has the following meanings: 
"Note:" Bit-count integrity is not the same as bit integrity, which requires that the delivered bits correspond exactly with the original bits.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40794" url="http://en.wikipedia.org/wiki?curid=40794" title="Bit error rate">
Bit error rate

In digital transmission, the number of bit errors is the number of received bits of a data stream over a communication channel that have been altered due to noise, interference, distortion or bit synchronization errors.
The bit error rate (BER) is the number of bit errors per unit time. The bit error ratio (also BER) is the number of bit errors divided by the total number of transferred bits during a studied time interval. BER is a unitless performance measure, often expressed as a percentage.
The bit error probability "pe" is the expectation value of the bit error ratio. The bit error ratio can be considered as an approximate estimate of the bit error probability. This estimate is accurate for a long time interval and a high number of bit errors.
Example.
As an example, assume this transmitted bit sequence:
0 1 1 0 0 0 1 0 1 1
and the following received bit sequence:
0 0 1 0 1 0 1 0 0 1,
The number of bit errors (the underlined bits) is in this case 3. The BER is 3 incorrect bits divided by 10 transferred bits, resulting in a BER of 0.3 or 30%.
Packet error ratio.
The packet error ratio (PER) is the number of incorrectly received data packets divided by the total number of received packets. A packet is declared incorrect if at least one bit is erroneous. The expectation value of the PER is denoted packet error probability "pp", which for a data packet length of "N" bits can be expressed as
assuming that the bit errors are independent of each other. For small bit error probabilities, this is approximately
Similar measurements can be carried out for the transmission of frames, blocks, or symbols.
Factors affecting the BER.
In a communication system, the receiver side BER may be affected by transmission channel noise, interference, distortion, bit synchronization problems, attenuation, wireless multipath fading, etc.
The BER may be improved by choosing a strong signal strength (unless this causes cross-talk and more bit errors), by choosing a slow and robust modulation scheme or line coding scheme, and by applying channel coding schemes such as redundant forward error correction codes.
The "transmission BER" is the number of detected bits that are incorrect before error correction, divided by the total number of transferred bits (including redundant error codes). The "information BER", approximately equal to the decoding error probability, is the number of decoded bits that remain incorrect after the error correction, divided by the total number of decoded bits (the useful information). Normally the transmission BER is larger than the information BER. The information BER is affected by the strength of the forward error correction code.
Analysis of the BER.
The BER may be evaluated using stochastic (Monte Carlo) computer simulations. If a simple transmission channel model and data source model is assumed, the BER may also be calculated analytically. An example of such a data source model is the Bernoulli source.
Examples of simple channel models used in information theory are:
A worst-case scenario is a completely random channel, where noise totally dominates over the useful signal. This results in a transmission BER of 50% (provided that a Bernoulli binary data source and a binary symmetrical channel are assumed, see below).
In a noisy channel, the BER is often expressed as a function of the normalized carrier-to-noise ratio measure denoted Eb/N0, (energy per bit to noise power spectral density ratio), or Es/N0 (energy per modulation symbol to noise spectral density).
For example, in the case of QPSK modulation and AWGN channel, the BER as function of the Eb/N0 is given by:
formula_3.
People usually plot the BER curves to describe the performance of a digital communication system. In optical communication, BER(dB) vs. Received Power(dBm) is usually used; while in wireless communication, BER(dB) vs. SNR(dB) is used.
Measuring the bit error ratio helps people choose the appropriate forward error correction codes. Since most such codes correct only bit-flips, but not bit-insertions or bit-deletions, the Hamming distance metric is the appropriate way to measure the number of bit errors. Many FEC coders also continuously measure the current BER.
A more general way of measuring the number of bit errors is the Levenshtein distance.
The Levenshtein distance measurement is more appropriate for measuring raw channel performance before frame synchronization, and when using error correction codes designed to correct bit-insertions and bit-deletions, such as Marker Codes and Watermark Codes.
Mathematical draft.
The BER is the likelihood of a bit misinterpretation due to electrical noise formula_4. Considering a bipolar NRZ transmission, we have
formula_5 for a "1" and formula_6 for a "0". Each of formula_7 and formula_8 has a period of formula_9.
Knowing that the noise has a bilateral spectral density formula_10,
formula_7 is formula_12
and formula_8 is formula_14.
Returning to BER, we have the likelihood of a bit misinterpretation formula_15.
formula_16 and formula_17
where formula_18 is the threshold of decision, set to 0 when formula_19.
We can use the average energy of the signal formula_20 to find the final expression :
formula_21
Bit error rate test.
BERT or bit error rate test is a testing method for digital communication circuits that uses predetermined stress patterns consisting of a sequence of logical ones and zeros generated by a test pattern generator.
A BERT typically consists of a test pattern generator and a receiver that can be set to the same pattern. They can be used in pairs, with one at either end of a transmission link, or singularly at one end with a loopback at the remote end. BERTs are typically stand-alone specialised instruments, but can be personal computer–based. In use, the number of errors, if any, are counted and presented as a ratio such as 1 in 1,000,000, or 1 in 1e06.
Bit error rate tester.
A bit error rate tester (BERT), also known as a "bit error ratio tester" or "bit error rate test solution" (BERTs) is electronic test equipment used to test the quality of signal transmission of single components or complete systems.
The main building blocks of a BERT are:
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40795" url="http://en.wikipedia.org/wiki?curid=40795" title="Bit inversion">
Bit inversion

In telecommunications, bit inversion means the changing of the state of a bit to the opposite state, i.e. the changing of a 0 bit to 1 or of a 1 bit to 0. It also refers to the changing of a "state representing a given bit" to the opposite state.
"Source: Federal Standard 1037C and MIL-STD-188"

</doc>
<doc id="40796" url="http://en.wikipedia.org/wiki?curid=40796" title="Bit pairing">
Bit pairing

In telecommunication, bit pairing is the practice of establishing, within a code set, a number of subsets that have an identical bit representation except for the state of a specified bit. 
"Note:" An example of bit pairing occurs in the International Alphabet No. 5 and the American Standard Code for Information Interchange (ASCII), where the upper case letters are related to their respective lower case letters by the state of bit six.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40798" url="http://en.wikipedia.org/wiki?curid=40798" title="Bit-sequence independence">
Bit-sequence independence

In telecommunication, bit-sequence independence is a characteristic of some digital data transmission systems that impose no restrictions on, or modification of, the transmitted bit sequence.
Bit-sequence-independent protocols are in contrast to protocols that reserve certain bit sequences for special meanings, such as the flag sequence, 01111110, for HDLC, SDLC, and ADCCP protocols.
Bit-sequence-independence allows only line codes that have the same number of transitions per bit, otherwise, the line code is dependent on the bit sequence and, therefore, bit-sequence dependent.

</doc>
<doc id="40799" url="http://en.wikipedia.org/wiki?curid=40799" title="Bit slip">
Bit slip

In digital transmission, bit slip is the loss or gain of a bit or bits, caused by clock drift – variations in the respective clock rates of the transmitting and receiving devices.
One cause of bit slippage is overflow of a receive buffer that occurs when the transmitter's clock rate exceeds that of the receiver. This causes one or more bits to be dropped for lack of storage capacity.
One way to maintain timing between transmitting and receiving devices is to employ an asynchronous protocol such as start-stop. Alternatively, bit slip can be prevented by using a self-clocking signal (such as a signal modulated using OQPSK) or using a line coding such as Manchester encoding.
Another cause is "losing count", as on a hard drive: if a hard drive encounters a long string of 0s, without any 1s (or a string of 1s without 0s), it may lose track of the frame between fields, and suffer bit slip.
When a pulse of N consecutive zero bits are sent, clock drift may cause the hardware to apparently detect N-1 zero bits or N+1 zero bits—both kinds of errors are called bit slip.
Thus one prevents long strings without change via such devices as run length limited codes.
Many communication systems use linear feedback shift register scrambling to prevent long strings of 0s (or other symbol),
including VSAT, 1000BASE-T, RFC 2615, etc.
While a scrambler makes the "losing count" type of bit slip error occur far less often,
when bit slip errors do occur (perhaps for other reasons), 
scramblers have the drawback of expanding small errors that add or lose a single bit into a much longer burst of errors.
The optimized cipher feedback mode (OCFB), the statistical self-synchronization mode, and the "one-bit CFB mode" also expand small bit-slip errors into a longer burst of errors, but eventually recover and produce the correct decrypted plaintext.
A bit-slip error when using any other block cipher mode of operation generally results in complete corruption of the rest of the message.

</doc>
<doc id="40801" url="http://en.wikipedia.org/wiki?curid=40801" title="Bit-stream transmission">
Bit-stream transmission

In telecommunication, the term bit-stream transmission has the following meanings: 
1. In bit-oriented systems, the transmission of bit strings. 
2. In character-oriented systems, the transmission of bit streams that represent characters. 
In bit-stream transmission, the bits usually occur at fixed time intervals, start and stop signals are not used, and the bit patterns follow each other in sequence without interruption.

</doc>
<doc id="40802" url="http://en.wikipedia.org/wiki?curid=40802" title="Bit stuffing">
Bit stuffing

In data transmission and telecommunication, bit stuffing (also known—uncommonly—as positive justification) is the insertion of non information bits into data. Stuffed bits should not be confused with overhead bits.
Bit stuffing is used for various purposes, such as for bringing bit streams that do not necessarily have the same or rationally related bit rates up to a common rate, or to fill buffers or frames. The location of the stuffing bits is communicated to the receiving end of the data link, where these extra bits are removed to return the bit streams to their original bit rates or form. Bit stuffing may be used to synchronize several channels before multiplexing or to rate-match two single channels to each other.
Applications include Plesiochronous Digital Hierarchy and Synchronous Digital Hierarchy.
Another use of bit stuffing is for run length limited coding: to limit the number of consecutive bits of the same value in the data to be transmitted. A bit of the opposite value is inserted after the maximum allowed number of consecutive bits. Since this is a general rule the receiver doesn't need extra information about the location of the stuffing bits in order to do the de-stuffing.
This is done to create additional signal transitions to ensure reliable reception or to escape special reserved code words such as frame sync sequences when the data happens to contain them.
Applications include Controller Area Network, HDLC, and Universal Serial Bus.
Bit stuffing does not ensure that the payload is intact ("i.e." not corrupted by transmission errors); it is merely a way of attempting to ensure that the transmission starts and ends at the correct places. Error detection and correction techniques are used to check the frame for corruption after its delivery and, if necessary, the frame will be resent.
Zero-bit insertion.
Zero-bit insertion is a particular type of bit stuffing used in some data transmission protocols to aid clock recovery from the data stream. It was popularized by IBM's SDLC (later renamed HDLC).
The name relates to the insertion of only 0 bits. No 1 bits are inserted to limit sequences of 0 bits.
SDLC and Low- and full-speed USB data are sent NRZI encoded: a 0 bit causes a signal transition, whereas a 1 bit causes no change. After a long sequence of 1 bits there could be no transitions in the transmitted data, and it would be possible for the transmitter and receiver clocks to lose synchronisation. By inserting a 0 after five (SDLC) or six (USB) sequential 1s the transmitter guarantees a maximum time between transitions. The receiver can synchronise its clock against the transitions to ensure proper data recovery.
In SDLC the transmitted bit sequence "01111110" containing six adjacent 1 bits is the Flag byte. Bit stuffing ensures that this pattern can never occur in normal data, so it can be used as a marker for the beginning and end of frame without any possibility of being confused with normal data.
The main disadvantage of this form of bit-stuffing is that the code rate is unpredictable; it depends on the data being transmitted.
"Source: from Federal Standard 1037C in support of MIL-STD-188"

</doc>
<doc id="40803" url="http://en.wikipedia.org/wiki?curid=40803" title="Bit-synchronous operation">
Bit-synchronous operation

Bit-synchronous operation is a type of digital communication in which the data circuit-terminating equipment (DCE), data terminal equipment (DTE), and transmitting circuits are all operated in bit synchronism with a clock signal.
In bit-synchronous operation, clock timing is usually delivered at twice the modulation rate, and one bit is transmitted or received during each clock cycle.
Bit-synchronous operation is sometimes erroneously referred to as digital synchronization.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40804" url="http://en.wikipedia.org/wiki?curid=40804" title="Black facsimile transmission">
Black facsimile transmission

In telecommunication, the term black facsimile transmission has the following meanings: 

</doc>
<doc id="40806" url="http://en.wikipedia.org/wiki?curid=40806" title="Black recording">
Black recording

In telecommunication, the term black recording has the following meanings: 
Source: Federal Standard 1037C and MIL-STD-188.

</doc>
<doc id="40807" url="http://en.wikipedia.org/wiki?curid=40807" title="Blind transmission">
Blind transmission

A blind transmission, in telecommunications, is a transmission made without obtaining a receipt, or acknowledgment of reception, from the intended receiving station. Blind transmissions may occur or be necessary when security constraints, such as radio silence, are imposed, when technical difficulties with a sender's receiver or receiver's transmitter occur, or when lack of time precludes the delay caused by waiting for receipts.

</doc>
<doc id="40808" url="http://en.wikipedia.org/wiki?curid=40808" title="Block">
Block

Block may refer to:

</doc>
<doc id="40809" url="http://en.wikipedia.org/wiki?curid=40809" title="Block check character">
Block check character

In telecommunications, a block check character (BCC) is a character added to a transmission block to facilitate error detection.
In longitudinal redundancy checking and cyclic redundancy checking, block check characters are computed for, and added to, each message block transmitted. This block check character is compared with a second block check character computed by the receiver to determine whether the transmission is error free.

</doc>
<doc id="40810" url="http://en.wikipedia.org/wiki?curid=40810" title="Blocking">
Blocking

Blocking may refer to:

</doc>
<doc id="40814" url="http://en.wikipedia.org/wiki?curid=40814" title="Branch">
Branch

A branch ( or , ) or tree branch (sometimes referred to in botany as a ramus) is a woody structural member connected to but not part of the central trunk of a tree (or sometimes a shrub). Large branches are known as boughs and small branches are known as twigs.
While branches can be nearly horizontal, vertical, or diagonal, the majority of trees have upwardly diagonal branches.
The term "twig" often refers to a terminus, while "bough" refers only to branches coming directly from the trunk.
Words.
Because of the enormous quantity of branches in the world, there are a variety of names in English alone for them. In general however, unspecific words for a branch (such as rise and rame) have been replaced by the word branch itself.
Specific terms.
A bough can also be called a limb or arm, and though these are arguably metaphors, both are widely accepted synonyms for bough.
A crotch or fork is an area where a trunk splits into two or more boughs.
A twig is frequently referred to as a sprig as well, especially when it has been plucked. Other words for twig include branchlet, spray, and surcle, as well as the technical terms surculus and ramulus.
Branches found under larger branches can be called underbranches.
Some branches from specific trees have their own names, such as osiers and withes or withies, which come from willows. Often trees have certain words which, in English, are naturally collocated, such as holly and mistletoe, which usually employ the phrase "sprig of" (as in, a "sprig of mistletoe"). Similarly, the branch of a cherry tree is generally referred to as a "cherry branch", while other such formations (i.e., "acacia branch" or "orange branch") carry no such alliance. A good example of this versatility is oak, which could be referred to as variously an "oak branch", an "oaken branch", a "branch of oak", or the "branch of an oak [tree]".
Once a branch has been cut or in any other way removed from its source, it is most commonly referred to as a stick, and a stick employed for some purpose (such as walking, spanking, or beating) is often called a rod. Thin, flexible sticks are called switches, wands, shrags, or vimina (singular vimen).
History and etymology.
In Old English, there are numerous words for branch, including "seten", "stofn", "telgor", and "hrīs". 
There are also numerous descriptive words, such as "blēd" (that is, something that has bled, or "bloomed", out), "bōgincel" (literally "little bough"), "ōwæstm" (literally "on growth"), and "tūdornes" (literally "offspringing"). Numerous other words for twigs and boughs abound, including "tān", which still surves as the "-toe" in "mistletoe".

</doc>
<doc id="40815" url="http://en.wikipedia.org/wiki?curid=40815" title="Brewster's angle">
Brewster's angle

Brewster's angle (also known as the polarization angle) is an angle of incidence at which light with a particular polarization is perfectly transmitted through a transparent dielectric surface, with no reflection. When "unpolarized" light is incident at this angle, the light that is reflected from the surface is therefore perfectly polarized. This special angle of incidence is named after the Scottish physicist Sir David Brewster (1781–1868).
Explanation.
When light encounters a boundary between two media with different refractive indices, some of it is usually reflected as shown in the figure above. The fraction that is reflected is described by the Fresnel equations, and is dependent upon the incoming light's polarization and angle of incidence. 
The Fresnel equations predict that light with the "p" polarization (electric field polarized in the same plane as the incident ray and the surface normal) will not be reflected if the angle of incidence is 
where "n"1 is the refractive index of the initial medium through which the light propagates (the "incident medium"), and "n"2 is the index of the other medium. This equation is known as Brewster's law, and the angle defined by it is Brewster's angle.
The physical mechanism for this can be qualitatively understood from the manner in which electric dipoles in the media respond to "p"-polarized light. One can imagine that light incident on the surface is absorbed, and then re-radiated by oscillating electric dipoles at the interface between the two media. The polarization of freely propagating light is always perpendicular to the direction in which the light is travelling. The dipoles that produce the transmitted (refracted) light oscillate in the polarization direction of that light. These same oscillating dipoles also generate the reflected light. However, dipoles do not radiate any energy in the direction of the dipole moment. If the refracted light is "p"-polarized and propagates exactly perpendicular to the direction in which the light is predicted to be specularly reflected, the dipoles point along the specular reflection direction and therefore no light can be reflected. (See diagram, above)
With simple geometry this condition can be expressed as
where "θ"1 is the angle of reflection (or incidence) and "θ"2 is the angle of refraction.
Using Snell's law,
one can calculate the incident angle "θ"1 = "θ"B at which no light is reflected:
Solving for "θ"B gives
For a glass medium ("n"2 ≈ 1.5) in air ("n"1 ≈ 1), Brewster's angle for visible light is approximately 56°, while for an air-water interface ("n"2 ≈ 1.33), it is approximately 53°. Since the refractive index for a given medium changes depending on the wavelength of light, Brewster's angle will also vary with wavelength.
The phenomenon of light being polarized by reflection from a surface at a particular angle was first observed by Étienne-Louis Malus in 1808. He attempted to relate the polarizing angle to the refractive index of the material, but was frustrated by the inconsistent quality of glasses available at that time. In 1815, Brewster experimented with higher-quality materials and showed that this angle was a function of the refractive index, defining Brewster's law. 
Brewster's angle is often referred to as the "polarizing angle", because light that reflects from a surface at this angle is entirely polarized perpendicular to the incident plane (""s"-polarized") A glass plate or a stack of plates placed at Brewster's angle in a light beam can, thus, be used as a polarizer. The concept of a polarizing angle can be extended to the concept of a Brewster wavenumber to cover planar interfaces between two linear bianisotropic materials. In the case of reflection at Brewster's angle, the reflected and refracted rays are mutually perpendicular.
Applications.
Polarized sunglasses use the principle of Brewster's angle to reduce glare from the sun reflecting off horizontal surfaces such as water or road. In a large range of angles around Brewster's angle, the reflection of "p"-polarized light is lower than "s"-polarized light. Thus, if the sun is low in the sky, reflected light is mostly "s"-polarized. Polarizing sunglasses use a polarizing material such as Polaroid sheets to block horizontally-polarized light, preferentially blocking reflections from horizontal surfaces. The effect is strongest with smooth surfaces such as water, but reflections from roads and the ground are also reduced.
Photographers use the same principle to remove reflections from water so that they can photograph objects beneath the surface. In this case, the polarizing filter camera attachment can be rotated to be at the correct angle (see figure).
When recording a hologram, light is typically incident at Brewster's angle. Because the incident light is p-polarized, it is not back reflected from the transparent back-plane of the holographic film. This avoids unwanted interference effects in the hologram. 
Brewster angle prisms are used in laser physics. The polarized laser light enters the prism at Brewster's angle without any reflective losses. 
Brewster windows.
Gas lasers typically use a window tilted at Brewster's angle to allow the beam to leave the laser tube. Since the window reflects some "s"-polarized light but no "p"-polarized light, the round trip loss for the "s" polarization is higher than that of the "p" polarization. This causes the laser's output to be "p" polarized due to competition between the two modes.

</doc>
<doc id="40816" url="http://en.wikipedia.org/wiki?curid=40816" title="Bridge-to-bridge station">
Bridge-to-bridge station

In telecommunication, a bridge-to-bridge station is a ship station operating in the port operations service in which messages are restricted to navigational communications and which is capable of operation from the ship's navigational bridge or, in the case of a dredge, from its main control station, operating on a frequency or frequencies in the 156-162 MHz band. 
Source: From Federal Standard 1037C and from the NTIA Manual of Regulations and Procedures for Federal Radio Frequency Management

</doc>
<doc id="40817" url="http://en.wikipedia.org/wiki?curid=40817" title="Bridging loss">
Bridging loss

Bridging loss is the loss, at a given frequency, that results when an impedance is connected across a transmission line. It is expressed as the ratio, in decibels, of the signal power delivered to a given point in a system downstream from the bridging point prior to bridging, to the signal power delivered to the given point after bridging.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40818" url="http://en.wikipedia.org/wiki?curid=40818" title="Brightness">
Brightness

Brightness is an attribute of visual perception in which a source appears to be radiating or reflecting light. In other words, brightness is the perception elicited by the luminance of a visual target. This is a subjective attribute/property of an object being observed and one of the color appearance parameters of color appearance models.
Terminology.
The adjective "bright" derives from an Old English "beorht" with the same meaning via metathesis giving Middle English "briht". The word is from a Common Germanic *"berhtaz", ultimately from a PIE root with a closely related meaning, *"bhereg-" "white, bright".
"Brightness" was formerly used as a synonym for the photometric term "luminance" and (incorrectly) for the radiometric term "radiance".
As defined by the US "Federal Glossary of Telecommunication Terms" (FS-1037C), "brightness" should now be used only for non-quantitative references to physiological sensations and perceptions of light.
A given target luminance can elicit different perceptions of brightness in different contexts; see, for example, White's illusion.
In the RGB color space, brightness can be thought of as the arithmetic mean "μ" of the red, green, and blue color coordinates (although some of the three components make the light seem brighter than others, which, again, may be compensated by some display systems automatically):
Brightness is also a color coordinate in the HSB or HSV color space (hue, saturation, and brightness or value).
With regard to stars, brightness is quantified as apparent magnitude and absolute magnitude.
Brightness is, at least in some respects, the antonym of darkness.
Brightness of sounds.
The term "brightness" is also used in discussions of sound timbres, in a rough analogy with visual brightness. Timbre researchers consider brightness to be one of the perceptually strongest distinctions between sounds, and formalize it acoustically as an indication of the amount of high-frequency content in a sound, using a measure such as the spectral centroid.

</doc>
<doc id="40821" url="http://en.wikipedia.org/wiki?curid=40821" title="Buffer">
Buffer

Buffer may refer to: 

</doc>
<doc id="40822" url="http://en.wikipedia.org/wiki?curid=40822" title="Burst switching">
Burst switching

In a packet switched network, burst switching is a capability in which each network switch extracts routing instructions from an incoming packet header to establish and maintain the appropriate switch connection for the duration of the packet, following which the connection is automatically released. 
In concept, burst switching is similar to connectionless mode transmission, but differs in that burst switching implies an intent to establish the switch connection in near real time so that only minimum buffering is required at the node switch.
A variant of burst switching used in optical networks is optical burst switching.
References.
</dl>

</doc>
<doc id="40823" url="http://en.wikipedia.org/wiki?curid=40823" title="Burst transmission">
Burst transmission

In telecommunication, the term burst transmission or data burst has the following meanings:
Burst transmission, however, enables communications between data terminal equipment (DTEs) and a data network operating at dissimilar data signaling rates.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40824" url="http://en.wikipedia.org/wiki?curid=40824" title="Busy hour">
Busy hour

Busy hour: In a communications system, the sliding 60-minute period during which occurs the maximum total traffic load in a given 24-hour period. 
The busy hour is determined by fitting a horizontal line segment equivalent to one hour under the traffic load curve about the peak load point. 
If the service time interval is less than 60 minutes, the busy hour is the 60-minute interval that contains the service timer interval. 
In cases where more than one busy hour occurs in a 24-hour period, "i.e.," when saturation occurs, the busy hour or hours most applicable to the particular situation are used. "Synonym" peak busy hour.
The idea behind the busy hour is that the traffic is not leveled throughout the day. There are some peak hours in a day which are busy and have more traffic than normal hours. During these hours traffic is at maximum level. The users will receive Grade of Services according to the traffic measured in those hours.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40825" url="http://en.wikipedia.org/wiki?curid=40825" title="Busy signal (disambiguation)">
Busy signal (disambiguation)

A busy signal is information communicated to a user or apparatus attempting a connection, indicating the requested connection cannot be completed.

</doc>
<doc id="40826" url="http://en.wikipedia.org/wiki?curid=40826" title="Busy verification">
Busy verification

In a public switched telephone network, busy verification is a network-provided service feature that permits an attendant to verify the busy or idle state of station lines and to break into the conversation.
A 440 Hz tone is applied to the line for 2 seconds, followed by a 0.5 second burst every 10 seconds, to alert both parties that the attendant is connected to the circuit.

</doc>
<doc id="40827" url="http://en.wikipedia.org/wiki?curid=40827" title="Bypass">
Bypass

Bypass may refer to:

</doc>
<doc id="40828" url="http://en.wikipedia.org/wiki?curid=40828" title="Cable television relay service station">
Cable television relay service station

In telecommunication, a cable television relay service station (CARS) is a fixed or mobile station used for the transmission of television and related audio signals, signals of standard and FM broadcast stations, signals of instructional television fixed stations, and cablecasting from the point of reception to a terminal point from which the signals are distributed to the public.
Source: from Federal Standard 1037C and from the Code of Federal Regulations, Telecommunications Parts 0-199

</doc>
<doc id="40829" url="http://en.wikipedia.org/wiki?curid=40829" title="Call">
Call

Call may refer to:

</doc>
<doc id="40830" url="http://en.wikipedia.org/wiki?curid=40830" title="Call collision">
Call collision

In telecommunications, a call collision (commonly known as glare) is one of two things:
If you have ever tried to make a call out on a PBX, and been accidentally connected to an incoming call, you have experienced glare. This can sometimes happen at home too, if you pick up your phone to make a call out at the exact second that a call is about to start ringing in.
Multi-line hunting attempts to avoid glare by selecting circuits in opposite preference order so the highest numbered line, which is last choice for incoming calls, is first choice for outgoing calls, like so:
With PRI circuits, the channel selection sequence is specified when the circuit is provisioned. Common practice is to have the PBX use descending channel selection, and the carrier to use ascending. Glare is not common on PRI circuits because the signalling is so fast, however it is not impossible (especially if there are subtle differences in the timers at either end, and the circuit is being used at near-capacity). The users will not experience a connection to an unexpected call (as would be the case with analog circuits), because glare causes protocol errors that generally prevent any sort of successful connection. Instead, one or both of the call attempts might fail, and ideally an error would appear in the logs (this depends on the logging capabilities of the systems at either end of the circuit). Glare is quite rare on PRI circuits, and can be difficult to troubleshoot.
For old, analog PBX trunks, glare would be reduced by using ground start signalling, which offered better answer and disconnect supervision. IE: Nortel BSP discouraged using loop start trunks for this and other reasons. Long Distance exchanges in the 1950s and 60s incorporated Glare Detectors to alleviate the problem.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40832" url="http://en.wikipedia.org/wiki?curid=40832" title="Call duration">
Call duration

In telecommunications, the term call duration has the following meanings: 
Source: from Federal Standard 1037C

</doc>
<doc id="40833" url="http://en.wikipedia.org/wiki?curid=40833" title="Called-party camp-on">
Called-party camp-on

In telecommunication, a called-party camp-on is a communication system service feature that enables the system to complete an access attempt in spite of issuance of a user blocking signal. This is most often found in a switchboard system at a company. Instead of going to voicemail or simply sitting on hold until the line is free, this feature places you in a queue whereby the moment the line clears, the call will be put through.
Systems that provide this feature monitor the busy user until the user blocking signal ends, and then proceed to complete the requested access. This feature permits holding an incoming telephone call until the called party is free.

</doc>
<doc id="40834" url="http://en.wikipedia.org/wiki?curid=40834" title="Calling-party camp-on">
Calling-party camp-on

In telecommunication, a calling-party camp-on is a service feature that enables the telephone exchange to complete an access attempt in spite of temporary unavailability of system transmission or switching facilities required to establish the requested access. 
Systems that provide calling party camp-on monitor the system facilities until the necessary facilities become available, and then proceed to complete the requested access. Such systems may or may not issue a system blocking signal to inform the calling party of the access delay.

</doc>
<doc id="40836" url="http://en.wikipedia.org/wiki?curid=40836" title="Call processing">
Call processing

In telecommunication, the term call processing has the following meanings: 
Related Information.
"Volume Call Processing": The handling of calls when there are far more incoming calls than can be answered by an individual or group of attendants.

</doc>
<doc id="40837" url="http://en.wikipedia.org/wiki?curid=40837" title="Call-second">
Call-second

In telecommunication, a call-second is a unit used to measure communications traffic density. 
"Note 1:" A call-second is equivalent to 1 call with a duration of 1 second. 
"Note 2:" One user making two 75-second calls is equivalent to two users each making one 75-second call. Each case produces 150 call-seconds of traffic. 
"Note 3:" The acronym CCS (Centum Call Seconds) is often used to describe 100 call-seconds.
"Note 4:" 3600 call-seconds = 36 CCS = 1 call-hour. 
"Note 5:" 3600 call-seconds per hour = 36 CCS per hour = 1 call-hour per hour = 1 erlang = 1 traffic unit.
In a communication network, a trunk (link) can carry numerous concurrent calls by means of multiplexing. Hence a particular number of CCS can be carried in infinitely many ways as calls are established and cleared over time. For example 3600 could be one call for an hour, or 2 (possibly concurrent) calls for half an hour each. CCS gives a measure of the average number of concurrent calls (i.e. Erlangs) over a time period of one hour.
Hence:
 1 CCS = 100 Call-Seconds = 1/36 erlangs.

</doc>
<doc id="40838" url="http://en.wikipedia.org/wiki?curid=40838" title="Call set-up time">
Call set-up time

In telecommunication, the term call set-up time has the following meanings: 
"Note:" Call set-up time is the summation of: (a) call request time—the time from initiation of a calling signal to the delivery to the caller of a proceed-to-select signal; (b) selection time—the time from the delivery of the proceed-to-select signal until all the selection signals have been transmitted; and (c) post selection time—the time from the end of the transmission of the selection signals until the delivery of the call-connected signal to the originating terminal.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40839" url="http://en.wikipedia.org/wiki?curid=40839" title="Call-sign allocation plan">
Call-sign allocation plan

In telecommunication, call-sign allocation plan is the table of allocation of international call sign series contained in the current edition of the "International Telecommunication Union (ITU) "Radio Regulations"."
"Note:" In the table of allocation, the first two characters of each call sign (whether two letters or one number and one letter, in that order) identify the nationality of the station. In certain instances where the complete alphabetical block is allocated to a single nation, the first letter is sufficient for national identity. Individual assignments are made by appropriate national assignment authorities from the national allocation.

</doc>
<doc id="40840" url="http://en.wikipedia.org/wiki?curid=40840" title="Call tracing">
Call tracing

In telecommunication, call tracing is a procedure that permits an entitled user to be informed about the routing of data for an established connection, identifying the entire route from the origin to the destination.
There are two types of call tracing. Permanent call tracing permits tracing of all calls. On-demand call tracing permits tracing, upon request, of a specific call, provided that the called party dials a designated code immediately after the call to be traced is disconnected.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40841" url="http://en.wikipedia.org/wiki?curid=40841" title="Camp-on busy signal">
Camp-on busy signal

In telecommunication, the term camp-on busy signal has the following meanings: 
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40842" url="http://en.wikipedia.org/wiki?curid=40842" title="Cancel character">
Cancel character

In telecommunication, the term cancel character has the following meanings: 

</doc>
<doc id="40843" url="http://en.wikipedia.org/wiki?curid=40843" title="Capacitive coupling">
Capacitive coupling

In electronics, capacitive coupling is the transfer of energy within an electrical network by means of the capacitance between circuit nodes. This coupling can have an intentional or accidental effect. Capacitive coupling is typically achieved by placing a capacitor in series with the signal to be coupled. 
Use in analog circuits.
In analog circuits, a coupling capacitor is used to connect two circuits such that only the AC signal from the first circuit can pass through to the next while DC is blocked. This technique helps to isolate the DC bias settings of the two coupled circuits. Capacitive coupling is also known as "AC coupling" and the capacitor used for the purpose is also known as a "DC-blocking capacitor".
Capacitive coupling has the disadvantage of degrading the low frequency performance of a system containing capacitively coupled units. Each coupling capacitor along with the input electrical impedance of the next stage forms a high-pass filter and the sequence of filters results in a cumulative filter with a −3dB frequency that may be higher than those of each individual filter. So for adequate low frequency response, the capacitors used must have high capacitance ratings. They should be high enough that the reactance of each is at most a tenth of the input impedance of each stage, at the lowest frequency of interest. This disadvantage of capacitively coupling DC biased, transistor amplifier circuits is largely minimized in directly coupled designs.
Use in digital circuits.
AC coupling is also widely used in digital circuits to transmit digital signal with a zero DC component, known as DC-balanced signals. DC-balanced waveforms are useful in communications systems, since they can be used over AC-coupled electrical connections to avoid voltage imbalance problems and charge accumulation between connected systems or components.
For this reason, most modern line codes are designed to produce DC-balanced waveforms. The most common classes of DC-balanced line codes are constant-weight codes and paired-disparity codes.
Gimmick loop.
A gimmick loop is a simple type of capacitive coupler: two closely spaced strands of wire. It provides capacitive coupling of a few picofarads between two nodes. Sometimes the wires are twisted together for physical stability.
Parasitic capacitive coupling.
Capacitive coupling is often unintended, such as the capacitance between two wires or PCB traces that are next to each other. Often one signal can capacitively couple with another and cause what appears to be noise. To reduce coupling, wires or traces are often separated as much as possible, or ground lines or ground planes are run in between signals that might affect each other, so that the lines are capacitively coupled to ground rather than each other. Breadboards are particularly prone to these issues due to the long pieces of metal that line every row creating a several-picofarad capacitor between lines. To prototype high-frequency (10s of MHz) or high-gain analog circuits, often the circuits are built over a ground plane so that the signals couple to ground more than to each other. If a high-gain amplifier's output capacitively couples to its input it often becomes an electronic oscillator.
One rule of thumb says that drivers should be able to drive 25 pF of capacitance which allows for PCB traces up to 0.30 meters.

</doc>
<doc id="40844" url="http://en.wikipedia.org/wiki?curid=40844" title="Capture effect">
Capture effect

In telecommunications, the capture effect, or FM capture effect, is a phenomenon associated with FM reception in which only the stronger of two signals at, or near, the same frequency will be demodulated. 
The capture effect is defined as the complete suppression of the weaker signal at the receiver limiter (if it has one) where the weaker signal is not amplified, but attenuated. When both signals are nearly equal in strength, or are fading independently, the receiver may switch from one to the other and exhibit picket fencing.
The capture effect can occur at the signal limiter, or in the demodulation stage, for circuits that do not require a signal limiter. Some types of radio receiver circuits have a stronger capture effect than others. The measurement of how well a receiver can reject a second signal on the same frequency is called the capture ratio for a specific receiver. It is measured as the lowest ratio of the power of two signals that will result in the suppression of the smaller signal. 
Amplitude modulation, or AM radio, transmission is not subject to this effect. This is one reason that the aviation industry, and others, have chosen to use AM for communications rather than FM, allowing multiple signals to be broadcast on the same channel. Similar phenomena to the capture effect are described in AM when offset carriers of different strengths are present in the passband of a receiver. For example, the aviation glideslope vertical guidance clearance beam is sometimes described as a "capture effect" system, even though it operates using AM signals.
Amplitude modulation immunity to capture effect.
In FM demodulation the receiver tracks the modulated frequency shift of the desired carrier while discriminating against any other signal since it can only follow the deviation of one signal at a time. In AM, the receiver tracks the signal strength of the AM signal as the basis for demodulation. This allows any other signal to be tracked as just another change in amplitude. So it is possible for an AM receiver to demodulate several carriers at the same time, resulting in an audio mix.
If the signals are close but not exactly on the same frequency the mix will not only include the audio from both carriers but depending on the carrier separation a whistle might be heard as well representing the difference in the carrier frequencies. This mix can also occur when an AM carrier is received on a channel that is adjacent to the desired channel. The resulting overlap forms the high pitched whistle (about 10 Kilohertz) that can often be heard behind an AM station at night when other carriers from adjacent channels are traveling long distances due to atmospheric bounce.
Since AM assumes short term changes in the amplitude to be information, any electrical impulse will be picked up and demodulated along with the desired carrier. Hence lightning causes crashing noises when picked up by an AM radio near a storm. FM radios suppress short term changes in amplitude and are therefore much less prone to noise during storms and during reception of electrical noise impulses. 
For digital modulation schemes it has been shown that for properly implemented on-off keying/amplitude-shift keying systems, co-channel rejection can be better than for frequency-shift keying systems.

</doc>
<doc id="40845" url="http://en.wikipedia.org/wiki?curid=40845" title="Carrier">
Carrier

Carrier may refer to:

</doc>
<doc id="40846" url="http://en.wikipedia.org/wiki?curid=40846" title="Carrier sense multiple access with collision avoidance">
Carrier sense multiple access with collision avoidance

 Carrier sense multiple access with collision avoidance (CSMA/CA) in computer networking, is a network multiple access method in which carrier sensing is used, but nodes attempt to avoid collisions by transmitting only when the channel is sensed to be "idle". When they do transmit, nodes transmit their packet data in its entirety.
It is particularly important for wireless networks, where the collision detection of the alternative CSMA/CD is unreliable due to the hidden node problem.
CSMA/CA is a protocol that operates in the Data Link Layer (Layer 2) of the OSI model.
Details.
Collision avoidance is used to improve the performance of the CSMA method by attempting to divide the channel somewhat equally among all transmitting nodes within the collision domain.
Although CSMA/CA has been used in a variety of wired communication systems, it is particularly beneficial in a wireless LAN due to a common problem of multiple stations being able to see the Access Point, but not each other. This is due to differences in transmit power, and receive sensitivity, as well as distance, and location with respect to the AP. This will cause a station to not be able to 'hear' another station's broadcast. This is the so-called 'hidden node', or 'hidden station' problem. Devices utilizing 802.11 based standards can enjoy the benefits of collision avoidance (RTS / CTS handshake, also Point coordination function), although they do not do so by default. By default they use a Carrier sensing mechanism called 'exponential backoff', or (Distributed coordination function) that relies upon a station attempting to 'listen' for another station's broadcast before sending. CA, or PCF relies upon the AP (or the 'receiver' for Ad hoc networks) granting a station the exclusive right to transmit for a given period of time after requesting it (Request to Send / Clear to Send).
IEEE 802.11 RTS/CTS Exchange.
CSMA/CA can optionally be supplemented by the exchange of a Request to Send (RTS) packet sent by the sender S, and a Clear to Send (CTS) packet sent by the intended receiver R. Thus alerting all nodes within range of the sender, receiver or both, to not transmit for the duration of the main transmission. This is known as the IEEE 802.11 RTS/CTS exchange. Implementation of RTS/CTS helps to partially solve the hidden node problem that is often found in wireless networking.
Performance.
CSMA/CA performance is based largely upon the modulation technique used to transmit the data between nodes. Studies show that under ideal propagation conditions (simulations), Direct Sequence Spread Spectrum (DSSS) provides the highest throughput for all nodes on a network when used in conjunction with CSMA/CA and the IEEE 802.11 RTS/CTS exchange under light network load conditions. Frequency Hopping Spread Spectrum (FHSS) follows distantly behind DSSS with regard to throughput with a greater throughput once network load becomes substantially heavy. However, the throughput is generally the same under real world conditions due to radio propagation factors.

</doc>
<doc id="40847" url="http://en.wikipedia.org/wiki?curid=40847" title="Carrier sense multiple access with collision detection">
Carrier sense multiple access with collision detection

Carrier Sense Multiple Access With Collision Detection (CSMA/CD) is a media access control method used most notably in local area networking using early Ethernet technology. It uses a carrier sensing scheme in which a transmitting data station detects other signals while transmitting a frame, and stops transmitting that frame, transmits a jam signal, and then waits for a random time interval before trying to resend the frame.
CSMA/CD is a modification of pure carrier sense multiple access (CSMA). CSMA/CD is used to improve CSMA performance by terminating transmission as soon as a collision is detected, thus shortening the time required before a retry can be attempted.
Collision detected procedure.
This can be likened to what happens at a dinner party, where all the guests talk to each other through a common medium (the air). Before speaking, each guest politely waits for the current speaker to finish. If two guests start speaking at the same time, both stop and wait for short, random periods of time (in Ethernet, this time is measured in microseconds). The hope is that by each choosing a random period of time, both guests will not choose the same time to try to speak again, thus avoiding another collision.
Methods for collision detection are media dependent, but on an electrical bus such as 10BASE-5 or 10BASE-2, collisions can be detected by comparing transmitted data with received data or by recognizing a higher than normal signal amplitude on the bus.
Jam signal.
The jam signal is a signal that carries a 32-bit binary pattern sent by a data station to inform the other stations that they must not transmit.
The maximum jam-time is calculated as follows: The maximum allowed diameter of an Ethernet installation is limited to 232 bits. This makes a round-trip-time of 464 bits. As the slot time in Ethernet is 512 bits, the difference between slot time and round-trip-time is 48 bits (6 bytes), which is the maximum "jam-time".
This in turn means: A station noting a collision has occurred is sending a 4 to 6 byte long pattern composed of 16 1-0 bit combinations. Note: The size of this jam signal is clearly beyond the minimum allowed frame-size of 64 bytes.
The purpose of this is to ensure that any other node which may currently be receiving a frame will receive the jam signal in place of the correct 32-bit MAC CRC, this causes the other receivers to discard the frame due to a CRC error.
Applications.
CSMA/CD was used in now obsolete shared media Ethernet variants (10BASE5, 10BASE2) and in the early versions of twisted-pair Ethernet which used repeater hubs. Modern Ethernet networks, built with switches and full-duplex connections, no longer need to utilize CSMA/CD because each Ethernet segment, or collision domain, is now isolated. CSMA/CD is still supported for backwards compatibility and for half-duplex connections. IEEE Std 802.3, which defines all Ethernet variants, for historical reasons still bears the title "Carrier sense multiple access with collision detection (CSMA/CD) access method and physical layer specifications".
Variations of the concept are used in radio frequency systems that rely on frequency sharing, including Automatic Packet Reporting System.

</doc>
<doc id="40848" url="http://en.wikipedia.org/wiki?curid=40848" title="Carrier shift">
Carrier shift

In telecommunication, the term carrier shift has the following meanings: 
"Note 1:" The carrier shift results in a change in carrier power. 
"Note 2:" The carrier shift may be a shift to a higher or to a lower frequency. 
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40849" url="http://en.wikipedia.org/wiki?curid=40849" title="Carrier system">
Carrier system

In telecommunication, a carrier system (loosely, a synonym with carrier) is a multichannel telecommunications system in which a number of individual channels (e.g. data, audio, video or combination thereof) are multiplexed for transmission. The transmission occurs between nodes of a network where the circuits are multiplexed / demultiplexed by channel banks. 
In carrier systems, many different forms of multiplexing may be used, such as time-division multiplexing and frequency-division multiplexing. A cable television system is an example of a carrier system that uses frequency-division multiplexing. Many different television programs are carried simultaneously on the same coaxial cable by sending each at a different frequency. Multiple layers of multiplexing may ultimately be performed upon a given input signal; "i.e.," the output resulting from one stage of modulation may in turn be modulated. For example, in the public telephone network, many telephone calls are sent over shared trunklines by time-division multiplexing; then for long distance calls several of these channels may be sent over a communications satellite link by frequency-division multiplexing. At a given node, specified channels, groups, supergroups, etc. may be demultiplexed by add-drop multiplexers without demultiplexing the others.
History.
The purpose of carrier systems is to save money by carrying more traffic on less infrastructure. 19th century telephone systems, operating at baseband, could only carry one telephone call on each wire, hence routes with heavy traffic needed many wires.
In the 1920s, frequency-division multiplexing could carry several circuits on the same balanced wires, and by the 1930s L-carrier and similar systems carried hundreds of calls at a time on coaxial cables.
Capacity of these systems increased in the middle of the century, while in the 1950s researchers began to take seriously the possibility of saving money on the terminal equipment by using time-division multiplexing. This work led to T-carrier and similar digital systems for local use.
Due to the shorter repeater spacings required by digital systems, long-distance still used FDM until the late 1970s when optical fiber was improved to the point that digital connections became the cheapest ones for all distances, short and long.

</doc>
<doc id="40850" url="http://en.wikipedia.org/wiki?curid=40850" title="Carrier-to-receiver noise density">
Carrier-to-receiver noise density

In satellite communications, carrier-to-receiver noise density ("C"/"kT") is the ratio of the received carrier power to the receiver noise power density. It tells us whether it's possible to lock on to the carrier and if the information encoded in the signal can be retrieved, given the amount of noise present in the received signal. The carrier-to-receiver noise density ratio is usually expressed in dBHz.
The carrier-to-receiver noise density is given by
where "C" is the received carrier power in watts, "k" is Boltzmann's constant in joules per kelvin, and "T" is the receiver system noise temperature in kelvins.
The receiver noise power density, "kT", is the receiver noise power per hertz.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40851" url="http://en.wikipedia.org/wiki?curid=40851" title="Carson bandwidth rule">
Carson bandwidth rule

In telecommunication, Carson's bandwidth rule defines the approximate bandwidth requirements of communications system components for a carrier signal that is frequency modulated by a continuous or broad spectrum of frequencies rather than a single frequency. Carson's rule does not apply well when the modulating signal contains discontinuities, such as a square wave. Carson's rule originates from John Renshaw Carson's 1922 paper.
Carson's bandwidth rule is expressed by the relation formula_1 where CBR is the bandwidth requirement, formula_2 is the peak frequency deviation, and formula_3 is the highest frequency in the modulating signal. 
For example, an FM signal with 5 kHz peak deviation, and a maximum audio frequency of 3 kHz, would require an approximate bandwidth 2(5+3) = 16 kHz.
For example, standard broadcast FM has a peak deviation of 75 kHz above and below the carrier. With stereo FM, the highest modulating frequency (which combines L+R and L-R) is 53 kHz.
So most of the energy of standard stereo FM falls in an approximate bandwidth of 2(75+53) = 256 kHz.
(Geographically close FM broadcast transmitters are almost always assigned nominal center frequencies at least 500 kHz apart).
Carson's bandwidth rule is often applied to transmitters, antennas, optical sources, receivers, photodetectors, and other communications system components.
Any frequency modulated signal will have an "infinite" number of sidebands and hence an infinite bandwidth but in practice all significant sideband energy (98% or more) is concentrated within the bandwidth defined by Carson's rule. It is a useful approximation, but setting the arbitrary definition of occupied bandwidth at 98% of the power still means that the power outside the band is only about formula_4 = 17 dB less than the carrier inside, therefore Carson's Rule is of little use in spectrum planning.

</doc>
<doc id="40853" url="http://en.wikipedia.org/wiki?curid=40853" title="Cassegrain antenna">
Cassegrain antenna

In telecommunications and radar, a Cassegrain antenna is a parabolic antenna in which the feed antenna is mounted at or behind the surface of the concave main parabolic reflector dish and is aimed at a smaller convex secondary reflector suspended in front of the primary reflector. The beam of radio waves from the feed illuminates the secondary reflector, which reflects it back to the main reflector dish, which reflects it forward again to form the desired beam. The Cassegrain design is widely used in parabolic antennas, particularly in large antennas such as those in satellite ground stations, radio telescopes, and communication satellites.
Geometry.
The primary reflector is a paraboloid, while the shape of the convex secondary reflector is a hyperboloid. The geometrical condition for radiating a collimated, plane wave beam is that the feed antenna is located at the far focus of the hyperboloid, while the focus of the primary reflector coincides with the near focus of the hyperboloid. Usually the secondary reflector and the feed antenna are located on the central axis of the dish. However in "offset Cassegrain" configurations, the primary dish reflector is asymmetric, and its focus, and the secondary reflector, are located to one side of the dish, so that the secondary reflector does not partially obstruct the beam.
Advantages.
This design is an alternative to the most common parabolic antenna design, called "front feed", in which the feed antenna itself is mounted suspended in front of the dish at the focus, pointed back toward the dish. The Cassegrain design has several advantages over front feed that can justify its increased complexity: 
A disadvantage of the Cassegrain is that the feed horn(s) must have a narrower beamwidth (higher gain) to focus its radiation on the smaller secondary reflector, instead of the wider primary reflector as in front-fed dishes. The angular width the secondary reflector subtends at the feed horn is typically 10° - 15°, as opposed to 120° - 180° the main reflector subtends in a front-fed dish. Therefore the feed horn must be longer for a given wavelength.
Beam waveguide antenna.
A beam waveguide antenna is a type of complicated Cassegrain antenna with a long radio wave path to allow the feed electronics to be located at ground level. It is used in very large steerable radio telescopes and satellite ground antennas, where the feed electronics is too complicated and bulky, or requires too much maintenance and alterations, to locate on the dish; for example those using cryogenically-cooled amplifiers. The beam of incoming radio waves from the secondary reflector is reflected by additional mirrors in a long twisting path through the axes of the altazimuth mount, so the antenna can be steered without interrupting the beam, and then down through the antenna tower to a feed building at ground level.
History.
The Cassegrain antenna design was adapted from the Cassegrain telescope, a type of reflecting telescope developed around 1672 and attributed to French priest Laurent Cassegrain. The first Cassegrain antenna was invented and built in Japan in 1963 by NTT, KDDI and Mitsubishi Electric. The 20 meter I-1 antenna operated at 6.4, 4.2, and 1.7 GHz, and was used in October 1963 in the first trans-Pacific satellite television relay experiments.

</doc>
<doc id="40854" url="http://en.wikipedia.org/wiki?curid=40854" title="Cell relay">
Cell relay

In computer networking, cell relay refers to a method of statistically multiplexing small fixed-length packets, called "cells", to transport data between computers or kinds of network equipment. It is an unreliable, connection-oriented packet switched data communications protocol.
Transmission Rates.
Cell relay transmission rates usually are between 56 kbit/s and several gigabits per second. ATM, a particularly popular form of cell relay, is most commonly used for home DSL connections, which often runs between 128 kbit/s and 1.544 Mbit/s (DS1), and for high-speed backbone connections (OC-3 and faster).
Cell relay protocols have neither flow control nor error correction capability, are information-content independent, and correspond only to layers one and two of the OSI Reference Model. 
Cell relay can be used for delay- and jitter-sensitive traffic such as voice and video.
How Cell Relay Works.
Cell relay systems break variable-length user packets into groups of fixed-length cells, that add addressing and verification information. Frame length is fixed in networking hardware, based on time delay and user packet-length considerations. One user data message may be segmented over many cells. 
Cell relay systems may also carry bitstream-based data such as PDH traffic, by breaking it into streams of cells, with a lightweight synchronization and clock recovery shim. Thus cell relay systems may potentially carry any combination of stream-based and packet-based data. This is a form of statistical time division multiplexing.
Cell relay is an implementation of fast packet-switching technology that is used in connection-oriented broadband integrated services digital networks (B-ISDN, and its better-known supporting technology ATM) and connectionless IEEE 802.6 switched multi-megabit data service (SMDS). 
At any time there is information to be transmitted; the switch basically sends the data units. Connections don’t have to be negotiated like circuit switching. Channels don’t have to be allocated because channels do not exist in ATM, and on condition that there is an adequate amount of bandwidth to maintain it, there can be indefinite transmissions over the same facility.
Cell relay utilizes data cells of a persistent size. Frames are comparable to data packets; however they contrast from cells in that they may fluctuate in size based on circumstances. This type of technology is not secure for the reason that its procedures do not support error handling or data recovery. Per se, all delicate and significant transmissions may perhaps be transported faster via fixed-sized cells, which are simpler to transmit compared to variable-sized frames or packets.
Reliability.
Cell relay is extremely reliable for transporting vital data. Switching devices give the precise method to cells as each endpoint address embedded in a cell. An example of cell relay is ATM, a prevalent form utilized to transfer a cell with a fixed size of 53 bytes.
References.
Minoli, Daniel, and Michael Vitella. ATM and Cell Relay Service for Corporate Environments. New York: McGraw-Hill, 1994. Print.
Minoli, Daniel, and George Dobrowski. Principles of Signaling for Cell Relay and Frame Relay. Boston: Artech House, 1995. Print.
Minoli, Daniel, and George Dobrowski. Principles of Signaling for Cell Relay and Frame Relay. Boston: Artech House, 1995. Print.
Any Transport over MPLS - Cisco Systems." Cisco Systems, Inc. Web. 29 Nov. 2011. <http://www.cisco.com/en/US/docs/ios/12_0s/feature/guide/fsatom26.html>.
Davidson, Robert P. Broadband Networking ABCs for Managers: ATM, BISDN, Cell/frame Relay to SONET. New York: John Wiley & Sons, 1994. Print.
Conti, Marco, Enrico Gregori, and Luciano Lenzini. Metropolitan Area Networks. London: Springer, 1997. Print.

</doc>
<doc id="40858" url="http://en.wikipedia.org/wiki?curid=40858" title="Caesium standard">
Caesium standard

A caesium standard or caesium atomic clock is a primary frequency standard in which electronic transitions between the two hyperfine ground states of caesium-133 atoms are used to control the output frequency. The first caesium clock was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. 
Caesium clocks are the most accurate commercially produced time and frequency standards, and serve as the primary standard for the definition of the second in SI (the metric system). By definition, radiation produced by the transition between the two hyperfine ground states of caesium (in the absence of external influences such as the Earth's magnetic field) has a frequency of exactly 9,192,631,770 Hz. That value was chosen so that the caesium second equalled, to the limit of human measuring ability in 1960 when it was adopted, the existing standard ephemeris second based on the Earth's orbit around the Sun. Because no other measurement involving time had been as precise, the effect of the change was less than the experimental uncertainty of all existing measurements.

</doc>
<doc id="40860" url="http://en.wikipedia.org/wiki?curid=40860" title="Channel">
Channel

Channel, channels, and similar terms may refer to:

</doc>
<doc id="40861" url="http://en.wikipedia.org/wiki?curid=40861" title="Channel noise level">
Channel noise level

In telecommunications, the term channel noise level has the following meanings:
Notes.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40862" url="http://en.wikipedia.org/wiki?curid=40862" title="Channel reliability">
Channel reliability

In telecommunication, channel reliability (ChR) is the percentage of time a channel was available for use in a specified period of scheduled availability. 
Channel reliability is given by 
where "T" o is the channel total outage time, "T" s is the channel total scheduled time, and "T" a is the channel total available time.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40863" url="http://en.wikipedia.org/wiki?curid=40863" title="Channel service unit">
Channel service unit

In telecommunications, a channel service unit (CSU) is a line Bridging device for use with T-carrier that:
Common varieties.
CSUs can be categorized by the class of service they support (DS1, DS3, DDS, etc.) and by the capabilities within that class. For example, basic DS1 (T1) CSUs support loopback of each interface and will produce Alarm indication signal to the provider's NIU (Network Interface Device) in the event of loss of signal from the customer-premises equipment (CPE). More advanced units will include internal monitors of the performance of the carrier in both directions and may have test pattern generation and monitor capabilities.
Common practice.
CSUs are required by PSTN providers at digital interfaces that terminate in a DSU on the customer side. They are not used when the service terminates in a modem, such as the DSL family of service. The maintenance capabilities of the CSU provide important guidance as to whether the provider needs to dispatch a repairman to the customer location.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40864" url="http://en.wikipedia.org/wiki?curid=40864" title="Character-count integrity">
Character-count integrity

Character-count integrity is a telecommunications term for the ability of a certain link to preserve the number of characters in a message (per unit time, in the case of a user-to-user connection). Character-count integrity is not the same as character integrity, which requires that the characters delivered be, in fact, exactly the same as they were originated.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40865" url="http://en.wikipedia.org/wiki?curid=40865" title="Character interval">
Character interval

Character interval: In a communications system, the total number of unit intervals required to transmit any given character, including synchronizing, information, error checking, or control characters, but not including signals that are not associated with individual characters. 
An example of a time interval that is excluded when determining character interval is any time added between the end of a stop signal and the beginning of the next start signal to accommodate changing transmission conditions, such as a change in data signaling rate or buffering requirements. This added time is defined as a part of the intercharacter interval.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40866" url="http://en.wikipedia.org/wiki?curid=40866" title="Characteristic impedance">
Characteristic impedance

The characteristic impedance or surge impedance (usually written Z0) of a uniform transmission line is the ratio of the amplitudes of voltage and current of a single wave propagating along the line; that is, a wave travelling in one direction in the absence of reflections in the other direction. Characteristic impedance is determined by the geometry and materials of the transmission line and, for a uniform line, is not dependent on its length. The SI unit of characteristic impedance is the ohm. 
The characteristic impedance of a lossless transmission line is purely real, with no reactive component. Energy supplied by a source at one end of such a line is transmitted through the line without being dissipated in the line itself. A transmission line of finite length (lossless or lossy) that is terminated at one end with a resistor equal to the characteristic impedance appears to the source like an infinitely long transmission line and produces no reflections. 
Transmission line model.
The characteristic impedance of a transmission line is the ratio of the voltage and current of a wave travelling along the line. When the wave reaches the end of the line, in general, there will be a reflected wave which travels back along the line in the opposite direction. When this wave reaches the source, it adds to the transmitted wave and the ratio of the voltage and current at the input to the line will no longer be the characteristic impedance. This new ratio is called the input impedance. The input impedance of an infinite line is equal to the characteristic impedance since the transmitted wave is never reflected back from the end. It can be shown that an equivalent definition is: the characteristic impedance of a line is that impedance which when terminating an arbitrary length of line at its output will produce an input impedance equal to the characteristic impedance. This is so because there is no reflection on a line terminated in its own characteristic impedance.
Applying the transmission line model based on the telegrapher's equations, the general expression for the characteristic impedance of a transmission line is:
where
Although an infinite line is assumed, since all quantities are per unit length, the characteristic impedance is independent of the length of the transmission line.
The voltage and current phasors on the line are related by the characteristic impedance as:
where the superscripts formula_9 and formula_10 represent forward- and backward-traveling waves, respectively. A surge of energy on a finite transmission line will see an impedance of "Z"0 prior to any reflections arriving, hence "surge impedance" is an alternative name for characteristic impedance.
Lossless line.
For a lossless line, "R" and "G" are both zero, so the equation for characteristic impedance reduces to:
The imaginary term "j" has also canceled out, making "Z0" a real expression, and so is purely resistive.
Surge impedance loading.
In electric power transmission, the characteristic impedance of a transmission line is expressed in terms of the surge impedance loading (SIL), or natural loading, being the power loading at which reactive power is neither produced nor absorbed:
in which formula_13 is the line-to-line voltage in volts.
Loaded below its SIL, a line supplies reactive power to the system, tending to raise system voltages. Above it, the line absorbs reactive power, tending to depress the voltage. The Ferranti effect describes the voltage gain towards the remote end of a very lightly loaded (or open ended) transmission line. Underground cables normally have a very low characteristic impedance, resulting in an SIL that is typically in excess of the thermal limit of the cable. Hence a cable is almost always a source of reactive power.
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40867" url="http://en.wikipedia.org/wiki?curid=40867" title="Chip">
Chip

Chip or chips may refer to:

</doc>
<doc id="40868" url="http://en.wikipedia.org/wiki?curid=40868" title="Chirping">
Chirping

Chirping may refer to: 

</doc>
<doc id="40870" url="http://en.wikipedia.org/wiki?curid=40870" title="Circuit">
Circuit

 
Circuit may mean:
In electrical engineering
In fluid power and fluid mechanics
In physics
In mathematics and computer science
In neuroscience
In economics
In government and law
In transportation and racing
Others

</doc>
<doc id="40871" url="http://en.wikipedia.org/wiki?curid=40871" title="Circuit noise level">
Circuit noise level

Circuit noise level: At any point in a transmission system, the ratio of the circuit noise at that point to an arbitrary level chosen as a reference. 
"Note:" The circuit noise level is usually expressed in dBrn0, signifying the reading of a circuit noise meter, or in dBa0, signifying circuit noise meter reading adjusted to represent an interfering effect under specified conditions.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40872" url="http://en.wikipedia.org/wiki?curid=40872" title="Circuit reliability">
Circuit reliability

Circuit reliability (also time availability) ("CiR") is the percentage of time an electronic circuit was available for use in a specified period of scheduled availability. Circuit reliability is given by where "T" o is the circuit total outage time, "Ts" is the circuit total scheduled time, and "T" a is the circuit total available time. formula_1 
In addition, circuit reliability is the expected lifespan of operation of a functioning system under nominal conditions.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40873" url="http://en.wikipedia.org/wiki?curid=40873" title="Circuit restoration">
Circuit restoration

In telecommunication, circuit restoration is the process by which a communications circuit is established between two users after disruption or loss of the original circuit. The loss may be widespread due to a natural disaster like an ice storm or hurricane, or local by being cut underground in construction or damaged in a thunderstorm or car accident.
Circuit restoration is usually performed in accordance with planned procedures and priorities. Restoration may be effected automatically, such as by switching to a hot standby, or manually, such as by manual patching.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40874" url="http://en.wikipedia.org/wiki?curid=40874" title="Circuit switching">
Circuit switching

Circuit switching is a methodology of implementing a telecommunications network in which two network nodes establish a dedicated communications channel (circuit) through the network before the nodes may communicate. The circuit guarantees the full bandwidth of the channel and remains connected for the duration of the communication session. The circuit functions as if the nodes were physically connected as with an electrical circuit.
The defining example of a circuit-switched network is the early analog telephone network. When a call is made from one telephone to another, switches within the telephone exchanges create a continuous wire circuit between the two telephones, for as long as the call lasts.
Circuit switching contrasts with packet switching which divides the data to be transmitted into packets transmitted through the network independently. In packet switching, instead of being dedicated to one communication session at a time, network links are shared by packets from multiple competing communication sessions, resulting in the loss of the quality of service guarantees that are provided by circuit switching.
In circuit switching, the bit delay is constant during a connection, as opposed to packet switching, where packet queues may cause varying and potentially indefinitely long packet transfer delays. No circuit can be degraded by competing users because it is protected from use by other callers until the circuit is released and a new connection is set up. Even if no actual communication is taking place, the channel remains reserved and protected from competing users.
Virtual circuit switching is a packet switching technology that emulates circuit switching, in the sense that the connection is established before any packets are transferred, and packets are delivered in order.
While circuit switching is commonly used for connecting voice circuits, the concept of a dedicated path persisting between two communicating parties or nodes can be extended to signal content other than voice. Its advantage is that it provides for continuous transfer without the overhead associated with packets making maximal use of available bandwidth for that communication. Its disadvantage is that it can be relatively inefficient because unused capacity guaranteed to a connection cannot be used by other connections on the same network.
The call.
For call setup and control (and other administrative purposes), it is possible to use a separate dedicated signalling channel from the end node to the network. ISDN is one such service that uses a separate signalling channel while plain old telephone service (POTS) does not.
The method of establishing the connection and monitoring its progress and termination through the network may also utilize a separate control channel as in the case of links between telephone exchanges which use CCS7 packet-switched signalling protocol to communicate the call setup and control information and use TDM to transport the actual circuit data.
Early telephone exchanges are a suitable example of circuit switching. The subscriber would ask the operator to connect to another subscriber, whether on the same exchange or via an inter-exchange link and another operator. In any case, the end result was a physical electrical connection between the two subscribers' telephones for the duration of the call. The copper wire used for the connection could not be used to carry other calls at the same time, even if the subscribers were in fact not talking and the line was silent.
Compared to datagram packet switching.
Circuit switching contrasts with packet switching which divides the data to be transmitted into small units, called packets, transmitted through the network independently. Packet switching shares available network bandwidth between multiple communication sessions.
Multiplexing multiple telecommunications connections over the same physical conductor has been possible for a long time, but nonetheless each channel on the multiplexed link was either dedicated to one call at a time, or it was idle between calls.
In circuit switching, and virtual circuit switching, a route and bandwidth is reserved from source to destination. Circuit switching can be relatively inefficient because capacity is guaranteed on connections which are set up but are not in continuous use, but rather momentarily. However, the connection is immediately available while established.
Packet switching is the process of segmenting a message/data to be transmitted into several smaller packets. Each packet is labeled with its destination and a sequence number for ordering related packets, precluding the need for a dedicated path to help the packet find its way to its destination. Each packet is dispatched independently and each may be routed via a different path. At the destination, the original message is reassembled in the correct order, based on the packet number. Datagram packet switching networks do not require a circuit to be established and allow many pairs of nodes to communicate concurrently over the same channel.

</doc>
<doc id="40875" url="http://en.wikipedia.org/wiki?curid=40875" title="Circular polarization">
Circular polarization

In electrodynamics, circular polarization of an electromagnetic wave is a polarization in which the electric field of the passing wave does not change strength but only changes direction in a rotary manner.
In electrodynamics the strength and direction of an electric field is defined by what is called an electric field vector. In the case of a circularly polarized wave, as seen in the accompanying animation, the tip of the electric field vector, at a given point in space, describes a circle as time progresses. If the wave is frozen in time, the electric field vector of the wave describes a helix along the direction of propagation.
Circular polarization is a limiting case of the more general condition of elliptical polarization. The other special case is the easier-to-understand linear polarization.
The phenomenon of polarization arises as a consequence of the fact that light behaves as a two-dimensional transverse wave.
General description.
On the right is an illustration of the electric field vectors of a circularly polarized electromagnetic wave. The electric field vectors have a constant magnitude but their direction changes in a rotary manner. Given that this is a plane wave, each vector represents the magnitude and direction of the electric field for an entire plane that is perpendicular to the axis. Specifically, given that this is a circularly polarized plane wave, these vectors indicate that the electric field, from plane to plane, has a constant strength while its direction steadily rotates. Refer to these two images in the plane wave article to better appreciate this. This light is considered to be right-hand, clockwise circularly polarized if viewed by the receiver. Since this is an electromagnetic wave each electric field vector has a corresponding, but not illustrated, magnetic field vector that is at a right angle to the electric field vector and proportional in magnitude to it. As a result, the magnetic field vectors would trace out a second helix if displayed.
Circular polarization is often encountered in the field of optics and in this section, the electromagnetic wave will be simply referred to as light.
The nature of circular polarization and its relationship to other polarizations is often understood by thinking of the electric field as being divided into two components which are at right angles to each other. Refer to the second illustration on the right. The vertical component and its corresponding plane are illustrated in blue while the horizontal component and its corresponding plane are illustrated in green. Notice that the rightward (relative to the direction of travel) horizontal component leads the vertical component by one quarter of a wavelength. It is this quadrature phase relationship which creates the helix and causes the points of maximum magnitude of the vertical component to correspond with the points of zero magnitude of the horizontal component, and vice versa. The result of this alignment is that there are select vectors, corresponding to the helix, which exactly match the maxima of the vertical and horizontal components. (To minimize visual clutter these are the only helix vectors displayed.)
To appreciate how this quadrature phase shift corresponds to an electric field that rotates while maintaining a constant magnitude, imagine a dot traveling clockwise in a circle. Consider how the vertical and horizontal displacements of the dot, relative to the center of the circle, vary sinusoidally in time and are out of phase by one quarter of a cycle. The displacements are said to be out of phase by one quarter of a cycle because the horizontal maximum displacement (toward the left) is reached one quarter of a cycle before the vertical maximum displacement is reached. Now referring again to the illustration, imagine the center of the circle just described, traveling along the axis from the front to the back. The circling dot will trace out a helix with the displacement toward our viewing left, leading the vertical displacement. Just as the horizontal and vertical displacements of the rotating dot are out of phase by one quarter of a cycle in time, the magnitude of the horizontal and vertical components of the electric field are out of phase by one quarter of a wavelength.
The next pair of illustrations is that of left-handed, counter-clockwise circularly polarized light when viewed by the receiver. Because it is left-handed, the rightward (relative to the direction of travel) horizontal component is now "lagging" the vertical component by one quarter of a wavelength rather than leading it.
Reversal of Handedness by Phase Shift.
To convert a given handedness of polarized light to the other handedness one can use a half-wave plate. A half-wave plate shifts a given component of light one half of a wavelength relative to the component to which it is orthogonal.
Reversal of Handedness by Reflection.
The handedness of polarized light is also reversed when it is reflected off of a surface at normal incidence. Upon such reflection, the rotation of the plane of polarization of the reflected light is identical to that of the incident field. However with propagation now in the "opposite" direction, the same rotation direction that would be described as "right handed" for the incident beam, is "left-handed" for propagation in the reverse direction, and vice versa. Aside from the reversal of handedness, the ellipticity of polarization is also preserved (except in cases of reflection by a birefringent surface).
Note that this principle only holds strictly for light reflected at normal incidence. For instance, right circularly polarized light reflected from a dielectric surface at grazing incidence (an angle beyond the Brewster angle) will still emerge as right handed, but elliptically, polarized. Light reflected by a metal at non-normal incidence will generally have its ellipticity changed as well. Such situations may be solved by decomposing the incident circular (or other) polarization into components of linear polarization parallel and perpendicular to the plane of incidence, commonly denoted "p" and "s" respectively. The reflected components in the "p" and "s" linear polarizations are found by applying the Fresnel coefficients of reflection, which are generally different for those two linear polarizations. Only in the special case of normal incidence, where there is no distinction between "p" and "s", are the Fresnel coefficients for the two components identical, leading to the above property.
Conversion to and from Linear Polarization.
Circularly polarized light can be converted into linearly polarized light by passing it through a quarter-wave plate. Passing linearly polarized light through a quarter-wave plate with its axes at 45° to its polarization axis will convert it to circular polarization. In fact, this is the most common way of producing circular polarization in practice. Note that passing linearly polarized light through a quarter-wave plate at an angle "other" than 45° will generally produce elliptical polarization.
Left/right handedness conventions.
 
Circular polarization may be referred to as right-handed or left-handed, and clockwise or anti-clockwise, depending on the direction in which the electric field vector rotates. Unfortunately, two opposing historical conventions exist.
From the point of view of the source.
Using this convention, polarization is defined from the point of view of the source. When using this convention, left or right handedness is determined by pointing one's left or right thumb away from the source, in the same direction that the wave is propagating, and matching the curling of one's fingers to the direction of the temporal rotation of the field at a given point in space. When determining if the wave is clockwise or anti-clockwise circularly polarized, one again takes the point of view of the source, and while looking away from the source and in the same direction of the wave’s propagation, one observes the direction of the field’s temporal rotation.
Using this convention, the electric field vector of a right handed circularly polarized wave is as follows:
formula_1
As a specific example, refer to the circularly polarized wave in the first animation. Using this convention that wave is defined as right-handed because when one points one's right thumb in the same direction of the wave’s propagation, the fingers of that hand curl in the same direction of the field’s temporal rotation. It is considered clockwise circularly polarized because from the point of view of the source, looking in the same direction of the wave’s propagation, the field rotates in the clockwise direction. The second animation is that of left-handed or anti-clockwise light using this same convention.
This convention is in conformity with the Institute of Electrical and Electronics Engineers (IEEE) standard and as a result it is generally used in the engineering community.
Quantum physicists also use this convention of handedness because it is consistent with their convention of handedness for a particle’s spin.
Radio astronomers also use this convention in accordance with an International Astronomical Union (IAU) resolution made in 1973.
From the point of view of the receiver.
In this alternative convention, polarization is defined from the point of view of the receiver. Using this convention, left or right handedness is determined by pointing one’s left or right thumb toward the source, against the direction of propagation, and then matching the curling of one's fingers to the temporal rotation of the field.
When using this convention, in contrast to the other convention, the defined handedness of the wave matches the handedness of the screw type nature of the field in space. Specifically, if one freezes a right-handed wave in time, when one curls the fingers of one’s right hand around the helix, the thumb will point in the direction which the helix progresses given that sense of rotation. Note that it is the nature of all screws and helices that it does not matter in which direction you point your thumb when determining its handedness.
When determining if the wave is clockwise or anti-clockwise circularly polarized, one again takes the point of view of the receiver and, while looking toward the source, against the direction of propagation, one observes the direction of the field’s temporal rotation.
Just as in the other convention, right-handedness corresponds to a clockwise rotation and left-handedness corresponds to an anti-clockwise rotation.
Many optics textbooks use this second convention.
Uses of the two conventions.
As stated earlier, there is significant confusion with regards to these two conventions. As a general rule the engineering, quantum physics, and radio astronomy communities use the first convention where the wave is observed from the point of view of the source. In many physics textbooks dealing with optics the second convention is used where the light is observed from the point of view of the receiver.
To avoid confusion, it is good practice to specify “as defined from the point of view of the source” or "as defined from the point of view of the receiver" when discussing polarization matters.
The archive of the proposes two contradictory conventions of handedness.
FM radio.
The term "circular polarization" is often used erroneously to describe mixed polarity signals used mostly in FM radio (87.5 to 108.0 MHz in the USA), where a vertical and a horizontal component are propagated simultaneously by a single or a combined array. This has the effect of producing greater penetration into buildings and difficult reception areas than a signal with just one plane of polarization. This would be an instance where the polarization would more appropriately be called random polarization because the polarization at a receiver, although constant, will vary depending on the direction from the transmitter and other factors in the transmitting antenna design. See Stokes parameters.
The term "FM radio" above refers to broadcast radio, not 2-way radio (more properly called Land Mobile Radio), which uses vertical polarization almost exclusively.
Circular dichroism.
Circular dichroism (CD) is the differential absorption of left- and right-handed circularly polarized light. Circular dichroism is the basis of a form of spectroscopy that can be used to determine the optical isomerism and secondary structure of molecules.
In general, this phenomenon will be exhibited in absorption bands of any optically active molecule. As a consequence, circular dichroism is exhibited by most biological molecules, because of the dextrorotary (e.g. some sugars) and levorotary (e.g. some amino acids) molecules they contain. Noteworthy as well is that a secondary structure will also impart a distinct CD to its respective molecules. Therefore, the alpha helix, beta sheet and random coil regions of proteins and the double helix of nucleic acids have CD spectral signatures representative of their structures.
Also, under the right conditions, even non-chiral molecules will exhibit magnetic circular dichroism, that is, circular dichroism induced by a magnetic field.
Circularly polarized luminescence.
"Circularly polarized luminescence" (CPL) can occur when either a luminophore or an ensemble of luminophores is chiral. The extent to which emissions are polarized is quantified in the same way it is for circular dichroism, in terms of the "dissymmetry factor", also sometimes referred to as the "anisotropy factor". This value is given by
where formula_3 corresponds to the quantum yield of left-handed circularly polarized light, and formula_4 to that of right-handed light. The maximum absolute value of "g"em, corresponding to purely left- or right-handed circular polarization, is therefore 2. Meanwhile the smallest absolute value that "g"em can achieve, corresponding to linearly polarized or unpolarized light, is zero.
Mathematical description.
The classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is
where k is the wavenumber,
is the angular frequency of the wave, formula_8 is an orthogonal formula_9 matrix whose columns span the transverse x-y plane and formula_10 is the speed of light.
Here
is the amplitude of the field and
is the Jones vector in the x-y plane.
If formula_13 is rotated by formula_14 radians with respect to formula_15 and the x amplitude equals the y amplitude the wave is circularly polarized. The Jones vector is
where the plus sign indicates left circular polarization and the minus sign indicates right circular polarization. In the case of circular polarization, the electric field vector of constant magnitude rotates in the x-y plane.
If basis vectors are defined such that
and
then the polarization state can be written in the "R-L basis" as
where
and
Antennas.
A number of different types of antenna elements can be utilized to produce circularly polarized (or nearly so) radiation; following Balanis, one can use "dipole elements":
"two crossed dipoles provide the two orthogonal field components... If the two dipoles are identical, the field intensity of each along zenith ... would be of the same intensity. Also, if the two dipoles were fed with a 90° degree time-phase difference (phase quadrature), the polarization along zenith would be circular... One way to obtain the 90° time-phase difference between the two orthogonal field components, radiated respectively by the two dipoles, is by feeding one of the two dipoles with a transmission line which is 1/4 wavelength longer or shorter than that of the other", p.80;
or "helical elements":
"To achieve circular polarization [in axial or end-fire mode] ... the circumference "C" of the helix must be ... with "C"/wavelength = 1 near optimum, and the spacing about "S" = wavelength/4." p.571;
or "patch elements":
"circular and elliptical polarizations can be obtained using various feed arrangements or slight modifications made to the elements... Circular polarization can be obtained if two orthogonal modes are excited with a 90° time-phase difference between them. This can be accomplished by adjusting the physical dimensions of the patch ... For a square patch element, the easiest way to excite ideally circular polarization is to feed the element at two adjacent edges ... The quadrature phase difference is obtained by feeding the element with a 90° power divider", p.859.
Quantum mechanics.
In the quantum mechanical view, light is composed of photons. Polarization is a manifestation of the intrinsic angular momentum (the spin) of the photon. More specifically, in quantum mechanics the direction of spin of a photon is tied to the handedness of the circularly polarized light and the spin of a beam of photons is similar to the spin of a beam of particles, such as electrons.
In nature.
Only a few mechanisms in nature are known to systematically produce circularly polarized light. In 1911, Albert Abraham Michelson discovered that light reflected from the golden scarab beetle "Chrysina resplendens" is preferentially left-polarized. Since then, circular polarization has been measured in several other scarab beetles like "Chrysina gloriosa", as well as some crustaceans such as the mantis shrimp. In these cases, the underlying mechanism is the molecular-level helicity of the chitinous cuticle.
The bioluminescence of the larvae of fireflies is also circularly polarized, as reported in 1980 for the species "Photuris lucicrescens" and "Photuris versicolor". For fireflies, it is more difficult to find a microscopic explanation for the polarization, because the left and right lanterns of the larvae were found to emit polarized light of opposite senses. The authors suggest that the light begins with a linear polarization due to inhomogeneties inside aligned photocytes, and it picks up circular polarization while passing through linearly birefringent tissue.
Water-air interfaces provide another source of circular polarization. Sunlight that gets scattered back up towards the surface is linearly polarized. If this light is then totally internally reflected back down, its vertical component undergoes a phase shift. To an underwater observer looking up, the faint light outside Snell's window therefore is (partially) circularly polarized.
Weaker sources of circular polarization in nature include multiple scattering by linear polarizers, as in the circular polarization of starlight, and selective absorption by circularly dichroic media.
Two species of Mantis Shrimp have been reported to be able to detect circular polarized light.
Starlight.
The circular polarization of starlight has been observed to be a function of the linear polarization of starlight.
Starlight becomes partially linearly polarized by scattering from elongated interstellar dust grains whose long axes tend to be oriented perpendicular to the galactic magnetic field. According to the Davis-Greenstein mechanism, the grains spin rapidly with their rotation axis along the magnetic field. Light polarized along the direction of the magnetic field perpendicular to the line of sight is transmitted, while light polarized in the plane defined by the rotating grain is blocked. Thus the polarization direction can be used to map out the galactic magnetic field. The degree of polarization is on the order of 1.5% for stars at 1000 parsecs distance.
Normally, a much smaller fraction of circular polarization is found in starlight. Serkowski, Mathewson and Ford measured the polarization of 180 stars in UBVR filters. They found a maximum fractional circular polarization of formula_23, in the R filter.
The explanation is that the interstellar medium is optically thin. Starlight traveling through a kiloparsec column undergoes about a magnitude of extinction, so that the optical depth ~ 1. An optical depth of 1 corresponds to a mean free path, which is the distance, on average that a photon travels before scattering from a dust grain. So on average, a starlight photon is scattered from a single interstellar grain; multiple scattering (which produces circular polarization) is much less likely. Observationally, the linear polarization fraction p ~ 0.015 from a single scattering; circular polarization from multiple scattering goes as formula_24, so we expect a circularly polarized fraction of formula_25.
Light from early-type stars has very little intrinsic polarization. Kemp et al. measured the optical polarization of the Sun at sensitivity of formula_26; they found upper limits of formula_27 for both formula_28 (fraction of linear polarization) and formula_29 (fraction of circular polarization).
The interstellar medium can produce circularly polarized (CP) light from unpolarized light by sequential scattering from elongated interstellar grains aligned in different directions. One possibility is twisted grain alignment along the line of sight due to variation in the galactic magnetic field; another is the line of sight passes through multiple clouds. For these mechanisms the maximum expected CP fraction is formula_30, where formula_28 is the fraction of linearly polarized (LP) light. Kemp & Wolstencroft found CP in six early-type stars (no intrinsic polarization), which they were able to attribute to the first mechanism mentioned above. In all cases, formula_32 in blue light.
Martin showed that the interstellar medium can convert LP light to CP by scattering from partially aligned interstellar grains having a complex index of refraction. This effect was observed for light from the Crab Nebula by Martin, Illing and Angel.
An optically thick circumstellar environment can potentially produce much larger CP than the interstellar medium. Martin suggested that LP light can become CP near a star by multiple scattering in an optically thick asymmetric circumstellar dust cloud. This mechanism was invoked by Bastien, Robert and Nadeau, to explain the CP measured in 6 T-Tauri stars at a wavelength of 768 nm. They found a maximum CP of formula_33. Serkowski measured CP of formula_34 for the red supergiant NML Cygni and formula_35 in the long period variable M star VY Canis Majoris in the H band, ascribing the CP to multiple scattering in circumstellar envelopes. Chrysostomou et al. found CP with q of up to 0.17 in the Orion OMC-1 star-forming region, and explained it by reflection of starlight from aligned oblate grains in the dusty nebula.
Circular polarization of zodiacal light and Milky Way diffuse galactic light was measured at wavelength of 550 nm by Wolstencroft and Kemp. They found values of formula_36, which is higher than for ordinary stars, presumably because of multiple scattering from dust grains.

</doc>
<doc id="40876" url="http://en.wikipedia.org/wiki?curid=40876" title="Circulator">
Circulator

A circulator is a passive non-reciprocal three- or four-port device, in which a microwave or radio frequency signal entering any port is transmitted to the next port in rotation (only). A "port" in this context is a point where an external waveguide or transmission line (such as a microstrip line or a coaxial cable), connects to the device. For a three-port circulator, a signal applied to port 1 only comes out of port 2; a signal applied to port 2 only comes out of port 3; a signal applied to port 3 only comes out of port 1, so to within a phase-factor, the scattering matrix for an ideal three-port circulator is
Types.
Circulators fall into two main classes: 4-port waveguide circulators based on Faraday rotation of waves propagating in a magnetised material, and 3-port "Y-junction" circulators based on cancellation of waves propagating over two different paths near a magnetised material. Waveguide circulators may be of either type, while more compact devices based on striplines are of the 3-port type. Sometimes two or more Y-junctions are combined in a single component to give four or more ports, but these differ in behaviour from a true 4-port circulator. Radio frequency circulators are composed of magnetised ferrite materials. A permanent magnet produces the magnetic flux through the waveguide. Ferrimagnetic garnet crystal is used in optical circulators.
In 2014 researchers at University of Texas at Austin reported the development of a tunable solid state resonator based circulator that does not rely on magnetic materials and thus could be space efficient at all frequencies. With further development, such devices could be embedded directly in conventional integrated circuits and tuned to whatever frequency is required in real-time. 
Circulators exist for many frequency bands, ranging from VHF up to optical frequencies, the latter being used in optical fiber networks. At frequencies much below VHF, ferrite circulators become impractically large. It is however possible to simulate circulator behaviour all the way down to d.c. using op-amp circuits. Unlike ferrite circulators, these "active circulators" are not lossless passive devices but require a supply of power to run. Also the power handling capability and linearity and signal to noise ratio of transistor-based circulators is not as high as those made from ferrites. Until the development of resonator based circulators it seemed that transistors were the only (space efficient) solution for low frequencies.
Applications.
Isolator.
When one port of a three-port circulator is terminated in a matched load, it can be used as an "isolator", since a signal can travel in only one direction between the remaining ports. An isolator is used to shield equipment on its input side from the effects of conditions on its output side; for example, to prevent a microwave source being detuned by a mismatched load.
Duplexer.
In radar, circulators are used as a type of duplexer, to route signals from the transmitter to the antenna and from the antenna to the receiver, without allowing signals to pass directly from transmitter to receiver. The alternative type of duplexer is a "transmit-receive switch" ("TR switch") that alternates between connecting the antenna to the transmitter and to the receiver. The use of chirped pulses and a high dynamic range may lead to temporal overlap of the sent and received pulses, however, requiring a circulator for this function.
Reflection amplifier.
A "reflection amplifier" is a type of microwave amplifier circuit utilizing negative resistance diodes such as tunnel diodes and Gunn diodes. Negative resistance diodes can amplify signals, and often perform better at microwave frequencies than two-port devices. However since the diode is a one-port (two terminal) device, a nonreciprocal component is needed to separate the outgoing amplified signal from the incoming input signal. By using a 3-port circulator with the signal input connected to one port, the biased diode connected to a second, and the output load connected to the third, the output and input can be uncoupled.

</doc>
<doc id="40877" url="http://en.wikipedia.org/wiki?curid=40877" title="Cladding">
Cladding

Cladding of one with another. It may refer to the following:

</doc>
<doc id="40878" url="http://en.wikipedia.org/wiki?curid=40878" title="Cladding mode">
Cladding mode

In fiber optics, a cladding mode is a mode that is confined to the cladding of an optical fiber by virtue of the fact that the cladding has a higher refractive index than the surrounding medium, which is either air or the primary polymer overcoat. These modes are generally undesired.
Modern fibers have a primary polymer overcoat with a refractive index that is slightly higher than that of the cladding, so that light propagating in the cladding is rapidly attenuated and disappears after only a few centimeters of propagation. An exception to this is double-clad fiber, which is designed to support a mode in its inner cladding, as well as one in its core.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40879" url="http://en.wikipedia.org/wiki?curid=40879" title="Clearing">
Clearing

Clearing may refer to:

</doc>
<doc id="40881" url="http://en.wikipedia.org/wiki?curid=40881" title="Thomas L. Cleave">
Thomas L. Cleave

Thomas Latimer (Peter) Cleave (1906–1983) was a surgeon captain who researched the negative health effects of consuming refined carbohydrate (notably sugar and white flour) which would not have been available during early human evolution. Known as `Peter' to his friends and colleagues, Cleave was born in Exeter in 1906, and educated at Clifton College. Between 1922-27, he attended medical schools at the Bristol Royal Infirmary, and St Mary's Hospital, London, London, achieving MRCS and LRCP.
Military career.
At Bristol, one of his teachers was Rendle Short, who had proposed that appendicitis is caused by a lack of cellulose in the diet (it is worth noting, perhaps, from a biographical perspective, that Cleave's sister had died at the age of eight years from a perforated appendix). Charles Darwin's writings provided the intellectual framework to Cleave's lifelong engagement with the relationship between diet and health, built upon the premise that the human body is ill-adapted to the diet of modern (western) man.
In this context, he considered refined carbohydrates (white flour and sugar) to be the most transformed food, and therefore the most dangerous. After completing his medical training, Cleave entered the Royal Navy in 1927 as Surgeon Lieutenant.
Between 1938-1940, he served as Medical Specialist at RN Hospital, Hong Kong. It was during his war service, in 1941, whilst on the battleship King George V, that he acquired his naval nickname `the bran man' when he had sacks of bran brought on board to combat the common occurrence of constipation amongst sailors.
Following war service, he worked at Royal Naval Hospitals in Chatham (1945–1948), Malta (1949–1951) and Plymouth (1952–1953). He retired from the Royal Navy in 1962 as Surgeon Captain, having finished his naval career as Director of Medical Research at the RN Medical School
Post-military career.
In 1969 Dr. Cleave brought public attention to the low amount of dietary fiber in modern diets that had become rich in processed ingredients. His work was bolstered by the supporting work of Dr. Denis Burkitt.
Awards and Honors.
Dr. Cleave was a 2009 inductee into the Orthomolecular Medicine Hall of Fame.

</doc>
<doc id="40882" url="http://en.wikipedia.org/wiki?curid=40882" title="Clipping">
Clipping

Clipping may refer to:

</doc>
<doc id="40883" url="http://en.wikipedia.org/wiki?curid=40883" title="Closed captioning">
Closed captioning

Closed captioning (CC) and subtitling are both processes of displaying text on a television, video screen, or other visual display to provide additional or interpretive information. Both are essentially the same and typically used as a transcription of the audio portion of a program as it occurs (either verbatim or in edited form), sometimes including descriptions of non-speech elements. Other uses have been to provide a textual alternative language translation of a presentation's primary audio language which is usually burned-in (or "open") to the video and not selectable (or "closed"). HTML5 defines subtitles as a "transcription or translation of the dialogue ... when sound is available but not understood" by the viewer (for example, dialogue in a foreign language) and captions as a "transcription or translation of the dialogue, sound effects, relevant musical cues, and other relevant audio information ... when sound is unavailable or not clearly audible" (for example, when audio is muted or the viewer is hearing impaired).
Terminology.
The term "closed" (versus "open") indicates that the captions are not visible until activated by the viewer, usually via the remote control or menu option. "Open", "burned-in", "baked on", or "hard-coded" captions are visible to all viewers.
Most of the world does not distinguish captions from subtitles. In the United States and Canada, however, these terms do have different meanings. "Subtitles" assume the viewer can hear but cannot understand the language or accent, or the speech is not entirely clear, so they transcribe only dialogue and some on-screen text. "Captions" aim to describe to the deaf and hard of hearing all significant audio content — spoken dialogue and non-speech information such as the identity of speakers and, occasionally, their manner of speaking – along with any significant music or sound effects using words or symbols. Also the term "closed caption" has come to be used to also refer to the North American EIA-608 encoding that is used with NTSC-compatible video.
The United Kingdom, Ireland, and most other countries do not distinguish between subtitles and closed captions, and use "subtitles" as the general term—the equivalent of "captioning" is usually referred to as "subtitles for the hard of hearing". Their presence is referenced on screen by notation which says "Subtitles", or previously "Subtitles 888" or just "888" (the latter two are in reference to the conventional teletext channel for captions), which is why the term "subtitle" is also used to refer to the Ceefax-based Teletext encoding that is used with PAL-compatible video. The term "subtitle" has been replaced with "caption" in a number of PAL markets that still use Teletext such as Australia and New Zealand that purchase large amounts of imported US material with much of that video having had the US CC logo already superimposed over the start of it. In New Zealand, broadcasters superimpose an ear logo with a line through it that represents "Subtitles for the hard of hearing" even though they are currently referred to as captions. In the UK, modern digital television services have subtitles for the majority of programs, so it is no longer necessary to highlight which have captioning and which do not.
History.
Open captioning.
Regular open captioned broadcasts began on PBS's "The French Chef" in 1972. WGBH began open captioning of "Zoom", "ABC World News Tonight", and "Once Upon a Classic" shortly thereafter.
Technical development of closed captioning.
Closed captioning was first demonstrated at the First National Conference on Television for the Hearing Impaired in Nashville, Tennessee in
1971. A second demonstration of closed captioning was held at Gallaudet College (now Gallaudet University) on February 15, 1972 where ABC and the National Bureau of Standards demonstrated closed captions embedded within a normal broadcast of "The Mod Squad".
The closed captioning system was successfully encoded and broadcast in 1973 with the cooperation of PBS station WETA. As a result of these tests, the FCC in 1976 set aside line 21 for the transmission of closed captions. PBS engineers then developed the caption editing consoles that would be used to caption prerecorded programs.
Real-time captioning, a process for captioning live broadcasts, was developed by the National Captioning Institute in 1982. In real-time captioning, court reporters trained to write at speeds of over 225 words per minute give viewers instantaneous access to live news, sports and entertainment. As a result, the viewer sees the captions within two to three seconds of the words being spoken.
Major US producers of captions are WGBH-TV, VITAC, CaptionMax and the National Captioning Institute. In the UK and Australasia, Red Bee Media, and are the major vendors.
Full-scale closed captioning.
The National Captioning Institute was created in 1979 in order to get the cooperation of the commercial television networks.
The first use of regularly scheduled uses of closed captioning on American television occurred on March 16, 1980. Sears had developed and sold the Telecaption adapter, a decoding unit that could be connected to a standard television set. The first programs seen with captioning were a "Disney's Wonderful World" presentation of the film "Son of Flubber" on NBC, an "ABC Sunday Night Movie" airing of "Semi-Tough", and "Masterpiece Theatre" on PBS.
Legislative development in the U.S..
Until the passage of the Television Decoder Circuitry Act of 1990, television captioning was performed by a set-top box manufactured by Sanyo Electric and marketed by The National Captioning Institute (NCI). (At that time a set-top decoder cost about as much as a TV set itself, approximately $200.) Through discussions with the manufacturer it was established that the appropriate circuitry integrated into the television set would be less expensive than the stand-alone box, and Ronald May, then a Sanyo employee, provided the expert witness testimony on behalf of Sanyo and Gallaudet University in support of the passage of the bill. On January 23, 1991, the Television Decoder Circuitry Act of 1990 was passed by US Congress. This Act gave the Federal Communications Commission (FCC) power to enact rules on the implementation of Closed Captioning. This Act required all analog television receivers with screens of at least 13 inches or greater, either sold or manufactured, to have the ability to display closed captioning by July 1, 1993.
Also in 1990, the Americans with Disabilities Act (ADA) was passed to ensure equal opportunity for persons with disabilities. The ADA prohibits discrimination against persons with disabilities in public accommodations or commercial facilities. Title III of the ADA requires that public facilities, such as hospitals, bars, shopping centers and museums (but not movie theaters), provide access to verbal information on televisions, films or slide shows.
The Telecommunications Act of 1996 expanded on the Decoder Circuity Act to place the same requirements on digital television receivers by July 1, 2002. All TV programming distributors in the U.S. are required to provide closed captions for Spanish language video programming as of January 1, 2010.
A bill, H.R. 3101, the Twenty-First Century Communications and Video Accessibility Act of 2010, was passed by the United States House of Representatives in July 2010. A similar bill, S. 3304, with the same name was passed by the United States Senate on August 5, 2010, by the House of Representatives on September 28, 2010, and was signed by President Barack Obama on October 8, 2010. The Act requires, in part, for ATSC-decoding set-top box remotes to have a button to turn on or off the closed captioning in the output signal. It also requires broadcasters to provide captioning for television programs redistributed on the Internet.
On February 20, 2014, the FCC unanimously approved the implementation of quality standards for closed captioning, addressing accuracy, timing, completeness, and placement. This is the first time the FCC has addressed quality issues in captions.
Legislative development in Australia.
The government of Australia provided seed funding in 1981 for the establishment of the Australian Caption Centre (ACC) and the purchase of equipment. Captioning by the ACC commenced in 1982 and a further grant from the Australian government enabled the ACC to achieve and maintain financial self-sufficiency. The ACC, now known as Media Access Australia, sold its commercial captioning division to Red Bee Media in December 2005. Red Bee Media continues to provide captioning services in Australia today.
Funding development in New Zealand.
In 1981, TVNZ held a telethon to raise funds for Teletext encoding equipment used for the creation and editing of text based broadcast services for the deaf. The service came into use in 1984 with caption creation and importing paid for as part of the public broadcasting fee until the creation of the NZ On Air tax payer fund which is used to provide captioning for NZ On Air content, TVNZ news shows and conversion of EIA-608 US captions to the preferred EBU STL format for only TV one, TV 2 and TV 3 with archived captions available to FOUR and select Sky programming. During the second half of 2012, TV3 and FOUR began providing non-Teletext DVB image based captions on their HD service and used the same format on the satellite service which has since caused major timing issues in relation to server load and the loss of captions from most SD DVB-S receivers such as the ones Sky Television provides their customers. As of April 2, 2013 only the Teletext page 801 caption service will remain in use with the informational Teletext non-caption content being discontinued.
Application.
Closed captions were created for deaf or hard of hearing individuals to assist in comprehension. They can also be used as a tool by those learning to read, learning to speak a non-native language, or in an environment where the audio is difficult to hear or is intentionally muted. Captions can also be used by viewers who simply wish to read a transcript along with the program audio.
In the United States, the National Captioning Institute noted that English as a foreign or second language (ESL) learners were the largest group buying decoders in the late 1980s and early 1990s before built-in decoders became a standard feature of US television sets. This suggested that the largest audience of closed captioning was people whose native language was not English. In the United Kingdom, of 7.5 million people using TV subtitles (closed captioning), 6 million have no hearing impairment.
Closed captions are also used in public environments, such as bars and restaurants, where patrons may not be able to hear over the background noise, or where multiple televisions are displaying different programs. In addition, online videos may be treated through digital processing of their audio content by various robotic algorithms (robots). Multiple chains of errors are the result. When a video is truly and accurately transcribed, then the closed-captioning publication serves a useful purpose, and the content is available for search engines to index and make available to users on the internet.
Some television sets can be set to automatically turn captioning on when the volume is muted.
Television and video.
For "live" programs, spoken words comprising the television program's soundtrack are transcribed by a human operator (a speech-to-text reporter) using stenotype or stenomask type of machines, whose phonetic output is instantly translated into text by a computer and displayed on the screen. This technique was developed in the 1970s as an initiative of the BBC's Ceefax teletext service. In collaboration with the BBC, a university student took on the research project of writing the first phonetics-to-text conversion program for this purpose. Sometimes, the captions of live broadcasts, like news bulletins, sports events, live entertainment shows, and other live shows fall behind by a few seconds. This delay is because the machine does not know what the person is going to say next, so after the person on the show says the sentence, the captions appear. Automatic computer speech recognition now works well when trained to recognize a single voice, and so since 2003, the BBC does live subtitling by having someone re-speak what is being broadcast. Live captioning is also a form of real-time text. Meanwhile, sport events on channels like ESPN are using court reporters, using a special (steno) keyboard and individually constructed "dictionaries."
In some cases, the transcript is available beforehand and captions are simply displayed during the program after being edited. For programs that have a mix of pre-prepared and live content, such as news bulletins, a combination of the above techniques is used.
For prerecorded programs, commercials, and home videos, audio is transcribed and captions are prepared, positioned, and timed in advance.
For all types of NTSC programming, captions are "encoded" into line 21 of the vertical blanking interval – a part of the TV picture that sits just above the visible portion and is usually unseen. For ATSC (digital television) programming, three streams are encoded in the video: two are backward compatible "line 21" captions, and the third is a set of up to 63 additional caption streams encoded in EIA-708 format.
Captioning is modulated and stored differently in PAL and SECAM 625 line 25 frame countries, where teletext is used rather than in EIA-608, but the methods of preparation and the line 21 field used are similar. For home Betamax and VHS videotapes, a shift down of this line 21 field must be done due to the greater number of VBI lines used in 625 line PAL countries, though only a small minority of European PAL VHS machines support this (or any) format for closed caption recording. Like all teletext fields, teletext captions can't be stored by a standard 625 line VHS recorder (due to the lack of field shifting support), they are available on all professional S-VHS recordings due to all fields being recorded. Recorded Teletext caption fields also suffer from a higher number of caption errors due to increased number of bits and a low SNR especially on low bandwidth VHS. This is why Teletext captions used to be stored separately on floppy disk to the analogue master tape. DVDs have their own system for subtitles and/or captions that is digitally inserted in the data stream and encoded on playback in video field lines.
For older televisions, a set-top box or other decoder is usually required. In the US, since the passage of the Television Decoder Circuitry Act, manufacturers of most television receivers sold have been required to include closed captioning display capability. High-definition TV sets, receivers, and tuner cards are also covered, though the technical specifications are different (high-definition display screens, as opposed to high-definition TVs, may lack captioning). Canada has no similar law, but receives the same sets as the US in most cases.
During transmission, single byte errors can be replaced by a white space which can appear at the beginning of the program. More byte errors during EIA-608 transmission can affect the screen momentarily, by defaulting to a real-time mode such as the "roll up" style, type random letters on screen, and then revert to normal. Uncorrectable byte errors within the teletext page header will cause whole captions to be dropped. EIA-608 due to using only two characters per video frame sends these captions ahead of time storing them in a second buffer awaiting a command to display them, Teletext sends these in real-time.
The use of capitalization varies between caption provider, most providers caption use capitalize all words, while providers such as WGBH and non-US providers prefer to use mixed case letters.
There are two main styles of line 21 closed captioning:
Caption formatting.
TVNZ Access Services and Red Bee Media for BBC and Australia example:
UK IMS for ITV and Sky example:
US WGBH Access Services example:
US National Captioning Institute example:
US other provider example:
US in-house real time roll-up example:
non-US in-house real time roll-up example:
Syntax.
For real time captioning done outside of captioning facilities, the following syntax is used.
Styles of syntax that are used by various captioning producers:
Technical aspects.
There were many shortcomings in the original Line 21 specification from a typographic standpoint, since, for example, it lacked many of the characters required for captioning in languages other than English. Since that time, the core Line 21 character set has been expanded to include quite a few more characters, handling most requirements for languages common in North and South America such as French, Spanish, and Portuguese, though those extended characters are not required in all decoders and are thus unreliable in everyday use. The problem has been almost eliminated with a market specific full set of Western European characters and a private adopted Norpak extension for South Korean and Japanese markets. The full EIA-708 standard for digital television has worldwide character set support, but there has been little use of it due to EBU Teletext dominating DVB countries, which has its own extended character sets.
Captions are often edited to make them easier to read and to reduce the amount of text displayed onscreen. This editing can be very minor, with only a few occasional unimportant missed lines, to severe, where virtually every line spoken by the actors is condensed. The measure used to guide this editing is words per minute, commonly varying from 180 to 300, depending on the type of program. Offensive words are also captioned, but if the program is censored for TV broadcast, the broadcaster might not have arranged for the captioning to be edited or censored also. The "TV Guardian", a television set top box, is available to parents who wish to censor offensive language of programs–the video signal is fed into the box and if it detects an offensive word in the captioning, the audio signal is bleeped or muted for that period of time.
Caption channels.
The Line 21 data stream can consist of data from several data channels multiplexed together. Odd field 1 can have four data channels: two separate synchronized captions (CC1, CC2) with caption related text, such as web site URLs (T1, T2). Even field 2 can have five additional data channels: two separate synchronized captions (CC3, CC4) with caption related text (T3, T4), and Extended Data Services (XDS) for Now/Next EPG details. XDS data structure is defined in CEA–608.
As CC1 and CC2 share bandwidth, if there is a lot of data in CC1, there will be little room for CC2 data and is generally only used for the primary audio captions. Similarly CC3 and CC4 share the second even field of line 21. Since some early caption decoders supported only single field decoding of CC1 and CC2, captions for SAP in a second language were often placed in CC2. This led to bandwidth problems, however, and the current U.S. Federal Communications Commission (FCC) recommendation is that bilingual programming should have the second caption language in CC3. Many spanish television networks such as Univision and Telemundo, for example, provides English subtitles for many of its Spanish programs in CC3. Canadian broadcasters use CC3 for French translated SAPs, which is also a similar practice in South Korea and Japan.
Ceefax and Teletext can have a larger number of captions for other languages due to the use of multiple VBI lines. However only European countries used a second subtitle page for second language audio tracks where either the NICAM dual mono or Zweikanalton were used.
HDTV interoperability issues.
Americas.
The US ATSC digital television system originally specified two different kinds of closed captioning datastream standards—the original analog compatible (available by Line 21) and the more modern digital only CEA-708 formats are delivered within the video stream. The US FCC mandates that broadcasters deliver (and generate, if necessary) both datastream formats with the CEA-708 format merely a conversion of the Line 21 format. The Canadian CRTC has not mandated that broadcasters either broadcast both datastream formats or exclusively in one format. Most broadcasters and networks to avoid large conversion cost outlays just provide EIA-608 captions along with a transcoded CEA-708 version encapsulated within CEA-708 packets.
Incompatibility issues with digital TV.
Many viewers find that when they acquire a digital television or set-top box they are unable to view closed caption (CC) information, even though the broadcaster is sending it and the TV is able to display it.
Originally, CC information was included in the picture ("line 21") via a composite video input, but there is no equivalent capability in the HDTV 720p/1080i interconnects (such as DVI, HDMI or component video) between the display and a "source". A "source", in this case, can be a DVD player or a terrestrial or cable digital television receiver. When CC information is encoded in the MPEG-2 data stream, only the device that decodes the MPEG-2 data (a source) has access to the closed caption information; there is no standard for transmitting the CC information to a display monitor separately. Thus, if there is CC information, the source device needs to overlay the CC information on the picture prior to transmitting to the display over the interconnect's video output.
Many source devices do not have the ability to overlay CC information, for controlling the CC overlay can be complicated. For example, the Motorola DCT-5xxx and -6xxx cable set-top receivers have the ability to decode CC information located on the MPEG-2 stream and overlay it on the picture, but turning CC on and off requires turning off the unit and going into a special setup menu (it is not on the standard configuration menu and it cannot be controlled using the remote). Historically, DVD players, VCRs and set-top tuners did not need to do this overlaying since they simply passed this information on to the TV, and they are not mandated to perform this overlaying.
Many modern digital television receivers can be directly connected to cables, but often cannot receive scrambled channels that the user is paying for. Thus, the lack of a standard way of sending CC information between components, along with the lack of a mandate to add this information to a picture, results in CC being unavailable to many hard-of-hearing and deaf users.
Europe/Australia.
The EBU Ceefax based teletext systems are the source for closed captioning signals, thus when teletext is embedded into DVB-T or DVB-S the closed captioning signal is included. However, for DVB-T and DVB-S, it is not necessary for a teletext page signal to also be present (ITV1, for example, does not carry analogue teletext signals on Sky Digital, but does carry the embedded version, accessible from the "Services" menu of the receiver, or more recently by turning them off/on from a mini menu accessible from the "help" button).
New Zealand.
In New Zealand, captions use a EBU Ceefax based teletext system on DVB broadcasts via satellite and cable television with the exception of MediaWorks New Zealand channels who completely switched to DVB RLE subtitles in 2012 on both Freeview satellite and UHF broadcasts, this decision was made based on the TVNZ practice of using this format on only DVB UHF broadcasts (aka Freeview HD). This made composite video connected TVs incapable of decoding the captions on their own. Also these pre-rendered subtitles use classic caption style opaque backgrounds with an overly large font size and obscure the picture more than the more modern partially transparent backgrounds.
DTV standard captioning improvements.
The CEA-708 specification provides for dramatically improved captioning
As of 2009, however, most closed captioning for DTV environments is done using tools designed for analog captioning (working to the CEA-608 NTSC spec rather than the CEA-708 DTV spec). The captions are then run through transcoders made by companies like EEG Enterprises or Evertz, which convert the analog Line 21 caption format to the digital format. This means that none of the CEA-708 features are used unless they were also contained in CEA-608.
Uses of captioning in other mediums.
DVDs, BDs, & HD DVDs.
NTSC DVDs may carry closed captions in data packets of the MPEG-2 video streams inside of the Video-TS folder. Once played out of the analog outputs of a set top DVD player, the caption data is converted to the Line 21 format. They are output by the player to the composite video (or an available RF connector) for a connected TV's built-in decoder or a set-top decoder as usual. They can not be output on S-Video or component video outputs due to the lack of a colorburst signal on line 21. (Actually, regardless of this, if the DVD player is in interlaced rather than progressive mode, closed captioning "will" be displayed on the TV over component video input if the TV captioning is turned on and set to CC1.) When viewed on a personal computer, caption data can be viewed by software that can read and decode the caption data packets in the MPEG-2 streams of the DVD-Video disc. Windows Media Player (before Windows 7) in Vista supported only closed caption channels 1 and 2 (not 3 or 4). And Apple's DVD Player does not have the ability to read and decode Line 21 caption data which is recorded on a DVD made from an over-the-air broadcast. Apple's DVD Player can display some movie DVD captions.
In addition to Line 21 closed captions, video DVDs may also carry subtitles, which generally rendered from the EIA-608 captions as a bitmap overlay that can be turned on and off via a set top DVD player or DVD player software, just like the textual captions. This type of captioning is usually carried in a subtitle track labeled either "English for the hearing impaired" or, more recently, "SDH" (Subtitled for the Deaf and Hard of hearing). Many popular Hollywood DVD-Videos can carry both subtitles and closed captions (see Stepmom DVD by Columbia Pictures). On some DVDs, the Line 21 captions may contain the same text as the subtitles; on others, only the Line 21 captions include the additional non-speech information (even sometimes song lyrics) needed for deaf and hard of hearing viewers. European Region 2 DVDs do not carry Line 21 captions, and instead list the subtitle languages available—English is often listed twice, one as the representation of the dialogue alone, and a second subtitle set which carries additional information for the deaf and hard of hearing audience. (Many deaf/HOH subtitle files on DVDs are reworkings of original teletext subtitle files.)
HD DVD and Blu-ray disc media cannot carry any VBI data such as Line 21 closed captioning due to the design of DVI based High-Definition Multimedia Interface (HDMI) specifications that was only extended for synchronized digital audio replacing older analog standards, such as VGA, S-Video, component video and SCART. Both Blu-ray disc and HD DVD can use either PNG bitmap subtitles or 'advanced subtitles' to carry SDH type subtitling, the latter being an XML based textual format which includes font, styling and positioning information as well as a unicode representation of the text. Advanced subtitling can also include additional media accessibility features such as "descriptive audio".
Movies.
There are several competing technologies used to provide captioning for movies in theaters. Cinema captioning falls into the categories of 'open' and 'closed.' The definition of "closed" captioning in this context is different from television, as it refers to any technology that allows as few as one member of the audience to view the captions.
Open captioning in a film theater can be accomplished through burned-in captions, projected text or bitmaps, or (rarely) a display located above or below the movie screen. Typically, this display is a large LED sign. In a digital theater, open caption display capability is built into the digital projector. Closed caption capability is also available, with the ability for 3rd party closed caption devices to plug into the digital cinema server.
Probably the best-known closed captioning option for film theaters is the Rear Window Captioning System from the National Center for Accessible Media. Upon entering the theater, viewers requiring captions are given a panel of flat translucent glass or plastic on a gooseneck stalk, which can be mounted in front of the viewer's seat. In the back of the theater is an LED display that shows the captions in mirror image. The panel reflects captions for the viewer, but is nearly invisible to surrounding patrons. The panel can be positioned so that the viewer watches the movie through the panel and captions appear either on or near the movie image. A company called Cinematic Captioning Systems has a similar reflective system called Bounce Back. A major problem for distributors has been that these systems are each proprietary, and require separate distributions to the theater to enable them to work. Proprietary systems also incur license fees.
For film projection systems, Digital Theater Systems, the company behind the DTS surround sound standard, has created a digital captioning device called the DTS-CSS or Cinema Subtitling System. It is a combination of a laser projector which places the captioning (words, sounds) anywhere on the screen and a thin playback device with a CD that holds many languages. If the Rear Window Captioning System is used, the DTS-CSS player is also required for sending caption text to the Rear Window sign located in the rear of the theater.
Special effort has been made to build accessibility features into digital projection systems (see digital cinema). Through SMPTE, standards now exist that dictate how open and closed captions, as well as hearing-impaired and visually impaired narrative audio, are packaged with the rest of the digital movie. This eliminates the proprietary caption distributions required for film, and the associated royalties. SMPTE has also standardized the communication of closed caption content between the digital cinema server and 3rd party closed caption systems (the CSP/RPL protocol). As a result, new, competitive closed caption systems for digital cinema are now emerging that will work with any standards-compliant digital cinema server. These newer closed caption devices include cup-holder-mounted electronic displays and wireless glasses which display caption text in front of the wearer's eyes. Bridge devices are also available to enable the use of Rear Window systems. As of mid-2010, the remaining challenge to the wide introduction of accessibility in digital cinema is the industry-wide transition to SMPTE DCP, the standardized packaging method for very high quality, secure distribution of digital movies.
Sports venues.
Within the last few years as the cost of LED displays has come down, along with meeting all ADA requirements, sports venues such as Major League Baseball, National Football League and major college football stadiums, along with NBA and NHL arenas now either set aside space on their scoreboards or place separate display boards at several points in their seating bowls to display captions of announcements from the public address system and of advertising and promotions carried on the scoreboards, both for the hard of hearing and those who might be distracted and want to reference what was just said. The internal stadium/arena television network which relays game action to concourse areas also often has captions displayed on-screen.
Video games.
Closed captioning of video games is becoming more common. One of the first video game companies to feature closed captioning was Bethesda Softworks in their 1990 release of Hockey League Simulator and The Terminator. Infocom also offered "Zork Grand Inquisitor" in 1997. Many games since then have at least offered subtitles for spoken dialog during cut scenes, and many include significant in-game dialog and sound effects in the captions as well; for example, with subtitles turned on in the "Metal Gear Solid" series of stealth games, not only are subtitles available during cut scenes, but any dialog spoken during real-time gameplay will be captioned as well, allowing players who can't hear the dialog to know what enemy guards are saying and when the main character has been detected. Also, in many of developer Valve's video games (such as "Half-Life 2" or "Left 4 Dead"), when closed captions are activated, dialog and nearly all sound effects either made by the player or from other sources (e.g. gunfire, explosions) will be captioned.
Video games don't offer Line 21 captioning, decoded and displayed by the television itself but rather a built-in subtitle display, more akin to that of a DVD. The game systems themselves have no role in the captioning either: each game must have its subtitle display programmed individually.
Reid Kimball, a game designer who is hearing impaired, is attempting to educate game developers about closed captioning for games. Reid started the ] group to closed caption games and serve as a research and development team to aid the industry. Kimball designed the Dynamic Closed Captioning system, writes articles, and speaks at developer conferences. Games[CC]'s first closed captioning project called Doom3[CC] was nominated for an award as for IGDA's Choice Awards 2006 show.
Online video streaming.
Internet video streaming service YouTube offers captioning services in videos. The author of the video can upload a SubViewer (*.SUB), SubRip (*.SRT) or *.SBV file. As a beta feature, the site also added the ability to automatically transcribe and generate captioning on videos, with varying degrees of success based upon the content of the video. However, the automatic captioning is often inaccurate on videos with background music and exaggerated emotion in speaking. On June 30, 2010, YouTube announced a new "YouTube Ready" designation for professional caption vendors in the United States. The initial list included twelve companies who passed a caption quality evaluation administered by the Described and Captioned Media Project, have a website and a YouTube channel where customers can learn more about their services, and have agreed to post rates for the range of services that they offer for YouTube content.
Flash video also supports captions via the Distribution Exchange profile (DFXP) of W3C Timed Text format. The latest Flash authoring software adds free player skins and caption components that enable viewers to turn captions on/off during playback from a webpage. Previous versions of Flash relied on the Captionate 3rd party component and skin to caption Flash video. Custom Flash players designed in Flex can be tailored to support the Timed Text exchange profile, Captionate .XML, or SAMI file (see Hulu captioning). This is the preferred method for most US broadcast and cable networks that are mandated by the U.S. Federal Communications Commission to provide captioned on-demand content. The media encoding firms generally use software such as to convert EIA-608 captions to this format.
The Silverlight Media Framework. also includes support for the Timed Text exchange profile for both download and adaptive streaming media.
Windows Media Video can support closed captions for both video on demand streaming or live streaming scenarios. Typically Windows Media captions support the SAMI file format but can also carry embedded closed caption data.
QuickTime video supports raw 608 caption data via proprietary Closed Caption Track, which are just EIA-608 byte pairs wrapped in a QuickTime packet container with different IDs for both line 21 fields. These captions can be turned on and off and appear in the same style as TV closed captions with all the standard formatting (pop-on, roll-up, paint-on) and can be positioned and split anywhere on the video screen. QuickTime Closed Caption tracks can be viewed in Mac or Windows versions of QuickTime Player, iTunes (via QuickTime), iPod Nano, iPod Classic, iPod Touch, iPhone, and iPad.
Theatre.
Live plays can be open captioned by a captioner who displays lines from the script and including non-speech elements on a large display screen near the stage.
Telephones.
A captioned telephone is a telephone that displays real-time captions of the current conversation. The captions are typically displayed on a screen embedded into the telephone base.
Media monitoring services.
In the United States especially, most media monitoring services capture and index closed captioning text from news and public affairs programs, allowing them to search the text for client references. The use of closed captioning for television news monitoring was pioneered by Universal Press Clipping Bureau (Universal Information Services) in 1992, and later in 1993 by Tulsa-based NewsTrak of Oklahoma (later known as Broadcast News of Mid-America, acquired by video news release pioneer Medialink Worldwide Incorporated in 1997). US patent 7,009,657 describes a "method and system for the automatic collection and conditioning of closed caption text originating from multiple geographic locations" as used by news monitoring services.
Conversations.
Software programs are now available that automatically generate a closed-captioning of conversations. Examples of such conversations include discussions in conference rooms, classroom lectures, and/or religious services.
Non-linear video editing systems and closed captioning.
In April 2010, Sony Creative Software released the Vegas Pro 9.0d update to the professional non-linear editor, Vegas Pro which implemented basic support for importing, editing, and delivering CEA608 Closed Captions. Vegas Pro 10, released on October 11, 2010, added several enhancements to the closed captioning support. TV-like CEA608 Closed Captioning can now be displayed as an overlay when played back in the Preview and Trimmer windows making it easy to check placement, edits, and timing of CC information. CEA708 style Closed Captioning is automatically created when the CEA608 data is created. Line 21 Closed Captioning is now supported as well as HD-SDI closed captioning capture and print from AJA and Blackmagic Design cards. Line 21 support provides a workflow for existing legacy media. Other improvements include increased support for multiple closed captioning file types, as well as the ability to export closed caption data for DVD Architect, YouTube, RealPlayer, QuickTime, and Windows Media Player.
In mid-2009, Apple released Final Cut Pro version 7 and began support for inserting closed caption data into SD and HD tape masters via firewire and compatible video capture cards. Up until this time it was not possible for video editors to insert caption data with both CEA-608 and CEA-708 to their tape masters. The typical workflow included first printing the SD or HD video to a tape and sending it to a professional closed caption service company that had a stand-alone closed caption hardware encoder.
This new closed captioning workflow known as e-Captioning involves making a proxy video from the non-linear system to import into a third-party non-linear closed captioning software. Once the closed captioning software project is completed, it must export a closed caption file compatible with the non-linear editing system. In the case of Final Cut Pro 7, three different file formats can be accepted: a .SCC file (Scenarist Closed Caption file) for Standard Definition video, a QuickTime 608 Closed Caption track (a special 608 coded track in the .mov file wrapper) for Standard Definition video, and finally a QuickTime 708 Closed Caption track (a special 708 coded track in the .mov file wrapper) for High Definition video output.
Alternatively, Matrox video systems devised another mechanism for inserting closed caption data by allowing the video editor to include CEA-608 and CEA-708 in a discrete audio channel on the video editing timeline. This allows real-time preview of the captions while editing and is compatible with Final Cut Pro 6 and 7.
Other non-linear editing systems indirectly support closed captioning only in Standard Definition line-21. Video files on the editing timeline must be composited with a line-21 VBI graphic layer known in the industry as a "blackmovie" with closed caption data. Alternately, video editors working with the DV25 and DV50 firewire workflows must encode their DV .avi or .mov file with VAUX data which includes CEA-608 closed caption data.
Logo.
The current and most familiar logo for closed captioning consists of two Cs (for "closed captioned") inside a television screen. It was created by WGBH. The other logo, trademarked by the National Captioning Institute, is that of a simple geometric rendering of a television set merged with the tail of a speech balloon; two such versions exist: one with a tail on the left, the other with a tail on the right.

</doc>
<doc id="40885" url="http://en.wikipedia.org/wiki?curid=40885" title="Closed-loop transfer function">
Closed-loop transfer function

A closed-loop transfer function in control theory is a mathematical expression (algorithm) describing the net result of the effects of a closed (feedback) loop on the input signal to the circuits enclosed by the loop.
Overview.
The closed-loop transfer function is measured at the output. The output signal waveform can be calculated from the closed-loop transfer function and the input signal waveform.
An example of a closed-loop transfer function is shown below:
The summing node and the "G"("s") and "H"("s") blocks can all be combined into one block, which would have the following transfer function:
Derivation.
We define an intermediate signal Z shown as follows:
Using this figure we write:

</doc>
<doc id="40888" url="http://en.wikipedia.org/wiki?curid=40888" title="Code conversion">
Code conversion

In telecommunication, the term code conversion has the following meanings: 
1. Conversion of signals, or groups of signals, in one code into corresponding signals, or groups of signals, in another code. 
2. A process for converting a code of some predetermined bit structure, such as 5, 7, or 14 bits per character interval, to another code with the same or a different number of bits per character interval. 
In code conversion, alphabetical order is not significant.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40890" url="http://en.wikipedia.org/wiki?curid=40890" title="Coded set">
Coded set

In telecommunication, a coded set is a set of elements onto which another set of elements has been mapped according to a code. 
Examples of coded sets include the list of names of airports that is mapped onto a set of corresponding three-letter representations of airport names, the list of classes of emission that is mapped onto a set of corresponding standard symbols, and the names of the months of the year mapped onto a set of two-digit decimal numbers.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40891" url="http://en.wikipedia.org/wiki?curid=40891" title="Code word">
Code word

In communication, a code word is an element of a standardized code or protocol. Each code word is assembled in accordance with the specific rules of the code and assigned a unique meaning. Code words are typically used for reasons of reliability, clarity, brevity, or secrecy.

</doc>
<doc id="40892" url="http://en.wikipedia.org/wiki?curid=40892" title="Coding">
Coding

Coding may refer to:

</doc>
<doc id="40893" url="http://en.wikipedia.org/wiki?curid=40893" title="Coherence length">
Coherence length

In physics, coherence length is the propagation distance over which a coherent wave (e.g. an electromagnetic wave) maintains a specified degree of coherence. Wave interference is strong when the paths taken by all of the interfering waves differ by less than the coherence length. A wave with a longer coherence length is closer to a perfect sinusoidal wave. Coherence length is important in holography and telecommunications engineering.
This article focuses on the coherence of classical electromagnetic fields. In quantum mechanics, there is a mathematically analogous concept of the quantum coherence length of a wave function.
Formulas.
In radio-band systems, the coherence length is approximated by
where "c" is the speed of light in a vacuum, "n" is the refractive index of the medium, and formula_2 is the bandwidth of the source.
In optical communications, the coherence length formula_3 is given by 
where formula_5 is the central wavelength of the source, formula_6 is the refractive index of the medium, and formula_7 is the spectral width of the source. If the source has a Gaussian spectrum with FWHM spectral width formula_7, then a path offset of ±formula_3 will reduce the fringe visibility to 50%.
"Coherence length" is usually applied to the optical regime.
The expression above is a frequently used approximation. Due to ambiguities in the definition of spectral width of a source, however, the following definition of coherence length has been suggested:
The coherence length can be measured using a Michelson interferometer and is the optical path length difference of a self-interfering laser beam which corresponds to a formula_10 fringe visibility, where the fringe visibility is defined as
where formula_12 is the fringe intensity.
In long-distance transmission systems, the coherence length may be reduced by propagation factors such as dispersion, scattering, and diffraction.
Lasers.
Multimode helium–neon lasers have a typical coherence length of 20 cm, while the coherence length of singlemode ones can exceed 100 m. Semiconductor lasers reach some 100 m. Singlemode fiber lasers with linewidths of a few kHz can have coherence lengths exceeding 100 km. Similar coherence lengths can be reached with optical frequency combs due to the narrow linewidth of each tooth. Non-zero visibility is present only for short intervals of pulses repeated after cavity length distances up to this long coherence length.

</doc>
<doc id="40894" url="http://en.wikipedia.org/wiki?curid=40894" title="Coherence time">
Coherence time

For an electromagnetic wave, the coherence time is the time over which a propagating wave (especially a laser or maser beam) may be considered coherent. In other words, it is the time interval within which its phase is, on average, predictable.
In long-distance transmission systems, the coherence time may be reduced by propagation factors such as dispersion, scattering, and diffraction.
Coherence time, "τ", is calculated by dividing the coherence length by the phase velocity of light in a medium; approximately given by
where "λ" is the central wavelength of the source, "Δν" and "Δλ" is the spectral width of the source in units of frequency and wavelength respectively, and "c" is the speed of light in vacuum.
A single mode fiber laser has a linewidth of a few kHz. The Schawlow-Townes limit for some cw lasers can be below 1 Hz. Hydrogen masers have linewidth around 1 Hz; their coherence length approximately corresponds to the distance from the Earth to the Moon.
s of 2009[ [update]], single electron spins show the longest room-temperature spin dephasing times ever observed in solid-state systems (1.8 ms).

</doc>
<doc id="40896" url="http://en.wikipedia.org/wiki?curid=40896" title="Collective routing">
Collective routing

Collective routing is routing in which a switching center automatically delivers messages to a specified list of destinations. 
Collective routing avoids the need to list each single address in the message heading. 
Major relay stations usually transmit messages bearing collective-routing indicators to tributary, minor, and other major relay stations.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40897" url="http://en.wikipedia.org/wiki?curid=40897" title="Collinear antenna array">
Collinear antenna array

In telecommunications, a collinear (or co-linear) antenna array is an array of dipole antennas mounted in such a manner that the corresponding elements of each antenna are parallel and collinear, that is they are located along a common line or axis. 
Colinear arrays of dipoles are high gain omnidirectional antennas. A dipole has an omnidirectional radiation pattern when in free space and not influenced by any other conductors in that it radiates equal radio power in all azimuthal directions perpendicular to the antenna, with the signal strength dropping to zero on the antenna axis. A colinear array of dipoles has gain, radiating more of its power in azimuthal directions and less toward the axes. A colinear array is usually mounted vertically in order to increase overall gain and directivity in the horizontal direction and reduce the power radiated into the sky or down toward the earth. Theoretically, when stacking idealised lossless dipole antennas in such a fashion, doubling their number will produce double the gain, with an increase of 3.01 dB. In practice, the gain realised will be below this due to imperfect radiation spread and losses.
Colinear dipole arrays are often used as the antennas for base stations for land mobile radio systems that communicate with mobile two-way radios in vehicles, such as police, fire, ambulance, and taxi dispatchers. 
Multiple directional antennas mounted vertically separated are referred to as "stacked" and if alongside each other as "bayed".
References.
 This article incorporates public domain material from websites or documents of the .
Chris Burks

</doc>
<doc id="40898" url="http://en.wikipedia.org/wiki?curid=40898" title="Collision">
Collision

A collision is an event in which two or more bodies exert forces on each other for a relatively short time. Although the most common colloquial use of the word "collision" refers to accidents in which two or more objects collide, the scientific use of the word "collision" implies nothing about the magnitude of the forces.
Some examples of physical interactions that scientists would consider collisions:
Some colloquial uses of the word collision are:
Overview.
Collision is short-duration interaction between two bodies or more than two bodies simultaneously causing change in motion of bodies involved due to internal forces acted between them during this. Collisions involve forces (there is a change in velocity). The magnitude of the velocity difference at impact is called the closing speed. All collisions conserve momentum. What distinguishes different types of collisions is whether they also conserve kinetic energy. Line of impact – It is the line which is common normal for surfaces are closest or in contact during impact. This is the line along which internal force of collision acts during impact and Newton's coefficient of restitution is defined only along this line.
Specifically, collisions can either be "elastic," meaning they conserve both momentum and kinetic energy, or "inelastic," meaning they conserve momentum but not kinetic energy. An inelastic collision is sometimes also called a "plastic collision."
A “perfectly inelastic” collision (also called a "perfectly plastic" collision) is a limiting case of inelastic collision in which the two bodies stick together after impact.
The degree to which a collision is elastic or inelastic is quantified by the coefficient of restitution, a value that generally ranges between zero and one. A perfectly elastic collision has a coefficient of restitution of one; a perfectly inelastic collision has a coefficient of restitution of zero.
Types of collisions.
There are two types of collisions between two bodies - 1) Head on collisions or one-dimensional collisions - where the velocity of each body just before impact is along the line of impact, and 2) Non-head on collisions, oblique collisions or two-dimensional collisions - where the velocity of each body just before impact is not along the line of impact.
According to the coefficient of restitution, there are two special cases of any collision as written below:
Allision.
In maritime law, it is occasionally desirable to distinguish between the situation of a vessel striking a moving object, and that of it striking a stationary object. The word "allision" is then used to mean the striking of a stationary object, while "collision" is used to mean the striking of a moving object.
Analytical vs. numerical approaches towards resolving collisions.
Relatively few problems involving collisions can be solved analytically; the remainder require numerical methods. An important problem in simulating collisions is determining whether two objects have in fact collided. This problem is called collision detection.
Examples of collisions that can be solved analytically.
Billiards.
Collisions play an important role in cue sports. Because the collisions between billiard balls are nearly elastic, and the balls roll on a surface that produces low rolling friction, their behavior is often used to illustrate Newton's laws of motion. After a zero-friction collision of a moving ball with a stationary one of equal mass, the angle between the directions of the two balls is 90 degrees. This is an important fact that professional billiards players take into account, although it assumes the ball is moving frictionlessly across the table rather than rolling with friction.
Consider an elastic collision in 2 dimensions of any 2 masses m1 and m2, with respective initial velocities u1 and u2 where u2 = 0, and final velocities V1 and V2.
Conservation of momentum gives m1u1 = m1V1+ m2V2.
Conservation of energy for an elastic collision gives (1/2)m1|u1|2 = (1/2)m1|V1|2 + (1/2)m2|V2|2.
Now consider the case m1 = m2: we obtain u1=V1+V2 and |u1|2 = |V1|2+|V2|2.
Taking the dot product of each side of the former equation with itself, |u1|2 = u1•u1 = |V1|2+|V2|2+2V1•V2. Comparing this with the latter equation gives V1•V2 = 0, so they are perpendicular unless V1 is the zero vector (which occurs if and only if the collision is head-on).
Perfectly inelastic collision.
In a perfectly inelastic collision, i.e., a zero coefficient of restitution, the colliding particles stick together. It is necessary to consider conservation of momentum:
where v is the final velocity, which is hence given by
The reduction of total kinetic energy is equal to the total kinetic energy before the collision in a center of momentum frame with respect to the system of two particles, because in such a frame the kinetic energy after the collision is zero. In this frame most of the kinetic energy before the collision is that of the particle with the smaller mass. In another frame, in addition to the reduction of kinetic energy there may be a transfer of kinetic energy from one particle to the other; the fact that this depends on the frame shows how relative this is.
With time reversed we have the situation of two objects pushed away from each other, e.g. shooting a projectile, or a rocket applying thrust (compare the derivation of the Tsiolkovsky rocket equation).
Examples of collisions analyzed numerically.
Animal locomotion.
Collisions of an animal's foot or paw with the underlying substrate are generally termed ground reaction forces. These collisions are inelastic, as kinetic energy is not conserved. An important research topic in prosthetics is quantifying the forces generated during the foot-ground collisions associated with both disabled and non-disabled gait. This quantification typically requires subjects to walk across a force platform (sometimes called a "force plate") as well as detailed kinematic and dynamic (sometimes termed kinetic) analysis.
Collisions used as an experimental tool.
Collisions can be used as an experimental technique to study material properties of objects and other physical phenomena.
Space exploration.
An object may deliberately be made to crash-land on another celestial body, to do measurements and send them to Earth before being destroyed, or to allow instruments elsewhere to observe the effect. See e.g.:
Mathematical description of molecular collisions.
Let the linear, angular and internal momenta of a molecule be given by the set of "r" variables { "p"i }. The state of a molecule may then be described by the range "δw"i = δ"p"1δ"p"2δ"p"3 ... δ"p"r. There are many such ranges corresponding to different states; a specific state may be denoted by the index "i". Two molecules undergoing a collision can thus be denoted by ("i", "j") (Such an ordered pair is sometimes known as a "constellation".)
It is convenient to suppose that two molecules exert a negligible effect on each other unless their centre of gravities approach within a critical distance "b". A collision therefore begins when the respective centres of gravity arrive at this critical distance, and is completed when they again reach this critical distance on their way apart. Under this model, a collision is completely described by the matrix formula_3, which refers to the constellation ("i", "j") before the collision, and the (in general different) constellation ("k", "l") after the collision.
This notation is convenient in proving Boltzmann's H-theorem of statistical mechanics.
Attack by means of a deliberate collision.
Types of attack by means of a deliberate collision include:
An attacking collision with a distant object can be achieved by throwing or launching a projectile.

</doc>
<doc id="40899" url="http://en.wikipedia.org/wiki?curid=40899" title="Combat-net radio">
Combat-net radio

In telecommunication, a combat-net radio (CNR) is a radio operating in a network that (a) provides a half-duplex circuit and (b) uses either a single radio frequency or a discrete set of radio frequencies when in a frequency hopping mode.
CNRs are primarily used for push-to-talk-operated radio nets for command and control of combat, combat support, and combat service support operations among military ground, sea, and air forces.
In the United States, two military standards govern the use of combat net radios and the host applications that communicate over the network: MIL-STD-188-220 and MIL-STD-2045-47001. In addition to IETF RFCs governing UDP, TCP, and IPv4/IPv6, all seven layers of the OSI communications architecture are addressed. MIL-STD-2045-47001 covers layer 7 (application), while MIL-STD-188-220 covers layers 1 through 3 (physical, data link, and network).
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40900" url="http://en.wikipedia.org/wiki?curid=40900" title="Combined distribution frame">
Combined distribution frame

In telecommunication, a combined distribution frame (CDF) is a distribution frame that combines the functions of main and intermediate distribution frames and contains both vertical and horizontal terminating blocks. 
The vertical blocks are used to terminate the permanent outside lines entering the station. Horizontal blocks are used to terminate inside plant equipment. This arrangement permits the association of any outside line with any desired terminal equipment. These connections are made either with twisted pair wire, normally referred to as jumper wire, or with optical fiber cables, normally referred to as jumper cables. 
In technical control facilities, the vertical side may be used to terminate equipment as well as outside lines. The horizontal side is then used for jackfields and battery terminations.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40901" url="http://en.wikipedia.org/wiki?curid=40901" title="Comma-free code">
Comma-free code

A comma-free code may refer to:

</doc>
<doc id="40906" url="http://en.wikipedia.org/wiki?curid=40906" title="Commercial refile">
Commercial refile

Commercial refile: In military communications systems, the processing of a message from (a) a given military network, such as a tape relay network, a point-to-point telegraph network, a radio-telegraph network, or the DSN to (b) a commercial communications network. 
Commercial refiling of a message will usually require a reformatting of the message, particularly the heading.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40908" url="http://en.wikipedia.org/wiki?curid=40908" title="Common battery">
Common battery

In telecommunication, a common battery is a single electrical power source used to energize more than one circuit, electronic component, equipment, or system. 
A common battery is usually an electrolytic device and is usually centrally located to the equipment that it serves. In many telecommunications applications, the common battery is at a nominal -48 VDC. A central office common battery in the battery room supplies power to operate all directly connected instruments. "Common battery" may include one or more power conversion devices to transform commercial power to direct current, with an electrolytic battery floating across the output. Common battery operation largely replaced local batteries in each telephone in the early 20th century. 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40909" url="http://en.wikipedia.org/wiki?curid=40909" title="Booting">
Booting

In computing, booting (or booting up) is the initialization of a computerized system. The system can be a computer or a computer appliance. The booting process can be "hard", after electrical power to the CPU is switched from off to on (in order to diagnose particular hardware errors), or "soft", when those power-on self-tests (POST) can be avoided. Soft booting can be initiated by hardware such as a button press, or by software command. Booting is complete when the normal, operative, runtime environment is attained.
A boot loader is a computer program that loads an operating system or some other system software for the computer after completion of the power-on self-tests; it is the loader for the operating system itself, which has its own loader for loading ordinary user programs and libraries. Within the hard reboot process, it runs after completion of the self-tests, then loads and runs the software. A boot loader is loaded into main memory from persistent memory, such as a hard disk drive or, in some older computers, from a medium such as punched cards, punched tape, or magnetic tape. The boot loader then loads and executes the processes that finalize the boot. Like POST processes, the boot loader code comes from a "hard-wired" and persistent location; if that location is too limited for some reason, that primary boot loader calls a second-stage boot loader or a secondary program loader.
On modern general purpose computers, the boot up process can take tens of seconds, and typically involves performing a power-on self-test, locating and initializing peripheral devices, and then finding, loading and starting an operating system. The process of hibernating or sleeping does not involve booting. Minimally, some embedded systems do not require a noticeable boot sequence to begin functioning and when turned on may simply run operational programs that are stored in ROM. All computing systems are state machines, and a reboot may be the only method to return to a designated zero-state from an unintended, locked state.
"Boot" is short for "bootstrap" or "bootstrap load" and derives from the phrase "to pull oneself up by one's bootstraps". The usage calls attention to the requirement that, if most software is loaded onto a computer by other software already running on the computer, some mechanism must exist to load the initial software onto the computer. Early computers used a variety of ad-hoc methods to get a small program into memory to solve this problem. The invention of read-only memory (ROM) of various types solved this paradox by allowing computers to be shipped with a start up program that could not be erased. Growth in the capacity of ROM has allowed ever more elaborate start up procedures to be implemented.
History.
There are many different methods available to load a short initial program into a computer. These methods reach from simple, physical input to removable media that can hold more complex programs.
Pre integrated-circuit-ROM examples.
Early computers.
Early computers in the 1940s and 1950s were one-of-a-kind engineering efforts that could take weeks to program and program loading was one of many problems that had to be solved. An early computer, ENIAC, had no "program" stored in memory, but was set up for each problem by a configuration of interconnecting cables. Bootstrapping did not apply to ENIAC, whose hardware configuration was ready for solving problems as soon as power was applied.
In 1960, the Ballistic Missile Early Warning System Display Information Processor (DIP) in Colorado Springs—&#X200B;before the NORAD facility was built in the underground Cheyenne Mountain Complex—&#X200B;ran only one program, which carried its own startup code. The program was stored as a bit image on a continuously running magnetic drum, and loaded in a fraction of a second. Core memory was probably cleared manually via the maintenance console, and startup from when power was fully up was very fast, only a few seconds. In its general design, the DIP compared roughly with a DEC PDP-8.
First commercial computers.
The first programmable computers for commercial sale, such as the UNIVAC I and the IBM 701 included features to make their operation simpler. They typically included instructions that performed a complete input or output operation. The same hardware logic could be used to load the contents of a punch card or other input media that contained a bootstrap program by pressing a single button. This booting concept was called a variety of names for IBM computers of the 1950s and early 1960s, but IBM used the term "Initial Program Load" with the IBM 7030 Stretch and later used it for their mainframe lines, starting with the System/360 in 1964. 
The IBM 701 computer (1952–1956) had a "Load" button that initiated reading of the first 36-bit word into main memory from a punched card in a card reader, a magnetic tape in a tape drive, or a magnetic drum unit, depending on the position of the Load Selector switch. The left 18-bit half-word was then executed as an instruction, which usually read additional words into memory. The loaded boot program was then executed, which, in turn, loaded a larger program from that medium into memory without further help from the human operator. The term "boot" has been used in this sense since at least 1958.
Other IBM computers of that era had similar features. For example, the IBM 1401 system (c. 1958) used a card reader to load a program from a punched card. The 80 characters stored in the punched card were read into memory locations 001 to 080, then the computer would branch to memory location 001 to read its first stored instruction. This instruction was always the same: move the information in these first 80 memory locations to an assembly area where the information in punched cards 2, 3, 4, and so on, could be combined to form the stored program. Once this information was moved to the assembly area, the machine would branch to an instruction in location 080 (read a card) and the next card would be read and its information processed.
Another example was the IBM 650 (1953), a decimal machine, which had a group of ten 10-position switches on its operator panel which were addressable as a memory word (address 8000) and could be executed as an instruction. Thus setting the switches to 7004000400 and pressing the appropriate button would read the first card in the card reader into memory (op code 70), starting at address 400 and then jump to 400 to begin executing the program on that card.
IBM's competitors also offered single button program load.
A noteworthy variation of this is found on the Burroughs B1700 where there is neither a bootstrap ROM nor a hardwired IPL operation. Instead, after the system is reset it reads and executes opcodes sequentially from a tape drive mounted on the front panel; this sets up a boot loader in RAM which is then executed. However, since this makes few assumptions about the system it can equally well be used to load diagnostic (Maintenance Test Routine) tapes which display an intelligible code on the front panel even in cases of gross CPU failure.
IBM System/360 and successors.
In the IBM System/360 and its successors, including the current z/Architecture machines, the boot process is known as "Initial Program Load" (IPL).
IBM coined this term for the 7030 (Stretch), revived it for the design of the System/360, and continues to use it in those environments today. In the System/360 processors, an IPL is initiated by the computer operator by selecting the three hexadecimal digit device address (CUU; C=I/O Channel address, UU=Control unit and Device address) followed by pressing the "LOAD" button. On most System/370 and some later systems, the functions of the switches and the LOAD button are simulated using selectable areas on the screen of a graphics console, often an IBM 2250-like device or an IBM 3270-like device. For example, on the System/370 Model 158, the keyboard sequence 0-7-X (zero, seven and X, in that order) results in an IPL from the device address which was keyed into the input area. Amdahl 470V/6 and related CPUs supported four hexadecimal digits on those CPUs which had the optional second channel unit installed, for a total of 32 channels. Later, IBM would also support more than 16 channels.
The IPL function in the System/360 and its successors, and its compatibles such as Amdahl's, reads 24 bytes from an operator-specified device into main storage starting at real address zero. The second and third groups of eight bytes are treated as Channel Command Words (CCWs) to continue loading the startup program (the first CCW is always simulated by the CPU and consists of a Read IPL command, 02h, with command chaining and suppress incorrect length indication being enforced). When the I/O channel commands are complete, the first group of eight bytes is then loaded into the processor's Program Status Word (PSW) and the startup program begins execution at the location designated by that PSW. The IPL device is usually a disk drive, hence the special significance of the 02h read-type command, but exactly the same procedure is also used to IPL from other input-type devices, such as tape drives, or even card readers, in a device-independent manner, allowing, for example, the installation of an operating system on a brand-new computer from an OS initial distribution magnetic tape. For disk controllers, the 02h command also causes the selected device to seek to cylinder 0000h, head 0000h, simulating a Seek cylinder and head command, 07h, and to search for record 01h, simulating a Search ID Equal command, 31h; seeks and searches are not simulated by tape and card controllers, as for these device classes an 02h command is simply a sequential read command, not a Read IPL command.
The disk, tape or card deck must contain a special program to load the actual operating system into main storage, and for this specific purpose "IPL Text" is placed on the disk by the stand-alone DASDI (Direct Access Storage Device Initialization) program or an equivalent program running under an operating system, e.g., ICKDSF, but IPL-able tapes and card decks are usually distributed with this "IPL Text" already present.
Minicomputers.
Minicomputers, starting with the Digital Equipment Corporation (DEC) PDP-5 and PDP-8 (1965) simplified design by using the CPU to assist input and output operations. This saved cost but made booting more complicated than pressing a single button. Minicomputers typically had some way to "toggle in" short programs by manipulating an array of switches on the front panel. Since the early minicomputers used magnetic core memory, which did not lose its information when power was off, these bootstrap loaders would remain in place unless they were erased. Erasure sometimes happened accidentally when a program bug caused a loop that overwrote all of memory.
Other minicomputers with such simple form of booting include Hewlett-Packard's HP 2100 series (mid-1960s), the original Data General Nova (1969), and DEC's PDP-11 (1970).
DEC later added an optional diode matrix read-only memory for the PDP-11 that stored a bootstrap program of up to 32 words (64 bytes). It consisted of a printed circuit card, the M792, that plugged into the Unibus and held a 32 by 16 array of semiconductor diodes. With all 512 diodes in place, the memory contained all "one" bits; the card was programmed by cutting off each diode whose bit was to be "zero". DEC also sold versions of the card, the BM792-Yx series, pre-programmed for many standard input devices by simply omitting the unneeded diodes.
Following the older approach, the earlier PDP-1 has a hardware loader, such that an operator need only push the "load" switch to instruct the paper tape reader to load a program directly into core memory. The Data General Supernova used front panel switches to cause the computer to automatically load instructions into memory from a device specified by the front panel's data switches, and then jump to loaded code; the Nova 800 and 1200 had a switch that loaded a program into main memory from a special read-only memory and jumped to it.
Early minicomputer boot loader examples.
In a minicomputer with a paper tape reader, the first program to run in the boot process, the boot loader, would read into core memory either the second-stage boot loader (often called a "Binary Loader") that could read paper tape with checksum or the operating system from an outside storage medium. Pseudocode for the boot loader might be as simple as the following eight instructions:
A related example is based on a loader for a Nicolet Instrument Corporation minicomputer of the 1970s, using the paper tape reader-punch unit on a Teletype Model 33 ASR teleprinter. The bytes of its second-stage loader are read from paper tape in reverse order.
The length of the second stage loader is such that the final byte overwrites location 7. After the instruction in location 6 executes, location 7 starts the second stage loader executing. The second stage loader then waits for the much longer tape containing the operating system to be placed in the tape reader. The difference between the boot loader and second stage loader is the addition of checking code to trap paper tape read errors, a frequent occurrence with relatively low-cost, "part-time-duty" hardware, such as the Teletype Model 33 ASR. (Friden Flexowriters were far more reliable, but also comparatively costly.)
Booting the first microcomputers.
The earliest microcomputers, such as the Altair 8800 and an even earlier, similar machine (based on the Intel 8008 CPU) had no bootstrapping hardware as such. When started, the CPU would see memory that would contain executable code containing only binary zeros—memory was cleared by resetting when powering up. The front panels of these machines carried toggle switches, one switch per bit of the computer memory word. Simple additions to the hardware permitted one memory location at a time to be loaded from those switches to store bootstrap code. Meanwhile, the CPU was kept from attempting to execute memory content. Once correctly loaded, the CPU was enabled to execute the bootstrapping code. This process was tedious and had to be error-free.
Integrated circuit read-only memory era.
The boot process was revolutionized by the introduction of integrated circuit read-only memory (ROM), with its many variants, including mask-programmed ROMs, programmable ROMs (PROM), erasable programmable ROMs (EPROM), and flash memory. These allowed firmware boot programs to be shipped installed on the computer.
Typically, every microprocessor will, after a reset or power-on condition, perform a start-up process that usually takes the form of "begin execution of the code that is found starting at a specific address" or "look for a multibyte code at a specific address and jump to the indicated location to begin execution". A system built using that microprocessor will have the permanent ROM occupying these special locations so that the system always begins operating without operator assistance. For example, Intel x86 processors always start by running the instructions beginning at FFFF:0000, while for the MOS 6502 processor, initialization begins by reading a two-byte vector address at $FFFD (MS byte) and $FFFC (LS byte) and jumping to that location to run the bootstrap code.
Apple Inc.'s first computer, the Apple 1 introduced in 1976, featured PROM chips that eliminated the need for a front panel for the boot process. According to Apple's ad announcing it "No More Switches, No More Lights ... the firmware in PROMS enables you to enter, display and debug programs (all in hex) from the keyboard."
Due to the expense of read-only memory at the time, the Apple II series booted its disk operating systems using a series of very small incremental steps, each passing control onward to the next phase of the gradually more complex boot process. (See Apple DOS: Boot loader). Because so little of the disk operating system relied on ROM, the hardware was also extremely flexible and supported a wide range of customized disk copy protection mechanisms. (See Software Cracking: History.)
Some operating systems, most notably pre-1995 Macintosh systems from Apple, are so closely interwoven with their hardware that it is impossible to natively boot an operating system other than the standard one. This is the opposite extreme of the scenario using switches mentioned above; it is highly inflexible but relatively error-proof and foolproof as long as all hardware is working normally. A common solution in such situations is to design a boot loader that works as a program belonging to the standard OS that hijacks the system and loads the alternative OS. This technique was used by Apple for its A/UX Unix implementation and copied by various freeware operating systems and BeOS Personal Edition 5.
Some machines, like the Atari ST microcomputer, were "instant-on", with the operating system executing from a ROM. Retrieval of the OS from secondary or tertiary store was thus eliminated as one of the characteristic operations for bootstrapping. To allow system customizations, accessories, and other support software to be loaded automatically, the Atari's floppy drive was read for additional components during the boot process. There was a timeout delay that provided time to manually insert a floppy as the system searched for the extra components. This could be avoided by inserting a blank disk. The Atari ST hardware was also designed so the cartridge slot could provide native program execution for gaming purposes as a holdover from Atari's legacy making electronic games; by inserting the Spectre GCR cartridge with the Macintosh system ROM in the game slot and turning the Atari on, it could "natively boot" the Macintosh operating system rather than Atari's own TOS system.
The IBM Personal Computer included ROM-based firmware called the BIOS; one of the functions of that firmware was to perform a power-on self test when the machine was powered up, and then to read software from a boot device and execute it. Firmware compatible with the BIOS on the IBM Personal Computer is used in IBM PC compatible computers. The Extensible Firmware Interface was developed by Intel, originally for Itanium-based machines, and later also used as an alternative to the BIOS in x86-based machines, including Apple Macs using Intel processors.
Unix workstations originally had vendor-specific ROM-based firmware. Sun Microsystems later developed OpenBoot, later known as Open Firmware, which incorporated a Forth interpreter, with much of the firmware being written in Forth. It was standardized by the IEEE as IEEE standard 1275-1994; firmware that implements that standard was used in PowerPC-based Macs and some other PowerPC-based machines, as well as Sun's own SPARC-based computers. The Advanced RISC Computing specification defined another firmware standard, which was implemented on some MIPS-based and Alpha-based machines and the SGI Visual Workstation x86-based workstations.
Modern boot loaders.
When a modern computer is turned off, its software—&#X200B;including operating systems, application code, and data—&#X200B;is stored on nonvolatile data storage devices such as hard drives, CDs, DVDs, flash memory cards (like SD cards), USB flash drives, and floppy disks. When the computer is powered on, it typically does not have an operating system or its loader in random access memory (RAM). The computer first executes a relatively small program stored in read-only memory (ROM) along with a small amount of needed data, to access the nonvolatile device or devices from which the operating system programs and data can be loaded into RAM.
The small program that starts this sequence is known as a "bootstrap loader", "bootstrap" or "boot loader". This small program's only job is to load other data and programs which are then executed from RAM. Often, multiple-stage boot loaders are used, during which several programs of increasing complexity load one after the other in a process of chain loading.
Some computer systems, upon receiving a boot signal from a human operator or a peripheral device, may load a very small number of fixed instructions into memory at a specific location, initialize at least one CPU, and then point the CPU to the instructions and start their execution. These instructions typically start an input operation from some peripheral device (which may be switch-selectable by the operator). Other systems may send hardware commands directly to peripheral devices or I/O controllers that cause an extremely simple input operation (such as "read sector zero of the system device into memory starting at location 1000") to be carried out, effectively loading a small number of boot loader instructions into memory; a completion signal from the I/O device may then be used to start execution of the instructions by the CPU.
Smaller computers often use less flexible but more automatic boot loader mechanisms to ensure that the computer starts quickly and with a predetermined software configuration. In many desktop computers, for example, the bootstrapping process begins with the CPU executing software contained in ROM (for example, the BIOS of an IBM PC) at a predefined address (some CPUs, including the Intel x86 series are designed to execute this software after reset without outside help). This software contains rudimentary functionality to search for devices eligible to participate in booting, and load a small program from a special section (most commonly the boot sector) of the most promising device.
Boot loaders may face peculiar constraints, especially in size; for instance, on the IBM PC and compatibles, a boot sector should typically work in only 32 KB (later relaxed to 64 KB) of system memory and not use instructions not supported by the original 8088/8086 processors. The first stage of boot loaders located on fixed disks and removable drives must fit into the first 446 bytes of the Master Boot Record in order to leave room for the default 64-byte partition table with four partition entries and the two-byte boot signature, which the BIOS requires for a proper boot loader — or even less, when additional features like more than four partition entries (up to 16 with 16 bytes each), a disk signature (6 bytes), a disk timestamp (6 bytes), an Advanced Active Partition (18 bytes) or special multi-boot loaders have to be supported as well in some environments. In floppy and superfloppy Volume Boot Records, up to 59 bytes are occupied for the Extended BIOS Parameter Block on FAT12 and FAT16 volumes since DOS 4.0, whereas the FAT32 EBPB introduced with DOS 7.1 requires even 71 bytes, leaving only 441 bytes for the boot loader when assuming a sector size of 512 bytes. Microsoft boot sectors therefore traditionally imposed certain restrictions on the boot process, for example, the boot file had to be located at a fixed position in the root directory of the file system and stored as consecutive sectors, conditions taken care of by the codice_1 command and slightly relaxed in later versions of DOS. The boot loader was then able to load the first three sectors of the file into memory, which happened to contain another embedded boot loader able to load the remainder of the file into memory. When they added LBA and FAT32 support, they even switched to a two-sector boot loader using 386 instructions. At the same time other vendors managed to squeeze much more functionality into a single boot sector without relaxing the original constraints on the only minimal available memory and processor support. For example, DR-DOS boot sectors are able to locate the boot file in the FAT12, FAT16 and FAT32 file system, and load it into memory as a whole via CHS or LBA, even if the file is not stored in a fixed location and in consecutive sectors.
Second-stage boot loader.
Second-stage boot loaders, such as GNU GRUB, BOOTMGR, Syslinux, NTLDR or BootX, are not themselves operating systems, but are able to load an operating system properly and transfer execution to it; the operating system subsequently initializes itself and may load extra device drivers. The second-stage boot loader does not need drivers for its own operation, but may instead use generic storage access methods provided by system firmware such as the BIOS or Open Firmware, though typically with restricted hardware functionality and lower performance.
Many boot loaders (like GNU GRUB, Windows's BOOTMGR, and Windows NT/2000/XP's NTLDR) can be configured to give the user multiple booting choices. These choices can include different operating systems (for dual or multi-booting from different partitions or drives), different versions of the same operating system (in case a new version has unexpected problems), different operating system loading options (e.g., booting into a rescue or safe mode), and some standalone programs that can function without an operating system, such as memory testers (e.g., memtest86+) or even games (see List of PC Booter games). Some boot loaders can also load other boot loaders; for example, GRUB loads BOOTMGR instead of loading Windows directly. Usually a default choice is preselected with a time delay during which a user can press a key to change the choice; after this delay, the default choice is automatically run so normal booting can occur without interaction.
The boot process can be considered complete when the computer is ready to interact with the user, or the operating system is capable of running system programs or application programs. Typical modern personal computers boot in about one minute, of which about 15 seconds are taken by a power-on self-test (POST) and a preliminary boot loader, and the rest by loading the operating system and other software. Time spent after the operating system loading can be considerably shortened to as little as 3 seconds by bringing the system up with all cores at once, as with coreboot. Large servers may take several minutes to boot and start all their services.
Many embedded systems must boot immediately. For example, waiting a minute for a digital television or a GPS navigation device to start is generally unacceptable. Therefore such devices have software systems in ROM or flash memory so the device can begin functioning immediately; little or no loading is necessary, because the loading can be precomputed and stored on the ROM when the device is made.
Large and complex systems may have boot procedures that proceed in multiple phases until finally the operating system and other programs are loaded and ready to execute. Because operating systems are designed as if they never start or stop, a boot loader might load the operating system, configure itself as a mere process within that system, and then irrevocably transfer control to the operating system. The boot loader then terminates normally as any other process would.
Network booting.
Most computers are also capable of booting over a computer network. In this scenario, the operating system is stored on the disk of a server, and certain parts of it are transferred to the client using a simple protocol such as the Trivial File Transfer Protocol (TFTP). After these parts have been transferred, the operating system takes over the control of the booting process.
As with the second-stage boot loader, network booting begins by using generic network access methods provided by the network interface's boot ROM, which typically contains a Preboot Execution Environment (PXE) image. No drivers are required, but the system functionality is limited until the operating system kernel and drivers are transferred and started. As a result, once the ROM-based booting has completed it is entirely possible to network boot into an operating system that itself does not have the ability to use the network interface.
Personal computers (PC).
Boot devices.
The boot device is the device from which the operating system is loaded. A modern PC BIOS supports booting from various devices, typically a local hard disk drive via the Master Boot Record (MBR) (and of several MS-DOS partitions on such a disk, or GPT through GRUB 2), an optical disc drive (using El Torito), a USB mass storage device (FTL-based flash drive, SD card, or multi-media card slot; hard disk drive, optical disc drive, etc.), or a network interface card (using PXE). Older, less common BIOS-bootable devices include floppy disk drives, SCSI devices, Zip drives, and LS-120 drives.
Typically, the BIOS will allow the user to configure a "boot order". If the boot order is set to "first, the DVD drive; second, the hard disk drive", then the BIOS will try to boot from the DVD drive, and if this fails (e.g. because there is no DVD in the drive), it will try to boot from the local hard drive.
For example, on a PC with Windows XP installed on the hard drive, the user could set the boot order to the one given above, and then insert a Linux Live CD in order to try out Linux without having to install an operating system onto the hard drive. This is an example of dual booting, in which the user chooses which operating system to start after the computer has performed its Power-on self-test (POST). In this example of dual booting, the user chooses by inserting or removing the CD from the computer, but it is more common to choose which operating system to boot by selecting from a BIOS or UEFI boot menu, by using the computer keyboard; the boot menu is typically entered by pressing Delete or F11 keys during the POST.
Several devices are available that enable the user to "quick-boot" into what is usually a variant of Linux for various simple tasks such as Internet access; examples are Splashtop and Latitude ON.
Boot sequence.
Upon starting, an IBM-compatible personal computer's x86 CPU executes, in real mode, the instruction located at reset vector (the physical memory address FFFF0h on 16-bit x86 processors and FFFFFFF0h on 32-bit and 64-bit x86 processors), usually pointing to the BIOS entry point inside the ROM. This memory location typically contains a jump instruction that transfers execution to the location of the BIOS start-up program. This program runs a power-on self-test (POST) to check and initialize required devices such as DRAM and the PCI bus (including running embedded ROMs). The most complicated step is setting up DRAM over SPI, made more difficult by the fact that at this point memory is very limited.
After initializing required hardware, the BIOS goes through a pre-configured list of non-volatile storage devices ("boot device sequence") until it finds one that is bootable. A bootable device is defined as one that can be read from, and where the last two bytes of the first sector contain the little-endian word AA55h, found as byte sequence 55h, AAh on disk (also known as the MBR boot signature), or where it is otherwise established that the code inside the sector is executable on x86 PCs.
Coreboot splits the initialization and boot services into distinct parts, supporting "payloads" such as SeaBIOS, TianoCore, GRUB, and Linux directly (from flash).
Once the BIOS has found a bootable device it loads the boot sector to linear address 7C00h (usually segment:offset 0000h:7C00h, but some BIOSes erroneously use 07C0h:0000h) and transfers execution to the boot code. In the case of a hard disk, this is referred to as the Master Boot Record (MBR) and is by definition not operating-system specific. The conventional MBR code checks the MBR's partition table for a partition set as "bootable" (the one with "active" flag set). If an active partition is found, the MBR code loads the boot sector code from that partition, known as Volume Boot Record (VBR), and executes it.
The VBR is often operating-system specific; however, in most operating systems its main function is to load and execute the operating system kernel, which continues startup.
If there is no active partition, or the active partition's boot sector is invalid, the MBR may load a secondary boot loader which will select a partition (often via user input) and load its boot sector, which usually loads the corresponding operating system kernel. In some cases, the MBR may also attempt to load secondary boot loaders before trying to boot the active partition. If all else fails, it should issue an INT 18h BIOS interrupt call (followed by an INT 19h just in case INT 18h would return) in order to give back control to the BIOS, which would then attempt to boot off other devices, attempt a remote boot via network or invoke ROM BASIC.
Some systems (particularly newer Macintoshes and new editions of Microsoft Windows) use Intel's EFI. Also coreboot allows a computer to boot without having the firmware/BIOS constantly running in system management mode. 16-bit BIOS interfaces are required by certain x86 operating systems, such as DOS and Windows 3.1/95/98 (and all when not booted via UEFI). However, most boot loaders retain 16-bit BIOS call support.
Other kinds of boot sequences.
Some modern CPUs and microcontrollers (for example, TI OMAP) or sometimes even DSPs may have boot ROM with boot code integrated directly into their silicon, so such a processor could perform quite a sophisticated boot sequence on its own and load boot programs from various sources like NAND flash, SD or MMC card and so on. It is hard to hardwire all the required logic for handling such devices, so an integrated boot ROM is used instead in such scenarios. Boot ROM usage enables more flexible boot sequences than hardwired logic could provide. For example, the boot ROM could try to perform boot from multiple boot sources. Also, a boot ROM is often able to load a boot loader or diagnostic program via serial interfaces like UART, SPI, USB and so on. This feature is often used for system recovery purposes when for some reasons usual boot software in non-volatile memory got erased, and it could also be used for initial non-volatile memory programming when there is clean non-volatile memory installed and hence no software available in the system yet.
Some embedded system designs may also include an intermediary boot sequence step in form of additional code that gets loaded into system RAM by the integrated boot ROM. Additional code loaded that way usually serves as a way for overcoming platform limitations, such as small amounts of RAM, so a dedicated primary boot loader, such as Das U-Boot, can be loaded as the next step in system's boot sequence. The additional code and boot sequence step are usually referred to as "secondary program loader" (SPL).
It is also possible to take control of a system by using a hardware debug interface such as JTAG. Such an interface may be used to write the boot loader program into bootable non-volatile memory (e.g. flash) by instructing the processor core to perform the necessary actions to program non-volatile memory. Alternatively, the debug interface may be used to upload some diagnostic or boot code into RAM, and then to start the processor core and instruct it to execute the uploaded code. This allows, for example, the recovery of embedded systems where no software remains on any supported boot device, and where the processor does not have any integrated boot ROM. JTAG is a standard and popular interface; many CPUs, microcontrollers and other devices are manufactured with JTAG interfaces (as of 2009).
Some microcontrollers provide special hardware interfaces which cannot be used to take arbitrary control of a system or directly run code, but instead they allow the insertion of boot code into bootable non-volatile memory (like flash memory) via simple protocols. Then at the manufacturing phase, such interfaces are used to inject boot code (and possibly other code) into non-volatile memory. After system reset, the microcontroller begins to execute code programmed into its non-volatile memory, just like usual processors are using ROMs for booting. Most notably this technique is used by Atmel AVR microcontrollers, and by others as well. In many cases such interfaces are implemented by hardwired logic. In other cases such interfaces could be created by software running in integrated on-chip boot ROM from GPIO pins.
Most digital signal processors have a serial mode boot, and a parallel mode boot, such as the host port interface (HPI boot)
In case of DSPs there is often a second microprocessor or microcontroller present in the system design, and this is responsible for overall system behavior, interrupt handling, dealing with external events, user interface, etc. while the DSP is dedicated to signal processing tasks only. In such systems the DSP could be booted by another processor which is sometimes referred as the "host processor" (giving name to a Host Port). Such a processor is also sometimes referred as the "master", since it usually boots first from its own memories and then controls overall system behavior, including booting of the DSP, and then further controlling the DSP's behavior. The DSP often lacks its own boot memories and relies on the host processor to supply the required code instead. The most notable systems with such a design are cell phones, modems, audio and video players and so on, where a DSP and a CPU/microcontroller are co-existing.
Many FPGA chips load their configuration from an external serial EEPROM ("configuration ROM") on power-up.

</doc>
<doc id="40910" url="http://en.wikipedia.org/wiki?curid=40910" title="Common carrier">
Common carrier

A common carrier in common law countries (corresponding to a public carrier in civil law systems, usually called simply a carrier) is a person or company that transports goods or people for any person or company and that is responsible for any possible loss of the goods during transport. A common carrier offers its services to the general public under license or authority provided by a regulatory body. The regulatory body has usually been granted "ministerial authority" by the legislation that created it. The regulatory body may create, interpret, and enforce its regulations upon the common carrier (subject to judicial review) with independence and finality, as long as it acts within the bounds of the enabling legislation.
A common carrier is distinguished from a contract carrier (also called a "public carrier" in UK English), which is a carrier that transports goods for only a certain number of clients and that can refuse to transport goods for anyone else, and from a private carrier. A common carrier holds itself out to provide service to the general public without discrimination (to meet the needs of the regulator's quasi judicial role of impartiality toward the public's interest) for the "public convenience and necessity". A common carrier must further demonstrate to the regulator that it is "fit, willing, and able" to provide those services for which it is granted authority. Common carriers typically transport persons or goods according to defined and published routes, time schedules, and rate tables upon the approval of regulators. Public airlines, railroads, bus lines, taxicab companies, cruise ships, motor carriers (i.e., trucking companies), and other freight companies generally operate as common carriers. Under US law, an ocean freight forwarder cannot act as a common carrier.
The term "common carrier" is a common law term, seldom used in continental Europe because it has no exact equivalent in civil-law systems. In continental Europe, the functional equivalent of a common carrier is referred to as a "public carrier" (or simply as a "carrier"). (However, "public carrier" in continental Europe is different from "public carrier" in British English, where it is a synonym for "contract carrier".)
General.
Although common carriers generally transport people or goods, in the United States the term may also refer to telecommunications service providers and public utilities. In certain U.S. states, amusement parks that operate roller coasters and comparable rides have been found to be common carriers; a famous example is Disneyland.
Regulatory bodies may also grant carriers the authority to operate under contract with their customers instead of under common carrier authority, rates, schedules and rules. These regulated carriers, known as contract carriers, must demonstrate that they are "fit, willing and able" to provide service, according to standards enforced by the regulator. However, contract carriers are specifically not required to demonstrate that they will operate for the "public convenience and necessity". A contract carrier may be authorized to provide service over either fixed routes and schedules, i.e., as regular route carrier or on an "ad hoc" basis as an irregular route carrier.
It should be mentioned that the carrier refers only to the person (legal or physical) that enters into a contract of carriage with the shipper. The carrier does not necessarily have to own or even be in the possession of a means of transport. Unless otherwise agreed upon in the contract, the carrier may use whatever means of transport approved in its operating authority, as long as it is the most favourable from the cargo interests' point of view. The carriers' duty is to get the goods to the agreed destination within the agreed time or within reasonable time. 
The person that is physically transporting the goods on a means of transport is referred to as the "actual carrier". When a carrier subcontracts with another provider, such as an independent contractor or a third-party carrier, the common carrier is said to be providing "substituted service". The same person may hold both common carrier and contract carrier authority. In the case of a rail line in the US, the owner of the property is said to retain a "residual common carrier obligation", unless otherwise transferred (such as in the case of a commuter rail system, where the authority operating passenger trains may acquire the property but not this obligation from the former owner), and must operate the line if service is terminated. 
In contrast, private carriers are not licensed to offer a service to the public. Private carriers generally provide transport on an irregular or "ad hoc" basis for their owners.
Carriers were very common in rural areas prior to motorised transport. Regular services by horse-drawn vehicles would ply to local towns, taking goods to market or bringing back purchases for the village. If space permitted, passengers could also travel.
Telecommunications.
In the telecommunications regulation context in the United States, telecommunications carriers are regulated by the Federal Communications Commission under title II of the Communications Act of 1934.
The Telecommunications Act of 1996 made extensive revisions to the "Title II" provisions regarding common carriers and repealed the judicial 1982 AT&T consent decree (often referred to as the "modification of final judgment" or "MFJ") that effectuated the breakup of AT&T's Bell System. Further, the Act gives telephone companies the option of providing video programming on a common carrier basis or as a conventional cable television operator. If it chooses the former, the telephone company will face less regulation but will also have to comply with FCC regulations requiring what the Act refers to as "open video systems". The Act generally bars, with certain exceptions including most rural areas, acquisitions by telephone companies of more than a 10 percent interest in cable operators (and vice versa) and joint ventures between telephone companies and cable systems serving the same areas.
Internet networks are treated like common carriers in many respects. ISPs are largely immune from liability for third-party content. The Good Samaritan provision of the Communications Decency Act established immunity from liability for third party content on grounds of libel or slander, and the DMCA established that ISPs that comply with the DMCA would not be liable for the copyright violations of third parties on their network.
Pipelines.
In the United States, many oil, gas and CO2 pipelines are common carriers. The Federal Energy Regulatory Commission (FERC) regulates rates charged and other tariff terms imposed by interstate common carrier pipelines. Intrastate common carrier pipeline tariffs are often regulated by state agencies. The US and many states have delegated the power of eminent domain to common carrier gas pipelines.
Legal implications.
Common carriers are subject to special laws and regulations that differ depending on the means of transport used, e.g. sea carriers are often governed by quite different rules from road carriers or railway carriers. In common law jurisdictions as well as under international law, a common carrier is absolutely liable for goods carried by it, with four exceptions:
A sea carrier may also, according to the Hague-Visby Rules, escape liability on other grounds than the above-mentioned, e.g. a sea carrier is not liable for damages to the goods if the damage is the result of a fire on board the ship or the result of a navigational error committed by the ship's master or other crewmember.
Carriers typically incorporate further exceptions into a contract of carriage, often specifically claiming not to be a common carrier.
An important legal requirement for common carrier as public provider is that it cannot "discriminate", that is refuse the service unless there is some compelling reason. As of 2007, the status of Internet service providers as common carriers and their rights and responsibilities is widely debated (network neutrality).
It is also important to remember that the term common carrier does not exist in continental Europe but is distinctive to common law systems, particularly law systems in the US.
In "Ludditt v Ginger Coote Airways" the Privy Council (Lord Macmillan, Lord Wright, Lord Porter and Lord Simonds) held the liability of a public or common carrier of passengers is only to carry with due care. This is more limited than that of a common carrier of goods. The complete freedom of a carrier of passengers at common law to make such contracts as he thinks fit was not curtailed by the Railway and Canal Traffic Act 1854, and a specific contract that enlarges, diminishes or excludes his duty to take care (e.g., by a condition that the passenger travels "at his own risk against all casualties") cannot be pronounced to be unreasonable if the law authorises it. There was nothing in the provisions of the Canadian Transport Act 1938 section 25 that would invalidate a provision excluding liability. "Grand Trunk Railway Co of Canada v Robinson" [1915] A.C. 740 was followed and "Peek v North Staffordshire Railway" 11 E.R. 1109 was distinguished.

</doc>
<doc id="40911" url="http://en.wikipedia.org/wiki?curid=40911" title="Common control">
Common control

In telecommunication, a common control is an automatic telephone exchange arrangement in which the control equipment necessary for the establishment of connections is shared by being associated with a given call only during the period required to accomplish the control function for the given call. The first examples deployed on a major scale were the Director telephone system in London and the panel switch in the Bell System. Direct control telephone exchanges became rare in the 1960s, leaving only common control ones.
Systems which have control subsystem as an integral part of the switching network itself were known as direct control switching systems. Systems in which the control subsytem is outside the switching network are known as Common control systems. Strowger exchanges are usually direct control systems, whereas crossbar, electronic exchanges including all stored program control systems are common control systems. Common control is also known as indirect control or register control.
During the 1970s, common control exchanges became stored program control exchanges, using in the 1980s common-channel signaling in which the channels that are used for signaling are not used for message traffic.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40912" url="http://en.wikipedia.org/wiki?curid=40912" title="Common Management Information Service">
Common Management Information Service

The Common Management Information Service (CMIS) is the service interface specified in that is employed by OSI network elements for network management. It defines the service interface that is implemented by the Common Management Information Protocol (CMIP) as specified in . CMIS is part of the Open Systems Interconnection (OSI) body of international network standards.
Note the term CMIP is sometimes used erroneously when CMIS is intended. CMIS/CMIP is most often used in telecommunication applications, in other areas SNMP has become more popular.
Services.
The following services are made available by the Common Management Information Service Element (CMISE) to allow management of network elements:
Management association services.
To transfer management information between open systems using CMIS/CMIP, peer connections, "i.e.," associations, must be established. This requires the establishment of an Application layer association, a Session layer connection, a Transport layer connection, and, depending on supporting communications technology, Network layer and Link layer connections.
CMIS initially defined management association services but it was later decided these services could be provided by ACSE and these services were removed. Below is a list of these services which were subsequently removed from ISO 9595:

</doc>
<doc id="40913" url="http://en.wikipedia.org/wiki?curid=40913" title="Common-mode interference">
Common-mode interference

In telecommunication, the term common-mode interference has the following meanings: 
Techniques for dealing with common-mode interference.
Common mode noise may be isolated from the desired signal by various means:

</doc>
<doc id="40915" url="http://en.wikipedia.org/wiki?curid=40915" title="Communications blackout">
Communications blackout

In telecommunications, communications blackouts are
Technical failures.
Uptime being a key goal of most communications networks, uninterruptible power supplies and backup generators are typically used to ensure high-reliability power.
Wireless networks may be subject to radio jamming; wired networks can be physically severed. Network design can also play a role in maintaining communications reliability; depending on the constraints in building a fiber-optic network, a self-healing ring topology may be used.
Spacecraft reentry.
The communications blackouts that affect spacecraft re-entering the Earth's atmosphere, which are also known as radio blackouts, ionization blackouts, or reentry blackouts, are caused by an envelope of ionized air around the craft, created by the heat from the compression of the atmosphere by the craft. The ionized air interferes with radio signals. For the Mercury, Gemini, and Apollo spacecraft, such communications blackouts lasted for several minutes. Gemini 2, for example, endured such a blackout for four minutes, beginning at 9 minutes 5 seconds into the flight.
For Apollo missions, the communications blackout was approximately three minutes long. For Apollo 16, for example, pre-advisory data (PAD) for re-entry listed the expected times for re-entry communications blackout to be from 0 minutes 16 seconds after entry interface to 3 minutes 33 seconds after entry interface (a total of 3 minutes 17 seconds). For the Apollo 13 mission, the blackout was much longer than normal because the flight path of the spacecraft was unexpectedly at a much shallower angle than normal. According to the mission log maintained by Gene Kranz, the Apollo 13 re-entry blackout lasted around 6 minutes, beginning at 142:39 and ending at 142:45, and was 1 minute 27 seconds longer than had been predicted.
Communications blackouts for re-entry are not solely confined to entry into Earth's atmosphere. They apply to entry into any atmosphere where such ionization occurs around a craft. The Mars Pathfinder endured a 30 second communications blackout as it entered Mars' atmosphere, for example. The Huygens probe endured a communications blackout as it entered the atmosphere of Titan.
Until the creation of the Tracking and Data Relay Satellite System (TDRSS), the Space Shuttle endured a 30-minute blackout. The TDRSS allowed the Shuttle to communicate by relay with a Tracking and Data Relay Satellite during re-entry, through a "hole" in the ionized air envelope at the tail end of the craft, created by the Shuttle's shape.
Space weather.
Radio blackouts on Earth caused by space weather are measured by the National Oceanic and Atmospheric Administration on a scale that goes from 1 (minor) to 5 (extreme). 
Solar position.
Communications can also be lost when the Sun is blocking or behind one station in the same line of sight; Sun outages periodically interrupt communications with geosynchronous satellites. It is also a common problem for interplanetary space missions.

</doc>
<doc id="40916" url="http://en.wikipedia.org/wiki?curid=40916" title="Communications center">
Communications center

In telecommunication, the term communications center has the following meanings: 

</doc>
<doc id="40918" url="http://en.wikipedia.org/wiki?curid=40918" title="Communications deception">
Communications deception

In telecommunication, the term communications deception has the following meanings:

</doc>
<doc id="40919" url="http://en.wikipedia.org/wiki?curid=40919" title="Communications-electronics">
Communications-electronics

In telecommunication, communications-electronics (C-E) is the specialized field concerned with the use of electronic devices and systems for the acquisition or acceptance, processing, storage, display, analysis, protection, disposition, and transfer of information. 
C-E includes the wide range of responsibilities and actions relating to:
Electronic Communications Equipment.
Communication electronics radio equipment has been a rapidly growing industry for more than a century. Homeland Security in the USA is one of the reasons for the fast growth. Since the invention of the “solid state” transistor in the 1950s and the TTL (transistor-transistor logic) that led to the development of the IC (integrated circuit) in the 1960s the growth in the field of electronics has been phenomenal. As now witnessed in the “radio communications” field. The latest trend is to send conventional LMR (land-mobile-radio) signals over the Internet (Internet Protocol) this is called RoIP (Radio over Internet Protocol), which is just like VoIP (Voice over Internet Protocol) but uses the radio. By sending signals over the Internet it allows radios to be connected together all over the world. Hence: the “Communications Revolution”.

</doc>
<doc id="40921" url="http://en.wikipedia.org/wiki?curid=40921" title="Communications protection">
Communications protection

In telecommunications, communications protection is the application of communications security (COMSEC) measures to telecommunications systems in order to: (a) deny unauthorized access to sensitive unclassified information of value, (b) prevent disruption of telecommunications services, or (c) ensure the authenticity of information handled by telecommunications systems.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40922" url="http://en.wikipedia.org/wiki?curid=40922" title="Communications security">
Communications security

Communications security is the discipline of preventing unauthorized interceptors from accessing telecommunications in an intelligible form, while still delivering content to the intended recipients. In the United States Department of Defense culture, it is often referred to by the abbreviation COMSEC. The field includes cryptosecurity, transmission security, and physical security of COMSEC equipment.
COMSEC is used to protect both classified and unclassified traffic on military communications networks, including voice, video, and data. It is used for both analog and digital applications, and both wired and wireless links.
Voice over secure internet protocol VOSIP has become the de facto standard for securing voice communication, replacing the need for Secure Terminal Equipment (STE) in much of the U.S. Department of Defense. USCENTCOM moved entirely to VOSIP in 2008.
Related terms.
Types of COMSEC equipment:
DoD key management system.
The EKMS is DoD key management, COMSEC material distribution, and logistics support system. The NSA established the EKMS program to supply electronic key to COMSEC devices in securely and timely manner, and to provide COMSEC managers with an automated system capable of ordering, generation, production, distribution, storage, security accounting, and access control.
The Army's platform in the four-tiered EKMS, AKMS, automates frequency management and COMSEC management operations. It eliminates paper keying material, hardcopy SOI, and associated time and resource-intensive courier distribution. It has 4 components:

</doc>
<doc id="40924" url="http://en.wikipedia.org/wiki?curid=40924" title="Communications survivability">
Communications survivability

In telecommunication, communications survivability is the ability of communications systems to continue to operate effectively under adverse conditions, though portions of the system may be damaged or destroyed. 
Various methods may be used to maintain communications services, such as using alternate routing, different transmission media or methods, redundant equipment, and sites and equipment that are radiation hardened.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40925" url="http://en.wikipedia.org/wiki?curid=40925" title="Communications system">
Communications system

In telecommunication, a communications system is a collection of individual communications networks, transmission systems, relay stations, tributary stations, and data terminal equipment (DTE) usually capable of interconnection and interoperation to form an integrated whole. The components of a communications system serve a common purpose, are technically compatible, use common procedures, respond to controls, and operate in union. Telecommunications is a method of communication (e.g., for sports broadcasting, mass media, journalism, etc.). A communications subsystem is a functional unit or operational assembly that is smaller than the larger assembly under consideration.
Types.
By media.
An optical communication system is any form of telecommunication that uses light as the transmission medium. Equipment consists of a transmitter, which encodes a "message" into an optical "signal", a "channel", which carries the signal to its destination, and a receiver, which reproduces the message from the received optical signal. Fiber-optic communication systems transmit information from one place to another by sending light through an optical fiber. The light forms an electromagnetic carrier wave that is modulated to carry information.
A radio communication system is composed of several communications subsystems that give exterior communications capabilities. A radio communication system comprises a transmitting conductor in which electrical oscillations or currents are produced and which is arranged to cause such currents or oscillations to be propagated through the free space medium from one point to another remote therefrom and a receiving conductor at such distant point adapted to be excited by the oscillations or currents propagated from the transmitter.
Power line communication systems operate by impressing a modulated carrier signal on power wires. Different types of powerline communications use different frequency bands, depending on the signal transmission characteristics of the power wiring used. Since the power wiring system was originally intended for transmission of AC power, the power wire circuits have only a limited ability to carry higher frequencies. The propagation problem is a limiting factor for each type of power line communications.
By technology.
A duplex communication system is a system composed of two connected parties or devices which can communicate with one another in both directions. The term "duplex" is used when describing communication between two parties or devices. Duplex systems are employed in nearly all communications networks, either to allow for a communication "two-way street" between two connected parties or to provide a "reverse path" for the monitoring and remote adjustment of equipment in the field.
Antenna:-an antenna is basically a small length of a conductor that is used to radiate or receive electromagnetic waves.
It acts as a conversion device.At the transmitting end it converts high frequency current into electromagnetic waves.At the receiving end it transforms electromagnetic waves into electrical signals that is fed into the input of the receiver. several types of antenna are used in communication.
Examples of communications subsystems include the Defense Communications System (DCS). 
By Application area.
A tactical communications system is a communications system that (a) is used within, or in direct support of, tactical forces, (b) is designed to meet the requirements of changing tactical situations and varying environmental conditions, (c) provides securable communications, such as voice, data, and video, among mobile users to facilitate command and control within, and in support of, tactical forces, and (d) usually requires extremely short installation times, usually on the order of hours, in order to meet the requirements of frequent relocation.
An Emergency communication system is any system (typically computer based) that is organized for the primary purpose of supporting the two way communication of emergency messages between both individuals and groups of individuals. These systems are commonly designed to integrate the cross-communication of messages between are variety of communication technologies.
An Automatic call distributor (ACD) is a communication system that automatically queues, assigns and connects callers to handlers. This is used often in customer service (such as for product or service complaints), ordering by telephone (such as in a ticket office), or coordination services (such as in air traffic control).
A Voice Communication Control System (VCCS) is essentially an ACD with characteristics that make it more adapted to use in critical situations (no waiting for dialtone, or lengthy recorded announcements, radio and telephone lines equally easily connected to, individual lines immediately accessible etc..)
See also.
Automatic call distributor

</doc>
<doc id="40927" url="http://en.wikipedia.org/wiki?curid=40927" title="Companding">
Companding

In telecommunication and signal processing companding (occasionally called compansion) is a method of mitigating the detrimental effects of a channel with limited dynamic range. The name is a portmanteau of compressing and expanding. The use of companding allows signals with a large dynamic range to be transmitted over facilities that have a smaller dynamic range capability. Companding is employed in telephony and other audio applications such as professional wireless microphones and analog recording.
How it works.
While the dynamic range compression used in audio recording and the like depends on a variable-gain amplifier, and so is a locally linear process (linear for short regions, but not globally), companding is non-linear and takes place in the same way at all points in time. The dynamic range of a signal is compressed before transmission and is expanded to the original value at the receiver.
The electronic circuit that does this is called a compander and works by compressing or expanding the dynamic range of an analog electronic signal such as sound recorded by a microphone. One variety is a triplet of amplifiers: a logarithmic amplifier, followed by a variable-gain linear amplifier and an exponential amplifier. Such a triplet has the property that its output voltage is proportional to the input voltage raised to an adjustable power.
Companded quantization is the combination of three functional building blocks – namely, a (continuous-domain) signal dynamic range "compressor", a limited-range uniform quantizer, and a (continuous-domain) signal dynamic range "expander" that inverts the compressor function. This type of quantization is frequently used in telephony systems. 
In practice, companders are designed to operate according to relatively simple dynamic range compressor functions that are designed to be suitable for implementation using simple analog electronic circuits. The two most popular compander functions used for telecommunications are the A-law and μ-law functions.
Applications.
Companding is used in digital telephony systems, compressing before input to an analog-to-digital converter, and then expanding after a digital-to-analog converter. This is equivalent to using a non-linear ADC as in a T-carrier telephone system that implements A-law or μ-law companding. This method is also used in digital file formats for better signal-to-noise ratio (SNR) at lower bit rates. For example, a linearly encoded 16-bit PCM signal can be converted to an 8-bit WAV or AU file while maintaining a decent SNR by compressing before the transition to 8-bit and expanding after a conversion back to 16-bit. This is effectively a form of lossy audio data compression.
Professional wireless microphones do this since the dynamic range of the microphone audio signal itself is larger than the dynamic range provided by radio transmission. Companding also reduces the noise and crosstalk levels at the receiver.
Compandors are used in concert audio systems and in some noise reduction schemes such as dbx and Dolby NR (all versions).
History.
The use of companding in an analog picture transmission system was patented by A. B. Clark of AT&T in 1928 (filed in 1925):
In the transmission of pictures by electric currents, the method which consists in sending currents varied in a non-linear relation to the light values of the successive elements of the picture to be transmitted, and at the receiving end exposing corresponding elements of a sensitive surface to light varied in inverse non-linear relation to the received current.—A. B. Clark patent
In 1942, Clark and his team completed the SIGSALY secure voice transmission system that included the first use of companding in a PCM (digital) system.
In 1953, B. Smith showed that a nonlinear DAC could result in the inverse nonlinearity in a successive-approximation ADC configuration, simplifying the design of digital companding systems.
In 1970, H. Kaneko developed the uniform description of segment (piecewise linear) companding laws that had by then been adopted in digital telephony.
In the 1980s (and '90s), many of the music equipment manufacturers (Roland, Yamaha, Korg) used companding when compressing the library waveform data in their digital synthesizers. This dates back to the late '80s when memory chips were often one of the most costly components in the instrument. Manufacturers usually quoted the amount of memory in its compressed form: i.e. 24MB of physical waveform ROM in a Korg Trinity is actually 48MB when uncompressed. Similarly, Roland SR-JV expansion boards were usually advertised as 8MB boards with '16MB-equivalent content'. Careless copying of this technical information, omitting the "equivalence" reference, can often cause confusion.

</doc>
<doc id="40928" url="http://en.wikipedia.org/wiki?curid=40928" title="Comparably efficient interconnection">
Comparably efficient interconnection

In telecommunication, a comparably efficient interconnection (CEI) is an equal-access concept developed by the FCC stating that, ". . . if a carrier offers an enhanced service, it should be required to offer network interconnection (or collocation) opportunities to others that are comparably efficient to the interconnection that its enhanced service enjoys. Accordingly, a carrier would be required to implement CEI only as it introduces new enhanced services." [FCC "Report and Order" June 16, 1986]
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40929" url="http://en.wikipedia.org/wiki?curid=40929" title="Comparator">
Comparator

In electronics, a comparator is a device that compares two voltages or currents and outputs a digital signal indicating which is larger. It has two analog input terminals formula_1 and formula_2 and one binary digital output formula_3. The output is ideally 
A comparator consists of a specialized high-gain differential amplifier. They are commonly used in devices that measure and digitize analog signals, such as analog-to-digital converters (ADCs), as well as relaxation oscillators.
Differential voltage.
The differential voltages must stay within the limits specified by the manufacturer. Early integrated comparators, like the LM111 family, and certain high-speed comparators like the LM119 family, require differential voltage ranges substantially lower than the power supply voltages (±15 V vs. 36 V). "Rail-to-rail" comparators allow any differential voltages within the power supply range. When powered from a bipolar (dual rail) supply, 
or, when powered from a unipolar TTL/CMOS power supply:
Specific rail-to-rail comparators with p-n-p input transistors, like the LM139 family, allow input potential to drop 0.3 volts "below" the negative supply rail, but do not allow it to rise above the positive rail. Specific ultra-fast comparators, like the LMH7322, allow input signal to swing below the negative rail "and" above the positive rail, although by a narrow margin of only 0.2 V. Differential input voltage (the voltage between two inputs) of a modern rail-to-rail comparator is usually limited only by the full swing of power supply.
Op-amp voltage comparator.
An operational amplifier (op-amp) has a well balanced difference input and a very high gain. This parallels the characteristics of comparators and can be substituted in applications with low-performance requirements.
In theory, a standard op-amp operating in open-loop configuration (without negative feedback) may be used as a low-performance comparator. When the non-inverting input (V+) is at a higher voltage than the inverting input (V-), the high gain of the op-amp causes the output to saturate at the highest positive voltage it can output. When the non-inverting input (V+) drops below the inverting input (V-), the output saturates at the most negative voltage it can output. The op-amp's output voltage is limited by the supply voltage. An op-amp operating in a linear mode with negative feedback, using a balanced, split-voltage power supply, (powered by ± VS) has its transfer function typically written as: formula_7. However, this equation may not be applicable to a comparator circuit which is non-linear and operates open-loop (no negative feedback)
In practice, using an operational amplifier as a comparator presents several disadvantages as compared to using a dedicated comparator:
Working.
A dedicated voltage comparator will generally be faster than a general-purpose operational amplifier pressed into service as a comparator. A dedicated voltage comparator may also contain additional features such as an accurate, internal voltage reference, an adjustable hysteresis and a clock gated input.
A dedicated voltage comparator chip such as LM339 is designed to interface with a digital logic interface (to a TTL or a CMOS). The output is a binary state often used to interface real world signals to digital circuitry (see analog to digital converter). If there is a fixed voltage source from, for example, a DC adjustable device in the signal path, a comparator is just the equivalent of a cascade of amplifiers. When the voltages are nearly equal, the output voltage will not fall into one of the logic levels, thus analog signals will enter the digital domain with unpredictable results. To make this range as small as possible, the amplifier cascade is high gain. The circuit consists of mainly Bipolar transistors. For very high frequencies, the input impedance of the stages is low. This reduces the saturation of the slow, large P-N junction bipolar transistors that would otherwise lead to long recovery times. Fast small Schottky diodes, like those found in binary logic designs, improve the performance significantly though the performance still lags that of circuits with amplifiers using analog signals. Slew rate has no meaning for these devices. For applications in flash ADCs the distributed signal across eight ports matches the voltage and current gain after each amplifier, and resistors then behave as level-shifters.
The LM339 accomplishes this with an open collector output. When the inverting input is at a higher voltage than the non inverting input, the output of the comparator connects to the negative power supply. When the non inverting input is higher than the inverting input, the output is 'floating' (has a very high impedance to ground).
The gain of op amp as comparator is given by this equation
V(out)=V(in)
Key specifications.
While it is easy to understand the basic task of a comparator, that is, comparing two voltages or currents, several parameters must be considered while selecting a suitable comparator:
Speed and power.
While in general comparators are "fast," their circuits are not immune to the classic speed-power tradeoff. High speed comparators use transistors with larger aspect ratios and hence also consume more power. Depending on the application, select either a comparator with high speed or one that saves power. For example, nano-powered comparators in space-saving chip-scale packages (UCSP), DFN or SC70 packages such as , , , and are ideal for ultra-low-power, portable applications. Likewise if a comparator is needed to implement a relaxation oscillator circuit to create a high speed clock signal then comparators having few nano seconds of propagation delay may be suitable. (CML output), (LVDS Output), (CMOS output / TTL output), (CMOS output / TTL output), (TTL output), and (PECL output) are examples of some good high speed comparators.
Hysteresis.
A comparator normally changes its output state when the voltage between its inputs crosses through approximately zero volts. Small voltage fluctuations due to noise, always present on the inputs, can cause undesirable rapid changes between the two output states when the input voltage difference is near zero volts. To prevent this output oscillation, a small hysteresis of a few millivolts is integrated into many modern comparators. 
For example, the , and have internal hysteresis desensitizing them from input noise. In place of one switching point, hysteresis introduces two: one for rising voltages, and one for falling voltages. The difference between the higher-level trip value (VTRIP+) and the lower-level trip value (VTRIP-) equals the hysteresis voltage (VHYST).
If the comparator does not have internal hysteresis or if the input noise is greater than the internal hysteresis then an external hysteresis network can be built using positive feedback from the output to the non-inverting input of the comparator. The resulting Schmitt trigger circuit gives additional noise immunity and a cleaner output signal. Some comparators such as , , , and also provide the hysteresis control through a separate hysteresis pin. These comparators make it possible to add a programmable hysteresis without feedback or complicated equations. Using a dedicated hysteresis pin is also convenient if the source impedance is high since the inputs are isolated from the hysteresis network. When hysteresis is added then a comparator cannot resolve signals within the hysteresis band.
Output type.
Because comparators have only two output states, their outputs are near zero or near the supply voltage. Bipolar rail-to-rail comparators have a common-emitter output that produces a small voltage drop between the output and each rail. That drop is equal to the collector-to-emitter voltage of a saturated transistor. When output currents are light, output voltages of CMOS rail-to-rail comparators, which rely on a saturated MOSFET, range closer to the rails than their bipolar counterparts.
On the basis of outputs, comparators can also be classified as open drain or push–pull. Comparators with an open-drain output stage use an external pull up resistor to a positive supply that defines the logic high level. Open drain comparators are more suitable for mixed-voltage system design. Since the output is high impedance for logic level high, open drain comparators can also be used to connect multiple comparators on to a single bus. Push pull output does not need a pull up resistor and can also source current unlike an open drain output.
Internal reference.
The most frequent application for comparators is the comparison between a voltage and a stable reference. Most comparator manufacturers also offer comparators in which a reference voltage is integrated on to the chip. Combining the reference and comparator in one chip not only saves space, but also draws less supply current than a comparator with an external reference. ICs with wide range of references are available such as (200 mV reference), (400 mV reference), (600 mV reference), (1.236 V reference), (2.048 V reference), (1.24 V reference) and (2.5 V reference).
Continuous versus clocked.
A continuous comparator will output either a "1" or a "0" any time a high or low signal is applied to its input and will change quickly when the inputs are updated. However, many applications only require comparator outputs at certain instances, such as in A/D converters and memory. By only strobing a comparator at certain intervals, higher accuracy and lower power can be achieved with a clocked (or dynamic) comparator structure, also called a latched comparator. Often latched comparators employ strong positive feedback for a "regeneration phase" when a clock is high, and have a "reset phase" when the clock is low. 
This is in contrast to a continuous comparator, which can only employ weak positive feedback since there is no reset period.
Applications.
Null detectors.
A null detector is one that functions to identify when a given value is zero. Comparators can be a type of amplifier distinctively for null comparison measurements. It is the equivalent to a very high gain amplifier with well-balanced inputs and controlled output limits. The circuit compares the two input voltages, determining the larger. The inputs are an unknown voltage and a reference voltage, usually referred to as vu and vr. A reference voltage is generally on the non-inverting input (+), while vu is usually on the inverting input (−). (A circuit diagram would display the inputs according to their sign with respect to the output when a particular input is greater than the other.) The output is either positive or negative, for example ±12 V. In this case, the idea is to detect when there is no difference between in the input voltages. This gives the identity of the unknown voltage since the reference voltage is known.
When using a comparator as a null detector, there are limits as to the accuracy of the zero value measurable. Zero output is given when the magnitude of the difference in the voltages multiplied by the gain of the amplifier is less than the voltage limits. For example, if the gain of the amplifier is 106, and the voltage limits are ±6 V, then no output will be given if the difference in the voltages is less than 6 μV. One could refer to this as a sort of uncertainty in the measurement.
Zero-crossing detectors.
For this type of detector, a comparator detects each time an ac pulse changes polarity. The output of the comparator changes state each time the pulse changes its polarity, that is the output is HI (high) for a positive pulse and LO (low) for a negative pulse squares the input signal.
Relaxation oscillator.
A comparator can be used to build a relaxation oscillator. It uses both positive and negative feedback. The positive feedback is a Schmitt trigger configuration. Alone, the trigger is a bistable multivibrator. However, the slow negative feedback added to the trigger by the RC circuit causes the circuit to oscillate automatically. That is, the addition of the RC circuit turns the hysteretic bistable multivibrator into an astable multivibrator.
Level shifter.
This circuit requires only a single comparator with an open-drain output as in the , or . The circuit provides great flexibility in choosing the voltages to be translated by using a suitable pull up voltage. It also allows the translation of bipolar ±5 V logic to unipolar 3 V logic by using a comparator like the .
Analog-to-digital converters.
When a comparator performs the function of telling if an input voltage is above or below a given threshold, it is essentially performing a 1-bit quantization. This function is used in nearly all analog to digital converters (such as flash, pipeline, successive approximation, delta-sigma modulation, folding, interpolating, dual-slope and others) in combination with other devices to achieve a multi-bit quantization.
Window detectors.
Comparators can also be used as window detectors. In a window detector, a comparator used to compare two voltages and determine whether a given input voltage is under voltage or over voltage.
See also.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40930" url="http://en.wikipedia.org/wiki?curid=40930" title="Compatibility">
Compatibility

Compatibility may refer to:

</doc>
<doc id="40931" url="http://en.wikipedia.org/wiki?curid=40931" title="Compatible sideband transmission">
Compatible sideband transmission

A Compatible sideband transmission, also known as amplitude modulation equivalent (AME) or Single sideband-reduced carrier (SSB-RC), is a type of single sideband RF modulation in which the carrier is deliberately reinserted at a lower level after its normal suppression to permit reception by conventional AM receivers. 
The benefits of AME over conventional AM are increased spectral efficiency due to a reduction in bandwidth of 50% as well as an increase in signal efficiency. Conventional AM transmitters waste 66% of the transmitter RF power due to AM's carrier and redundant sideband. By using AME, less RF power is required at the transmitter to transmit the same quality of signal the same distance. 
AME is currently most popular in high frequency military communications.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40933" url="http://en.wikipedia.org/wiki?curid=40933" title="Complementary network service">
Complementary network service

In telecommunication, a complementary network service (CNS) is a means for an enhanced-service provider customer to connect to a network and to the enhanced service provider. 
Complementary network services usually consist of the customer local service, such as a business or residence, and several associated service features, such as a call-forwarding service.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40934" url="http://en.wikipedia.org/wiki?curid=40934" title="Component">
Component

Component may refer to:

</doc>
<doc id="40936" url="http://en.wikipedia.org/wiki?curid=40936" title="Compromise">
Compromise

To compromise is to make a deal between different parties where each party gives up part of their demand. In arguments, compromise is a concept of finding agreement through communication, through a mutual acceptance of terms—often involving variations from an original goal or desire.
Extremism is often considered as antonym to compromise, which, depending on context, may be associated with concepts of balance and tolerance. In the negative connotation, compromise may be referred to as capitulation, referring to a "surrender" of objectives, principles, or material, in the process of negotiating an agreement. In human relationships, "compromise" is frequently said to be an agreement with which no party is happy because the parties involved often feel that they either gave away too much or that they received too little.
Political compromise.
In international politics, the compromises most often discussed are usually regarded as nefarious deals with dictators, such as Neville Chamberlain's appeasement of Adolf Hitler. Margalit calls these "rotten compromises."
In democratic politics, great challenges of contemporary democracy and has become more difficult in the era of the permanent campaign, as Gutmann and Thompson show. The problem of political compromise in general is an important subject in political ethics.
Studies in compromise.
Defining and finding the best possible compromise is an important problem in fields like game theory and the voting system.
Research has indicated that suboptimal compromises are often the result of negotiators failing to realize when they have interests that are completely compatible with those of the other party and settle for suboptimal agreements. Mutually better outcomes can often be found by careful investigation of both parties' interests, especially if done early in negotiations.
The compromise solution of a multicriteria decision making or multi-criteria decision analysis problem that is the closest to the ideal could be determined by the VIKOR method, which provides a maximum utility of the majority, and a minimum individual regret of the opponent.

</doc>
<doc id="40937" url="http://en.wikipedia.org/wiki?curid=40937" title="Computer conferencing">
Computer conferencing

Computer conferencing may refer to:

</doc>
<doc id="40940" url="http://en.wikipedia.org/wiki?curid=40940" title="Concentrator">
Concentrator

In the evolution of modern telecommunications systems there was a requirement to connect large numbers of low-speed access devices with large telephone company 'central office' switches over common paths. During the first generations of digital networks, analog signals were digitized on line cards attached to the central office switches. In an effort to reduce costs, it was decided to push this conversion closer to the customer premises by deploying small conversion devices in customer neighborhoods. These devices would combine multiple digital signals on a single link to a larger telephone switch, which would provide service to the customer. These devices were initially called remote concentrators or simply remotes.
In fibre-optic distribution systems which offer triple-play services (voice, television, internet) the digitization has arrived at the customer premises and signals are digitized at the source and combined using customer edge routers. This traffic enters the distribution network at an Optical Network Termination and is carried to the central office using Wavelength Division Multiplexing and Passive Optical Networking. 
In telecommunication, the term concentrator has the following meanings:
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40941" url="http://en.wikipedia.org/wiki?curid=40941" title="Concentricity error">
Concentricity error

The concentricity error of an optical fiber is the distance between the center of the two concentric circles that specify the cladding diameter and the center of the two concentric circles that specify the core diameter. 
The concentricity error is used in conjunction with tolerance fields to specify or characterize optical fiber core and cladding geometry.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40942" url="http://en.wikipedia.org/wiki?curid=40942" title="Conditioning equipment">
Conditioning equipment

In telecommunication, the term conditioning equipment has the following meanings: 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40943" url="http://en.wikipedia.org/wiki?curid=40943" title="Conducted interference">
Conducted interference


</doc>
<doc id="40944" url="http://en.wikipedia.org/wiki?curid=40944" title="Conduction band">
Conduction band

The conduction band quantifies the range of energy required to free an electron from its bond to an atom. Once freed from this bond, the electron becomes a 'delocalized electron', moving freely within the atomic lattice of the material to which the atom belongs. Various materials may be classified by their band gap: this is defined as the difference between the valence and conduction bands.
Electrons within the conduction band are mobile charge carriers in solids, responsible for conduction of electric currents in metals and other good electrical conductors.
The concept has wide applications in the solid-state physics field of semiconductors and insulators.
<br>"Semiconductor band structure"<br>

</doc>
<doc id="40946" url="http://en.wikipedia.org/wiki?curid=40946" title="Conference operation">
Conference operation

In a communications network, a conference operation is an operation that allows a call to be established among three or more stations in such a manner that each of the stations is able to communicate directly with all the other stations. In radio systems, the stations may receive simultaneously, but must transmit one at a time. The common operational modes are "push-to-talk" for telephone operation and "push-to-type" for telegraph and data transmission.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40948" url="http://en.wikipedia.org/wiki?curid=40948" title="Configuration management">
Configuration management

Configuration management (CM) is a systems engineering process for establishing and maintaining consistency of a product's performance, functional and physical attributes with its requirements, design and operational information throughout its life. 
The CM process is widely used by military engineering organizations to manage complex systems, such as weapon systems, vehicles, and information systems. Outside the military, the CM process is also used with IT service management as defined by ITIL, resp. ISO/IEC 20000, and with other domain models in the civil engineering and other industrial engineering segments such as roads, bridges, canals, dams, and buildings.
Introduction.
CM, when applied over the life cycle of a system, provides visibility and control of its performance, functional and physical attributes. CM verifies that a system performs as intended, and is identified and documented in sufficient detail to support its projected life cycle. The CM process facilitates orderly management of system information and system changes for such beneficial purposes as to revise capability; improve performance, reliability, or maintainability; extend life; reduce cost; reduce risk and liability; or correct defects. The relatively minimal cost of implementing CM is returned many fold in cost avoidance. The lack of CM, or its ineffectual implementation, can be very expensive and sometimes can have such catastrophic consequences such as failure of equipment or loss of life.
CM emphasizes the functional relation between parts, subsystems, and systems for effectively controlling system change. It helps to verify that proposed changes are systematically considered to minimize adverse effects. Changes to the system are proposed, evaluated, and implemented using a standardized, systematic approach that ensures consistency, and proposed changes are evaluated in terms of their anticipated impact on the entire system. CM verifies that changes are carried out as prescribed and that documentation of items and systems reflects their true configuration. A complete CM program includes provisions for the storing, tracking, and updating of all system information on a component, subsystem, and system basis.
A structured CM program ensures that documentation (e.g., requirements, design, test, and acceptance documentation) for items is accurate and consistent with the actual physical design of the item. In many cases, without CM, the documentation exists but is not consistent with the item itself. For this reason, engineers, contractors, and management are frequently forced to develop documentation reflecting the actual status of the item before they can proceed with a change. This reverse engineering process is wasteful in terms of human and other resources and can be minimized or eliminated using CM.
History.
Configuration Management originates in the United States Department of Defense in the 1950s as a technical management discipline for hardware material items—and it is now a standard practice in virtually every industry. The CM process became its own technical discipline sometime in the late 1960s when the DoD developed a series of military standards called the "480 series" (i.e., MIL-STD-480 and MIL-STD-481) that were subsequently issued in the 1970s. In 1991, the "480 series" was consolidated into a single standard known as the MIL–STD–973 that was then replaced by MIL–HDBK–61 pursuant to a general DoD goal that reduced the number of military standards in favor of industry technical standards supported by standards developing organizations (SDO). This marked the beginning of what has now evolved into the most widely distributed and accepted standard on CM, ANSI–EIA–649–1998. Now widely adopted by numerous organizations and agencies, the CM discipline's concepts include systems engineering (SE), integrated logistics support (ILS), Capability Maturity Model Integration (CMMI), ISO 9000, Prince2 project management methodology, COBIT, Information Technology Infrastructure Library (ITIL), product lifecycle management, and application lifecycle management. Many of these functions and models have redefined CM from its traditional holistic approach to technical management. Some treat CM as being similar to a librarian activity, and break out change control or change management as a separate or stand alone discipline.
Overview.
CM is the practice of handling changes systematically so that a system maintains its integrity over time. CM implements the policies, procedures,
techniques, and tools that are required to manage, evaluate proposed changes, track the status of changes, and to maintain an inventory of system and support documents as the system changes. CM programs and plans provide technical and administrative direction to the development and implementation of the procedures, functions, services, tools, processes, and resources required to successfully develop and support a complex system. During system development, CM allows program management to track requirements throughout the life cycle through acceptance and operations and maintenance. As changes are inevitably made to the requirements and design, they must be approved and documented, creating an accurate record of the system status. Ideally the CM process is applied throughout the system lifecycle.
The CM process for both hardware and software configuration items comprises five distinct disciplines as established in the and . These disciplines are carried out as policies and procedures for establishing baselines and performing a standard change management process.
Software.
The traditional software configuration management (SCM) process is looked upon by practitioners as the best solution to handling changes in software projects. It identifies the functional and physical attributes of software at various points in time, and performs systematic control of changes to the identified attributes for the purpose of maintaining software integrity and traceability throughout the software development life cycle.
The SCM process further defines the need to trace changes, and the ability to verify that the final delivered software has all of the planned enhancements that are supposed to be included in the release. It identifies four procedures that must be defined for each software project to ensure that a sound SCM process is implemented. They are:
These terms and definitions change from standard to standard, but are essentially the same.
Configuration management database.
The Information Technology Infrastructure Library, also known as ITIL, specifies the use of a Configuration management system (CMS) / Configuration management database (CMDB) as a means of achieving industry best practices for Configuration Management. CMDBs are used to track Configuration Items (CIs) and the dependencies between them, where CIs represent the things in an enterprise that are worth tracking and managing, such as but not limited to computers, software, software licenses, racks, network devices, storage, and even the components within such items.
The benefits of a CMS/CMDB includes being able to perform functions like root cause analysis, impact analysis, change management, and current state assessment for future state strategy development.
Information assurance.
For information assurance, CM can be defined as the management of security features and assurances through control of changes made to hardware, software, firmware, documentation, test, test fixtures, and test documentation throughout the life cycle of an information system. CM for information assurance, sometimes referred to as Secure Configuration Management, relies upon performance, functional, and physical attributes of IT platforms and products and their environments to determine the appropriate security features and assurances that are used to measure a system configuration state. For example, configuration requirements may be different for a network firewall that functions as part of an organization's Internet boundary versus one that functions as an internal local network firewall.
Maintenance systems.
Configuration management is used to maintain an understanding of the status of complex assets with a view to maintaining the highest level of serviceability for the lowest cost. Specifically, it aims to ensure that operations are not disrupted due to the asset (or parts of the asset) overrunning limits of planned lifespan or below quality levels.
In the military, this type of activity is often classed as "mission readiness", and seeks to define which assets are available and for which type of mission; a classic example is whether aircraft on board an aircraft carrier are equipped with bombs for ground support or missiles for defense.
Operating System configuration management.
Configuration management can be used to maintain OS configuration files. Example systems include Quattor, CFEngine, Bcfg2, Puppet, Ansible, Vagrant and Chef.
A theory of configuration maintenance was worked out by Mark Burgess, with a practical implementation on present day computer systems in the software CFEngine able to perform real time repair as well as preventive maintenance.
Preventive maintenance.
Understanding the "as is" state of an asset and its major components is an essential element in preventive maintenance as used in maintenance, repair, and overhaul and enterprise asset management systems.
Complex assets such as aircraft, ships, industrial machinery etc. depend on many different components being serviceable. This serviceability is often defined in terms of the amount of usage the component has had since it was new, since fitted, since repaired, the amount of use it has had over its life and several other limiting factors. Understanding how near the end of their life each of these components is has been a major undertaking involving labor-intensive record keeping until recent developments in software.
Predictive maintenance.
Many types of component use electronic sensors to capture data which provides live condition monitoring. This data is analyzed on board or at a remote location by computer to evaluate its current serviceability and increasingly its likely future state using algorithms which predict potential future failures based on previous examples of failure through field experience and modeling. This is the basis for "predictive maintenance".
Availability of accurate and timely data is essential in order for CM to provide operational value and a lack of this can often be a limiting factor. Capturing and disseminating the operating data to the various support organizations is becoming an industry in itself.
The consumers of this data have grown more numerous and complex with the growth of programs offered by original equipment manufacturers (OEMs). These are designed to offer operators guaranteed availability and make the picture more complex with the operator managing the asset but the OEM taking on the liability to ensure its serviceability. In such a situation, individual components within an asset may communicate directly to an analysis center provided by the OEM or an independent analyst.
Construction.
More recently configuration management has been applied to large construction projects which can often be very complex and have a huge amount of details and changes that need to be documented. Construction agencies such as the Federal Highway Administration have used configuration management for their infrastructure projects. There are construction-based configuration management tools that aim to document change orders and RFIs in order to ensure a project stays on schedule and on budget. These programs can also store information to aid in the maintenance and modification of the infrastructure when it is completed. One such application, ccsNet, was tested in a case study funded by the Federal Transportation Administration (FTA) in which the efficacy of configuration management was measured through comparing the approximately 80% complete construction of the Los Angeles County Metropolitan Transit Agency (LACMTA) 1st and 2nd segments of the Red Line, a $5.3 billion rail construction project. This study yielded results indicating a benefit to using configuration management on projects of this nature.
Certification.
Formal training and certification is available for configuration managers.

</doc>
<doc id="40949" url="http://en.wikipedia.org/wiki?curid=40949" title="Congestion">
Congestion

Congestion may refer to:

</doc>
<doc id="40950" url="http://en.wikipedia.org/wiki?curid=40950" title="Connectionless communication">
Connectionless communication

Connectionless communication is a data transmission method used in packet switching networks by which each data unit is individually addressed and routed based on information carried in each unit, rather than in the setup information of a prearranged, fixed data channel as in connection-oriented communication.
Connectionless communication is often referred to as CL-mode communication.
Under connectionless communication between two network end points, a message can be sent from one end point to another without prior arrangement. The device at one end of the communication transmits data addressed to the other, without first ensuring that the recipient is available and ready to receive the data. Some protocols allow for error correction by requested retransmission. Internet Protocol (IP) and User Datagram Protocol (UDP) are connectionless protocols.
A packet transmitted in a connectionless mode is frequently called a datagram. 
Connectionless protocols are usually described as stateless protocols because the end points have no protocol-defined way to remember where they are in a "conversation" of message exchanges.
In connection-oriented communication the communicating peers must first establish a logical or physical data channel or "connection" in a dialog preceding the exchange of user data.
The connectionless communications has the advantage over connection-oriented communications in that it has low overhead. It also allows for multicast and broadcast operations in which the same data is transmitted to several recipients in a single transmission. 
In connectionless transmissions the service provider usually cannot guarantee that there will be no loss, error insertion, misdelivery, duplication, or out-of-sequence delivery of the packet. However, the effect of errors may be reduced by implementing error correction within an application protocol.
In connectionless mode no optimizations are possible when sending several data units between the same two peers. By establishing a connection at the beginning of such a data exchange the components (routers, bridges) along the network path would be able to pre-compute (and hence cache) routing-related information, avoiding re-computation for every packet. Network components could also reserve capacity for the transfer of the subsequent data units of a video download, for example.
Distinction between connectionless and connection-oriented transmission may take place at several layers of the OSI Reference Model:

</doc>
<doc id="40951" url="http://en.wikipedia.org/wiki?curid=40951" title="Connections per circuit hour">
Connections per circuit hour

In telecommunication, the term connections per circuit hour (CCH) has the following meanings: 
The magnitude of the CCH is an instantaneous value subject to change as a function of time (i.e. from moment to moment), and is subject to study including load curve and busy hour as other measures of traffic are.
See also.
Busy Hour Call Attempts
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40952" url="http://en.wikipedia.org/wiki?curid=40952" title="Connectivity exchange">
Connectivity exchange

Connectivity exchange (CONEX): In an adaptive or manually operated high-frequency (HF) radio network, the automatic or manual exchange of information concerning routes to stations that are not directly reachable by the exchange originator. 
The purpose of the exchange is to identify indirect paths and/or possible relay stations to those stations that are not directly reachable.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40954" url="http://en.wikipedia.org/wiki?curid=40954" title="Contention">
Contention

Contention may refer to:
Contention may also refer to:

</doc>
<doc id="40955" url="http://en.wikipedia.org/wiki?curid=40955" title="Continuous operation">
Continuous operation

In telecommunication, continuous operation is an operation in which certain components, such as nodes, facilities, circuits, or equipment, are in an operational state at all times. Continuous operation usually requires that there be fully redundant configuration, or at least a sufficient "X" out of "Y" degree of redundancy for compatible equipment, where "X" is the number of spare components and "Y" is the number of operational components. 
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40956" url="http://en.wikipedia.org/wiki?curid=40956" title="Contrast">
Contrast

Contrast may refer to:

</doc>
<doc id="40957" url="http://en.wikipedia.org/wiki?curid=40957" title="Control communications">
Control communications

In telecommunication, control communications is the branch of technology devoted to the design, development, and application of communications facilities used specifically for control purposes, such as for controlling (a) industrial processes, (b) movement of resources, (c) electric power generation, distribution, and utilization, (d) communications networks, and (e) transportation systems.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40958" url="http://en.wikipedia.org/wiki?curid=40958" title="Controlled area">
Controlled area

In telecommunication, a controlled area is an area in which uncontrolled movement will not result in compromise of classified information, that is designed to provide administrative control and safety, or that serves as a buffer for controlling access to limited-access areas. It can also refer to an area to which security controls have been applied to protect an information-processing system's equipment and wirelines, equivalent to that required for the information transmitted through the system.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40961" url="http://en.wikipedia.org/wiki?curid=40961" title="Control operation">
Control operation

In telecommunication, a control operation (control function) is an operation that affects the recording, processing, transmission, or interpretation of data. 
Examples of control operations a font change, or a rewind; and transmitting an end-of-transmission (EOT) control character.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40962" url="http://en.wikipedia.org/wiki?curid=40962" title="Convolutional code">
Convolutional code

In telecommunication, a convolutional code is a type of error-correcting code that generates parity symbols via the sliding application of a boolean polynomial function to a data stream. The sliding application represents the 'convolution' of the encoder over the data, which gives rise to the term 'convolutional coding.' 
The sliding nature of the convolutional codes facilities trellis decoding using a time invariant trellis. Time invariant trellis decoding allows convolutional codes to be maximum likelihood soft decision decoded with reasonable complexity. 
The ability to perform economical maximum likelihood soft decision decoding is one of the major benefits of convolutional codes. This is in contrast to classic block codes which are generally represented by a time variant trellis and therefore are typically hard decision decoded. 
Convolutional codes are often characterized by the base code rate and the depth (or memory) of the encoder [n,k,K]. The base code rate is typically given as n/k, where n is the input data rate and k is the output symbol rate. The depth is often called the "constraint length" 'K', where the output is a function of the previous K-1 inputs. The depth may also be given as the number of memory elements 'v' in the polynomial or the maximum possible number of states of the encoder (typically 2^v).
Convolutional codes are often described as continuous. However, it may also be said that convolutional codes have arbitrary block length, rather than that they are continuous, since most real world convolutional encoding is performed on blocks of data. Convolutionally encoded block codes typically employ termination.
The arbitrary block length of convolutional codes can also be contrasted to classic block codes, which generally have fixed block lengths that are determined by algebraic properties.
The code rate of a convolutional code is commonly modified via symbol puncturing. For example, a convolutional code with a 'mother' code rate n/k=1/2 may be punctured to a higher rate of, for example, 7/8 simply by not transmitting a portion of code symbols. The performance of a punctured convolutional code generally scales well with the amount of parity transmitted.
The ability to perform economical soft decision decoding on convolutional codes, as well as the block length and code rate flexibility of convolutional codes, makes them very popular for digital communications.
History.
Convolutional codes were introduced in 1965 by Peter Elias. It was thought that convolutional codes could be decoded with arbitrary quality at the expense of computation and delay. In 1967 Andrew Viterbi determined that convolutional codes could be maximum likelihood decoded with reasonable complexity using time invariant trellis based decoders -- the Viterbi algorithm. Other trellis based decoder algorithms were later developed including the BCJR decoding algorithm. 
Recursive systematic convolutional codes were invented by Claude Berrou around 1991. These codes proved especially useful for iterative processing including the processing of concatenated codes such as turbo codes.
Using the "convolutional" terminology, a classic convolutional code might be considered a Finite impulse response (FIR) filter, while a recursive convolutional code might be considered an Infinite impulse response (IIR) filter.
Where convolutional codes are used.
Convolutional codes are used extensively in numerous applications in order to achieve reliable data transfer, including digital video, radio, mobile communication, and satellite communication. These codes are often implemented in concatenation with a hard-decision code, particularly Reed Solomon. Prior to turbo codes, such constructions were the most efficient, coming closest to the Shannon limit.
Convolutional encoding.
To convolutionally encode data, start with "k" memory registers, each holding 1 input bit. Unless otherwise specified, all memory registers start with a value of 0. The encoder has "n" modulo-2 adders (a modulo 2 adder can be implemented with a single Boolean XOR gate, where the logic is: 0+0 = 0, 0+1 = 1, 1+0 = 1, 1+1 = 0), and "n" generator polynomials — one for each adder (see figure below). An input bit "m"1 is fed into the leftmost register. Using the generator polynomials and the existing values in the remaining registers, the encoder outputs "n" symbols. These symbols may be transmitted or punctured depending on the desired code rate. Now bit shift all register values to the right ("m"1 moves to "m"0, "m"0 moves to "m"-1) and wait for the next input bit. If there are no remaining input bits, the encoder continues shifting until all registers have returned to the zero state (flush bit termination).
The figure below is a rate 1/3 ("m"/"n") encoder with constraint length ("k") of 3. Generator polynomials are "G"1 = (1,1,1), "G"2 = (0,1,1), and "G"3 = (1,0,1). Therefore, output bits are calculated (modulo 2) as follows:
Recursive and non-recursive codes.
The encoder on the picture above is a "non-recursive" encoder. Here's an example of a recursive one and as such it admits a feedback structure:
The example encoder is "systematic" because the input data is also used in the output symbols (Output 2). Codes with output symbols that do not include the input data are called "non-systematic."
Recursive codes are typically systematic and, conversely, non-recursive codes are typically non-systematic. It isn't a strict requirement, but a common practice.
The example encoder in Img. 2. is an 8-state encoder because the 3 registers will create 8 possible encoder states (23). A corresponding decoder trellis will typically use 8 states as well.
Recursive systematic convolutional (RSC) codes have become more popular due to their use in Turbo Codes. Recursive systematic codes are also referred to as pseudo-systematic codes.
Other RSC codes and example applications include:
Useful for LDPC code implementation and as inner constituent code for serial concatenated convolutional codes (SCCC's).
Useful for SCCC's and multidimensional turbo codes.
Useful as constituent code in low error rate turbo codes for applications such as satellite links. Also suitable as SCCC outer code.
Impulse response, transfer function, and constraint length.
A convolutional encoder is called so because it performs a "convolution" of the input stream with the encoder's "impulse responses":
where formula_2 is an input sequence, formula_3 is a sequence from output formula_4 and formula_5 is an impulse response for output formula_4.
A convolutional encoder is a discrete linear time-invariant system. Every output of an encoder can be described by its own transfer function, which is closely related to the generator polynomial. An impulse response is connected with a transfer function through Z-transform.
Transfer functions for the first (non-recursive) encoder are:
Transfer functions for the second (recursive) encoder are:
Define formula_12 by
where, for any rational function formula_14,
Then formula_12 is the maximum of the polynomial degrees of the 
formula_17, and the "constraint length" is defined as formula_18. For instance, in the first example the constraint length is 3, and in the second the constraint length is 4.
Trellis diagram.
A convolutional encoder is a finite state machine. An encoder with "n" binary cells will have 2"n" states.
Imagine that the encoder (shown on Img.1, above) has '1' in the left memory cell ("m"0), and '0' in the right one ("m"-1). ("m"1 is not really a memory cell because it represents a current value). We will designate such a state as "10". According to an input bit the encoder at the next turn can convert either to the "01" state or the "11" state. One can see that not all transitions are possible for (e.g., a decoder can't convert from "10" state to "00" or even stay in "10" state).
All possible transitions can be shown as below:
An actual encoded sequence can be represented as a path on this graph. One valid path is shown in red as an example.
This diagram gives us an idea about "decoding": if a received sequence doesn't fit this graph, then it was received with errors, and we must choose the nearest "correct" (fitting the graph) sequence. The real decoding algorithms exploit this idea.
Free distance and error distribution.
The free distance ("d") is the minimal Hamming distance between different encoded sequences. The "correcting capability" ("t") of a convolutional code is the number of errors that can be corrected by the code. It can be calculated as
Since a convolutional code doesn't use blocks, processing instead a continuous bitstream, the value of "t" applies to a quantity of errors located relatively near to each other. That is, multiple groups of "t" errors can usually be fixed when they are relatively far apart.
Free distance can be interpreted as the minimal length of an erroneous "burst" at the output of a convolutional decoder. The fact that errors appear as "bursts" should be accounted for when designing a concatenated code with an inner convolutional code. The popular solution for this problem is to interleave data before convolutional encoding, so that the outer block (usually Reed-Solomon) code can correct most of the errors.
Decoding convolutional codes.
Several algorithms exist for decoding convolutional codes. For relatively small values of "k", the Viterbi algorithm is universally used as it provides maximum likelihood performance and is highly parallelizable. Viterbi decoders are thus easy to implement in VLSI hardware and in software on CPUs with SIMD instruction sets.
Longer constraint length codes are more practically decoded with any of several sequential decoding algorithms, of which the Fano algorithm is the best known. Unlike Viterbi decoding, sequential decoding is not maximum likelihood but its complexity increases only slightly with constraint length, allowing the use of strong, long-constraint-length codes. Such codes were used in the Pioneer program of the early 1970s to Jupiter and Saturn, but gave way to shorter, Viterbi-decoded codes, usually concatenated with large Reed-Solomon error correction codes that steepen the overall bit-error-rate curve and produce extremely low residual undetected error rates.
Both Viterbi and sequential decoding algorithms return hard decisions: the bits that form the most likely codeword. An approximate confidence measure can be added to each bit by use of the Soft output Viterbi algorithm. Maximum a posteriori (MAP) soft decisions for each bit can be obtained by use of the BCJR algorithm.
Popular convolutional codes.
An especially popular Viterbi-decoded convolutional code, used at least since the Voyager program has a constraint length "k" of 7 and a rate "r" of 1/2.
Punctured convolutional codes.
Puncturing is a technique used to make a "m"/"n" rate code from a "basic" low-rate (e.g., 1/"n") code. It is reached by deletion of some bits in the encoder output. Bits are deleted according to a "puncturing matrix". The following puncturing matrices are the most frequently used:
For example, if we want to make a code with rate 2/3 using the appropriate matrix from the above table, we should take a basic encoder output and transmit every second bit from the first branch and every bit from the second one. The specific order of transmission is defined by the respective communication standard.
Punctured convolutional codes are widely used in the satellite communications, for example, in INTELSAT systems and Digital Video Broadcasting.
Punctured convolutional codes are also called "perforated".
Turbo codes: replacing convolutional codes.
Simple Viterbi-decoded convolutional codes are now giving way to turbo codes, a new class of iterated short convolutional codes that closely approach the theoretical limits imposed by Shannon's theorem with much less decoding complexity than the Viterbi algorithm on the long convolutional codes that would be required for the same performance. Concatenation with an outer algebraic code (e.g., Reed-Solomon) addresses the issue of error floors inherent to turbo code designs.
MATLAB implementation.
MATLAB supports convolutional codes.
For example the encoder shown on Img. 1 can be implemented as follows:
The output is the following:
codedWord1 =
 1 0 1
codedWord2 =
 1 0 1 1 1 0 1 1 1 0 0 0
The bits of the first output stream are at positions 1,4,7...,3k+1... in output vector "codedWord",
respectively second stream at positions 2,5...,3k+2... and the third 3,6...,3k...
Initial state is by default initialized by all zeros.
Convolution code can also be implemented using Verilog HDL language,by making use of corresponding state diagrams and state tables.

</doc>
<doc id="40964" url="http://en.wikipedia.org/wiki?curid=40964" title="Cooperation factor">
Cooperation factor

In facsimile systems, the cooperation factor is the product of the total image scanning length and the scanning density, given by "CF" = "L" σ, where "L" is the scanning line length and σ is the scanning line density, both in compatible units. 
For example, a 20 cm line and a line density of 6 scanning pitches per centimeter would yield a cooperation factor of 120.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40965" url="http://en.wikipedia.org/wiki?curid=40965" title="Copy">
Copy

Copy may refer to: to copy a word from a book to a paper or laptop or computer

</doc>
<doc id="40966" url="http://en.wikipedia.org/wiki?curid=40966" title="Cord circuit">
Cord circuit

In telecommunication, a cord circuit is a switchboard circuit in which a plug-terminated cord is used to establish connections manually between user lines or between trunks and user lines. A number of cord circuits are furnished as part of the switchboard position equipment. The cords may be referred to as front cord and rear cord or trunk cord and station cord. In modern cordless switchboards, the cord-circuit function is switch operated and may be programmable.
In early and middle 20th century telephone exchanges this task was done by a supervisory relay set known variously as junctor circuit or district junctor. Later designs made it a function of the trunk circuit or absorbed it into software.

</doc>
<doc id="40967" url="http://en.wikipedia.org/wiki?curid=40967" title="Core">
Core

Core may refer to:

</doc>
<doc id="40968" url="http://en.wikipedia.org/wiki?curid=40968" title="Corner reflector">
Corner reflector

A corner reflector is a retroreflector consisting of three mutually perpendicular, intersecting flat surfaces, which reflects waves back directly towards the source, but translated. The three intersecting surfaces often have square shapes. Radar corner reflectors made of metal are used to reflect radio waves from radar sets. Optical corner reflectors, called corner cubes, made of three-sided glass prisms, are used in surveying and laser rangefinding.
The corner reflector should not be confused with the corner reflector antenna, which consists of two flat metal surfaces at a right angle, with a dipole antenna in front of them.
How it works.
The incoming ray is reflected three times, once by each surface, which results in a reversal of direction. To see this, the three corresponding normal vectors of the corner's perpendicular sides can be considered to form a basis (a rectangular coordinate system) ("x", "y", "z") in which to represent the direction of an arbitrary incoming ray, ["a", "b", "c"]. When the ray reflects from the first side, say "x", the ray's "x" component, "a", is reversed to −"a" while the "y" and "z" components are unchanged, resulting in a direction of [−"a", "b", "c"]. Similarly, when reflected from side "y" and finally from side "z", the "b" and "c" components are reversed. So the ray direction goes from ["a", "b", "c"] to [−"a", "b", "c"] to [−"a", −"b", "c"] to [−"a", −"b", −"c"] and it leaves the corner reflector with all three components of direction exactly reversed. The distance travelled, relative to a plane normal to the direction of the rays, is also equal for any ray entering the reflector, regardless of the location where it first reflects.
In diagram 1 the green ray is shown as reflecting from only one surface. This is a special case where the incoming ray is exactly normal (perpendicular) to one of the reflective faces. In the same diagram the orange and red rays are shown reflecting off of two surfaces. This is again a special case, the requirement being that the incoming ray is parallel to one of the reflecting planes.
Radar corner reflectors.
Radar corner reflectors are designed to reflect the microwave radio waves emitted by radar sets back toward the radar antenna. This causes them to show a strong "return" on radar screens. A simple corner reflector consists of three conducting sheet metal or screen surfaces at 90° angles to each other, attached to one another at the edges, forming a "corner". These reflect radio waves coming from in front of them back parallel to the incoming beam. To create a corner reflector that will reflect radar waves coming from any direction, 8 corner reflectors are placed back-to-back in an octahedron (diamond) shape. The reflecting surfaces must be larger than several wavelengths of the radio waves to function.
In maritime navigation they are placed on bridge abutments, buoys, ships and, especially, lifeboats, to ensure that these show up strongly on ship radar screens. Corner reflectors are placed on the vessel's masts at a height of at least 4.6 meters (15 feet) above sea level (giving them an approximate minimum horizon distance of 8 kilometers or 4.5 nautical miles). Marine radar uses X-band microwaves with wavelengths of 2.5 - 3.75 cm, so small reflectors less than 30 cm across are used. In aircraft navigation, corner reflectors are installed on rural runways, to make them show up on aircraft radar.
Optical corner reflectors.
In optics, corner reflectors typically consist of three mirrors or reflective prism faces which return an incident light beam in the opposite direction. In surveying, retroreflector prisms are commonly used as targets for long-range electronic distance measurement using a total station.
NASA has put several optical corner reflectors made of quartz, known as the Lunar Laser Ranging Experiment, on the Moon for use in laser time-of-flight measurement to measure the Moon’s orbit more precisely than was possible before.
Automobile and bicycle tail lights are molded with arrays of small corner reflectors, with different sections oriented for viewing from different angles. Reflective paint for visibility at night usually contains retroreflective spherical beads.
Thin plastic with microscopic corner reflector structures can be used as tape, on signs, or sewn or molded onto clothing.
Other examples.
Corner reflectors can also occur accidentally. Tower blocks with balconies are often accidental corner reflectors for sound and return a distinctive echo to an observer making a sharp noise, such as a hand clap, nearby. Similarly, in radar interpretation, an object that has multiple reflections from smooth surfaces produces a radar return of greater magnitude than might be expected from the physical size of the object. This effect was put to use on the ADM-20 Quail, a small missile which had the same radar cross section as a B-52.

</doc>
<doc id="40969" url="http://en.wikipedia.org/wiki?curid=40969" title="Cosmic noise">
Cosmic noise

Cosmic noise and galactic radio noise is random noise that originates outside the Earth's atmosphere. It can be detected and heard on radio receivers.
Elaboration.
Cosmic noise characteristics are similar to the those of thermal noise. Cosmic noise is experienced at frequencies above about 15 MHz when highly directional antennas are pointed toward the sun or to certain other regions of the sky such as the center of the Milky Way Galaxy. Celestial objects like Quasars, super dense objects that lie far from Earth, emit electromagnetic waves in its full spectrum including radio waves. We can also hear the fall of a meteorite in a radio receiver; as the falling object burns from friction with the Earth's atmosphere, ionizing surrounding gases, thereby producing radio waves. Cosmic Microwave Background Radiation (CMBR) from outer space, discovered by Arno Penzias and Robert Wilson, who later won the Nobel Prize for this discovery, is also a form of cosmic noise. CMBR is thought to be a relic of the Big Bang, and pervades the space almost homogeneously over the entire celestial sphere. The bandwidth of the CMBR is wide, though the peak is in the microwave range.

</doc>
<doc id="40970" url="http://en.wikipedia.org/wiki?curid=40970" title="Costas loop">
Costas loop

A Costas loop is a phase-locked loop based circuit which is used for carrier phase recovery from suppressed-carrier modulation signals, such as from double-sideband suppressed carrier signals. It was invented by John P. Costas at General Electric in the 1950s. Its invention was described as having had "a profound effect on modern digital communications".
The primary application of Costas loops is in wireless receivers. Its advantage over the PLL-based detectors is that at small deviations the Costas loop error voltage is sin(2("θi"−"θf")) vs sin("θi"−"θf"). This translates to double the sensitivity and also makes the Costas loop uniquely suited for tracking doppler-shifted carriers esp. in OFDM and GPS receivers
Classical implementation.
In the classical implementation of a Costas loop, a local voltage-controlled oscillator (VCO) provides quadrature outputs, one to each of two phase detectors, "e.g.", product detectors. The same phase of the input signal is also applied to both phase detectors and the output of each phase detector is passed through a low-pass filter. The outputs of these low-pass filters are inputs to another phase detector, the output of which passes through noise-reduction filter before being used to control the voltage-controlled oscillator. The overall loop response is controlled by the two individual low-pass filters that precede the third phase detector while the third low-pass filter serves a trivial role in terms of gain and phase margin.
Mathematical models of Costas loop.
Model of Costas loop in the time domain.
In the simplest case formula_1. Therefore, formula_1 does not affect the input of noise-reduction filter.
Carrier and VCO signals are periodic oscillations formula_3 with high-frequencies formula_4.
Block formula_5 shifts phase of VCO signal by formula_6.
Block formula_7 is an Analog multiplier.
From the mathematical point of view, a linear filter can be described by a system of linear differential equations
Here, formula_9 is a constant matrix, formula_10 is a state vector of filter, formula_11 and formula_12 are constant vectors.
The model of voltage-controlled oscillator is usually assumed to be linear
where formula_14 is a free-running frequency of voltage-controlled oscillator and formula_15 is an oscillator gain. Similar it is possible to consider various nonlinear models of VCO.
Suppose that the frequency of master generator is constant
formula_16
Equation of VCO and equation of filter yield
The system is nonautonomous and rather difficult for investigation.
Model of Costas loop in phase-frequency domain.
In the simplest case, when
standard engineering assumption is that the filter removes the upper sideband
with frequency from the input but leaves the lower sideband without change.
Thus it is assumed that VCO input is formula_19.
This makes Costas loop equivalent to Phase-Locked Loop with phase detector characteristic formula_20 corresponding to the particular waveforms formula_21 and formula_22 of input and VCO signals. It can be proved, that inputs formula_23 and formula_24 of VCO for phase-frequency domain and time domain models are almost equal.
Thus it is possible
to study more simple autonomous system of differential equations
Well-known Krylov–Bogoliubov averaging method allows one to prove that solutions of nonautonomous and autonomous equations are close under some assumptions.
Thus the block-scheme of Costas Loop in the time space can be asymptotically changed to the block-scheme on the level of phase-frequency relations.
The passage to analysis of autonomous dynamical model of Costas loop (in place of the nonautonomous one)
allows one to overcome the difficulties, related with modeling Costas loop in time domain where one has to simultaneously observe very fast time scale of the input signals and slow time scale of signal's phase.

</doc>
<doc id="40972" url="http://en.wikipedia.org/wiki?curid=40972" title="Coupling">
Coupling

A coupling is a device used to connect two shafts together at their ends for the purpose of transmitting power. Couplings do not normally allow disconnection of shafts during operation, however there are torque limiting couplings which can slip or disconnect when some torque limit is exceeded.
The primary purpose of couplings is to join two pieces of rotating equipment while permitting some degree of misalignment or end movement or both. By careful selection, installation and maintenance of couplings, substantial savings can be made in reduced maintenance costs and downtime.
Uses.
Shaft couplings are used in machinery for several purposes. 
The most common of which are the following.
Types.
Rigid.
A rigid coupling is a unit of hardware used to join two shafts within a motor or mechanical system. It may be used to connect two separate systems, such as a motor and a generator, or to repair a connection within a single system. A rigid coupling may also be added between shafts to reduce shock and wear at the point where the shafts meet.
When joining shafts within a machine, mechanics can choose between flexible and rigid couplings. While flexible units offer some movement and give between the shafts, rigid couplings are the most effective choice for precise alignment and secure hold. By precisely aligning the two shafts and holding them firmly in place, rigid couplings help to maximize performance and increase the expected life of the machine. These rigid couplings are available in two basic designs to fit the needs of different applications. Sleeve-style couplings are the most affordable and easiest to use. They consist of a single tube of material with an inner diameter that's equal in size to the shafts. The sleeve slips over the shafts so they meet in the middle of the coupling. A series of set screws can be tightened so they touch the top of each shaft and hold them in place without passing all the way through the coupling.
Clamped or compression rigid couplings come in two parts and fit together around the shafts to form a sleeve. They offer more flexibility than sleeved models, and can be used on shafts that are fixed in place. They generally are large enough so that screws can pass all the way through the coupling and into the second half to ensure a secure hold.Flanged rigid couplings are designed for heavy loads or industrial equipment. They consist of short sleeves surrounded by a perpendicular flange. One coupling is placed on each shaft so the two flanges line up face to face. A series of screws or bolts can then be installed in the flanges to hold them together. Because of their size and durability, flanged units can be used to bring shafts into alignment before they are joined together.
Rigid couplings are used when precise shaft alignment is required; shaft misalignment will affect the coupling's performance as well as its life. Examples:
Sleeve coupling.
A sleeve coupling consists of a pipe whose bore is finished to the required tolerance based on the shaft size. Based on the usage of the coupling a keyway is made in the bore in order to transmit the torque by means of the key. Two threaded holes are provided in order to lock the coupling in position.
Sleeve couplings are also known as Box Couplings. In this case shaft ends are coupled together and abutted against each other which are enveloped by "muff" or "sleeve". A gib head sunk keys hold the two shafts and sleeve together.
in other words,
this is the simplest type of the coupling. It is made from the cast iron and very simple to design and manufacture. It consists of a hollow pipe whose inner diameter is same as diameter of the shafts.
The hollow pipe is fitted over a two or more ends of the shafts with the help of the taper sunk key.a key and sleeve are useful to transmit power from one shaft to another shaft.
Flange coupling.
This coupling has two separate cast iron flanges. Each flange is mounted on the shaft end and keyed to it. The two flanges are coupled together with the help of bolts and nuts. The projected portion of one of the flanges and corresponding recess on the other flange help to bring the shaft into line and to maintain alignment. A flange which is provided with a shroud which shelters the bolts heads and nuts is called protected type flange coupling.
Clamp or split-muff coupling.
In this coupling, the muff or sleeve is made into two halves parts of the cast iron and they are joined together by means of mild steel studs or bolts. The advantages of this coupling is that assembling or disassembling of the coupling is possible without changing the position of the shaft. This coupling is used for heavy power transmission at moderate speed.
Tapered shaft lock.
A tapered lock is a form of keyless shaft locking device that does not require any material to be removed from the shaft. The basic idea is similar to a clamp coupling but the moment of rotation is closer to the center of the shaft. An alternative coupling device to the traditional parallel key, the tapered lock removes the possibility of play due to worn keyways. It is more robust than using a key because maintenance only requires one tool and the self-centering balanced rotation means it lasts longer than a keyed joint would, but the downside is that it costs more.
Hirth.
Hirth joints use tapered teeth on two shaft ends meshed together to transmit torque.
Flexible.
Flexible couplings are used to transmit torque from one shaft to another when the two shafts are slightly misaligned. Flexible couplings can accommodate varying degrees of misalignment up to 3° and some parallel misalignment. In addition, they can also be used for vibration damping or noise reduction.This coupling is used to protect the driving and driven shaft members against harmful effects produce due to misalignment of the shafts, sudden shock loads, shaft expansion or vibrations etc.
Bush pin Type flange coupling.
This is used for slightly imperfect alignment of the two shafts.
This is modified form of the protected type flange coupling. This type of coupling has pins and it works with coupling bolts. The rubber or leather bushes are used over the pins. The coupling has two halves dissimilar in construction. The pins are rigidly fastened by nuts to one of the flange and kept loose on the other flange. This coupling is used to connect of shafts which having a small parallel misalignment, angular misalignment or axial misalignment. In this coupling the rubber bushing absorbs shocks and vibration during its operations. This type of coupling is mostly used to couple electric motors and machines.
Beam.
A "beam" coupling, also known as "helical" coupling, is a flexible coupling for transmitting torque between two shafts while allowing for angular misalignment, parallel offset and even axial motion, of one shaft relative to the other. This design utilizes a single piece of material and becomes flexible by removal of material along a spiral path resulting in a curved flexible beam of helical shape. Since it is made from a single piece of material, the Beam Style coupling does not exhibit the backlash found in some multi-piece couplings. Another advantage of being an all machined coupling is the possibility to incorporate features into the final product while still keep the single piece integrity.
Changes to the lead of the helical beam provide changes to misalignment capabilities as well as other performance characteristics such as torque capacity and torsional stiffness. It is even possible to have multiple starts within the same helix.
The material used to manufacture the beam coupling also affects its performance and suitability for specific applications such as food, medical and aerospace. Materials are typically aluminum alloy and stainless steel, but they can also be made in acetal, maraging steel and titanium. The most common applications are attaching encoders to shafts and motion control for robotics.
Constant velocity.
There are various types of constant-velocity (CV) couplings: Rzeppa joint, Double cardan joint, and Thompson coupling.
Diaphragm.
Diaphragm couplings transmit torque from the outside diameter of a flexible plate to the inside diameter, across the spool or spacer piece, and then from inside to outside diameter. The deforming of a plate or series of plates from I.D. to O.D accomplishes the misalignment.
Disc.
Disc couplings transmit torque from a driving to a driven bolt tangentially on a common bolt circle. Torque is transmitted between the bolts through a series of thin, stainless steel discs assembled in a pack. Misalignment is accomplished by deforming of the material between the bolts.
Gear.
A "gear coupling" is a mechanical device for transmitting torque between two shafts that are not collinear. It consists of a flexible joint fixed to each shaft. The two joints are connected by a third shaft, called the spindle.
Each joint consists of a 1:1 gear ratio internal/external gear pair. The tooth flanks and outer diameter of the external gear are crowned to allow for angular displacement between the two gears. Mechanically, the gears are equivalent to rotating splines with modified profiles. They are called gears because of the relatively large size of the teeth.
Gear couplings and universal joints are used in similar applications. Gear couplings have higher torque densities than universal joints designed to fit a given space while universal joints induce lower vibrations. The limit on torque density in universal joints is due to the limited cross sections of the cross and yoke. The gear teeth in a gear coupling have high backlash to allow for angular misalignment. The excess backlash can contribute to vibration.
Gear couplings are generally limited to angular misalignments, i.e., the angle of the spindle relative to the axes of the connected shafts, of 4-5°. Universal joints are capable of higher misalignments.
Single joint gear couplings are also used to connected two nominally coaxial shafts. In this application the device is called a gear-type flexible, or flexible coupling. The single joint allows for minor misalignments such as installation errors and changes in shaft alignment due to operating conditions. These types of gear couplings are generally limited to angular misalignments of 1/4-1/2°.
Grid.
A "grid coupling" is composed of two shaft hubs, a metallic grid spring, and a split cover kit. Torque is transmitted between the two coupling shaft hubs through the metallic grid spring element.
Like metallic gear and disc couplings, grid couplings have a high torque density. A benefit of grid couplings, over either gear or disc couplings, is the ability their grid coupling spring elements have to absorb and spread peak load impact energy over time. This reduces the magnitude of peak loads and offers some vibration dampening capability. A negative of the grid coupling design is that it generally is very limited in its ability to accommodate misalignment.
Oldham.
An "Oldham coupling" has three discs, one coupled to the input, one coupled to the output, and a middle disc that is joined to the first two by tongue and groove. The tongue and groove on one side is perpendicular to the tongue and groove on the other. The middle disc rotates around its center at the same speed as the input and output shafts. Its center traces a circular orbit, twice per rotation, around the midpoint between input and output shafts. Often springs are used to reduce backlash of the mechanism. An advantage to this type of coupling, as compared to two universal joints, is its compact size. The coupler is named for John Oldham who invented it in Ireland, in 1821, to solve a paddle placement problem in a paddle steamer design.
Rag joint.
Rag joints are commonly used on automotive steering linkages and drive trains. When used on a drive train they are sometimes known as giubos.
Magnetic Coupling.
A magnetic coupling uses magnetic forces to transmit the power from one shaft to the other without any contact, this allows for full medium separation.
Coupling maintenance and failure.
Coupling maintenance is generally a simple matter, requiring a regularly scheduled inspection of each coupling. It consists of:
Even with proper maintenance, however, couplings can fail. Underlying reasons for failure, other than maintenance, include:
The only way to improve coupling life is to understand what caused the failure and to correct it prior to installing a new coupling.
Some external signs that indicate potential coupling failure include:
Checking the coupling balance.
Couplings are normally balanced at the factory prior to being shipped, but they occasionally go out of balance in operation. Balancing can be difficult and expensive, and is normally done only when operating tolerances are such that the effort and the expense are justified. The amount of coupling unbalance that can be tolerated by any system is dictated by the characteristics of the specific connected machines and can be determined by detailed analysis or experience.

</doc>
<doc id="40973" url="http://en.wikipedia.org/wiki?curid=40973" title="Cover">
Cover

Cover or covers may refer to:

</doc>
<doc id="40974" url="http://en.wikipedia.org/wiki?curid=40974" title="Critical angle">
Critical angle

Critical angle can refer to:

</doc>
<doc id="40975" url="http://en.wikipedia.org/wiki?curid=40975" title="Critical frequency">
Critical frequency

In telecommunication, the term critical frequency has the following meanings: 
Critical Frequency changes with time of day, atmospheric conditions and angle of fire of the radio waves by antenna.
The existence of the critical frequency is the result of electron limitation, "i.e.," the inadequacy of the existing number of free electrons to support reflection at higher frequencies.
In signal processing the "critical frequency" it is also another name for the Nyquist frequency.
Critical frequency is the highest magnitude of frequency above which the waves penetrates the ionosphere and below which the waves are reflected back from the ionosphere.
It is denoted by "fc".
Its value is not fixed and it depends upon electron density of ionosphere.
It is given by:
fc=9√Nmax
Nmax is maximum electron density.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40976" url="http://en.wikipedia.org/wiki?curid=40976" title="Crosstalk (disambiguation)">
Crosstalk (disambiguation)

Crosstalk may mean:

</doc>
<doc id="40977" url="http://en.wikipedia.org/wiki?curid=40977" title="International Cryptology Conference">
International Cryptology Conference

CRYPTO, the International Cryptology Conference, is one of the largest academic conferences in cryptography and cryptanalysis. It is organized by the International Association for Cryptologic Research (IACR), and it is held yearly in August in Santa Barbara, California at the University of California, Santa Barbara.
The first CRYPTO was held in 1981. It was the first major conference on cryptology, and was all the more important because relations between government, industry and academia were rather tense. Encryption was considered a very sensitive subject and the coming together of delegates from different countries was unheard-of at the time. The initiative for the formation of the IACR came during CRYPTO '82, and CRYPTO '83 was the first IACR sponsored conference.

</doc>
<doc id="40978" url="http://en.wikipedia.org/wiki?curid=40978" title="Cryptochannel">
Cryptochannel

In telecommunication, a cryptochannel is a complete system of crypto-communications between two or more holders. The basic unit for naval cryptographic communication. It includes: (a) the cryptographic aids prescribed; (b) the holders thereof; (c) the indicators or other means of identification; (d) the area or areas in which effective; (e) the special purpose, if any, for which provided; and (f) pertinent notes as to distribution, usage, "etc." A cryptochannel is analogous to a radio circuit. 

</doc>
<doc id="40979" url="http://en.wikipedia.org/wiki?curid=40979" title="Crystal oscillator">
Crystal oscillator

A crystal oscillator is an electronic oscillator circuit that uses the mechanical resonance of a vibrating crystal of piezoelectric material to create an electrical signal with a very precise frequency. This frequency is commonly used to keep track of time (as in quartz wristwatches), to provide a stable clock signal for digital integrated circuits, and to stabilize frequencies for radio transmitters and receivers. The most common type of piezoelectric resonator used is the quartz crystal, so oscillator circuits incorporating them became known as crystal oscillators, but other piezoelectric materials including polycrystalline ceramics are used in similar circuits.
Quartz crystals are manufactured for frequencies from a few tens of kilohertz to hundreds of megahertz. More than two billion crystals are manufactured annually. Most are used for consumer devices such as wristwatches, clocks, radios, computers, and cellphones. Quartz crystals are also found inside test and measurement equipment, such as counters, signal generators, and oscilloscopes.
History.
Piezoelectricity was discovered by Jacques and Pierre Curie in 1880. Paul Langevin first investigated quartz resonators for use in sonar during World War I. The first crystal-controlled oscillator, using a crystal of Rochelle salt, was built in 1917 and patented in 1918 by Alexander M. Nicholson at Bell Telephone Laboratories, although his priority was disputed by Walter Guyton Cady. Cady built the first quartz crystal oscillator in 1921.
Other early innovators in quartz crystal oscillators include G. W. Pierce and Louis Essen.
Quartz crystal oscillators were developed for high-stability frequency references during the 1920s and 1930s. Prior to crystals, radio stations controlled their frequency with tuned circuits, which could easily drift off frequency by 3-4 kHz. Since broadcast stations were assigned frequencies only 10 kHz apart, interference between adjacent stations due to frequency drift was a common problem. In 1925 Westinghouse installed a crystal oscillator in its flagship station KDKA, and by 1926 quartz crystals were used to control the frequency of many broadcasting stations and were popular with amateur radio operators. In 1928, Warren Marrison (of Bell Telephone Laboratories) developed the first quartz crystal clock. With accuracies of up to 1 sec in 30 years (30 ms/year or 10−7), quartz clocks replaced precision pendulum clocks as the world's most accurate timekeepers until atomic clocks were developed in the 1950s. Utilizing the early work at Bell Labs, AT&T eventually established their Frequency Control Products division, later spun off and known today as Vectron International.
A number of firms started producing quartz crystals for electronic use during this time. Using what are now considered primitive methods, about 100,000 crystal units were produced in the United States during 1939. Through World War II crystals were made from natural quartz crystal, virtually all from Brazil. Shortages of crystals during the war caused by the demand for accurate frequency control of military and naval radios and radars spurred postwar research into culturing synthetic quartz, and by 1950 a hydrothermal process for growing quartz crystals on a commercial scale was developed at Bell Laboratories. By the 1970s virtually all crystals used in electronics were synthetic.
In 1968, Juergen Staudte invented a photolithographic process for manufacturing quartz crystal oscillators while working at North American Aviation (now Rockwell) that allowed them to be made small enough for portable products like watches.
Although crystal oscillators still most commonly use quartz crystals, devices using other materials are becoming more common, such as ceramic resonators.
Operation.
A crystal is a solid in which the constituent atoms, molecules, or ions are packed in a regularly ordered, repeating pattern extending in all three spatial dimensions.
Almost any object made of an elastic material could be used like a crystal, with appropriate transducers, since all objects have natural resonant frequencies of vibration. For example, steel is very elastic and has a high speed of sound. It was often used in mechanical filters before quartz. The resonant frequency depends on size, shape, elasticity, and the speed of sound in the material. High-frequency crystals are typically cut in the shape of a simple, rectangular plate. Low-frequency crystals, such as those used in digital watches, are typically cut in the shape of a tuning fork. For applications not needing very precise timing, a low-cost ceramic resonator is often used in place of a quartz crystal.
When a crystal of quartz is properly cut and mounted, it can be made to distort in an electric field by applying a voltage to an electrode near or on the crystal. This property is known as electrostriction or inverse piezoelectricity. When the field is removed, the quartz will generate an electric field as it returns to its previous shape, and this can generate a voltage. The result is that a quartz crystal behaves like a circuit composed of an inductor, capacitor and resistor, with a precise resonant frequency. (See RLC circuit.)
Quartz has the further advantage that its elastic constants and its size change in such a way that the frequency dependence on temperature can be very low. The specific characteristics will depend on the mode of vibration and the angle at which the quartz is cut (relative to its crystallographic axes). Therefore, the resonant frequency of the plate, which depends on its size, will not change much, either. This means that a quartz clock, filter or oscillator will remain accurate. For critical applications the quartz oscillator is mounted in a temperature-controlled container, called a crystal oven, and can also be mounted on shock absorbers to prevent perturbation by external mechanical vibrations.
Modeling.
Electrical model.
A quartz crystal can be modeled as an electrical network with a low-impedance (series) and a high-impedance (parallel) resonance points spaced closely together. Mathematically (using the Laplace transform), the impedance of this network can be written as:
or
where "s" is the complex frequency (formula_4), formula_5 is the series resonant angular frequency, and formula_6 is the parallel resonant angular frequency.
Adding capacitance across a crystal will cause the (parallel) resonance frequency to decrease. Adding inductance across a crystal will cause the (parallel) resonance frequency to increase. These effects can be used to adjust the frequency at which a crystal oscillates. Crystal manufacturers normally cut and trim their crystals to have a specified resonance frequency with a known "load" capacitance added to the crystal. For example, a crystal intended for a 6 pF load has its specified parallel resonance frequency when a 6.0 pF capacitor is placed across it. Without the load capacitance, the resonance frequency is higher.
Resonance modes.
A quartz crystal provides both series and parallel resonance. The series resonance is a few kilohertz lower than the parallel one. Crystals below 30 MHz are generally operated between series and parallel resonance, which means that the crystal appears as an inductive reactance in operation, this inductance forming a parallel resonant circuit with externally connected parallel capacitance. Any small additional capacitance in parallel with the crystal will thus pull the frequency downwards. Moreover, the effective inductive reactance of the crystal can be reduced by adding a capacitor in series with the crystal. This latter technique can provide a useful method of trimming the oscillatory frequency within a narrow range; in this case inserting a capacitor in series with the crystal will raise the frequency of oscillation. For a crystal to operate at its specified frequency, the electronic circuit has to be exactly that specified by the crystal manufacturer. Note that these points imply a subtlety concerning crystal oscillators in this frequency range: the crystal does not usually oscillate at precisely either of its resonant frequencies.
Crystals above 30 MHz (up to >200 MHz) are generally operated at series resonance where the impedance appears at its minimum and equal to the series resistance. For these crystals the series resistance is specified (<100 Ω) instead of the parallel capacitance. To reach higher frequencies, a crystal can be made to vibrate at one of its overtone modes, which occur near multiples of the fundamental resonant frequency. Only odd numbered overtones are used. Such a crystal is referred to as a 3rd, 5th, or even 7th overtone crystal. To accomplish this, the oscillator circuit usually includes additional LC circuits to select the desired overtone.
Temperature effects.
A crystal's frequency characteristic depends on the shape or 'cut' of the crystal. A tuning fork crystal is usually cut such that its frequency over temperature is a parabolic curve centered around 25 °C. This means that a tuning fork crystal oscillator will resonate close to its target frequency at room temperature, but will slow down when the temperature either increases or decreases from room temperature. A common parabolic coefficient for a 32 kHz tuning fork crystal is −0.04 ppm/°C².
In a real application, this means that a clock built using a regular 32 kHz tuning fork crystal will keep good time at room temperature, lose 2 minutes per year at 10 degrees Celsius above (or below) room temperature and lose 8 minutes per year at 20 degrees Celsius above (or below) room temperature due to the quartz crystal.
Electrical oscillators.
The crystal oscillator circuit sustains oscillation by taking a voltage signal from the quartz resonator, amplifying it, and feeding it back to the resonator. The rate of expansion and contraction of the quartz is the resonant frequency, and is determined by the cut and size of the crystal. When the energy of the generated output frequencies matches the losses in the circuit, an oscillation can be sustained.
An oscillator crystal has two electrically conductive plates, with a slice or tuning fork of quartz crystal sandwiched between them. During startup, the controlling circuit places the crystal into an unstable equilibrium, and due to the positive feedback in the system, any tiny fraction of noise will start to get amplified, ramping up the oscillation. The crystal resonator can also be seen as a highly frequency-selective filter in this system: it will only pass a very narrow subband of frequencies around the resonant one, attenuating everything else. Eventually, only the resonant frequency will be active. As the oscillator amplifies the signals coming out of the crystal, the signals in the crystal's frequency band will become stronger, eventually dominating the output of the oscillator. The narrow resonance band of the quartz crystal filters out all the unwanted frequencies.
The output frequency of a quartz oscillator can be either that of the fundamental resonance or of a multiple of that resonance, called a harmonic frequency. Harmonics are an exact integer multiple of the fundamental frequency. But, like many other mechanical resonators, crystals exhibit several modes of oscillation, usually at approximately odd integer multiples of the fundamental frequency. These are termed "overtone modes", and oscillator circuits can be designed to excite them. The overtone modes are at frequencies which are approximate, but not exact odd integer multiples of that of the fundamental mode, and overtone frequencies are therefore not exact harmonics of the fundamental.
High frequency crystals are often designed to operate at third, fifth, or seventh overtones. Manufacturers have difficulty producing crystals thin enough to produce fundamental frequencies over 30 MHz. To produce higher frequencies, manufacturers make overtone crystals tuned to put the 3rd, 5th, or 7th overtone at the desired frequency, because they are thicker and therefore easier to manufacture than a fundamental crystal that would produce the same frequency—although exciting the desired overtone frequency requires a slightly more complicated oscillator circuit.
A fundamental crystal oscillator circuit is simpler and more efficient and has more pullability than a third overtone circuit.
Depending on the manufacturer, the highest available fundamental frequency may be 25 MHz to 66 MHz.
A major reason for the wide use of crystal oscillators is their high Q factor. A typical "Q" value for a quartz oscillator ranges from 104 to 106, compared to perhaps 102 for an LC oscillator. The maximum "Q" for a high stability quartz oscillator can be estimated as "Q" = 1.6 × 107/"f", where "f" is the resonance frequency in megahertz.
One of the most important traits of quartz crystal oscillators is that they can exhibit very low phase noise.
In many oscillators, any spectral energy at the resonant frequency will be amplified by the oscillator, resulting in a collection of tones at different phases.
In a crystal oscillator, the crystal mostly vibrates in one axis, therefore only one phase is dominant.
This property of low phase noise makes them particularly useful in telecommunications where stable signals are needed, and in scientific equipment where very precise time references are needed.
Environmental changes of temperature, humidity, pressure, and vibration can change the resonant frequency of a quartz crystal, but there are several designs that reduce these environmental effects. These include the TCXO, MCXO, and OCXO (defined below). These designs (particularly the OCXO) often produce devices with excellent short-term stability. The limitations in short-term stability are due mainly to noise from electronic components in the oscillator circuits. Long term stability is limited by aging of the crystal.
Due to aging and environmental factors (such as temperature and vibration), it is difficult to keep even the best quartz oscillators within one part in 1010 of their nominal frequency without constant adjustment. For this reason, atomic oscillators are used for applications requiring better long-term stability and accuracy.
Spurious frequencies.
For crystals operated at series resonance or pulled away from the main mode by the inclusion of a series inductor or capacitor, significant (and temperature-dependent) spurious responses may be experienced. Though most spurious modes are typically some tens of kilohertz above the wanted series resonance their temperature coefficient will be different from the main mode and the spurious response may move through the main mode at certain temperatures. Even if the series resistances at the spurious resonances appear higher than the one at wanted frequency a rapid change in the main mode series resistance can occur at specific temperatures when the two frequencies are coincidental.
A consequence of these activity dips is that the oscillator may lock at a spurious frequency (at specific temperatures). This is generally minimized by ensuring that the maintaining circuit has insufficient gain to activate unwanted modes.
Spurious frequencies are also generated by subjecting the crystal to vibration. This modulates the resonance frequency to a small degree by the frequency of the vibrations. SC-cut crystals are designed to minimize the frequency effect of mounting stress and they are therefore less sensitive to vibration. Acceleration effects including gravity are also reduced with SC cut crystals as is frequency change with time due to long term mounting stress variation.
There are disadvantages with SC cut shear mode crystals, such as the need for the maintaining oscillator to discriminate against other closely related unwanted modes and increased frequency change due to temperature when subject to a full ambient range. SC cut crystals are most advantageous where temperature control at their temperature of zero temperature coefficient (turnover) is possible, under these circumstances an overall stability performance from premium units can approach the stability of Rubidium frequency standards.
Commonly used crystal frequencies.
Crystals can be manufactured for oscillation over a wide range of frequencies, from a few kilohertz up to several hundred megahertz. Many applications call for a crystal oscillator frequency conveniently related to some other desired frequency, so hundreds of standard crystal frequencies are made in large quantities and stocked by electronics distributors. For example, many (non-television) applications use 3.579545 MHz crystals since they are made in large quantities for NTSC color television receivers. Using frequency dividers, frequency multipliers and phase locked loop circuits, it is practical to derive a wide range of frequencies from one reference frequency.
Crystal structures and materials.
The most common material for oscillator crystals is quartz. At the beginning of the technology, natural quartz crystals were used; now synthetic crystalline quartz grown by hydrothermal synthesis is predominant due to higher purity, lower cost, and more convenient handling. One of the few remaining uses of natural crystals is for pressure transducers in deep wells. During World War II and for some time afterwards, natural quartz was considered a strategic material by the USA. Large crystals were imported from Brazil. Raw "lascas", the source material quartz for hydrothermal synthesis, are imported to USA or mined locally by Coleman Quartz. The average value of as-grown synthetic quartz in 1994 was 60 USD/kg.
Two types of quartz crystals exist: left-handed and right-handed, differing in the optical rotation but identical in other physical properties. Both left and right-handed crystals can be used for oscillators, if the cut angle is correct. In manufacture, right-handed quartz is generally used. The SiO4 tetrahedrons form parallel helices; the direction of twist of the helix determines the left- or right-hand orientation. The helixes are aligned along the z-axis and merged, sharing atoms. The mass of the helixes forms a mesh of small and large channels parallel to the z-axis; the large ones are large enough to allow some mobility of smaller ions and molecules through the crystal.
Quartz exists in several phases. At 573 °C at 1 atmosphere (and at higher temperatures and higher pressures) the α-quartz undergoes quartz inversion, transforms reversibly to β-quartz. The reverse process however is not entirely homogeneous and crystal twinning occurs. Care has to be taken during manufacture and processing to avoid the phase transformation. Other phases, e.g. the higher-temperature phases tridymite and cristobalite, are not significant for oscillators. All quartz oscillator crystals are the α-quartz type.
Infrared spectrophotometry is used as one of the methods for measuring the quality of the grown crystals. The wavenumbers 3585, 3500, and 3410 cm−1 are commonly used. The measured value is based on the absorption bands of the OH radical and the infrared Q value is calculated. The electronic grade crystals, grade C, have Q of 1.8 million or above; the premium grade B crystals have Q of 2.2 million, and special premium grade A crystals have Q of 3.0 million. The Q value is calculated only for the z region; crystals containing other regions can be adversely affected. Another quality indicator is the etch channel density; when the crystal is etched, tubular channels are created along linear defects. For processing involving etching, e.g. the wristwatch tuning fork crystals, low etch channel density is desirable. The etch channel density for swept quartz is about 10–100 and significantly more for unswept quartz. Presence of etch channels and etch pits degrades the resonator's Q and introduces nonlinearities.
Quartz crystals can be grown for specific purposes.
Crystals for AT-cut are the most common in mass production of oscillator materials; the shape and dimensions are optimized for high yield of the required wafers. High-purity quartz crystals are grown with especially low content of aluminium, alkali metal and other impurities and minimal defects; the low amount of alkali metals provides increased resistance to ionizing radiation. Crystals for wrist watches, for cutting the tuning fork 32768 Hz crystals, are grown with very low etch channel density.
Crystals for SAW devices are grown as flat, with large X-size seed with low etch channel density.
Special high-Q crystals, for use in highly stable oscillators, are grown at constant slow speed and have constant low infrared absorption along the entire Z axis. Crystals can be grown as Y-bar, with a seed crystal in bar shape and elongated along the Y axis, or as Z-plate, grown from a plate seed with Y-axis direction length and X-axis width. The region around the seed crystal contains a large number of crystal defects and should not be used for the wafers.
Crystals grow anisotropically; the growth along the Z axis is up to 3 times faster than along the X axis. The growth direction and rate also influences the rate of uptake of impurities. Y-bar crystals, or Z-plate crystals with long Y axis, have four growth regions usually called +X, -X, Z, and S. The distribution of impurities during growth is uneven; different growth areas contain different levels of contaminants. The z regions are the purest, the small occasionally present s regions are less pure, the +x region is yet less pure, and the -x region has the highest level of impurities. The impurities have a negative impact on radiation hardness, susceptibility to twinning, filter loss, and long and short term stability of the crystals. Different-cut seeds in different orientations may provide other kinds of growth regions. The growth speed of the -x direction is slowest due to the effect of adsorption of water molecules on the crystal surface; aluminium impurities suppress growth in two other directions. The content of aluminium is lowest in z region, higher in +x, yet higher in -x, and highest in s; the size of s regions also grows with increased amount of aluminium present. The content of hydrogen is lowest in z region, higher in +x region, yet higher in s region, and highest in -x. Aluminium inclusions transform into color centers with gamma ray irradiation, causing a darkening of the crystal proportional to the dose and level of impurities; the presence of regions with different darkness reveals the different growth regions.
The dominant type of defect of concern in quartz crystals is the substitution of an Al(III) for a Si(IV) atom in the crystal lattice. The aluminium ion has an associated interstitial charge compensator present nearby, which can be a H+ ion (attached to the nearby oxygen and forming a hydroxyl group, called Al-OH defect), Li+ ion, Na+ ion, K+ ion (less common), or an electron hole trapped in a nearby oxygen atom orbital. The composition of the growth solution, whether it is based on lithium or sodium alkali compounds, determines the charge compensating ions for the aluminium defects. The ion impurities are of concern as they are not firmly bound and can migrate through the crystal, altering the local lattice elasticity and the resonant frequency of the crystal. Other common impurities of concern are e.g. iron(III) (interstitial), fluorine, boron(III), phosphorus(V) (substitution), titanium(IV) (substitution, universally present in magmatic quartz, less common in hydrothermal quartz), and germanium(IV) (substitution). Sodium and iron ions can cause inclusions of acnite and elemeusite crystals. Inclusions of water may be present in fast-grown crystals; interstitial water molecules are abundant near the crystal seed. Another defect of importance is the hydrogen containing growth defect, when instead of a Si-O-Si structure, a pair of Si-OH HO-Si groups is formed; essentially a hydrolyzed bond. Fast-grown crystals contain more hydrogen defects than slow-grown ones. These growth defects source as supply of hydrogen ions for radiation-induced processes and forming Al-OH defects. Germanium impurities tend to trap electrons created during irradiation; the alkali metal cations then migrate towards the negatively charged center and form a stabilizing complex. Matrix defects can also be present; oxygen vacancies, silicon vacancies (usually compensated by 4 hydrogens or 3 hydrogens and a hole), peroxy groups, etc. Some of the defects produce localized levels in the forbidden band, serving as charge traps; Al(III) and B(III) typically serve as hole traps while electron vacancies, titanium, germanium, and phosphorus atoms serve as electron traps. The trapped charge carriers can be released by heating; their recombination is the cause of thermoluminescence.
The mobility of interstitial ions depends strongly on temperature. Hydrogen ions are mobile down to 10 K, but alkali metal ions become mobile only at temperatures around and above 200 K.
The hydroxyl defects can be measured by near-infrared spectroscopy. The trapped holes can be measured by electron spin resonance. The Al-Na+ defects show as an acoustic loss peak due to their stress-induced motion; the Al-Li+ defects do not form a potential well so are not detectable this way. Some of the radiation-induced defects during their thermal annealing produce thermoluminescence; defects related to aluminium, titanium, and germanium can be distinguished.
Swept crystals are crystals that have undergone a solid-state electrodiffusion purification process. Sweeping involves heating the crystal above 500 °C in a hydrogen-free atmosphere, with a voltage gradient of at least 1 kilovolt/cm, for several (usually over 12) hours. The migration of impurities and the gradual replacement of alkali metal ions with hydrogen (when swept in air) or electron holes (when swept in vacuum) causes a weak electric current through the crystal; decay of this current to a constant value signals the end of the process. The crystal is then left to cool, while the electric field is maintained. The impurities are concentrated at the cathode region of the crystal, which is cut off afterwards and discarded. Swept crystals have increased resistance to radiation, as the dose effects are dependent on the level of alkali metal impurities; they are suitable for use in devices exposed to ionizing radiation, e.g. for nuclear and space technology. Sweeping under vacuum at higher temperatures and higher field strengths yields yet more radiation-hard crystals. The level and character of impurities can be measured by infrared spectroscopy. Quartz can be swept in both α and β phase; sweeping in β phase is faster, but the phase transition may induce twinning. Twinning can be mitigated by subjecting the crystal to compression stress in the X direction, or an AC or DC electric field along the X axis while the crystal cools through the phase transformation temperature region.
Sweeping can be also used to introduce one kind of an impurity into the crystal. Lithium, sodium, and hydrogen swept crystals are used for e.g. studying quartz behavior.
Very small crystals for high fundamental mode frequencies can be manufactured by photolithography.
Crystals can be adjusted to exact frequencies by laser trimming. A technique used in the world of amateur radio for slight decrease of the crystal frequency may be achieved by exposing crystals with silver electrodes to vapors of iodine, which causes a slight mass increase on the surface by forming a thin layer of silver iodide; such crystals however had problematic long-term stability. Another method commonly used is electrochemical increase or decrease of silver electrode thickness by submerging a resonator in lapis dissolved in water, citric acid in water, or water with salt, and using the resonator as one electrode, and a small silver electrode as another.
By choosing the direction of current, one can either increase, or decrease the mass of electrodes.
Details were published in "Radio" magazine (3/1978) by UB5LEV.
Raising frequency by scratching off parts of the electrodes is advised against, as this may damage the crystal and lower its Q factor. Capacitor trimmers can be also used for frequency adjustment of the oscillator circuit.
Some other piezoelectric materials than quartz can be employed; e.g. single crystals of lithium tantalate, lithium niobate, lithium borate, berlinite, gallium arsenide, lithium tetraborate, aluminium phosphate, bismuth germanium oxide, polycrystalline zirconium titanate ceramics, high-alumina ceramics, silicon-zinc oxide composite, or dipotassium tartrate; some materials may be more suitable for specific applications. An oscillator crystal can be also manufactured by depositing the resonator material on the silicon chip surface. Crystals of gallium phosphate, langasite, langanite and langanate are about 10 times more pullable than the corresponding quartz crystals, and are used in some VCXO oscillators.
Stability and aging.
The frequency stability is determined by the crystal's Q. It is inversely dependent on the frequency, and on the constant that is dependent on the particular cut. Other factors influencing Q are the overtone used, the temperature, the level of driving of the crystal, the quality of the surface finish, the mechanical stresses imposed on the crystal by bonding and mounting, the geometry of the crystal and the attached electrodes, the material purity and defects in the crystal, type and pressure of the gas in the enclosure, interfering modes, and presence and absorbed dose of ionizing and neutron radiation.
Temperature influences the operating frequency; various forms of compensation are used, from analog compensation (TCXO) and microcontroller compensation (MCXO) to stabilization of the temperature with a crystal oven (OCXO). The crystals possess temperature hysteresis; the frequency at a given temperature achieved by increasing the temperature is not equal to the frequency on the same temperature achieved by decreasing the temperature. The temperature sensitivity depends primarily on the cut; the temperature compensated cuts are chosen as to minimize frequency/temperature dependence. Special cuts can be made with linear temperature characteristics; the LC cut is used in quartz thermometers. Other influencing factors are the overtone used, the mounting and electrodes, impurities in the crystal, mechanical strain, crystal geometry, rate of temperature change, thermal history (due to hysteresis), ionizing radiation, and drive level.
Crystals tend to suffer anomalies in their frequency/temperature and resistance/temperature characteristics, known as activity dips. These are small downward (in frequency) or upward (in resistance) excursions localized at certain temperatures, with their temperature position dependent on the value of the load capacitors.
Mechanical stresses also influence the frequency. The stresses can be induced by mounting, bonding, and application of the electrodes, by differential thermal expansion of the mounting, electrodes, and the crystal itself, by differential thermal stresses when there is a temperature gradient present, by expansion or shrinkage of the bonding materials during curing, by the air pressure that is transferred to the ambient pressure within the crystal enclosure, by the stresses of the crystal lattice itself (nonuniform growth, impurities, dislocations), by the surface imperfections and damage caused during manufacture, and by the action of gravity on the mass of the crystal; the frequency can therefore be influenced by position of the crystal. Other dynamic stress inducing factors are shocks, vibrations, and acoustic noise. Some cuts are less sensitive to stresses; the SC (Stress Compensated) cut is an example. Atmospheric pressure changes can also introduce deformations to the housing, influencing the frequency by changing stray capacitances.
Atmospheric humidity influences the thermal transfer properties of air, and can change electrical properties of plastics by diffusion of water molecules into their structure, altering the dielectric constants and electrical conductivity.
Other factors influencing the frequency are the power supply voltage, load impedance, magnetic fields, electric fields (in case of cuts that are sensitive to them, e.g. SC), the presence and absorbed dose of γ-particles and ionizing radiation, and the age of the crystal.
Crystals undergo slow gradual change of frequency with time, known as aging. There are many mechanisms involved. The mounting and contacts may undergo relief of the built-in stresses. Molecules of contamination either from the residual atmosphere, outgassed from the crystal, electrodes or packaging materials, or introduced during sealing the housing can be adsorbed on the crystal surface, changing its mass; this effect is exploited in quartz crystal microbalances. The composition of the crystal can be gradually altered by outgassing, diffusion of atoms of impurities or migrating from the electrodes, or the lattice can be damaged by radiation. Slow chemical reactions may occur on or in the crystal, or on the inner surfaces of the enclosure. Electrode material, e.g. chromium or aluminium, can react with the crystal, creating layers of metal oxide and silicon; these interface layers can undergo changes in time. The pressure in the enclosure can change due to varying atmospheric pressure, temperature, leaks, or outgassing of the materials inside. Factors outside of the crystal itself are e.g. aging of the oscillator circuitry (and e.g. change of capacitances), and drift of parameters of the crystal oven. External atmosphere composition can also influence the aging; hydrogen can diffuse through nickel housing. Helium can cause similar issues when it diffuses through glass enclosures of rubidium standards.
Gold is a favored electrode material for low-aging resonators; its adhesion to quartz is strong enough to maintain contact even at strong mechanical shocks, but weak enough to not support significant strain gradients (unlike chromium, aluminium, and nickel). Gold also does not form oxides; it adsorbs organic contaminants from the air, but these are easy to remove. However, gold alone can undergo delamination; a layer of chromium is therefore sometimes used for improved binding strength. Silver and aluminium are often used as electrodes; however both form oxide layers with time that increases the crystal mass and lowers frequency. Silver can be passivated by exposition to iodine vapors, forming a layer of silver iodide. Aluminium oxidizes readily but slowly, until about 5 nm thickness is reached; increased temperature during artificial aging does not significantly increase the oxide forming speed; a thick oxide layer can be formed during manufacture by anodizing. Exposition of silver-plated crystal to iodine vapors can be also used in amateur conditions for lowering the crystal frequency slightly; the frequency can be also increased by scratching off parts of the electrodes, but that carries risk of damage to the crystal and loss of Q.
A DC voltage bias between the electrodes can accelerate the initial aging, probably by induced diffusion of impurities through the crystal. Placing a capacitor in series with the crystal and a several-megohm resistor in parallel can minimize such voltages.
Crystals suffer from minor short-term frequency fluctuations as well. The main causes of such noise are e.g. thermal noise (which limits the noise floor), phonon scattering (influenced by lattice defects), adsorption/desorption of molecules on the surface of the crystal, noise of the oscillator circuits, mechanical shocks and vibrations, acceleration and orientation changes, temperature fluctuations, and relief of mechanical stresses. The short-term stability is measured by four main parameters: Allan variance (the most common one specified in oscillator data sheets), phase noise, spectral density of phase deviations, and spectral density of fractional frequency deviations. The effects of acceleration and vibration tend to dominate the other noise sources; surface acoustic wave devices tend to be more sensitive than bulk acoustic wave (BAW) ones, and the stress-compensated cuts are even less sensitive. The relative orientation of the acceleration vector to the crystal dramatically influences the crystal's vibration sensitivity. Mechanical vibration isolation mountings can be used for high-stability crystals.
Crystals are sensitive to shock. The mechanical stress causes a short-term change in the oscillator frequency due to the stress-sensitivity of the crystal, and can introduce a permanent change of frequency due to shock-induced changes of mounting and internal stresses (if the elastic limits of the mechanical parts are exceeded), desorption of contamination from the crystal surfaces, or change in parameters of the oscillator circuit. High magnitudes of shocks may tear the crystals off their mountings (especially in the case of large low-frequency crystals suspended on thin wires), or cause cracking of the crystal. Crystals free of surface imperfections are highly shock-resistant; chemical polishing can produce crystals able to survive tens of thousands g.
Phase noise plays significant role in frequency synthesis systems using frequency multiplication; a multiplication of a frequency by N increases the phase noise power by N2. A frequency multiplication by 10 times multiplies the magnitude of the phase error by 10 times. This can be disastrous for systems employing e.g. PLL or FSK technologies.
Crystals are somewhat sensitive to radiation damage. Natural quartz is much more sensitive than artificially grown crystals, and sensitivity can be further reduced by sweeping the crystal – heating the crystal to at least 400 °C in a hydrogen-free atmosphere in an electric field of at least 500 V/cm for at least 12 hours. Such swept crystals have a very low response to steady ionizing radiation. Some Si(IV) atoms are replaced with Al(III) impurities, each having a compensating Li+ or Na+ cation nearby. Ionization produces electron-hole pairs; the holes are trapped in the lattice near the Al atom, the resulting Li and Na atoms are loosely trapped along the Z axis; the change of the lattice near the Al atom and the corresponding elastic constant then causes a corresponding change in frequency. Sweeping removes the Li+ and Na+ ions from the lattice, reducing this effect. The Al3+ site can also trap hydrogen atoms. All crystals have a transient negative frequency shift after exposure to an X-ray pulse; the frequency then shifts gradually back; natural quartz reaches stable frequency after 10–1000 seconds, with a negative offset to pre-irradiation frequency, artificial crystals return to a frequency slightly lower or higher than pre-irradiation, swept crystals anneal virtually back to original frequency. The annealing is faster at higher temperatures. Sweeping under vacuum at higher temperatures and field strength can further reduce the crystal's response to X-ray pulses. Series resistance of unswept crystals increases after an X-ray dose, and anneals back to a somewhat higher value for a natural quartz (requiring a corresponding gain reserve in the circuit) and back to pre-irradiation value for synthetic crystals. Series resistance of swept crystals is unaffected. Increase of series resistance degrades Q; too high increase can stop the oscillations. Neutron radiation induces frequency changes by introducing dislocations into the lattice by knocking out atoms, a single fast neutron can produce many defects; the SC and AT cut frequency increases roughly linearly with absorbed neutron dose, while the frequency of the BT cuts decreases. Neutrons also alter the temperature-frequency characteristics. Frequency change at low ionizing radiation doses is proportionally higher than for higher doses. High-intensity radiation can stop the oscillator by inducing photoconductivity in the crystal and transistors; with a swept crystal and properly designed circuit the oscillations can restart within 15 microseconds after the radiation burst. Quartz crystals with high levels of alkali metal impurities lose Q with irradiation; Q of swept artificial crystals is unaffected. Irradiation with higher doses (over 105 rad) lowers sensitivity to subsequent doses. Very low radiation doses (below 300 rad) have disproportionately higher effect, but this nonlinearity saturates at higher doses. At very high doses, the radiation response of the crystal saturates as well, due to the finite number of impurity sites that can be affected.
Magnetic fields have little effect on the crystal itself, as quartz is diamagnetic; eddy currents or AC voltages can however be induced into the circuits, and magnetic parts of the mounting and housing may be influenced.
After the power-up, the crystals take several seconds to minutes to "warm up" and stabilize their frequency. The oven-controlled OCXOs require usually 3–10 minutes for heating up to reach thermal equilibrium; the oven-less oscillators stabilize in several seconds as the few milliwatts dissipated in the crystal cause a small but noticeable level of internal heating.
Crystals have no inherent failure mechanisms; some have operated in devices for decades. Failures may be, however, introduced by faults in bonding, leaky enclosures, corrosion, frequency shift by aging, breaking the crystal by too high mechanical shock, or radiation-induced damage when nonswept quartz is used. Crystals can be also damaged by overdriving.
The crystals have to be driven at the appropriate drive level. While AT cuts tend to be fairly forgiving, with only their electrical parameters, stability and aging characteristics being degraded when overdriven, low-frequency crystals, especially flexural-mode ones, may fracture at too high drive levels. The drive level is specified as the amount of power dissipated in the crystal. The appropriate drive levels are about 5 microwatts for flexural modes up to 100 kHz, 1 microwatt for fundamental modes at 1–4 MHz, 0.5 microwatts for fundamental modes 4–20 MHz, and 0.5 microwatts for overtone modes at 20–200 MHz. Too low drive level may cause problems with starting the oscillator. Low drive levels are better for higher stability and lower power consumption of the oscillator. Higher drive levels, in turn, reduce the impact of noise by increasing the signal-to-noise ratio.
The stability of AT cut crystals decreases with increasing frequency. For more accurate higher frequencies it is better to use a crystal with lower fundamental frequency, operating at an overtone.
Aging decreases logarithmically with time, the largest changes occurring shortly after manufacture. Artificially aging a crystal by prolonged storage at 85 to 125 °C can increase its long-term stability.
A badly designed oscillator circuit may suddenly begin oscillating on an overtone. In 1972, a train in Fremont, California crashed due to a faulty oscillator. An inappropriate value of the tank capacitor caused the crystal in a control board to be overdriven, jumping to an overtone, and causing the train to speed up instead of slow down.
Crystal cuts.
The resonator plate can be cut from the source crystal in many different ways. The orientation of the cut influences the crystal's aging characteristics, frequency stability, thermal characteristics, and other parameters. These cuts operate at bulk acoustic wave (BAW); for higher frequencies, surface acoustic wave (SAW) devices are employed.
The T in the cut name marks a temperature-compensated cut, a cut oriented in a way that the temperature coefficients of the lattice are minimal; the FC and SC cuts are also temperature-compensated.
The high frequency cuts are mounted by their edges, usually on springs; the stiffness of the spring has to be optimal, as if it is too stiff, mechanical shocks could be transferred to the crystal and cause it to break, and too little stiffness may allow the crystal to collide with the inside of the package when subjected to a mechanical shock, and break. Strip resonators, usually AT cuts, are smaller and therefore less sensitive to mechanical shocks. At the same frequency and overtone, the strip will have less pullability, higher resistance, and higher temperature coefficient.
The low frequency cuts are mounted at the nodes where they are virtually motionless; thin wires are attached at such points on each side between the crystal and the leads. The large mass of the crystal suspended on the thin wires makes the assembly sensitive to mechanical shocks and vibrations.
The crystals are usually mounted in hermetically sealed glass or metal cases, filled with a dry and inert atmosphere, usually vacuum, nitrogen, or helium. Plastic housings can be used as well, but those are not hermetic and another secondary sealing has to be built around the crystal.
Several resonator configurations are possible, in addition to the classical way of directly attaching leads to the crystal. E.g. the BVA resonator (Boîtier à Vieillissement Amélioré, Enclosure with Improved Aging), developed in 1976; the parts that influence the vibrations are machined from a single crystal (which reduces the mounting stress), and the electrodes are deposited not on the resonator itself but on the inner sides of two condenser discs made of adjacent slices of the quartz from the same bar, forming a three-layer sandwich with no stress between the electrodes and the vibrating element. The gap between the electrodes and the resonator act as two small series capacitors, making the crystal less sensitive to circuit influences. The architecture eliminates the effects of the surface contacts between the electrodes, the constraints in the mounting connections, and the issues related to ion migration from the electrodes into the lattice of the vibrating element. The resulting configuration is rugged, resistant to shock and vibration, resistant to acceleration and ionizing radiation, and has improved aging characteristics. AT cut is usually used, though SC cut variants exist as well. BVA resonators are often used in spacecraft applications.
In the 1930s to 1950s, it was fairly common for people to adjust the frequency of the crystals by manual grinding. The crystals were ground using a fine abrasive slurry, or even a toothpaste, to increase their frequency. A slight decrease by 1–2 kHz when the crystal was overground was possible by marking the crystal face with a pencil lead, at the cost of a lowered Q.
The frequency of the crystal is slightly adjustable ("pullable") by modifying the attached capacitances. A varactor, a diode with capacitance depending on applied voltage, is often used in voltage-controlled crystal oscillators, VCXO. The crystal cuts are usually AT or rarely SC, and operate in fundamental mode; the amount of available frequency deviation is inversely proportional to the square of the overtone number, so a third overtone will have only one-ninth of the pullability of the fundamental mode. SC cuts, while more stable, are significantly less pullable.
Circuit notations and abbreviations.
On electrical schematic diagrams, "crystals" are designated with the class letter "Y" (Y1, Y2, etc.) Oscillators, whether they are crystal "oscillators" or other, are designated with the class letter "G" (G1, G2, etc.) (See IEEE Std 315-1975, or ANSI Y32.2-1975.) On occasion, one may see a crystal designated on a schematic with "X" or "XTAL", or a crystal oscillator with "XO", but these forms are deprecated.
Crystal oscillator types and their abbreviations:

</doc>
<doc id="40980" url="http://en.wikipedia.org/wiki?curid=40980" title="Curve-fitting compaction">
Curve-fitting compaction

Curve-fitting compaction is data compaction accomplished by replacing data to be stored or transmitted with an analytical expression.
Examples of curve-fitting compaction consisting of discretization and then interpolation are:
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40981" url="http://en.wikipedia.org/wiki?curid=40981" title="Customer office terminal">
Customer office terminal

In telecommunications, the term customer office terminal has the following meanings:
"Note:" An example of a customer office terminal is a stand-alone multiplexer located on the customer premises. 
"Note:" This function may be integrated into the ET.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40982" url="http://en.wikipedia.org/wiki?curid=40982" title="Customer-premises equipment">
Customer-premises equipment

Customer-premises equipment or customer-provided equipment (CPE) is any terminal and associated equipment located at a subscriber's premises and connected with a carrier's telecommunication channel at the demarcation point ("demarc"). The demarc is a point established in a building or complex to separate customer equipment from the equipment located in either the distribution infrastructure or central office of the communications service provider.
CPE generally refers to devices such as telephones, routers, switches, residential gateways (RG), set-top boxes, fixed mobile convergence products, home networking adapters and Internet access gateways that enable consumers to access communications service providers' services and distribute them around their house via a local area network (LAN). 
Also included are key telephone systems and most private branch exchanges. Excluded from CPE are overvoltage protection equipment and pay telephones.
CPE can refer to devices purchased by the subscriber, or to those provided by the operator or service provider.
History.
The two phrases, "customer-"premises" equipment" and "customer-"provided" equipment", reflect the history of this equipment.
Under the Bell System monopoly in the United States (post Communications Act of 1934), the Bell System owned the phones, and one could not attach one's own devices to the network, or even attach anything to the phones. Thus phones were property of the Bell System, located on customers' premises – hence, customer-"premises" equipment. In the U.S. Federal Communications Commission (FCC) proceeding the Second Computer Inquiry, the FCC ruled that telecommunications carriers could no longer bundle CPE with telecommunications service, uncoupling the market power of the telecommunications service monopoly from the CPE market, and creating a competitive CPE market.
With the gradual breakup of the Bell monopoly, starting with Hush-A-Phone v. United States [1956], which allowed some non-Bell owned equipment to be connected to the network (a process called interconnection), equipment on customers' premises became increasingly owned by customers, not the telco. Indeed, one eventually became able to purchase one's own phone – hence, customer-"provided" equipment.
In the Pay TV industry many operators and service providers offer subscribers a set-top box with which to receive video services, in return for a monthly fee. As offerings have evolved to include multiple services [voice and data] operators have increasingly given consumers the opportunity to rent additional devices like access modems, internet gateways and video extenders that enable them to access multiple services, and distribute them to a range of Consumer Electronics devices around the home.
Technology evolution.
Hybrid devices.
The growth of multiple-service operators, offering triple or quad-play services, required the development of hybrid CPE to make it easy for subscribers to access voice, video and data services. The development of this technology was led by Pay TV operators looking for a way to deliver video services via both traditional broadcast and broadband IP networks. Spain’s Telefonica was the first operator to launch a hybrid broadcast and broadband TV service in 2003 with its Movistar TV DTT/IPTV offering, while Polish satellite operator 'n' was the first to offer its subscribers a Three-way hybrid (or Tri-brid) broadcast and broadband TV service, which launched in 2009
Set-back box.
The term set-back box is used in the digital TV industry to describe a piece of consumer hardware that enables them to access both linear broadcast and internet-based video content, plus a range of interactive services like Electronic Programme Guides (EPG), Pay Per View (PPV) and Video on Demand (VOD) as well as internet browsing, and view them on a large screen television set. Unlike standard set-top boxes, which sit on top of or below the , a set-back box has a smaller form factor to enable it to be mounted to the rear of the display panel flat panel TV, hiding it from view.
Home Gateway.
A residential gateway is a home networking device used to connect devices in the home to the Internet or other WAN.
It is an umbrella term, used to cover multi-function networking appliances used in homes, which may combine a DSL modem or cable modem, a network switch, a consumer-grade router, and a wireless access point. In the past, such functions were provided by separate devices, but in recent years technological convergence has enabled multiple functions to merged into a single device.
One of the first home gateway devices to be launched was selected by Telecom Italia to enable the operator to offer triple play services in 2002 . Along with a SIP VoIP handset for making voice calls, it enabled subscribers to access voice, video and data services over a 10MB symmetrical ADSL fiber connection.
Virtual gateway.
The virtual gateway concept enables consumers to access video and data services and distribute them around their homes using software rather than hardware. The first virtual gateway was introduced in 2010 by Advanced Digital Broadcast at the IBC exhibition in Amsterdam. The ADB Virtual Gateway uses software that resides within the middleware and is based on open standards, including DLNA home networking and the DTCP-IP standard, to ensure that all content, including paid-for encrypted content like Pay TV services, can only be accessed by secure CE devices.
Broadband.
A subscriber unit, or SU is a broadband radio that is installed at a business or residential location to connect to an access point to send/receive high speed data wired or wirelessly. Devices commonly referred to as a subscriber unit include cable modems, access gateways, home networking adapters and mobile phones.
WAN.
The terms “customer-premises equipment”, “Customer-provided Equipment”, or “CPE” may also refer to any devices that terminate a WAN circuit, such as an ISDN, E-carrier/T-carrier, DSL, or metro Ethernet. This includes any customer-owned hardware at the customer′s site: routers, firewalls, switches, PBXs, VoIP gateways, sometimes CSU/DSU and modems.
Application Areas

</doc>
<doc id="40983" url="http://en.wikipedia.org/wiki?curid=40983" title="Customer service unit">
Customer service unit

In telecommunication, a customer service unit (CSU) is a device that provides an accessing arrangement at a user location to either switched or point-to-point, data-conditioned circuits at a specifically established data signaling rate. 
A CSU provides local loop equalization, transient protection, isolation, and central office loop-back testing capability.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40985" url="http://en.wikipedia.org/wiki?curid=40985" title="Cutback technique">
Cutback technique

In telecommunications, a cutback technique is a destructive technique for determining certain optical fiber transmission characteristics, such as attenuation and bandwidth.
Procedure.
The measurement technique consists of:
The cut should be made to retain 1 meter or more of the fiber, in order to establish equilibrium mode distribution conditions for the second measurement. In a multimode fiber, the lack of an equilibrium mode distribution could introduce errors in the measurement due to output coupling effects. In a single-mode fiber, measuring a shorter cutback fiber could result in significant transmission of cladding modes (light carried in the cladding rather than the core of the optical fiber), distorting the measurement. The errors introduced will result in conservative results ("i.e.", higher transmission losses and lower bandwidths) than would be realized under equilibrium conditions. 
Benefits.
The benefit of this technique is that it allows measurement of the fiber characteristics without introducing errors due to variation in the launch conditions. For example, the coupling efficiency of the light source is kept consistent between the initial and the cutback measurements.
Several characteristics may be determined using the same test fiber. 
Attenuation measurement.
Since the attenuation is defined as proportional to the logarithm of the ratio between formula_1 and formula_2, where formula_3 is the power at point formula_4 and formula_5 respectively. Using the cutback technique, the power transmitted through a fiber of known length is measured and compared with the same measurement for the same fiber cut to a length of formula_6 approximately.
Related techniques.
A variation of the cutback technique is the substitution method, in which measurements are made on a full length of fiber, and then on a short length of fiber having the same characteristics (core size, numerical aperture), with the results from the short length being subtracted to give the results for the full length.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40986" url="http://en.wikipedia.org/wiki?curid=40986" title="Cutoff frequency">
Cutoff frequency

In physics and electrical engineering, a cutoff frequency, corner frequency, or break frequency is a boundary in a system's frequency response at which energy flowing through the system begins to be reduced (attenuated or reflected) rather than passing through.
Typically in electronic systems such as filters and communication channels, cutoff frequency applies to an edge in a lowpass, highpass, bandpass, or band-stop characteristic – a frequency characterizing a boundary between a passband and a stopband. It is sometimes taken to be the point in the filter response where a transition band and passband meet, for example, as defined by a 3 dB corner (a frequency for which the output of the circuit is −3 dB of the nominal passband value). Alternatively, a stopband corner frequency may be specified as a point where a transition band and a stopband meet: a frequency for which the attenuation is larger than the required stopband attenuation, which for example may be 30 dB or 100 dB.
In the case of a waveguide or an antenna, the cutoff frequencies correspond to the lower and upper cutoff wavelengths.
Electronics.
In electronics, cutoff frequency or corner frequency is the frequency either above or below which the power output of a circuit, such as a line, amplifier, or electronic filter has fallen to a given proportion of the power in the passband. Most frequently this proportion is one half the passband power, also referred to as the 3 dB point since a fall of 3 dB corresponds approximately to half power. As a voltage ratio this is a fall to formula_1 of the passband voltage. Other ratios besides the 3 dB point may also be relevant, for example see Chebyshev Filters below.
Single-pole transfer function example.
The simplest low-pass filter transfer function,
has a single pole at "s" = -1/α. The magnitude of this function in the "j"ω plane is
At cutoff
Hence, the cutoff frequency is given by
Where "s" is the s-plane variable, ω is angular frequency and "j" is the imaginary unit.
Chebyshev Filters.
Sometimes other ratios are more convenient than the 3 dB point. For instance, in the case of the Chebyshev filter it is usual to define the cutoff frequency as the point after the last peak in the frequency response at which the level has fallen to the design value of the passband ripple. The amount of ripple in this class of filter can be set by the designer to any desired value, hence the ratio used could be any value.
Communications.
In communications, the term cutoff frequency can mean the frequency below which a radio wave fails to penetrate a layer of the ionosphere at the incidence angle required for transmission between two specified points by reflection from the layer.
Waveguides.
The cutoff frequency of an electromagnetic waveguide is the lowest frequency for which a mode will propagate in it. In fiber optics, it is more common to consider the cutoff wavelength, the maximum wavelength that will propagate in an optical fiber or waveguide. The cutoff frequency is found with the characteristic equation of the Helmholtz equation for electromagnetic waves, which is derived from the electromagnetic wave equation by setting the longitudinal wave number equal to zero and solving for the frequency. Thus, any exciting frequency lower than the cutoff frequency will attenuate, rather than propagate. The following derivation assumes lossless walls. The value of c, the speed of light, should be taken to be the group velocity of light in whatever material fills the waveguide.
For a rectangular waveguide, the cutoff frequency is
where the integers formula_7 are the mode numbers, and "a" and "b" the lengths of the sides of the rectangle. For TE modes, formula_8 (but formula_9 is not allowed), while for TM modes formula_10.
The cutoff frequency of the TM01 mode (next higher from dominant mode TE11) in a waveguide of circular cross-section (the transverse-magnetic mode with no angular dependence and lowest radial dependence) is given by 
where formula_12 is the radius of the waveguide, and formula_13 is the first root of formula_14, the bessel function of the first kind of order 1.
The dominant mode TE11 cutoff frequency is given by
For a single-mode optical fiber, the cutoff wavelength is the wavelength at which the normalized frequency is approximately equal to 2.405.
Mathematical analysis.
The starting point is the wave equation (which is derived from the Maxwell equations),
which becomes a Helmholtz equation by considering only functions of the form 
Substituting and evaluating the time derivative gives
The function formula_19 here refers to whichever field (the electric field or the magnetic field) has no vector component in the longitudinal direction - the "transverse" field. It is a property of all the eigenmodes of the electromagnetic waveguide that at least one of the two fields is transverse. The "z" axis is defined to be along the axis of the waveguide.
The "longitudinal" derivative in the Laplacian can further be reduced by considering only functions of the form 
where formula_21 is the longitudinal wavenumber, resulting in
where subscript T indicates a 2-dimensional transverse Laplacian. The final step depends on the geometry of the waveguide. The easiest geometry to solve is the rectangular waveguide. In that case the remainder of the Laplacian can be evaluated to its characteristic equation by considering solutions of the form 
Thus for the rectangular guide the Laplacian is evaluated, and we arrive at
The transverse wavenumbers can be specified from the standing wave boundary conditions for a rectangular geometry crossection with dimensions "a" and "b":
where "n" and "m" are the two integers representing a specific eigenmode. Performing the final substitution, we obtain
which is the dispersion relation in the rectangular waveguide. The cutoff frequency formula_28 is the critical frequency between propagation and attenuation, which corresponds to the frequency at which the longitudinal wavenumber formula_29 is zero. It is given by
The wave equations are also valid below the cutoff frequency, where the longitudinal wave number is imaginary. In this case, the field decays exponentially along the waveguide axis and the wave is thus evanescent.

</doc>
<doc id="40989" url="http://en.wikipedia.org/wiki?curid=40989" title="Data access arrangement">
Data access arrangement

The term data access arrangement (DAA) has the following meanings: 
Data access arrangements are an integral part of all modems built for the public telephone network. In view of mixed voice and data access, DAAs are more generally referred to as direct access arrangements.

</doc>
<doc id="40990" url="http://en.wikipedia.org/wiki?curid=40990" title="Data bank">
Data bank

In telecommunications, a data bank is a repository of information on one or more subjects that is organized in a way that facilitates local or remote information retrieval. A data bank may be either centralized or decentralized.
In computers the data bank is the same as in telecommunication (i.e. it is the repository of data. The data in the data bank can be things such as credit card transactions or it can be any data base of a company where large quantities of queries are being processed on daily bases). 
Data bank may also refer to an organization primarily concerned with the construction and maintenance of a database.

</doc>
<doc id="40991" url="http://en.wikipedia.org/wiki?curid=40991" title="Data compaction">
Data compaction

In telecommunication, data compaction is the reduction of the number of data elements, bandwidth, cost, and time for the generation, transmission, and storage of data without loss of information by eliminating unnecessary redundancy, removing irrelevancy, or using special coding. 
Examples of data compaction methods are the use of fixed-tolerance bands, variable-tolerance bands, slope-keypoints, sample changes, curve patterns, curve fitting, variable-precision coding, frequency analysis, and probability analysis. 
Simply squeezing noncompacted data into a smaller space, for example by increasing packing density or by transferring data on punched cards onto magnetic tape, is not data compaction. 
Whereas data compaction reduces the amount of data used to represent a given amount of information, data compression does not.
Everyday examples.
The use of acronyms in texting is an everyday example. The number of bits required to transmit and store "WYSIWYG" is reduced from its expanded equivalent (7 characters vs 28). The representation of Mersenne primes is another example. The largest known as of 2013[ [update]] is over 17 million digits long but it is represented as "M"57885161 in a much more compacted form.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40992" url="http://en.wikipedia.org/wiki?curid=40992" title="Data element">
Data element

In metadata, the term data element is an atomic unit of data that has precise meaning or precise semantics. A data element has:
Data elements usage can be discovered by inspection of software applications or application data files through a process of manual or automated Application Discovery and Understanding. Once data elements are discovered they can be registered in a metadata registry. 
In telecommunication, the term data element has the following components: 
In the areas of databases and data systems more generally a data element is a concept forming part of a data model. As an element of data representation, a collection of data elements forms a data structure.
In Practice.
In practice, data elements (fields, columns, attributes, etc.) are sometimes "over loaded", meaning a given data element will have multiple potential meanings. While a known bad practice, over loading is nevertheless a very real factor or barrier to understanding what a system is doing.

</doc>
<doc id="40993" url="http://en.wikipedia.org/wiki?curid=40993" title="Data forwarder">
Data forwarder

In telecommunications, a data forwarder is a device that 
and 
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40994" url="http://en.wikipedia.org/wiki?curid=40994" title="Datagram">
Datagram

A datagram is a basic transfer unit associated with a packet-switched network. The delivery, arrival time, and order of arrival need not be guaranteed by the network.
History.
The term "datagram" appeared first within the project CYCLADES, a packet-switched network created in the early 1970s, and was coined by Louis Pouzin by combining the words "data" and "telegram". CYCLADES was the first network to make the hosts responsible for the reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms.
“The inspiration for datagrams had two sources. One was Donald Davies' studies. He had done some simulation of datagram networks, although he had not built any, and it looked technically viable. The second inspiration was I like things simple. I didn't see any real technical motivation to overlay two levels of end-to-end protocols. I thought one was enough.”—Louis Pouzin
These concepts were later adopted for the creation of the Internet Protocol (IP) and other network protocols.
Definition.
RFC 1594 defines the term Datagram as follows:
“A self-contained, independent entity of data carrying sufficient information to be routed from the source to the destination computer without reliance on earlier exchanges between this source and destination computer and the transporting network.”—RFC 1594
A datagram needs to be self-contained without reliance on earlier exchanges because there is no connection of fixed duration between the two communicating points as there is, for example, in most voice telephone conversations.
Datagram service is often compared to a mail delivery service, the user only provides the destination address, but receives no guarantee of delivery, and no confirmation upon successful delivery. Datagram service is therefore considered unreliable. Datagram service routes datagrams without first creating a predetermined path. Datagram service is therefore considered connectionless. There is also no consideration given to the order in which it and other datagrams are sent or received. In fact, many datagrams in the same group can travel along different paths before reaching the same destination.
Structure.
Each datagram has two components, a header and a data payload. The header contains all the information sufficient for routing from the originating equipment to the destination without relying on prior exchanges between the equipment and the network. Headers may include source and destination addresses as well as a type field. The payload is the data to be transported. This process of nesting data payloads in a tagged header is called encapsulation.
Examples.
Internet Protocol.
The Internet Protocol (IP) defines standards for several types of datagrams. 
The internet layer is a datagram service provided by a IP. For example UDP is run by a datagram service in the internet layer. IP is entirely a connectionless, best effort, unreliable, message delivery service. TCP is a higher level protocol running on top of IP that can provide the addition of a connection-oriented service. 
The term "datagram" is often considered synonymous to "packet" but there are some nuances. The term "datagram" is generally reserved for packets of an unreliable service, which cannot notify the sender if delivery fails, while the term "packet" applies to any packet, reliable or not. Datagrams are the IP packets that provide a quick and unreliable service like UDP, and all IP packets are datagrams;
however, at the TCP layer what is termed a TCP "segment" is the sometimes necessary IP fragmentation of a datagram, but those are referred to as "packets".

</doc>
<doc id="40995" url="http://en.wikipedia.org/wiki?curid=40995" title="Data integrity">
Data integrity

Data integrity refers to maintaining and assuring the accuracy and consistency of data over its entire life-cycle, and is a critical aspect to the design, implementation and usage of any system which stores, processes, or retrieves data. The term data integrity is broad in scope and may have widely different meanings depending on the specific context – even under the same general umbrella of computing. This article provides only a broad overview of some of the different types and concerns of data integrity.
Data integrity is the opposite of data corruption, which is a form of data loss. The overall intent of any data integrity technique is the same: ensure data is recorded exactly as intended (such as a database correctly rejecting mutually exclusive possibilities,) and upon later retrieval, ensure the data is the same as it was when it was originally recorded. In short, data integrity aims to prevent unintentional changes to information. Data integrity is not to be confused with data security, the discipline of protecting data from unauthorized parties.
Any unintended changes to data as the result of a storage, retrieval or processing operation, including malicious intent, unexpected hardware failure, and human error, is failure of data integrity. If the changes are the result of unauthorized access, it may also be a failure of data security. Depending on the data involved this could manifest itself as benign as a single pixel in an image appearing a different color than was originally recorded, to the loss of vacation pictures or a business-critical database, to even catastrophic loss of human life in a life-critical system.
Physical vs. logical integrity.
Data integrity can be roughly divided into two overlapping categories:
Physical integrity - deals with challenges associated with correctly storing and fetching the data itself. Challenges with physical integrity may include electromechanical faults, design flaws, material fatigue, corrosion, power outages, natural disasters, acts of war and terrorism, and other special environmental hazards such as ionizing radiation, extreme temperatures, pressures and g-forces. Ensuring physical integrity includes methods such as redundant hardware, an uninterruptible power supply, certain types of RAID arrays, radiation hardened chips, ECC memory, use of a clustered file system, using file systems that employ block level checksums such as ZFS, storage arrays that compute parity calculations such as Exclusive or or use a Cryptographic hash function and even having a watchdog timer on critical subsystems.
Physical integrity often makes extensive use of error detecting algorithms known as error-correcting codes. Human induced data integrity errors are often detected through the use of simpler check digits and algorithms used to detect them such as the Damm algorithm or Luhn algorithm. These are used to maintain data integrity after manual transcription from one computer system to another by a human intermediary. Examples include credit card and bank routing numbers. Computer induced transcription errors can be detected through hash functions.
In production systems these techniques are used in combination to ensure various degrees of data integrity. For example a computer file system may configured on a fault tolerant RAID array, but might not provide block level checksums to detect and prevent silent data corruption. A database management system might be ACID compliant, but the raid controller or hard drive's internal write-cache might not be.
Logical integrity - concerned with the correctness or rationality of a piece of data, given a particular context. This includes topics such as referential integrity and entity integrity in a relational database or correctly ignoring impossible sensor data in robotic systems. These concerns involve making certain the data "makes sense" given its environment. Challenges include software bugs, design flaws, human error. Common methods of ensuring logical integrity include things such as a Check constraint, foreign key constraint, program assertion (computing) and other runtime sanity checks.
Both physical and logical integrity often share many common challenges such as human error, design flaws and both must appropriately deal with concurrent requests to record and retrieve data, the later of which is its own subject entirely. See mutex and Copy-on-write.
Databases.
Data integrity contains guidelines for data retention, specifying or guaranteeing the length of time data can be retained in a particular database. It specifies what can be done with data values when their validity or usefulness expires. In order to achieve data integrity, these rules are consistently and routinely applied to all data entering the system, and any relaxation of enforcement could cause errors in the data. Implementing checks on the data as close as possible to the source of input (such as human data entry), causes less erroneous data to enter the system. Strict enforcement of data integrity rules causes the error rates to be lower, resulting in time saved troubleshooting and tracing erroneous data and the errors it causes algorithms.
Data integrity also includes rules defining the relations a piece of data can have, to other pieces of data, such as a "Customer" record being allowed to link to purchased "Products", but not to unrelated data such as "Corporate Assets". Data integrity often includes checks and correction for invalid data, based on a fixed schema or a predefined set of rules. An example being textual data entered where a date-time value is required. Rules for data derivation are also applicable, specifying how a data value is derived based on algorithm, contributors and conditions. It also specifies the conditions on how the data value could be re-derived.
Types of integrity constraints.
Data integrity is normally enforced in a database system by a series of integrity constraints or rules. Three types of integrity constraints are an inherent part of the relational data model: entity integrity, referential integrity and domain integrity:
If a database supports these features it is the responsibility of the database to insure data integrity as well as the consistency model for the data storage and retrieval. If a database does not support these features it is the responsibility of the applications to ensure data integrity while the database supports the consistency model for the data storage and retrieval.
Having a single, well-controlled, and well-defined data-integrity system increases
s of 2012[ [update]], since all modern databases support these features (see Comparison of relational database management systems), it has become the de facto responsibility of the database to ensure data integrity. Out-dated and legacy systems that use file systems (text, spreadsheets, ISAM, flat files, etc.) for their consistency model lack any kind of data-integrity model. This requires organizations to invest a large amount of time, money and personnel in building data-integrity systems on a per-application basis that needlessly duplicate the existing data integrity systems found in modern databases. Many companies, and indeed many database systems themselves, offer products and services to migrate out-dated and legacy systems to modern databases to provide these data-integrity features. This offers organizations substantial savings in time, money and resources because they do not have to develop per-application data-integrity systems that must be refactored each time the business requirements change.
Examples.
An example of a data-integrity mechanism is the parent-and-child relationship of related records. If a parent record owns one or more related child records all of the referential integrity processes are handled by the database itself, which automatically ensures the accuracy and integrity of the data so that no child record can exist without a parent (also called being orphaned) and that no parent loses their child records. It also ensures that no parent record can be deleted while the parent record owns any child records. All of this is handled at the database level and does not require coding integrity checks into each applications.
File systems.
Various research results show that neither widespread filesystems (including UFS, Ext, XFS, JFS and NTFS) nor hardware RAID solutions provide sufficient protection against data integrity problems.
Some filesystems (including Btrfs and ZFS) provide internal data and metadata checksumming, what is used for detecting silent data corruption and improving data integrity. If a corruption is detected that way and internal RAID mechanisms provided by those filesystems are also used, such filesystems can additionally reconstruct corrupted data in a transparent way. This approach allows improved data integrity protection covering the entire data paths, which is usually known as end-to-end data protection.
Data storage.
Apart from data in databases, standards exist to address the integrity of data on storage devices.

</doc>
<doc id="40996" url="http://en.wikipedia.org/wiki?curid=40996" title="Data link">
Data link

In telecommunication a data link is the means of connecting one location to another for the purpose of transmitting and receiving digital information. It can also refer to a set of electronics assemblies, consisting of a transmitter and a receiver (two pieces of data terminal equipment) and the interconnecting data telecommunication circuit. These are governed by a link protocol enabling digital data to be transferred from a data source to a data sink.
There are at least three types of basic data-link configurations that can be conceived of and used:
In civil aviation, a data-link system (known as Controller Pilot Data Link Communications) is used to send information between aircraft and air traffic controllers when an aircraft is too far from the ATC to make voice radio communication and radar observations possible. Such systems are used for aircraft crossing the Atlantic and Pacific oceans. One such system, used by NavCanada and NATS over the North Atlantic, uses a five-digit data link sequence number confirmed between air traffic control and the pilots of the aircraft before the aircraft proceeds to cross the ocean. This system uses the aircraft's flight management computer to send location, speed and altitude information about the aircraft to the ATC. ATC can then send messages to the aircraft regarding any necessary change of course. 
In military aviation, a data-link may also carry weapons targeting information or information to help warplanes land on aircraft carriers.
In unmanned aircraft, land vehicles, boats, and spacecraft, a two-way (full-duplex or half-duplex) data-link is used to send control signals, and to receive telemetry.

</doc>
<doc id="40997" url="http://en.wikipedia.org/wiki?curid=40997" title="Data service unit">
Data service unit

A data service unit, sometimes called a digital service unit, is a piece of telecommunications circuit terminating equipment that transforms digital data between telephone company lines and local equipment. The device converts bipolar digital signals coming ultimately from a digital circuit and directly from a Channel service unit (CSU), into a format (e.g. RS- 530) compatible with the piece of data terminal equipment (DTE) (e.g. a router) to which the data is sent. The DSU also performs a similar process in reverse for data heading from the DTE toward the circuit. The telecommunications service a DSU supports can be a point-to-point or multipoint operation in a digital data network.
A DSU is a two or more port device; one port is called the WAN (Wide Area Network) port and the other is called a DTE port. The purpose of the DSU is to transfer serial data synchronously between the WAN port and the DTE ports. If more than one DTE port is used, the DSU assigns the DTE data according to time slots (channels) on the WAN side.
On the WAN side, the DSU, via a CSU, interfaces with a digital carrier such as DS1 or DS3 or a low speed Digital Data Service. On the DTE side, the DSU provides control lines, timing lines and appropriate physical and electrical interface. To maintain the synchronous relationship between the ports, the DSU manages timing by slaving ports to the bit rate of another or to its internal clock. Typically, the DTE port provides timing to the data terminal equipment while the WAN port dictates the rate.
DSUs usually include some maintenance capabilities. At minimum, they can loop data back at either the WAN or DTE ports, or at both. When only one port is looped back, the data received at that port is simultaneously sent back toward the port and passed in normal fashion to the other port. Most DSUs also allow various data patterns to be generated and monitored to measure error rate of the communication link. A DSU may be a separate piece of equipment, or may be combined in a CSU/DSU.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40998" url="http://en.wikipedia.org/wiki?curid=40998" title="Data signaling rate">
Data signaling rate

In telecommunication, data signaling rate (DSR), also known as gross bit rate, is the aggregate rate at which data pass a point in the transmission path of a data transmission system.
Notes:
Maximum rate.
The "maximum user signaling rate", synonymous to gross bitrate or data signaling rate, is the maximum rate, in bits per second, at which binary information can be transferred in a given direction between users over the telecommunications system facilities dedicated to a particular information transfer transaction, under conditions of continuous transmission and no overhead information. 
For a single channel, the signaling rate is given by , where "SCSR" is the single-channel signaling rate in bits per second, "T" is the minimum time interval in seconds for which each level must be maintained, and n is the number of significant conditions of modulation of the channel. 
In the case where an individual end-to-end telecommunications service is provided by parallel channels, the parallel-channel signaling rate is given by , where "PCSR" is the total signaling rate for "m" channels, "m" is the number of parallel channels, "Ti" is the minimum interval between significant instants for the "I"-th channel, and "ni" is the number of significant conditions of modulation for the "I"-th channel. 
In the case where an end-to-end telecommunications service is provided by tandem channels, the end-to-end signaling rate is the lowest signaling rate among the component channels.
Transmission Data Rate Terminology.
"Based upon" proposal from davisnetworks.com. 1 Mbit/s is defined as 1,000,000 bits per second signal data rate (OSI Layer 1).
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40999" url="http://en.wikipedia.org/wiki?curid=40999" title="Data transmission circuit">
Data transmission circuit

In telecommunication, a data transmission circuit is the transmission media and the intervening equipment used for the data transfer between data terminal equipments (DTEs). 
"Note 1:" A data transmission circuit includes any required signal conversion equipment. 
"Note 2:" A data transmission circuit may transfer information in (a) one direction only, (b) either direction but one way at a time, or (c) both directions simultaneously. See duplex (telecommunications).
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41000" url="http://en.wikipedia.org/wiki?curid=41000" title="Date-time group">
Date-time group

In communications messages, a date-time group (DTG) is a set of characters, usually in a prescribed format, used to express the year, the month, the day of the month, the hour of the day, the minute of the hour, and the time zone, if different from Coordinated Universal Time (UTC). The order in which these elements are presented may vary. The DTG is usually placed in the header of the message. One example is " (UTC)".
The DTG may indicate either the date and time a message was dispatched by a transmitting station or the date and time it was handed into a transmission facility by a user or originator for dispatch.
The DTG may be used as a message identifier if it is unique for each message. 
The DTG is used in message traffic. EXAMPLE: Ø9163ØZ JUL 11 represents 163Ø GMT on 9 July 2Ø11.
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41001" url="http://en.wikipedia.org/wiki?curid=41001" title="DB (car)">
DB (car)

DB (until 1947 known as Deutsch-Bonnet) was a French automobile maker between 1938 and 1961, based in Champigny-sur-Marne near Paris. The firm was founded by Charles Deutsch and René Bonnet. Immediately before the war the partners concentrated on making light-weight racing cars, but a few years after the war, starting with the presentation of a Panhard based cabriolet at the 1950 Paris Motor Show, the company began to produce small road-going sports cars. By 1952 the company no longer had its own stand at the Paris motorshow, but one of their cars appeared as a star attraction on the large Panhard stand, reflecting the level of cooperation between the two businesses.
Light-weight engineering.
The business produced light sports cars, originally in steel but subsequently with fibreglass bodies mainly powered by Panhard flat-twin engines, most commonly of 610, 744, or 851 cc. Deutsch was a "theoretical engineer who had a natural instinct for aerodynamics," while Bonnet was a more "pragmatic mechanical engineer".
The fibreglass bodies covered a tubular central beam chassis made from steel, with front wheel drive and four wheel independent suspension directly lifted from the Panhard donors. Until 1952 all DBs had been intended for competition purposes only.
Racing origins.
Deutsch and Bonnet had been promised a works drive in the 1936 French GP for sports cars, but when this failed to materialise they set about building their own racer. The 1938 DB1 was a special, built using the remains of a Citroën Traction Avant 11CV. A series of numbered successors followed, with a DB4 in 1945 and a 2-litre DB5. Their two specials both placed in the first postwar race in France, in Paris in 1945. An open-wheeled DB7 appeared in 1947, after which the "Automobiles Deutsch & Bonnet" was officially formed. Their early cars were all built using Citroën parts, but supply was troublesome and DB soon moved on to using Panhard technology.
DB was very active in competition especially in Le Mans 24 Hours.
Road cars.
The steel-bodied 1952 "Mille Miles" (celebrating class victories at the Mille Miglia) was a mini-GT with a 65 hp Panhard two-cylinder; it was quickly followed by the Chausson-designed DB Coach in fibreglass. The HBR 4/5 model (1954–1959) was the partners' most successful project to date, with several hundred of the little cars produced between 1954 and 1959. This was followed by the Le Mans convertible and hardtop, which was shown in 1959 and built by DB until 1962, and continued until 1964 by René Bonnet. About 660 of the Mille Miles/Coach/HBR were built, and 232 DB Le Mans (not including the Bonnet-built cars). Later versions could be equipped with engines of 1 and 1.3 litres, and superchargers were also available. No two cars may have been alike, as they were built according to customer specifications from a wide range of options.
More racing success.
Deutsch's very efficient and influential aerodynamic designs allowed DB race cars to reach impressive top speeds despite the small Panhard flat-twin engine. Deutsch and Bonnet disagreed whether they should build cars of front-wheel drive or mid-engined design. DB's received class victories at Le Mans (three times), Sebring (twice), and Mille Miglia (four times). DB even managed an outright win in the handicapped 1954 Tourist Trophy sports car race, with Laureau and Armagnac driving.
Disagreement and the end of the partnership.
Charles Deutsch, wanting to stick to Panhard engines, left DB in 1961 to found his own firm (CD). Bonnet founded "Automobiles René Bonnet", producing cars powered by Renault engines: this business was later to become part of Matra Automobiles. Deutsch ended up an engineering consultant.

</doc>
<doc id="41002" url="http://en.wikipedia.org/wiki?curid=41002" title="Weighting filter">
Weighting filter

A weighting filter is used to emphasize or suppress some aspects of a phenomenon compared to others, for measurement or other purposes.
Audio applications.
In each field of audio measurement, special units are used to indicate a weighted measurement as opposed to a basic physical measurement of energy level. For sound, the unit is the phon (1 kHz equivalent level).
Loudness measurements.
In the measurement of loudness, for example, an A-weighting filter is commonly used to emphasize frequencies around 3–6 kHz where the human ear is most sensitive, while attenuating very high and very low frequencies to which the ear is insensitive. The aim is to ensure that measured loudness corresponds well with subjectively perceived loudness.
A-weighting is only really valid for relatively quiet sounds and for pure tones as it is based on the 40-phon Fletcher–Munson equal-loudness contour. The B and C curves were intended for louder sounds (though they are less used) while the D curve is used in assessing loud aircraft noise (IEC 537).
Telecommunications.
In the field of telecommunications, weighting filters are widely used in the measurement of electrical noise on telephone circuits, and in the assessment of noise as perceived through the acoustic response of different types of instrument (handset). Other noise-weighting curves have existed, e.g. DIN standards. The term "psophometric weighting", though referring in principle to any weighting curve intended for noise measurement, is often used to refer to a particular weighting curve, used in telephony for narrow-bandwidth voiceband speech circuits.
Environmental noise measurement.
A-weighted decibels are abbreviated dB(A) or dBA. When acoustic (calibrated microphone) measurements are being referred to, then the units used will be dB SPL (sound pressure level) referenced to 20 micropascals = 0 dB SPL. dBrn adjusted is a synonym for dBA.
The A-weighting curve has been widely adopted for environmental noise measurement, and is standard in many sound level meters (see ITU-R 468 weighting for a further explanation).
A-weighting is also in common use for assessing potential hearing damage caused by loud noise, though this seems to be based on the widespread availability of sound level meters incorporating A-Weighting rather than on any good experimental evidence to suggest that such use is valid. The distance of the measuring microphone from a sound source is often "forgotten", when SPL measurements are quoted, making the data useless. In the case of environmental or aircraft noise, distance need not be quoted as it is the level at the point of measurement that is needed, but when measuring refrigerators and similar appliances the distance should be stated; where not stated it is usually one metre (1 m). An extra complication here is the effect of a reverberant room, and so noise measurement on appliances should state "at 1 m in an open field" or "at 1 m in anechoic chamber". Measurements made outdoors will approximate well to anechoic conditions.
A-weighted SPL measurements of noise level are increasingly to be found on sales literature for domestic appliances such as refrigerators and freezers, and computer fans. Although the threshold of hearing is typically around 0 dB SPL, this is in fact very quiet indeed, and appliances are more likely to have noise levels of 30 to 40 dB SPL.
Audio reproduction and broadcasting equipment.
Human sensitivity to noise in the region of 6 kHz became particularly apparent in the late 1960s with the introduction of compact cassette recorders and Dolby-B noise reduction. A-weighted noise measurements were found to give misleading results because they did not give sufficient prominence to the 6 kHz region where the noise reduction was having greatest effect, and sometimes one piece of equipment would even measure worse than another and yet sound better, because of differing spectral content.
ITU-R 468 noise weighting was therefore developed to more accurately reflect the subjective loudness of all types of noise, as opposed to tones. This curve, which came out of work done by the BBC Research Department, and was standardised by the CCIR and later adopted by many other standards bodies (IEC, BSI/) and, as of 2006[ [update]], is maintained by the ITU. Noise measurements using this weighting typically also use a quasi-peak detector law rather than slow averaging. This also helps to quantify the audibility of bursty noise, ticks and pops that might go undetected with a slow rms measurement.
ITU-R 468 noise weighting with quasi-peak detection is widely used in Europe, especially in telecommunications, and in broadcasting particularly after it was adopted by the Dolby corporation who realised its superior validity for their purposes. Its advantages over A-weighting seem to be less well appreciated in the USA and in consumer electronics, where the use of A-weighting predominates—probably because A-weighting produces a 9 to 12 dB "better" specification, see specsmanship. It is commonly used by broadcasters in Britain, Europe, and former countries of the British Empire such as Australia and South Africa.
Though the noise level of 16-bit audio systems (such as CD players) is commonly quoted (on the basis of calculations that take no account of subjective effect) as −96 dB relative to FS (full scale), the best 468-weighted results are in the region of −68 dB relative to Alignment Level (commonly defined as 18 dB below FS) i.e. −86 dB relative to FS.
The use of weighting curves is in no way to be regarded as 'cheating', provided that the proper curve is used. Nothing of relevance is being 'hidden', and even when, for example, hum is present at 50 or 100 Hz at a level above the quoted (weighted) noise floor this is of no importance because our ears are very insensitive to low frequencies at low levels, so it will not be heard. A-weighting is often used to compare and qualify ADCs, for instance, because it more accurately represents the way noise shaping hides dither noise in the ultrasonic range.
Other applications of weighting.
In the measurement of gamma rays or other ionising radiation, a radiation monitor or dosimeter will commonly use a filter to attenuate those energy levels or wavelengths that cause the least damage to the human body, while letting through those that do the most damage, so that any source of radiation may be measured in terms of its true danger rather than just its 'strength'. The sievert is a unit of weighted radiation dose for ionising radiation, which supersedes the older unit the REM (roentgen equivalent man).
Weighting is also applied to the measurement of sunlight when assessing the risk of skin damage through sunburn, since different wavelengths have different biological effects. Common examples are the SPF of sunscreen, and the UV index.
Another use of weighting is in television, where the red, green and blue components of the signal are weighted according to their perceived brightness. This ensures compatibility with black and white receivers, and also benefits noise performance and allows separation into meaningful luminance and chrominance signals for transmission.

</doc>
<doc id="41003" url="http://en.wikipedia.org/wiki?curid=41003" title="DBm">
DBm

dBm (sometimes dBmW or Decibel-milliwatts) is an abbreviation for the power ratio in decibels (dB) of the measured power referenced to one milliwatt (mW). It is used in radio, microwave and fiber optic networks as a convenient measure of absolute power because of its capability to express both very large and very small values in a short form. Compare dBW, which is referenced to one watt (1000 mW).
Since it is referenced to the watt, it is an absolute unit, used when measuring absolute power. By comparison, the decibel (dB) is a dimensionless unit, used for quantifying the ratio between two values, such as signal-to-noise ratio.
In audio and telephony, dBm is typically referenced relative to a 600 ohm impedance, while in radio frequency work dBm is typically referenced relative to a 50 ohm impedance.
Unit conversions.
A power level of 0 dBm corresponds to a power of 1 milliwatt. A 3 dB increase in level is approximately equivalent to doubling the power, which means that a level of 3 dBm corresponds roughly to a power of 2 mW. For each 3 dB decrease in level, the power is reduced by about one half, making −3 dBm correspond to a power of about 0.5 mW.
To express an arbitrary power "P" in mW as "x" in dBm, or vice versa, the following equivalent expressions may be used:
idem with "P" in watts
where "P" is the power in W and "x" is the power level in dBm. Below is a table summarizing useful cases:
The signal intensity (power per unit area) can be converted to received signal power by multiplying by the square of the wavelength and dividing by 4π (see Free-space path loss).
In United States Department of Defense practice, unweighted measurement is normally understood, applicable to a certain bandwidth, which must be stated or implied.
In European practice, psophometric weighting may be, as indicated by context, equivalent to dBm0p, which is preferred.
In audio, 0 dBm often corresponds to approximately 0.775 volts, since 0.775 volts dissipates 1 mW in a 600 Ω load. dBu measures against this reference voltage without the 600 Ω restriction. Conversely, for RF situations with a 50 Ω load, 0 dBm corresponds to approximately 0.224 volts since 0.224 volts dissipates 1 mW in a 50 Ω load. 
The dBm is not a part of the International System of Units and therefore is discouraged from use in documents or systems that adhere to SI units (the corresponding SI unit is the watt). However the straight decibel (dB), being a unitless ratio of two numbers, is perfectly acceptable.
Expression in dBm is typically used for optical and electrical power measurements, not for other types of power (such as thermal). A listing by power levels in watts is available that includes a variety of examples not necessarily related to electrical or optical power.
The dBm was first proposed as an industry standard in the paper "A New Standard Volume Indicator and Reference Level".
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41004" url="http://en.wikipedia.org/wiki?curid=41004" title="DBrn">
DBrn

The symbol dBrn or dB(rn) is an abbreviation for decibels above reference noise. 
Weighted noise power in dB is referred to 1.0 picowatt. Thus, 0 dBrn = -90 dBm. Use of 144 line, 144-receiver, or C-message weighting, or flat weighting, can be indicated in parentheses. 
With C-message weighting, a one-milliwatt, 1000 Hz tone will read +90 dBrn, but the same power as white noise, randomly distributed over a 3 kHz band will read approximately +88.5 dBrn, because of the frequency weighting. 
With 144 weightings, a one milliwatt, 1000 Hz white noise tone will also read +90 dBrn, but the same 3 kHz power will only read +82 dBrn, because of the different frequency weighting.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41005" url="http://en.wikipedia.org/wiki?curid=41005" title="Data circuit-terminating equipment">
Data circuit-terminating equipment

A data circuit-terminating equipment (DCE) is a device that sits between the data terminal equipment (DTE) and a data transmission circuit. It is also called data communication(s) equipment and data carrier equipment. Usually, the DTE device is the terminal (or computer), and the DCE is a modem.
In a data station, the DCE performs functions such as signal conversion, coding, and line clocking and may be a part of the DTE or intermediate equipment. Interfacing equipment may be required to couple the data terminal equipment (DTE) into a transmission circuit or channel and from a transmission circuit or channel into the DTE.
Although the terms are most commonly used with RS-232, several data communications standards define different types of interfaces between a DCE and a DTE. The DCE is a device that communicates with a DTE device in these standards. Standards that use this nomenclature include:
A general rule is that DCE devices provide the clock signal (internal clocking) and the DTE device synchronizes on the provided clock (external clocking). D-sub connectors follow another rule for pin assignment. DTE devices usually transmit on pin connector number 2 and receive on pin connector number 3. DCE devices are just the opposite: pin connector number 2 receives and pin connector number 3 transmits the signals.
When two devices, that are both DTE or both DCE, must be connected together without a modem or a similar media translator between them, a crossover cable must be used, e.g. a null modem for RS-232 or an Ethernet crossover cable.

</doc>
<doc id="41008" url="http://en.wikipedia.org/wiki?curid=41008" title="Degradation">
Degradation

Degradation may refer to:

</doc>
<doc id="41009" url="http://en.wikipedia.org/wiki?curid=41009" title="Degree of isochronous distortion">
Degree of isochronous distortion

The degree of isochronous distortion, in data transmission, is the ratio of the absolute value of the maximum measured difference between the actual and the theoretical intervals separating any two significant instants of modulation (or demodulation), to the unit interval. These instants are not necessarily consecutive. This value is usually expressed as a percentage.
The result of the measurement should be qualified by an indication if the period, usually limited, of the observation. For a prolonged modulation (or demodulation), it will be appropriate to consider the probability that an assigned value of the degree of distortion will be exceeded.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41010" url="http://en.wikipedia.org/wiki?curid=41010" title="Degree of start-stop distortion">
Degree of start-stop distortion

In telecommunication, the term degree of start-stop distortion has the following meanings: 
The degree of distortion of a start-stop modulation (or demodulation) is usually expressed as a percentage. Distinction can be made between the degree of late (positive) distortion and the degree of early (negative) distortion.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41012" url="http://en.wikipedia.org/wiki?curid=41012" title="Delay">
Delay

Delay may refer to:

</doc>
<doc id="41013" url="http://en.wikipedia.org/wiki?curid=41013" title="Delay encoding">
Delay encoding

In telecommunications, delay encoding is the encoding of binary data to form a two-level signal where (a) a "0" causes no change of signal level unless it is followed by another "0" in which case a transition to the other level takes place at the end of the first bit period; and (b) a "1" causes a transition from one level to the other in the middle of the bit period. 
Delay encoding is used primarily for encoding radio signals because the frequency spectrum of the encoded signal contains less low-frequency energy than a conventional non-return-to-zero (NRZ) signal and less high-frequency energy than a biphase signal.
Delay encoding is an encoding using only half the bandwidth for biphase encoding but features all the advantages of biphase encoding:
"To be rewritten: It is guaranteed to have transitions every other bit, meaning that decoding systems can adjust their clock/DC threshold continuously".
One drawback is human readability (e.g. on an oscilloscope).
Delay encoding is also known as Miller encoding (named after Armin Miller, its inventor). 
Some RFID cards, in particular EPC UHF Gen 2 RF cards, use a variant called "Miller sub-carrier coding".
In this system, 2, 4 or 8 cycles of a subcarrier square wave are transmitted for each bit time. The Miller encoding transitions are indicated by 180° phase shifts in the subcarrier, i.e. the subcarrier pauses for 1/2 of a cycle at each transition. (The resultant binary subcarrier is itself either ASK or PSK modulated on another carrier.)
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41014" url="http://en.wikipedia.org/wiki?curid=41014" title="Delay line">
Delay line

Delay line may refer to:

</doc>
<doc id="41015" url="http://en.wikipedia.org/wiki?curid=41015" title="Delta modulation">
Delta modulation

A Delta modulation (DM or Δ-modulation) is an analog-to-digital and digital-to-analog signal conversion technique used for transmission of voice information where quality is not of primary importance. DM is the simplest form of differential pulse-code modulation (DPCM) where the difference between successive samples are encoded into n-bit data streams. In delta modulation, the transmitted data are reduced to a 1-bit data stream.
Its main features are:
To achieve high signal-to-noise ratio, delta modulation must use oversampling techniques, that is, the analog signal is sampled at a rate several times higher than the Nyquist rate.
Derived forms of delta modulation are continuously variable slope delta modulation, delta-sigma modulation, and differential modulation. Differential pulse-code modulation is the super-set of DM.
Principle.
Rather than quantizing the absolute value of the input analog waveform, delta modulation quantizes the difference between the current and the previous step, as shown in the block diagram in Fig. 1.
The modulator is made by a quantizer which converts the difference between the input signal and the average of the previous steps. In its simplest form, the quantizer can be realized with a comparator referenced to 0 (two levels quantizer), whose output is "1" or "0" if the input signal is positive or negative. It is also a bit-quantizer as it quantizes only a bit at a time. The demodulator is simply an integrator (like the one in the feedback loop) whose output rises or falls with each 1 or 0 received. The integrator itself constitutes a low-pass filter.
Transfer characteristics.
The transfer characteristics of a delta modulated system follows a signum function,as it quantizes only two levels and also one-bit at a time.
The two sources of noise in delta modulation are "slope overload", when steps are too small to track the original waveform, and "granularity", when steps are too large.
But a 1971 study shows that slope overload is less objectionable compared to granularity than one might expect based solely on SNR measures.
Output signal power.
In delta modulation there is a restriction on the amplitude of the input signal, because if the transmitted signal has a large derivative (Abrupt changes) then modulated signal can not follow the input signal and Slope Overload occurs. Lets say if the input signal is
formula_1
Modulated Signal (Derivative of Input Signal) which is transmitted by the Modulator
formula_2
Whereas the condition to avoid Slope Overload is
formula_3
So Maximum Amplitude of Input signal can be
formula_4
Where fs is Sampling Frequency and ω is the Frequency of the input Signal and σ is Step Size in Quantization.
So Amax is the Maximum Amplitude that DM can transmit without causing the Slope Overload and the Power of Transmitted Signal depends on the Maximum Amplitude.
Bit-rate.
If the communication channel is of limited bandwidth,there is the possibility of interference in either DM or PCM. Hence, 'DM' and 'PCM' operate at same bit-rate which is equal to N times the sampling frequency.
Adaptive delta modulation.
Adaptive delta modulation (ADM) was first published by Dr. John E. Abate (AT&T Bell Laboratories Fellow) in his doctorial thesis at NJ Institute Of Technology in 1967. ADM was later selected as the standard for all NASA communications between mission control and space-craft.
Adaptive delta modulation or [continuously variable slope delta modulation](CVSD) is a modification of DM in which the step size is not fixed.
Rather, when several consecutive bits have the same direction value, the encoder and decoder assume that slope overload is occurring, and the step size becomes progressively larger.
Otherwise, the step size becomes gradually smaller over time.
ADM reduces slope error,at the expense of increasing quantizing error.This error can be reduced by using a low pass filter.
ADM provides robust performance in the presence of bit errors meaning error detection and correction are not typically used in an ADM radio design, this allows for a reduction in host processor workload (allowing a low-cost processor to be used).
Applications.
Contemporary applications of Delta Modulation includes, but is not limited to, recreating legacy synthesizer waveforms. With the increasing availability of FPGAs and game-related ASICs, sample rates are easily controlled so as to avoid slope overload and granularity issues. For example, the C64DTV used a 32 MHz sample rate, providing ample dynamic range to recreate the SID output to acceptable levels. 
SBS Application 24Kbps Delta Modulation.
Delta Modulation was used by Satellite Business Systems or SBS for its voice ports to provide long distance phone service to large domestic corporations with a significant inter-corporation communications need (such as IBM). This system was in service throughout the 1980s. The voice ports used digitally implemented 24kbit/s Delta Modulation with Voice Activity Compression or VAC and Echo Suppressors to control the half second echo path through the satellite. They performed formal listening tests to verify the 24kbit/s Delta Modulator achieved full voice quality with no discernible degradation as compared to a high quality phone line or the standard 64kbit/s µ-law Companded PCM. This provided an eight to three improvement in satellite channel capacity. IBM developed the Satellite Communications Controller and the voice port functions.
The original proposal in 1974 used a state-of-the-art 24kbit/s Delta Modulator with a single integrator and a Shindler Compander modified for gain error recovery. This proved to have less than full phone line speech quality. In 1977one engineer with two assistants in the IBM Research Triangle Park, NC laboratory was assigned to improve the quality.
The final implementation replaced the integrator with a Predictor implemented with a two pole complex pair low pass filter designed to approximate the long term average speech spectrum. The theory was that ideally the integrator should be a predictor designed to match the signal spectrum. A nearly perfect Shindler Compander replaced the modified version. It was found the modified compander resulted in a less than perfect step size at most signal levels and the fast gain error recovery increased the noise as determined by actual listening tests as compared to simple signal to noise measurements. The final compander achieved a very mild gain error recovery due to the natural truncation rounding error caused by twelve bit arithmetic.
The complete function of Delta Modulation, VAC and Echo Control for six ports was implemented in a single digital integrated circuit chip with twelve bit arithmetic. A single DAC was shared by all six ports providing voltage compare functions for the modulators and feeding sample and hold circuits for the demodulator outputs. A single card held the chip, DAC and all the analog circuits for the phone line interface including transformers.

</doc>
<doc id="41016" url="http://en.wikipedia.org/wiki?curid=41016" title="Demand assignment">
Demand assignment

In telecommunication, a demand assignment is a method which several users share access to a communications channel on a real-time basis, "i.e.", a user needing to communicate with another user on the same network requests the required circuit, uses it, and when the call is finished, the circuit is released, making the circuit available to other users. 
Demand assignment is similar to conventional telephone switching, in which common trunks are provided for many users, on a demand basis, through a limited-size trunk group.

</doc>
<doc id="41017" url="http://en.wikipedia.org/wiki?curid=41017" title="Demand factor">
Demand factor

In telecommunication, electronics and the electrical power industry, the term demand factor is used to refer to the fractional amount of some quantity being used relative to the maximum amount that could be used by the same system. The demand factor is always less than or equal to one. As the amount of demand is a time dependant quantity so is the demand factor.
formula_1
The demand factor is often implicitly averaged over time when the time period of demand is understood by the context.
Electrical engineering.
In electrical engineering the demand factor is taken as a time independent quantity where the numerator is taken as the maximum demand in the specified time period instead of the averaged or instantaneous demand. the possible 
formula_2
This is the peak in the load profile divided by the full load of the device.
Example: 
If a residence has equipment which would draw 6,000 W when all equipment was drawing a full load draw a maximum of 3,000 W in a specified time, then the demand factor = 3,000 W / 6,000 W = 0.5
This quantity is relevant when trying to establish the amount of load a system should be rated for. In the above example it would be unlikely that the system would be rated to 6,000 W even though there may be a slight possibility that this amount of power can be drawn. This is closely related to the load factor which is the average load divided by the peak load in a specified time period.
formula_3

</doc>
<doc id="41018" url="http://en.wikipedia.org/wiki?curid=41018" title="Demand load">
Demand load

In telecommunication, the term demand load can have the following meanings: 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41019" url="http://en.wikipedia.org/wiki?curid=41019" title="Desensitation">
Desensitation

In telecommunication, desensitation is the reduction of desired signal gain as a result of receiver reaction to an undesired signal. 
The gain reduction is generally due to overload of some portion of the receiver (e.g., the automatic gain control circuitry) resulting in suppression of the desired signal because the receiver will no longer respond linearly to incremental changes in input voltage.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41020" url="http://en.wikipedia.org/wiki?curid=41020" title="Design objective">
Design objective

Design objective (DO): In communications systems, a desired performance characteristic for communications circuits and equipment that is based on engineering analyses, but (a) is not considered feasible to mandate in a standard, or (b) has not been tested. 
DOs are used because applicable systems standards are not in existence. 
Examples of reasons for designating a performance characteristic as a DO rather than as a standard are (a) it may be bordering on an advancement in the state of the art, (b) the requirement may not have been fully confirmed by measurement or experience with operating circuits, and (c) it may not have been demonstrated that the requirement can be met considering other constraints, such as cost and size. 
A DO is sometimes established in a standard for developmental consideration. A DO may also specify a performance characteristic that may be used in the preparation of specifications for development or procurement of new equipment or systems.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41021" url="http://en.wikipedia.org/wiki?curid=41021" title="Detector (disambiguation)">
Detector (disambiguation)

A detector is a device capable of registering a specific substance or physical phenomenon.
Detector may also refer to:

</doc>
<doc id="41022" url="http://en.wikipedia.org/wiki?curid=41022" title="Deterministic routing">
Deterministic routing

In telecommunications, deterministic routing is the advance determination of the routes between given pairs of nodes. Examples:
Notes and references.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41023" url="http://en.wikipedia.org/wiki?curid=41023" title="D4 framing standard">
D4 framing standard

In telecommunication, a D-4 is a framing standard for traditional time-division multiplexing, which standard describes user channels multiplexed onto a trunk that has been segmented (framed) into 24 bytes of 8 bits each. 
"Note:" The multiplexing function is performed in the D-4 framing structure by interleaving bits of consecutive bytes as they are presented from individual circuits into each D-4 frame.
Within Telecommunications operations, a D4 Bank also refers to a type of multiplexer which mux's 24 DS0 signals into a single DS1. 
Source: from Federal Standard 1037C
A minor correction from above, there is no actual D4 framing format. The D4 Framing format that is referenced above is actually more of a marketable term, adopted by "techno Jargon" to simplify what is actually called Superframe Framing Format, an embedded 12 bit framing code. The D4 is (as stated above) a 48-channel (dual 24 channel) channel bank chassis, which can multiplex two banks of 24 DS0 channels into 2 T1's or a single T1C.
The Superfame Format was originally introduced with the D2 Channel Bank. The D2 Channel bank was a 96 channel based chassis which could provide four 24 channel T1s, two T1C's or a DS2. The T1 Extended Superframe (ESF) Format (a 24 bit code composed of 6 framing bits, 6 CRC Error Checking Bits and 12 Signaling Bits)was introduced with the D5 Channel Bank, a true 24-channel channel bank chassis, which could also be software optioned to use the Superframe framing format.
So, why is the T1 Superframe framing format commonly called D4 Framing? There are may iteration of the answer but the most common by me over the years was that the abbreviations for Superframe (SF) was in conflict with the still commonly used technology of the day, Single Frequency Signaling (SF Signaling). SF Signaling was the method of Inband circuit control before the advent of the Extended Superframe (ESF) framing format. The ESF T1 framing format allowed for the signaling information to be carried out of band and not as an Inband tone. Consequently, the D4 channel bank was in deployment and so the Superframe format became associated with the D4 Channel Bank, or D4 Framing.
So, is it wrong to say D4 Framing, not really, but understanding what it means is important.
This information can be found in the original AT&T Publications.

</doc>
<doc id="41024" url="http://en.wikipedia.org/wiki?curid=41024" title="Pulse dialing">
Pulse dialing

Pulse dialing is a signaling technology in telecommunications in which a direct current local loop circuit is interrupted according to a defined coding system for each signal transmitted, usually a digit. This lends the method the often used name loop disconnect dialing. In the most common variant of pulse dialing, decadic dialing, each of the ten arabic numerals are encoded in a sequence of up to ten pulses. The most common version decodes the digits 1 through 9, as one to nine pulses, respectively, and the digit 0 as ten pulses. Historically, the most common device to produce such pulse trains is the rotary dial of the telephone, lending the technology another name, rotary dialing.
The pulse repetition rate was historically determined based on the response time needed for electromechanical switching systems to operate reliably. Most telephone systems used the nominal rate of ten pulses per second, but operator dialing within and between central offices often used pulse rates up to twenty per second.
Early automatic exchanges.
Automatic telephone exchange systems were developed in the late 19th and early 20th century. For identification, telephone subscribers were assigned a telephone number unique to each circuit. Various methods evolved to signal the desired destination telephone number for a telephone call directly dialed by the subscriber.
The first commercial automatic telephone exchange, designed by Almon Brown Strowger, opened in La Porte, Indiana on 3 November 1892, and used two telegraph-type keys on the telephone, which had to be operated the correct number of times to control the vertical and horizontal relay magnets in the exchange. But the use of separate keys with separate conductors to the exchange was not practical. The most common signaling system became a system of using direct-current pulse trains generated in the telephone sets of subscribers by interrupting the single-pair wire loop of the telephone circuit.
Rotary dial.
Strowger also filed the first patent for a rotary dial in 1891. The first dials worked by direct, forward action. The pulses were sent as the user rotated the dial to the finger stop starting at a different position for each digit transmitted. Operating the dial error-free required smooth rotary motion of the finger wheel by the user, but was found as too unreliable. This mechanism was soon refined to include a recoil spring and a centrifugal governor to control the recoil speed. The user selected a digit to be dialed by inserting a finger into the corresponding hole and rotated the dial to the finger stop. When released from this position, the dial pulsing contacts were opened and closed repeatedly, thus interrupting the loop current in a pattern on the return to the home position. The exchange switch decoded the pattern for each digit thus transmitted by stepping relays or by accumulation in digit registers.
Pulse rate and coding.
When electromechanical switching system were still in use, the current pulses generated by the rotary dial on the local loop operated electrical relays in the switches at the central office. The mechanical nature of these relays and the loop capacitance, affecting pulse shape, generally limited the speed of operation, the pulsing rate, to ten pulses per second.
The specifications of the Bell System in the US required service personnel to adjust dials in customer stations to a precision of 9.5 to 10.5 pulses per second (pps), but the tolerance of the switching equipment was generally between 8 and 11 pps. The British (BPO, later Post Office Telecommunications) standard for Strowger exchanges was 10 impulses per second (allowable range 7 to 12) and a 66% break ratio (allowable range 63% to 72%)
In most countries one pulse is used for the digit 1, two pulses for 2, and so on, with ten pulses for the digit 0; this makes the code unary, excepting the digit 0. Exceptions to this are: Sweden (), with one click for 0, two clicks for 1, and so on; and New Zealand with ten clicks for 0, nine clicks for 1, etc. Oslo, the capital city of Norway, used the New Zealand system, but the rest of the country did not. Systems that used this encoding of the 10 digits in a sequence of up to 10 pulses, are sometimes known as decadic dialing systems.
Some later switching systems used digit registers which doubled the allowable pulse rate to 20 pulses per second, and the inter-digital pause could be reduced as the switch selection did not have to be completed during the pause. These included some Crossbar systems, the later version (7A2) of the Rotary system, and the earlier 1970s stored program control exchanges.
In some telephones, the pulses may be heard in the receiver as clicking sounds. However, in general, such effects were undesirable and telephone designers suppressed them by mechanical means with off-normal switches on the dial, or greatly attenuated them by electrical means with a varistor connected across the receiver.
Switch-hook dialing.
As pulse dialing is achieved by interruption of the local loop, it was in principle possible to dial a telephone number by rapidly tapping, i.e. depressing, the switch hook the corresponding number of times for each digit at approximately ten taps per second. However, many telephone makers implemented a slow switch hook release to prevent rapid switching.
In the United Kingdom, it used to be possible to make calls from coin-box phones (payphones) by tapping the switch hook without depositing coins. A person caught tapping could be charged with 'abstracting electricity' from the General Post Office and several cases were prosecuted under this offense.
In popular culture, tapping was used in the film "Red Dragon" as a way for prisoner Hannibal Lecter to dial out on a phone with no dialing mechanism. This method was also used by the character 'Phantom Phreak' to call 'Acid Burn' when taken to prison in the film "Hackers".
Successor technologies.
It was recognized as early as the 1940s that faster dialing and more accurate dialing could be achieved with push-button systems, but the technology was too unreliable in customer trials until transistor technology transformed the industry. In 1963, the Bell System officially introduced dual-tone multi-frequency (DTMF) technology under the name Touch-Tone to the general public. Touch-Tone was a trademark in the U.S. until 1984. The Touch-Tone system used push-button telephones. In the decades following, pulse dialing was gradually phased out as the primary dialing method to the central office, but many systems still support rotary telephones today. Some models of keypad telephones have a switch for the selection of tone or pulse dialing.
Mobile telephones as well as most voice-over-IP systems use out-of-band signaling and do not send any digits until the entire number has been keyed by the user. Many VoIP systems are based on the Session Initiation Protocol (SIP) which uses a form of Uniform Resource Identifiers (URI) for addressing, instead of digits alone.

</doc>
<doc id="41026" url="http://en.wikipedia.org/wiki?curid=41026" title="Dielectric">
Dielectric

A dielectric material (dielectric for short) is an electrical insulator that can be polarized by an applied electric field. When a dielectric is placed in an electric field, electric charges do not flow through the material as they do in a conductor, but only slightly shift from their average equilibrium positions causing dielectric polarization. Because of dielectric polarization, positive charges are displaced toward the field and negative charges shift in the opposite direction. This creates an internal electric field that reduces the overall field within the dielectric itself. If a dielectric is composed of weakly bonded molecules, those molecules not only become polarized, but also reorient so that their symmetry axes align to the field.
The study of dielectric properties concerns storage and dissipation of electric and magnetic energy in materials. Dielectrics are important for explaining various phenomena in electronics, optics, and solid-state physics.
Terminology.
While the term "insulator" implies low electrical conduction, "dielectric" typically means materials with a high polarizability. The latter is expressed by a number called the relative permittivity (also known in older texts as dielectric constant). The term insulator is generally used to indicate electrical obstruction while the term dielectric is used to indicate the energy storing capacity of the material (by means of polarization). A common example of a dielectric is the electrically insulating material between the metallic plates of a capacitor. The polarization of the dielectric by the applied electric field increases the capacitor's surface charge for the given electric field strength.
The term "dielectric" was coined by William Whewell (from "dia-electric") in response to a request from Michael Faraday. A "perfect dielectric" is a material with zero electrical conductivity (cf. perfect conductor), thus exhibiting only a displacement current; therefore it stores and returns electrical energy as if it were an ideal capacitor.
Electric susceptibility.
The electric susceptibility χe of a dielectric material is a measure of how easily it polarizes in response to an electric field. This, in turn, determines the electric permittivity of the material and thus influences many other phenomena in that medium, from the capacitance of capacitors to the speed of light.
It is defined as the constant of proportionality (which may be a tensor) relating an electric field E to the induced dielectric polarization density P such that
where formula_2 is the electric permittivity of free space.
The susceptibility of a medium is related to its relative permittivity formula_3 by
So in the case of a vacuum,
The electric displacement D is related to the polarization density P by
Dispersion and causality.
In general, a material cannot polarize instantaneously in response to an applied field. The more general formulation as a function of time is
That is, the polarization is a convolution of the electric field at previous times with time-dependent susceptibility given by formula_8. The upper limit of this integral can be extended to infinity as well if one defines formula_9 for formula_10. An instantaneous response corresponds to Dirac delta function susceptibility formula_11.
It is more convenient in a linear system to take the Fourier transform and write this relationship as a function of frequency. Due to the convolution theorem, the integral becomes a simple product,
Note the simple frequency dependence of the susceptibility, or equivalently the permittivity. The shape of the susceptibility with respect to frequency characterizes the dispersion properties of the material.
Moreover, the fact that the polarization can only depend on the electric field at previous times (i.e., formula_9 for formula_10), a consequence of causality, imposes Kramers–Kronig constraints on the real and imaginary parts of the susceptibility formula_15.
Dielectric polarization.
Basic atomic model.
In the classical approach to the dielectric model, a material is made up of atoms. Each atom consists of a cloud of negative charge (electrons) bound to and surrounding a positive point charge at its center. In the presence of an electric field the charge cloud is distorted, as shown in the top right of the figure.
This can be reduced to a simple dipole using the superposition principle. A dipole is characterized by its dipole moment, a vector quantity shown in the figure as the blue arrow labeled "M". It is the relationship between the electric field and the dipole moment that gives rise to the behavior of the dielectric. (Note that the dipole moment points in the same direction as the electric field in the figure. This isn't always the case, and is a major simplification, but is true for many materials.)
When the electric field is removed the atom returns to its original state. The time required to do so is the so-called relaxation time; an exponential decay.
This is the essence of the model in physics. The behavior of the dielectric now depends on the situation. The more complicated the situation, the richer the model must be to accurately describe the behavior. Important questions are:
The relationship between the electric field E and the dipole moment M gives rise to the behavior of the dielectric, which, for a given material, can be characterized by the function F defined by the equation:
When both the type of electric field and the type of material have been defined, one then chooses the simplest function "F" that correctly predicts the phenomena of interest. Examples of phenomena that can be so modeled include:
Dipolar polarization.
Dipolar polarization is a polarization that is either inherent to polar molecules (orientation polarization), or can be induced in any molecule in which the asymmetric distortion of the nuclei is possible (distortion polarization). Orientation polarization results from a permanent dipole, e.g., that arising from the 104.45° angle between the asymmetric bonds between oxygen and hydrogen atoms in the water molecule, which retains polarization in the absence of an external electric field. The assembly of these dipoles forms a macroscopic polarization.
When an external electric field is applied, the distance between charges within each permanent dipole, which is related to chemical bonding, remains constant in orientation polarization; however, the direction of polarization itself rotates. This rotation occurs on a timescale that depends on the torque and surrounding local viscosity of the molecules. Because the rotation is not instantaneous, dipolar polarizations lose the response to electric fields at the highest frequencies. A molecule rotates about 1 radian per picosecond in a fluid, thus this loss occurs at about 1011 Hz (in the microwave region). The delay of the response to the change of the electric field causes friction and heat.
When an external electric field is applied at infrared frequencies or less, the molecules are bent and stretched by the field and the molecular dipole moment changes. The molecular vibration frequency is roughly the inverse of the time it takes for the molecules to bend, and this distortion polarization disappears above the infrared.
Ionic polarization.
Ionic polarization is polarization caused by relative displacements between positive and negative ions in ionic crystals (for example, NaCl).
If a crystal or molecule consists of atoms of more than one kind, the distribution of charges around an atom in the crystal or molecule leans to positive or negative. As a result, when lattice vibrations or molecular vibrations induce relative displacements of the atoms, the centers of positive and negative charges are also displaced. The locations of these centers are affected by the symmetry of the displacements. When the centers don't correspond, polarizations arise in molecules or crystals. This polarization is called ionic polarization.
Ionic polarization causes the ferroelectric effect as well as dipolar polarization. The ferroelectric transition, which is caused by the lining up of the orientations of permanent dipoles along a particular direction, is called an order-disorder phase transition. The transition caused by ionic polarizations in crystals is called a displacive phase transition.
Dielectric dispersion.
In physics, dielectric dispersion is the dependence of the permittivity of a dielectric material on the frequency of an applied electric field. Because there is a lag between changes in polarization and changes in the electric field, the permittivity of the dielectric is a complicated function of frequency of the electric field. Dielectric dispersion is very important for the applications of dielectric materials and for the analysis of polarization systems.
This is one instance of a general phenomenon known as material dispersion: a frequency-dependent response of a medium for wave propagation.
When the frequency becomes higher:
In the frequency region above ultraviolet, permittivity approaches the constant "ε"0 in every substance, where "ε"0 is the permittivity of the free space. Because permittivity indicates the strength of the relation between an electric field and polarization, if a polarization process loses its response, permittivity decreases.
Dielectric relaxation.
Dielectric relaxation is the momentary delay (or lag) in the dielectric constant of a material. This is usually caused by the delay in molecular polarization with respect to a changing electric field in a dielectric medium (e.g., inside capacitors or between two large conducting surfaces). Dielectric relaxation in changing electric fields could be considered analogous to hysteresis in changing magnetic fields (for inductors or transformers). Relaxation in general is a delay or lag in the response of a linear system, and therefore dielectric relaxation is measured relative to the expected linear steady state (equilibrium) dielectric values. The time lag between electrical field and polarization implies an irreversible degradation of Gibbs free energy.
In physics, dielectric relaxation refers to the relaxation response of a dielectric medium to an external, oscillating electric field. This relaxation is often described in terms of permittivity as a function of frequency, which can, for ideal systems, be described by the Debye equation. On the other hand, the distortion related to ionic and electronic polarization shows behavior of the resonance or oscillator type. The character of the distortion process depends on the structure, composition, and surroundings of the sample.
Debye relaxation.
Debye relaxation is the dielectric relaxation response of an ideal, noninteracting population of dipoles to an alternating external electric field. It is usually expressed in the complex permittivity formula_17 of a medium as a function of the field's frequency formula_18:
where formula_20 is the permittivity at the high frequency limit, formula_21 where formula_22 is the static, low frequency permittivity, and formula_23 is the characteristic relaxation time of the medium.
This relaxation model was introduced by and named after the physicist Peter Debye (1913).
Paraelectricity.
Paraelectricity is the ability of many materials (specifically ceramics) to become polarized under an applied electric field. Unlike ferroelectricity, this can happen even if there is no permanent electric dipole that exists in the material, and removal of the fields results in the polarization in the material returning to zero. The mechanisms that cause paraelectric behaviour are the distortion of individual ions (displacement of the electron cloud from the nucleus) and polarization of molecules or combinations of ions or defects.
Paraelectricity can occur in crystal phases where electric dipoles are unaligned and thus have the potential to align in an external electric field and weaken it.
An example of a paraelectric material of high dielectric constant is strontium titanate.
The LiNbO3 crystal is ferroelectric below 1430 K, and above this temperature it transforms into a disordered paraelectric phase. Similarly, other perovskites also exhibit paraelectricity at high temperatures.
Paraelectricity has been explored as a possible refrigeration mechanism; polarizing a paraelectric by applying an electric field under adiabatic process conditions raises the temperature, while removing the field lowers the temperature. A heat pump that operates by polarizing the paraelectric, allowing it to return to ambient temperature (by dissipating the extra heat), bringing it into contact with the object to be cooled, and finally depolarizing it, would result in refrigeration.
Tunability.
"Tunable dielectrics" are insulators whose ability to store electrical charge changes when a voltage is applied.
Generally, strontium titanate (SrTiO3) is used for devices operating at low temperatures, while barium strontium titanate (Ba1−xSrxTiO3) substitutes for room temperature devices. Other potential materials include microwave dielectrics and carbon nanotube (CNT) composites.
In 2013 multi-sheet layers of strontium titanate interleaved with single layers of strontium oxide produced a dielectric capable of operating at up to 125 GHz. The material was created via molecular beam epitaxy. The two have mismatched crystal spacing that produces strain within the strontium titanate layer that makes it less stable and tunable.
Systems such as Ba1−xSrxTiO3 have a paraelectric–ferroelectric transition just below ambient temperature, providing high tunability. Such films suffer significant losses arising from defects.
Applications.
Capacitors.
Commercially manufactured capacitors typically use a solid dielectric material with high permittivity as the intervening medium between the stored positive and negative charges. This material is often referred to in technical contexts as the "capacitor dielectric".
The most obvious advantage to using such a dielectric material is that it prevents the conducting plates, on which the charges are stored, from coming into direct electrical contact. More significantly, however, a high permittivity allows a greater stored charge at a given voltage. This can be seen by treating the case of a linear dielectric with permittivity ε and thickness formula_24 between two conducting plates with uniform charge density σε. In this case the charge density is given by
and the capacitance per unit area by
From this, it can easily be seen that a larger ε leads to greater charge stored and thus greater capacitance.
Dielectric materials used for capacitors are also chosen such that they are resistant to ionization. This allows the capacitor to operate at higher voltages before the insulating dielectric ionizes and begins to allow undesirable current.
Dielectric resonator.
A "dielectric resonator oscillator" (DRO) is an electronic component that exhibits resonance of the polarization response for a narrow range of frequencies, generally in the microwave band. It consists of a "puck" of ceramic that has a large dielectric constant and a low dissipation factor. Such resonators are often used to provide a frequency reference in an oscillator circuit. An unshielded dielectric resonator can be used as a Dielectric Resonator Antenna (DRA).
Some practical dielectrics.
Dielectric materials can be solids, liquids, or gases. In addition, a high vacuum can also be a useful, nearly lossless dielectric even though its relative dielectric constant is only unity.
Solid dielectrics are perhaps the most commonly used dielectrics in electrical engineering, and many solids are very good insulators. Some examples include porcelain, glass, and most plastics. Air, nitrogen and sulfur hexafluoride are the three most commonly used gaseous dielectrics.

</doc>
<doc id="41027" url="http://en.wikipedia.org/wiki?curid=41027" title="Dielectric strength">
Dielectric strength

In physics, the term dielectric strength has the following meanings:
The theoretical dielectric strength of a material is an intrinsic property of the bulk material and is dependent on the configuration of the material or the electrodes with which the field is applied. The "intrinsic dielectric strength" is measured using pure materials under ideal laboratory conditions. At breakdown, the electric field frees bound electrons. If the applied electric field is sufficiently high, free electrons from background radiation may become accelerated to velocities that can liberate additional electrons during collisions with neutral atoms or molecules in a process called avalanche breakdown. Breakdown occurs quite abruptly (typically in nanoseconds), resulting in the formation of an electrically conductive path and a disruptive discharge through the material. For solid materials, a breakdown event severely degrades, or even destroys, its insulating capability.
Factors affecting apparent dielectric strength
Breakdown field strength.
The field strength at which breakdown occurs depends on the respective geometries of the dielectric (insulator) and the electrodes with which the electric field is applied, as well as the rate of increase at which the electric field is applied. Because dielectric materials usually contain minute defects, the practical dielectric strength will be a fraction of the intrinsic dielectric strength of an ideal, defect-free, material. Dielectric films tend to exhibit greater dielectric strength than thicker samples of the same material. For instance, the dielectric strength of silicon dioxide films of a few hundred nm to a few μm thick is approximately 0.5GV/m. However very thin layers (below, say, 100 nm) become partially conductive because of electron tunneling. Multiple layers of thin dielectric films are used where maximum practical dielectric strength is required, such as high voltage capacitors and pulse transformers. Since the dielectric strength of gases varies depending on the shape and configuration of the electrodes, it is usually measured as a fraction of the dielectric strength of Nitrogen gas.
Dielectric strength (in MV/m, or 106 Volt/meter) of various common materials:
Units.
In SI, the unit of dielectric strength is volts per meter (V/m). It is also common to see related units such as volts per centimeter (V/cm), megavolts per meter (MV/m), and so on.
In United States customary units, dielectric strength is often specified in volts per mil (a mil is 1/1000 inch). The conversion is:

</doc>
<doc id="41030" url="http://en.wikipedia.org/wiki?curid=41030" title="Differential Manchester encoding">
Differential Manchester encoding

Differential Manchester encoding is a line code in which data and clock signals are combined to form a single 2-level self-synchronizing data stream. It is a differential encoding, using the presence or absence of transitions to indicate logical value. It is not necessary to know the polarity of the sent signal since the information is not kept in the actual values of the voltage but in their change: in other words it does not matter whether a logical 1 or 0 is received, but only whether the polarity is the same or different from the previous value; this makes synchronization easier.
Differential Manchester encoding is not to be confused with biphase mark code (BMC) or FM1, biphase space coding, and biphase level coding since these four lines codes are each unique.
Differential Manchester encoding has the following advantages over some other line codes:
These positive features are achieved at the expense of doubling clock frequency - the symbol rate is twice the bitrate of the original signal.
Each bit period is divided into two half-periods: clock and data.
The clock half-period always begins with a transition from low to high or from high to low.
The data half-period makes a transition for one value and no transition for the other value.
One version of the code makes a transition for 0 and no transition for 1 in the data half-period; the other makes a transition for 1 and no transition for 0.
Thus, if a "1" is represented by one transition, then a "0" is represented by two transitions and vice versa, making Differential Manchester a form of frequency shift keying.
Either code can be interpreted with the clock half-period either before or after the data half-period.
Biphase mark coding transitions on every positive edge of the clock signal (when the clock goes from 0 to 1) and also transitions on the negative edge of the clock signal when the data is a 1. 
Differential Manchester is specified in the IEEE 802.5 standard for token ring LANs, and is used for many other applications, including magnetic and optical storage.
Biphase Mark Code (BMC) is used as the encoding method in AES3 and S/PDIF. Many magnetic stripe cards also use BMC encoding, often called F2F (frequency/double frequency) or Aiken Biphase. That standard is described in ISO/IEC 7811. SMPTE time code also uses BMC.
BMC is also the original "frequency modulation" used on single-density floppy disks, before being replaced by "double-density" modified frequency modulation.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41031" url="http://en.wikipedia.org/wiki?curid=41031" title="Diffraction grating">
Diffraction grating

In optics, a diffraction grating is an optical component with a periodic structure, which splits and diffracts light into several beams travelling in different directions. The emerging coloration is a form of structural coloration. The directions of these beams depend on the spacing of the grating and the wavelength of the light so that the grating acts as the dispersive element. Because of this, gratings are commonly used in monochromators and spectrometers.
For practical applications, gratings generally have ridges or "rulings" on their surface rather than dark lines. Such gratings can be either transmissive or reflective. Gratings which modulate the phase rather than the amplitude of the incident light are also produced, frequently using holography.
The principles of diffraction gratings were discovered by James Gregory, about a year after Newton's prism experiments, initially with items such as bird feathers. The first man-made diffraction grating was made around 1785 by Philadelphia inventor David Rittenhouse, who strung hairs between two finely threaded screws. This was similar to notable German physicist Joseph von Fraunhofer's wire diffraction grating in 1821.
Diffraction can create "rainbow" colors when illuminated by a wide spectrum (e.g., continuous) light source. The sparkling effects from the closely spaced narrow tracks on optical storage disks such as CD's or DVDs are an example, while the similar rainbow effects caused by thin layers of oil (or gasoline, etc.) on water are not caused by a grating, but rather by interference effects in reflections from the closely spaced transmissive layers (see Examples, below). A grating has parallel lines, while a CD has a spiral of finely-spaced data tracks. Diffraction colors also appear when one looks at a bright point source through a translucent fine-pitch umbrella-fabric covering. Decorative patterned plastic films based on reflective grating patches are very inexpensive, and are commonplace.
Theory of operation.
The relationship between the grating spacing and the angles of the incident and diffracted beams of light is known as the grating equation.
According to the Huygens–Fresnel principle, each point on the wavefront of a propagating wave can be considered to act as a point source, and the wavefront at any subsequent point can be found by adding together the contributions from each of these individual point sources.
Gratings may be of the 'reflective' or 'transmissive' type, analogous to a mirror or lens respectively. A grating has a 'zero-order mode' (where "m" = 0), in which there is no diffraction and a ray of light behaves according to the laws of reflection and refraction the same as with a mirror or lens respectively.
An idealised grating is considered here which is made up of a set of slits of spacing "d", that must be wider than the wavelength of interest to cause diffraction. Assuming a plane wave of monochromatic light of wavelength "λ" with normal incidence (perpendicular to the grating), each slit in the grating acts as a quasi point-source from which light propagates in all directions (although this is typically limited to a hemisphere). After light interacts with the grating, the diffracted light is composed of the sum of interfering wave components emanating from each slit in the grating. At any given point in space through which diffracted light may pass, the path length to each slit in the grating will vary. Since the path length varies, generally, so will the phases of the waves at that point from each of the slits, and thus will add or subtract from one another to create peaks and valleys, through the phenomenon of additive and destructive interference. When the path difference between the light from adjacent slits is equal to half the wavelength, "λ"/2, the waves will all be out of phase, and thus will cancel each other to create points of minimum intensity. Similarly, when the path difference is "λ", the phases will add together and maxima will occur. The maxima occur at angles "θ"m, which satisfy the relationship "d" sin"θ"m/"λ" = |"m"|, where "θ"m is the angle between the diffracted ray and the grating's normal vector, and "d" is the distance from the center of one slit to the center of the adjacent slit, and "m" is an integer representing the propagation-mode of interest.
Thus, when light is normally incident on the grating, the diffracted light will have maxima at angles "θ"m given by:
It is straightforward to show that if a plane wave is incident at any arbitrary angle "θ"i, the grating equation becomes:
When solved for the diffracted angle maxima, the equation is:
The light that corresponds to direct transmission (or specular reflection in the case of a reflection grating) is called the zero order, and is denoted "m" = 0. The other maxima occur at angles which are represented by non-zero integers "m". Note that "m" can be positive or negative, resulting in diffracted orders on both sides of the zero order beam.
This derivation of the grating equation is based on an idealised grating. However, the relationship between the angles of the diffracted beams, the grating spacing and the wavelength of the light apply to any regular structure of the same spacing, because the phase relationship between light scattered from adjacent elements of the grating remains the same. The detailed distribution of the diffracted light depends on the detailed structure of the grating elements as well as on the number of elements in the grating, but it will always give maxima in the directions given by the grating equation.
Gratings can be made in which various properties of the incident light are modulated in a periodic pattern; these include
The grating equation applies in all these cases.
Quantum electrodynamics.
Quantum electrodynamics (QED) offers another derivation of the properties of a diffraction grating in terms of photons as particles (at some level). QED can be described intuitively with the path integral formulation of quantum mechanics. As such it can model photons as potentially following all paths from a source to a final point, each path with a certain probability amplitude. These probability amplitudes can be represented as a complex number or equivalent vector—or, as Richard Feynman simply calls them in his book on QED, "arrows".
For the probability that a certain event will happen, one sums the probability amplitudes for all of the possible ways in which the event can occur, and then takes the square of the length of the result. The probability amplitude for a photon from a monochromatic source to arrive at a certain final point at a given time, in this case, can be modeled as an arrow that spins rapidly until it is evaluated when the photon reaches its final point. For example, for the probability that a photon will reflect off of a mirror and be observed at a given point a given amount of time later, one sets the photon's probability amplitude spinning as it leaves the source, follows it to the mirror, and then to its final point, even for paths that do not involve bouncing off of the mirror at equal angles. One can then evaluate the probability amplitude at the photon's final point; next, one can integrate over all of these arrows (see vector sum), and square the length of the result to obtain the probability that this photon will reflect off of the mirror in the pertinent fashion. The times these paths take are what determine the angle of the probability amplitude arrow, as they can be said to "spin" at a constant rate (which is related to the frequency of the photon).
The times of the paths near the classical reflection site of the mirror will be nearly the same, so as a result the probability amplitudes will point in nearly the same direction—thus, they will have a sizable sum. Examining the paths towards the edges of the mirror reveals that the times of nearby paths are quite different from each other, and thus we wind up summing vectors that cancel out quickly. So, there is a higher probability that light will follow a near-classical reflection path than a path further out. However, a diffraction grating can be made out of this mirror, by scraping away areas near the edge of the mirror that usually cancel nearby amplitudes out—but now, since the photons would not reflect from the scraped-off portions, the probability amplitudes which would all wind up pointing, for instance, at forty-five degrees can have a sizable sum. Thus, this would let light of the right frequency to make this happen sum to a larger probability amplitude, and as such possess a larger probability of reaching the appropriate final point.
This particular description involves many simplifications: a point source, a "surface" that light can reflect off of (thus neglecting the interactions with electrons) and so forth. The biggest simplification is perhaps in the fact that the "spinning" of the probability amplitude arrows is actually more accurately explained as a "spinning" of the source, as the probability amplitudes of photons do not "spin" while they are in transit. We obtain the same variation in probability amplitudes by allowing the time at which the photon left the source to be indeterminate, and the time of the path now tells us when the photon would have left the source, and thus what the angle of its "arrow" would be. However, this model and approximation is a reasonable one to illustrate a diffraction grating conceptually. Light of a different frequency may also reflect off of the same diffraction grating, but with a different final point.
Gratings as dispersive elements.
The wavelength dependence in the grating equation shows that the grating separates an incident polychromatic beam into its constituent wavelength components, i.e., it is dispersive. Each wavelength of input beam spectrum is sent into a different direction, producing a rainbow of colors under white light illumination. This is visually similar to the operation of a prism, although the mechanism is very different.
The diffracted beams corresponding to consecutive orders may overlap, depending on the spectral content of the incident beam and the grating density. The higher the spectral order, the greater the overlap into the next order.
The grating equation shows that the angles of the diffracted orders only depend on the grooves' period, and not on their shape. By controlling the cross-sectional profile of the grooves, it is possible to concentrate most of the diffracted energy in a particular order for a given wavelength. A triangular profile is commonly used. This technique is called "blazing." The incident angle and wavelength for which the diffraction is most efficient are often called "blazing angle" and "blazing wavelength." The efficiency of a grating may also depend on the polarization of the incident light. Gratings are usually designated by their "groove density", the number of grooves per unit length, usually expressed in grooves per millimeter (g/mm), also equal to the inverse of the groove period. The groove period must be on the order of the wavelength of interest; the spectral range covered by a grating is dependent on groove spacing and is the same for ruled and holographic gratings with the same grating constant. The maximum wavelength that a grating can diffract is equal to twice the grating period, in which case the incident and diffracted light will be at ninety degrees to the grating normal. To obtain frequency dispersion over a wider frequency one must use a prism. In the optical regime, in which the use of gratings is most common, this corresponds to wavelengths between 100 nm and 10 µm. In that case, the groove density can vary from a few tens of grooves per millimeter, as in "echelle gratings", to a few thousands of grooves per millimeter.
When groove spacing is less than half the wavelength of light, the only present order is the "m" = 0 order. Gratings with such small periodicity are called subwavelength gratings and exhibit special optical properties. Made on an isotropic material the subwavelength gratings give rise to form birefringence, in which the material behaves as if it were birefringent.
Fabrication.
Originally, high-resolution gratings were ruled using high-quality "ruling engines" whose construction was a large undertaking. Henry Joseph Grayson designed a machine to make diffraction gratings, succeeding with one of 120,000 lines to the inch (approx. 47 000 per cm) in 1899. Later, photolithographic techniques allowed gratings to be created from a holographic interference pattern. Holographic gratings have sinusoidal grooves and may not be as efficient as ruled gratings, but are often preferred in monochromators because they lead to much less stray light. A copying technique allows high quality replicas to be made from master gratings of either type, thereby lowering fabrication costs.
Another method for manufacturing diffraction gratings uses a photosensitive gel sandwiched between two substrates. A holographic interference pattern exposes the gel which is later developed. These gratings, called "volume phase holography diffraction gratings" (or VPH diffraction gratings) have no physical grooves, but instead a periodic modulation of the refractive index within the gel. This removes much of the surface scattering effects typically seen in other types of gratings. These gratings also tend to have higher efficiencies, and allow for the inclusion of complicated patterns into a single grating. In older versions of such gratings, environmental susceptibility was a trade-off, as the gel had to be contained at low temperature and humidity. Typically, the photosensitive substances are sealed between two substrates which make them resistant to humidity, thermal and mechanical stresses. VPH diffraction gratings are not destroyed by accidental touches and are more scratch resistant than typical relief gratings.
Semiconductor technology today is also utilized to etch holographically patterned gratings into robust materials such as fused silica. In this way, low stray-light holography is combined with the high efficiency of deep, etched transmission gratings, and can be incorporated into high volume, low cost semiconductor manufacturing technology.
A new technology for grating insertion into integrated photonic lightwave circuits is digital planar holography (DPH). DPH gratings are generated in computer and fabricated on one or several interfaces of an optical waveguide planar with standard micro-lithography or nano-imprinting methods, compatible with mass-production. Light propagates inside the DPH gratings, confined by the refractive index gradient, which provides longer interaction path and greater flexibility in light steering.
Examples.
Diffraction gratings are often used in monochromators, spectrometers, lasers, wavelength division multiplexing devices, optical pulse compressing devices, and many other optical instruments.
Ordinary pressed CD and DVD media are every-day examples of diffraction gratings and can be used to demonstrate the effect by reflecting sunlight off them onto a white wall. This is a side effect of their manufacture, as one surface of a CD has many small pits in the plastic, arranged in a spiral; that surface has a thin layer of metal applied to make the pits more visible. The structure of a DVD is optically similar, although it may have more than one pitted surface, and all pitted surfaces are inside the disc.
In a standard pressed vinyl record when viewed from a low angle perpendicular to the grooves, a similar but less defined effect to that in a CD/DVD is seen. This is due to viewing angle (less than the critical angle of reflection of the black vinyl) and the path of the light being reflected due to this being changed by the grooves, leaving a rainbow relief pattern behind.
Diffraction gratings are also used to distribute evenly the frontlight of e-readers such as the Nook Simple Touch with GlowLight.
Natural gratings.
Striated muscle is the most commonly found natural diffraction grating and, this has helped physiologists in determining the structure of such muscle. Aside from this, the chemical structure of crystals can be thought of as diffraction gratings for types of electromagnetic radiation other than visible light, this is the basis for techniques such as X-ray crystallography. 
Most commonly confused with diffraction gratings are the iridescent colors of peacock feathers, mother-of-pearl, and butterfly wings. Iridescence in birds, fishes and insects is often caused by thin-film interference rather than a diffraction grating. Diffraction will produce the entire spectrum of colors as the viewing angle changes, whereas thin-film interference usually produces a much narrower range. The surfaces of flowers can also create a diffraction, but the cell structures in plants are usually too irregular to produce the fine slit geometry necessary for a diffraction grating. The iridescence signal of flowers is thus only appreciable very locally and hence not visible to man and flower visiting insects. However, natural gratings do occur in some invertebrate marine animals, like the antennae of seed shrimp, and have even been discovered in Burgess Shale fossils.
Diffraction grating effects are sometimes seen in meteorology. Diffraction coronas are colorful rings surrounding a source of light, such as the sun. These are usually observed much closer to the light source than halos, and are caused by very fine particles, like water droplets, ice crystals, or smoke particles in a hazy sky. When the particles are all nearly the same size they diffract the incoming light at very specific angles. The exact angle depends on the size of the particles. Diffraction coronas are commonly observed around light sources, like candle flames or street lights, in the fog. Cloud iridescence is caused by diffraction, occurring along coronal rings when the particles in the clouds are all uniform in size.
References.
</dl>

</doc>
<doc id="41033" url="http://en.wikipedia.org/wiki?curid=41033" title="Digital filter">
Digital filter

In signal processing, a digital filter is a system that performs mathematical operations on a sampled, discrete-time signal to reduce or enhance certain aspects of that signal. This is in contrast to the other major type of electronic filter, the analog filter, which is an electronic circuit operating on continuous-time analog signals.
A digital filter system usually consists of an analog-to-digital converter to sample the input signal, followed by a microprocessor and some peripheral components such as memory to store data and filter coefficients etc. Finally a digital-to-analog converter to complete the output stage. Program Instructions (software) running on the microprocessor implement the digital filter by performing the necessary mathematical operations on the numbers received from the ADC. In some high performance applications, an FPGA or ASIC is used instead of a general purpose microprocessor, or a specialized DSP with specific paralleled architecture for expediting operations such as filtering.
Digital filters may be more expensive than an equivalent analog filter due to their increased complexity, but they make practical many designs that are impractical or impossible as analog filters. When used in the context of real-time analog systems, digital filters sometimes have problematic latency (the difference in time between the input and the response) due to the associated analog-to-digital and digital-to-analog conversions and anti-aliasing filters, or due to other delays in their implementation.
Digital filters are commonplace and an essential element of everyday electronics such as radios, cellphones, and AV receivers.
Characterization.
A digital filter is characterized by its transfer function, or equivalently, its difference equation. Mathematical analysis of the transfer function can describe how it will respond to any input. As such, designing a filter consists of developing specifications appropriate to the problem (for example, a second-order low pass filter with a specific cut-off frequency), and then producing a transfer function which meets the specifications.
The transfer function for a linear, time-invariant, digital filter can be expressed as a transfer function in the "Z"-domain; if it is causal, then it has the form:
where the order of the filter is the greater of "N" or "M".
See "Z"-transform's LCCD equation for further discussion of this transfer function.
This is the form for a recursive filter with both the inputs (Numerator) and outputs (Denominator), which typically leads to an IIR infinite impulse response behaviour, but if the denominator is made equal to unity i.e. no feedback, then this becomes an FIR or finite impulse response filter.
Analysis techniques.
A variety of mathematical techniques may be employed to analyze the behaviour of a given digital filter. Many of these analysis techniques may also be employed in designs, and often form the basis of a filter specification.
Typically, one characterizes filters by calculating how they will respond to a simple input such as an impulse. One can then extend this information to compute the filter's response to more complex signals.
Impulse response.
The impulse response, often denoted formula_2 or formula_3, is a measurement of how a filter will respond to the Kronecker delta function. For example, given a difference equation, one would set formula_4 and formula_5 for formula_6 and evaluate. The impulse response is a characterization of the filter's behaviour. Digital filters are typically considered in two categories: infinite impulse response (IIR) and finite impulse response (FIR).
In the case of linear time-invariant FIR filters, the impulse response is exactly equal to the sequence of filter coefficients:
IIR filters on the other hand are recursive, with the output depending on both current and previous inputs as well as previous outputs. The general form of an IIR filter is thus:
Plotting the impulse response will reveal how a filter will respond to a sudden, momentary disturbance.
Difference equation.
In discrete-time systems, the digital filter is often implemented by converting the transfer function to a linear constant-coefficient difference equation (LCCD) via the Z-transform. The discrete frequency-domain transfer function is written as the ratio of two polynomials. For example:
This is expanded:
and to make the corresponding filter causal, the numerator and denominator are divided by the highest order of formula_11:
The coefficients of the denominator, formula_13, are the 'feed-backward' coefficients and the coefficients of the numerator are the 'feed-forward' coefficients, formula_14. The resultant linear difference equation is:
or, for the example above:
rearranging terms:
then by taking the inverse "z"-transform:
and finally, by solving for formula_19:
This equation shows how to compute the next output sample, formula_19, in terms of the past outputs, formula_22, the present input, formula_23, and the past inputs, formula_24. Applying the filter to an input in this form is equivalent to a Direct Form I or II realization, depending on the exact order of evaluation.
Filter design.
The design of digital filters is a deceptively complex topic. Although filters are easily understood and calculated, the practical challenges of their design and implementation are significant and are the subject of much advanced research.
There are two categories of digital filter: the recursive filter and the nonrecursive filter. These are often referred to as infinite impulse response (IIR) filters and finite impulse response (FIR) filters, respectively.
Filter realization.
After a filter is designed, it must be "realized" by developing a signal flow diagram that describes the filter in terms of operations on sample sequences.
A given transfer function may be realized in many ways. Consider how a simple expression such as formula_25 could be evaluated – one could also compute the equivalent formula_26. In the same way, all realizations may be seen as "factorizations" of the same transfer function, but different realizations will have different numerical properties. Specifically, some realizations are more efficient in terms of the number of operations or storage elements required for their implementation, and others provide advantages such as improved numerical stability and reduced round-off error. Some structures are better for fixed-point arithmetic and others may be better for floating-point arithmetic.
Direct Form I.
A straightforward approach for IIR filter realization is Direct Form I, where the difference equation is evaluated directly. This form is practical for small filters, but may be inefficient and impractical (numerically unstable) for complex designs. In general, this form requires 2N delay elements (for both input and output signals) for a filter of order N.
Direct Form II.
The alternate Direct Form II only needs "N" delay units, where "N" is the order of the filter – potentially half as much as Direct Form I. This structure is obtained by reversing the order of the numerator and denominator sections of Direct Form I, since they are in fact two linear systems, and the commutativity property applies. Then, one will notice that there are two columns of delays (formula_27) that tap off the center net, and these can be combined since they are redundant, yielding the implementation as shown below.
The disadvantage is that Direct Form II increases the possibility of arithmetic overflow for filters of high Q or resonance. It has been shown that as Q increases, the round-off noise of both direct form topologies increases without bounds. This is because, conceptually, the signal is first passed through an all-pole filter (which normally boosts gain at the resonant frequencies) before the result of that is saturated, then passed through an all-zero filter (which often attenuates much of what the all-pole half amplifies).
Cascaded second-order sections.
A common strategy is to realize a higher-order (greater than 2) digital filter as a cascaded series of second-order "biquadratric" (or "biquad") sections (see digital biquad filter). The advantage of this strategy is that the coefficient range is limited. Cascading direct form II sections results in N delay elements for filters of order N. Cascading direct form I sections results in N+2 delay elements since the delay elements of the input of any section (except the first section) are redundant with the delay elements of the output of the preceding section.
Other forms.
Other forms include:
Comparison of analog and digital filters.
Digital filters are not subject to the component non-linearities that greatly complicate the design of analog filters. Analog filters consist of imperfect electronic components, whose values are specified to a limit tolerance (e.g. resistor values often have a tolerance of ±5%) and which may also change with temperature and drift with time. As the order of an analog filter increases, and thus its component count, the effect of variable component errors is greatly magnified. In digital filters, the coefficient values are stored in computer memory, making them far more stable and predictable.
Because the coefficients of digital filters are definite, they can be used to achieve much more complex and selective designs – specifically with digital filters, one can achieve a lower passband ripple, faster transition, and higher stopband attenuation than is practical with analog filters. Even if the design could be achieved using analog filters, the engineering cost of designing an equivalent digital filter would likely be much lower. Furthermore, one can readily modify the coefficients of a digital filter to make an adaptive filter or a user-controllable parametric filter. While these techniques are possible in an analog filter, they are again considerably more difficult.
Digital filters can be used in the design of finite impulse response filters. Analog filters do not have the same capability, because finite impulse response filters require delay elements.
Digital filters rely less on analog circuitry, potentially allowing for a better signal-to-noise ratio. A digital filter will introduce noise to a signal during analog low pass filtering, analog to digital conversion, digital to analog conversion and may introduce digital noise due to quantization. With analog filters, every component is a source of thermal noise (such as Johnson noise), so as the filter complexity grows, so does the noise.
However, digital filters do introduce a higher fundamental latency to the system. In an analog filter, latency is often negligible; strictly speaking it is the time for an electrical signal to propagate through the filter circuit. In digital systems, latency is introduced by delay elements in the digital signal path, and by analog-to-digital and digital-to-analog converters that enable the system to process analog signals.
In very simple cases, it is more cost effective to use an analog filter. Introducing a digital filter requires considerable overhead circuitry, as previously discussed, including two low pass analog filters.
Another argument for analog filters is low power consumption. Analog filters require substantially less power and are therefor the only solution when power requirements are tight.
When making an electrical circuit on a PCB it is generally easier to use a digital solution, because the processing units are highly optimized over the years. Making the same circuit with analog components would take up a lot more space when using discrete components. Two alternatives are FPAA's and ASIC's, but they are expensive for low quantities.
Types of digital filters.
Many digital filters are based on the fast Fourier transform, a mathematical algorithm that quickly extracts the frequency spectrum of a signal, allowing the spectrum to be manipulated (such as to create band-pass filters) before converting the modified spectrum back into a time-series signal.
Another form of a digital filter is that of a state-space model.
A well used state-space filter is the Kalman filter published by Rudolf Kalman in 1960.
Traditional linear filters are usually based on attenuation. Alternatively nonlinear filters can be designed, including energy transfer filters which allow the user to move energy in a designed way. So that unwanted noise or effects can be moved to new frequency bands either lower or higher in frequency, spread over a range of frequencies, split, or focused. Energy transfer filters complement traditional filter designs and introduce many more degrees of freedom in filter design. Digital energy transfer filters are relatively easy to design and to implement and exploit nonlinear dynamics.

</doc>
<doc id="41035" url="http://en.wikipedia.org/wiki?curid=41035" title="Digital milliwatt">
Digital milliwatt

In digital telephony, the digital milliwatt is a standard test signal consisting of eight 8-bit words corresponding to one pulse-code modulated cycle of a sinusoidal signal approximately 1 kHz in frequency and one milliwatt in power. 
The digital milliwatt is stored in ROM. A continuous signal of arbitrary length, "i.e." an indefinite number of cycles, may be realized by continually reading out and concatenating the stored information into a data stream to be converted into analog form. 
The digital milliwatt is used in place of separate test equipment. It has the advantage of being tied in frequency and amplitude to the relatively stable digital clock signal and power (voltage) supply, respectively, that are used by the digital channel bank. 
Source: from Federal Standard 1037C and from the Code of Federal Regulations, Telecommunications Parts 0-199 (edited for Wikipedia).
unsigned char ulaw_digital_milliwatt[8] =
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41036" url="http://en.wikipedia.org/wiki?curid=41036" title="Digital multiplex hierarchy">
Digital multiplex hierarchy

In telecommunications, a digital multiplex hierarchy is a hierarchy consisting of an ordered repetition of tandem digital multiplexers that produce signals of successively higher data rates at each level of the hierarchy. 
Digital multiplexing hierarchies may be implemented in many different configurations depending on; (a) the number of channels desired, (b) the signaling system to be used, and (c) the bit rate allowed by the communications media. 
Some currently available digital multiplexers have been designated as Dl-, DS-, or M-series, all of which operate at T-carrier rates. 
In the design of digital multiplex hierarchies, care must be exercised to ensure interoperability of the multiplexers used in the hierarchy.

</doc>
<doc id="41037" url="http://en.wikipedia.org/wiki?curid=41037" title="Digital Signal 0">
Digital Signal 0

Digital Signal 0 (DS0) is a basic digital signaling rate of 64 kbit/s, corresponding to the capacity of one voice-frequency-equivalent channel. The DS0 rate, and its equivalents E0 and J0, form the basis for the digital multiplex transmission hierarchy in telecommunications systems used in North America, Europe, Japan, and the rest of the world, for both the early plesiochronous systems such as T-carrier and for modern synchronous systems such as SDH/SONET.
The DS0 rate was introduced to carry a single digitized voice call. For a typical phone call, the audio sound is digitized at an 8 kHz sample rate, or 8000 samples per second, using 8-bit pulse-code modulation for each of the samples. This results in a data rate of 64 kbit/s.
Because of its fundamental role in carrying a single phone call, the DS0 rate forms the basis for the digital multiplex transmission hierarchy in telecommunications systems used in North America. To limit the number of wires required between two involved in exchanging voice calls, a system was built in which multiple DS0s are multiplexed together on higher capacity circuits. In this system, twenty-four (24) DS0s are multiplexed into a DS1 signal. Twenty-eight (28) DS1s are multiplexed into a DS3. When carried over copper wire, this is the well-known T-carrier system, with T1 and T3 corresponding to DS1 and DS3, respectively. 
Besides its use for voice communications, the DS0 rate may support twenty 2.4 kbit/s channels, ten 4.8 kbit/s channels, five 9.67 kbit/s channels, one 56 kbit/s channel, or one 64 kbit/s clear channel.
E0 (standardized as ITU G.703) is the European equivalent of the North American DS0 for carrying . However, there are some subtle differences in implementation. Voice signals are encoded for carriage over E0 according to ITU G.711. Note that when a T-carrier system is used as in North America, robbed bit signaling can mean that a DS0 channel carried over that system is not an error-free bit-stream. The out-of-band signaling used in the European E-carrier system avoids this.
See also.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41038" url="http://en.wikipedia.org/wiki?curid=41038" title="Digital subscriber line">
Digital subscriber line

Digital subscriber line (DSL; originally digital subscriber loop) is a family of technologies that are used to provide internet access by transmitting digital data over telephone lines. In telecommunications marketing, the term DSL is widely understood to mean asymmetric digital subscriber line (ADSL), the most commonly installed DSL technology. DSL service is delivered simultaneously with wired telephone service on the same telephone line. This is possible because DSL uses higher frequency bands for data. On the customer premises, a DSL filter on each non-DSL outlet blocks any high-frequency interference to enable simultaneous use of the voice and DSL services.
The bit rate of consumer DSL services typically ranges from 256 kbit/s to over 100 Mbit/s in the direction to the customer (downstream), depending on DSL technology, line conditions, and service-level implementation. Bit rates of 1 Gbit/s have been reached in trials, but most homes are likely to be limited to 500-800 Mbit/s. In ADSL, the data throughput in the upstream direction (the direction to the service provider) is lower, hence the designation of "asymmetric" service. In symmetric digital subscriber line (SDSL) services, the downstream and upstream data rates are equal. Researchers at Bell Labs have reached speeds of 10 Gbit/s, while delivering 1 Gbit/s symmetrical broadband access services using traditional copper telephone lines. These higher speeds are lab results, however. A 2012 survey found that "DSL continues to be the dominant technology for broadband access" with 364.1 million subscribers worldwide.
History.
The theoretical foundations of DSL, like much of communication technology, can be traced back to Claude Shannon's seminal 1948 paper: "A Mathematical Theory of Communication".
A patent was filed in 1979 for the use of existing telephone wires for both telephones and data terminals that were connected to a remote computer via a digital data carrier system.
The motivation for digital subscriber line technology was the Integrated Services Digital Network (ISDN) specification proposed in 1984 by the CCITT (now ITU-T) as part of Recommendation I.120, later reused as ISDN Digital Subscriber Line (IDSL). Employees at Bellcore (now Telcordia Technologies) developed Asymmetric Digital Subscriber Line (ADSL) and filed a patent in 1988, by placing wide-band digital signals above the existing baseband analog voice signal carried between telephone company telephone exchanges and customers on conventional twisted pair cabling facilities.
Consumer-oriented ADSL was designed to operate on existing lines already conditioned for Basic Rate Interface ISDN services, which itself is a digital circuit switching service (non-IP), though most incumbent local exchange carriers (ILECs) provision Rate-Adaptive Digital Subscriber Line (RADSL) to work on virtually any available copper pair facility, whether conditioned for BRI or not.
Engineers developed high speed DSL facilities such as High bit rate Digital Subscriber Line (HDSL) and Symmetric Digital Subscriber Line (SDSL) to provision traditional Digital Signal 1 (DS1) services over standard copper pair facilities.
A DSL circuit provides digital service. The underlying technology of transport across DSL facilities uses high-frequency sinusoidal carrier wave modulation, which is an analog signal transmission. A DSL circuit terminates at each end in a modem which modulates patterns of bits into certain high-frequency impulses for transmission to the opposing modem. Signals received from the far-end modem are demodulated to yield a corresponding bit pattern that the modem retransmits, in digital form, to its interfaced equipment, such as a computer, router, switch, etc. Unlike traditional dial-up modems, which modulate bits into signals in the 300–3400 Hz baseband (voice service), DSL modems modulate frequencies from 4000 Hz to as high as 4 MHz. This frequency band separation enables DSL service and plain old telephone service (POTS) to coexist on the same copper pair facility. Generally, higher bit rate transmissions require a wider frequency band, though the ratio of bit rate to symbol rate and thus to bandwidth are not linear due to significant innovations in digital signal processing and digital modulation methods.
Early DSL service required a dedicated dry loop, but when the U.S. Federal Communications Commission (FCC) required ILECs to lease their lines to competing DSL service providers, shared-line DSL became available. Also known as DSL over Unbundled Network Element, this unbundling of services allows a single subscriber to receive two separate services from two separate providers on one cable pair. The DSL service provider's equipment is co-located in the same central office (telephone exchange) as that of the ILEC supplying the customer's pre-existing voice service. The subscriber's circuit is rewired to interface with hardware supplied by the ILEC which combines a DSL frequency and POTS signals on a single copper pair facility.
On the subscriber's end of the circuit, inline low-pass DSL filters (splitters) are installed on each telephone to filter the high-frequency signals that would otherwise be heard as hiss, but pass voice frequencies. Conversely, high-pass filters already incorporated in the circuitry of DSL modems filter out voice frequencies. Although ADSL and RADSL modulations do not use the voice-frequency band, nonlinear elements in the phone could otherwise generate audible intermodulation and may impair the operation of the data modem in the absence of low-pass filters.
Older ADSL standards delivered 8 Mbit/s to the customer over about 2 km of unshielded twisted-pair copper wire. Newer variants improved these rates. Distances greater than 2 km significantly reduce the bandwidth usable on the wires, thus reducing the data rate. But ADSL loop extenders increase these distances by repeating the signal allowing the LEC to deliver DSL speeds to any distance.
By 2012 some carriers in the United States reported that DSL remote terminals with fiber backhaul are replacing older ADSL systems.
Operation.
Telephones are connected to the telephone exchange via a local loop, which is a physical pair of wires. The local loop was originally intended mostly for the transmission of speech, encompassing an audio frequency range of 300 to 3400 hertz (voiceband or commercial bandwidth). However, as long-distance trunks were gradually converted from analog to digital operation, the idea of being able to pass data through the local loop (by utilizing frequencies above the voiceband) took hold, ultimately leading to DSL.
For a long time it was thought that it was not possible to operate a conventional phone-line beyond low-speed limits (typically under 9600 bit/s). In the 1950s, ordinary twisted-pair telephone-cable often carried four megahertz (MHz) television signals between studios, suggesting that such lines would allow transmitting many megabits per second. One such circuit in the UK ran some ten miles (16 km) between the BBC studios in Newcastle-upon-Tyne and the Pontop Pike transmitting station. It was able to give the studios a low quality cue feed but not one suitable for transmission. However, these cables had other impairments besides Gaussian noise, preventing such rates from becoming practical in the field. The 1980s saw the development of techniques for broadband communications that allowed the limit to be greatly extended.
The local loop connecting the telephone exchange to most subscribers has the capability of carrying frequencies well beyond the 3.4 kHz upper limit of POTS. Depending on the length and quality of the loop, the upper limit can be tens of megahertz. DSL takes advantage of this unused bandwidth of the local loop by creating 4312.5 Hz wide channels starting between 10 and 100 kHz, depending on how the system is configured. Allocation of channels continues at higher and higher frequencies (up to 1.1 MHz for ADSL) until new channels are deemed unusable. Each channel is evaluated for usability in much the same way an analog modem would on a POTS connection. More usable channels equates to more available bandwidth, which is why distance and line quality are a factor (the higher frequencies used by DSL travel only short distances). The pool of usable channels is then split into two different frequency bands for upstream and downstream traffic, based on a preconfigured ratio. This segregation reduces interference. Once the channel groups have been established, the individual channels are bonded into a pair of virtual circuits, one in each direction. Like analog modems, DSL transceivers constantly monitor the quality of each channel and will add or remove them from service depending on whether they are usable.
Joseph W. Lechleider's contribution to DSL was his insight that an asymmetric arrangement offered more than double the bandwidth capacity of symmetric DSL. This allowed Internet service providers to offer efficient service to consumers, who benefited greatly from the ability to download large amounts of data but rarely needed to upload comparable amounts. ADSL supports two modes of transport—fast channel and interleaved channel. Fast channel is preferred for streaming multimedia, where an occasional "dropped bit" is acceptable, but lags are less so. Interleaved channel works better for file transfers, where the delivered data must be error-free but latency (time delay) incurred by the retransmission of error-containing packets is acceptable.
Because DSL operates above the 3.4 kHz voice limit, it cannot pass through a load coil, which is an inductive coil that is designed to counteract loss caused by shunt capacitance (capacitance between the two wires of the twisted pair). Load coils are commonly set at regular intervals in lines placed only for POTS. A DSL signal cannot pass through a properly installed and working load coil, while voice service cannot be maintained past a certain distance without such coils. Therefore, some areas that are within range for DSL service are disqualified from eligibility because of load coil placement. Because of this, phone companies endeavor to remove load coils on copper loops that can operate without them, and by conditioning other lines to avoid them through the use of fiber to the neighborhood or node (FTTN).
The commercial success of DSL and similar technologies largely reflects the advances made in electronics over the decades that have increased performance and reduced costs even while digging trenches in the ground for new cables (copper or fiber optic) remains expensive. Several factors contributed to the popularity of DSL technology.
Until the late 1990s, the cost of digital signal processors for DSL was prohibitive. All types of DSL employ highly complex digital signal processing algorithms to overcome the inherent limitations of the existing twisted pair wires. Due to the advancements of very-large-scale integration (VLSI) technology, the cost of the equipment associated with a DSL deployment lowered significantly. The two main pieces of equipment are a digital subscriber line access multiplexer (DSLAM) at one end and a DSL modem at the other end.
A DSL connection can be deployed over existing cable. Such deployment, even including equipment, is much cheaper than installing a new, high-bandwidth fiber-optic cable over the same route and distance. This is true both for ADSL and SDSL variations.
In the case of ADSL, competition in Internet access caused subscription fees to drop significantly over the years, thus making ADSL more economical than dial up access. Telephone companies were pressured into moving to ADSL largely due to competition from cable companies, which use DOCSIS cable modem technology to achieve similar speeds. Demand for high bandwidth applications, such as video and file sharing, also contributed to popularize ADSL technology.
Most residential and small-office DSL implementations reserve low frequencies for POTS, so that (with suitable filters and/or splitters) the existing voice service continues to operate independent of the DSL service. Thus POTS-based communications, including fax machines and analog modems, can share the wires with DSL. Only one DSL modem can use the subscriber line at a time. The standard way to let multiple computers share a DSL connection uses a router that establishes a connection between the DSL modem and a local Ethernet, Powerline, or Wi-Fi network on the customer's premises.
Once upstream and downstream channels are established, a subscriber can connect to a service such as an Internet service provider.
Naked DSL.
A naked DSL (also known as standalone or dry loop DSL) is a way of providing DSL services without a PSTN (analogue telephony) service. It is useful when the customer does not need the traditional telephony voice service because voice service is received either on top of the DSL services (usually VoIP) or through another network (mobile telephony).
It is also commonly called a "UNE" (for Unbundled Network Element) in the United States; in Australia it is known as a "ULL" (Unconditioned Local Loop)., in Belgium it is known as "Raw Copper" and in Turkey it's known as "Yalın Internet" It started making a comeback in the United States in 2004 when Qwest started offering it, closely followed by Speakeasy. As a result of AT&T's merger with SBC, and Verizon's merger with MCI, those telephone companies have an obligation to offer naked DSL to consumers.
In Turkey, since 2011, telephone companies are obliged to offer naked DSL as a result of consumer pressure to the regulatory bodies, however companies can incur additional fees under various label, such as circuit preparation service (devre hazırlama ücreti) or an additional naked DSL fee (yalın adsl ücreti). Although circuit preparation service fee is one-time, the latter is fixed and can take as much as %20 of the monthly bill.
Even without the regulatory mandate, however, many ILECs offered naked DSL to consumers. The number of telephone landlines in the United States dropped from 188 million in 2000 to 115 million in 2010, while the number of cellular subscribers has grown to 277 million (as of 2010). This lack of demand for landline voice services has resulted in the expansion of naked DSL availability.
Naked DSL products are also marketed in some other countries e.g., Australia, New Zealand, and Canada.
Typical setup.
On the customer side, the DSL Transceiver, or ATU-R, or more commonly known as a DSL modem, is hooked up to a phone line. The telephone company connects the other end of the line to a DSLAM, which concentrates a large number of individual DSL connections into a single box. The location of the DSLAM depends on the telco, but it cannot be located too far from the user because of attenuation between the DSLAM and the user's DSL modem. It is common for a few residential blocks to be connected to one DSLAM.
When the DSL modem powers up it goes through a sync procedure. The actual process varies from modem to modem but generally involves the following steps:
Modern DSL gateways have more functionality and usually go through an initialization procedure very similar to a PC boot up. The system image is loaded from the flash memory; the system boots, synchronizes the DSL connection and establishes the IP connection between the local network and the service provider, using protocols such as DHCP or PPPoE. (According to a 2007 book, the PPPoE method far outweighed DHCP in terms of deployment on DSL lines, and PAP was the predominant form of subscriber authentication used in such circumstances.) The system image can usually be updated to correct bugs, or to add new functionality.
The accompanying figure is a schematic of a simple DSL connection (in blue). The right side shows a DSLAM residing in the telephone company's central office. The left side shows the customer premises equipment with an optional router. This router manages a local area network (LAN) off of which are connected some number of PCs. With many service providers, the customer may opt for a modem which contains a wireless router. This option (within the dashed bubble) often simplifies the connection.
Exchange equipment.
At the exchange, a digital subscriber line access multiplexer (DSLAM) terminates the DSL circuits and aggregates them, where they are handed off to other networking transports. In the case of ADSL, the voice component is also separated at this step, either by a filter integrated in the DSLAM or by a specialized filtering equipment installed before it. The DSLAM terminates all connections and recovers the original digital information.
Customer equipment.
The customer end of the connection consists of a terminal adaptor or "DSL modem". This converts data between the digital signals used by computers and the voltage signal of a suitable frequency range which is then applied to the phone line.
In some DSL variations (for example, HDSL), the terminal adapter connects directly to the computer via a serial interface, using protocols such as ethernet or V.35. In other cases (particularly ADSL), it is common for the customer equipment to be integrated with higher level functionality, such as routing, firewalling, or other application-specific hardware and software. In this case, the equipment is referred to as a "gateway".
Most DSL technologies require installation of appropriate filters to separate, or "split", the DSL signal from the low-frequency voice signal. The separation can take place either at the demarcation point, or with filters installed at the telephone outlets inside the customer premises. Either way has its practical and economical limitations.
Protocols and configurations.
Many DSL technologies implement an Asynchronous Transfer Mode (ATM) layer over the low-level bitstream layer to enable the adaptation of a number of different technologies over the same link.
DSL implementations may create bridged or routed networks. In a bridged configuration, the group of subscriber computers effectively connect into a single subnet. The earliest implementations used DHCP to provide network details such as the IP address to the subscriber equipment, with authentication via MAC address or an assigned host name. Later implementations often use Point-to-Point Protocol (PPP) or Asynchronous Transfer Mode (ATM) (Point-to-Point Protocol over Ethernet (PPPoE) or Point-to-Point Protocol over ATM (PPPoA)), while authenticating with a user ID and password and using Point-to-Point Protocol (PPP) mechanisms to provide network details..
Transmission methods.
Transmission methods vary by market, region, carrier, and equipment.
DSL technologies.
The line-length limitations from telephone exchange to subscriber impose more restrictions on higher data-transmission rates. Technologies such as VDSL provide very high-speed, short-range links as a method of delivering "triple play" services (typically implemented in fiber to the curb network architectures). Technologies like GDSL can further increase the data rate of DSL.
Fiber optic technologies exist today that allow the conversion of copper based ISDN, ADSL and DSL over fiber optics.
DSL technologies (sometimes summarized as xDSL) include:

</doc>
<doc id="41040" url="http://en.wikipedia.org/wiki?curid=41040" title="Digital transmission group">
Digital transmission group

In telecommunication, a digital transmission group is a group of digitized voice or data channels or both with bit streams that are combined into a single digital bit stream for transmission over communications media. 
Digital transmission groups usually are categorized by their maximum capacity, not by a specific number of channels. However, the maximum digital transmission group capacity must be equal to or greater than the sum of the individual multiplexer input channel capacities.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41043" url="http://en.wikipedia.org/wiki?curid=41043" title="Digroup">
Digroup

Digroup is an abbreviation for digital group. In telephony, a basic digital multiplexing group. 
In the North American and Japanese T-carrier digital hierarchies, each digroup supports 12 PCM voice channels or their equivalent in other services. The DS1 line rate (2 digroups plus overhead bits) is 1.544 Mbit/s, supporting 24 voice channels or their equivalent in other services. 
In the E-carrier European hierarchy, each digroup supports 15 PCM channels or their equivalent in other services. The DS1 line rate (2 digroups plus overhead bits) is 2.048 Mbit/s, supporting 30 voice channels or their equivalent in other services.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41044" url="http://en.wikipedia.org/wiki?curid=41044" title="Direct access">
Direct access

Direct access may refer to:

</doc>
<doc id="41045" url="http://en.wikipedia.org/wiki?curid=41045" title="Direct connect">
Direct connect

Direct connect may refer to:

</doc>
<doc id="41046" url="http://en.wikipedia.org/wiki?curid=41046" title="Direct distance dialing">
Direct distance dialing

Direct distance dialing (DDD) or direct dial is a telecommunications term for a network-provided service feature in which a call originator may, without operator assistance, call any other user outside the local calling area. DDD requires more digits in the number dialed than are required for calling within the local area or area code. DDD also extends beyond the boundaries of national public telephone network, in which case it is called international direct dialing or international direct distance dialing (IDDD).
DDD is a North American Numbering Plan term considered obsolete since completing a call in any manner other than direct dialing became rare. In the United Kingdom and other parts of the Commonwealth of Nations, the equivalent terms are or were "STD", for subscriber trunk dialing, and "ISD" for international subscriber trunk dialing.
History.
The telephone industry made a United States "first" in the New Jersey communities of Englewood and Teaneck with the introduction of what is known now as "direct distance dialing". Starting on November 10, 1951, customers of the ENglewood 3, ENglewood 4 and TEaneck 7 exchanges (who could already dial New York City and area) were able to dial 11 cities across the United States, simply by dialing the three-digit area code and the seven digit number (which at the time consisted of the first two letters of the exchange name and five digits).
The 11 cities and their area codes at that time were:
Many other cities could not yet be included as they did not yet have the necessary switching equipment to handle incoming calls automatically on their long-distance calling circuits. Other cities still had either a mixture of local number lengths or were all still six-digit numbers; Montreal and Toronto, Canada, for example, had a mix of six- and seven-digit numbers from 1951 to 1957, and did not have DDD until 1958. Whitehorse, Yukon, had seven-digit numbers from 1965, but the necessary switching equipment was not in place locally until 1972.
Hardware.
The Number 4 Crossbar, or 4XB switch, had been introduced in the 1940s to switch four-wire circuits and replace the incoming operator. With semiautomatic operation analogous to the early days of panel switch, the operator in the originating city used a multifrequency keypad to dial an access code to connect to the correct city and to send the seven digit number to incoming equipment at the terminating city. This design was further refined to serve DDD.
The card sorter of the 4A/CTS (Number 4A Crossbar / Card Translator System) allowed six digit translation of the central office code number dialed by the customer. This determined the proper trunk circuits to use, where separate circuit groups were used for different cities in the same area code, as in the case of Oakland and San Francisco. The new device used metal cards similar in principle to computer punched cards, and they were rapidly scanned as they fell past a light beam. On busy days, it sounded like a machine gun firing. CTS machines were called 4A (Advanced) if the translator was included in the original installation, and 4M (Modified) if it was added later. A 1970s version of 4XB, the 4A/ETS, used a computer to translate. For international dialing, Traffic Service Position System (TSPS) provided the extra computer power.
The reach of DDD was limited due to the inefficiency and expense of switching equipment, and the limited ability to process records of completed calls. One obstacle was that the majority of switching gear did not provide Automatic Number Identification (ANI). Common control switches such as 1XB switch were fairly quickly retrofitted to provide ANI, and most 5XB switches were initially installed with ANI services. Panel switch were eventually retrofitted, as were some step-by-step that were not scheduled for immediate replacement. Even if a switch had ANI, it could not identify callers on party lines. This was only partly overcome by Tip Party Identification. As the cost of subscriber line carrier declined, party lines were gradually phased out.
As this and other improved technologies became available, as well as Automatic Message Accounting (AMA) computers to process the long distance records into customer bills, the reach of DDD was slow in the 1950s, but quickened in the early 1960s. Electronic switching systems with stored-program capability allowed electronic processing of the dialed digits, referring to electronic memories to determine call routing, and this has reached the state of the art, with digital telephone exchanges which are basically specialized computers that route voice traffic from one "peripheral" to another as digital data. Call routing can now be done based on the area code, central office code and even the first two digits of the line number, although routing based on digits past the central office code is usually limited to cases of competitive local exchange carriers, number pooling and number portability.
IDDD.
In the 1960s, with the domestic conversion still underway, plans were laid to extend Direct Dialing beyond North America and its nearby islands. With so much new equipment already working that could only handle ten digit phone numbers, a system was devised by which most toll offices did not have to store and forward the whole international phone number. Gateway offices were set up in New York, London and Paris, connected to the ordinary automatic toll network. The New York gateway was at 32 Avenue of the Americas. The new LT1 5XB switch on the tenth floor of 435 West 50th Street received new Originating Registers and Outgoing Senders able to handle 15 digit telephone numbers, with appropriate modifications to Completing Markers and other equipment. Other 5XB in the next few years were installed with IDDD as original equipment, and in the 1970s ESS offices also provided the service.
The key to the new system was two-stage multi-frequency pulsing. The outgoing sender sent its Class 4 toll center an off-hook signal as usual, received a wink as usual as a "proceed to send" signal, and outpulsed only a special 3-digit (later 6-digit) access code. The toll center picked a trunk through the long distance network to the gateway office, which sent a second wink to the originating office, which then sent the whole dialed number. Thus the toll switching system needed no modification except at the gateway. The international trunks used Signaling System No. 5, a "North Atlantic" version of the North American multi-frequency signaling system, with minor modifications including slightly higher digit rate. European MF systems of the time used compelled signalling, which would slow down too much on a long transoceanic connection.
In the 1970s, toll centers were modified by adding TSPS. With these new computers in place, digit storage in the toll system was no longer a problem. End offices were less extensively modified, and sent all their digits in a single stream. TSPS handled the gateway codes and other complexities of toll connections to the gateway office.

</doc>
<doc id="41049" url="http://en.wikipedia.org/wiki?curid=41049" title="Direct-sequence spread spectrum">
Direct-sequence spread spectrum

In telecommunications, direct-sequence spread spectrum (DSSS) is a spread spectrum modulation technique. Spread spectrum systems are such that they transmit the message bearing signals using a bandwidth that is in excess of the bandwidth that is actually needed by the message signal. This spreading of the transmitted signal over a large bandwidth make the resulting wideband signal appear as a noise signal which allows greater resistance to intentional and unintentional interference with the transmitted signal.
One of the methods of achieving this spreading of the message signal is provided by DSSS modulation. In DSSS the message signal is used to modulate a bit sequence known as the Pseudo Noise (PN) code; this PN code consists of pulses of a much shorter duration (larger bandwidth) than the pulse duration of the message signal, therefore the modulation by the message signal has the effect of chopping up the pulses of the message signal and thereby resulting in a signal which has a bandwidth nearly as large as that of the PN sequence. In this context the duration of the pulse of the PN code is referred to as the chip duration and the smaller this value, the larger the bandwidth of the resultant DSSS signal and the more immune to interference the resultant signal becomes.
Some of the uses of DSSS include the Code Division Multiple Access (CDMA) channel access method and the IEEE 802.11b specification used in Wi-Fi networks.
Transmission method.
Direct-sequence spread-spectrum transmissions multiply the data being transmitted by a "noise" signal. This noise signal is a pseudorandom sequence of codice_1 and codice_2 values, at a frequency much higher than that of the original signal.
The resulting signal resembles white noise, like an audio recording of "static". However, this noise-like signal is used to exactly reconstruct the original data at the receiving end, by multiplying it by the same pseudorandom sequence (because 1 × 1 = 1, and −1 × −1 = 1). This process, known as "de-spreading", mathematically constitutes a correlation of the transmitted PN sequence with the PN sequence that the receiver already knows the transmitter is using.
The resulting effect of enhancing signal to noise ratio on the channel is called "process gain". This effect can be made larger by employing a longer PN sequence and more chips per bit, but physical devices used to generate the PN sequence impose practical limits on attainable processing gain.
If an undesired transmitter transmits on the same channel but with a different PN sequence (or no sequence at all), the de-spreading process has reduced processing gain for that signal. This effect is the basis for the code division multiple access (CDMA) property of DSSS, which allows multiple transmitters to share the same channel within the limits of the cross-correlation properties of their PN sequences.
As this description suggests, a plot of the transmitted waveform has a roughly bell-shaped envelope centered on the carrier frequency, just like a normal AM transmission, except that the added noise causes the distribution to be much wider than that of an AM transmission.
In contrast, frequency-hopping spread spectrum pseudo-randomly re-tunes the carrier, instead of adding pseudo-random noise to the data, the latter process results in a uniform frequency distribution whose width is determined by the output range of the pseudorandom number generator.

</doc>
<doc id="41050" url="http://en.wikipedia.org/wiki?curid=41050" title="Disengagement originator">
Disengagement originator

In telecommunication, a disengagement originator is the user or execution unit that initiates a disengagement attempt. The disengagement originator may be the originating user, the destination user, or the communications system. The communications system may deliberately originate the disengagement because of preemption, or inadvertently due to system malfunction.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41051" url="http://en.wikipedia.org/wiki?curid=41051" title="Dispersion-limited operation">
Dispersion-limited operation

A dispersion-limited operation is an operation of a communications link in which signal waveform degradation attributable to the dispersive effects of the communications medium is the dominant mechanism that limits link performance. The dispersion is the filter-like effect that a link has on the signal, due to the different propagation speeds of the eigenmodes of the link. Practically, this means that the waveform at the input will be different from the waveform at the output of the link.
Note that the amount of allowable degradation is dependent on the quality of the receiver. Note also that in fiber optic communications, "dispersion-limited operation" is often confused with "distortion-limited operation".

</doc>
<doc id="41052" url="http://en.wikipedia.org/wiki?curid=41052" title="Distortion">
Distortion

Distortion is the alteration of the original shape (or other characteristic) of something, such as an object, image, sound or waveform. Distortion is usually unwanted, and so engineers strive to eliminate distortion, or minimize it. In some situations, however, distortion may be desirable. The important signal processing operation of heterodyning is based on nonlinear mixing of signals to cause intermodulation. Distortion is also used as a musical effect, particularly with electric guitars.
The addition of noise or other outside signals (hum, interference) is not deemed distortion, though the effects of quantization distortion are sometimes deemed noise. A quality measure that explicitly reflects both the noise and the distortion is the Signal-to-noise-and-distortion (SINAD) ratio.
Electronic signals.
In telecommunication and signal processing, a noise-free system can be characterised by a transfer function, such that the output formula_1 can be written as a function of the input formula_2 as
When the transfer function comprises only a perfect gain constant "A" and perfect delay "T"
the output is undistorted. Distortion occurs when the transfer function "F" is more complicated than this. If "F" is a linear function, for instance a filter whose gain and/or delay varies with frequency, the signal suffers linear distortion. Linear distortion does not introduce new frequency components to a signal but does alter the balance of existing ones.
This diagram shows the behaviour of a signal (made up of a square wave followed by a sine wave) as it is passed through various distorting functions.
The transfer function of an ideal amplifier, with perfect gain and delay, is only an approximation. The true behavior of the system is usually different. Nonlinearities in the transfer function of an active device (such as vacuum tubes, transistors, and operational amplifiers) are a common source of non-linear distortion; in passive components (such as a coaxial cable or optical fiber), linear distortion can be caused by inhomogeneities, reflections, and so on in the propagation path.
Amplitude distortion.
Amplitude distortion is distortion occurring in a system, subsystem, or device when the output amplitude is not a linear function of the input amplitude under specified conditions.
Harmonic distortion.
Harmonic distortion adds overtones that are whole number multiples of a sound wave's frequencies. Nonlinearities that give rise to amplitude distortion in audio systems are most often measured in terms of the harmonics (overtones) added to a pure sinewave fed to the system. Harmonic distortion may be expressed in terms of the relative strength of individual components, in decibels, or the root mean square of all harmonic components: Total harmonic distortion (THD), as a percentage. The level at which harmonic distortion becomes audible depends on the exact nature of the distortion. Different types of distortion (like crossover distortion) are more audible than others (like soft clipping) even if the THD measurements are identical. Harmonic distortion in radio frequency applications is rarely expressed as THD.
Frequency response distortion.
Non-flat frequency response is a form of distortion that occurs when different frequencies are amplified by different amounts in a filter. For example, the non-uniform frequency response curve of AC-coupled cascade amplifier is an example of frequency distortion. In the audio case, this is mainly caused by room acoustics, poor loudspeakers and microphones, long loudspeaker cables in combination with frequency dependent loudspeaker impedance, etc.
Phase distortion.
This form of distortion mostly occurs due to electrical reactance. Here, all the components of the input signal are not amplified with the same phase shift, hence making some parts of the output signal out of phase with the rest of the output.
Group delay distortion.
Can be found only in dispersive media. In a waveguide, phase velocity varies with frequency. In a filter, group delay tends to peak near the cut-off frequency, resulting in pulse distortion. When analog long distance trunks were commonplace, for example in 12 channel carrier, group delay distortion had to be corrected in repeaters.
Correction of distortion.
As the system output is given by y(t) = F(x(t)), then if the inverse function F−1 can be found, and used intentionally to distort either the input or the output of the system, then the distortion is corrected.
An example of a similar correction is where LP/vinyl recordings or FM audio transmissions are deliberately pre-emphasised by a linear filter, the reproducing system applies an inverse filter to make the overall system undistorted.
Correction is not possible if the inverse does not exist—for instance if the transfer function has flat spots (the inverse would map multiple input points to a single output point). This produces an uncorrectable loss of information. Such a situation can occur when an amplifier is overdriven—causing clipping or slew rate distortion when, for a moment, the amplifier characteristics alone and not the input signal determine the output.
Cancellation of even-order harmonic distortion.
Many symmetrical electronic circuits reduce the magnitude of even harmonics generated by the non-linearities of the amplifier's components, by combining two signals from opposite halves of the circuit where distortion components that are roughly the same magnitude but out of phase. Examples include push-pull amplifiers and long-tailed pairs.
Teletypewriter or modem signaling.
In binary signaling such as FSK, distortion is the shifting of the significant instants of the signal pulses from their proper positions relative to the beginning of the start pulse. The magnitude of the distortion is expressed in percent of an ideal unit pulse length. This is sometimes called 'bias' distortion.
Telegraphic distortion is a similar older problem, distorting the ratio between "mark" and "space" intervals. 
Distortion in art.
In the art world, a distortion is any change made by an artist to the size, shape or visual character of a form to express an idea, convey a feeling or enhance visual impact. Often referred to as "abstraction," examples of distortion include "The Weeping Woman" by Picasso and "The Adoration of the Shepherds" by El Greco.
Audio distortion.
In this context, distortion refers to any kind of deformation of an output waveform compared to its input, usually clipping, harmonic distortion, or intermodulation distortion (mixing phenomena) caused by non-linear behavior of electronic components and power supply limitations. Terms for specific types of nonlinear audio distortion include: crossover distortion, slew-Induced Distortion (SID) and transient intermodulation (TIM).
Distortion in music is sometimes intentionally used as an effect, see also overdrive and distortion synthesis. Other forms of audio distortion that may be referred to are non-flat frequency response, compression, modulation, aliasing, quantization noise, wow and flutter from analog media such as vinyl records and magnetic tape. The human ear cannot hear phase distortion, except that it may affect the stereo imaging. (See also: Audio system measurements.)
In most fields, distortion is characterized as unwanted change to a signal.
Optics.
In optics, image/optical distortion is a divergence from rectilinear projection caused by a change in magnification with increasing distance from the optical axis of an optical system.
Map projections.
In cartography, a distortion is the misrepresentation of the area or shape of a feature. The Mercator projection, for example, distorts by exaggerating the size of regions at high latitude.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41053" url="http://en.wikipedia.org/wiki?curid=41053" title="Distortion-limited operation">
Distortion-limited operation

In telecommunication, distortion-limited operation is the condition prevailing when distortion of a received signal, rather than its attenuated amplitude (or power), limits performance under stated operational conditions and limits. 
"Note:" Distortion-limited operation is reached when the system distorts the shape of the waveform beyond specified limits. For linear systems, distortion-limited operation is equivalent to bandwidth-limited operation.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41054" url="http://en.wikipedia.org/wiki?curid=41054" title="Distributed database">
Distributed database

A distributed database is a database in which storage devices are not all attached to a common processing unit such as the CPU, controlled by a distributed database management system (together sometimes called a distributed database system). It may be stored in multiple computers, located in the same physical location; or may be dispersed over a network of interconnected computers. Unlike parallel systems, in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely-coupled sites that share no physical components.
System administrators can distribute collections of data (e.g. in a database) across multiple physical locations. A distributed database can reside on network servers on the Internet, on corporate intranets or extranets, or on other company networks. Because they store data across multiple computers, distributed databases can improve performance at end-user worksites by allowing transactions to be processed on many machines, instead of being limited to one.
Two processes ensures that the distributed databases remain up-to-date and current: replication and duplication.
Both replication and duplication can keep the data current in all distributive locations.
Besides distributed database replication and fragmentation, there are many other distributed database design technologies. For example, local autonomy, synchronous and asynchronous distributed database technologies. These technologies' implementations can and do depend on the needs of the business and the sensitivity/confidentiality of the data stored in the database, and hence the price the business is willing to spend on ensuring data security, consistency and integrity.
When discussing access to distributed databases, Microsoft favors the term distributed query, which it defines in protocol-specific manner as "[a]ny SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources".
Oracle provides a more language-centric view in which distributed queries and distributed transactions form part of distributed SQL.
Today the distributed DBMS market is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and NoSQL DBMS engines, as well as XML databases and NewSQL databases. These databases are increasingly supporting distributed database architecture that provides high availability and fault tolerance through replication and scale out ability. Some examples are Aerospike, Cassandra, Clusterpoint, ClustrixDB, Couchbase, Druid (open-source data store), FoundationDB, NuoDB, Riak and OrientDB.
Architecture.
A database user accesses the distributed database through:
A homogeneous distributed database has identical software and hardware running all databases instances, and may appear through a single interface as if it were a single database. A heterogeneous distributed database may have different hardware, operating systems, database management systems, and even data models for different databases.
Homogeneous Distributed Databases Management System.
In homogeneous distributed database all sites have identical software and are aware of each other and agree to cooperate in processing user requests. Each site surrenders part of its autonomy in terms of right to change schema or software. A homogeneous DDBMS appears to the user as a single system. The homogeneous system is much easier to design and manage. The following conditions must be satisfied for homogeneous database:
Heterogeneous DDBMS.
In a heterogeneous distributed database, different sites may use different schema and software. Difference in schema is a major problem for query processing and transaction processing. Sites may not be aware of each other and may provide only limited facilities for cooperation in transaction processing. In heterogeneous systems, different nodes may have different hardware & software and data structures at various nodes or locations are also incompatible. Different computers and operating systems, database applications or data models may be used at each of the locations. For example, one location may have the latest relational database management technology, while another location may store data using conventional files or old version of database management system. Similarly, one location may have the Windows NT operating system, while another may have UNIX. Heterogeneous systems are usually used when individual sites use their own hardware and software. On heterogeneous system, translations are required to allow communication between different sites (or DBMS). In this system, the users must be able to make requests in a database language at their local sites. Usually the SQL database language is used for this purpose. If the hardware is different, then the translation is straightforward, in which computer codes and word-length is changed. The heterogeneous system is often not technically or economically feasible. In this system, a user at one location may be able to read but not update the data at another location.
Important considerations.
Care with a distributed database must be taken to ensure the following:
There are two principal approaches to store a relation r in a distributed database system:
A) Replication: In replication, the system maintains several identical replicas of the same relation r in different sites.
B) Fragmentation: The relation r is fragmented into several relations r1, r2, r3...rn in such a way that the actual relation could be reconstructed from the fragments and then the fragments are scattered to different locations. There are basically two schemes of fragmentation:
A distributed database can be run by independent or even competing parties as, for example, in Bitcoin or Hasq.
Advantages.
The Merge Replication Method is popularly used to consolidate the data between databases.

</doc>
<doc id="41055" url="http://en.wikipedia.org/wiki?curid=41055" title="Distributed-queue dual-bus">
Distributed-queue dual-bus

In telecommunication, a distributed-queue dual-bus network (DQDB) is a distributed multi-access network that (a) supports integrated communications using a dual bus and distributed queuing, (b) provides access to local or metropolitan area networks, and (c) supports connectionless data transfer, connection-oriented data transfer, and isochronous communications, such as voice communications.
IEEE 802.6 is an example of a network providing DQDB access methods.
Concept of operation.
The DQDB Medium Access Control (MAC) algorithm is generally credited to Robert Newman who developed this algorithm in his PhD thesis in the 1980s at the University of Western Australia. To appreciate the innovative value of the DQDB MAC algorithm, it must be seen against the background of LAN protocols at that time, which were based on broadcast (such as ethernet IEEE 802.3) or a ring (like token ring IEEE 802.5 and FDDI). The DQDB may be thought of as two token rings, one carrying data in each direction around the ring. This improves reliability which is important in Metropolitan Area Networks (MAN), where repairs may take longer than in a LAN and Wi-Fi because the damage may be inaccessible.
The DQDB standard IEEE 802.6 was developed while ATM (Broadband ISDN) was still in early development, but there was strong interaction between the two standards. ATM cells and DQDB frames were harmonized. They both settled on essentially a 48-byte data frame with a 5-byte header. In the DQDB algorithm, a distributed queue was implemented by communicating queue state information via the header. Each node in a DQDB network maintains a pair of state variables which represent its position in the distributed queue and the size of the queue. The headers on the reverse bus communicated requests to be inserted in the distributed queue so that upstream nodes would know that they should allow DQDB cells to pass unused on the forward bus. The algorithm was remarkable for its extreme simplicity.
Currently DQDB systems are being installed by many carriers in entire cities, with lengths that reach up to 160 km with speeds of a DS3 line (44.736 Mbit/s). Other implementations use optical fiber for a length of up to 100 km and speeds around 150 Mbit/s
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41056" url="http://en.wikipedia.org/wiki?curid=41056" title="Distributed switching">
Distributed switching

Distributed switching is an architecture in which multiple processor-controlled switching units are distributed. There is often a hierarchy of switching elements, with a centralized host switch and with remote switches located close to concentrations of users.
Use in telephony networks.
Distributed switching is often used in telephone networks, though it is often called "host-remote switching".
In rural areas, population centers tend to be too small for economical deployment of a full-featured dedicated telephone exchange, and distances between these centers make transmission costs relatively high. Normal telephone traffic patterns show that most calling is done between people in a community of interest, in this case a geographical one: the population center. Use of distributed switching allows for the majority of calls that are local to that population center to be switched there without needing to be transported to and from the host switch.
The host switch provides connectivity between the remote switches and to the larger network, and the host may also directly handle some rare and complex call types (conference calling, for example) that the remote itself is not equipped to handle. Host switches also perform OAM&P (Operation, Administration, Maintenance, and Provisioning) functions, including billing, for the entire cluster of the host and its remote switches.
A key capability of a remote switch is the ability to act in emergency standalone (ESA) mode, wherein local calls can still be placed even in the event that the connection between that remote and the host has been lost. In this mode, only local calling is available anyway, so the billing capability of the host switch is not required. ESA is increasingly available on digital loop carrier platforms as well as on purpose-built remote switches in order to improve the scope of their utility.
Use within telecommunications equipment platforms.
Many data-centric telecommunications platforms such as routers and Ethernet switches utilize distributed switching on separate cards within an equipment chassis. Even when this is used, it is common to have a centralized switching fabric to interconnect the distributed switches. This architecture has become less common as backplane bus speeds and centralized switch fabric capacities have increased.

</doc>
<doc id="41057" url="http://en.wikipedia.org/wiki?curid=41057" title="Disturbance voltage">
Disturbance voltage

In telecommunication, a disturbance voltage is an unwanted voltage induced in a system by natural or man-made sources.
In telecommunications systems, the disturbance voltage creates currents that limit or interfere with the interchange of information. An example of a disturbance voltage is a voltage that produces (a) false signals in a telephone, (b) Noise (radio) in a radio receiver, or (c) distortion in a received signal.

</doc>
<doc id="41058" url="http://en.wikipedia.org/wiki?curid=41058" title="Diurnal phase shift">
Diurnal phase shift

In telecommunication, diurnal phase shift is the phase shift of electromagnetic signals associated with daily changes in the ionosphere. The major changes usually occur during the period of time when sunrise or sunset is present at critical points along the path. Significant phase shifts may occur on paths wherein a reflection area of the path is subject to a large tidal range. In cable systems, significant phase shifts can be occasioned by diurnal temperature variance.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41061" url="http://en.wikipedia.org/wiki?curid=41061" title="Department of Defense master clock">
Department of Defense master clock

The Department of Defense master clock is the master clock to which time and frequency measurements for the United States Department of Defense are referenced. 
The U.S. Naval Observatory master clock is designated as the DOD Master Clock. The U.S. Naval Observatory master clock is one of the two standard time and frequency references for the U.S. Government in accordance with Federal Standard 1002-A. The other standard time and frequency reference for the U.S. Government is the National Institute of Standards and Technology (NIST) master clock.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41062" url="http://en.wikipedia.org/wiki?curid=41062" title="Double-ended synchronization">
Double-ended synchronization

For two connected exchanges in a communications network, a double-ended synchronization (also called double-ended control) is a synchronization control scheme in which the phase error signals used to control the clock at one telephone exchange are derived by comparison with the phase of the incoming digital signal and the phase of the internal clocks at both exchanges.

</doc>
<doc id="41063" url="http://en.wikipedia.org/wiki?curid=41063" title="Double-sideband reduced-carrier transmission">
Double-sideband reduced-carrier transmission

Double-sideband reduced carrier transmission (DSB-RC): transmission in which (a) the frequencies produced by amplitude modulation are symmetrically spaced above and below the carrier and (b) the carrier level is reduced for transmission at a fixed level below that which is provided to the modulator. 
"Note:" In DSB-RC transmission, the carrier is usually transmitted at a level suitable for use as a reference by the receiver, except for the case in which it is reduced to the minimum practical level, i.e. the carrier is suppressed.

</doc>
<doc id="41064" url="http://en.wikipedia.org/wiki?curid=41064" title="Double-sideband suppressed-carrier transmission">
Double-sideband suppressed-carrier transmission

Double-sideband suppressed-carrier transmission (DSB-SC) is transmission in which frequencies produced by amplitude modulation (AM) are symmetrically spaced above and below the carrier frequency and the carrier level is reduced to the lowest practical level, ideally being completely suppressed. 
In the DSB-SC modulation, unlike in AM, the wave carrier is not transmitted; thus, much of the power is distributed between the sidebands, which implies an increase of the cover in DSB-SC, compared to AM, for the same power used.
DSB-SC transmission is a special case of double-sideband reduced carrier transmission. It is used for radio data systems.
Spectrum.
DSB-SC is basically an amplitude modulation wave without the carrier, therefore reducing power waste, giving it a 50% efficiency. This is an increase compared to normal AM transmission (DSB), which has a maximum efficiency of 33.333%, since 2/3 of the power is in the carrier which carries no intelligence, and each sideband carries the same information. Single Side Band (SSB) Suppressed Carrier is 100% efficient.
Spectrum plot of an DSB-SC signal:
Generation.
DSB-SC is generated by a mixer. This consists of a message signal multiplied by a carrier signal. The mathematical representation of this process is shown below, where the product-to-sum trigonometric identity is used.
formula_1
Demodulation.
Demodulation is done by multiplying the DSB-SC signal with the carrier signal just like the modulation process. This resultant signal is then passed through a low pass filter to produce a scaled version of original message signal. DSB-SC can be demodulated by a simple envelope detector, like AM, if the modulation index is less than unity. Full depth modulation requires carrier re-insertion.
The equation above shows that by multiplying the modulated signal by the carrier signal, the result is a scaled version of the original message signal plus a second term. Since formula_4, this second term is much higher in frequency than the original message. Once this signal passes through a low pass filter, the higher frequency component is removed, leaving just the original message.
Distortion and attenuation.
For demodulation, the demodulation oscillator's frequency and phase must be exactly the same as modulation oscillator's, otherwise, distortion and/or attenuation will occur. 
To see this effect, take the following conditions:
The resultant signal can then be given by
The formula_11 terms results in distortion and attenuation of the original message signal. In particular, if the frequencies are correct, but the phase is wrong, contribution from formula_12 is a constant attenuation factor, also formula_13 represents a cyclic inversion of the recovered signal, which is a serious form of distortion.
How it works.
This is best shown graphically. Below is a message signal that one may wish to modulate onto a carrier, consisting of a couple of sinusoidal components.
The equation for this message signal is formula_14.
The carrier, in this case, is a plain 5 kHz (formula_15) sinusoid—pictured below.
The modulation is performed by multiplication in the time domain, which yields a 5 kHz carrier signal, whose amplitude varies in the same manner as the message signal.
formula_16
The name "suppressed carrier" comes about because the carrier signal component is suppressed—it does not appear in the output signal. This is apparent when the spectrum of the output signal is viewed:
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41067" url="http://en.wikipedia.org/wiki?curid=41067" title="Drift">
Drift


</doc>
<doc id="41068" url="http://en.wikipedia.org/wiki?curid=41068" title="Drop (liquid)">
Drop (liquid)

A drop or droplet is a small column of liquid, bounded completely or almost completely by free surfaces. A drop may form when liquid accumulates at the lower end of a tube or other surface boundary, producing a hanging drop called a pendant drop. Drops may also be formed by the condensation of a vapor or by atomization of a larger mass of liquid.
Surface tension.
Liquid forms drops because the liquid exhibits surface tension.
A simple way to form a drop is to allow liquid to flow slowly from the lower end of a vertical tube of small diameter. The surface tension of the liquid causes the liquid to hang from the tube, forming a pendant. When the drop exceeds a certain size it is no longer stable and detaches itself. The falling liquid is also a drop held together by surface tension.
Pendant drop test.
In the pendant drop test, a drop of liquid is suspended from the end of a tube by surface tension. The force due to surface tension is proportional to the length of the boundary between the liquid and the tube, with the proportionality constant usually denoted formula_1. Since the length of this boundary is the circumference of the tube, the force due to surface tension is given by
where "d" is the tube diameter.
The mass "m" of the drop hanging from the end of the tube can be found by equating the force due to gravity (formula_3) with the component of the surface tension in the vertical direction (formula_4) giving the formula
where α is the angle of contact with the tube, and "g" is the acceleration due to gravity.
The limit of this formula, as α goes to 90°, gives the maximum weight of a pendant drop for a liquid with a given surface tension, formula_1.
This relationship is the basis of a convenient method of measuring surface tension, commonly used in the petroleum industry. More sophisticated methods are available when the surface tension is unknown that consider the developing shape of the pendant as the drop grows.
In medicine, droppers have a standardized diameter, in such a way that 1 millilitre is equivalent to 20 drops. And, for the cases when smaller amounts are necessary, microdroppers are used, in which, 1 millilitre = 60 microdrops.
Drop adhesion to a solid.
The drop adhesion to a solid can be divided to two categories: lateral adhesion and normal adhesion. Lateral adhesion resembles friction (though tribologically lateral adhesion is a more accurate term) and refers to the force required to slide a drop on the surface, namely the force to detach the drop from its position on the surface only to translate it to another position on the surface. Normal adhesion is the adhesion required to detach a drop from the surface in the normal direction, namely the force to cause the drop to fly off from the surface. The measurement of both adhesion forms can be done with the Centrifugal Adhesion Balance (CAB). The CAB uses a combination of centrifugal and gravitational forces to obtain any ratio of lateral and normal forces. For example it can apply a normal force at zero lateral force for the drop to fly off away from the surface in the normal direction or it can induce a lateral force at zero normal force (simulating zero gravity).
Droplet.
The term droplet is a diminutive form of 'drop' - and as a guide is typically used for liquid particles of less than 500 µm diameter. In spray application, droplets are usually described by their perceived size (i.e., diameter) whereas the dose (or number of infective particles in the case of biopesticides) is a function of their volume. This increases by a cubic function relative to diameter; thus a 50 µm droplet represents a dose in 65 pl and a 500 µm drop represents a dose in 65 nanolitres.
Speed.
A droplet with a diameter of 3 mm has a terminal velocity of approximately 8 m/s.
Drops smaller than 1 mm in diameter will attain 95% of their terminal velocity within 2 m. But above this size the distance to get to terminal velocity increases sharply. An example is a drop with a diameter of 2 mm that may achieve this at 5.6 m.
Optics.
Due to the different refractive index of water and air, refraction and reflection occur on the surfaces of raindrops, leading to rainbow formation.
Sound.
The major source of sound when a droplet hits a liquid surface is the resonance of excited bubbles trapped underwater. These oscillating bubbles are responsible for most liquid sounds, such as running water or splashes, as they actually consist of many drop-liquid collisions.
Shape.
The classic shape associated with a drop (with a pointy end in its upper side) comes from the observation of a droplet clinging to a surface. The shape of a drop falling through a gas is actually more or less spherical for drops less than 2mm in diameter. Larger drops tend to be flatter on the bottom part due to the pressure of the gas they move through. As a result, as drops get larger, a concave depression forms which leads to the eventual breakup of the drop.
Size.
Raindrop sizes typically range from 0.5mm to 4mm, with size distributions quickly decreasing past diameters larger than 2-2.5mm. 
Scientists traditionally thought that the variation in the size of raindrops was due to collisions on the way down to the ground. In 2009 French researchers succeeded in showing that the distribution of sizes is due to the drops' interaction with air, which deforms larger drops and causes them to fragment into smaller drops, effectively limiting the largest raindrops to about 6 mm diameter. However, drops up to 10mm are theoretically stable and could be levitated in a wind tunnel.
The largest recorded raindrop was 8.8mm in diameter, located at the base of a cumulus congestus cloud in the vicinity of Kwajalein Atoll in July 1999. An identical raindrop was detected over northern Brazil in September 1995.

</doc>
<doc id="41069" url="http://en.wikipedia.org/wiki?curid=41069" title="Drop and insert">
Drop and insert

In a multichannel transmission system, a drop and insert is a process that diverts (drops) a portion of the multiplexed aggregate signal at an intermediate point, and introduces (inserts) a different signal for subsequent transmission in the same position, "e.g.", time slot or frequency band, previously occupied by the diverted signal. 
The diverted signal may be demodulated or reinserted into another transmission system in the same or another time slot or frequency band. 
The time slot or frequency band vacated by the diverted signal need not necessarily be reoccupied by another signal. Likewise, a previously unoccupied time slot or frequency band may be occupied by a signal inserted at the drop-and-insert point.
Signals not of interest at the drop-and-insert point are not diverted.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41070" url="http://en.wikipedia.org/wiki?curid=41070" title="Dropout">
Dropout

Dropout may refer to:

</doc>
<doc id="41071" url="http://en.wikipedia.org/wiki?curid=41071" title="DTE">
DTE

DTE may refer to:

</doc>
<doc id="41072" url="http://en.wikipedia.org/wiki?curid=41072" title="Dual access">
Dual access

In telecommunication, the term dual access has the following meanings: 
Also, network hardware company D-Link has named technology which allows two simultaneous connections over one cable, for example 1) Internet and 2) provider's local FTP or game servers or IPTV data flow.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41073" url="http://en.wikipedia.org/wiki?curid=41073" title="Dual in-line package">
Dual in-line package

In microelectronics, a dual in-line package (DIP or DIL), or dual in-line pin package (DIPP) is an electronic component package with a rectangular housing and two parallel rows of electrical connecting pins. The package may be through-hole mounted to a printed circuit board or inserted in a socket. The dual-inline format was invented by Don Forbes, Rex Rice and Bryant Rogers at Fairchild R&D in 1964, when the restricted number of leads available on circular transistor-style packages became a limitation in the use of integrated circuits. Increasingly complex circuits required more signal and power supply leads (as observed in Rent's rule); eventually microprocessors and similar complex devices required more leads than could be put on a DIP package, leading to development of higher-density packages. Furthermore, square and rectangular packages made it easier to route printed-circuit traces beneath the packages.
A DIP is usually referred to as a DIP"n", where "n" is the total number of pins. For example, a microcircuit package with two rows of seven vertical leads would be a DIP14. The photograph at the upper right shows three DIP14 ICs. Common packages have as few as four and as many as 64 leads. Many analog and digital integrated circuit types are available in DIP packages, as are arrays of transistors, switches, light emitting diodes, and resistors. DIP plugs for ribbon cables can be used with standard IC sockets.
DIP packages are usually made from an opaque molded epoxy plastic pressed around a tin-, silver-, or gold-plated lead frame that supports the device die and provides connection pins. Some types of IC are made in ceramic DIP packages, where high temperature or high reliability is required, or where the device has an optical window to the interior of the package. Most DIP packages are secured to a printed circuit board by inserting the pins through holes in the board and soldering them in place. Where frequent replacement of the parts is desired, such as in test fixtures or where programmable devices must be removed for changes, a DIP socket is used. Some sockets include a zero insertion force mechanism.
Variations of the DIP package include those with only a single row of pins, possibly including a heat sink tab in place of the second row of pins, and types with four rows of pins, two rows, staggered, on each side of the package. DIP packages have been mostly displaced by surface-mount package types, which avoid the expense of drilling holes in a printed circuit board and which allow higher density of interconnections. 
Applications.
Types of devices.
DIPs are commonly used for integrated circuits (ICs). Other devices in DIP packages include resistor packs, DIP switches, LED segmented and bargraph displays, and electromechanical relays.
DIP connector plugs for ribbon cables are common in computers and other electronic equipment.
Dallas Semiconductor manufactured integrated DIP real-time clock (RTC) modules which contained an IC chip and a non-replaceable 10-year lithium battery.
DIP header blocks on to which discrete components could be soldered were used where groups of components needed to be easily removed, for configuration changes, optional features or calibration.
Uses.
The original dual-in-line package was invented by Bryant "Buck" Rogers in 1964 while working for Fairchild Semiconductor. The first devices had 14 pins and looked much like they do today. The rectangular shape allowed integrated circuits to be packaged more densely than previous round packages. The package was well-suited to automated assembly equipment; a printed circuit board could be populated with scores or hundreds of ICs, then all the components on the circuit board could be soldered at one time on a wave soldering machine and passed on to automated testing machines, with very little human labor required. DIP packages were still large with respect to the integrated circuits within them. By the end of the 20th century, surface-mount packages allowed further reduction in the size and weight of systems. DIP chips are still popular for circuit prototyping on a breadboard because of how easily they can be inserted and utilized there.
DIPs were the mainstream of the microelectronics industry in the 1970s and 80s. Their use has declined in the first decade of the 21st century due to the emerging new surface-mount technology (SMT) packages such as plastic leaded chip carrier (PLCC) and small-outline integrated circuit (SOIC), though DIPs continued in extensive use through the 1990s, and still continue to be used substantially as the year 2011 passes. Because some modern chips are available only in surface-mount package types, a number of companies sell various prototyping adapters to allow those SMT devices to be used like DIP devices with through-hole breadboards and soldered prototyping boards (such as stripboard and perfboard). (SMT can pose quite a problem, at least an inconvenience, for prototyping in general; most of the characteristics of SMT that are advantages for mass production are difficulties for prototyping.)
For programmable devices like EPROMs and GALs, DIPs remained popular for many years due to their easy handling with external programming circuitry (i.e., the DIP devices could be simply plugged into a socket on the programming device.) However, with In-System Programming (ISP) technology now state of the art, this advantage of DIPs is rapidly losing importance as well.
Through the 1990s, devices with fewer than 20 leads were manufactured in a DIP format in addition to the newer formats. Since about 2000, newer devices are often unavailable in the DIP format.
Mounting.
DIPs can be mounted either by through-hole soldering or in sockets. Sockets allow easy replacement of a device and eliminates the risk of damage from overheating during soldering. Generally sockets were used for high-value or large ICs, which cost much more than the socket. Where devices would be frequently inserted and removed, such as in test equipment or EPROM programmers, a zero insertion force socket would be used.
DIPs are also used with breadboards, a temporary mounting arrangement for education, design development or device testing. Some hobbyists, for one-off construction or permanent prototyping, use point-to-point wiring with DIPs, and their appearance when physically inverted as part of this method inspires the informal term "dead bug style" for the method.
Construction.
The body (housing) of a DIP containing an IC chip is usually made from molded plastic or ceramic. The hermetic nature of a ceramic housing is preferred for extremely high reliability devices. However, the vast majority of DIPs are manufactured via a thermoset molding process in which an epoxy mold compound is heated and transferred under pressure to encapsulate the device. Typical cure cycles for the resins are less than 2 minutes and a single cycle may produce hundreds of devices.
The leads emerge from the longer sides of the package along the seam, parallel to the top and bottom planes of the package, and are bent downward approximately 90 degrees (or slightly less, leaving them angled slightly outward from the centerline of the package body.) (The SOIC, the SMT package that most resembles a typical DIP, appears essentially the same, notwithstanding size scale, except that after being bent down the leads are bent upward again by an equal angle to become parallel with the bottom plane of the package.) In ceramic (CERDIP) packages, an epoxy or grout is used to hermetically seal the two halves together, providing an air and moisture tight seal to protect the IC die inside. Plastic DIP (PDIP) packages are usually sealed by fusing or cementing the plastic halves around the leads, but a high degree of hermeticity is not achieved because the plastic itself is usually somewhat porous to moisture and the process cannot ensure a good microscopic seal between the leads and the plastic at all points around the perimeter. However, contaminants are usually still kept out well enough that the device can operate reliably for decades with reasonable care in a controlled environment.
Inside the package, the lower half has the leads embedded, and at the center of the package is a rectangular space, chamber, or void into which the IC die is cemented. The leads of the package extend diagonally inside the package from their positions of emergence along the periphery to points along a rectangular perimeter surrounding the die, tapering as they go to become fine contacts at the die. Ultra-fine bond wires (barely visible to the naked human eye) are welded between these die periphery contacts and bond pads on the die itself, connecting one lead to each bond pad, and making the final connection between the microcircuits and the external DIP leads. The bond wires are not usually taut but loop upward slightly to allow slack for thermal expansion and contraction of the materials; if a single bond wire breaks or detaches, the entire IC may become useless. The top of the package covers all of this delicate assemblage without crushing the bond wires, protecting it from contamination by foreign materials.
Usually, a company logo, alphanumeric codes and sometimes words are printed on top of the package to identify its manufacturer and type, when it was made (usually as a year and a week number), sometimes where it was made, and other proprietary information (perhaps revision numbers, manufacturing plant codes, or stepping ID codes.)
The necessity of laying out all of the leads in a basically radial pattern in a single plane from the die perimeter to two rows on the periphery of the package is the main reason that DIP packages with higher lead counts must have wider spacing between the lead rows, and it effectively limits the number of leads which a practical DIP package may have. Even for a very small die with many bond pads (e.g. a chip with 15 inverters, requiring 32 leads), a wider DIP would still be required to accommodate the radiating leads internally. This is one of the reasons that four-sided and multiple rowed packages, such as PGAs, were introduced (around the early 1980s.)
A large DIP package (such as the DIP64 used for the Motorola 68000 CPU) has long leads inside the package between pins and the die, making such a package unsuitable for high speed devices.
Some other types of DIP devices are built very differently. Most of these have molded plastic housings and straight leads or leads that extend directly out of the bottom of the package. For some, LED displays particularly, the housing is usually a hollow plastic box with the bottom/back open, filled (around the contained electronic components) with a hard translucent epoxy material from which the leads emerge. Others, such as DIP switches, are composed of two (or more) plastic housing parts snapped, welded, or glued together around a set of contacts and tiny mechanical parts, with the leads emerging through molded-in holes or notches in the plastic.
Variants.
Several DIP variants for ICs exist, mostly distinguished by packaging material:
EPROMs were sold in ceramic DIPs manufactured with a circular window of clear quartz over the chip die to allow the part to be erased by ultraviolet light. Often, the same chips were also sold in less expensive windowless PDIP or CERDIP packages as one-time programmable (OTP) versions. Windowed and windowless packages were also used for microcontrollers, and other devices, containing EPROM memory. Windowed CERDIP-packaged EPROMs were used for the BIOS ROM of many early IBM PC clones with an adhesive label covering the window to prevent inadvertent erasure through exposure to ambient light.
Molded plastic DIPs are much lower in cost than ceramic packages; one 1979 study showed that a plastic 14 pin DIP cost around US 6. 3 cents, and a ceramic package cost 82 cents.
Single in-line.
A single in-line (pin) package (SIP or SIPP) has one row of connecting pins. It is not as popular as the DIP, but has been used for packaging RAM chips and multiple resistors with a common pin. SIPs group RAM chips together on a small board either by the DIP process or surface mounting SMD process. The board itself has a single row of pin-leads that resembles a comb extending from its bottom edge, which plug into a special socket on a system or system-expansion board. SIPs are commonly found in memory modules. As compared to DIPs with a typical maximum I/O count of 64, SIPs have a typical maximum I/O count of 24 with lower package costs.
One variant of the single in-line package uses part of the lead frame for a heat sink tab. This multi-leaded power package is useful for such applications as audio power amplifiers, for example.
Quad in-line.
Rockwell used a quad in-line package with 42 leads formed into staggered rows for their PPS-4 microprocessor family introduced in 1973,
and other microprocessors and microcontrollers, some with higher lead counts, through the early 1990s.
The QIP, sometimes called a QIL package, has the same dimensions as a DIL package, but the leads on each side are bent into an alternating zigzag configuration so as to fit 4 lines of solder pads (instead of 2 with a DIL). The QIL design increased the spacing between solder pads without increasing package size, for two reasons:
Some QIL packaged ICs had added heatsinking tabs, such as the 
Intel and 3M developed the ceramic leadless quad in-line package (QUIP), introduced in 1979, to boost microprocessor density and economy. The ceramic leadless QUIP is not designed for surface-mount use, and requires a socket. It was used by Intel for the iAPX 432 microprocessor chip set, and by Zilog for the Z8-02 external-ROM prototyping version of the Z8 microcontroller.
Lead count and spacing.
Commonly found DIP packages that conform to JEDEC standards use an inter-lead spacing (lead pitch) of 0.1 inch (2.54 mm). Row spacing varies depending on lead counts, with 0.3 in. (7.62 mm)(JEDEC MS001) or 0.6 inch (15.24 mm)(JEDEC MS-010) the most common. Less common standardized row spacings include 0.4 inch (10.16 mm) and 0.9 inch (22.86 mm), as well as a row spacing of 0.3 inch, 0.6 inch or 0.75 inch with a 0.07 inch (1.778 mm) lead pitch.
The former Soviet Union and Eastern bloc countries used similar packages, but with a metric inter-lead spacing of 2.5 mm rather than 2.54 mm (0.1 inch).
The number of leads is always even. For 0.3 inch spacing, typical lead counts are 8 to 24; less common are 4 or 28 lead counts. For 0.6 inch spacing, typical lead counts are 24, 28, 32 or 40; less common are 36, 48 or 52 lead counts. Some microprocessors, such as the Motorola 68000 and Zilog Z180, used lead counts as high as 64; this is typically the maximum number of leads for a DIP package.
Orientation and lead numbering.
As shown in the diagram, leads are numbered consecutively from Pin 1. When the identifying notch in the package is at the top, Pin 1 is the top left corner of the device. Sometimes Pin 1 is identified with an indent or paint dot mark.
For example, for a 14-lead DIP, with the notch at the top, the left leads are numbered from 1 to 7 (top to bottom) and the right row of leads are numbered 8 to 14 (bottom to top).
Some DIP devices, such as segmented LED displays, or those that replace leads with a heat sink fin, skip some leads; the remaining leads are numbered as if all positions had leads.
In addition to providing for human visual identification of the orientation of the package, the notch allows automated chip-insertion machinery to confirm correct orientation of the chip by mechanical sensing.
Descendants.
The SOIC (Small Outline IC), a surface-mount package which is currently very popular, particularly in consumer electronics and personal computers, is essentially a shrunk version of the standard IC PDIP, the fundamental difference which makes it an SMT device being a second bend in the leads to flatten them parallel to the bottom plane of the plastic housing. The SOJ (Small Outline J-lead) and other SMT packages with "SOP" (for "Small Outline Package") in their names can be considered further relatives of the DIP, their original ancestor. SOIC packages tend to have half the pitch of DIP, and SOP are half that, a fourth of DIP. (0.1"/2.54mm, 0.05"/1.27mm, and 0.025"/0.635mm, respectively)
Pin grid array (PGA) packages may be considered to have evolved from the DIP. PGAs with the same 0.1 inch pin centers as most DIPs were popular for microprocessors from the early to mid-1980s through the 1990s. Owners of personal computers containing Intel 80286 through P5 Pentium processors may be most familiar with these PGA packages, which were often inserted into ZIF sockets on motherboards. The similarity is such that a PGA socket may be physically compatible with some DIP devices, though the converse is rarely true.
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41075" url="http://en.wikipedia.org/wiki?curid=41075" title="Duct">
Duct

A duct may refer to:

</doc>
<doc id="41077" url="http://en.wikipedia.org/wiki?curid=41077" title="Duplexer">
Duplexer

A duplexer is an electronic device that allows bi-directional (duplex) communication over a single path. In radar and radio communications systems, it isolates the receiver from the transmitter while permitting them to share a common antenna. Most radio repeater systems include a duplexer. Duplexers can be based on frequency (often a waveguide filter), polarization (such as an orthomode transducer, or timing (as is typical in radar).
Types.
Transmit-receive switch.
In radar, a transmit-receive (TR) switch alternately connects the transmitter and receiver to a shared antenna. In the simplest arrangement, the switch consists of a gas-discharge tube between the input terminals of the receiver. When the transmitter is active, the resulting high voltage causes the tube to conduct, shorting together the receiver terminals.
Frequency domain.
In radio communications (as opposed to radar), the transmitted and received signals can occupy different frequency bands, and so may be separated by frequency-selective filters. See diplexer.
"Note 1:" A duplexer must be designed for operation in the frequency band used by the receiver and transmitter, and must be capable of handling the output power of the transmitter.
"Note 2:" A duplexer must provide adequate rejection of transmitter noise occurring at the receive frequency, and must be designed to operate at, or less than, the frequency separation between the transmitter and receiver.
"Note 3:" A duplexer must provide sufficient isolation to prevent receiver desensitization.
Source: from Federal Standard 1037C

</doc>
<doc id="41078" url="http://en.wikipedia.org/wiki?curid=41078" title="Duty cycle">
Duty cycle

A duty cycle is the percentage of one period in which a signal is active. A period is the time it takes for a signal to complete an on-and-off cycle. As a formula, a duty cycle may be expressed as:
where formula_2 is the duty cycle, formula_3 is the time the signal is active, and formula_4 is the total period of the signal. Thus, a 60% duty cycle means the signal is on 60% of the time but off 40% of the time. The "on time" for a 60% duty cycle could be a fraction of a second, a day, or even a week, depending on the length of the period.
Duty cycles can be used to describe the percent time of an active signal in an electrical device such as the power switch in a switching power supply or the firing of action potentials by a living system such as a neuron.
Applications.
Electrical devices.
Electrical motors typically use less than a 100% duty cycle. For example, if a motor runs for one out of 100 seconds, or 1/100 of the time, then, its duty cycle is 1/100, or 1 percent.
Pulse-width modulation (PWM) is used in a variety of electronic situations, such as power delivery and voltage regulation. 
In electronic music, music synthesizers vary the duty cycle of their audio-frequency oscillators to obtain a subtle effect on the tone colors. This technique is known as pulse-width modulation.
In the printer / copier industry, the duty cycle specification refers to the rated throughput (that is, printed pages) of a device per month.
In a welding power supply, the maximum duty cycle is defined as the percentage of time in a 10 minute period that it can be operated continuously before overheating.
Biological systems.
The concept of duty cycles is also used to describe the activity of neurons and muscle fibers. In a neural network for example, a duty cycle specifically refers to the proportion of a cycle period in which a neuron remains active.

</doc>
<doc id="41079" url="http://en.wikipedia.org/wiki?curid=41079" title="Dynamic range">
Dynamic range

Dynamic range, abbreviated DR or DNR, is the ratio between the largest and smallest possible values of a changeable quantity, such as in signals like sound and light. It is measured as a ratio, or as a base-10 (decibel) or base-2 (doublings, bits or stops) logarithmic value.
Dynamic range and human perception.
The human senses of sight and hearing have a very high dynamic range. A human is capable of hearing (and usefully discerning) anything from a quiet murmur in a soundproofed room to the sound of the loudest heavy metal concert. Such a difference can exceed 100 dB which represents a factor of 100,000 in amplitude and a factor 10,000,000,000 in power. A human can see objects in starlight (although colour differentiation is reduced at low light levels) or in bright sunlight, even though on a moonless night objects receive 1/1,000,000,000 of the illumination they would on a bright sunny day: that is a dynamic range of 90 dB.
A human cannot perform these feats of perception at both extremes of the scale at the same time. The eyes take time to adjust to different light levels, and the dynamic range of the human eye in a given scene is actually quite limited due to optical glare. The instantaneous dynamic range of human audio perception is similarly subject to masking so that, for example, a whisper cannot be heard in loud surroundings.
In practice, it is difficult to achieve the full dynamic range experienced by humans using electronic equipment. Electronically reproduced audio and video often uses some trickery to fit original material with a wide dynamic range into a narrower recorded dynamic range that can more easily be stored and reproduced; these techniques are called dynamic range compression. For example, a good quality LCD display has a dynamic range of around 1000:1 (commercially the dynamic range is often called the "contrast ratio" meaning the full-on/full-off luminance ratio), and some of the latest CMOS image sensors now have measured dynamic ranges of about 11,000:1 (reported as 13.5 stops, or doublings, equivalent to binary bits). Paper reflectance can achieve a dynamic range of about 100:1. A professional ENG camcorder such as the Sony Digital Betacam achieves a dynamic range of greater than 90dB in audio recording.
When showing a movie or a game, a display is able to show both shadowy nighttime scenes and bright outdoor sunlit scenes, but in fact the level of light coming from the display is much the same for both types of scene (perhaps different by a factor of 10). Knowing that the display does not have a huge dynamic range, the program makers do not attempt to make the nighttime scenes millions of times less bright than the daytime scenes, but instead use other cues to suggest night or day. A nighttime scene will usually contain duller colours and will often be lit with blue lighting, which reflects the way that the human eye sees colours at low light levels.
Examples of usage.
Audio.
Audio engineers often use "dynamic range" to describe the ratio of the amplitude of the loudest possible undistorted sine wave to the root mean square (rms) noise amplitude, say of a microphone or loudspeaker.
The dynamic range of human hearing is roughly 140 dB. The dynamic range of music as normally perceived in a concert hall doesn't exceed 80 dB, and human speech is normally perceived over a range of about 40 dB.
The dynamic range differs from the ratio of the maximum to minimum amplitude a given device can record, as a properly dithered recording device can record signals well below the rms noise amplitude (noise floor).
For example, if the ceiling of a device is 5V (rms) and the noise floor is 10µV (rms) then the dynamic range is 500000:1, or 114 dB:
In digital audio theory the dynamic range is limited by quantization error. The maximum achievable dynamic range for a digital audio system with "Q"-bit uniform quantization is calculated as the ratio of the largest sine-wave rms to rms noise is:
The maximum achievable signal-to-noise ratio (SNR) for a digital audio system with "Q"-bit uniform quantization is
The 16-bit compact disc has a theoretical dynamic range of about 96 dB for a triangle wave or 98 dB for sinusoidal signals (see Quantization noise model). The "perceived" dynamic range of 16-bit audio can be as high as 120 dB with noise-shaped dither, taking advantage of the frequency response of the human ear. Digital audio with undithered 20-bit digitization is also theoretically capable of 120 dB dynamic range. Similarly, 24-bit digital audio calculates to 144 dB dynamic range. All digital audio recording and playback chains include input and output converters and associated analog circuitry, significantly limiting practical dynamic range. Observed 16-bit digital audio dynamic range is about 90 dB.
Dynamic range in analog audio is the difference between low-level thermal noise in the electronic circuitry and high-level signal saturation resulting in increased distortion and, if pushed higher, clipping. Multiple noise processes determine the noise floor of a system. Noise can be picked up from microphone self-noise, preamp noise, wiring and interconnection noise, media noise, etc.
Early 78 rpm phonograph discs had a dynamic range of up to 40 dB, soon reduced to 30 dB and worse due to wear from repeated play. Vinyl microgroove phonograph records typically yield 55-65 dB, though the first play of the higher-fidelity outer rings can achieve a dynamic range of 70 dB.
German magnetic tape in 1941 was reported to have had a dynamic range of 60 dB, though modern day restoration experts of such tapes note 45-50 dB as the observed dynamic range. Ampex tape recorders in the 1950s achieved 60 dB in practical usage, though tape formulations such as Scotch 111 boasted 68 dB dynamic range. In the 1960s, improvements in tape formulation processes resulted in 7 dB greater range, and Ray Dolby developed the Dolby A-Type noise reduction system that increased low- and mid-frequency dynamic range on magnetic tape by 10 dB, and high-frequency by 15 dB, using companding (compression and expansion) of four frequency bands. The peak of professional analog magnetic recording tape technology reached 90 dB dynamic range in the midband frequencies at 3% distortion, or about 80 dB in practical broadband applications. The Dolby SR noise reduction system gave a 20 dB further increased range resulting in 110 dB in the midband frequencies at 3% distortion. Compact Cassette tape performance ranges from 50 to 56 dB depending on tape formulation, with Metal Type IV tapes giving the greatest dynamic range, and systems such as XDR, dbx and Dolby noise reduction system increasing it further. Specialized bias and record head improvements by Nakamichi and Tandberg combined with Dolby C noise reduction yielded 72 dB dynamic range for the cassette.
The rugged elements of moving-coil microphones can have a dynamic range of up to 140 dB (at increased distortion), while condenser microphones are limited by the overloading of their associated electronic circuitry. Practical considerations of acceptable distortion levels in microphones combined with typical practices in a recording studio result in a useful operating range of 125 dB.
In 1981, researchers at Ampex determined that a dynamic range of 118 dB on a dithered digital audio stream was necessary for subjective noise-free playback of music in quiet listening environments.
Since the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society, that measurements of dynamic range be made with an audio signal present, which is then filtered out to get the noise floor. This avoids questionable measurements based on the use of blank media, or muting circuits.
Electronics.
Electronics engineers apply the term to: 
Metrology.
In metrology, such as when performed in support of science, engineering or manufacturing objectives, dynamic range refers to the range of values that can be measured by a sensor or metrology instrument. Often this dynamic range of measurement is limited at one end of the range by saturation of a sensing signal sensor or by physical limits that exist on the motion or other response capability of a mechanical indicator. The other end of the dynamic range of measurement is often limited by one or more sources of random noise or uncertainty in signal levels that may be described as the defining the sensitivity of the sensor or metrology device. When digital sensors or sensor signal converters are a component of the sensor or metrology device, the dynamic range of measurement will be also related to the number of binary digits (bits) used in a digital numeric representation in which the measured value is linearly related to the digital number. For example, a 12-bit digital sensor or converter can provide a dynamic range in which the ratio of the maximum measured value to the minimum measured value is up to 212 = 4096. With gamma correction, this limitation can be relaxed somewhat; for example, the 8-bit encoding used in sRGB image encoding represents a maximum to minimum ratio of about 3000.
Metrology systems and devices may use several basic methods to increase their basic dynamic range. These methods include averaging and other forms of filtering, repetition of measurements, nonlinear transformations to avoid saturation, etc. In more advance forms of metrology, such as multiwavelength digital holography, interferometry measurements made at different scales (different wavelengths) can be combined to retain the same low-end resolution while extending the upper end of the dynamic range of measurement by orders of magnitude.
Music.
In music, "dynamic range" is the difference between the quietest and loudest volume of an instrument, part or piece of music. In modern recording, this range is often limited through dynamic range compression, which allows for louder volume, but can make the recording sound less exciting or live.
The term "dynamic range" may be confusing in music because it has two conflicting definitions, particularly in the understanding of the loudness war phenomenon. "Dynamic range" may refer to micro-dynamics, related to crest factor, whereas the European Broadcasting Union, in EBU3342 Loudness Range, defines "dynamic range" as the difference between the quietest and loudest volume, a matter of macro-dynamics.
Photography.
Photographers use "dynamic range" for the luminance range of a scene being photographed, or the limits of luminance range that a given digital camera or film can capture,
 or the "opacity range" of developed film images, or the "reflectance range" of images on photographic papers.
Graduated neutral density filters are used to decrease the dynamic range of scene luminance that can be captured on photographic film (or on the image sensor of a digital camera): The filter is positioned in front of the lens at the time the exposure is made; the top half is dark and the bottom half is clear. The dark area is placed over a scene's high-intensity region, such as the sky. The result is more even exposure in the focal plane, with increased detail in the shadows and low-light areas. Though this doesn't increase the fixed dynamic range available at the film or sensor, it stretches usable dynamic range in practice.
The dynamic range of sensors used in digital photography is many times less than that of the human eye and generally not as wide as that of chemical photographic media. In the domain of digital imaging, algorithms have been developed to map the image differently in shadow and in highlight in order to better distribute the lighting range across the image. These techniques are known as local tone mapping, and usually involves overcoming the limited dynamic range of the sensor array by selectively combining multiple exposures of the same scene in order to retain detail in light and dark areas. The same approach has been used in chemical photography to capture an extremely wide dynamic range: A three-layer film with each underlying layer at 1/100 the sensitivity of the next higher one has, for example, been used to record nuclear-weapons tests.
The most severe dynamic-range limitation in photography may not involve encoding, but rather reproduction to, say, a paper print or computer screen. In that case, not only local tone mapping, but also "dynamic range adjustment" can be effective in revealing detail throughout light and dark areas: The principle is the same as that of dodging and burning (using different lengths of exposures in different areas when making a photographic print) in the chemical darkroom. The principle is also similar to gain riding or automatic level control in audio work, which serves to keep a signal audible in a noisy listening environment and to avoid peak levels which overload the reproducing equipment, or which are unnaturally or uncomfortably loud.

</doc>
<doc id="41081" url="http://en.wikipedia.org/wiki?curid=41081" title="Echo (mythology)">
Echo (mythology)

In Greek mythology, Echo (; Greek: Ἠχώ, "Ēkhō", "echo", from ἦχος ("ēchos"), "sound") was an Oread who resided on Mount Kithairon. Zeus loved consorting with beautiful nymphs and visited them on Earth often. Eventually, Zeus's wife, Hera, became suspicious, and came from Mt. Olympus in an attempt to catch Zeus with the nymphs.
Classical Depiction.
Metamorphoses.
In Metamorphoses the poet, Ovid, tells of Juno and the jealousy she felt towards her husband Jupiter for his many affairs. Though vigilant whenever she was about to catch him Echo distracted her with lengthy conversations. When at last Juno realized the truth she cursed Echo. From that moment on the once loquacious nymph could only repeat the most recently spoken words of another person.
Sometime after being cursed Echo spied a young man, Narcissus, whilst he was out hunting deer with his companions. She immediately fell for him and, infatuated, followed quietly. The more she looked at the young man the more she longed for him. Though she wished with all her heart to call out to Narcissus, Juno's curse prevented her.
During the hunt Narcissus became separated from his companions and called out, ‘is anyone there’ only for Echo to repeat his words. Startled Narcissus answered the voice, ‘come here’ only to be told the same. When Narcissus saw that nobody had emerged from the glade he concluded that the owner of the voice must be running away from him and called out again. Finally he shouted, ‘this way, we must come together.’ Taking this to be a reciprocation of her love Echo concurred ecstatically, ‘we must come together!’
In her delight Echo rushed to Narcissus ready to throw her arms around her beloved. Narcissus however was appalled and, spurning her, exclaimed, ‘Hands off! May I die before you enjoy my body.’ All Echo could whisper in reply was, ‘enjoy my body’ and having done so she fled, scorned, humiliated and shamed.
Despite the harshness of her rejection Echo’s love for Narcissus only grew. When Narcissus died, wasting away before his own reflection - consumed by a love that could not be, Echo mourned over his body. When Narcissus, looking one last time into the pool uttered, ‘oh marvellous boy, I loved you in vain, farewell’ Echo too chorused, ‘farewell.’
Eventually Echo too began to waste away. Her beauty faded, her skin shrivelled and her bones turned to stone. Today all that remains of Echo is the sound of her voice.
Daphnis and Chloe.
The tale of Daphnis and Chloe is a 2nd-century romance by Greek author Longus. At one point in the novel Daphnis and Chloe are staring out at the boats gliding across the sea. Chloe, having never heard an echo before, is confused on hearing the fisherman’s song repeated in a nearby valley. Daphnis promises to tell her the story of Echo in exchange for ten more kisses.
Daphnis’ rendition differs radically from Ovid’s account. According to Daphnis, Echo was raised among the Nymphæ because her mother was a nymph. Her father however was merely a man and hence Echo was not herself a nymph but mortal. Echo spent her days dancing with the Nymphæ and singing with the Muses who taught her all manner of musical instruments. Pan however grew angry with her, envious of her musical virtuosity and covetous of her virginity which she would yield neither to men nor gods. Pan drove the men of the fields mad and, like wild animals, they tore Echo apart and scattered the still singing fragments of her body across the earth.
Showing favour to the Nymphæ Gaia hid the shreds of Echo within herself providing shelter for her music and, at the Muses’ command, Echo’s body will still sing, imitating with perfect likeness the sound of any earthly thing. Daphnis recounts that Pan himself oft hears his very own pipes and, giving chase across the mountains, looks in vain for the secret student he can never find.
Other.
Both the Homeric and Orphic Hymns to Pan reiterate Longus’ tale of Pan chasing Echo’s secret voice across the mountains.
Codex 190 of Photius' Bibliotheca states that Pan's unrequited love for Echo was placed there by Aphrodite, angry at his verdict in a beauty contest.
Nonnus’ Dionysiaca contains a number of references to Echo. In Nonnus’ account, though Pan frequently chased Echo, he never won her affection. Book VI also makes reference to Echo in the context of the Great Deluge. Nonnus states that the waters rose so far that even high on the hills Echo was forced to swim. Having escaped the advances of Pan she feared now the lust of Poseidon.
Whereas Nonnus is adamant that Pan never wins Echo, in Apuleius' The Golden Ass Pan is described with Echo in his arms, teaching the nymph to repeat all manner of songs. Similarly in the Suda Echo is described as bearing Pan a child, Inyx. Other fragments mention a second daughter, Iambe.
Medieval Depiction.
The Lay of Narcissus.
The Lay of Narcissus, one of many titles by which the work is known, is Norman-French verse narrative written towards the end of the 12th century. In the four manuscripts which remain an unknown author borrows from the Echo and Narcissus of Ovid to create a story better suited to the needs of his time.
In this medieval account the characters of both Echo and Narcissus are altered. In Ovid’s account Echo is a beautiful nymph residing with the Muses and Narcissus a haughty prince. In the Lay of Narcissus Echo is replaced by the princess Dané. Conversely Narcissus loses the royal status he bore in Ovid account. In this rendition he is no more than a commoner, a vassal of Dané’s father the King.
In the lay, Dané is pierced by the arrows of Amor and falls madly in love with Narcissus. Though aware that she should first consult her father Dané nonetheless shares her feelings with Narcissus. Despite emphasising her royal lineage Narcissus spurns her just as he spurns and flees from all women.
Humiliated Dané calls out to Amor and, in response, the god curses Narcissus. In a classic example of poetic justice Narcissus is forced to suffer the same pain he inflicted on others, namely the pain of unrequited love. The vehicle of this justice is a pool of water in which Narcissus falls in love with his own reflection which, at first, he mistakes for a woman. Deranged by lust Dané searches for Narcissus, naked but for a cloak, and finds him at the point of death. Devastated Dané repents ever calling to Amor. Dané expresses her love for the last time, pulls close to her beloved and dies in his arms. The poet warns men and women alike not to disdain suitors lest they suffer a similar fate.
While Ovid’s story is still recognisable many of the details have changed considerably. Almost all references to pagan deities are gone, save Amor who is little more than a personification of love. Narcissus is demoted to the status of a commoner while Echo elevated to the status of princess. Allusions to Narcissus’ homosexuality are expunged. While Ovid talks of Narcissus' disdain for both male and female suitors the lay only mentions his hatred of women. Similarly in the lay Narcissus mistakes his reflection for that of a woman whereas no mention is made of this in Ovid’s account. Finally the tale is overtly moralized with messages about courtly love. Such exhortations were entirely absent from the Metamorphoses rendition.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="41082" url="http://en.wikipedia.org/wiki?curid=41082" title="Effective data transfer rate">
Effective data transfer rate

In telecommunication, effective data transfer rate is the average number of units of data, such as bits, characters, blocks, or frames, transferred per unit time from a source and accepted as valid by a sink.
"Note:" The effective data transfer rate is usually expressed in bits, characters, blocks, or frames per second. The effective data transfer rate may be averaged over a period of seconds, minutes, or hours.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41084" url="http://en.wikipedia.org/wiki?curid=41084" title="Effective height">
Effective height

In telecommunication, the effective height of an antenna is the height of the antenna's center of radiation above the ground. It is defined as the ratio of the induced voltage to the incident field .
In low-frequency applications involving loaded or nonloaded vertical antennas, the effective height is the moment of the current distribution in the vertical section, divided by the input current. For an antenna with a symmetrical current distribution, the center of radiation is the center of the distribution. For an antenna with asymmetrical current distribution, the center of radiation is the center of current moments when viewed from points near the direction of maximum radiation.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41085" url="http://en.wikipedia.org/wiki?curid=41085" title="Effective input noise temperature">
Effective input noise temperature

In telecommunications, effective input noise temperature is the source noise temperature in a two-port network or amplifier that will result in the same output noise power, when connected to a noise-free network or amplifier, as that of the actual network or amplifier connected to a noise-free source. If "F" is the noise figure numeric and 290 K the standard noise temperature, then the effective noise temperature is given by "T" n = 290("F"-1).
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41086" url="http://en.wikipedia.org/wiki?curid=41086" title="Effective mode volume">
Effective mode volume

For an optical fiber, the effective mode volume is the square of the product of the diameter of the near-field pattern and the sine of the radiation angle of the far-field pattern. The diameter of the near-field radiation pattern is defined here as the full width at half maximum and the radiation angle at half maximum radiant intensity. Effective mode volume is proportional to the breadth of the relative distribution of power amongst the modes in a multimode fiber. It is not truly a spatial volume but rather an "optical volume" equal to the product of area and solid angle. The power divided by the effective mode volume is proportional to the radiance of the light emitted by the fiber.

</doc>
<doc id="41088" url="http://en.wikipedia.org/wiki?curid=41088" title="Effective transmission rate">
Effective transmission rate

In telecommunications, effective transmission rate (average rate of transmission, effective speed of transmission) is the rate at which information is processed by a transmission facility. 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41089" url="http://en.wikipedia.org/wiki?curid=41089" title="Efficiency factor">
Efficiency factor

Efficiency factor is a ratio of some measure of performance to an expected value.
Data communication.
In data communications, the factor is the ratio of the time to transmit a text automatically at a specified modulation rate to the time actually required to receive the same text at a specified maximum error rate. All of the communication facilities are assumed to be in the normal condition of adjustment and operation. The practical conditions of measurement should be specified, especially the duration of the measurement.
Telegraph communications may have different temporal efficiency factors for the two directions of transmission.
Industrial engineering.
In industrial engineering, the efficiency factor is the relationship between the allowance time and the time taken, in the form of percentage.
Efficiency factors are used in performance rating and remuneration calculation exercises. The efficiency factor is an extremely simple to use and readily comprehensible index, the prerequisite being exact time management for maintaining the allowed times.

</doc>
<doc id="41091" url="http://en.wikipedia.org/wiki?curid=41091" title="Electrical length">
Electrical length

In telecommunications and electrical engineering, electrical length (or phase length)
refers to the length of an electrical conductor in terms of the phase shift introduced by transmission over that conductor at some frequency.
Usage of the term.
Depending on the specific usage, the term "electrical length" is used rather than simple physical length to incorporate one or more of the following three concepts:
Phase length.
The first usage of the term "electrical length" assumes a sine wave of some frequency, or at least a narrowband waveform centered around some frequency "f". The sine wave will repeat with a period of "T" = 1/"f". The frequency "f" will correspond to a particular wavelength λ along a particular conductor. For conductors (such as bare wire or air-filled coax) which transmit signals at the speed of light "c", the wavelength is given by λ="c"/"f". A distance "L" along that conductor corresponds to "N" wavelengths where "N"= "L" / λ.
In the figure at the right, the wave shown is seen to be "N"=1.5 wavelengths long. A wave crest at the beginning of the graph, moving towards the right, will arrive at the end after a time 1.5"T". The "electrical length" of that segment is said to be "1.5 wavelengths" or, expressed as a phase angle, "540°" (or 3π radians) where "N" wavelengths corresponds to φ = 360°•"N" (or φ = 2π•"N" radians). In radio frequency applications, when a delay is introduced due to a transmission line, it is often the phase shift φ that is of importance, so specifying a design in terms of the phase or electrical length allows one to adapt that design to an arbitrary frequency by employing the wavelength λ applying to that frequency.
Velocity factor.
In a transmission line, a signal travels at a rate controlled by the effective capacitance and inductance per unit of length of the transmission line. Some transmission lines consist only of bare conductors, in which case their signals propagate at the speed of light, "c". More often the signal travels at a reduced velocity κ"c", where κ is the "velocity factor", a number less than 1 representing the ratio of that velocity to the speed of light.
Most transmission lines contain a dielectric material (insulator) filling some or all of the space in between the conductors. The relative permittivity or "dielectric constant" of that material increases the distributed capacitance in the cable, which reduces the velocity factor below unity. It is also possible for κ to be reduced due to a relative permeability of that material which increases the distributed inductance, but this is almost never the case. Now, if one fills a space with a dielectric of relative permittivity formula_1, then the velocity of an electromagnetic plane wave is reduced by the velocity factor:
This reduced velocity factor would also apply to propagation of signals along wires immersed in a large space filled with that dielectric. However with only part of the space around the conductors filled with that dielectric, there is less reduction of the wave velocity. Part of the electromagnetic wave surrounding each conductor "feels" the effect of the dielectric, and part is in free space. Then it is possible to define an "effective relative permittivity" formula_3 which then predicts the velocity factor according to
formula_3 is computed as a weighted average of the relative permittivity of free space (1) and that of the dielectric:
where the "fill factor" F expresses the effective proportion of space so affected by the dielectric.
In the case of coaxial cable, where all of the volume in between the inner conductor and the shield is filled with a dielectric, the fill factor is unity, since the electromagnetic wave is confined to that region. In other types of cable, such as twin lead, the fill factor can be much smaller. Regardless, any cable intended for radio frequencies will have its velocity factor (as well as its characteristic impedance) specified by the manufacturer. In the case of coaxial cable, where F=1, the velocity factor is solely determined by the sort of dielectric used as specified here.
For example, a typical velocity factor for coaxial cable is .66, corresponding to a dielectric constant of 2.25. Suppose we wish to send a 30 MHz signal down a short section of such a cable, and delay it by a quarter wave (90°). In free space, this frequency corresponds to a wavelength of λ0=10m, so a delay of .25λ would require an "electrical length" of 2.5m. Applying the velocity factor of .66, this results in a "physical" length of cable 1.67m long.
The velocity factor likewise applies to antennas in cases where the antenna conductors are (partly) surrounded by a dieletric. This particularly applies to microstrip antennas such as the patch antenna. Waves on microstrip are affected by the dielectric of the circuit board beneath them, but not the air above them. Their velocity factors thus depend not directly on the permittivity of the circuit board material but on the "effective" permittivity formula_3 which is often specified for a circuit board material (or can be calculated). Note that the fill factor and therefore formula_3 are somewhat dependent on the width of the trace compared to the thickness of the board.
Antennas.
While there are certain wideband antenna designs, many antennas are classified as resonant and perform according to design around a particular frequency. This applies especially to broadcasting stations and communication systems which are confined to one frequency or narrow frequency band. This includes the dipole and monopole antennas and all of the designs based on them (Yagi, dipole or monopole arrays, folded dipole, etc.). In addition to the directive gain in beam antennas suffering away from the design frequency, the antenna feedpoint impedance is very sensitive to frequency offsets. Especially for transmitting, the antenna is often intended to operate at the resonant frequency. At the resonant frequency, by definition, that impedance is a pure resistance which matches the characteristic impedance of the transmission line and the output (or input) impedance of the transmitter (or receiver). At frequencies away from the resonant frequency, the impedance includes some reactance (capacitance or inductance). It is possible for an antenna tuner to be used to cancel that reactance (and to change the resistance to match the transmission line), however that is often avoided as an extra complication (and needs to be controlled at the antenna side of the transmission line).
The condition for resonance in a monopole antenna is for the element to be an odd multiple of a quarter-wavelength, "λ"/4. In a dipole antenna both driven conductors must be that long, for a total dipole length of "(2N+1)λ"/2.
The electrical length of an antenna element is, in general, different from its physical length
For example, increasing the diameter of the conductor, or the presence of nearby metal objects, will decrease the velocity of the waves in the element, increasing the electrical length.
An antenna which is shorter than its resonant length is described as ""electrically short", and exhibits capacitive reactance. Similarly, an antenna which is longer than its resonant length is described as "electrically long"" and exhibits inductive reactance.
Changing electrical length by loading.
An antenna's effective electrical length can be changed without changing its physical length by adding reactance, (inductance or capacitance) in series with it. This is called "lumped-impedance matching" or "loading".
For example, a monopole antenna such as a metal rod fed at one end, will be resonant when its electrical length is equal to a quarter wavelength, "λ"/4, of the frequency used. If the antenna is shorter than a quarter wavelength, the feedpoint impedance will include capacitive reactance; this causes reflections on the feedline and a mismatch at the transmitter or receiver, even if the resistive component of the impedance is correct. To cancel the capacitive reactance, an inductance, called a loading coil, is inserted in between the feedline and the antenna terminal. Selecting an inductance with the same reactance as the (negative) capacitative reactance seen at the antenna terminal, cancels that capacitance, and the "antenna system" (antenna and coil) will again be resonant. The feedline sees a purely resistive impedance. Since an antenna which had been too short now appears as if it were resonant, the addition of the loading coil is sometimes referred to as "electrically lengthening" the antenna.
Similarly, the feedpoint impedance of a monopole antenna longer than "λ"/4 (or a dipole with arms longer than "λ"/4) will include inductive reactance. A capacitor in series with the antenna can cancel this reactance to make it resonant, which can be referred to as "electrically shortening" the antenna.
Inductive loading is widely used to reduce the length of whip antennas on portable radios such as walkie-talkies and short wave antennas on cars, to meet physical requirements.
Advantages.
The electrical lengthening allows the construction of shorter aerials. It is applied in particular for aerials for VLF, longwave and medium-wave transmitters. Because those radio waves are several hundred meters to many kilometers long, mast radiators of the necessary height cannot be realised economically. It is also used widely for whip antennas on portable devices such as walkie-talkies to allow antennas much shorter than the standard quarter-wavelength to be used. The most widely used example is the rubber ducky antenna.
Disadvantages.
The electrical lengthening reduces the bandwidth of the antenna if other phase control measures are not undertaken. An electrically extended aerial is less efficient than a non-extended antenna.
Technical realization.
There are two possibilities for the realisation of the electric lengthening.
Often both measures are combined. The coils switched in series must be sometimes be placed in the middle of the aerial construction. The cabin installed at a height of 150-metres on the Blosenbergturm in Beromünster is such a construction, in which a lengthening coil is installed for the supply of the upper tower part (the Blosenbergturm has in addition a ring-shaped roof capacitor on its top)
Application.
Transmission aerials of transmitters working at frequencies below the longwave broadcasting band always apply electric lengthening. Broadcasting aerials of longwave broadcasting stations apply it often. However, for transmission aerials of NDBs electrical lengthening is extensively applied, because these use antennas which are considerably less tall than a quarter of the radiated wavelength.

</doc>
<doc id="41092" url="http://en.wikipedia.org/wiki?curid=41092" title="Electric field">
Electric field

The electric field is a component of the electromagnetic field. It is a vector field, and it is generated by electric charges or time-varying magnetic fields as described by Maxwell's equations.
The concept of an electric field was introduced by Michael Faraday.
Definition.
The electric field formula_1 at a given point is defined as the (vectorial) force formula_2 that would be exerted on a stationary test particle of unit charge by electromagnetic forces (i.e. the Lorentz force). A particle of charge formula_3 would be subject to a force formula_4.
Its SI units are newtons per coulomb (N⋅C−1) or, equivalently, volts per metre (V⋅m−1), which in terms of SI base units are kg⋅m⋅s−3⋅A−1.
Sources of electric field.
Causes and description.
Electric fields are caused by electric charges or varying magnetic fields. The former effect is described by Gauss's law, the latter by Faraday's law of induction, which together are enough to define the behavior of the electric field as a function of charge repartition and magnetic field. However, since the magnetic field is described as a function of electric field, the equations of both fields are coupled and together form Maxwell's equations that describe both fields as a function of charges and currents.
In the special case of a steady state (stationary charges and currents), the Maxwell-Faraday inductive effect disappears. The resulting two equations (Gauss's law formula_5 and Faraday's law with no induction term formula_6), taken together, are equivalent to Coulomb's law, written as formula_7 for a charge density
formula_8 (formula_9 denotes the position in space). Notice that formula_10, the permittivity of vacuum, must be substituted if charges are considered in non-empty media.
Continuous vs. discrete charge repartition.
The equations of electromagnetism are best described in a continuous description. However, charges are sometimes best described as discrete points; for example, some models may describe electrons as punctual sources where charge density is infinite on an infinitesimal section of space.
A charge formula_3 located in formula_12 can be described mathematically as a charge density formula_13, where the Dirac delta function (in three dimensions) is used. Conversely, a charge distribution can be approximated by many small punctual charges.
Superposition principle.
Electric fields satisfy the superposition principle, because Maxwell's equations are linear. As a result, if formula_14 and formula_15 are the electric fields resulting from distribution of charges formula_16 and formula_17, a distribution of charges formula_18 will create an electric field formula_19; for instance, Coulomb's law is linear in charge density as well.
This principle is useful to calculate the field created by multiple point charges. If charges formula_20 are stationary in space at formula_21, in the absence of currents, the superposition principle proves that the resulting field is the sum of fields generated by each particle as described by Coulomb's law:
This suggests similarities between the electric field E and the gravitational field g, or their associated potentials. Mass is sometimes called "gravitational charge" because of that similarity.
Electrostatic and gravitational forces both are central, conservative and obey an inverse-square law.
Uniform fields.
A uniform field is one in which the electric field is constant at every point. It can be approximated by placing two conducting plates parallel to each other and maintaining a voltage (potential difference) between them; it is only an approximation because of boundary effects (near the edge of the planes, electric field is distorted because the plane does not continue). Assuming infinite planes, the magnitude of the electric field "E" is:
where Δ"ϕ" is the potential difference between the plates and "d" is the distance separating the plates. The negative sign arises as positive charges repel, so a positive charge will experience a force away from the positively charged plate, in the opposite direction to that in which the voltage increases. In micro- and nanoapplications, for instance in relation to semiconductors, a typical magnitude of an electric field is in the order of , achieved by applying a voltage of the order of 1 volt between conductors spaced 1 µm apart.
Electrodynamic fields.
Electrodynamic fields are E-fields which do change with time, for instance when charges are in motion.
The electric field cannot be described independently of the magnetic field in that case. If A is the magnetic vector potential, defined so that formula_24, one can still define an electric potential formula_25 such that:
One can recover Faraday's law of induction by taking the curl of that equation
which justifies, a posteriori, the previous form for E.
Energy in the electric field.
If the magnetic field B is nonzero,
The total energy per unit volume stored by the electromagnetic field is
where "ε" is the permittivity of the medium in which the field exists, formula_29 its magnetic permeability, and E and B are the electric and magnetic field vectors.
As E and B fields are coupled, it would be misleading to split this expression into "electric" and "magnetic" contributions. However, in the steady-state case, the fields are no longer coupled (see Maxwell's equations). It makes sense in that case to compute the electrostatic energy per unit volume:
The total energy "U" stored in the electric field in a given volume "V" is therefore
Further extensions.
Definitive equation of vector fields.
In the presence of matter, it is helpful in electromagnetism to extend the notion of the electric field into three vector fields, rather than just one:
where P is the electric polarization – the volume density of electric dipole moments, and D is the electric displacement field. Since E and P are defined separately, this equation can be used to define D. The physical interpretation of D is not as clear as E (effectively the field applied to the material) or P (induced field due to the dipoles in the material), but still serves as a convenient mathematical simplification, since Maxwell's equations can be simplified in terms of free charges and currents.
Constitutive relation.
The E and D fields are related by the permittivity of the material, "ε".
For linear, homogeneous, isotropic materials E and D are proportional and constant throughout the region, there is no position dependence: For inhomogeneous materials, there is a position dependence throughout the material:
For anisotropic materials the E and D fields are not parallel, and so E and D are related by the permittivity tensor (a 2nd order tensor field), in component form:
For non-linear media, E and D are not proportional. Materials can have varying extents of linearity, homogeneity and isotropy.

</doc>
<doc id="41093" url="http://en.wikipedia.org/wiki?curid=41093" title="Electromagnetic compatibility">
Electromagnetic compatibility

Electromagnetic compatibility (EMC) is the branch of electrical sciences which studies the unintentional generation, propagation and reception of electromagnetic energy with reference to the unwanted effects (electromagnetic interference, or EMI) that such energy may induce. The goal of EMC is the correct operation, in the same electromagnetic environment, of different equipment which use electromagnetic phenomena, and the avoidance of any interference effects.
In order to achieve this, EMC pursues two different kinds of issues. Emission issues are related to the unwanted generation of electromagnetic energy by some source, and to the countermeasures which should be taken in order to reduce such generation and to avoid the escape of any remaining energies into the external environment. Susceptibility or immunity issues, in contrast, refer to the correct operation of electrical equipment, referred to as the victim, in the presence of unplanned electromagnetic disturbances.
Interference mitigation and hence electromagnetic compatibility is achieved by addressing both emission and susceptibility issues, i.e., quieting the sources of interference and hardening the potential victims. The coupling path between source and victim may also be separately addressed to increase its attenuation.
Introduction.
While electromagnetic interference (EMI) is a "phenomenon" - the radiation emitted and its effects - electromagnetic compatibility (EMC) is an equipment "characteristic" or "property" - not to behave unacceptably in the EMI environment.
EMC ensures the correct operation, in the same electromagnetic environment, of different equipment items which use or respond to electromagnetic phenomena, and the avoidance of any interference effects. Another way of saying this is that EMC is the control of EMI so that unwanted effects are prevented.
Besides understanding the phenomena in themselves, EMC also addresses the countermeasures, such as control regimes, design and measurement, which should be taken in order to prevent emissions from causing any adverse effect.
Types of interference.
Electromagnetic interference divides into several categories according to the source and signal characteristics.
The origin of interference, often called "noise" in this context, can be man-made or natural.
Continuous interference.
Continuous, or Continuous Wave (CW), interference arises where the source continuously emits at a given range of frequencies. This type is naturally divided into sub-categories according to frequency range, and as a whole is sometimes referred to as "DC to daylight".
Pulse or transient interference.
An electromagnetic pulse (EMP), sometimes called a transient disturbance, arises where the source emits a short-duration pulse of energy. The energy is usually broadband by nature, although it often excites a relatively narrow-band "damped sine wave" response in the victim.
Sources divide broadly into isolated and repetitive events.
Coupling mechanisms.
Some of the technical words employed can be used with differing meanings. These terms are used here in a widely accepted way, which is consistent with other articles in the encyclopedia.
The basic arrangement of noise source, coupling path and victim, receptor or sink is shown in the figure below. Source and victim are usually electronic hardware devices, though the source may be a natural phenomenon such as a lightning strike, electrostatic discharge (ESD) or, in one famous case, the Big Bang at the origin of the Universe.
There are four basic coupling mechanisms: conductive, capacitive, magnetic or inductive, and radiative. Any coupling path can be broken down into one or more of these coupling mechanisms working together. For example the lower path in the diagram involves inductive, conductive and capacitive modes.
Conductive coupling.
Conductive coupling occurs when the coupling path between the source and the receptor is formed by direct contact with a conducting body, for example a transmission line, wire, cable, PCB trace or metal enclosure.
Conducted noise is also characterised by the way it appears on different conductors:
Inductive coupling.
Inductive coupling occurs where the source and receiver are separated by a short distance (typically less than a wavelength). Strictly, "Inductive coupling" can be of two kinds, electrical induction and magnetic induction. It is common to refer to electrical induction as "capacitive coupling", and to magnetic induction as "inductive coupling".
Capacitive coupling.
Capacitive coupling occurs when a varying electrical field exists between two adjacent conductors typically less than a wavelength apart, inducing a change in voltage across the gap.
Magnetic coupling.
Inductive coupling or magnetic coupling (MC) occurs when a varying magnetic field exists between two parallel conductors typically less than a wavelength apart, inducing a change in voltage along the receiving conductor.
Radiative coupling.
Radiative coupling or electromagnetic coupling occurs when source and victim are separated by a large distance, typically more than a wavelength. Source and victim act as radio antennas: the source emits or radiates an electromagnetic wave which propagates across the open space in between and is picked up or received by the victim.
EMC control.
The damaging effects of electromagnetic interference pose unacceptable risks in many areas of technology, and it is necessary to control such interference and reduce the risks to acceptable levels.
The control of electromagnetic interference (EMI) and assurance of EMC comprises a series of related disciplines:
For a complex or novel piece of equipment, this may require the production of a dedicated "EMC control plan" summarizing the application of the above and specifying additional documents required.
Characterising the threat.
Characterisation of the problem requires understanding of:
The risk posed by the threat is usually statistical in nature, so much of the work in threat characterisation and standards setting is based on reducing the probability of disruptive EMI to an acceptable level, rather than its assured elimination.
Laws and regulators.
Regulatory and standards bodies.
Several international organizations work to promote international co-operation on standardization (harmonization), including publishing various EMC standards. Where possible, a standard developed by one organization may be adopted with little or no change by others. This helps for example to harmonize national standards across Europe. Standards organizations include:
Among the more well known national organizations are:
Laws.
Compliance with national or international standards is usually required by laws passed by individual nations. Different nations can require compliance with different standards.
By European law, manufacturers of electronic devices are advised to run EMC tests in order to comply with compulsory CE-labeling. Undisturbed usage of electric devices for all customers should be ensured and the electromagnetic field strength should be kept on a minimum level. EU directive 2004/108/EC (previously 89/336/EEC) on EMC announces the rules for the distribution of electric devices within the European Union. A good overview of EME limits and EMI demands is given in List of EMC directives.
EMC design.
Electromagnetic noise is produced in the source due to rapid current and voltage changes, and spread via the coupling mechanisms described earlier.
Since breaking a coupling path is equally effective at either the start or the end of the path, many aspects of good EMC design practice apply equally to potential emitters and to potential victims. Further, a circuit which easily couples energy to the outside world will equally easily couple energy in and will be susceptible. A single design improvement often reduces both emissions and susceptibility.
Grounding and shielding.
Grounding and shielding aim to reduce emissions or divert EMI away from the victim by providing an alternative, low-impedance path. Techniques include:
Emissions suppression.
Additional measures to reduce emissions include:
Susceptibility hardening.
Additional measures to reduce susceptibility include:
EMC testing.
Testing is required to confirm that a particular device meets the required standards. It divides broadly into emissions testing and susceptibility testing.
Open-air test sites, or OATS, are the reference sites in most standards. They are especially useful for emissions testing of large equipment systems.
However RF testing of a physical prototype is most often carried out indoors, in a specialised EMC test chamber. Types of chamber include anechoic, reverberation and the GTEM cell.
Sometimes computational electromagnetics simulations are used to test virtual models.
Like all compliance testing, it is important that the test equipment, including the test chamber or site and any software used, be properly calibrated and maintained.
Typically, a given run of tests for a particular piece of equipment will require an "EMC test plan" and follow-up "Test report". The full test program may require the production of several such documents.
Emissions testing.
Emissions are typically measured for radiated field strength and where appropriate for conducted emissions along cables and wiring. Inductive (magnetic) and capacitive (electric) field strengths are near-field effects, and are only important if the device under test (DUT) is designed for location close to other electrical equipment.
Typically a spectrum analyzer is used to measure the emission levels of the DUT across a wide band of frequencies (frequency domain). Specialized spectrum analyzers for EMC testing are available, called EMI Test Receivers or EMI Analyzers. These incorporate bandwidths and detectors as specified by international EMC standards. EMI Receivers along with specified transducers can often be used for both conducted and radiated emissions. Pre-selector filters may also be used to reduce the effect of strong out-of-band signals on the front-end of the receiver.
For conducted emissions, typical transducers include the LISN (Line Impedance Stabilisation Network) or AMN (Artificial Mains Network) and the RF current clamp.
For radiated emission measurement, antennas are used as transducers. Typical antennas specified include dipole, biconical, log-periodic, double ridged guide and conical log-spiral designs. Radiated emissions must be measured in all directions around the DUT.
Some pulse emissions are more usefully characterized using an oscilloscope to capture the pulse waveform in the time domain.
Susceptibility testing.
Radiated field susceptibility testing typically involves a high-powered source of RF or EM pulse energy and a radiating antenna to direct the energy at the potential victim or device under test (DUT).
Conducted voltage and current susceptibility testing typically involves a high-powered signal or pulse generator, and a current clamp or other type of transformer to inject the test signal.
Transient immunity is used to test the immunity of the DUT against powerline disturbances including surges, lightning strikes and switching noise. In motor vehicles, similar tests are performed on battery and signal lines.
Electrostatic discharge testing is typically performed with a piezo spark generator called an "ESD pistol". Higher energy pulses, such as lightning or nuclear EMP simulations, can require a large current clamp or a large antenna which completely surrounds the DUT. Some antennas are so large that they are located outdoors, and care must be taken not to cause an EMP hazard to the surrounding environment.
History.
The earliest EMC issue was lightning strike (lightning electromagnetic pulse, or LEMP) on buildings. Lightning rods or lightning conductors began to appear in the mid-18th century. With the advent of widespread electricity generation and power supply lines from the late 19th century on, problems also arose with equipment short-circuit failure affecting the power supply, and with local fire and shock hazard when the power line was struck by lightning. Power stations were provided with output circuit breakers. Buildings and appliances would soon be provided with input fuses, and later in the 20th century miniature circuit breakers (MCB) would come into use.
As radio communications developed in the first half of the 20th century, interference between broadcast radio signals began to occur and an international regulatory framework was set up to ensure interference-free communications.
As switching devices became commonplace, typically in petrol powered cars and motorcycles but also in domestic appliances such as thermostats and refrigerators, transient interference with domestic radio and (after World War II) TV reception became problematic, and in due course laws were passed requiring the suppression of such interference sources.
ESD problems first arose with accidental electric spark discharges in hazardous environments such as coal mines and when refuelling aircraft or motor cars. Safe working practices had to be developed.
After World War II the military became increasingly concerned with the effects of nuclear electromagnetic pulse (NEMP), lightning strike, and even high-powered radar beams, on vehicle and mobile equipment of all kinds, and especially aircraft electrical systems.
When high RF emission levels from other sources became a potential problem (such as with the advent of microwave ovens), certain frequency bands were designated for Industrial, Scientific and Medical (ISM) use, allowing unlimited emissions. A variety of issues such as sideband and harmonic emissions, broadband sources, and the increasing popularity of electrical switching devices and their victims, resulted in a steady development of standards and laws.
From the 1970s, the popularity of modern digital circuitry rapidly grew. As the technology developed, with faster switching speeds (increasing emissions) and lower circuit voltages (increasing susceptibility), EMC increasingly became a source of concern. Many more nations became aware of EMC as a growing problem and issued directives to the manufacturers of digital electronic equipment, which set out the essential manufacturer requirements before their equipment could be marketed or sold. Organizations in individual nations, across Europe and worldwide, were set up to maintain these directives and associated standards. This regulatory environment led to a sharp growth in the EMC industry supplying specialist devices and equipment, analysis and design software, and testing and certification services.
Low-voltage digital circuits, especially CMOS transistors, became more susceptible to ESD damage as they were miniaturised, and a new ESD regulatory regime had to be developed.
From the 1980s, the ever-increasing use of mobile communications and broadcast media channels has put huge pressure on the available airspace. Regulatory authorities are squeezing band allocations closer and closer together, relying on increasingly sophisticated EMC control methods, especially in the digital communications arena, to keep cross-channel interference to acceptable levels. Digital systems are inherently less susceptible than analog systems, and also offer far easier ways (such as software) to implement highly sophisticated protection measures.
Most recently, even the ISM bands are being used for low-power mobile digital communications. This approach relies on the intermittent nature of ISM interference and use of sophisticated error-correction methods to ensure lossless reception during the quiet gaps between bursts of interference.

</doc>
<doc id="41094" url="http://en.wikipedia.org/wiki?curid=41094" title="Electromagnetic environment">
Electromagnetic environment

In telecommunication, the term electromagnetic environment (EME) has the following meanings: 

</doc>
<doc id="41096" url="http://en.wikipedia.org/wiki?curid=41096" title="Electromagnetic interference control">
Electromagnetic interference control

In telecommunication, electromagnetic interference control (EMI) is the control of radiated and conducted energy such that emissions that are unnecessary for system, subsystem, or equipment operation are reduced, minimized, or eliminated. 
"Note:" Electromagnetic radiated and conducted emissions are controlled regardless of their origin within the system, subsystem, or equipment. Successful EMI control with effective susceptibility control leads to electromagnetic compatibility.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41097" url="http://en.wikipedia.org/wiki?curid=41097" title="Nuclear electromagnetic pulse">
Nuclear electromagnetic pulse

An electromagnetic pulse is a burst of electromagnetic radiation. Nuclear explosions create a characteristic pulse of electromagnetic radiation called a nuclear EMP or NEMP.
Electromagnetic pulse is commonly abbreviated as EMP and pronounced by saying the three letters separately (E-M-P).
The resulting rapidly changing electric fields and magnetic fields may couple with electrical/electronic systems to produce damaging current and voltage surges. The specific characteristics of any particular nuclear EMP event vary according to a number of factors. The greatest of these factors is the altitude of the detonation.
In military terminology, a nuclear warhead detonated hundreds of kilometers above the Earth's surface is known as a high-altitude electromagnetic pulse (HEMP) device. Effects of a HEMP device depend on factors including the altitude of the detonation, energy yield, gamma ray output, interactions with the Earth's magnetic field and electromagnetic shielding of targets.
The HEMP acronym is commonly pronounced as one syllable.
History.
The fact that an electromagnetic pulse is produced by a nuclear explosion was known in the earliest days of nuclear weapons testing. The magnitude of the EMP and the significance of its effects, however, were not immediately realized.
During the first United States nuclear test on 16 July 1945, electronic equipment was shielded due to Enrico Fermi's expectation of the electromagnetic pulse. The official technical history for that first nuclear test states, "All signal lines were completely shielded, in many cases doubly shielded. In spite of this many records were lost because of spurious pickup at the time of the explosion that paralyzed the recording equipment."  During British nuclear testing in 1952–1953 instrumentation failures were attributed to "radioflash", which was their term for EMP.
The first openly reported observation of the unique aspects of high-altitude nuclear EMP occurred during the helium balloon lofted Yucca nuclear test of the Hardtack I series on 28 April 1958. In that test, the electric field measurements from the 1.7 kiloton weapon went off the scale of the test instruments and was estimated to be about 5 times the oscilloscope limits. The Yucca EMP was initially positive-going whereas low-altitude bursts were negative pulses. Also, the polarization of the Yucca EMP signal was horizontal, whereas low-altitude nuclear EMP was vertically polarized. In spite of these many differences, the unique EMP results were dismissed as a possible wave propagation anomaly.
The high-altitude nuclear tests of 1962, as discussed below, confirmed the unique results of the Yucca high-altitude test and increased the awareness of high-altitude nuclear EMP beyond the original group of defense scientists.
The larger scientific community became aware of the significance of the EMP problem after a three-article series on nuclear EMP was published in 1981 by William J. Broad in "Science".
Starfish Prime.
In July 1962, a 1.44 megaton (≈ 6.0 PJ) United States nuclear test in space, 400 km above the mid-Pacific Ocean, called the Starfish Prime test, demonstrated to nuclear scientists that the magnitude and effects of a high-altitude nuclear explosion were much larger than had been previously calculated. Starfish Prime made those effects known to the public by causing electrical damage in Hawaii, about 1445 km away from the detonation point, knocking out about 300 streetlights, setting off numerous burglar alarms and damaging a microwave link.
Starfish Prime was the first success in the series of United States high-altitude nuclear tests in 1962 known as Operation Fishbowl. Subsequent tests gathered more data on the high-altitude EMP phenomenon.
The Bluegill Triple Prime and Kingfish high-altitude nuclear tests of October and November 1962 in Operation Fishbowl provided data that was clear enough to enable physicists to accurately identify the physical mechanisms behind the electromagnetic pulses.
The EMP damage of the Starfish Prime test was quickly repaired because of the ruggedness (compared to today) of Hawaii's electrical and electronic infrastructure in 1962.
The relatively small magnitude of the Starfish Prime EMP in Hawaii (about 5.6 kilovolts/metre) and the relatively small amount of damage (for example, only 1 to 3 percent of streetlights extinguished) led some scientists to believe, in the early days of EMP research, that the problem might not be significant. Newer calculations showed that if the Starfish Prime warhead had been detonated over the northern continental United States, the magnitude of the EMP would have been much larger (22 to 30 kv/m) because of the greater strength of the Earth's magnetic field over the United States, as well as its different orientation at high latitudes. These new calculations, combined with the accelerating reliance on EMP-sensitive microelectronics, heightened awareness that EMP could be a significant problem.
Soviet Test 184.
In 1962, the Soviet Union also performed three EMP-producing nuclear tests in space over Kazakhstan, the last in the "Soviet Project K nuclear tests". Although these weapons were much smaller (300 kiloton or 1.3 PJ) than the Starfish Prime test, they were over a populated, large land mass and at a location where the Earth's magnetic field was greater, the damage caused by the resulting EMP was reportedly much greater than in Starfish Prime. The geomagnetic storm–like E3 pulse from Test 184 induced a current surge in a long underground power line that caused a fire in the power plant in the city of Karaganda.
After the collapse of the Soviet Union, the level of this damage was communicated informally to U.S. scientists.  After the 1991 Soviet Union collapse, there was a period of a few years of cooperation between United States and Russian scientists on the HEMP phenomenon. In addition, funding was secured to enable Russian scientists to formally report on some of the Soviet EMP results in international scientific journals.  As a result, formal documentation of some of the EMP damage in Kazakhstan exists but is still sparse in the open scientific literature, especially in relation to the level of damage that was indicated in the open reports.
For one of the K Project tests, Soviet scientists instrumented a 570 km section of telephone line in the area that they expected to be affected by the pulse. The monitored telephone line was divided into sub-lines of 40 to in length, separated by repeaters. Each sub-line was protected by fuses and by gas-filled overvoltage protectors. The EMP from the 22 October (K-3) nuclear test (also known as Test 184) blew all of the fuses and fired all of the overvoltage protectors in all of the sub-lines.
Published reports, including a 1998 IEEE article, have stated that there were significant problems with ceramic insulators on overhead electrical power lines during the tests. A 2010 technical report written for Oak Ridge National Laboratory stated, "Power line insulators were damaged, resulting in a short circuit on the line and some lines detaching from the poles and falling to the ground."
Characteristics of nuclear EMP.
Nuclear EMP is a complex multi-pulse, usually described in terms of three components, as defined by the International Electrotechnical Commission (IEC).
The three components of nuclear EMP, as defined by the IEC, are called "E1", "E2" and "E3".
E1.
The E1 pulse is the very fast component of nuclear EMP. E1 is a very brief but intense electromagnetic field that induces very high voltages in electrical conductors. E1 causes most of its damage by causing electrical breakdown voltages to be exceeded. E1 can destroy computers and communications equipment and it changes too quickly for ordinary surge protectors to provide effective protection against it, although there are special fast-acting surge protectors that will block the E1 pulse.
E1 is produced when gamma radiation from the nuclear detonation ionizes (strips electrons from) atoms in the upper atmosphere. This is known as the Compton effect and the resulting current is called the "Compton current". The electrons travel in a generally downward direction at relativistic speeds (more than 90 percent of the speed of light). In the absence of a magnetic field, this would produce a large, vertical pulse of electric current over the entire affected area. The Earth's magnetic field deflects the electron flow at a right angle to the field. This interaction produces a very large, but very brief, electromagnetic pulse over the affected area.
Several physicists worked on the problem of identifying the mechanism of the uniquely large E1 pulse produced by a nuclear weapon detonated at high altitude (HEMP). The correct mechanism was finally identified by Conrad Longmire of Los Alamos National Laboratory in 1963.
Conrad Longmire gives numerical values for a typical case of E1 pulse produced by a second-generation nuclear weapon such as those of Operation Fishbowl in 1962. The typical gamma rays given off by the weapon have an energy of about 2 MeV (million electron volts). The gamma rays transfer about half of their energy to the ejected free electrons, giving an energy of about 1 MeV.
In a vacuum and absent a magnetic field, the electrons would travel with a current density of tens of amperes per square metre. Because of the downward tilt of the Earth's magnetic field at high latitudes, the area of peak field strength is a U-shaped region to the equatorial side of the nuclear detonation. As shown in the diagram at the right, for nuclear detonations over the continental United States, this U-shaped region is south of the detonation point. Near the equator, where the Earth's magnetic field is more nearly horizontal, the E1 field strength is more nearly symmetrical around the burst location.
At geomagnetic field strengths typical of the central United States, central Europe or Australia, these initial electrons spiral around the magnetic field lines with a typical radius of about 85 metres (about 280 feet). These initial electrons are stopped by collisions with other air molecules at an average distance of about 170 metres (a little less than 580 feet). This means that most of the electrons are stopped by collisions with air molecules before completing a full spiral around the field lines.
This interaction of the very rapidly moving negatively charged electrons with the magnetic field radiates a pulse of electromagnetic energy. The pulse typically rises to its peak value in some 5 nanoseconds. Its magnitude typically decays to half of its peak value within 200 nanoseconds. (By the IEC definition, this E1 pulse ends 1000 nanoseconds after it begins.) This process occurs simultaneously on about 1025 electrons.  The simultaneous action of the very large number of electrons causes the resulting electromagnetic pulses from each electron to radiate coherently, thus adding to produce a single very large amplitude, but very narrow, radiated electromagnetic pulse.
Secondary collisions cause subsequent electrons to lose energy before they reach ground level. The electrons generated by these subsequent collisions have such reduced energy that they do not contribute significantly to the E1 pulse.
These 2 MeV gamma rays typically produce an E1 pulse near ground level at moderately high latitudes that peaks at about 50,000 volts per metre. This is a peak power density of 6.6 megawatts per square metre.
The ionization process in the mid-stratosphere causes this region to become an electrical conductor, a process that blocks the production of further electromagnetic signals and causes the field strength to saturate at about 50,000 volts per metre. The strength of the E1 pulse depends upon the number and intensity of the gamma rays and upon the rapidity of the gamma ray burst. Strength is also somewhat dependent upon altitude.
There are reports of "super-EMP" nuclear weapons that are able to exceed the 50,000 volt per metre limit by the nearly instantaneous release of a burst of much higher gamma radiation levels than are known to be produced by second-generation nuclear weapons. The reality and possible construction details of these weapons are classified and unconfirmed in the open scientific literature.
E2.
The E2 component is generated by scattered gamma rays and inelastic gammas produced by neutrons. This E2 component is an "intermediate time" pulse that, by the IEC definition, lasts from about 1 microsecond to 1 second after the explosion. E2 has many similarities to lightning, although lightning-induced E2 may be considerably larger than a nuclear E2. Because of the similarities and the widespread use of lightning protection technology, E2 is generally considered to be the easiest to protect against.
According to the United States EMP Commission, the main problem with E2 is the fact that it immediately follows E1, which may have damaged the devices that would normally protect against E2.
The EMP Commission Executive Report of 2004 states, "In general, it would not be an issue for critical infrastructure systems since they have existing protective measures for defense against occasional lightning strikes. The most significant risk is synergistic, because the E2 component follows a small fraction of a second after the first component's insult, which has the ability to impair or destroy many protective and control features. The energy associated with the second component thus may be allowed to pass into and damage systems."
E3.
The E3 component is very different from E1 and E2. E3 is a very slow pulse, lasting tens to hundreds of seconds. It is caused by the nuclear detonation's temporary distortion of the Earth's magnetic field. The E3 component has similarities to a geomagnetic storm caused by a solar flare. Like a geomagnetic storm, E3 can produce geomagnetically induced currents in long electrical conductors, damaging components such as power line transformers.
Because of the similarity between solar-induced geomagnetic storms and nuclear E3, it has become common to refer to solar-induced geomagnetic storms as "solar EMP." "Solar EMP", however, does not include an E1 or E2 component.
Generation.
Factors that control weapon effectiveness include altitude, yield, construction details, target distance, intervening geographical features, and local strength of the Earth's magnetic field.
Weapon altitude.
According to an internet primer published by the Federation of American Scientists
Thus, for equipment to be affected, the weapon needs to be above the visual horizon.
The altitude indicated above is greater than that of the International Space Station and many low Earth orbit satellites. Large weapons could have a dramatic impact on satellite operations and communications such as occurred during Operation Fishbowl. The damaging effects on orbiting satellites are usually due to factors other than EMP. In the Starfish Prime nuclear test, most damage was to the satellites' solar panels while passing through radiation belts created by the explosion.
For detonations within the atmosphere, the situation is more complex. Within the range of gamma ray deposition, simple laws no longer hold as the air is ionised and there are other EMP effects, such as a radial electric field due to the separation of Compton electrons from air molecules, together with other complex phenomena. For a surface burst, absorption of gamma rays by air would limit the range of gamma ray deposition to approximately 10 miles, while for a burst in the lower-density air at high altitudes, the range of deposition would be far greater.
Weapon yield.
Typical nuclear weapon yields used during Cold War planning for EMP attacks were in the range of 1 to 10 megatons (4.2 to 42 PJ) This is roughly 50 to 500 times the sizes of the weapons the Hiroshima and Nagasaki bombs. Physicists have testified at United States Congressional hearings that weapons with yields of 10 kilotons (42 TJ) or less can produce a large EMP.
The EMP at a fixed distance from an explosion increases at most as the square root of the yield (see the illustration to the right). This means that although a 10 kiloton weapon has only 0.7% of the energy release of the 1.44-megaton Starfish Prime test, the EMP will be at least 8% as powerful. Since the E1 component of nuclear EMP depends on the prompt gamma ray output, which was only 0.1% of yield in Starfish Prime but can be 0.5% of yield in low yield pure nuclear fission weapons, a 10 kiloton bomb can easily be 5 x 8% = 40% as powerful as the 1.44 megaton Starfish Prime at producing EMP.
The total prompt gamma ray energy in a fission explosion is 3.5% of the yield, but in a 10 kiloton detonation the triggering explosive around the bomb core absorbs about 85% of the prompt gamma rays, so the output is only about 0.5% of the yield. In the thermonuclear Starfish Prime the fission yield was less than 100% and the thicker outer casing absorbed about 95% of the prompt gamma rays from the pusher around the fusion stage. Thermonuclear weapons are also less efficient at producing EMP because the first stage can pre-ionize the air which becomes conductive and hence rapidly shorts out the Compton currents generated by the fusion stage. Hence, small pure fission weapons with thin cases are far more efficient at causing EMP than most megaton bombs.
This analysis, however, only applies to the fast E1 and E2 components of nuclear EMP. The geomagnetic storm-like E3 component of nuclear EMP is more closely proportional to the total energy yield of the weapon.
Target distance.
In nuclear EMP all of the components of the electromagnetic pulse are generated outside of the weapon.
For high-altitude nuclear explosions, much of the EMP is generated far from the detonation (where the gamma radiation from the explosion hits the upper atmosphere). This electric field from the EMP is remarkably uniform over the large area affected.
According to the standard reference text on nuclear weapons effects published by the U.S. Department of Defense, "The peak electric field (and its amplitude) at the Earth's surface from a high-altitude burst will depend upon the explosion yield, the height of the burst, the location of the observer, and the orientation with respect to the geomagnetic field. As a general rule, however, the field strength may be expected to be tens of kilovolts per metre over most of the area receiving the EMP radiation."
The text also states that, "... over most of the area affected by the EMP the electric field strength on the ground would exceed 0.5"E"max. For yields of less than a few hundred kilotons, this would not necessarily be true because the field strength at the Earth's tangent could be substantially less than 0.5"E"max."
In other words, the electric field strength in the entire area that is affected by the EMP will be fairly uniform for weapons with a large gamma ray output. For smaller weapons, the electric field may fall at a faster rate as distance increases.
Effects.
On aircraft.
Many nuclear detonations have taken place using bombs. The B-29 aircraft that delivered the nuclear weapons at Hiroshima and Nagasaki did not lose power due to electrical damage, because electrons (ejected from the air by gamma rays) are stopped quickly in normal air for bursts below roughly 10 km, so they are not significantly deflected by the Earth's magnetic field.
If the aircraft carrying the Hiroshima and Nagasaki bombs had been within the intense nuclear radiation zone when the bombs exploded over those cities, then they would have suffered effects from the charge separation (radial) EMP. But this only occurs within the severe blast radius for detonations below about 10 km altitude.
During Operation Fishbowl, EMP disruptions were suffered aboard a KC-135 photographic aircraft flying 300 km from the 410 ktonTNT detonations at 48 and burst altitudes. The vital electronics were less sophisticated than today's and the aircraft was able to land safely.
Vacuum tube versus solid state electronics.
Older, vacuum tube (valve) based equipment is generally much less vulnerable to nuclear EMP than newer solid state equipment. Soviet Cold War–era military aircraft often had avionics based on vacuum tubes due to limited solid-state capabilities and a belief that the vacuum-tube gear would be more likely to survive.
Other components in vacuum tube circuitry can be damaged by EMP. Vacuum tube equipment was damaged in the 1962 testing. The solid state PRC-77 VHF manpackable 2-way radio survived extensive EMP testing. The earlier PRC-25, nearly identical except for a vacuum tube final amplification stage, was tested in EMP simulators, but was not certified to remain fully functional.
Post–Cold War attack scenarios.
The United States military services developed, and in some cases published, hypothetical EMP attack scenarios.
The United States EMP Commission was created by the United States Congress in 2001. The commission is formally known as the Commission to Assess the Threat to the United States from Electromagnetic Pulse (EMP) Attack.
The Commission brought together notable scientists and technologists to compile several reports. In 2008, the EMP Commission released the "Critical National Infrastructures Report". This report describes the likely consequences of a nuclear EMP on civilian infrastructure. Although this report covered the United States, most of the information can be generalized to other industrialized countries. The 2008 report was a followup to a more generalized report issued by the commission in 2004.
In written testimony delivered to the United States Senate in 2005, an EMP Commission staff member reported:
The EMP Commission sponsored a worldwide survey of foreign scientific and military literature to evaluate the knowledge, and possibly the intentions, of foreign states with respect to electromagnetic pulse (EMP) attack. The survey found that the physics of EMP phenomenon and the military potential of EMP attack are widely understood in the international community, as reflected in official and unofficial writings and statements. The survey of open sources over the past decade finds that knowledge about EMP and EMP attack is evidenced in at least Britain, France, Germany, Israel, Egypt, Taiwan, Sweden, Cuba, India, Pakistan, Iraq under Saddam Hussein, Iran, North Korea, China and Russia.
Many foreign analysts–particularly in Iran, North Korea, China, and Russia–view the United States as a potential aggressor that would be willing to use its entire panoply of weapons, including nuclear weapons, in a first strike. They perceive the United States as having contingency plans to make a nuclear EMP attack, and as being willing to execute those plans under a broad range of circumstances.
Russian and Chinese military scientists in open source writings describe the basic principles of nuclear weapons designed specifically to generate an enhanced-EMP effect, that they term "Super-EMP" weapons. "Super-EMP" weapons, according to these foreign open source writings, can destroy even the best protected U.S. military and civilian electronic systems.
The United States EMP Commission determined that long-known protections are almost completely absent in the civilian infrastructure of the United States and that large parts of US military services were less-protected against EMP than during the Cold War. In public statements, the EMP experts on the EMP Commission recommended making electronic equipment and electrical components resistant to EMP — and maintaining spare parts inventories that would enable prompt repairs. The United States EMP Commission did not look at the civilian infrastructures of other nations.
In 2011 the Defense Science Board published a report about the ongoing efforts to defend critical military and civilian systems against EMP and other nuclear weapons effects.
Common misconceptions.
A 2010 technical report written for the US government's Oak Ridge National Laboratory included a brief section addressing common EMP myths. The remainder of this section is a direct quotation from that Oak Ridge report regarding common HEMP Myths:
Protecting infrastructure.
In 2013, the US House of Representatives considered the "Secure High-voltage Infrastructure for Electricity from Lethal Damage Act" that would provide surge protection for some 300 large transformers around the country.
The problem of protecting civilian infrastructure from electromagnetic pulse has also been intensively studied throughout the European Union, and in particular by the United Kingdom.
In fiction and popular culture.
Especially since the 1980s, Nuclear EMP weapons have gained a significant presence in fiction and popular culture.
The popular media often depict EMP effects incorrectly, causing misunderstandings among the public and even professionals, and official efforts have been made in the United States to set the record straight. See, for example, the Oak Ridge quotation in the above section of this article on "Common Misconceptions." Also, the United States Space Command commissioned science educator Bill Nye to produce a video called "Hollywood vs. EMP" so that Hollywood fiction would not confuse those who must deal with real EMP events. The U.S. Space Command video is not available to the general public.

</doc>
<doc id="41098" url="http://en.wikipedia.org/wiki?curid=41098" title="Electromagnetic radiation and health">
Electromagnetic radiation and health

Electromagnetic radiation can be classified into two types: ionizing radiation and non-ionizing radiation, based on its capability of ionizing atoms and breaking chemical bonds. Ultraviolet and higher frequencies, such as X-rays or gamma rays are ionizing, and these pose their own special hazards: see "radiation" and "radiation poisoning". The electricity that comes out of every power socket has associated low-frequency electromagnetic fields. And various kinds of higher-frequency radiowaves are used to transmit information – whether via TV antennas, radio stations or mobile phone base stations.
By far the most common health hazard of radiation is sunburn, which causes over one million new skin cancers annually.
Types of hazards.
Electrical hazards.
Very strong radiation can induce current capable of delivering an electric shock to persons or animals. It can also overload and destroy electrical equipment. The induction of currents by oscillating magnetic fields is also the way in which solar storms disrupt the operation of electrical and electronic systems, causing damage to and even the explosion of power distribution transformers, blackouts (as occurred in 1989), and interference with electromagnetic signals ("e.g." radio, TV, and telephone signals). 
Fire hazards.
Extremely high power electromagnetic radiation can cause electric currents strong enough to create sparks (electrical arcs) when an induced voltage exceeds the breakdown voltage of the surrounding medium ("e.g." air). These sparks can then ignite flammable materials or gases, possibly leading to an explosion.
This can be a particular hazard in the vicinity of explosives or pyrotechnics, since an electrical overload might ignite them. This risk is commonly referred to as Hazards of Electromagnetic Radiation to Ordnance (HERO) by the United States Navy (USN). United States Military Standard 464A (MIL-STD-464A) mandates assessment of HERO in a system, but USN document OD 30393 provides design principles and practices for controlling electromagnetic hazards to ordnance.
On the other hand, the risk related to fueling is known as Hazards of Electromagnetic Radiation to Fuel (HERF). NAVSEA OP 3565 Vol. 1 could be used to evaluate HERF, which states a maximum power density of 0.09 W/m² for frequencies under 225 MHz (i.e. 4.2 meters for a 40 W emitter).
Biological hazards.
The best understood biological effect of electromagnetic fields is to cause dielectric heating. For example, touching or standing around an antenna while a high-power transmitter is in operation can cause severe burns. These are exactly the kind of burns that would be caused inside a microwave oven.
This heating effect varies with the power and the frequency of the electromagnetic energy. A measure of the heating effect is the specific absorption rate or SAR, which has units of watts per kilogram (W/kg). The IEEE and many national governments have established safety limits for exposure to various frequencies of electromagnetic energy based on SAR, mainly based on Guidelines, which guard against thermal damage.
There are publications which support the existence of complex biological effects of weaker "non-thermal" electromagnetic fields (see Bioelectromagnetics), including weak ELF magnetic fields and modulated RF and microwave fields. Fundamental mechanisms of the interaction between biological material and electromagnetic fields at non-thermal levels are not fully understood.
A 2009 study at the University of Basel in Switzerland found that intermittent (but not continuous) exposure of human cells to a 50 Hz electromagnetic field at a flux density of 1 mT (or 10 G) induced a slight but significant increase of DNA fragmentation in the Comet assay. However that level of exposure is already above current established safety exposure limits.
Lighting.
Compact fluorescent light bulbs.
Compact energy efficient fluorescent light bulbs may emit dangerous levels of Ultraviolet radiation when the protective coating around the phosphor, which creates light inside the bulb, is cracked by mishandling or faulty manufacturing. This cracking of bulb's shielding allows UV rays to escape at levels that could cause burns or even skin cancer. The light generated inside the bulb of a fluorescent light is invisible UV which is converted into visible light by the phosphor coating.
LED lights.
White light, emitting at wavelengths of 400-500 nanometers suppresses the production of melatonin produced by the pineal gland. The effect is disruption of a human being’s biological clock resulting in poor sleeping and rest periods.
EMR effects on the human body by frequency.
While the most acute exposures to harmful levels of electromagnetic radiation are immediately realized as burns, the health effects due to chronic or occupational exposure may not manifest effects for months or years.
Extremely-low-frequency RF.
High-power extremely-low-frequency RF with electric field levels in the low kV/m range are known to induce perceivable currents within the human body that create an annoying tingling sensation. These currents will typically flow to ground through a body contact surface such as the feet, or arc to ground where the body is well insulated.
Shortwave frequency RF.
Shortwave Diathermy heating of human tissue only heats tissues that are good electrical conductors, such as blood vessels and muscle. Adipose tissue (fat) receives little heating by induction fields because an electrical current is not actually going through the tissues.
Microwaves.
Microwave exposure at low-power levels below the specific absorption rate set by government regulatory bodies is considered harmless non-ionizing radiation and has no effect on the human body. Levels above the specific absorption rate set by the U.S. Federal Communications Commission are those they considered to be potentially harmful. ANSI standards for safe exposure levels to RF and microwave radiation are set to a SAR level of 4 W/kg, the threshold before hazardous thermical effects occur due to energy absorption in the body. A safety factor of ten was then incorporated to arrive at the final recommended protection guidelines of a SAR exposure threshold of 0.4 W/kg for RF and microwave radiation. There is disagreement over exactly what levels of RF radiation are safe, particularly with regard to low levels of exposure. Russia and eastern European countries set SAR thresholds for microwaves and RF much lower than western countries.
Two areas of the body, the eyes and the testes, can be particularly susceptible to heating by RF energy because of the relative lack of available blood flow to dissipate the excessive heat load. Laboratory experiments have shown that short-term exposure to high levels of RF radiation (100-200 mW/cm²) can cause cataracts in rabbits. Temporary sterility, caused by such effects as changes in sperm count and in sperm motility, is possible after exposure of the testes to high-level RF radiation
Long-term exposure to high-levels of microwaves, is recognized, from experimental animal studies and epidemiological studies in humans, to cause cataracts. The mechanism is unclear but may include changes in heat sensitive enzymes that normally protect cell proteins in the lens. Another mechanism that has been advanced is direct damage to the lens from pressure waves induced in the aqueous humor.
Exposure to sufficiently high-power microwave RF is known to create effects ranging from a burning sensation on the skin and microwave auditory effect, to extreme pain at the mid-range, to physical microwave burns and blistering of skin and internals at high power levels.
Millimeter waves.
Recent technology advances in the developments of Millimeter wave scanners for airport security and WiGig for Personal area networks have opened the 60 GHz and above Microwave band to SAR exposure regulations. Previously, microwave applications in these bands were for point-to-point satellite communication with minimal human exposure. Radiation levels in the millimeter wavelength represent the high microwave band or close to Infrared wavelengths.
Infrared.
Infrared wavelengths longer than 750 nm can produce changes in the lens of the eye. Glassblower's cataract is an example of a heat injury that damages the anterior lens capsule among unprotected glass and iron workers. Cataract-like changes can occur in workers who observe glowing masses of glass or iron without protective eyewear for many hours a day.
Another important factor is the distance between the worker and the source of radiation. In the case of arc welding, infrared radiation decreases rapidly as a function of distance, so that farther than 3 feet away from where welding takes place, it does not pose an ocular hazard anymore but, ultraviolet radiation still does. This is why welders wear tinted glasses and surrounding workers only have to wear clear ones that filter UV.
Visible Light.
Moderate and high-power lasers are potentially hazardous because they can burn the retina of the eye, or even the skin. To control the risk of injury, various specifications – for example ANSI Z136 in the US, and IEC 60825 internationally – define "classes" of lasers depending on their power and wavelength. These regulations also prescribe required safety measures, such as labeling lasers with specific warnings, and wearing laser safety goggles during operation (see laser safety).
As with its infrared and ultraviolet radiation dangers, welding creates an intense brightness in the visible light spectrum, which may cause temporary flash blindness. Some sources state that there is no minimum safe distance for exposure to these radiation emissions without adequate eye protection.
Ultraviolet.
Short-term exposure to strong ultraviolet sunlight causes sunburn within hours of exposure.
Ultraviolet light, specifically UV-B, has been shown to cause cataracts and there is some evidence that sunglasses worn at an early age can slow its development in later life. Most UV light from the sun is filtered out by the atmosphere and consequently airline pilots often have high rates of cataracts because of the increased levels of UV radiation in the upper atmosphere. It is hypothesised that depletion of the ozone layer and a consequent increase in levels of UV light on the ground may increase future rates of cataracts. Note that the lens filters UV light, so once that is removed via surgery, one may be able to see UV light.
Prolonged exposure to ultraviolet radiation from the sun can lead to melanoma and other skin malignancies. Clear evidence establishes ultraviolet radiation, especially the non-ionizing medium wave UVB, as the cause of most non-melanoma skin cancers, which are the most common forms of cancer in the world. UV rays can also cause wrinkles, liver spots, moles, and freckles. In addition to sunlight, other sources include tanning beds, and bright desk lights. Damage is cumulative over one's lifetime, so that permanent effects may not be evident for some time after exposure.
Ultraviolet radiation of wavelengths shorter than 300 nm (actinic rays) can damage the corneal epithelium. This is most commonly the result of exposure to the sun at high altitude, and in areas where shorter wavelengths are readily reflected from bright surfaces, such as snow, water, and sand. UV generated by a welding arc can similarly cause damage to the cornea, known as "arc eye" or welding flash burn, a form of photokeratitis.
Radio frequency fields.
Apart from some suspicion that the electromagnetic fields emitted by mobile phones may be responsible for an increased risk of glioma and acoustic neuroma, the fields otherwise pose no risk to human health. This designation of mobile phone signals as "possibly carcinogenic" by the World Health Organization has often been misinterpreted as indicating that of some measure of risk has been observed – however the designation indicates that the possibility could not be conclusively ruled out using the available data.

</doc>
<doc id="41099" url="http://en.wikipedia.org/wiki?curid=41099" title="Electromagnetic survivability">
Electromagnetic survivability

In telecommunication, electromagnetic survivability is the ability of a system, subsystem, or equipment to resume functioning without evidence of degradation following temporary exposure to an adverse electromagnetic environment. 
The system, subsystem, or equipment performance may be degraded during exposure to the adverse electromagnetic environment, but the system will not experience permanent damage, such as component burnout, that will prevent proper operation when the adverse electromagnetic environment is removed.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41100" url="http://en.wikipedia.org/wiki?curid=41100" title="Electronic deception">
Electronic deception

In telecommunication, the term electronic deception means the deliberate radiation, reradiation, alteration, suppression, absorption, denial, enhancement, or reflection of electromagnetic energy in a manner intended to convey misleading information and to deny valid information to an enemy or to enemy electronics-dependent weapons.
Among the types of electronic deception are:

</doc>
<doc id="41101" url="http://en.wikipedia.org/wiki?curid=41101" title="Electronic switching system">
Electronic switching system

In telecommunications, an electronic switching system (ESS) is a telephone switch that uses digital electronics and computerized control to interconnect telephone circuits for the purpose of establishing telephone calls.
The generations of telephone switches before the advent of electronic switching in the 1950s used purely electro-mechanical relay systems and analog voice paths. These early machines typically utilized the step-by-step technique. The first generation of electronic switching systems in the 1960s were not entirely digital in nature, but used reed relay-operated metallic paths or crossbar switches operated by stored program control (SPC) systems.
First announced in 1955, the first customer trial installation of an all-electronic central office commenced in Morris, Illinois in November 1960 by Bell Laboratories. The first prominent large-scale electronic switching system was the Number One Electronic Switching System (1ESS) of the Bell System in the United States, introduced in Succasunna, New Jersey, in May 1965.
Later electronic switching systems implemented the digital representation of the electrical audio signals on subscriber loops by digitizing the analog signals and processing the resulting data for transmission between central offices. Time-division multiplexing (TDM) technology permitted the simultaneous transmission of multiple telephone calls on a single wire connection between central offices or other electronic switches, resulting in dramatic capacity improvements of the telephone network.
With the advances of digital electronics starting in the 1960s telephone switches employed semiconductor device components in increasing measure.
In the late 20th century most telephone exchanges without TDM processing were eliminated and the term "electronic switching system" became largely a historical distinction for the older SPC systems.

</doc>
<doc id="41102" url="http://en.wikipedia.org/wiki?curid=41102" title="Electronic warfare support measures">
Electronic warfare support measures

In military telecommunications, the terms Electronic Support (ES) or Electronic Support Measures (ESM) describe the division of electronic warfare involving actions taken under direct control of an operational commander to detect, intercept, identify, locate, record, and/or analyze sources of radiated electromagnetic energy for the purposes of immediate threat recognition (such as warning that fire control RADAR has locked on a combat vehicle, ship, or aircraft) or longer-term operational planning. Thus, Electronic Support provides a source of information required for decisions involving Electronic Protection (EP), Electronic Attack (EA), avoidance, targeting, and other tactical employment of forces. Electronic Support data can be used to produce signals intelligence (SIGINT), communications intelligence (COMINT) and electronics intelligence (ELINT).
Electronic support measures gather intelligence through passive "listening" to electromagnetic radiations of military interest. Electronic support measures can provide (1) initial detection or knowledge of foreign systems, (2) a library of technical and operational data on foreign systems, and (3) tactical combat information utilizing that library. ESM collection platforms can remain electronically silent and detect and analyze RADAR transmissions beyond the RADAR detection range because of the greater power of the transmitted electromagnetic pulse with respect to a reflected echo of that pulse. United States airborne ESM receivers are designated in the AN/ALR series.
Desirable characteristics for electromagnetic surveillance and collection equipment include (1) wide-spectrum or bandwidth capability because foreign frequencies are initially unknown, (2) wide dynamic range because signal strength is initially unknown, (3) narrow bandpass to discriminate the signal of interest from other electromagnetic radiation on nearby frequencies, and (4) good angle-of arrival measurement for bearings to locate the transmitter. The frequency spectrum of interest ranges from 30 MHz to 50 GHz. Multiple receivers are typically required for surveillance of the entire spectrum, but tactical receivers may be functional within a specific signal strength threshold of a smaller frequency range.

</doc>
<doc id="41103" url="http://en.wikipedia.org/wiki?curid=41103" title="Electro-optic effect">
Electro-optic effect

An electro-optic effect is a change in the optical properties of a material in response to an electric field that varies slowly compared with the frequency of light. The term encompasses a number of distinct phenomena, which can be subdivided into
Changes in absorption can have a strong effect on refractive index for wavelengths near the absorption edge, due to the Kramers–Kronig relation. 
Using a less strict definition of the electro-optic effect allowing also electric fields oscillating at optical frequencies, one could also include nonlinear absorption (absorption depends on the light intensity) to category a) and the optical Kerr effect (refractive index depends on the light intensity) to category b). Combined with the photoeffect and photoconductivity, the electro-optic effect gives rise to the photorefractive effect.
The term "electro-optic" is often erroneously used as a synonym for "optoelectronic".
Main applications.
Electro-optic modulators.
Electro-optic modulators are usually built with electro-optic crystals exhibiting the Pockels effect. The transmitted beam is phase modulated with the electric signal applied to the crystal. Amplitude modulators can be built by putting the electro-optic crystal between two linear polarizers or in one path of a Mach–Zehnder interferometer.
Additionally, Amplitude modulators can be constructed by deflecting the beam into and out of a small aperture such as a fiber. This design can be low loss (<3 dB) and polarization independent depending on the crystal configuration.
Electro-optic deflectors.
Electro-optic deflectors utilize prisms of electro-optic crystals. The index of refraction is changed by the Pockels effect, thus changing the direction of propagation of the beam inside the prism. Electro-optic deflectors have only a small number of resolvable spots, but possess a fast response time. There are few commercial models available at this time. This is because of competing acousto-optic deflectors, the small number of resolvable spots and the relatively high price of electro-optic crystals.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41104" url="http://en.wikipedia.org/wiki?curid=41104" title="Electro-optic modulator">
Electro-optic modulator

Electro-optic modulator (EOM) is an optical device in which a signal-controlled element exhibiting the electro-optic effect is used to modulate a beam of light. The modulation may be imposed on the phase, frequency, amplitude, or polarization of the beam. Modulation bandwidths extending into the gigahertz range are possible with the use of laser-controlled modulators.
The electro-optic effect is the change in the refractive index of a material resulting from the application of a DC or low-frequency electric field. This is caused by forces that distort the position, orientation, or shape of the molecules constituting the material. Generally, a nonlinear optical material (organic polymers have the fastest response rates, and thus are best for this application) with an incident static or low frequency optical field will see a modulation of its refractive index.
The simplest kind of EOM consists of a crystal, such as lithium niobate, whose refractive index is a function of the strength of the local electric field. That means that if lithium niobate is exposed to an electric field, light will travel more slowly through it. But the phase of the light leaving the crystal is directly proportional to the length of time it takes that light to pass through it. Therefore, the phase of the laser light exiting an EOM can be controlled by changing the electric field in the crystal.
Note that the electric field can be created by placing a parallel plate capacitor across the crystal. Since the field inside a parallel plate capacitor depends linearly on the potential, the index of refraction depends linearly on the field (for crystals where Pockels effect dominates), and the phase depends linearly on the index of refraction, the phase modulation must depend linearly on the potential applied to the EOM.
The voltage required for inducing a phase change of formula_1 is called the half-wave voltage (formula_2). For a Pockels cell, it is usually hundreds or even thousands of volts, so that a high-voltage amplifier is required. Suitable electronic circuits can switch such large voltages within a few nanoseconds, allowing the use of EOMs as fast optical switches.
Liquid crystal devices are electro-optical phase modulators if no polarizers are used.
Phase Modulation.
A very common application of EOMs is for creating sidebands in a monochromatic laser beam. To see how this works, first imagine that the strength of a laser beam with frequency formula_3 entering the EOM is given by
Now suppose we apply a sinusoidally varying potential voltage to the EOM with frequency formula_5 and small amplitude formula_6. This adds a time dependent phase to the above expression,
Since formula_6 is small, we can use the Taylor expansion for the exponential
to which we apply a simple identity for sine,
This expression we interpret to mean that we have the original carrier frequency plus two small sidebands, one at formula_11 and another at formula_12. Notice however that we only used the first term in the Taylor expansion - in truth there are an infinite number of sidebands. There is a useful identity involving Bessel functions called the Jacobi-Anger expansion which can be used to derive
which gives the amplitudes of all the sidebands. Notice that if one modulates the amplitude instead of the phase, one gets only the first set of sidebands, 
Amplitude modulation.
A phase modulating EOM can also be used as an amplitude modulator by using a Mach-Zehnder interferometer. A beam splitter divides the laser light into two paths, one of which has a phase modulator as described above. The beams are then recombined. Changing the electric field on the phase modulating path will then determine whether the two beams interfere constructively or destructively at the output, and thereby control the amplitude or intensity of the exiting light. This device is called a Mach-Zehnder modulator.
Polarization modulation.
Depending on the type and orientation of the nonlinear crystal, and on the direction of the applied electric field, the phase delay can depend on the polarization direction. A Pockels cell can thus be seen as a voltage-controlled waveplate, and it can be used for modulating the polarization state. For a linear input polarization (often oriented at 45° to the crystal axis), the output polarization will in general be elliptical, rather than simply a linear polarization state with a rotated direction.

</doc>
<doc id="41105" url="http://en.wikipedia.org/wiki?curid=41105" title="Electro-optics">
Electro-optics

Electro-optics is a branch of electrical engineering and material physics involving components, devices (e.g Lasers, LEDs, waveguides etc) and systems which operate by the propagation and interaction of light with various tailored materials. It is essentially the same, as what is popularly described today as photonics. It is not only concerned with the "Electro-Optic effect". Thus it concerns the interaction between the electromagnetic (optical) and the electrical (electronic) states of materials.
Electro-optical devices.
The electro-optic effect relates to a change in the optical properties of the medium, which is usually a change in the birefringence, and not simply the refractive index.
In a Kerr cell, the change in birefringence is proportional to the square of the electric field, and the material is usually a liquid. In a Pockels cell, the change in birefringence varies linearly with the electric field, and the material is a crystal.
Non-crystalline, solid electro-optical materials have caught interest because of their low cost of production. These organic, polymer-based materials are also known as organic EO material, plastic EO material, or polymer EO material. They consist of nonlinear optical chromophores in a polymer lattice. The nonlinear optical chromophores produce Pockel's effect.

</doc>
<doc id="41106" url="http://en.wikipedia.org/wiki?curid=41106" title="Elliptical polarization">
Elliptical polarization

In electrodynamics, elliptical polarization is the polarization of electromagnetic radiation such that the tip of the electric field vector describes an ellipse in any fixed plane intersecting, and normal to, the direction of propagation. An elliptically polarized wave may be resolved into two linearly polarized waves in phase quadrature, with their polarization planes at right angles to each other. Since the electric field can rotate clockwise or counterclockwise as it propagates, elliptically polarized waves exhibit chirality.
Other forms of polarization, such as circular and linear polarization, can be considered to be special cases of elliptical polarization.
Mathematical description of elliptical polarization.
The classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is (cgs units)
for the magnetic field, where k is the wavenumber,
is the angular frequency of the wave propagating in the +z direction, and formula_4 is the speed of light.
Here formula_5 is the amplitude of the field and
is the normalized Jones vector. This is the most complete representation of polarized electromagnetic radiation and corresponds in general to elliptical polarization.
Polarization ellipse.
At a fixed point in space (or for fixed z), the electric vector formula_7 traces out an ellipse in the x-y plane. The semi-major and semi-minor axes of the ellipse have lengths a and b, respectively, that are given by
and 
where formula_10.
The orientation of the ellipse is given by the angle formula_11 the semi-major axis makes with the x-axis. This angle can be calculated from
If formula_13, the wave is linearly polarized. The ellipse collapses to a straight line formula_14) oriented at an angle formula_15. This is the case of superposition of two simple harmonic motions (in phase), one in the x direction with an amplitude formula_16, and the other in the y direction with an amplitude formula_17. When formula_18 increases from zero, i.e., assumes positive values, the line evolves into an ellipse that is being traced out in the counterclockwise direction (looking into the propagating wave); this then corresponds to Left-Handed Elliptical Polarization; the semi-major axis is now oriented at an angle formula_19. Similarly, if formula_18 becomes negative from zero, the line evolves into an ellipse that is being traced out in the clockwise direction; this corresponds to Right-Handed Elliptical Polarization. 
If formula_21 and formula_22, formula_23, i.e.,the wave is circularly polarized. When formula_24, the wave is left-circularly polarized, and when formula_25, the wave is right-circularly polarized.

</doc>
<doc id="41107" url="http://en.wikipedia.org/wiki?curid=41107" title="Emphasis (telecommunications)">
Emphasis (telecommunications)

In telecommunications emphasis is the intentional alteration of the amplitude-vs.-frequency characteristics of the signal to reduce adverse effects of noise in a communication system.
The whole system of pre-emphasis and de-emphasis is called emphasis.
The high-frequency signal components are emphasized to produce a more equal modulation index for the transmitted frequency spectrum, and therefore a better signal-to-noise ratio for the entire frequency range.
Emphasis is commonly used in LP records and FM broadcasting.
Pre-emphasis.
In processing electronic audio signals, pre-emphasis refers to a system process designed to increase (within a frequency band) the magnitude of some (usually higher) frequencies with respect to the magnitude of other (usually lower) frequencies in order to improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation distortion or saturation of recording media in subsequent parts of the system. The mirror operation is called de-emphasis, and the system as a whole is called emphasis.
Pre-emphasis is achieved with a pre-emphasis network which is essentially a calibrated filter. The frequency response is decided by special time constants. The cutoff frequency can be calculated from that value.
Pre-emphasis is commonly used in telecommunications, digital audio recording, record cutting, in FM broadcasting transmissions, and in displaying the spectrograms of speech signals.
One example of this is the RIAA equalization curve on 33 rpm and 45 rpm vinyl records. Another is the Dolby noise-reduction system as used with magnetic tape.
In high speed digital transmission, pre-emphasis is used to improve signal quality at the output of a data transmission. In transmitting signals at high data rates, the transmission medium may introduce distortions, so pre-emphasis is used to distort the transmitted signal to correct for this distortion. When done properly this produces a received signal which more closely resembles the original or desired signal, allowing the use of higher frequencies or producing fewer bit errors.
Pre-emphasis is employed in frequency modulation or phase modulation transmitters to equalize the modulating signal drive power in terms of deviation ratio. The receiver demodulation process includes a reciprocal network, called a de-emphasis network, to restore the original signal power distribution.
De-emphasis.
In telecommunication, de-emphasis is the complement of pre-emphasis, in the antinoise system called emphasis. Emphasis is a system process designed to decrease, (within a band of frequencies), the magnitude of some (usually higher) frequencies with respect to the magnitude of other (usually lower) frequencies in order to improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation differences or saturation of recording media in subsequent parts of the system.
Special time constants dictate the frequency response curve, from which one can calculate the cutoff frequency.
Pre-emphasis is commonly used in audio digital recording, record cutting and FM radio transmission.
In serial data transmission, de-emphasis has a different meaning, which is to reduce the level of all bits except the first one after a transition. That causes the high frequency content due to the transition to be emphasized compared to the low frequency content which is de-emphasized. This is a form of transmitter equalization; it compensates for losses over the channel which are larger at higher frequencies. Well known serial data standards such as PCI Express, SATA and SAS require transmitted signals to use de-emphasis.
Red Book Audio.
Although rarely used, there exists the capability for standardized emphasis in Red Book CD mastering. As CD's were intended to work on 14 bit audio, a specification for 'pre-emphasis' was included to compensate for quantization noise. After production spec was set at 16 bits, quantization noise became less of a concern, but emphasis remained an option through standards revisions. The pre-emphasis curve is described as 50/15 µs at 20 dB/decade.

</doc>
<doc id="41108" url="http://en.wikipedia.org/wiki?curid=41108" title="Encode">
Encode

Encode or encoder may refer to:

</doc>
<doc id="41109" url="http://en.wikipedia.org/wiki?curid=41109" title="End distortion">
End distortion

End distortion: In start-stop teletypewriter operation, the shifting of the end of all marking pulses, except the stop pulse, from their proper positions in relation to the beginning of the next start pulse. 
Shifting of the end of the stop pulse is a deviation in character time and rate rather than an end distortion. 
Spacing end distortion is the termination of marking pulses before the proper time. Marking end distortion is the continuation of marking pulses past the proper time. 
The magnitude of the distortion is expressed as a percentage of an ideal pulse length.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41110" url="http://en.wikipedia.org/wiki?curid=41110" title="End-of-transmission character">
End-of-transmission character

In telecommunication, an end-of-transmission character (EOT) is a transmission control character. Its intended use is to indicate the conclusion of a transmission that may have included one or more texts and any associated message headings.
An EOT is often used to initiate other functions, such as releasing circuits, disconnecting terminals, or placing receive terminals in a standby condition. Its most common use today is to cause a Unix terminal driver to signal end of file and thus exit programs that are awaiting input.
In ASCII and Unicode, the character is encoded at . It can be referred to as Ctrl+D, ^D in caret notation. Unicode provides the character for when EOT needs to be displayed graphically. In addition, can also be used as a graphic representation of EOT; it is defined in Unicode as "symbol for End of Transmission".
Meaning in Unix.
The EOT character in Unix is different from the Control-Z in DOS. The DOS Control-Z byte is actually sent and/or placed in files to indicate where the text ends. In contrast the Control-D causes the Unix terminal driver to signal the EOF condition, which is not a character, while the byte has no special meaning if actually read or written from a file or terminal.
In Unix the end-of-file character (by default EOT) causes the terminal driver to make available all characters in its input buffer immediately; normally the driver would collect characters until it sees an end-of-line character. If the input buffer is empty (because no characters have been typed since the last end-of-line or end-of-file), a program reading from the terminal reads a count of zero bytes. In Unix, such a condition is understood as having reached the end of the file.
This can be demonstrated with the cat program on Unix-based operating systems such as Linux: Run the codice_1 command with no arguments, so it accepts its input from the keyboard and prints output to the screen. Type a few characters without pressing ↵ Enter, then type Ctrl+D. The characters typed to that point are sent to cat, which then writes them to the screen. If Ctrl+D is typed without typing any characters first, the input stream is terminated and the program ends. An actual EOT is obtained by typing Ctrl+V then Ctrl+D.
If the terminal driver is in "raw" mode, it no longer interprets control characters, and the EOT character is sent unchanged to the program, which is free to interpret it any way it likes. A program may then decide to handle the EOT byte as an indication that it should end the text, this would then be similar to how Ctrl+Z is handled by DOS programs.
Usage in mainframe computer system communications protocols.
The EOT character is used in legacy communications protocols by mainframe computer manufacturers such as IBM, Burroughs Corporation, and the BUNCH. Terminal transmission control protocols such as IBM 3270 Poll/Select, or Burroughs TD830 Contention Mode protocol use the EOT character to terminate a communications sequence between two cooperating stations (such as a host multiplexer or Input/Output terminal).
A single Poll (ask the station for data) or Select (send data to the station) operation will include two round-trip send-reply operations between the polling station and the station being polled, the final operation being transmission of a single EOT character to the initiating station.

</doc>
<doc id="41111" url="http://en.wikipedia.org/wiki?curid=41111" title="Endurability">
Endurability

In telecommunication, endurability is the property of a system, subsystem, equipment, or process that enables it to continue to function within specified performance limits for an extended period of time, usually months, despite a severe natural or man-made disturbance, such as a nuclear attack, or a loss of external logistic or utility support. 
Endurability is not compromised by temporary failures when the local capability exists to restore and maintain the system, subsystem, equipment, or process to an acceptable performance level.

</doc>
<doc id="41112" url="http://en.wikipedia.org/wiki?curid=41112" title="Enhanced service">
Enhanced service

Enhanced service is service offered over commercial carrier transmission facilities used in interstate communications, that employs computer processing applications that act on the format, content, code, protocol, or similar aspects of the subscriber's transmitted information; provides the subscriber with additional, different, or restructured information; or involves subscriber interaction with stored information.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41113" url="http://en.wikipedia.org/wiki?curid=41113" title="Epoch (reference date)">
Epoch (reference date)

In the fields of chronology and periodization, an epoch is an instant in time chosen as the origin of a particular era. The "epoch" then serves as a reference point from which time is measured. Time measurement units are counted from the epoch so that the date and time of events can be specified unambiguously.
Events taking place before the epoch can be dated by counting negatively from the epoch, though in pragmatic periodization practice, epochs are defined for the past, and another epoch is used to start the next era, therefore serving as the ending of the older preceding era. The whole purpose and criteria of such definitions are to clarify and co-ordinate scholarship about a period, at times, across disciplines.
Epochs are generally chosen to be convenient or significant by a consensus of the time scale's initial users, or by authoritarian fiat. The epoch moment or date is usually defined by "a specific clear event", condition, or criterion — the epoch event or epoch criterion — from which the period or era or age is usually characterized or described.
Calendars.
Each calendar era starts from an arbitrary epoch, which is often chosen to commemorate an important historical or mythological event. Many current and historical calendar eras exist, each with its own epoch.
Astronomy.
In astronomy, an epoch is a specific moment in time for which celestial coordinates or orbital elements are specified, and from which other orbital parametrics are thereafter calculated in order to predict future position. The applied tools of the mathematics disciplines of Celestial mechanics or its subfield Orbital mechanics (both predict orbital paths and positions) about a center of gravity are used to generate an ephemeris (plural: "ephemerides"; from the Greek word "ephemeros" = daily) which is a table of values that gives the positions of astronomical objects in the sky at a given time or times, or a formula to calculate such given the proper time offset from the epoch. Such calculations generally result in an elliptical path on a plane defined by some point on the orbit, and the two foci of the ellipse. Viewing from another orbiting body, following its own trace and orbit, creates shifts in three dimensions in the spherical trigonometry used to calculate relative positions. Interestingly, these dynamics in three dimensions are also elliptical, which means the ephemeris need only specify one set of equations to be a useful predictive tool to predict future location of the object of interest.
Over time, inexactitudes and other errors accumulate, creating more and greater errors of prediction, so ephemeris factors need to be recalculated from time to time, and that requires a new epoch to be defined. Different astronomers or groups of astronomers used to define epochs to suit themselves, but in these days of speedy communications, the epochs are generally defined in an international agreement, so astronomers world wide can collaborate more effectively. It was inefficient and error prone for data observed by one group to need translation (mathematic transformation) so other groups could compare information.
J2000.0.
The current standard epoch is called "J2000.0" (and is approximately noon January 1, 2000, Gregorian calendar, at the Royal Observatory, Greenwich, in London, England). This is equivalent to:
When dates or times are expressed as years with a decimal fraction from J2000, the years are of exactly 365.25 days, which is the average length of a year in the Julian calendar.
Computing.
The time kept internally by a computer system is usually expressed as the number of time units that have elapsed since a specified epoch, which is nearly always specified as midnight Universal Time on some particular date.
Software timekeeping systems vary widely in the granularity of their time units; some systems may use time units as large as a day, while others may use nanoseconds. For example, for an epoch date of midnight UTC on January 1, 1900, and a time unit of a second, the time of the midnight between January 1 and 2, 1900 is represented by the number 86400, the number of seconds in one day. When times prior to the epoch need to be represented, it is common to use the same system, but with negative numbers.
These representations of time are mainly for internal use. If an end user interaction with dates and times is required, the software will nearly always convert this internal number into a date and time representation that is comprehensible to humans.
Notable epoch dates in computing.
The following table lists epoch dates used by popular software and other computer-related systems. The time in these systems is stored as the quantity of a particular time unit (days, seconds, nanoseconds, etc.) that has elapsed since a stated time (usually midnight UTC at the beginning of the given date).
Problems with epoch-based computer time representation.
Computers do not generally store arbitrarily large numbers. Instead, each number stored by a computer is allotted a fixed amount of space. Therefore, when the number of time units that have elapsed since a system's epoch exceeds the largest number that can fit in the space allotted to the time representation, the time representation overflows, and problems can occur. While a system's behavior after overflow occurs is not necessarily predictable, in most systems the number representing the time will reset to zero, and the computer system will think that the current time is the epoch time again.
Most famously, older systems which counted time as the number of years elapsed since the epoch of January 1, 1900 and which only allotted enough space to store the numbers 0 through 99, experienced the Year 2000 problem. These systems (if not corrected beforehand) would interpret the date January 1, 2000 as January 1, 1900, leading to unpredictable errors at the beginning of the year 2000.
Even systems which allocate more storage to the time representation are not immune from this kind of error. Many Unix-like operating systems which keep time as seconds elapsed from the epoch date of January 1, 1970, and allot timekeeping enough storage to store numbers as large as 2 147 483 647 will experience an overflow problem on January 19, 2038 if not fixed beforehand. This is known as the Year 2038 problem. A correction involving doubling the storage allocated to timekeeping on these systems will allow them to represent dates more than 290 billion years into the future.
Other more subtle timekeeping problems exist in computing, such as accounting for leap seconds, which are not observed with any predictability or regularity. Additionally, applications which need to represent historical dates and times (for example, representing a date prior to the switch from the Julian calendar to the Gregorian calendar) must use specialized timekeeping libraries.
Finally, some software must maintain compatibility with older software that does not keep time in strict accordance with traditional timekeeping systems. For example, Microsoft Excel observes the fictional date of February 29, 1900 in order to maintain compatibility with older versions of Lotus 1-2-3. Lotus 1-2-3 observed the date due to an error; by the time the error was discovered, it was too late to fix it—"a change now would disrupt formulas which were written to accommodate this anomaly".
Epoch in satellite-based time systems.
There are at least six satellite navigation systems, all of which function by transmitting time signals. Of the only two satellite systems with global coverage, GPS calculates its time signal from an epoch, whereas GLONASS calculates time as an offset from UTC, with the UTC input adjusted for leap seconds. Of the only two other systems aiming for global coverage, Galileo calculates from an epoch and Beidou calculates from UTC without adjustment for leap seconds. GPS also transmits the offset between UTC time and GPS time, and must update this offset every time there is a leap second, requiring GPS receiving devices to handle the update correctly. In contrast, leap seconds are transparent to GLONASS users.
The complexities of calculating UTC from an epoch are explained by the European Space Agency in Galileo documentation under "Equations to correct system timescale to reference timescale" 

</doc>
<doc id="41115" url="http://en.wikipedia.org/wiki?curid=41115" title="Equivalent noise resistance">
Equivalent noise resistance

In telecommunication, an equivalent noise resistance is a quantitative representation in resistance units of the spectral density of a noise-voltage generator, given by
formula_1
where formula_2 is the spectral density, formula_3 is the Boltzmann's constant, formula_4 is the standard noise temperature (290 K), so formula_5.
"Note:" The equivalent noise resistance in terms of the mean-square noise-generator voltage, "e"2, within a frequency increment, Δ "f", is given by
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41116" url="http://en.wikipedia.org/wiki?curid=41116" title="Equivalent pulse code modulation noise">
Equivalent pulse code modulation noise

In telecommunication, equivalent pulse code modulation noise (PCM) is the amount of thermal noise power on a frequency-division multiplexing (FDM) or wire channel necessary to approximate the same judgment of speech quality created by quantizing noise in a PCM channel. 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41118" url="http://en.wikipedia.org/wiki?curid=41118" title="Error">
Error

The word error entails different meanings and usages relative to how it is conceptually applied. The concrete meaning of the Latin word "error" is "wandering" or "straying". Unlike an illusion, an error or a mistake can sometimes be dispelled through knowledge (knowing that one is looking at a mirage and not at real water does not make the mirage disappear). For example, a person who uses too much of an ingredient in a recipe and has a failed product can learn the right amount to use and avoid repeating the mistake. However, some errors can occur even when individuals have the required knowledge to perform a task correctly. Examples include forgetting to collect change after buying chocolate from a vending machine, forgetting the original document after making photocopies, and forgetting to turn the gas off after cooking a meal. Some errors occur when an individual is distracted by something else.
Human behavior.
One reference differentiates between "error" and "mistake" as follows:
An 'error' is a deviation from accuracy or correctness. A 'mistake' is an error caused by a fault: the fault being misjudgment, carelessness, or forgetfulness. Now, say that I run a stop sign because I was in a hurry, and wasn't concentrating, and the police stop me, that is a mistake. If, however, I try to park in an area with conflicting signs, and I get a ticket because I was incorrect on my interpretation of what the signs meant, that would be an error. The first time it would be an error. The second time it would be a mistake since I should have known better.
In human behavior the norms or expectations for behavior or its consequences can be derived from the intention of the actor or from the expectations of other individuals or from a social grouping or from social norms. (See deviance.) Gaffes and faux pas can be labels for certain instances of this kind of error. More serious departures from social norms carry labels such as misbehavior and labels from the legal system, such as misdemeanor and crime. Departures from norms connected to religion can have other labels, such as sin.
Oral and written language.
An individual language user's deviations from standard language norms in grammar, syntax, pronunciation and punctuation are sometimes referred to as errors. However in light of the role of language usage in everyday social class distinctions, many feel that linguistics should be descriptive rather than prescriptive to avoid reinforcing dominant class value judgments about what linguistic forms should and should not be used. One may distinguish various kinds of linguistic errors – some, such as aphasia or speech disorders, where the user is unable to say what they intend to, are generally considered errors, while cases where natural, intended speech is non-standard (as in dialects), are considered correct speech in descriptive linguistics, but errors in prescriptive linguistics. See also Error analysis (linguistics).
Gaffe.
A gaffe is a verbal mistake, usually made in a social environment. The mistake may come from saying something that is true, but inappropriate. It may also be an erroneous attempt to reveal a truth. Finally, gaffes can be malapropisms, grammatical errors or other verbal and gestural weaknesses or revelations through body language.
Actually revealing factual or social truth through words or body language, however, can commonly result in embarrassment or, when the gaffe has negative connotations, friction between people involved.
Philosophers and psychologists interested in the nature of the gaffe include Freud and Gilles Deleuze. Deleuze, in his "Logic of Sense", places the gaffe in a developmental process that can culminate in stuttering.
As used by some journalists, particularly sportswriters, "gaffe" becomes an imagined synonym for any kind of mistake, e.g., a dropped ball by a player in a baseball game.
Medicine.
See medical error for a description of error in medicine.
Science and engineering.
In statistics, an error (or "residual") is not a "mistake" but rather a difference between a computed, estimated, or measured value and the accepted true, specified, or theoretically correct value.
In science and engineering in general an error is defined as a difference between the desired and actual performance or behavior of a system or object. This definition is the basis of operation for many types of Control systems, in which error is defined as the difference between a set point and the process value. An example of this would be the thermostat in a home heating system—the operation of the heating equipment is controlled by the difference (the error) between the thermostat setting and the sensed air temperature. Another approach is related to considering a scientific hypothesis as true or false, giving birth to two types or errors: Type 1 and Type 2. The first one is when a true hypothesis is considered false, while the second is the reverse (a false one is considered true).
Engineers seek to design devices, machines and systems and in such a way as to mitigate or preferably avoid the effects of error, whether unintentional or not. Such errors in a system can be latent design errors that may go unnoticed for years, until the right set of circumstances arises that cause them to become active. Other errors in engineered systems can arise due to human error, which includes cognitive bias. Human factors engineering is often applied to designs in an attempt to minimize this type of error by making systems more forgiving or error-tolerant.
Numerical analysis.
Numerical analysis provides a variety of techniques to represent (store) and compute approximations to mathematical numerical values. Errors arise from a trade-off between efficiency (space and computation time) and precision, which is limited anyway, since (using common floating-point arithmetic) only a finite amount of values can be represented exactly. The discrepancy between the exact mathematical value and the stored/computed value is called the approximation error.
Cybernetics.
The word "cybernetics" stems from the Greek Κυβερνήτης ("kybernētēs", steersman, governor, pilot, or rudder — the same root as government). In applying corrections to the trajectory or course being steered cybernetics can be seen as the most general approach to error and its correction for the achievement of any goal. The term was suggested by Norbert Wiener to describe a new science of control and information in the animal and the machine. Wiener's early work was on noise.
The cybernetician Gordon Pask held that the error that drives a servomechanism can be seen as a difference between a pair of analogous concepts in a servomechanism: the current state and the goal state. Later he suggested error can also be seen as an innovation or a contradiction depending on the context and perspective of interacting (observer) participants. The founder of management cybernetics, Stafford Beer, applied these ideas most notably in his Viable System Model.
Biology.
In biology, an error is said to occur when perfect fidelity is lost in the copying of information. For example, in an asexually reproducing species, an error (or mutation) has occurred for each DNA nucleotide that differs between the child and the parent. Many of these mutations can be harmful, but unlike other types of errors, some are neutral or even beneficial. Mutations are an important force driving evolution. Mutations that make organisms more adapted to their environment increase in the population through natural selection as organisms with favorable mutations have more offspring.
Philately.
In philately, an error refers to a postage stamp or piece of postal stationery that exhibits a printing or production mistake that differentiates it from a normal specimen or from the intended result. Examples are stamps printed in the wrong color or missing one or more colors, printed with a vignette inverted in relation to its frame, produced without any perforations on one or more sides when the normal stamps are perforated, or printed on the wrong type of paper. Legitimate errors must always be produced and sold unintentionally. Such errors may or may not be scarce or rare. A design error may refer to a mistake in the design of the stamp, such as a mislabeled subject, even if there are no printing or production mistakes.
Law.
In appellate review, error typically refers to mistakes made by a trial court or some other court of first instance in applying the law in a particular legal case. This may involve such mistakes as improper admission of evidence, inappropriate instructions to the jury, or applying the wrong standard of proof.
Governmental policy.
Within United States government intelligence agencies, such as Central Intelligence Agency agencies, error refers to intelligence error, as previous assumptions that used to exist at a senior intelligence level within senior intelligence agencies, but has since been disproven, and is sometimes eventually listed as unclassified, and therefore more available to the American public and citizenry of the United States. The Freedom of information act provides American citizenry with a means to read intelligence reports that were mired in error. Per United States Central Intelligence Agency's website (as of August, 2008) intelligence error is described as:
"Intelligence errors are factual inaccuracies in analysis resulting from poor or missing data; intelligence failure is systemic organizational surprise resulting from incorrect, missing, discarded, or inadequate hypotheses."
Numismatics.
In numismatics, an error refers to a coin or medal that has a minting mistake, similar to errors found in philately. Because the U.S. Bureau of the Mint keeps a careful eye on all potential errors, errors on U.S. coins are very few and usually very scarce. Examples of numismatic errors: extra metal attached to a coin, a clipped coin caused by the coin stamp machine stamping a second coin too early, double stamping of a coin. A coin that has been overdated, e.g.: 1942/41, is also considered an error.

</doc>
<doc id="41119" url="http://en.wikipedia.org/wiki?curid=41119" title="Burst error">
Burst error

In telecommunication, a burst error or error burst is a contiguous sequence of symbols, received over a data transmission channel, such that the first and last symbols are in error and there exists no contiguous subsequence of "m" correctly received symbols within the error burst.
The integer parameter "m" is referred to as the guard band of the error burst. The last symbol in a burst and the first symbol in the following burst are accordingly separated by "m" correct bits or more. The parameter "m" should be specified when describing an error burst.
For example, imagine sending a packet containing all of the letters of the alphabet, A through Z. If the recipient's computer "opens" the packet and finds that the first letter in the sequence is "Q" and the last letter in the sequence is "R," that is a burst error. The "burst" of data in the packet is corrupt.
Although in the example the first and last letters are defined as corrupt, that does not mean that every letter within the packet is damaged. Imagine that every other letter is as it should be; only position one, "A," and position 26, "Z," have been damaged. The number of correct bits of information between the damaged ends is called the guard band. In this case, the guard band would be 24, because there are 24 correct letters separating the two damaged ones.
Channel model.
The Gilbert–Elliott model is a simple channel model introduced by Edgar Gilbert and E. O. Elliott widely used for describing burst error patterns in transmission channels, that enables simulations of the digital error performance of communications links. It is based on a Markov chain with two states "G" (for good or gap) and "B" (for bad or burst). In state "G" the probability of transmitting a bit correctly is "k" and in state "B" it is "h". Usually, it is assumed that "k" = 1 and, Gilbert also assumed that "h" = 0.5.
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41123" url="http://en.wikipedia.org/wiki?curid=41123" title="Escape character">
Escape character

In computing and telecommunication, an escape character is a character which invokes an alternative interpretation on subsequent characters in a character sequence. An escape character is a particular case of metacharacters. Generally, the judgement of whether something is "an escape character" or not depends on context.
Definition.
Escape characters are part of the syntax for many programming languages, data formats, and communication protocols. For a given alphabet an escape character's purpose is to start character sequences (so named escape sequences), which have to be interpreted differently from the same characters occurring without the prefixed escape character. An escape character may not have its own meaning, so all escape sequences are of two or more characters.
There are usually two functions of escape sequences. The first is to encode a syntactic entity, such as device commands or special data, which cannot be directly represented by the alphabet. The second use, referred to as "character quoting", is to represent characters, which cannot be typed in current context, or would have an undesired interpretation. In the latter case an escape sequence is a digraph consisting of an escape character itself and a "quoted" character.
Control character.
Generally, an escape character is not a particular case of (device) control characters, nor vice versa. If we define control characters as non-graphic, or as having a special meaning for an output device (e.g. printer or text terminal) then any escape character for this device is a control one. But escape characters used in programming (such as the backslash, "\") are graphic, hence are not control characters. Conversely most (but not all) of the ASCII "control characters" have some control function in isolation, therefore are not escape characters.
In many programming languages, an escape character also forms some escape sequences which are referred to control characters. For example, line break has an escape sequence of codice_1.
Examples.
JavaScript.
JavaScript uses the \ (backslash) as an escape characters for:
Note that the \v and \0 escapes are not allowed in JSON strings.
ASCII escape character.
The ASCII "escape" character (octal: \033, hexadecimal: \x1B, or ^[, or, in decimal, 27) is used in many output devices to start a series of characters called a control sequence or escape sequence. Typically, the escape character was sent first in such a sequence to alert the device that the following characters were to be interpreted as a control sequence rather than as plain characters, then one or more characters would follow to specify some detailed action, after which the device would go back to interpreting characters normally. For example, the sequence of ^[, followed by the printable characters codice_2, would cause a DEC VT102 terminal to move its cursor to the 10th cell of the 2nd line of the screen. This was later developed to ANSI escape codes covered by the ANSI X3.64 standard. The escape character also starts each command sequence in the Hewlett Packard Printer Command Language.
Early reference to the term "escape character" is found in Bob Bemer's IBM technical publications. Apparently, it is he who invented this mechanism, during his work on the ASCII character set.
The Escape key is usually found on standard PC keyboards. However it is commonly absent from keyboards for PDAs and other devices not designed primarily for ASCII communications. The DEC VT220 series was one of the few popular keyboards that did not have a dedicated Esc key, instead using one of the keys above the main keypad. In user interfaces of the 1970s–1980s it was not uncommon to use this key as an escape character, but in modern desktop computers such use is dropped. Sometimes the key was identified with AltMode (for alternative mode). Even with no dedicated key, the escape character code could be generated by typing '[' while simultaneously holding down the Control key, 'Ctrl'.
Programming and data formats.
Many modern programming languages specify the doublequote character (codice_3) as a delimiter for a string literal. The backslash (codice_4) escape character typically provides two ways to include doublequotes inside a string literal, either by modifying the meaning of the doublequote character embedded in the string (codice_5 becomes codice_3), or by modifying the meaning of a sequence of characters including the hexadecimal value of a doublequote character (codice_7 becomes codice_3).
C, C++, Java, and Ruby all allow exactly the same two backslash escape styles. The PostScript language and Microsoft Rich Text Format also use backslash escapes. The quoted-printable encoding uses the equals sign as an escape character.
URL and URI use %-escapes to quote characters with a special meaning, as for non-ASCII characters. The ampersand (codice_9) character may be considered as an escape character in SGML and derived formats such as HTML and XML.
Some programming languages also provide other ways to represent special characters in literals, without requiring an escape character (see e.g. delimiter collision).
Communication protocols.
The Point-to-Point Protocol uses the 0x7D octet (\175, or ASCII: } ) as an escape character. The octet immediately following should be XORed by 0x20 before being passed to a higher level protocol. This is applied to both 0x7D itself and the control character 0x7E (which is used in PPP to mark the beginning and end of a frame) when those octets need to be transmitted by a higher level protocol encapsulated by PPP, as well as other octets negotiated when the link is established. That is, when a higher level protocol wishes to transmit 0x7D, it is transmitted as the sequence 0x7D 0x5D, and 0x7E is transmitted as 0x7D 0x5E.
Bourne shell.
In Bourne shell (sh), the asterisk (codice_10) and question mark (codice_11) characters are wildcard characters expanded via globbing. Without a preceding escape character, an codice_10 will expand to the names of all files in the working directory that don't start with a period iff there are such files, otherwise codice_10 remains unexpanded. So to refer to a file literally called "*", the shell must be told not to interpret it in this way, by preceding it with a backslash (codice_4). This modifies the interpretation of the asterisk (codice_10). Compare:
Windows Command Prompt.
The Windows command-line interpreter uses a caret character (codice_16) to escape reserved characters that have special meanings (in particular: codice_17). The DOS command-line interpreter, though it supports similar syntax, does not support this.
For example, on the Windows Command Prompt, this will result in a syntax error.
whereas this will output the string: codice_18
See also.
Not to be confused with:
External links.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41124" url="http://en.wikipedia.org/wiki?curid=41124" title="Essential service (telecommunications)">
Essential service (telecommunications)

In telecommunication, an essential service (critical service) is a network-provided service feature in which a priority dial tone is furnished. Essential service is typically provided to fewer than 10% of network users, and recommended for use in conjunction with NS/EP telecommunications services.

</doc>
<doc id="41125" url="http://en.wikipedia.org/wiki?curid=41125" title="Exchange">
Exchange

Exchange may refer to:

</doc>
<doc id="41126" url="http://en.wikipedia.org/wiki?curid=41126" title="Exempted addressee">
Exempted addressee

In telecommunication, an exempted addressee is an organization, activity, or person included in the collective address group of a message and deemed by the message originator as having no need for the information in the message. 
Exempted addressees may be explicitly excluded from the collective address group for the particular message to which the exemption applies.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41128" url="http://en.wikipedia.org/wiki?curid=41128" title="Extended superframe">
Extended superframe

In telecommunications, an extended superframe (ESF) is a T1 framing standard, sometimes called D5 framing because it was first used in the D5 channel bank, invented in the 1980s. It is preferred to its predecessor, superframe, because it includes a cyclic redundancy check (CRC) and 4000 bit/s channel capacity for a data link channel (used to pass out-of-band data between equipment.) It requires less frequent synchronization than the earlier superframe or D-4 format, and provides on-line, real-time testing of circuit capability and operating condition. 
In ESF, a superframe is 24 frames long, and the 193rd bit of each superframe is used in the following manner:
Note: Less-frequent synchronization frees overhead bits for use in testing and .

</doc>
<doc id="41130" url="http://en.wikipedia.org/wiki?curid=41130" title="Extinction ratio">
Extinction ratio

In telecommunications, extinction ratio ("r"e) is the ratio of two optical power levels of a digital signal generated by an optical source, "e.g.," a laser diode. The extinction ratio may be expressed as a fraction, in dB, or as a percentage. It may be given by
where "P"1 is the optical power level generated when the light source is on, and "P"0 is the power level generated when the light source is off.
<br>
The polarization extinction ratio (PER) is the ratio of optical powers of perpendicular polarizations, usually called TE (transverse electrical) and TM (transverse magnetic). In telecommunications, the PER is used to characterize the degree of polarization in a polarization maintaining device or fiber. For coherent transmitter and receiver, the PER is a key parameter since X-polarization and Y-polarization are coded with different signals.

</doc>
<doc id="41131" url="http://en.wikipedia.org/wiki?curid=41131" title="Eye pattern">
Eye pattern

In telecommunication, an eye pattern, also known as an eye diagram, is an oscilloscope display in which a digital data signal from a receiver is repetitively sampled and applied to the vertical input, while the data rate is used to trigger the horizontal sweep. It is so called because, for several types of coding, the pattern looks like a series of eyes between a pair of rails. It is an experimental tool for the evaluation of the combined effects of channel noise and intersymbol interference on the performance of a baseband pulse-transmission system. It is the synchronised superposition of all possible realisations of the signal of interest viewed within a particular signalling interval.
Several system performance measures can be derived by analyzing the display. If the signals are too long, too short, poorly synchronized with the system clock, too high, too low, too noisy, or too slow to change, or have too much undershoot or overshoot, this can be observed from the eye diagram. An open eye pattern corresponds to minimal signal distortion. Distortion of the signal waveform due to intersymbol interference and noise appears as closure of the eye pattern.
Measurements.
There are many measurements that can be obtained from an Eye Diagram: 
Amplitude Measurements
Time Measurements

</doc>
<doc id="41132" url="http://en.wikipedia.org/wiki?curid=41132" title="Telecommunications facility">
Telecommunications facility

In telecommunications, a facility is defined by Federal Standard 1037C as:
In Canada.
Under Canadian federal and Québécois provincial law, a telecommunications facility, for the purposes of determining whether GST applies, is defined by §123(1) of the GST Act to be "any facility, apparatus, or other thing (including any wire, cable, radio, optical, or other electromagnetic system, or any similar technical system or any part thereof) that is used or is capable of being used for telecommunications". This is a very broad definition that includes a wide range of things from satellites and earth stations, to telephones and fax machines. The consequence of its application is that even a simple LAN connector jack can be considered to be a telecommunications facility in Canada, for tax purposes.

</doc>
<doc id="41133" url="http://en.wikipedia.org/wiki?curid=41133" title="Facsimile converter">
Facsimile converter

In telecommunication, the term facsimile converter has the following meanings: 
1. In a facsimile receiver, a device that changes the signal modulation from frequency-shift keying (FSK) to amplitude modulation (AM). 
2. In a facsimile transmitter, a device that changes the signal modulation from amplitude modulation (AM) to frequency-shift keying (FSK).
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41134" url="http://en.wikipedia.org/wiki?curid=41134" title="Fade margin">
Fade margin

In telecommunication, the term fade margin (fading margin) has the following meanings: 
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41135" url="http://en.wikipedia.org/wiki?curid=41135" title="Fading distribution">
Fading distribution

In telecommunications, a fading distribution is the probability distribution of the value of signal fading relative to a specified reference level.
In the case of phase interference fading, the time distribution of the instantaneous field strength usually approximates a Rayleigh distribution when several signal components of equal amplitude are present. 
The field strength is usually measured in volts per meter. 
The fading distribution may also be measured in terms of power level, where the unit of measure is usually watts per square meter and the expression is in decibels.

</doc>
<doc id="41136" url="http://en.wikipedia.org/wiki?curid=41136" title="Fail-safe">
Fail-safe

A fail-safe or fail-secure device is one that, in the event of a specific type of failure, responds in a way that will cause no harm, or at least a minimum of harm, to other devices or danger to personnel.
Fail-safe and fail-secure are similar but distinct concepts. Fail-safe means that a device will not endanger lives or property when it fails. Fail-secure means that access or data will not fall into the wrong hands in a failure. Sometimes the approaches suggest opposite solutions. For example, if a building catches fire, fail-safe systems would unlock doors to ensure quick escape and allow firefighters inside, while fail-secure would lock doors to prevent unauthorized access to the building.
Significantly, a system's being "fail-safe" means not that failure is impossible/improbable, but rather that the system's design prevents or mitigates unsafe consequences of the system's failure. That is, if and when a "fail-safe" system "fails", it is "safe" or at least no less safe than when it is operating correctly.
Since many types of failure are possible, it must be specified to what failure a component is fail safe. For example, a system may be fail-safe in the event of a power outage (electrical failure), but may not be fail safe in the event of mechanical failures.
Examples.
Mechanical or physical.
Examples include:
Electrical or electronic.
Examples include:
Procedural.
As well as physical devices and systems fail-safe procedures can be created so that if a procedure is not carried out or carried out incorrectly no dangerous action results. For example:
Other terminology.
Fail-safe (foolproof) devices are also known as "poka-yoke" devices. "Poka-yoke", a Japanese term, was coined by Shigeo Shingo, a quality expert. "Safe to fail" refers to civil engineering designs such as the Room for the River project in Netherlands and the Thames Estuary 2100 Plan which incorporate flexible adaptation strategies or climate change adaptation which provide for, and limit, damage, should severe events such as 500-year floods occur.

</doc>
<doc id="41138" url="http://en.wikipedia.org/wiki?curid=41138" title="Fall time">
Fall time

In electronics, fall time (pulse decay time) formula_1 is the time taken for the amplitude of a pulse to decrease (fall) from a specified value (usually 90% of the peak value exclusive of overshoot or undershoot) to another specified value (usually 10% of the maximum value exclusive of overshoot or undershoot). 
Limits on undershoot and oscillation (also known as ringing and hunting) are sometimes additionally stated when specifying fall time limits.

</doc>
<doc id="41142" url="http://en.wikipedia.org/wiki?curid=41142" title="Fast packet switching">
Fast packet switching

In telecommunications, fast packet switching is a variant of packet switching that increases the throughput by eliminating overhead associated with flow control and error correction functions, which are either offloaded to upper layer networking protocols or removed altogether. ATM and Frame Relay are two major implementations of fast packet switching.

</doc>
<doc id="41143" url="http://en.wikipedia.org/wiki?curid=41143" title="Fault">
Fault

Fault may refer to:

</doc>
<doc id="41144" url="http://en.wikipedia.org/wiki?curid=41144" title="Fault management">
Fault management

In network management, fault management is the set of functions that detect, isolate, and correct malfunctions in a telecommunications network, compensate for environmental changes, and include maintaining and examining error logs, accepting and acting on error detection notifications, tracing and identifying faults, carrying out sequences of diagnostics tests, correcting faults, reporting error conditions, and localizing and tracing faults by examining and manipulating database information.
When a fault or event occurs, a network component will often send a notification to the network operator using a protocol such as SNMP. An alarm is a persistent indication of a fault that clears only when the triggering condition has been resolved. A current list of problems occurring on the network component is often kept in the form of an active alarm list such as is defined in RFC 3877, the Alarm MIB. A list of cleared faults is also maintained by most network management systems.
Fault management systems may use complex filtering systems to assign alarms to severity levels. These can range in severity from debug to emergency, as in the syslog protocol. Alternatively, they could use the ITU X.733 Alarm Reporting Function's perceived severity field. This takes on values of cleared, indeterminate, critical, major, minor or warning. Note that the latest version of the syslog protocol draft under development within the IETF includes a mapping between these two different sets of severities. It is considered good practice to send a notification not only when a problem has occurred, but also when it has been resolved. The latter notification would have a severity of clear.
A fault management console allows a network administrator or system operator to monitor events from multiple systems and perform actions based on this information. Ideally, a fault management system should be able to correctly identify events and automatically take action, either launching a program or script to take corrective action, or activating notification software that allows a human to take proper intervention (i.e. send e-mail or SMS text to a mobile phone). Some notification systems also have escalation rules that will notify a chain of individuals based on availability and severity of alarm.
Types.
There are two primary ways to perform fault management - these are active and passive. Passive fault management is done by collecting alarms from devices (normally via SNMP(simple network management protocol)) when something happens in the devices. In this mode, the fault management system only knows if a device it is monitoring is intelligent enough to generate an error and report it to the management tool. However, if the device being monitored fails completely or locks up, it won't throw an alarm and the problem will not be detected. Active fault management addresses this issue by actively monitoring devices via tools such as ping to determine if the device is active and responding. If the device stops responding, active monitoring will throw an alarm showing the device as unavailable and allows for the proactive correction of the problem.
Fault management includes any tools or procedure for testing, diagnosing or repairing the network when a failure occurs.

</doc>
<doc id="41145" url="http://en.wikipedia.org/wiki?curid=41145" title="FCC (disambiguation)">
FCC (disambiguation)

FCC may refer to:

</doc>
<doc id="41146" url="http://en.wikipedia.org/wiki?curid=41146" title="FCC registration program">
FCC registration program

In telecommunication, FCC registration program is the Federal Communications Commission (FCC) program and associated directives intended to assure that all connected terminal equipment and protective circuitry will not harm the public switched telephone network or certain private line services. 
"Note 1:" The FCC registration program requires the registering of terminal equipment and protective circuitry in accordance with Subpart C of part 68, Title 47 of the "Code of Federal Regulations." This includes the assignment of identification numbers to the equipment and the testing of the equipment. 
"Note 2:" The FCC registration program contains no requirement that accepted terminal equipment be compatible with, or function with, the network. 
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41147" url="http://en.wikipedia.org/wiki?curid=41147" title="Feed">
Feed

Feed or The Feed may refer to:

</doc>
<doc id="41148" url="http://en.wikipedia.org/wiki?curid=41148" title="Optical amplifier">
Optical amplifier

An optical amplifier is a device that amplifies an optical signal directly, without the need to first convert it to an electrical signal. An optical amplifier may be thought of as a laser without an optical cavity, or one in which feedback from the cavity is suppressed. Optical amplifiers are important in optical communication and laser physics.
There are several different physical mechanisms that can be used to amplify a light signal, which correspond to the major types of optical amplifiers. In doped fibre amplifiers and bulk lasers, stimulated emission in the amplifier's gain medium causes amplification of incoming light. In semiconductor optical amplifiers (SOAs), electron-hole recombination occurs. In Raman amplifiers, Raman scattering of incoming light with phonons in the lattice of the gain medium produces photons coherent with the incoming photons. Parametric amplifiers use parametric amplification.
Laser amplifiers.
Almost any laser active gain medium can be pumped to produce gain for light at the wavelength of a laser made with the same material as its gain medium. Such amplifiers are commonly used to produce high power laser systems. Special types such as regenerative amplifiers and chirped-pulse amplifiers are used to amplify ultrashort pulses.
Doped fiber amplifiers.
Doped fibre amplifiers (DFAs) are optical amplifiers that use a doped optical fibre as a gain medium to amplify an optical signal. They are related to fibre lasers. The signal to be amplified and a pump laser are multiplexed into the doped fibre, and the signal is amplified through interaction with the doping ions. The most common example is the Erbium Doped Fibre Amplifier (EDFA), where the core of a silica fibre is doped with trivalent erbium ions and can be efficiently pumped with a laser at a wavelength of 980 nm or 1,480 nm, and exhibits gain in the 1,550 nm region.
An "erbium-doped waveguide amplifier" ("EDWA") is an optical amplifier that uses a waveguide to boost an optical signal.
Amplification is achieved by stimulated emission of photons from dopant ions in the doped fibre. The pump laser excites ions into a higher energy from where they can decay via stimulated emission of a photon at the signal wavelength back to a lower energy level. The excited ions can also decay spontaneously (spontaneous emission) or even through nonradiative processes involving interactions with phonons of the glass matrix. These last two decay mechanisms compete with stimulated emission reducing the efficiency of light amplification.
The "amplification window" of an optical amplifier is the range of optical wavelengths for which the amplifier yields a usable gain. The amplification window is determined by the spectroscopic properties of the dopant ions, the glass structure of the optical fibre, and the wavelength and power of the pump laser.
Although the electronic transitions of an isolated ion are very well defined, broadening of the energy levels occurs when the ions are incorporated into the glass of the optical fibre and thus the amplification window is also broadened. This broadening is both homogeneous (all ions exhibit the same broadened spectrum) and inhomogeneous (different ions in different glass locations exhibit different spectra). Homogeneous broadening arises from the interactions with phonons of the glass, while inhomogeneous broadening is caused by differences in the glass sites where different ions are hosted. Different sites expose ions to different local electric fields, which shifts the energy levels via the Stark effect. In addition, the Stark effect also removes the degeneracy of energy states having the same total angular momentum (specified by the quantum number J). Thus, for example, the trivalent erbium ion (Er+3) has a ground state with J = 15/2, and in the presence of an electric field splits into J + 1/2 = 8 sublevels with slightly different energies. The first excited state has J = 13/2 and therefore a Stark manifold with 7 sublevels. Transitions from the J = 13/2 excited state to the J= 15/2 ground state are responsible for the gain at 1.5 µm wavelength. The gain spectrum of the EDFA has several peaks that are smeared by the above broadening mechanisms. The net result is a very broad spectrum (30 nm in silica, typically). The broad gain-bandwidth of fibre amplifiers make them particularly useful in 
wavelength-division multiplexed communications systems as a single amplifier can be utilized to amplify all signals being carried on a fibre and whose wavelengths fall within the gain window.
Basic principle of EDFA.
A relatively high-powered beam of light is mixed with the input signal using a wavelength selective coupler. The input signal and the excitation light must be at significantly different wavelengths.
The mixed light is guided into a section of fibre with erbium ions included in the core.
This high-powered light beam excites the erbium ions to their higher-energy state.
When the photons belonging to the signal at a different wavelength from the pump light meet the excited erbium atoms, the erbium atoms give up some of their energy to the signal and return to their lower-energy state.
A significant point is that the erbium gives up its energy in the form of additional photons which are exactly in the same phase and direction as the signal being amplified. So the signal is amplified along its direction of travel only. This is not unusual - when an atom “lases” it always gives up its energy in the same direction and phase as the incoming light. Thus all of the additional signal power is guided in the same fibre mode as the incoming signal.There is usually an isolator placed at the output to prevent reflections returning from the attached fibre. Such reflections disrupt amplifier operation and in the extreme case can cause the amplifier to become a laser. The erbium doped amplifier is a high gain amplifier.
Noise.
The principal source of noise in DFAs is Amplified Spontaneous Emission (ASE), which has a spectrum approximately the same as the gain spectrum of the amplifier. Noise figure in an ideal DFA is 3 dB, while practical amplifiers can have noise figure as large as 6–8 dB.
As well as decaying via stimulated emission, electrons in the upper energy level can also decay by spontaneous emission, which occurs at random, depending upon the glass structure and inversion level. Photons are emitted spontaneously in all directions, but a proportion of those will be emitted in a direction that falls within the numerical aperture of the fibre and are thus captured and guided by the fibre. Those photons captured may then interact with other dopant ions, and are thus amplified by stimulated emission. The initial spontaneous emission is therefore amplified in the same manner as the signals, hence the term "Amplified Spontaneous Emission". ASE is emitted by the amplifier in both the forward and reverse directions, but only the forward ASE is a direct concern to system performance since that noise will co-propagate with the signal to the receiver where it degrades system performance. Counter-propagating ASE can, however, lead to degradation of the amplifier's performance since the ASE can deplete the inversion level and thereby reduce the gain of the amplifier.
Gain saturation.
Gain is achieved in a DFA due to population inversion of the dopant ions. The inversion level of a DFA is set, primarily, by the power of the pump wavelength and the power at the amplified wavelengths. As the signal power increases, or the pump power decreases, the inversion level will reduce and thereby the gain of the amplifier will be reduced. This effect is known as gain saturation – as the signal level increases, the amplifier saturates and cannot produce any more output power, and therefore the gain reduces. Saturation is also commonly known as gain compression.
To achieve optimum noise performance DFAs are operated under a significant amount of gain compression (10 dB typically), since that reduces the rate of spontaneous emission, thereby reducing ASE. Another advantage of operating the DFA in the gain saturation region is that small fluctuations in the input signal power are reduced in the output amplified signal: smaller input signal powers experience larger (less saturated) gain, while larger input powers see less gain.
The leading edge of the pulse is amplified, until the saturation energy of the gain medium is reached. In some condition, the width (FWHM) of the pulse is reduced.
Inhomogeneous broadening effects.
Due to the inhomogeneous portion of the linewidth broadening of the dopant ions, the gain spectrum has an inhomogeneous component and gain saturation occurs, to a small extent, in an inhomogeneous manner. This effect is known as "spectral hole burning" because a high power signal at one wavelength can 'burn' a hole in the gain for wavelengths close to that signal by saturation of the inhomogeneously broadened ions. Spectral holes vary in width depending on the characteristics of the optical fibre in question and the power of the burning signal, but are typically less than 1 nm at the short wavelength end of the C-band, and a few nm at the long wavelength end of the C-band. The depth of the holes are very small, though, making it difficult to observe in practice.
Polarization effects.
Although the DFA is essentially a polarization independent amplifier, a small proportion of the dopant ions interact preferentially with certain polarizations and a small dependence on the polarization of the input signal may occur (typically < 0.5 dB). This is called Polarization Dependent Gain (PDG). 
The absorption and emission cross sections of the ions can be modeled as ellipsoids with the major axes aligned at random in all directions in different glass sites. The random distribution of the orientation of the ellipsoids in a glass produces a macroscopically isotropic medium, but a strong pump laser induces an anisotropic distribution by selectively exciting those ions that are more aligned with the optical field vector of the pump. Also, those excited ions aligned with the signal field produce more stimulated emission. The change in gain is thus dependent on the alignment of the polarizations of the pump and signal lasers – i.e. whether the two lasers are interacting with the same sub-set of dopant ions or not. 
In an ideal doped fibre without birefringence, the PDG would be inconveniently large. Fortunately, in optical fibres small amounts of birefringence are always present and, furthermore, the fast and slow axes vary randomly along the fibre length. A typical DFA has several tens of meters, long enough to already show this randomness of the birefringence axes. These two combined effects (which in transmission fibres give rise to polarization mode dispersion) produce a misalignment of the relative polarizations of the signal and pump lasers along the fibre, thus tending to average out the PDG. The result is that PDG is very difficult to observe in a single amplifier (but is noticeable in links with several cascaded amplifiers).
Erbium-doped optical fibre amplifiers.
The erbium-doped fibre amplifier (EDFA) is the most deployed fibre amplifier as its amplification window coincides with the third transmission window of silica-based optical fibre.
Two bands have developed in the third transmission window – the "Conventional", or C-band, from approximately 1525 nm – 1565 nm, and the "Long", or L-band, from approximately 1570 nm to 1610 nm. Both of these bands can be amplified by EDFAs, but it is normal to use two different amplifiers, each optimized for one of the bands.
The principal difference between C- and L-band amplifiers is that a longer length of doped fibre is used in L-band amplifiers. The longer length of fibre allows a lower inversion level to be used, thereby giving at longer wavelengths (due to the band-structure of Erbium in silica) while still providing a useful amount of gain.
EDFAs have two commonly-used pumping bands – 980 nm and 1480 nm. The 980 nm band has a higher absorption cross-section and is generally used where low-noise performance is required. The absorption band is relatively narrow and so wavelength stabilised laser sources are typically needed. The 1480 nm band has a lower, but broader, absorption cross-section and is generally used for higher power amplifiers. A combination of 980 nm and 1480 nm pumping is generally utilised in amplifiers.
The optical fibre amplifier was invented by H. J. Shaw and Michel Digonnet at Stanford University, California, in the early 1980s. The EDFA was first demonstrated several years later by a group including David N. Payne, R. Mears, I.M Jauncey and L. Reekie, from the University of Southampton and a group from AT&T Bell Laboratories, E. Desurvire, P. Becker, and J. Simpson. The dual-stage optical amplifier which enabled Dense Wave Division Multiplexing (DWDM,) was invented by Stephan B. Alexander at Ciena Corporation. 
Doped fibre amplifiers for other wavelength ranges.
Thulium doped fibre amplifiers have been used in the S-band (1450–1490 nm) and Praseodymium doped amplifiers in the 1300 nm region. However, those regions have not seen any significant commercial use so far and so those amplifiers have not been the subject of as much development as the EDFA. However, Ytterbium doped fibre lasers and amplifiers, operating near 1 micrometre wavelength, have many applications in industrial processing of materials, as these devices can be made with extremely high output power (tens of kilowatts).
Semiconductor optical amplifier.
Semiconductor optical amplifiers (SOAs) are amplifiers which use a semiconductor to provide the gain medium. These amplifiers have a similar structure to Fabry–Pérot laser diodes but with anti-reflection design elements at the end faces. Recent designs include anti-reflective coatings and tilted wave guide and window regions which can reduce end face reflection to less than 0.001%. Since this creates a loss of power from the cavity which is greater than the gain, it prevents the amplifier from acting as a laser. Another type of SOA consists of two regions. One part has a structure of a Fabry-Pérot laser diode and the other has a tapered geometry in order to reduce the power density on the output facet.
Semiconductor optical amplifiers are typically made from group III-V compound semiconductors such as GaAs/AlGaAs, InP/InGaAs, InP/InGaAsP and InP/InAlGaAs, though any direct band gap semiconductors such as II-VI could conceivably be used. Such amplifiers are often used in telecommunication systems in the form of fibre-pigtailed components, operating at signal wavelengths between 0.85 µm and 1.6 µm and generating gains of up to 30 dB.
The semiconductor optical amplifier is of small size and electrically pumped. It can be potentially less expensive than the EDFA and can be integrated with semiconductor lasers, modulators, etc. However, the performance is still not comparable with the EDFA. The SOA has higher noise, lower gain, moderate polarization dependence and high nonlinearity with fast transient time. The main advantage of SOA is that all four types of nonlinear operations (cross gain modulation, cross phase modulation, wavelength conversion and four wave mixing) can be conducted. Furthermore, SOA can be run with a low power laser.
This originates from the short nanosecond or less upper state lifetime, so that the gain reacts rapidly to changes of pump or signal power and the changes of gain also cause phase changes which can distort the signals.
This nonlinearity presents the most severe problem for optical communication applications. However it provides the possibility for gain in different wavelength regions from the EDFA. "Linear optical amplifiers" using gain-clamping techniques have been developed.
High optical nonlinearity makes semiconductor amplifiers attractive for all optical signal processing like all-optical switching and wavelength conversion. There has been much research on semiconductor optical amplifiers as elements for optical signal processing, wavelength conversion, clock recovery, signal demultiplexing, and pattern recognition.
Vertical-cavity SOA.
A recent addition to the SOA family is the vertical-cavity SOA (VCSOA). These devices are similar in structure to, and share many features with, vertical-cavity surface-emitting lasers (VCSELs). The major difference when comparing VCSOAs and VCSELs is the reduced mirror reflectivities used in the amplifier cavity. With VCSOAs, reduced feedback is necessary to prevent the device from reaching lasing threshold. Due to the extremely short cavity length, and correspondingly thin gain medium, these devices exhibit very low single-pass gain (typically on the order of a few percent) and also a very large free spectral range (FSR). The small single-pass gain requires relatively high mirror reflectivities to boost the total signal gain. In addition to boosting the total signal gain, the use of the resonant cavity structure results in a very narrow gain bandwidth; coupled with the large FSR of the optical cavity, this effectively limits operation of the VCSOA to single-channel amplification. Thus, VCSOAs can be seen as amplifying filters.
Given their vertical-cavity geometry, VCSOAs are resonant cavity optical amplifiers that operate with the input/output signal entering/exiting normal to the wafer surface. In addition to their small size, the surface normal operation of VCSOAs leads to a number of advantages, including low power consumption, low noise figure, polarization insensitive gain, and the ability to fabricate high fill factor two-dimensional arrays on a single semiconductor chip. These devices are still in the early stages of research, though promising preamplifier results have been demonstrated. Further extensions to VCSOA technology are the demonstration of wavelength tunable devices. These MEMS-tunable vertical-cavity SOAs utilize a microelectromechanical systems (MEMS) based tuning mechanism for wide and continuous tuning of the peak gain wavelength of the amplifier. SOAs have a more rapid gain response, which is in the order of 1 to 100 ps.
Tapered amplifiers.
For high output power and broader wavelength range, tapered amplifiers are used. These amplifiers consist of a lateral single-mode section and a section with a tapered structure, where the laser light is amplified. The tapered structure leads to a reduction of the power density at the output facett.
Typical parameters:
Raman amplifier.
In a Raman amplifier, the signal is intensified by Raman amplification. Unlike the EDFA and SOA the amplification effect is achieved by a nonlinear interaction between the signal and a pump laser within an optical fibre. There are two types of Raman amplifier: distributed and lumped. A distributed Raman amplifier is one in which the transmission fibre is utilised as the gain medium by multiplexing a pump wavelength with signal wavelength, while a lumped Raman amplifier utilises a dedicated, shorter length of fibre to provide amplification. In the case of a lumped Raman amplifier highly nonlinear fibre with a small core is utilised to increase the interaction between signal and pump wavelengths and thereby reduce the length of fibre required.
The pump light may be coupled into the transmission fibre in the same direction as the signal (co-directional pumping), in the opposite direction (contra-directional pumping) or both. Contra-directional pumping is more common as the transfer of noise from the pump to the signal is reduced.
The pump power required for Raman amplification is higher than that required by the EDFA, with in excess of 500 mW being required to achieve useful levels of gain in a distributed amplifier. Lumped amplifiers, where the pump light can be safely contained to avoid safety implications of high optical powers, may use over 1 W of optical power.
The principal advantage of Raman amplification is its ability to provide distributed amplification within the transmission fibre, thereby increasing the length of spans between amplifier and regeneration sites. The amplification bandwidth of Raman amplifiers is defined by the pump wavelengths utilised and so amplification can be provided over wider, and different, regions than may be possible with other amplifier types which rely on dopants and device design to define the amplification 'window'.
Raman amplifiers have some fundamental advantages. First, Raman gain exists in every fiber, which provides a cost-effective means of upgrading from the terminal ends. Second, the gain is nonresonant, which means that gain is available over the entire transparency region of the fiber ranging from approximately 0.3 to 2μm. A third advantage of Raman amplifiers is that the gain spectrum can be tailored by adjusting the pump wavelengths. For instance, multiple pump lines can be used to increase the optical bandwidth, and the pump distribution determines the gain flatness. Another advantage of Raman amplification is that it is a relatively broad-band amplifier with a bandwidth > 5 THz, and the gain is reasonably flat over a wide wavelength range.
However, a number of challenges for Raman amplifiers prevented their earlier adoption. First, compared to the EDFAs, Raman amplifiers have relatively poor pumping efficiency at lower signal powers. Although a disadvantage, this lack of pump efficiency also makes gain clamping easier in Raman amplifiers. Second, Raman amplifiers require a longer gain fiber. However, this disadvantage can be mitigated by combining gain and the dispersion compensation in a single fiber. A third disadvantage of Raman amplifiers is a fast response time, which gives rise to new sources of noise, as further discussed below. Finally, there are concerns of nonlinear penalty in the amplifier for the WDM signal channels.
"Note: The text of an earlier version of this article was taken from the public domain Federal Standard 1037C."
Optical parametric amplifier.
An optical parametric amplifier allows the amplification of a weak signal-impulse in a noncentrosymmetric nonlinear medium (e.g. Beta barium borate (BBO)). In contrast to the previously mentioned amplifiers, which are mostly used in telecommunication environments, this type finds its main application in expanding the frequency tunability of ultrafast solid-state lasers (e.g. Ti:sapphire). By using a noncollinear interaction geometry optical parametric amplifiers are capable of extremely broad amplification bandwidths.
Recent achievements.
The adoption of high power fiber lasers as an industrial material processing tool has been ongoing for several years and is now expanding into other markets including the medical and scientific markets. One key enhancement enabling penetration into the scientific market has been the improvements in high finesse fiber amplifiers, which are now capable of delivering single frequency linewidths (<5 kHz) together with excellent beam quality and stable linearly polarized output. Systems meeting these specifications, have steadily progressed in the last few years from a few Watts of output power, initially to the 10s of Watts and now into the 100s of Watts power level. This power scaling has been achieved with developments in the fiber technology, such as the adoption of stimulated brillouin scattering (SBS) suppression/mitigation techniques within the fiber, along with improvements in the overall amplifier design. The latest generation of high finesse, high power fiber amplifiers now deliver power levels exceeding what is available from commercial solid-state single frequency sources and are opening up new scientific applications as a result of the higher power levels and stable optimized performance.
Implementations.
There are several simulation tools that can be used to design optical amplifiers. Popular commercial tools have been developed by Optiwave Systems and VPI Systems.

</doc>
<doc id="41149" url="http://en.wikipedia.org/wiki?curid=41149" title="Fiber Distributed Data Interface">
Fiber Distributed Data Interface

Fiber Distributed Data Interface (FDDI) is a standard for data transmission in a local area network.
It uses optical fiber as its standard underlying physical medium, although it was also later specified to use copper cable, in which case it may be called CDDI (Copper Distributed Data Interface), standardized as TP-PMD (Twisted-Pair Physical Medium-Dependent), also referred to as TP-DDI (Twisted-Pair Distributed Data Interface). 
Description.
FDDI provides a 100 Mbit/s optical standard for data transmission in local area network that can extend in range up to 200 km. Although FDDI logical topology is a ring-based token network, it did not use the IEEE 802.5 token ring protocol as its basis; instead, its protocol was derived from the IEEE 802.4 token bus "timed token" protocol. In addition to covering large geographical areas, FDDI local area networks can support thousands of users. FDDI offers both a Dual-Attached Station (DAS), counter-rotating token ring topology and a Single-Attached Station (SAS), token bus passing ring topology.
FDDI, as a product of American National Standards Institute X3T9.5 (now X3T12), conforms to the Open Systems Interconnection (OSI) model of functional layering using other protocols. The standards process started in the mid 1980s.
FDDI-II, a version of FDDI described in 1989, added circuit-switched service capability to the network so that it could also handle voice and video signals. Work started to connect FDDI networks to synchronous optical networking (SONET) technology.
A FDDI network contains two rings, one as a secondary backup in case the primary ring fails. The primary ring offers up to 100 Mbit/s capacity. When a network has no requirement for the secondary ring to do backup, it can also carry data, extending capacity to 200 Mbit/s. The single ring can extend the maximum distance; a dual ring can extend 100 km. FDDI had a larger maximum-frame size (4,352 bytes) than the standard Ethernet family, which only supports a maximum-frame size of 1,500 bytes, allowing better effective data rates in some cases.
Topology.
Designers normally constructed FDDI rings in a network topology such as a "dual ring of trees". A small number of devices, typically infrastructure devices such as routers and concentrators rather than host computers, were "dual-attached" to both rings. Host computers then connect as single-attached devices to the routers or concentrators. The dual ring in its most degenerate form simply collapses into a single device. Typically, a computer-room contained the whole dual ring, although some implementations deployed FDDI as a metropolitan area network.
FDDI requires this network topology because the dual ring actually passes through each connected device and requires each such device to remain continuously operational.
The standard actually allows for optical bypasses, but network engineers consider these unreliable and error-prone. Devices such as workstations and minicomputers that might not come under the control of the network managers are not suitable for connection to the dual ring.
As an alternative to using a dual-attached connection, a workstation can obtain the same degree of resilience through a dual-homed connection made simultaneously to two separate devices in the same FDDI ring. One of the connections becomes active while the other one is automatically blocked. If the first connection fails, the backup link takes over with no perceptible delay.
Frame format.
The FDDI data frame format is:
Where PA is the preamble, SD is a start delimiter, FC is frame control, DA is the destination address, SA is the source address, PDU is the protocol data unit (or packet data unit), FCS is the frame check Sequence (or checksum), and ED/FS are the end delimiter and frame status.
The Internet Engineering Task Force defined a standard for transmission of the Internet Protocol (which would be the protocol data unit in this case) over FDDI.
It was first proposed in June 1989 and revised in 1990.
Some aspects of the protocol were compatible with the IEEE 802.2 standard for logical link control. For example, the 48-bit MAC addresses that became popular with the Ethernet family. Thus other protocols such as the Address Resolution Protocol (ARP) could be common as well.
Deployment.
FDDI was considered an attractive campus backbone network technology in the early to mid 1990s since existing Ethernet networks only offered 10 Mbit/s data rates and token ring networks only offered 4 Mbit/s or 16 Mbit/s rates. Thus it was a relatively high-speed choice of that era.
By 1994, vendors included Cisco Systems, National Semiconductor, Network Peripherals, SysKonnect (acquired by Marvell Technology Group), and 3Com.
By the mid to late 1990s, Asynchronous Transfer Mode (ATM) technology became more common for backbone networks.
FDDI was effectively made obsolete in local networks by fast Ethernet which offered the same 100 Mbit/s speeds, but at a much lower cost and, since 1998, by Gigabit Ethernet due to its speed, and even lower cost, and ubiquity.
Standards.
FDDI standards included:

</doc>
<doc id="41150" url="http://en.wikipedia.org/wiki?curid=41150" title="Field strength">
Field strength

In physics, field strength or intensity means the magnitude of a vector-valued field. 
For example, electromagnetic field results in both electric field strength and magnetic field strength.
As an application, in radio frequency telecommunications, the signal strength excites a receiving antenna and thereby induce a voltage at a specific frequency and polarization in order to provide an input signal to a radio receiver. Field strength meters are used for such applications as cellular, broadcasting, wi-fi and a wide variety of other radio-related applications.

</doc>
<doc id="41151" url="http://en.wikipedia.org/wiki?curid=41151" title="File server">
File server

In computing, a file server is a computer attached to a network that has the primary purpose of providing a location for shared disk access, i.e. shared storage of computer files (such as documents, sound files, photographs, movies, images, databases, etc.) that can be accessed by the workstations that are attached to the same computer network. The term "server" highlights the role of the machine in the client–server scheme, where the "clients" are the workstations using the storage. A file server is not intended to perform computational tasks, and does not run programs on behalf of its clients.
It is designed primarily to enable the storage and retrieval of data while the computation is carried out by the workstations.
File servers are commonly found in schools and offices, where users use a LAN to connect their client computers.
Types of file servers.
A file server may be dedicated or non-dedicated. A dedicated server is designed specifically for use as a file server, with workstations attached for reading and writing files and databases.
File servers may also be categorized by the method of access: Internet file servers are frequently accessed by File Transfer Protocol (FTP) or by HTTP (but are different from web servers, that often provide dynamic web content in addition to static files). Servers on a LAN are usually accessed by SMB/CIFS protocol (Windows and Unix-like) or NFS protocol (Unix-like systems).
Database servers, that provide access to a shared database via a database device driver, are "not" regarded as file servers as they may require Record locking.
Design of file servers.
In modern businesses the design of file servers is complicated by competing demands for storage space, access speed, recoverability, ease of administration, security, and budget. This is further complicated by a constantly changing environment, where new hardware and technology rapidly obsolesces old equipment, and yet must seamlessly come online in a fashion compatible with the older machinery. To manage throughput, peak loads, and response time, vendors may utilize queuing theory to model how the combination of hardware and software will respond over various levels of demand. Servers may also employ dynamic load balancing scheme to distribute requests across various pieces of hardware.
The primary piece of hardware equipment for servers over the last couple of decades has proven to be the hard disk drive. Although other forms of storage are viable (such as magnetic tape and solid-state drives) disk drives have continued to offer the best fit for cost, performance, and capacity.
Storage.
Since the crucial function of a file server is storage, technology has been developed to operate multiple disk drives together as a team, forming a disk array. A disk array typically has cache (temporary memory storage that is faster than the magnetic disks), as well as advanced functions like RAID and storage virtualization. Typically disk arrays increase level of availability by using redundant components other than RAID, such as power supplies. Disk arrays may be consolidated or virtualized in a SAN.
Network-attached storage.
Network-attached storage (NAS) is file-level computer data storage connected to a computer network providing data access to heterogeneous clients. NAS devices specifically are distinguished from file servers generally in a NAS being a computer appliance – a specialized computer built from the ground up for serving files – rather than a general purpose computer being used for serving files (possibly with other functions). In discussions of NASs, the term "file server" generally stands for a contrasting term, referring to general purpose computers only.
 NAS devices are gaining popularity, offering a convenient method for sharing files between multiple computers. Potential benefits of network-attached storage, compared to non-dedicated file servers, include faster data access, easier administration, and simple configuration.
NAS systems are networked appliances containing one or more hard drives, often arranged into logical, redundant storage containers or RAID arrays. Network Attached Storage removes the responsibility of file serving from other servers on the network. They typically provide access to files using network file sharing protocols such as NFS, SMB/CIFS (Server Message Block/Common Internet File System), or AFP.
Security.
File servers generally offer some form of system security to limit access to files to specific users or groups. In large organizations, this is a task usually delegated to what is known as directory services such as openLDAP, Novell's eDirectory or Microsoft's Active Directory.
These servers work within the hierarchical computing environment which treat users, computers, applications and files as distinct but related entities on the network and grant access based on user or group credentials. In many cases, the directory service spans many file servers, potentially hundreds for large organizations. In the past, and in smaller organizations, authentication could take place directly at the server itself.

</doc>
<doc id="41152" url="http://en.wikipedia.org/wiki?curid=41152" title="Filled cable">
Filled cable

In telecommunication, a filled cable is a cable that has a non-hygroscopic material, usually a gel called icky-pick, inside the jacket or sheath. 
The nonhygroscopic material fills the spaces between the interior parts of the cable, preventing moisture from entering minor leaks in the sheath and migrating inside the cable. 
A metallic cable, such as a coaxial cable or a metal waveguide, filled with a dielectric material, is not considered as a filled cable.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).
Further reading.
See Telcordia , "Generic Requirements for Metallic Telecommunications Cables," for filled, polyolefin-insulated conductor (PIC) cable requirements.

</doc>
<doc id="41155" url="http://en.wikipedia.org/wiki?curid=41155" title="Firmware">
Firmware

In electronic systems and computing, firmware is "the combination of a hardware device, e.g. an integrated circuit, and computer instructions and data that reside as read only software on that device". As a result, firmware usually cannot be modified during normal operation of the device. Typical examples of devices containing firmware are embedded systems (such as traffic lights, consumer appliances, and digital watches), computers, computer peripherals, mobile phones, and digital cameras. The firmware contained in these devices provides the control program for the device.
Firmware is held in non-volatile memory devices such as ROM, EPROM, or flash memory. Changing the firmware of a device may rarely or never be done during its economic lifetime; some firmware memory devices are permanently installed and cannot be changed after manufacture. Common reasons for updating firmware include fixing bugs or adding features to the device. This may require ROM integrated circuits to be physically replaced, or flash memory to be reprogrammed through a special procedure. Firmware such as the ROM BIOS of a personal computer may contain only elementary basic functions of a device and may only provide services to higher-level software. Firmware such as the program of an embedded system may be the only program that will run on the system and provide all of its functions.
Before integrated circuits, other firmware devices included a discrete semiconductor diode matrix. The Apollo guidance computer had firmware consisting of a specially manufactured core memory plane, called "core rope memory", where data were stored by physically threading wires through (1) or around (0) the core storing each data bit.
Origin of the term.
Ascher Opler coined the term "firmware" in a 1967 "Datamation" article. Originally, it meant the contents of a writable control store (a small specialized high speed memory), containing microcode that defined and implemented the computer's instruction set, and that could be reloaded to specialize or modify the instructions that the central processing unit (CPU) could execute. As originally used, firmware contrasted with hardware (the CPU itself) and software (normal instructions executing on a CPU). It was not composed of CPU machine instructions, but of lower-level microcode involved in the implementation of machine instructions. It existed on the boundary between hardware and software; thus the name "firmware".
Still later, popular usage extended the word "firmware" to denote anything ROM-resident, including processor machine-instructions for BIOS, bootstrap loaders, or specialized applications.
Until the mid-1990s, updating firmware typically involved replacing a storage medium containing firmware, usually a socketed ROM integrated circuit. Flash memory allows firmware to be updated without physically removing an integrated circuit from the system. An error during the update process may make the device non-functional, or "bricked".
Personal computers.
In some respects, the various firmware components are as important as the operating system in a working computer. However, unlike most modern operating systems, firmware rarely has a well-evolved automatic mechanism of updating itself to fix any functionality issues detected after shipping the unit.
The BIOS may be "manually" updated by a user, using a small utility program. In contrast, firmware in storage devices (harddisks, DVD drives, flash storage) rarely gets updated, even when flash (rather than ROM) storage is used for the firmware; there are no standardized mechanisms for detecting or updating firmware versions.
Most computer peripherals are themselves special-purpose computers. Devices such as printers, scanners, cameras, USB drives, have firmware stored internally. Some devices may permit field replacement of firmware.
Some low-cost peripherals no longer contain non-volatile memory for firmware, and instead rely on the host system to transfer the device control program from a disk file or CD.
Consumer products.
s of 2010[ [update]] most portable music players support firmware upgrades. Some companies use firmware updates to add new playable file formats (codecs); iriver added Vorbis playback support this way, for instance. Other features that may change with firmware updates include the GUI or even the battery life. Most mobile phones have a Firmware Over The Air firmware upgrade capability for much the same reasons; some may even be upgraded to enhance reception or sound quality, illustrating the fact that firmware is used at more than one level in complex products (in a CPU-like microcontroller versus in a digital signal processor, in this particular case).
Automobiles.
Since 1996 most automobiles have employed an on-board computer and various sensors to detect mechanical problems. s of 2010[ [update]] modern vehicles also employ computer-controlled ABS systems and computer-operated Transmission Control Units (TCU). The driver can also get in-dash information while driving in this manner, such as real-time fuel-economy and tire-pressure readings. Local dealers can update most vehicle firmware.
Examples.
Examples of firmware include:
Flashing.
Flashing involves the overwriting of existing firmware or data on EEPROM modules present in an electronic device with new data. This can be done to upgrade a device or to change the provider of a service associated with the function of the device, such as changing from one mobile phone service provider to another or installing a new operating system. If firmware is upgradable, it is often done via a program from the provider, and will often allow the old firmware to be saved before upgrading so it can be reverted to if the process fails, or if the newer version performs worse.
Firmware hacking.
Sometimes third parties create an unofficial new or modified ("aftermarket") version of firmware to provide new features or to unlock hidden functionality. Examples include:
Most firmware hacks are free software.
These hacks usually take advantage of the firmware update facility on many devices to install or run themselves. Some, however, must resort to exploits in order to run, because the manufacturer has attempted to lock the hardware to stop it from running unlicensed code.
HDD firmware hacks.
The Moscow-based Kaspersky Lab discovered that a group of developers it refers to as the "Equation Group" has developed hard disk drive firmware modifications for various drive models, containing a trojan horse that allows data to be stored on the drive in locations that will not be erased even if the drive is formatted or wiped. Although the Kaspersky Lab report did not explicitly claim that this group is part of the United States National Security Agency (NSA), evidence obtained from the code of various Equation Group software suggests that they are part of the NSA.
Researchers from the Kaspersky Lab categorized the undertakings by Equation Group as the most advanced hacking operation ever uncovered, also documenting around 500 infections caused by the Equation Group in at least 42 countries.
Security risks.
Mark Shuttleworth, founder of the Ubuntu Linux distribution, has described proprietary firmware as a security risk, saying that "firmware on your device is the NSA's best friend" and calling firmware "a trojan horse of monumental proportions". He has pointed out that low-quality, nonfree firmware is a major threat to system security: "Your biggest mistake is to assume that the NSA is the only institution abusing this position of trust – in fact, it's reasonable to assume that all firmware is a cesspool of insecurity, courtesy of incompetence of the highest degree from manufacturers, and competence of the highest degree from a very wide range of such agencies". As a solution to this problem, he has called for declarative firmware, which would describe "hardware linkage and dependencies" and "should not include executable code".
Custom firmware hacks have also focused on injecting malware into devices such as smartphones or USB devices. One such smartphone injection was demonstrated on the Symbian OS at MalCon, a hacker convention. A USB device firmware hack called "BadUSB" was presented at Black Hat USA 2014 conference, demonstrating how a USB flash drive microcontroller can be reprogrammed to spoof various other device types in order to take control of a computer, exfiltrate data, or spy on the user. Other security researchers have worked further on how to exploit the principles behind BadUSB, releasing at the same time the source code of hacking tools that can be used to modify the behavior of USB flash drives.

</doc>
<doc id="41156" url="http://en.wikipedia.org/wiki?curid=41156" title="Fixed access">
Fixed access

Fixed access: In personal communications service (PCS), terminal access to a network in which there is a set relationship between a terminal and the access interface. A single "identifier" serves for both the access interface and the terminal. If the terminal moves to another access interface, that terminal assumes the identity of the new interface.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41157" url="http://en.wikipedia.org/wiki?curid=41157" title="Flag sequence">
Flag sequence

Flag sequence: In data transmission or processing, a sequence of bits used to delimit, "i.e." mark the beginning and end of a frame. 
"Note 1:" An 8-bit sequence is usually used as the flag sequence; for example, the 8-bit flag sequence 01111110. 
"Note 2:" Flag sequences are used in bit-oriented protocols, such as Advanced Data Communication Control Procedures (ADCCP), Synchronous Data Link Control (SDLC), and High-Level Data Link Control (HDLC).
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41158" url="http://en.wikipedia.org/wiki?curid=41158" title="Flat weighting">
Flat weighting

Flat weighting: In a noise-measuring set, a noise weighting based on an amplitude-frequency characteristic that is flat over a frequency range that must be stated. 
"Note 1:" Flat noise power is expressed in dBrn (f1 - f2) or in dBm (f1 - f2). "Note 2: "3 kHz flat weighting"" and "15 kHz flat weighting" are based on amplitude-frequency characteristics that are flat between 30 Hz and the frequency indicated.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41159" url="http://en.wikipedia.org/wiki?curid=41159" title="Flood search routing">
Flood search routing

In a telephone network, flood search routing is non-deterministic routing in which a dialed number received at a switch is transmitted to all switches, "i.e.," flooded, in the area code directly connected to that switch; if the dialed number is not an affiliated subscriber at that switch, the number is then retransmitted to all directly connected switches, and then routed through the switch that has the dialed number corresponding to the particular user end instrument affiliated with it. All digits of the numbering plan are used to identify a particular subscriber. Flood search routing allows subscribers to have telephone numbers independent of switch codes. Flood search routing provides the highest probability that a telephone call will go through even though a number of switches and links fail.
Flood search routing is used in military telecommunication systems, such as the mobile subscriber equipment (MSE) system.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41160" url="http://en.wikipedia.org/wiki?curid=41160" title="Flutter (electronics and communication)">
Flutter (electronics and communication)

In electronics and communication, flutter is the rapid variation of signal parameters, such as amplitude, phase, and frequency. Examples of electronic flutter are:
Aeroelastic flutter.
In the field of mechanics and structures, Aeroelastic flutter is an aeroelastic phenomenon where a body's own aerodynamic forces couple with its natural mode of vibration to produce rapid periodic motion.
Aeroelastic flutter occurs under steady flow conditions, when a structure's aerodynamic forces are affected by and in turn affect the movement of the structure. This sets up a positive feedback loop exciting the structure's free vibration. Flutter is self-starting and results in large amplitude vibration which often lead to rapid failure. 
The aerodynamic conditions required for flutter vary with the structure's external design and flexibility, but can range from very low velocities to supersonic flows. Large or flexible structures such as pipes, suspension bridges, chimneys and tall buildings are prone to flutter. Designing to avoid flutter is a fundamental requirement for rigid airfoils (fixed wing aircraft and helicopters) as well as for aircraft propellers and gas turbine blades. 
Prediction of flutter prior to modern unsteady computational fluid dynamics was based on empirical testing. As a result many pioneering designs failed due to unforeseen vibrations. The most famous of these was the opening of the original Tacoma Narrows Suspension Bridge in mid 1940, which failed spectacularly 4 months later during a sustained 67 km/h crosswind and became known as Galloping Gertie for its flutter movement. 
During the 1950s over 100 incidents were recorded of military or civilian aircraft being lost or damaged due to unforeseen flutter events. While as recently as the 1990s jet engine flutter has grounded military aircraft. 
Techniques to avoid flutter include changes to the structure's aerodynamics, stiffening the structure to change the excitation frequency and increasing the damping within the structure.
See also.
"Electronic Flutter"
"Structural Flutter"
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41161" url="http://en.wikipedia.org/wiki?curid=41161" title="Flywheel effect">
Flywheel effect

The flywheel effect is the continuation of oscillations in an oscillator circuit after the control stimulus has been removed. This is usually caused by interacting inductive and capacitive elements in the oscillator. Circuits undergoing such oscillations are said to be flywheeling.
The flywheel effect may be desirable, such as in phase-locked loops used in synchronous systems, or undesirable, such as in voltage-controlled oscillators.
Flywheel effect is used in class C modulation where efficiency of modulation can be achieved as high as 90%.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41164" url="http://en.wikipedia.org/wiki?curid=41164" title="Foreign exchange service">
Foreign exchange service

Foreign exchange service may refer to:

</doc>
<doc id="41165" url="http://en.wikipedia.org/wiki?curid=41165" title="Foreign instrumentation signals intelligence">
Foreign instrumentation signals intelligence

In telecommunication, the term foreign instrumentation signals intelligence (FISINT) has the following meanings: 
1. Intelligence information derived from electromagnetic emissions associated with the testing and operational deployment of foreign aerospace, surface, and subsurface systems. 
2. Technical information and intelligence information derived from the intercept of foreign instrumentation signals by other than the intended recipients. Foreign instrumentation signals intelligence is a category of signals intelligence. 
Foreign instrumentation signals include but are not limited to signals from telemetry, beaconry, electronic interrogators, tracking/fusing/arming/firing command systems, and video data links.

</doc>
<doc id="41166" url="http://en.wikipedia.org/wiki?curid=41166" title="Forward echo">
Forward echo

Forward echo: In a transmission line, a reflection propagating in the same direction as the original wave and consisting of energy reflected back by one discontinuity and then forward again by another discontinuity. Forward echoes can be supported by reflections caused by splices or other discontinuities in the transmission medium (e.g. optical fiber, twisted pair, or coaxial tube). In metallic lines, they may be supported by impedance mismatches between the source or load and the characteristic impedance of the transmission medium. They may cause attenuation distortion.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41167" url="http://en.wikipedia.org/wiki?curid=41167" title="Forward error correction">
Forward error correction

In telecommunication, information theory, and coding theory, forward error correction (FEC) or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels.
The central idea is the sender encodes the message in a redundant way by using an error-correcting code (ECC).
The American mathematician Richard Hamming pioneered this field in the 1940s and invented the first error-correcting code in 1950: the Hamming (7,4) code.
The redundancy allows the receiver to detect a limited number of errors that may occur anywhere in the message, and often to correct these errors without retransmission. FEC gives the receiver the ability to correct errors without needing a reverse channel to request retransmission of data, but at the cost of a fixed, higher forward channel bandwidth. FEC is therefore applied in situations where retransmissions are costly or impossible, such as one-way communication links and when transmitting to multiple receivers in multicast. FEC information is usually added to mass storage devices to enable recovery of corrupted data, and is widely used in modems.
FEC processing in a receiver may be applied to a digital bit stream or in the demodulation of a digitally modulated carrier. For the latter, FEC is an integral part of the initial analog-to-digital conversion in the receiver. The Viterbi decoder implements a soft-decision algorithm to demodulate digital data from an analog signal corrupted by noise. Many FEC coders can also generate a bit-error rate (BER) signal which can be used as feedback to fine-tune the analog receiving electronics.
The noisy-channel coding theorem establishes bounds on the theoretical maximum information transfer rate of a channel with some given noise level.
Some advanced FEC systems come very close to the theoretical maximum.
The maximum fractions of errors or of missing bits that can be corrected is determined by the design of the FEC code, so different forward error correcting codes are suitable for different conditions.
How it works.
FEC is accomplished by adding redundancy to the transmitted information using an algorithm. A redundant bit may be a complex function of many original information bits. The original information may or may not appear literally in the encoded output; codes that include the unmodified input in the output are systematic, while those that do not are non-systematic.
A simplistic example of FEC is to transmit each data bit 3 times, which is known as a (3,1) repetition code. Through a noisy channel, a receiver might see 8 versions of the output, see table below.
This allows an error in any one of the three samples to be corrected by "majority vote" or "democratic voting". The correcting ability of this FEC is:
Though simple to implement and widely used, this triple modular redundancy is a relatively inefficient FEC. Better FEC codes typically examine the last several dozen, or even the last several hundred, previously received bits to determine how to decode the current small handful of bits (typically in groups of 2 to 8 bits).
Averaging noise to reduce errors.
FEC could be said to work by "averaging noise"; since each data bit affects many transmitted symbols, the corruption of some symbols by noise usually allows the original user data to be extracted from the other, uncorrupted received symbols that also depend on the same user data.
Most telecommunication systems used a fixed channel code designed to tolerate the expected worst-case bit error rate, and then fail to work at all if the bit error rate is ever worse.
However, some systems adapt to the given channel error conditions: some instances of hybrid automatic repeat-request use a fixed FEC method as long as the FEC can handle the error rate, then switch to ARQ when the error rate gets too high;
adaptive modulation and coding uses a variety of FEC rates, adding more error-correction bits per packet when there are higher error rates in the channel, or taking them out when they are not needed.
Types of FEC.
The two main categories of FEC codes are block codes and convolutional codes.
There are many types of block codes, but among the classical ones the most notable is Reed-Solomon coding because of its widespread use on the Compact disc, the DVD, and in hard disk drives. Other examples of classical block codes include Golay, BCH, Multidimensional parity, and Hamming codes.
Hamming ECC is commonly used to correct NAND flash memory errors.
This provides single-bit error correction and 2-bit error detection.
Hamming codes are only suitable for more reliable single level cell (SLC) NAND.
Denser multi level cell (MLC) NAND requires stronger multi-bit correcting ECC such as BCH or Reed–Solomon.
NOR Flash typically does not use any error correction.
Classical block codes are usually decoded using hard-decision algorithms, which means that for every input and output signal a hard decision is made whether it corresponds to a one or a zero bit. In contrast, convolutional codes are typically decoded using soft-decision algorithms like the Viterbi, MAP or BCJR algorithms, which process (discretized) analog signals, and which allow for much higher error-correction performance than hard-decision decoding.
Nearly all classical block codes apply the algebraic properties of finite fields. Hence classical block codes are often referred to as algebraic codes.
In contrast to classical block codes that often specify an error-detecting or error-correcting ability, many modern block codes such as LDPC codes lack such guarantees. Instead, modern codes are evaluated in terms of their bit error rates.
On upper layers, FEC solution for mobile broadcast standards are Raptor code or RaptorQ.
Most forward error correction correct only bit-flips, but not bit-insertions or bit-deletions.
In this setting, the Hamming distance is the appropriate way to measure the bit error rate.
A few forward error correction codes are designed to correct bit-insertions and bit-deletions, such as Marker Codes and Watermark Codes.
The Levenshtein distance is a more appropriate way to measure the bit error rate when using such codes.
Concatenated FEC codes for improved performance.
Classical (algebraic) block codes and convolutional codes are frequently combined in concatenated coding schemes in which a short constraint-length Viterbi-decoded convolutional code does most of the work and a block code (usually Reed-Solomon) with larger symbol size and block length "mops up" any errors made by the convolutional decoder. Single pass decoding with this family of error correction codes can yield very low error rates, but for long range transmission conditions (like deep space) iterative decoding is recommended.
Concatenated codes have been standard practice in satellite and deep space communications since Voyager 2 first used the technique in its 1986 encounter with Uranus. The Galileo craft used iterative concatenated codes to compensate for the very high error rate conditions caused by having a failed antenna.
Low-density parity-check (LDPC).
Low-density parity-check (LDPC) codes are a class of recently re-discovered highly efficient linear block
codes made from many single parity check (SPC) codes. They can provide performance very close to the channel capacity (the theoretical maximum) using an iterated soft-decision decoding approach, at linear time complexity in terms of their block length. Practical implementations rely heavily on decoding the constituent SPC codes in parallel.
LDPC codes were first introduced by Robert G. Gallager in his PhD thesis in 1960,
but due to the computational effort in implementing encoder and decoder and the introduction of Reed–Solomon codes,
they were mostly ignored until recently.
LDPC codes are now used in many recent high-speed communication standards, such as DVB-S2 (Digital video broadcasting), WiMAX (IEEE 802.16e standard for microwave communications), High-Speed Wireless LAN (IEEE 802.11n), 10GBase-T Ethernet (802.3an) and G.hn/G.9960 (ITU-T Standard for networking over power lines, phone lines and coaxial cable). Other LDPC codes are standardized for wireless communication standards within 3GPP MBMS (see fountain codes).
Turbo codes.
Turbo coding is an iterated soft-decoding scheme that combines two or more relatively simple convolutional codes and an interleaver to produce a block code that can perform to within a fraction of a decibel of the Shannon limit. Predating LDPC codes in terms of practical application, they now provide similar performance.
One of the earliest commercial applications of turbo coding was the CDMA2000 1x (TIA IS-2000) digital cellular technology developed by Qualcomm and sold by Verizon Wireless, Sprint, and other carriers. It is also used for the evolution of CDMA2000 1x specifically for Internet access, 1xEV-DO (TIA IS-856). Like 1x, EV-DO was developed by Qualcomm, and is sold by Verizon Wireless, Sprint, and other carriers (Verizon's marketing name for 1xEV-DO is "Broadband Access", Sprint's consumer and business marketing names for 1xEV-DO are "Power Vision" and "Mobile Broadband", respectively).
Local decoding and testing of codes.
Sometimes it is only necessary to decode single bits of the message, or to check whether a given signal is a codeword, and do so without looking at the entire signal. This can make sense in a streaming setting, where codewords are too large to be classically decoded fast enough and where only a few bits of the message are of interest for now. Also such codes have become an important tool in computational complexity theory, e.g., for the design of probabilistically checkable proofs.
Locally decodable codes are error-correcting codes for which single bits of the message can be probabilistically recovered by only looking at a small (say constant) number of positions of a codeword, even after the codeword has been corrupted at some constant fraction of positions. Locally testable codes are error-correcting codes for which it can be checked probabilistically whether a signal is close to a codeword by only looking at a small number of positions of the signal.
Interleaving.
Interleaving is frequently used in digital communication and storage systems to improve the performance of forward error correcting codes. Many communication channels are not memoryless: errors typically occur in bursts rather than independently. If the number of errors within a code word exceeds the error-correcting code's capability, it fails to recover the original code word. Interleaving ameliorates this problem by shuffling source symbols across several code words, thereby creating a more uniform distribution of errors. Therefore, interleaving is widely used for burst error-correction.
The analysis of modern iterated codes, like turbo codes and LDPC codes, typically assumes an independent distribution of errors. Systems using LDPC codes therefore typically employ additional interleaving across the symbols within a code word.
For turbo codes, an interleaver is an integral component and its proper design is crucial for good performance. The iterative decoding algorithm works best when there are not short cycles in the factor graph that represents the decoder; the interleaver is chosen to avoid short cycles.
Interleaver designs include:
In multi-carrier communication systems, interleaving across carriers may be employed to provide frequency diversity, e.g., to mitigate frequency-selective fading or narrowband interference.
Example.
Transmission without interleaving:
 Error-free message: aaaabbbbccccddddeeeeffffgggg
 Transmission with a burst error: aaaabbbbccc____deeeeffffgggg
Here, each group of the same letter represents a 4-bit one-bit error-correcting codeword. The codeword cccc is altered in one bit and can be corrected, but the codeword dddd is altered in three bits, so either it cannot be decoded at all or it might be decoded incorrectly.
With interleaving:
 Error-free code words: aaaabbbbccccddddeeeeffffgggg
 Interleaved: abcdefgabcdefgabcdefgabcdefg
 Transmission with a burst error: abcdefgabcd____bcdefgabcdefg
 Received code words after deinterleaving: aa_abbbbccccdddde_eef_ffg_gg
In each of the codewords aaaa, eeee, ffff, gggg, only one bit is altered, so one-bit error-correcting code will decode everything correctly.
Transmission without interleaving:
 Original transmitted sentence: ThisIsAnExampleOfInterleaving
 Received sentence with a burst error: ThisIs______pleOfInterleaving
The term "AnExample" ends up mostly unintelligible and difficult to correct.
With interleaving:
 Transmitted sentence: ThisIsAnExampleOfInterleaving...
 Error-free transmission: TIEpfeaghsxlIrv.iAaenli.snmOten.
 Received sentence with a burst error: TIEpfe______Irv.iAaenli.snmOten.
 Received sentence after deinterleaving: T_isI_AnE_amp_eOfInterle_vin_...
No word is completely lost and the missing letters can be recovered with minimal guesswork.
Disadvantages of interleaving.
Use of interleaving techniques increases latency. This is because the entire interleaved block must be received before the packets can be decoded. Also interleavers hide the structure of errors; without an interleaver, more advanced decoding algorithms can take advantage of the error structure and achieve more reliable communication than a simpler decoder combined with an interleaver.
List of error-correcting codes.
 distance code
 2 (single-error detecting) parity
 3 (single-error correcting) triple modular redundancy
 3 (single-error correcting) perfect Hamming such as Hamming(7,4)
 4 (SECDED) extended Hamming
 5 double-error correcting
 6 DEC-TED
 7 (three-error correcting) perfect binary Golay code
 8 (TECFED) extended binary Golay code

</doc>
<doc id="41168" url="http://en.wikipedia.org/wiki?curid=41168" title="Forward scatter">
Forward scatter

In telecommunication and astronomy, forward scatter is the deflection—by diffraction, nonhomogeneous refraction, or nonspecular reflection by particulate matter of dimensions that are large with respect to the wavelength in question but small with respect to the beam diameter—of a portion of an incident electromagnetic wave, in such a manner that the energy so deflected propagates in a direction that is within 90° of the direction of propagation of the incident wave (i.e., the phase angle is greater than 90°). 
The scattering process may be polarization-sensitive, "i.e.", incident waves that are identical in every respect but their polarization may be scattered differently.
Comets.
Forward scattering can make a back-lit comet appear significantly brighter because the dust and ice crystals are reflecting and enhancing the apparent brightness of the comet by scattering that light towards the observer. Comets studied forward-scattering in visible-thermal photometry include C/1927 X1 (Skjellerup–Maristany), C/1975 V1 (West), and C/1980 Y1 (Bradfield). Comets studied forward-scattering in SOHO non-thermal C3 coronograph photometry include 96P/Machholz and C/2004 F4 (Bradfield). The brightness of the great comets C/2006 P1 (McNaught) and Comet Skjellerup–Maristany near perihelion were enhanced by forward scattering.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41169" url="http://en.wikipedia.org/wiki?curid=41169" title="Frequency of optimum transmission">
Frequency of optimum transmission

Frequency of optimum transmission (FOT), in the transmission of radio waves via ionospheric reflection, is the highest effective (i.e. working) frequency that is predicted to be usable for a specified path and time for 90% of the days of the month. The FOT is normally just below the value of the maximum usable frequency (MUF). In the prediction of usable frequencies, the FOT is commonly taken as 15% below the monthly median value of the MUF for the specified time and path. 
The FOT is usually the most effective frequency for ionospheric reflection of radio waves between two specified points on Earth. 
Synonyms for this term include:

</doc>
<doc id="41170" url="http://en.wikipedia.org/wiki?curid=41170" title="Four-wire circuit">
Four-wire circuit

In telecommunication, a four-wire circuit is a two-way circuit using two paths so arranged that the respective signals are transmitted in one direction only by one path and in the other direction by the other path. Late in the 20th century, almost all connections between telephone exchanges were four-wire circuits, while conventional phone lines into residences and businesses were two-wire circuits.
The four-wire circuit gets its name from the fact that, historically, a balanced pair of conductors were used in each of two directions for full-duplex operation. The name may still be applied to, for example, optical fibers, even though only one fiber is required for transmission in each direction. A system can separate the frequency directions by frequency duplex and realize the benefits of a four-wire circuit even while the same wire pair is used in both directions.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41171" url="http://en.wikipedia.org/wiki?curid=41171" title="Four-wire terminating set">
Four-wire terminating set

A four-wire terminating set (4WTS) is a balanced transformer used to perform a conversion between four-wire and two-wire operation in telecommunication systems.
For example, a 4-wire circuit may, by means of a 4-wire terminating set, be connected to a 2-wire telephone set. Also, a pair of 4-wire terminating sets may be used to introduce an intermediate 4-wire circuit into a 2-wire circuit, in which loop repeaters may be situated to amplify signals in each direction without positive feedback and oscillation. 
The 4WTS differs from a simple hybrid coil in being equipped to adjust its impedance to maximize return loss.
Four-wire terminating sets were largely supplanted by resistance hybrids in the late 20th century.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41172" url="http://en.wikipedia.org/wiki?curid=41172" title="Frame (networking)">
Frame (networking)

A frame is a digital data transmission unit in computer networking and telecommunication. A frame typically includes frame synchronization features consisting of a sequence of bits or symbols that indicate to the receiver the beginning and end of the payload data within the stream of symbols or bits it receives. If a receiver is connected to the system in the middle of a frame transmission, it ignores the data until it detects a new frame synchronization sequence. 
In computer networking, a frame is a data packet in Layer 2 of the OSI model. A frame is "the unit of transmission in a link layer protocol, and consists of a link layer header followed by a packet." Examples are Ethernet frames, Point-to-Point Protocol (PPP) frames, and V.42 modem frames. 
In telecommunications, specifically in time-division multiplex (TDM) and time-division multiple access (TDMA) variants, a frame is a cyclically repeated data block that consists of a fixed number of time slots, one for each logical TDM channel or TDMA transmitter. In this context, a frame is typically an entity at the physical layer. TDM application examples are SONET/SDH and the ISDN circuit switched B-channel, while TDMA examples are the 2G and 3G circuit-switched cellular voice services. The frame is also an entity for time-division duplex, where the mobile terminal may transmit during some timeslots and receive during others.

</doc>
<doc id="41173" url="http://en.wikipedia.org/wiki?curid=41173" title="Frame rate">
Frame rate

Frame rate, also known as frame frequency, is the frequency (rate) at which an imaging device produces unique consecutive images called frames. The term applies equally well to film and video cameras, computer graphics, and motion capture systems. Frame rate is most often expressed in frames per second (FPS) and is also expressed in progressive scan monitors as hertz (Hz).
Background.
The human eye and its brain interface, the human visual system, can process 10 to 12 separate images per second, perceiving them individually. The threshold of human visual perception varies depending on what is being measured. When looking at a lighted display, people begin to notice a brief interruption of darkness if it is about 16 milliseconds or longer. Observers can recall one specific image in an unbroken series of different images, each of which lasts as little as 13 milliseconds. When given very short single-millisecond visual stimulus people report a duration of between 100 ms and 400 ms due to persistence of vision in the visual cortex. This may cause images perceived in this duration to appear as one stimulus, such as a 10 ms green flash of light immediately followed by a 10 ms red flash of light perceived as a single yellow flash of light. Persistence of vision may also create an illusion of continuity, allowing a sequence of still images to give the impression of motion.
Early silent films had stated frame rates anywhere from 16 to 24 FPS, but since the cameras were hand-cranked, the rate often changed during the scene to fit the mood. Projectionists could also change the frame rate in the theater by adjusting a rheostat controlling the voltage powering the film-carrying mechanism in the projector. Silent films were often intended to be shown at higher frame rates than those used during filming. These frame rates were enough for the sense of motion, but it was perceived as jerky motion. By using projectors with dual- and triple-blade shutters, the rate was multiplied two or three times as seen by the audience. Thomas Edison said that 46 frames per second was the minimum needed by the visual cortex: "Anything less will strain the eye." In the mid to late 1920s, the frame rate for silent films increased to between 20 and 26 FPS.
When sound film was introduced in 1926, variations in film speed were no longer tolerated as the human ear is more sensitive to changes in audio frequency. Many theaters had shown silent films at 22 to 26 FPS which is why 24 FPS was chosen for sound. From 1927 to 1930, as various studios updated equipment, the rate of 24 FPS became standard for 35 mm sound film. At 24 FPS the film travels through the projector at a rate of 456 mm per second. This allowed for simple two-blade shutters to give a projected series of images at 48 per second, satisfying Edison's recommendation. Many modern 35 mm film projectors use three-blade shutters to give 72 images per second—each frame is flashed on screen three times.
Motion picture film.
In the motion picture industry, where traditional film stock is used, the industry standard filming and projection formats are 24 frames per second (FPS). Shooting at a slower frame rate would create fast motion when projected, while shooting at a frame rate higher than 24 FPS would create slow motion when projected. Other examples of historical experiments in frame rates that were not widely accepted were Maxivision 48 and Showscan, developed by "" special effects creator Douglas Trumbull.
The silent home movie film frame rate was 16 FPS or 18 FPS for 16 mm and standard 8 mm, 18 FPS for Super 8. Sound speed was normally 24 FPS for all formats.
Digital video and television.
There are three main frame rate standards in the TV and digital cinema business: 24p, 25p, and 30p. However, there are many variations on these as well as newer emerging standards.

</doc>
<doc id="41174" url="http://en.wikipedia.org/wiki?curid=41174" title="Constitution of Vermont">
Constitution of Vermont

The Constitution of the State of Vermont is the fundamental body of law of the U.S. State of Vermont. It was adopted in 1793 following Vermont's admission to the Union in 1791 and is largely based upon the 1777 Constitution of the Vermont Republic which was ratified at Windsor in the Old Constitution House and amended in 1786. At 8,295 words, it is the shortest U.S. state constitution.
Before 1791, Vermont was known as the Vermont Republic, an independent nation governed under the Constitution of the Vermont Republic. When adopted in 1777, that constitution was among the most far reaching in guaranteeing personal freedoms and individual rights.
The 1777 constitution's Declaration of Rights of the Inhabitants of the State of Vermont anticipated the United States Bill of Rights by a dozen years.
The first chapter of that earlier document, a "Declaration of Rights of the Inhabitants of the State of Vermont", was drafted in 1777 and is followed by a "Plan or Frame of Government" outlining the structure of governance with powers distributed between three co-equal branches: executive, legislative and judiciary.
Amending the constitution.
The Vermont Constitution, Chapter 2, Section 72 establishes the procedure for amending the constitution. The Vermont General Assembly, the state's bi-cameral legislature, has the sole power to propose amendments to the Constitution of Vermont. The process must be initiated by a Senate that has been elected in an "off-year", that is, an election that does not coincide with the election of the U.S. president. An amendment must originate in the Senate and be approved by a two-thirds vote. It must then receive a majority vote in the House. Then, after a newly elected legislature is seated, the amendment must receive a majority vote in each chamber, first in the Senate, then in the House. The proposed amendment must then be presented to the voters as a referendum and receive a majority of the votes cast.
1990s revision to gender-neutral language.
In 1991 and again in 1993, the Vermont General Assembly approved a constitutiuonal amendment authorizing the justices of the Vermont Supreme Court to revise the Constitution in "gender-inclusive" language, replacing gender-specific terms. (Examples: "men" and "women" were replaced by "persons" and the "Freeman's Oath," which requires all newly registered voters in the state to swear by, was renamed the "Voters' Oath"). The revision was ratified by the voters in the general election of November 8, 1994. Vermont is one of six states whose constitutions are written in gender-neutral language.

</doc>
<doc id="41175" url="http://en.wikipedia.org/wiki?curid=41175" title="Frame slip">
Frame slip

In the reception of framed data, a frame slip is the loss of synchronization between a received frame and the receiver clock signal, causing a frame misalignment event, and resulting in the loss of the data contained in the received frame. 
A frame slip should not be confused with a dropped frame where synchronization is not lost, as in the case of buffer overflow, for example.

</doc>
<doc id="41176" url="http://en.wikipedia.org/wiki?curid=41176" title="Frame synchronization">
Frame synchronization

In telecommunication, frame synchronization or framing is the process by which, while receiving a stream of framed data, incoming frame alignment signals (i.e., a distinctive bit sequences or syncwords) are identified (that is, distinguished from data bits), permitting the data bits within the frame to be extracted for decoding or retransmission.
Framing.
If the transmission is temporarily interrupted, or a bit slip event occurs, the receiver must re-synchronize.
The transmitter and the receiver must agree ahead of time on which frame synchronization scheme they will use.
Common frame synchronization schemes are:
Frame synchronizer.
In telemetry applications, a "frame synchronizer" is used to frame-align a serial pulse code-modulated (PCM) binary stream.
The frame synchronizer immediately follows the bit synchronizer in most telemetry applications. Without frame synchronization, decommutation is impossible.
The frame synchronization pattern is a known binary pattern which repeats at a regular interval within the PCM stream. The frame synchronizer recognizes this pattern and aligns the data into minor frames or sub-frames. Typically the frame sync pattern is followed by a counter (sub-frame ID) which dictates which minor or sub-frame in the series is being transmitted. This becomes increasingly important in the decommutation stage where all data is deciphered as to what attribute was sampled. Different commutations require a constant awareness of which section of the major frame is being decoded.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41177" url="http://en.wikipedia.org/wiki?curid=41177" title="Framing">
Framing

Framing may refer to:

</doc>
<doc id="41179" url="http://en.wikipedia.org/wiki?curid=41179" title="Free-space path loss">
Free-space path loss

In telecommunication, free-space path loss (FSPL) is the loss in signal strength of an electromagnetic wave that would result from a line-of-sight path through free space (usually air), with no obstacles nearby to cause reflection or diffraction. It is defined in "Standard Definitions of Terms for Antennas", IEEE Std 145-1983, as "The loss between two isotropic radiators in free space, expressed as a power ratio." Usually it is expressed in dB, although the IEEE standard does not say that. So it assumes that the antenna gain is a power ratio of 1.0, or 0 dB. It does not include any loss associated with hardware imperfections, or the effects of any antennas gain. A discussion of these losses may be found in the article on link budget. The FSPL is rarely used standalone, but rather as a part of the Friis transmission equation, which includes the gain of antennas. 
Free-space path loss formula.
Free-space path loss is proportional to the square of the distance between the transmitter and receiver, and also proportional to the square of the frequency of the radio signal.
The equation for FSPL is
where:
This equation is only accurate in the far field where spherical spreading can be assumed; it does not hold close to the transmitter.
Free-space path loss in decibels.
A convenient way to express FSPL is in terms of dB:
where the units are as before.
For typical radio applications, it is common to find formula_3 measured in units of GHz and formula_4 in km, in which case the FSPL equation becomes
For formula_10 in meters and kilohertz, respectively, the constant becomes formula_11 .
For formula_10 in meters and megahertz, respectively, the constant becomes formula_13 .
For formula_10 in kilometers and megahertz, respectively, the constant becomes formula_15 .
Physical explanation.
The FSPL expression above often leads to the erroneous belief that free space attenuates an electromagnetic wave according to its frequency. This is not the case, as there is no physical mechanism that could cause this. The expression for FSPL actually encapsulates two effects.
Distance dependency.
Dependency of the FSPL on distance is caused by the spreading out of electromagnetic energy in free space and is described by the inverse square law, i.e.
where:
Note that this is not a frequency-dependent effect.
Frequency dependency.
The frequency dependency is somewhat more confusing. The question is often asked: Why should path loss, which is just a geometric inverse-square loss, be a function of frequency? The answer is that path loss is "defined" on the use of an isotropic receiving antenna (formula_20).:sec. 5.3.3, p. 256 This can be seen if we derive the FSPL from the Friis transmission equation.
Hence path loss is a convenient tool; it represents a hypothetical received-power loss that would occur if the receiving antenna were isotropic. Therefore the FSPL can be viewed as a convenient collection of terms that have been assigned the unfortunate name "path loss". This name calls up an image of purely geometric effect and fails to emphasize the requirement that formula_20. A better choice of the name would have been "unity-gain propagation loss".:sec. 5.3.3, p. 256 
Hence frequency dependency of the path loss is caused by the frequency dependency of the receiving antenna's aperture in case the antenna gain is fixed. Antenna aperture in turn determines how well an antenna can pick up power from an incoming electromagnetic wave.
Dependency of antenna aperture from antenna gain is described by the formula::Chap3, p. 22,(3.52)
This formula represents a well-known fact, that the lower the frequency (the longer the wavelength), the bigger antenna is needed to achieve certain antenna gain. Therefore for a theoretical isotropic antenna (formula_24), the received power formula_25 is described by a formula:
where formula_27 is a power density of an electromagnetic wave at a location of theoretical isotropic receiving antenna. Note that this is entirely dependent on wavelength, which is how the frequency-dependent behaviour arises.
In simple terms the frequency dependency of the path loss can be explained like this: with the increase of the frequency the requirement to keep the gain of the receiving antenna intact will cause an antenna aperture to be decreased, which will result in less energy being captured with the smaller antenna, which is similar to increasing the path loss in the situation when receiving antenna gain would not have been fixed.

</doc>
<doc id="41180" url="http://en.wikipedia.org/wiki?curid=41180" title="Freeze frame television">
Freeze frame television

Freeze frame television: Television in which fixed ("still") images (the frames of the video) are transmitted sequentially at a rate far too slow to be perceived as continuous motion by human vision. The receiving device typically holds each frame in memory, displaying it until the next complete frame is available.
For an image of specified quality, "e.g.", resolution and color fidelity, freeze-frame television has a lower bandwidth requirement than that of full-motion television. For this reason, NASA, which refers to this technique as "sequential still video", uses it on UHF when Ku band full-motion video signals are not available.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41181" url="http://en.wikipedia.org/wiki?curid=41181" title="F region">
F region

The F region of the ionosphere is home to the F layer of ionization, also called the Appleton–Barnett layer, after the English physicist Edward Appleton and New Zealander Miles Barnett. As with other ionospheric sectors, 'layer' implies a concentration of plasma, while 'region' is the area that contains the said layer. The F region contains ionized gases at a height of around 150–800 km above sea level, placing it in the Earth’s thermosphere, a hot region in the upper atmosphere, and also in the heterosphere, where chemical composition varies with height. Generally speaking, the F region has the highest concentration of free electrons and ions anywhere in the atmosphere. It may be thought of as comprising two layers, the F1-and F2-layers.
The F-region is located directly above the E region (formerly the Kennelly-Heaviside layer) and below the protonosphere. It acts as a dependable reflector of radio signals as it is not affected by atmospheric conditions, although its ionic composition varies with the sunspot cycle. It reflects normal-incident frequencies at or below the critical frequency (approximately 10 MHz) and partially absorbs waves of higher frequency.
The F region is the region of the ionosphere which is very important for HF radio wave propagation. This F region is very anomalous in nature.
F1 and F2 Layers.
The F1 layer is the lower sector of the F layer and exists from about 150 to 220 km above the surface of the Earth and only during daylight hours. It is composed of a mixture of molecular ions O2+ and NO+, and atomic ions O+. Above the F1 region, atomic oxygen becomes the dominant constituent because lighter particles tend to occupy higher altitudes above the turbopause (at ~100 km). This atomic oxygen provides the O+ atomic ions that make up the F2 layer.
The F1 layer has approximately 5 × 105 e/cm3 (free electrons per cubic centimeter) at noontime and minimum sunspot activity, and increases to roughly 2 × 106 e/cm3 during maximum sunspot activity. The density falls off to below 104 e/cm3 at night.
Usage in Radio Communication.
Critical F2 layer frequencies are the ones that will not go through the F2 layer.

</doc>
<doc id="41183" url="http://en.wikipedia.org/wiki?curid=41183" title="Frequency assignment authority">
Frequency assignment authority

In telecommunication, frequency assignment authority is the power granted for the administration, designation or delegation to an agency or administrator via treaty or law, to specify frequencies, or frequency bands, in the electromagnetic spectrum for use in systems or equipment.
International frequency assignment authority is vested in the Radiocommunication Bureau of the International Telecommunication Union (ITU). In the United States, primary frequency assignment authority is exercised by the National Telecommunications and Information Administration (NTIA) for the Federal Government and by the Federal Communications Commission (FCC) for non-Federal Government organizations.
In Europe each country has regulatory input into the progress of European and international policy, standards, and legislation governing these sectors through their respective telecom regulators.
Frequency management for Europe is driven by a number of organisations. These include the:
In July 2002, the European Commission also established the European Regulators Group for Electronic Communications Networks and Services; creating, for the first time, a formal structure for interaction and coordination between the European Commission and regulators in all EU Member States to ensure consistent application of European legislation.

</doc>
<doc id="41184" url="http://en.wikipedia.org/wiki?curid=41184" title="Frequency averaging">
Frequency averaging

In telecommunication, the term frequency averaging has the following meanings: 
In frequency averaging, all oscillators are assigned equal weight in determining the ultimate network frequency. 
In terms of musical note frequency, the averaging of the frequency of low or high notes in a solo instrumental piece is a technique used to match different instruments together so they may be played together. The musical note frequency calculation formula is used: F=(2^12/n)*440, where n equals the amount of positive or negative steps away from the base note of A4(440 hertz) and F equals the frequency. The formula is used in calculating the frequency of each note in the piece. The values are then added together and divided the amount of notes. This is the average frequency of those notes. It is said that such techniques were used by classical composers, especially those who involved mathematics heavily in their music. 

</doc>
<doc id="41185" url="http://en.wikipedia.org/wiki?curid=41185" title="Frequency-change signaling">
Frequency-change signaling

In telecommunication, frequency-change signaling is a telegraph signaling method in which one or more particular frequencies correspond to each desired signaling condition of a telegraph code. The transition from one set of frequencies to the other may be a continuous or a discontinuous change in the frequency or phase.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41186" url="http://en.wikipedia.org/wiki?curid=41186" title="Frequency compatibility">
Frequency compatibility

In telecommunication, the term frequency compatibility has the following meanings: 
1. Of an electronic device, the extent to which it will operate at its designed performance level in its intended operational environment (including the presence of interference) without causing interference to other devices. 
2. The degree to which an electrical or electronic device or devices operating on or responding to a specified frequency or frequencies is capable of functioning with other such devices.
See also electromagnetic compatibility
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41187" url="http://en.wikipedia.org/wiki?curid=41187" title="Frequency deviation">
Frequency deviation

Frequency deviation (Δf) is used in FM radio to describe the maximum instantaneous difference between an FM modulated frequency and the nominal carrier frequency. The term is sometimes mistakenly used as synonymous with frequency drift, which is an unintended offset of an oscillator from its nominal frequency.
The frequency deviation of a radio is of particular importance in relation to bandwidth, because less deviation means that more channels can fit into the same amount of frequency spectrum. The FM broadcasting range (87.5–108 MHZ, NOTE: In some countries the 87.5-88.0 MHZ part of the band is not used) uses a channel spacing of 200 kHz, with a maximum frequency deviation of 75 kHz, leaving a 25 kHz buffer above the highest and below the lowest frequency to reduce interaction with other channels. AM broadcasting uses a channel spacing of 10 kHz, but with amplitude modulation frequency deviation is irrelevant. 
FM applications use peak deviations of 75 kHz (200 kHz spacing), 5 kHz (25 kHz spacing), 2.5 kHz (12.5 kHz spacing), and 2 kHz (8.33 kHz spacing).

</doc>
<doc id="41188" url="http://en.wikipedia.org/wiki?curid=41188" title="Frequency-exchange signaling">
Frequency-exchange signaling

In telegraphy, frequency-exchange signaling or two-source frequency keying is frequency-change signaling in which the change from one significant condition to another is accompanied by decay in amplitude of one or more frequencies and by buildup in amplitude of one or more other frequencies.
Frequency-exchange signaling applies to supervisory signaling and user-information transmission. 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41189" url="http://en.wikipedia.org/wiki?curid=41189" title="Frequency frogging">
Frequency frogging

In telecommunication, the term frequency frogging has the following meanings: 
"Note:" Frequency frogging is accomplished by having modulators, which are integrated into specially designed repeaters, translate a low-frequency group to a high-frequency group, and vice versa. A channel will appear in the low group for one repeater section and will then be translated to the high group for the next section because of frequency frogging. This results in nearly constant attenuation with frequency over two successive repeater sections, and eliminates the need for large slope equalization and adjustments. Singing and crosstalk are minimized because the high-level output of a repeater is at a different frequency than the low-level input to other repeaters. It also diminishes group delay distortion. A repeater that receives on the high band from both direction and sends on the low band is called Hi-Lo; the other kind Lo-Hi.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41191" url="http://en.wikipedia.org/wiki?curid=41191" title="Frequency sharing">
Frequency sharing

In telecommunication, frequency sharing is the assignment to or use of the same radio frequency by two or more stations that are separated geographically or that use the frequency at different times. 
Frequency sharing reduces the potential for mutual interference where the assignment of different frequencies to each user is not practical or possible. 
In a communications net, frequency sharing does not pertain to stations that use the same frequency.

</doc>
<doc id="41192" url="http://en.wikipedia.org/wiki?curid=41192" title="Frequency shift">
Frequency shift

In the physical sciences and in telecommunication, the term frequency shift may refer to:

</doc>
<doc id="41193" url="http://en.wikipedia.org/wiki?curid=41193" title="Frequency-shift keying">
Frequency-shift keying

Frequency-shift keying (FSK) is a frequency modulation scheme in which digital information is transmitted through discrete frequency changes of a carrier wave. The simplest FSK is binary FSK (BFSK). BFSK uses a pair of discrete frequencies to transmit binary (0s and 1s) information. With this scheme, the "1" is called the mark frequency and the "0" is called the space frequency. The time domain of an FSK modulated carrier is illustrated in the figures to the right.
Implementations of FSK Modems.
Reference implementations of FSK modems exist and are documented in detail. The demodulation of a binary FSK signal can be done using the Goertzel algorithm very efficiently, even on low-power microcontrollers.
Other forms of FSK.
Continuous-phase frequency-shift keying.
In principle FSK can be implemented by using completely independent free-running oscillators, and switching between them at the beginning of each symbol period.
In general, independent oscillators will not be at the same phase and therefore the same amplitude at the switch-over instant,
causing sudden discontinuities in the transmitted signal.
In practice, many FSK transmitters use only a single oscillator, and the process of switching to a different frequency at the beginning of each symbol period preserves the phase.
The elimination of discontinuities in the phase (and therefore elimination of sudden changes in amplitude) reduces sideband power, reducing interference with neighboring channels.
Gaussian frequency-shift keying.
Rather than directly modulating the frequency with the digital data symbols,
"instantaneously" changing the frequency at the beginning of each symbol period,
Gaussian frequency-shift keying (GFSK) filters the data pulses with a Gaussian filter to make the transitions smoother.
This filter has the advantage of reducing sideband power, reducing interference with neighboring channels, at the cost of increasing intersymbol interference.
Minimum-shift keying.
Minimum frequency-shift keying or minimum-shift keying (MSK) is a particular spectrally efficient form of coherent FSK. In MSK, the difference between the higher and lower frequency is identical to half the bit rate. Consequently, the waveforms that represent a 0 and a 1 bit differ by exactly half a carrier period. The maximum frequency deviation is δ = 0.25 "fm", where "fm" is the maximum modulating frequency. As a result, the modulation index "m" is 0.5. This is the smallest FSK modulation index that can be chosen such that the waveforms for 0 and 1 are orthogonal.
Gaussian minimum shift keying.
A variant of MSK called Gaussian minimum shift keying (GMSK) is used in the GSM mobile phone standard.
Audio FSK.
"Audio frequency-shift keying" (AFSK) is a modulation technique by which digital data is represented by changes in the frequency (pitch) of an audio tone, yielding an encoded signal suitable for transmission via radio or telephone. Normally, the transmitted audio alternates between two tones: one, the "mark", represents a binary one; the other, the "space", represents a binary zero.
AFSK differs from regular frequency-shift keying in performing the modulation at baseband frequencies. In radio applications, the AFSK-modulated signal normally is being used to modulate an RF carrier (using a conventional technique, such as AM or FM) for transmission.
AFSK is not always used for high-speed data communications, since it is far less efficient in both power and bandwidth than most other modulation modes. In addition to its simplicity, however, AFSK has the advantage that encoded signals will pass through AC-coupled links, including most equipment originally designed to carry music or speech.
AFSK is used in the U.S. based Emergency Alert System to notify stations of the type of emergency, locations affected, and the time of issue without actually hearing the text of the alert.
Continuous 4 level FM.
Phase 1 radios in the Project 25 system use continuous 4 level FM (C4FM) modulation.
Applications.
In 1910, Reginald Fessenden invented a two-tone method of transmitting Morse code. Dots and dashes were different tones of equal length. The intent was to minimize transmission time.
Some early CW transmitters employed an arc converter that could not be conveniently keyed. Instead of turning the arc on and off, the key slightly changed the transmitter frequency in a technique known as the "compensation-wave method". The compensation-wave was not used at the receiver. The method consumed a lot of bandwidth and caused interference, so it was discouraged by 1921.
Most early telephone-line modems used audio frequency-shift keying (AFSK) to send and receive data at rates up to about 1200 bits per second. The common Bell 103 and Bell 202 modems used this technique. Even today, North American caller ID uses 1200 baud AFSK in the form of the Bell 202 standard. Some early microcomputers used a specific form of AFSK modulation, the Kansas City standard, to store data on audio cassettes . AFSK is still widely used in amateur radio, as it allows data transmission through unmodified voiceband equipment. Radio control gear uses FSK, but calls it FM and PPM instead.
AFSK is also used in the United States' Emergency Alert System to transmit warning information . It is used at higher bitrates for Weathercopy used on Weatheradio by NOAA in the U.S.
The CHU shortwave radio station in Ottawa, Canada broadcasts an exclusive digital time signal encoded using AFSK modulation.
FSK is commonly used in Caller ID and remote metering applications: see FSK standards for use in Caller ID and remote metering for more details

</doc>
<doc id="41194" url="http://en.wikipedia.org/wiki?curid=41194" title="Frequency standard">
Frequency standard

A frequency standard is a stable oscillator used for frequency calibration or reference. A frequency standard generates a fundamental frequency with a high degree of accuracy and precision. Harmonics of this fundamental frequency are used to provide reference points.
Since time is the reciprocal of frequency, it is relatively easy to derive a time standard from a frequency standard. A standard clock comprises a frequency standard, a device to count off the cycles of the oscillation emitted by the frequency standard, and a means of displaying or outputting the result.
Frequency standards in a network or facility are sometimes administratively designated as "primary" or "secondary". The terms "primary" and "secondary", as used in this context, should not be confused with the respective technical meanings of these words in the discipline of precise time and frequency.
Frequency reference.
A frequency reference is an instrument used for providing a stable frequency of some kind. There are different sorts of frequency references, acoustic ones such as tuning forks but also electrical ones that emit a signal of a certain frequency (a frequency standard).
Among the most stable frequency references in the world are cesium standards, including cesium fountains, and hydrogen masers. Cesium standards are widely recognized as having better long-term stability, whereas hydrogen masers can attain superior short-term performance; therefore, several national standards laboratories use ensembles of cesium standards and hydrogen masers in order to combine the best attributes of both.
The carrier of time signal transmitters, LORAN-C transmitters and of several longwave and mediumwave broadcasting stations is derived from an atomic clock and can be therefore used as frequency standard.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41196" url="http://en.wikipedia.org/wiki?curid=41196" title="Fresnel zone">
Fresnel zone

In optics and radio communications (indeed, in any situation involving the radiation of waves, which includes electrodynamics, acoustics, gravitational radiation and seismology), a Fresnel zone ( ), named for physicist Augustin-Jean Fresnel, is one of a (theoretically infinite) number of concentric ellipsoids which define volumes in the radiation pattern of a (usually) circular aperture. Fresnel zones result from diffraction by the circular aperture. The cross section of the first (innermost) Fresnel zone is circular. Subsequent Fresnel zones are annular (doughnut-shaped) in cross section, and concentric with the first. 
Importance of Fresnel zones.
If unobstructed, radio waves will travel in a straight line from the transmitter to the receiver. But if there are reflective surfaces along the path, such as bodies of water or smooth terrain, the radio waves reflecting off those surfaces may arrive either out of phase or in phase with the signals that travel directly to the receiver. Waves that reflect off of surfaces within an even Fresnel zone are out of phase with the direct-path wave and reduce the power of the received signal. Waves that reflect off of surfaces within an odd Fresnel zone are in phase with the direct-path wave and can enhance the power of the received signal. Sometimes this results in the counter-intuitive finding that reducing the height of an antenna increases the signal-to-noise ratio.
Fresnel provided a means to calculate where the zones are--where a given obstacle will cause mostly in phase or mostly out of phase reflections between the transmitter and the receiver. Obstacles in the first Fresnel zone will create signals with a path-length phase shift of 0 to 180 degrees, in the second zone they will be 180 to 360 degrees out of phase, and so on. Even numbered zones have the maximum phase cancelling effect and odd numbered zones may actually add to the signal power.
To maximize receiver strength, one needs to minimize the effect of obstruction loss by removing obstacles from the radio frequency line of sight (RF LoS). The strongest signals are on the direct line between transmitter and receiver and always lie in the first Fresnel zone.
Determining Fresnel zone clearance.
The concept of Fresnel zone clearance may be used to analyze interference by obstacles near the path of a radio beam. The first zone must be kept largely free from obstructions to avoid interfering with the radio reception. However, some obstruction of the Fresnel zones can often be tolerated. 
For establishing Fresnel zones, first determine the RF Line of Sight (RF LoS), which in simple terms is a straight line between the transmitting and receiving antennas. Now the zone surrounding the RF LoS is said to be the Fresnel zone.
The general equation for calculating the Fresnel zone radius at any point P in between the endpoints of the link is the following:
where,
Fn = The nth Fresnel Zone radius in metres
d1 = The distance of P from one end in metres
d2 = The distance of P from the other end in metres
formula_2 = The wavelength of the transmitted signal in metres
The cross sectional radius of each Fresnel zone is the longest at the midpoint of the RF LoS, shrinking to a point at the antenna on each end. For practical applications, it is often useful to know the maximum radius of the first Fresnel zone. From the above formula, the following formulas can be derived, using formula_3, formula_4, and formula_5. Now we have an easy way to calculate the radius of the first Fresnel zone (F1 in the above equation), knowing the distance between the two antennas and the frequency of the transmitted signal.
In SI:
Or in imperial units:

</doc>
<doc id="41197" url="http://en.wikipedia.org/wiki?curid=41197" title="Front-to-back ratio">
Front-to-back ratio

In telecommunication, the term front-to-back ratio ("also known as front-to-rear ratio") can mean:
The ratio compares the antenna gain in a specified direction, "i.e.", azimuth, usually that of maximum gain, to the gain in a direction 180° from the specified azimuth. A front-to-back ratio is usually expressed in dB. 
In point-to-point microwave antennas, a "high performance" antenna usually has a higher front to back ratio than other antennas. For example, an unshrouded 38 GHz microwave dish may have a front to back ratio of 64 dB, while the same size reflector equipped with a shroud would have a front to back ratio of 70 dB. Other factors affecting the front to back ratio of a parabolic microwave antenna include the material of the dish and the precision with which the reflector itself was formed.
In other electrical engineering the front to back ratio is a ratio of parameters used to characterize rectifiers or other devices, in which electrical current, signal strength, resistance, or other parameters, in one direction is compared with that in the opposite direction. 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41199" url="http://en.wikipedia.org/wiki?curid=41199" title="FTS2000">
FTS2000

Federal Telecommunications System 2000 (FTS2000) is a long distance telecommunications service for the United States federal government, including services such as switched voice service for voice or data up to 4.8 kbit/s, switched data at 56 kbit/s and 64 kbit/s, switched digital integrated service for voice, data, image, and video up to 1.544 Mbit/s, packet switched service for data in packet form, video transmission for both compressed and wideband video, and dedicated point-to-point private line for voice and data. 
"Note 1:" Use of FTS2000 contract services is mandatory for use by U.S. Government agencies for all acquisitions subject to 40 U.S.C. 759. 
"Note 2:" No U.S. Government information processing equipment or customer premises equipment other than that which are required to provide an FTS2000 service are furnished. 
"Note 3:" The FTS2000 contractors will be required to provide service directly to an agency's terminal equipment interface. For example, the FTS2000 contractor might provide a terminal adapter to an agency location in order to connect FTS2000 ISDN services to the agency's terminal equipment. 
"Note 4:" GSA awarded two 10-year, fixed-price contracts covering FTS2000 services on December 7, 1988. 
"Note 5:" The Warner Amendment excludes the mandatory use of FTS2000 in instances related to maximum security.
FTS2000 was completed in 2000, then replaced by FTS2001, and thereafter, in 2008, by Networx.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41200" url="http://en.wikipedia.org/wiki?curid=41200" title="Full width at half maximum">
Full width at half maximum

Full width at half maximum (FWHM) is an expression of the extent of a function, given by the difference between the two extreme values of the independent variable at which the dependent variable is equal to half of its maximum value. Half width at half maximum (HWHM) is half of the FWHM.
FWHM is applied to such phenomena as the duration of pulse waveforms and the spectral width of sources used for optical communications and the resolution of spectrometers.
The term full duration at half maximum (FDHM) is preferred when the independent variable is time.
The convention of "width" meaning "half maximum" is also widely used in signal processing to define bandwidth as "width of frequency range where less than half the signal's power is attenuated", i.e., the power is at least half the maximum. In signal processing terms, this is at most −3 dB of attenuation, called "half power point".
If the considered function is the normal distribution of the form
where formula_2 is the standard deviation and formula_3 can be any value (the width of the function does not depend on translation), then the relationship between FWHM and the standard deviation is
In spectroscopy half the width at half maximum (here γ), HWHM, is in common use. For example, a Lorentzian/Cauchy distribution of height (1/πγ) can be defined by
Another important distribution function, related to solitons in optics, is the hyperbolic secant:
Any translating element was omitted, since it does not affect the FWHM. For this impulse we have:
where "arsech" is the inverse hyperbolic secant.

</doc>
<doc id="41201" url="http://en.wikipedia.org/wiki?curid=41201" title="Functional profile">
Functional profile

In telecommunication, a functional profile is a standardization document that characterizes the requirements of a standard or group of standards, and specifies how the options and ambiguities in the standard(s) should be interpreted or implemented to (a) provide a particular information technology function, (b) provide for the development of uniform, recognized tests, and (c) promote interoperability among different network elements and terminal equipment that implement a specific profile.
Sources.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41202" url="http://en.wikipedia.org/wiki?curid=41202" title="Fuse">
Fuse

Fuse or fuze may refer to:

</doc>
<doc id="41203" url="http://en.wikipedia.org/wiki?curid=41203" title="Garble">
Garble


</doc>
<doc id="41204" url="http://en.wikipedia.org/wiki?curid=41204" title="Gateway">
Gateway

Gateway may refer to:

</doc>
<doc id="41205" url="http://en.wikipedia.org/wiki?curid=41205" title="Gating">
Gating

Gating may refer to:
Other.
and see also

</doc>
<doc id="41206" url="http://en.wikipedia.org/wiki?curid=41206" title="Gaussian beam">
Gaussian beam

In optics, a Gaussian beam is a beam of electromagnetic radiation whose transverse electric field and intensity (irradiance) distributions are well approximated by Gaussian functions. Many lasers emit beams that approximate a Gaussian profile, in which case the laser is said to be operating on the "fundamental transverse mode", or "TEM00 mode" of the laser's optical resonator. When refracted by a diffraction-limited lens, a Gaussian beam is transformed into another Gaussian beam (characterized by a different set of parameters), which explains why it is a convenient, widespread model in laser optics.
The mathematical function that describes the Gaussian beam is a solution to the paraxial form of the Helmholtz equation. The solution, in the form of a Gaussian function, represents the complex amplitude of the beam's electric field. The electric field and magnetic field together propagate as an electromagnetic wave. A description of just one of the two fields is sufficient to describe the properties of the beam.
The behavior of the field of a Gaussian beam as it propagates is described by a few parameters such as the spot size, the radius of curvature, and the Gouy phase.
Other solutions to the paraxial form of the Helmholtz equation exist. Solving the equation in Cartesian coordinates leads to a family of solutions known as the Hermite–Gaussian modes, while solving the equation in cylindrical coordinates leads to the Laguerre–Gaussian modes. For both families, the lowest-order solution describes a Gaussian beam, while higher-order solutions describe higher-order transverse modes in an optical resonator.
Mathematical form.
The Gaussian beam is a transverse electromagnetic (TEM) mode. A mathematical expression for its complex electric field amplitude can be found by solving the paraxial Helmholtz equation, yielding
where
Additionally, the field has a time dependence factor formula_12 that has been suppressed in the above expression.
The corresponding time-averaged intensity (or irradiance) distribution is
where formula_14 is the intensity at the center of the beam at its waist. The constant formula_15 is the characteristic impedance of the medium in which the beam is propagating. For free space, formula_16.
Beam parameters.
The geometry and behavior of a Gaussian beam are governed by a set of beam parameters, which are defined in the following sections.
Beam width or spot size.
For a Gaussian beam propagating in free space, the spot size (radius) "w"("z") will be at a minimum value "w"0 at one place along the beam axis, known as the "beam waist". For a beam of wavelength λ at a distance "z" along the beam from the beam waist, the variation of the spot size is given by
where the origin of the z-axis is defined, without loss of generality, to coincide with the beam waist, and where
is called the Rayleigh range.
Rayleigh range and confocal parameter.
At a distance from the waist equal to the Rayleigh range "z"R, the width "w" of the beam is
The distance between these two points is called the "confocal parameter" or "depth of focus" of the beam:
Radius of curvature.
"R"("z") is the "radius of curvature" of the wavefronts comprising the beam. Its value as a function of position is
Beam divergence.
The parameter formula_8 increases linearly with formula_3 for formula_24. This means that far from the waist, the beam is cone-shaped. The angle between the straight line formula_25 and the central axis of the beam (formula_26) is called the "divergence" of the beam. It is given by
The total angular spread of the beam far from the waist is then given by
Because the divergence is inversely proportional to the spot size, a Gaussian beam that is focused to a small spot spreads out rapidly as it propagates away from that spot. To keep a laser beam very well collimated, it must have a large diameter. This relationship between beam width and divergence is due to diffraction. Non-Gaussian beams also exhibit this effect, but a Gaussian beam is a special case where the product of width and divergence is the smallest possible.
Since the gaussian beam model uses the paraxial approximation, it fails when wavefronts are tilted by more than about 30° from the direction of propagation. From the above expression for divergence, this means the Gaussian beam model is valid only for beams with waists larger than about formula_29.
Laser beam quality is quantified by the beam parameter product (BPP). For a Gaussian beam, the BPP is the product of the beam's divergence and waist size formula_30. The BPP of a real beam is obtained by measuring the beam's minimum diameter and far-field divergence, and taking their product. The ratio of the BPP of the real beam to that of an ideal Gaussian beam at the same wavelength is known as "M"2 ("M squared"). The "M"2 for a Gaussian beam is one. All real laser beams have "M"2 values greater than one, although very high quality beams can have values very close to one.
The numerical aperture of a Gaussian beam is defined to be formula_31, where "n" is the index of refraction of the medium through which the beam propagates. This means that the Rayleigh range is related to the numerical aperture by 
Gouy phase.
The on-axis "longitudinal phase delay" or "Gouy phase" of the beam is
The Gouy phase shift indicates that as a Gaussian beam passes through a focus, it acquires an additional phase shift of π, in addition to the usual formula_34 phase shift that would be expected from a plane wave.
Complex beam parameter.
Information about the spot size and radius of curvature of a Gaussian beam can be encoded in the complex beam parameter, formula_35:
The reciprocal formula_37 shows the relationship between formula_35, formula_8, and formula_10 explicitly:
The complex beam parameter plays a key role in the analysis of Gaussian beam propagation, and especially in the analysis of optical resonator cavities using ray transfer matrices.
In terms of the complex beam parameter formula_42, a Gaussian field with one transverse dimension is proportional to
In two dimensions one can write the potentially elliptical or astigmatic beam as the product
which for the common case of circular symmetry where formula_45 and formula_46 yields
where formula_48 is integer, formula_49 is real valued, formula_50 is the gamma function and formula_51 is a confluent hypergeometric
function.
Some subfamilies of hypergeometric-Gaussian (HyGG) modes can be listed as the modified Bessel-Gaussian modes, the modified exponential Gaussian modes, and the modified Laguerre–Gaussian modes.
The set of hypergeometric-Gaussian modes is overcomplete and is not an orthogonal set of modes. In spite of its complicated field profile, HyGG modes have a very simple profile at the pupil plane (formula_52):
See Optical vortex, which explains that the outcoming wave from a pitch-fork hologram is a sub-family of HyGG modes. The HyGG profile while beam propagates along formula_54 has a dramatic change and it is not a stable mode below the Rayleigh range.

</doc>
<doc id="41207" url="http://en.wikipedia.org/wiki?curid=41207" title="Gel">
Gel

A gel (coined by 19th-century Scottish chemist Thomas Graham, by clipping from "gelatine") is a solid, jelly-like material that can have properties ranging from soft and weak to hard and tough. Gels are defined as a substantially dilute cross-linked system, which exhibits no flow when in the steady-state. By weight, gels are mostly liquid, yet they behave like solids due to a three-dimensional cross-linked network within the liquid. It is the crosslinking within the fluid that give a gel its structure (hardness) and contribute to the adhesive stick (tack). In this way gels are a dispersion of molecules of a liquid within a solid in which the solid is the continuous phase and the liquid is the discontinuous phase.
IUPAC definition
 Gel: Nonfluid colloidal network or polymer network that is expanded throughout its whole volume by a fluid.
"Note 1": A gel has a finite, usually rather small, yield stress.
"Note 2": A gel can contain:
"Note 3": Corrected from ref., where the definition is via the property identified in Note 1 (above) rather than of the structural characteristics that describe a gel.
Hydrogel: "Gel" in which the swelling agent is water.
"Note 1": The "network" component of a hydrogel is usually a "polymer network".
"Note 2": A hydrogel in which the "network" component is a "colloidal network" may be referred to as an "aquagel".
"Note 3": Definition quoted from refs. 
Composition.
Gels consist of a solid three-dimensional network that spans the volume of a liquid medium and ensnares it through surface tension effects. This internal network structure may result from physical bonds (physical gels) or chemical bonds (chemical gels), as well as crystallites or other junctions that remain intact within the extending fluid. Virtually any fluid can be used as an extender including water (hydrogels), oil, and air (aerogel). Both by weight and volume, gels are mostly fluid in composition and thus exhibit densities similar to those of their constituent liquids. Edible jelly is a common example of a hydrogel and has approximately the density of water.
Polyionic polymers.
Polyionic polymers are polymers with an ionic functional group. The ionic charges prevent the formation of tightly coiled polymer chains. This allows them to contribute more to viscosity in their stretched state, because the stretched-out polymer takes up more space. See Polyelectrolyte for more information.
Types.
Hydrogels.
A hydrogel is a network of polymer chains that are hydrophilic, sometimes found as a colloidal gel in which water is the dispersion medium. Hydrogels are highly absorbent (they can contain over 90% water) natural or synthetic polymeric networks.
Hydrogels also possess a degree of flexibility very similar to natural tissue, due to their significant water content.
The first appearance of the term 'hydrogel' in the literature was in 1894.
Common uses for hydrogels include:
Other, less common uses include
Common ingredients include polyvinyl alcohol, sodium polyacrylate, acrylate polymers and copolymers with an abundance of hydrophilic groups.
Natural hydrogel materials are being investigated for tissue engineering; these materials include agarose, methylcellulose, hyaluronan, and other naturally derived polymers.
Organogels.
An organogel is a non-crystalline, non-glassy thermoreversible (thermoplastic) solid material composed of a liquid organic phase entrapped in a three-dimensionally cross-linked network. The liquid can be, for example, an organic solvent, mineral oil, or vegetable oil. The solubility and particle dimensions of the structurant are important characteristics for the elastic properties and firmness of the organogel. Often, these systems are based on self-assembly of the structurant molecules.
Organogels have potential for use in a number of applications, such as in pharmaceuticals, cosmetics, art conservation, and food. An example of formation of an undesired thermoreversible network is the occurrence of wax crystallization in petroleum.
Xerogels.
A xerogel is a solid formed from a gel by drying with unhindered shrinkage. Xerogels usually retain high porosity (15–50%) and enormous surface area (150–900 m2/g), along with very small pore size (1–10 nm). When solvent removal occurs under supercritical conditions, the network does not shrink and a highly porous, low-density material known as an "aerogel" is produced. Heat treatment of a xerogel at elevated temperature produces viscous sintering (shrinkage of the xerogel due to a small amount of viscous flow) and effectively transforms the porous gel into a dense glass.
Nanocomposite Hydrogels.
Nanocomposite hydrogels are also known as hybrid hydrogels, can be defined as highly hydrated polymeric networks, either physically or covalently crosslinked with each other and/or with nanoparticles or nanostructures. Nanocomposite hydrogels can mimic native tissue properties, structure and microenvironment due to their hydrated and interconnected porous structure. A wide range of nanoparticles, such as carbon-based, polymeric, ceramic, and metallic nanomaterials can be incorporated within the hydrogel structure to obtain nanocomposites with tailored functionality. Nanocomposite hydrogels can be engineered to possess superior physical, chemical, electrical, and biological properties.
Properties.
Many gels display thixotropy – they become fluid when agitated, but resolidify when resting.
In general, gels are apparently solid, jelly-like materials. 
By replacing the liquid with gas it is possible to prepare aerogels, materials with exceptional properties including very low density, high specific surface areas, and excellent thermal insulation properties.
Animal produced.
Some species secrete gels that are effective in parasite control. For example, the long-finned pilot whale secretes an enzymatic gel that rests on the outer surface of this animal and helps prevent other organisms from establishing colonies on the surface of these whales' bodies.
Hydrogels existing naturally in the body include mucus, the vitreous humor of the eye, cartilage, tendons and blood clots. Their viscoelastic nature results in the soft tissue component of the body, disparate from the mineral-based hard tissue of the skeletal system. Researchers are actively developing synthetically derived tissue replacement technologies derived from hydrogels, for both temporary implants (degradable) and permanent implants (non-degradable). A review article on the subject discusses the use of hydrogels for nucleus pulposus replacement, cartilage replacement, and synthetic tissue models.
Applications.
Many substances can form gels when a suitable thickener or gelling agent is added to their formula. This approach is common in manufacture of wide range of products, from foods to paints and adhesives.
In fiber optics communications, a soft gel resembling "hair gel" in viscosity is used to fill the plastic tubes containing the fibers. The main purpose of the gel is to prevent water intrusion if the buffer tube is breached, but the gel also buffers the fibers against mechanical damage when the tube is bent around corners during installation, or flexed. Additionally, the gel acts as a processing aid when the cable is being constructed, keeping the fibers central whilst the tube material is extruded around it.

</doc>
<doc id="41210" url="http://en.wikipedia.org/wiki?curid=41210" title="Geostationary orbit">
Geostationary orbit

A geostationary orbit, geostationary Earth orbit or geosynchronous equatorial orbit (GEO) is a circular orbit 35786 km above the Earth's equator and following the direction of the Earth's rotation. An object in such an orbit has an orbital period equal to the Earth's rotational period (one sidereal day), and thus appears motionless, at a fixed position in the sky, to ground observers. Communications satellites and weather satellites are often placed in geostationary orbits, so that the satellite antennas which communicate with them do not have to rotate to track them, but can be pointed permanently at the position in the sky where they stay. Using this characteristic, ocean color satellites with visible sensors (e.g. the Geostationary Ocean Color Imager (GOCI)) can also be operated in geostationary orbit in order to monitor sensitive changes of ocean environments.
A geostationary orbit is a particular type of geosynchronous orbit, the distinction being that while an object in geosynchronous orbit returns to the same point in the sky at the same time each day, an object in geostationary orbit never leaves that position.
The notion of a geosynchronous satellite for communication purposes was first published in 1928 (but not widely so) by Herman Potočnik. The first appearance of a geostationary orbit in popular literature was in the first Venus Equilateral story by George O. Smith, but Smith did not go into details. British science fiction author Arthur C. Clarke disseminated the idea widely, with more details on how it would work, in a 1945 paper entitled "Extra-Terrestrial Relays — Can Rocket Stations Give Worldwide Radio Coverage?", published in "Wireless World" magazine. Clarke acknowledged the connection in his introduction to "The Complete Venus Equilateral". The orbit, which Clarke first described as useful for broadcast and relay communications satellites, is sometimes called the Clarke Orbit. Similarly, the Clarke Belt is the part of space about 35786 km above sea level, in the plane of the Equator, where near-geostationary orbits may be implemented. The Clarke Orbit is about 265000 km long.
Practical uses.
Most commercial communications satellites, broadcast satellites and SBAS satellites operate in geostationary orbits. A geostationary transfer orbit is used to move a satellite from low Earth orbit (LEO) into a geostationary orbit. The first satellite placed into a geostationary orbit was the Syncom-3, launched by a Delta D rocket in 1964.
A worldwide network of operational geostationary meteorological satellites is used to provide visible and infrared images of Earth's surface and atmosphere. These satellite systems include:
A statite, a hypothetical satellite that uses a solar sail to modify its orbit, could theoretically hold itself in a geostationary "orbit" with different altitude and/or inclination from the "traditional" equatorial geostationary orbit.
Orbital stability.
A geostationary orbit can only be achieved at an altitude very close to 35786 km, and directly above the Equator. This equates to an orbital velocity of 3.07 km/s or an orbital period of 1,436 minutes, which equates to almost exactly one sidereal day or 23.934461223 hours. This ensures that the satellite will match the Earth's rotational period and has a stationary footprint on the ground. All geostationary satellites have to be located on this ring.
A combination of lunar gravity, solar gravity, and the flattening of the Earth at its poles causes a precession motion of the orbital plane of any geostationary object, with an orbital period of about 53 years and an initial inclination gradient of about 0.85 degrees per year, achieving a maximum inclination of 15 degrees after 26.5 years. To correct for this orbital perturbation, regular orbital stationkeeping manoeuvres are necessary, amounting to a delta-v of approximately 50 m/s per year.
A second effect to be taken into account is the longitude drift, caused by the asymmetry of the Earth – the Equator is slightly elliptical. There are two stable (at 75.3°E, and at 104.7°W) and two unstable (at 165.3°E, and at 14.7°W) equilibrium points. Any geostationary object placed between the equilibrium points would (without any action) be slowly accelerated towards the stable equilibrium position, causing a periodic longitude variation. The correction of this effect requires orbit control manoeuvres with a maximum delta-v of about 2 m/s per year, depending on the desired longitude.
Solar wind and radiation pressure also exert small forces on satellites which, over time, cause them to slowly drift away from their prescribed orbits.
In the absence of servicing missions from the Earth or a renewable propulsion method, the consumption of thruster propellant for station-keeping places a limitation on the lifetime of the satellite.
Communications.
Satellites in geostationary orbits are far enough away from Earth that communication latency becomes significant — about a quarter of a second for a trip from one ground-based transmitter to the satellite and back to another ground-based transmitter; close to half a second for a round-trip communication from one Earth station to another and then back to the first.
For example, for ground stations at latitudes of "φ" = ±45° on the same meridian as the satellite, the time taken for a signal to pass from Earth to the satellite and back again can be computed using the cosine rule, given the geostationary orbital radius "r" (derived below), the Earth's radius "R" and the speed of light "c", as
This delay presents problems for latency-sensitive applications such as voice communication.
Geostationary satellites are directly overhead at the Equator, and become lower in the sky the further north or south one travels. As the observer's latitude increases, communication becomes more difficult due to factors such as atmospheric refraction, Earth's thermal emission, line-of-sight obstructions, and signal reflections from the ground or nearby structures. At latitudes above about 81°, geostationary satellites are below the horizon and cannot be seen at all. Because of this, some Russian communication satellites have used elliptical Molniya and Tundra orbits, which have excellent visibility at high latitudes.
Orbit allocation.
Satellites in geostationary orbit must all occupy a single ring above the Equator. The requirement to space these satellites apart to avoid harmful radio-frequency interference during operations means that there are a limited number of orbital "slots" available, thus only a limited number of satellites can be operated in geostationary orbit. This has led to conflict between different countries wishing access to the same orbital slots (countries near the same longitude but differing latitudes) and radio frequencies. These disputes are addressed through the International Telecommunication Union's allocation mechanism. In the 1976 Bogotá Declaration, eight countries located on the Earth's equator claimed sovereignty over the geostationary orbits above their territory, but the claims gained no international recognition.
Limitations to usable life of geostationary satellites.
When they run out of thruster fuel, the satellites are at the end of their service life as they are no longer able to keep in their allocated orbital position. The transponders and other onboard systems generally outlive the thruster fuel and, by stopping N-S station keeping, some satellites can continue to be used in inclined orbits (where the orbital track appears to follow a figure-eight loop centred on the Equator), or else be elevated to a "graveyard" disposal orbit.
Derivation of geostationary altitude.
In any circular orbit, the centripetal force required to maintain the orbit (Fc) is provided by the gravitational force on the satellite (Fg). To calculate the geostationary orbit altitude, one begins with this equivalence:
By Newton's second law of motion, we can replace the forces F with the mass "m" of the object multiplied by the acceleration felt by the object due to that force:
We note that the mass of the satellite "m" appears on both sides — geostationary orbit is independent of the mass of the satellite.
So calculating the altitude simplifies into calculating the point where the magnitudes of the centripetal acceleration required for orbital motion and the gravitational acceleration provided by Earth's gravity are equal.
The centripetal acceleration's magnitude is:
where "ω" is the angular speed, and "r" is the orbital radius as measured from the Earth's center of mass.
The magnitude of the gravitational acceleration is:
where "M" is the mass of Earth, 5.9736 × 1024 kg, and "G" is the gravitational constant,
6.67428 ± 0.00067 × 10−11 m3 kg−1 s−2.
Equating the two accelerations gives:
The product "GM" is known with much greater precision than either factor alone; it is known as the geocentric gravitational constant "μ" = 398,600.4418 ± 0.0008 km3 s−2:
The angular speed "ω" is found by dividing the angle travelled in one revolution (360° = 2π rad) by the orbital period (the time it takes to make one full revolution). In the case of a geostationary orbit, the orbital period is one sidereal day, or 86,164.09054 seconds). This gives:
The resulting orbital radius is 42164 km. Subtracting the Earth's equatorial radius, 6378 km, gives the altitude of 35786 km.
Orbital speed is calculated by multiplying the angular speed by the orbital radius:
By the same formula we can find the geostationary-type orbit of an object in relation to Mars (this type of orbit above is referred to as an areostationary orbit if it is above Mars). The geocentric gravitational constant GM (which is μ) for Mars has the value of 42,828 km3s−2, and the known rotational period ("T") of Mars is 88,642.66 seconds. Since ω = 2π/"T", using the formula above, the value of ω is found to be approx 7.088218×10−5 s−1. Thus, "r"3 = 8.5243×1012 km3, whose cube root is 20,427 km; subtracting the equatorial radius of Mars (3396.2 km) we have 17,031 km.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41211" url="http://en.wikipedia.org/wiki?curid=41211" title="Graded-index fiber">
Graded-index fiber

In fiber optics, a graded-index or gradient-index fiber is an optical fiber whose core has a refractive index that decreases with increasing radial distance from the optical axis of the fiber.
Because parts of the core closer to the fiber axis have a higher refractive index than the parts near the cladding, light rays follow sinusoidal paths down the fiber. The most common refractive index profile for a graded-index fiber is very nearly parabolic. The parabolic profile results in continual refocusing of the rays in the core, and minimizes modal dispersion.
Multi-mode optical fiber can be built with either graded index or step index. The advantage of the multi-mode graded index compared to the multi-mode step index is the considerable decrease in modal dispersion. Modal dispersion can be further decreased By selecting a smaller core size (less than 5-10μm) and forming a single mode step index fiber.
This type of fiber is normalized by the International Telecommunications Union ITU-T at recommendation G.651.1.
Pulse dispersion.
Pulse dispersion in a graded index optical fiber is given by
formula_1     
where
formula_2 is the difference in refractive indices of core and cladding,
formula_3 is the refractive index of the cladding,
formula_4 is the length of the fiber taken for observing the pulse dispersion,
formula_5 is the speed of light, and
formula_6 is the constant of graded index profile.

</doc>
<doc id="41212" url="http://en.wikipedia.org/wiki?curid=41212" title="Grade of service">
Grade of service

In telecommunication engineering, and in particular teletraffic engineering, the quality of voice service is specified by two measures: the grade of service (GoS) and the quality of service (QoS).
Grade of service is the probability of a call in a circuit "group" being blocked or delayed for more than a specified interval, expressed as a vulgar fraction or decimal fraction. This is always with reference to the busy hour when the traffic intensity is the greatest. Grade of service may be viewed independently from the perspective of incoming versus outgoing calls, and is not necessarily equal in each direction or between different source-destination pairs.
On the other hand, the quality of service which a "single" circuit is designed or conditioned to provide, e.g. voice grade or program grade is called the quality of service. Quality criteria for such circuits may include equalization for amplitude over a specified band of frequencies, or in the case of digital data transported via analogue circuits, may include equalization for phase. Criteria for mobile quality of service in cellular telephone circuits include the probability of abnormal termination of the call.
What is Grade of Service and how is it measured?
When a user attempts to make a telephone call, the routing equipment handling the call has to determine whether to accept the call, reroute the call to alternative equipment, or reject the call entirely. Rejected calls occur as a result of heavy traffic loads (congestion) on the system and can result in the call either being delayed or lost. If a call is delayed, the user simply has to wait for the traffic to decrease, however if a call is lost then it is removed from the system.
The Grade of Service is one aspect of the quality a customer can expect to experience when making a telephone call. In a Loss System, the Grade of Service is described as that proportion of calls that are lost due to congestion in the busy hour. 
For a Lost Call system, the Grade of Service can be measured using "Equation 1". 
For a delayed call system, the Grade of Service is measured using three separate terms:
Where and when is Grade of Service measured?
The Grade of Service can be measured using different sections of a network. When a call is routed from one end to another, it will pass through several exchanges. If the Grade of Service is calculated based on the number of calls rejected by the final circuit group, then the Grade of Service is determined by the final circuit group blocking criteria. If the Grade of Service is calculated based on the number of rejected calls between exchanges, then the Grade of Service is determined by the exchange-to-exchange blocking criteria.
The Grade of Service should be calculated using both the access networks and the core networks as it is these networks that allow a user to complete an end-to-end connection. Furthermore, the Grade of Service should be calculated from the average of the busy hour traffic intensities of the 30 busiest traffic days of the year. This will cater for most scenarios as the traffic intensity will seldom exceed the reference level.
The grade of service is a measure of the ability of a user to access a trunk system during the busiest hour. The busy is based upon customer demand at the busiest hour during a week month or year.
Class of Service.
Different telecommunications applications require different Qualities of Service. For example, if a telecommunications service provider decides to offer different qualities of voice connection, then a premium voice connection will require a better connection quality compared to an ordinary voice connection. Thus different Qualities of Service are appropriate, depending on the intended use. To help telecommunications service providers to market their different services, each service is placed into a specific class. Each Class of Service determines the level of service required.
To identify the Class of Service for a specific service, the network’s switches and routers examine the call based on several factors. Such factors can include:
Quality of Service in broadband networks.
In broadband networks, the Quality of Service is measured using two criteria. The first criterion is the probability of packet losses or delays in already accepted calls. The second criterion refers to the probability that a new incoming call will be rejected or blocked. To avoid the former, broadband networks limit the number of active calls so that packets from established calls will not be lost due to new calls arriving. As in circuit-switched networks, the Grade of Service can be calculated for individual switches or for the whole network.
Maintaining a Grade of Service.
The telecommunications provider is usually aware of the required Grade of Service for a particular product. To achieve and maintain a given Grade of Service, the operator must ensure that sufficient telecommunications circuits or routes are available to meet a specific level of demand. It should also be kept in mind that too many circuits will create a situation where the operator is providing excess capacity which may never be used, or at the very least may be severely underutilized. This adds costs which must be borne by other parts of the network. To determine the correct number of circuits that are required, telecommunications service providers make use of Traffic Tables. An example of a Traffic Table can be viewed in "Figure 1". It follows that in order for a telecommunications network to continue to offer a given Grade of Service, the number of circuits provided in a circuit group must increase (non-linearly) if the traffic intensity increases.
Erlang's lost call assumptions.
To calculate the Grade of Service of a specified group of circuits or routes, Agner Krarup Erlang used a set of assumptions that relied on the network losing calls when all circuits in a group were busy. These assumptions are:
From these assumptions Erlang developed the Erlang-B formula which describes the probability of congestion in a circuit group. The probability of congestion gives the Grade of Service experienced.
Calculating the Grade of Service.
To determine the Grade of Service of a network when the traffic load and number of circuits are known, telecommunications network operators make use of "Equation 2", which is the Erlang-B equation. 
"A" = Expected traffic intensity in Erlangs,
"N" = Number of circuits in group.
This equation allows operators to determine whether each of their circuit groups meet the required Grade of Service, simply by monitoring the reference traffic intensity. 

</doc>
<doc id="41214" url="http://en.wikipedia.org/wiki?curid=41214" title="Graphic character">
Graphic character

In ISO/IEC 646 (commonly known as ASCII) and related standards including ISO 8859 and Unicode, a graphic character is any character intended to be written, printed, or otherwise displayed in a form that can be read by humans. In other words, it is any encoded character that is associated with one or more glyphs.
ISO/IEC 646.
In ISO 646, graphic characters are contained in rows 2 through 7 of the code table. However, two of the characters in these rows, namely the space character SP at row 2 column 0 and the delete character DEL (also called the rubout character) at row 7 column 15, require special mention.
The space is considered to be "both" a graphic character and a control character in ISO 646; this is probably due to it having a visible form on computer terminals but a control function (of moving the print head) on teletypes. 
The delete character is strictly a control character, not a graphic character. This is true not only in ISO 646, but also in all related standards including Unicode. However, many modern character sets deviate from ISO 646, and as a result a graphic character might occupy the position originally reserved for the delete character.
Unicode.
In Unicode, Graphic characters are those with General Category Letter, Mark, Number, Punctuation, Symbol or Zs=space. Other code points (General categories Control, Zl=line separator, Zp=paragraph separator) are Format, Control, Private Use, Surrogate, Noncharacter or Reserved (unassigned).
Spacing and non-spacing characters.
Most graphic characters are spacing characters, which means that each instance of a spacing character has to occupy some area in a graphic representation. For a teletype or a typewriter this implies moving of the carriage after typing of a character. In the context of text mode display, each spacing character occupies one rectangular character box of equal sizes. Or maybe two adjacent ones, for non-alphabetic characters of East Asian languages. If a text is rendered using proportional fonts, widths of character boxes are not equal, but are positive.
There exists also "non-spacing" graphic characters. Most of non-spacing characters are "modifiers", also called combining characters in Unicode, such as diacritical marks. Although non-spacing graphic characters are uncommon in traditional code pages, there are many such in Unicode. A combining character has its distinct glyph, but it applies to a character box of another character, a spacing one. In some historical systems such as line printers this was implemented as overstrike.
Note that not all modifiers are non-spacing – there exists Spacing Modifier Letters Unicode block.

</doc>
<doc id="41215" url="http://en.wikipedia.org/wiki?curid=41215" title="Ground (electricity)">
Ground (electricity)

In electrical engineering, ground or earth is the reference point in an electrical circuit from which voltages are measured, a common return path for electric current, or a direct physical connection to the Earth.
Electrical circuits may be connected to ground (earth) for several reasons. In mains powered equipment, exposed metal parts are connected to ground to prevent user contact with dangerous voltage if electrical insulation fails. Connections to ground limit the build-up of static electricity when handling flammable products or electrostatic-sensitive devices. In some telegraph and power transmission circuits, the earth itself can be used as one conductor of the circuit, saving the cost of installing a separate return conductor (see single-wire earth return).
For measurement purposes, the Earth serves as a (reasonably) constant potential reference against which other potentials can be measured. An electrical ground system should have an appropriate current-carrying capability to serve as an adequate zero-voltage reference level. In electronic circuit theory, a "ground" is usually idealized as an infinite source or sink for charge, which can absorb an unlimited amount of current without changing its potential. Where a real ground connection has a significant resistance, the approximation of zero potential is no longer valid. Stray voltages or earth potential rise effects will occur, which may create noise in signals or if large enough will produce an electric shock hazard.
The use of the term ground (or earth) is so common in electrical and electronics applications that circuits in portable electronic devices such as cell phones and media players as well as circuits in vehicles may be spoken of as having a "ground" connection without any actual connection to the Earth, despite "common" being a more appropriate term for such a connection. This is usually a large conductor attached to one side of the power supply (such as the "ground plane" on a printed circuit board) which serves as the common return path for current from many different components in the circuit.
History.
Long-distance electromagnetic telegraph systems from 1820 onwards used two or more wires to carry the signal and return currents. It was then discovered, probably by the German scientist Carl August Steinheil in 1836–1837, that the ground could be used as the return path to complete the circuit, making the return wire unnecessary. However, there were problems with this system, exemplified by the transcontinental telegraph line constructed in 1861 by the Western Union Company between Saint Joseph, Missouri, and Sacramento, California. During dry weather, the ground connection often developed a high resistance, requiring water to be poured on the ground rod to enable the telegraph to work or phones to ring.
Later, when telephony began to replace telegraphy, it was found that the currents in the earth induced by power systems, electrical railways, other telephone and telegraph circuits, and natural sources including lightning caused unacceptable interference to the audio signals, and the two-wire or 'metallic circuit' system was reintroduced around 1883.
Radio communications.
An electrical connection to earth can be used as a reference potential for radio frequency signals for certain kinds of antennas. The part directly in contact with the earth - the "earth electrode" - can be as simple as a metal rod or stake driven into the earth, or a connection to buried metal water piping (the pipe must be conductive). Because high frequency signals can flow to earth due to capacitative effects, capacitance to ground is an important factor in effectiveness of signal grounds. Because of this, a complex system of buried rods and wires can be effective. An ideal signal ground maintains a fixed potential (zero) regardless of how much electric current flows into ground or out of ground. Low impedance at the signal frequency of the electrode-to-earth connection determines its quality, and that quality is improved by increasing the surface area of the electrode in contact with the earth, increasing the depth to which it is driven, using several connected ground rods, increasing the moisture content of the soil, improving the conductive mineral content of the soil, and increasing the land area covered by the ground system.
Some types of transmitting antenna systems in the VLF, LF, MF and lower SW range must have a good ground to operate efficiently. For example, a vertical monopole antenna requires a ground plane that often consists of an interconnected network of wires running radially away from the base of the antenna for a distance about equal to the height of the antenna. Sometimes a counterpoise is used as a ground plane, supported above the ground.
Building wiring installations.
Electrical power distribution systems are often connected to ground to limit the voltage that can appear on distribution circuits. A distribution system insulated from ground may attain a high potential due to transient voltages caused by arcing, static electricity, or accidental contact with higher potential circuits. A ground connection of the system dissipates such potentials and limits the rise in voltage of the grounded system.
In a mains electricity (AC power) wiring installation, the term "ground conductor" typically refers to three different conductors or conductor systems as listed below.
"Equipment earthing conductors" provide an electrical connection between non-current-carrying metallic parts of equipment and the earth. According to the U.S. National Electrical Code (NEC), the reason for doing this is to limit the voltage imposed by lightning, line surges, and contact with higher voltage lines. The equipment earthing conductor is usually also used as the equipment bonding conductor (see below).
"Equipment bonding conductors" provide a low impedance path between non-current-carrying metallic parts of equipment and one of the conductors of that electrical system's source, so that if a part becomes energized for any reason, such as a frayed or damaged conductor, a short circuit will occur and operate a circuit breaker or fuse to disconnect the faulted circuit. The earth itself has no role in this fault-clearing process since current must return to its source; however, the sources are very frequently connected to earth. (see Kirchhoff's circuit laws). By bonding (interconnecting) all exposed non-current carrying metal objects together, they should remain near the same potential thus reducing the chance of a shock. This is especially important in bathrooms where one may be in contact with several different metallic systems such as supply and drain pipes and appliance frames. The equipment bonding conductor is usually also used as the equipment earthing conductor (see above).
A grounding electrode conductor (GEC) connects one leg of an electrical system to one or more earth electrodes. This is called "system grounding" and most systems are required to be grounded. The U.S. NEC and the UK's BS 7671 list systems that are required to be grounded. The grounding electrode conductor connects the leg of the electrical system that is the "neutral wire" to the grounding electrode(s). The grounding electrode conductor is also usually bonded to pipework and structural steel in larger structures. According to the NEC, the purpose of earthing an electrical system is to limit the voltage to earth imposed by lightning events and contact with higher voltage lines, and also to stabilize the voltage to earth during normal operation. In the past, water supply pipes were often used as grounding electrodes, but this was banned where plastic pipes are popular. This type of ground applies to radio antennas and to lightning protection systems.
Permanently installed electrical equipment usually also has permanently connected grounding conductors. Portable electrical devices with metal cases may have them connected to earth ground by a pin in the interconnecting plug (see Domestic AC power plugs and sockets). The size of power ground conductors is usually regulated by local or national wiring regulations.
Earthing systems.
In electricity supply systems, an earthing (grounding) system defines the electrical potential of the conductors relative to that of the Earth's conductive surface. The choice of earthing system has implications for the safety and electromagnetic compatibility of the power supply. Regulations for earthing systems vary considerably between different countries.
A functional earth connection serves a purpose other than providing protection against electrical shock. In contrast to a protective earth connection, a functional earth connection may carry a current during the normal operation of a device. Functional earth connections may be required by devices such as surge suppression and electromagnetic-compatibility filters, some types of antennas and various measurement instruments. Generally the protective earth is also used as a functional earth, though this requires care in some situations.
Impedance grounding.
Distribution power systems may be solidly grounded, with one circuit conductor directly connected to an earth grounding electrode system. Alternatively, some amount of electrical impedance may be connected between the distribution system and ground, to limit the current that can flow to earth. The impedance may be a resistor, or an inductor (coil). In a high-impedance grounded system, the fault current is limited to a few amperes (exact values depend on the voltage class of the system); a low-impedance grounded system will permit several hundred amperes to flow on a fault. A large solidly-grounded distribution system may have thousands of amperes of ground fault current.
In a polyphase AC system, an artificial neutral grounding system may be used. Although no phase conductor is directly connected to ground, a specially constructed transformer (a "zig zag" transformer) blocks the power frequency current from flowing to earth, but allows any leakage or transient current to flow to ground.
Low-resistance grounding systems use a neutral grounding resistor (NGR) to limit the fault current to 25 A or greater. Low resistance grounding systems will have a time rating (say, 10 seconds) that indicates how long the resistor can carry the fault current before overheating. A ground fault protection relay must trip the breaker to protect the circuit before overheating of the resistor occurs.
High-resistance grounding (HRG) systems use an NGR to limit the fault current to 25 A or less. They have a continuous rating, and are designed to operate with a single-ground fault. This means that the system will not immediately trip on the first ground fault. If a second ground fault occurs, a ground fault protection relay must trip the breaker to protect the circuit. On an HRG system, a sensing resistor is used to continuously monitor system continuity. If an open-circuit is detected (e.g., due to a broken weld on the NGR), the monitoring device will sense voltage through the sensing resistor and trip the breaker. Without a sensing resistor, the system could continue to operate without ground protection (since an open circuit condition would mask the ground fault) and transient overvoltages could occur.
Ungrounded systems.
Where the danger of electric shock is high, special ungrounded power systems may be used to minimize possible leakage current to ground. Examples of such installations include patient care areas in hospitals, where medical equipment is directly connected to a patient and must not permit any power-line current to pass into the patient's body. Medical systems include monitoring devices to warn of any increase of leakage current. On wet construction sites or in shipyards, isolation transformers may be provided so that a fault in a power tool or its cable does not expose users to shock hazard.
Circuits used to feed sensitive audio/video production equipment or measurement instruments may be fed from an isolated ungrounded technical power system to limit the injection of noise from the power system.
Power transmission.
In single-wire earth return (SWER) AC electrical distribution systems, costs are saved by using just a single high voltage conductor for the power grid, while routing the AC return current through the earth. This system is mostly used in rural areas where large earth currents will not otherwise cause hazards.
Some high-voltage direct-current (HVDC) power transmission systems use the ground as second conductor. This is especially common in schemes with submarine cables, as sea water is a good conductor. Buried grounding electrodes are used to make the connection to the earth. The site of these electrodes must be chosen carefully to prevent electrochemical corrosion on underground structures.
A particular concern in design of electrical substations is earth potential rise. When very large fault currents are injected into the earth, the area around the point of injection may rise to a high potential with respect to distant points. This is due to the limited finite conductivity of the layers of soil in the earth. The gradient of the voltage (changing voltage within a distance) may be so high that two points on the ground may be at significantly different potentials, creating a hazard to anyone standing on the ground in the area. Pipes, rails, or communication wires entering a substation may see different ground potentials inside and outside the substation, creating a dangerous touch voltage.
Electronics.
Ground symbols
Signal grounds serve as return paths for signals and power (at extra low voltages, less than about 50 V) within equipment, and on the signal interconnections between equipment. Many electronic designs feature a single return that acts as a reference for all signals. Power and signal grounds often get connected, usually through the metal case of the equipment. Designers of printed circuit boards must take care in the layout of electronic systems so that high-power or rapidly-switching currents in one part of a system do not inject noise into low-level sensitive parts of a system due to some common impedance in the grounding traces of the layout.
Circuit ground versus earth.
Voltage is a differential quantity. To measure the voltage of a single point, a reference point must be selected to measure against. This common reference point is called "ground" and considered to have zero voltage. This signal ground may not be connected to a power ground. A system where the system ground is not connected to another circuit or to earth (though there may still be AC coupling) is often referred to as a floating ground.
Separating low signal ground from a noisy ground.
In television stations, recording studios, and other installations where sound quality is critical, a special signal ground known as a "technical ground" (or "technical earth", "special earth" and "audio earth") is often installed, to prevent ground loops. This is basically the same thing as an AC power ground, but no general appliance ground wires are allowed any connection to it, as they may carry electrical interference. For example only audio equipment is connected to the technical ground in a recording studio. In most cases, the studio's metal equipment racks are all joined together with heavy copper cables (or flattened copper tubing or busbars) and similar connections are made to the technical ground. Great care is taken that no general chassis grounded appliances are placed on the racks, as a single AC ground connection to the technical ground will destroy its effectiveness. For particularly demanding applications, the main technical ground may consist of a heavy copper pipe, if necessary fitted by drilling through several concrete floors, such that all technical grounds may be connected by the shortest possible path to a grounding rod in the basement.
Lightning protection systems.
Lightning protection systems are designed to mitigate the effects of lightning through connection to extensive grounding systems that provide a large surface area connection to earth. The large area is required to dissipate the high current of a lightning strike without damaging the system conductors by excess heat. Since lightning strikes are pulses of energy with very high frequency components, grounding systems for lighting protection tend to use short straight runs of conductors to reduce the self-inductance and skin effect.
Bonding.
Strictly speaking, the terms "grounding" or "earthing" are meant to refer to an electrical connection to ground/earth. Bonding is the practice of intentionally electrically connecting metallic items not designed to carry electricity. This brings all the bonded items to the same electrical potential as a protection from electrical shock. The bonded items can then be connected to ground to bring them to earth potential.
Ground (earth) mat.
In an electrical substation a ground (earth) mat is a mesh of conductive material installed at places where a person would stand to operate a switch or other apparatus; it is bonded to the local supporting metal structure and to the handle of the switchgear, so that the operator will not be exposed to a high differential voltage due to a fault in the substation.
In the vicinity of electrostatic sensitive devices, a ground (earth) mat or grounding (earthing) mat is used to ground static electricity generated by people and moving equipment. There are two types used in static control: Static Dissipative Mats, and Conductive Mats.
A static dissipative mat that rests on a conductive surface (commonly the case in military facilities) are typically made of 3 layers (3-ply) with static dissipative vinyl layers surrounding a conductive substrate which is electrically attached to ground (earth). For commercial uses, static dissipative rubber mats are traditionally used that are made of 2 layers (2-ply) with a tough solder resistant top static dissipative layer that makes them last longer than the vinyl mats, and a conductive rubber bottom. Conductive mats are made of carbon and used only on floors for the purpose of drawing static electricity to ground as quickly as possible. Normally conductive mats are made with cushioning for standing and are referred to as "anti-fatigue" mats.
For a static dissipative mat to be reliably grounded it must be attached to a path to ground. Normally, both the mat and the wrist strap are connected to ground by using a common point ground system (CPGS).
In computer repair shops and electronics manufacturing workers must be grounded before working on devices sensitive to voltages capable of being generated by humans. For that reason static dissipative mats can be and are also used on production assembly floors as "floor runner" along the assembly line to draw static generated by people walking up and down.
Isolation.
Isolation is a mechanism that defeats grounding. It is frequently used with low-power consumer devices, and when electronics engineers, hobbyists, or repairmen are working on circuits that would normally be operated using the power line voltage. Isolation can be accomplished by simply placing a "1:1 wire ratio" transformer with an equal number of turns between the device and the regular power service, but applies to any type of transformer using two or more coils electrically insulated from each other.
For an isolated device, touching a single powered conductor does not cause a severe shock, because there is no path back to the other conductor through the ground. However, shocks and electrocution may still occur if both poles of the transformer are contacted by bare skin. Previously it was suggested that repairmen "work with one hand behind their back" to avoid touching two parts of the device under test at the same time, thereby preventing a circuit from crossing through the chest and interrupting cardiac rhythms/ causing cardiac arrest.
Generally every AC power line transformer acts as an isolation transformer, and every step up or down has the potential to form an isolated circuit. However, this isolation would prevent failed devices from blowing fuses when shorted to their ground conductor. The isolation that could be created by each transformer is defeated by always having one leg of the transformers grounded, on both sides of the input and output transformer coils. Power lines also typically ground one specific wire at every pole, to ensure current equalization from pole to pole if a short to ground is occurring.
In the past, grounded appliances have been designed with internal isolation to a degree that allowed the simple disconnection of ground by cheater plugs without apparent problem (a dangerous practice, since the safety of the resulting floating equipment relies on the insulation in its power transformer). Modern appliances however often include power entry modules which are designed with deliberate capacitive coupling between the AC power lines and chassis, to suppress electromagnetic interference. This results in a significant leakage current from the power lines to ground. If the ground is disconnected by a cheater plug or by accident, the resulting leakage current can cause mild shocks, even without any fault in the equipment. Even small leakage currents are a significant concern in medical settings, as the accidental disconnection of ground can introduce these currents into sensitive parts of the human body. As a result, medical power supplies are designed to have low capacitance.
Class II appliances and power supplies (such as cell phone chargers) do not provide any ground connection, and are designed to isolate the output from input. Safety is ensured by double-insulation, so that two failures of insulation are required to cause a shock.

</doc>
<doc id="41216" url="http://en.wikipedia.org/wiki?curid=41216" title="Ground constants">
Ground constants

In telecommunication, ground constants are the electrical parameters of earth, such as conductivity, permittivity, and magnetic permeability. 
The values of these parameters vary with the local chemical composition and density of the Earth. For a propagating electromagnetic wave, such as a surface wave propagating along the surface of the Earth, these parameters vary with frequency and direction. 

</doc>
<doc id="41217" url="http://en.wikipedia.org/wiki?curid=41217" title="Ground loop">
Ground loop

Ground loop may refer to:

</doc>
<doc id="41218" url="http://en.wikipedia.org/wiki?curid=41218" title="Ground plane">
Ground plane

In electrical engineering, a ground plane is an electrically conductive surface, usually connected to electrical ground. The term has two different meanings in separate areas of electrical engineering. In antenna theory, a ground plane is a conducting surface large in comparison to the wavelength, such as the Earth, which is connected to the transmitter's ground wire and serves as a reflecting surface for radio waves. In printed circuit boards, a ground plane is a large area of copper foil on the board which is connected to the power supply ground terminal and serves as a return path for current from different components on the board.
Radio antenna theory.
In telecommunication, a "ground plane" is a flat or nearly flat horizontal conducting surface that serves as part of an antenna, to reflect the radio waves from the other antenna elements. The plane does not necessarily have to be connected to ground. Ground planes are particularly used with monopole antennas.
To function as a ground plane, the conducting surface must be at least a quarter of the wavelength ("λ"/4) of the radio waves in size. In lower frequency antennas, such as the mast radiators used for broadcast antennas, the Earth itself (or a body of water such as a salt marsh or ocean) is used as a ground plane. For higher frequency antennas, in the VHF or UHF range, the ground plane can be smaller, and metal disks, screens and wires are used as ground planes. At upper VHF and UHF frequencies, the metal skin of a car or aircraft can serve as a ground plane for whip antennas projecting from it. The ground plane doesn't have to be a continuous surface. In the "ground plane antenna" style whip antenna, the "plane" consists of several wires "λ"/4 long radiating from the base of a quarter-wave whip antenna.
The radio waves from an antenna element that reflect off a ground plane appear to come from a mirror image of the antenna located on the other side of the ground plane. In a monopole antenna, the radiation pattern of the monopole plus the virtual "image antenna" make it appear as a two element center-fed dipole antenna. So a monopole mounted over an ideal ground plane has a radiation pattern identical to a dipole antenna. The feedline from the transmitter or receiver is connected between the bottom end of the monopole element and the ground plane. The ground plane must have good conductivity; any resistance in the ground plane is in series with the antenna, and serves to dissipate power from the transmitter.
Printed circuit boards.
A "ground plane" on a printed circuit board (PCB) is a large area or layer of copper foil connected to the circuit's ground point, usually one terminal of the power supply. It serves as the return path for current from many different components.
A ground plane is often made as large as possible, covering most of the area of the PCB which is not occupied by circuit traces. In multilayer PCBs, it is often a separate layer covering the entire board. This serves to make circuit design easier, allowing the designer to ground any component without having to run additional traces; component leads needing grounding are routed directly through a hole in the board to the ground plane on another layer. The large area of copper also conducts the large return currents from many components without significant voltage drops, ensuring that the ground connection of all the components are at the same reference potential.
In digital and radio frequency PCBs, the major reason for using large ground planes is to reduce electrical noise and interference through ground loops and to prevent crosstalk between adjacent circuit traces. When digital circuits switch state, large current pulses flow from the active devices (transistors or integrated circuits) through the ground circuit. If the power supply and ground traces have significant impedance, the voltage drop across them may create noise voltage pulses that disturb other parts of the circuit (ground bounce). The large conducting area of the ground plane has much lower impedance than a circuit trace, so the current pulses cause less disturbance.
In addition, a ground plane under printed circuit traces can reduce crosstalk between adjacent traces. When two traces run parallel, an electrical signal in one can be coupled into the other through electromagnetic induction by magnetic field lines from one linking the other; this is called crosstalk. When a ground plane layer is present underneath, it forms a transmission line with the trace. The oppositely-directed return currents flow through the ground plane directly beneath the trace. This confines most of the electromagnetic fields to the area near the trace and consequently reduces crosstalk.
A power plane is often used in addition to a ground plane in a multilayer circuit board, to distribute DC power to the active devices. The two facing areas of copper create a large parallel plate decoupling capacitor that prevents noise from being coupled from one circuit to another through the power supply.
Ground planes are sometimes split and then connected by a thin trace. This allows the separation of analog and digital sections of a board or the inputs and outputs of amplifiers. The thin trace has low enough impedance to keep the two sides very close to the same potential while keeping the ground currents of one side from coupling into the other side, causing ground loop.
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41220" url="http://en.wikipedia.org/wiki?curid=41220" title="Group alerting and dispatching system">
Group alerting and dispatching system

In telecommunication, a group alerting and dispatching system is a service feature that (a) enables a controlling telephone to place a call to a specified number of telephones simultaneously, (b) enables the call to be recorded, (c) if any of the called lines is busy, enables the equipment to camp on until the busy line is free, and (d) rings the free line and plays the recorded message.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41222" url="http://en.wikipedia.org/wiki?curid=41222" title="Group delay and phase delay">
Group delay and phase delay

In signal processing, group delay is a measure of the time delay of the amplitude envelopes of the various sinusoidal components of a signal through a device under test, and is a function of frequency for each component. Phase delay is similar, however it is a measure of the time delay of the "phase" as opposed to the time delay of the "amplitude envelope".
All frequency components of a signal are delayed when passed through a device such as an amplifier, a loudspeaker, or propagating through space or a medium, such as air. This signal delay will be different for the various frequencies unless the device has the property of being linear phase. (Linear phase and minimum phase are often confused. They are quite different.) The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device. This changes the shape of the signal in addition to any constant delay or scale change. A sufficiently large delay variation can cause problems such as poor fidelity in audio or intersymbol interference (ISI) in the demodulation of digital information from an analog carrier signal. High speed modems use adaptive equalizers to compensate for non-constant group delay.
Introduction.
Group delay is a useful measure of time distortion, and is calculated by differentiating, with respect to frequency, the phase response versus frequency of the device under test (DUT): the group delay is a measure of the slope of the phase response at any given frequency. Variations in group delay cause signal distortion, just as deviations from linear phase cause distortion. 
In LTI system theory, control theory, and in digital or analog signal processing, the relationship between the input signal, formula_1, to output signal, formula_2, of an LTI system is governed by the convolution operation:
Or, in the frequency domain,
where
and
Here formula_8 is the time-domain impulse response of the LTI system and formula_9, formula_10, formula_11, are the Laplace transforms of the input formula_1, output formula_2, and impulse response formula_8, respectively. formula_11 is called the transfer function of the LTI system and, as does the impulse response, formula_8, "fully" defines the input-output characteristics of the LTI system.
When such a system is driven by a quasi-sinusoidal signal, (a sinusoid with a slowly changing amplitude envelope formula_17, relative to the rate of change of phase, formula_18, of the sinusoid), 
the output of such an LTI system is very well approximated as
if 
and formula_22 and formula_23, the group delay and phase delay respectively, are equal to the expressions below (and potentially are functions of angular frequency formula_18). In a linear phase system (with non-inverting gain), both formula_22 and formula_23 are constant and equal to the same overall delay of the system and the unwrapped phase shift of the system is negative with magnitude increasing linearly with frequency formula_18.
It can be shown that for an LTI system with transfer function formula_11 driven by a complex sinusoid of unit amplitude, 
the output is
where the phase shift formula_31 is
Additionally, it can be shown that the group delay, formula_22, and phase delay, formula_23, are related to the phase shift formula_31 as
Group delay in optics.
In physics, and in particular in optics, the term group delay has the following meanings:
It is often desirable for the group delay to be constant across all frequencies; otherwise there is temporal smearing of the signal. Because group delay is formula_43, as defined in (1), it therefore follows that a constant group delay can be achieved if the transfer function of the device or medium has a linear phase response (i.e., formula_44 where the group delay formula_45 is a constant).
The degree of nonlinearity of the phase indicates the deviation of the group delay from a constant.
Group delay in audio.
Group delay has some importance in the audio field and especially in the sound reproduction field. Many components of an audio reproduction chain, notably loudspeakers and multiway loudspeaker crossover networks, introduce group delay in the audio signal. It is therefore important to know the threshold of audibility of group delay with respect to frequency, especially if the audio chain is supposed to provide high fidelity reproduction. The best thresholds of audibility table has been provided by .
Flanagan, Moore and Stone conclude that at 1, 2 and 4 kHz, a group delay of about 1.6 ms is audible with headphones in a non-reverberant condition.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41223" url="http://en.wikipedia.org/wiki?curid=41223" title="Guided ray">
Guided ray

A guided ray (also bound ray or trapped ray) is a ray of light in a multi-mode optical fiber, which is confined by the core. For step index fiber, light entering the fiber will be guided if it falls within the acceptance cone of the fiber, that is if it makes an angle with the fiber axis that is less than the acceptance angle,
where
This result can be derived from Snell's law by considering the critical angle.
Rays that fall within this angular range are reflected from the core-cladding boundary by total internal reflection, and so are confined by the core. The confinement of light by the fiber can also be described in terms of bound modes or guided modes. This treatment is necessary when considering singlemode fiber, since the ray model does not accurately describe the propagation of light in this type of fiber.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41224" url="http://en.wikipedia.org/wiki?curid=41224" title="Hagelbarger code">
Hagelbarger code

In telecommunication, a Hagelbarger code is a convolutional code that enables error bursts to be corrected provided that there are relatively long error-free intervals between the error bursts. 
In the Hagelbarger code, inserted parity check bits are spread out in time so that an error burst is not likely to affect more than one of the groups in which parity is checked.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41225" url="http://en.wikipedia.org/wiki?curid=41225" title="Halftone characteristic">
Halftone characteristic

In a facsimile system the halftone characteristic is either:
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41226" url="http://en.wikipedia.org/wiki?curid=41226" title="Hamming code">
Hamming code

In telecommunication, Hamming codes are a family of linear error-correcting codes that generalize the Hamming(7,4)-code invented by Richard Hamming in 1950. Hamming codes can detect up to two-bit errors or correct one-bit errors without detection of uncorrected errors. By contrast, the simple parity code cannot correct errors, and can detect only an odd number of bits in error. Hamming codes are perfect codes, that is, they achieve the highest possible rate for codes with their block length and minimum distance 3.
In mathematical terms, Hamming codes are a class of binary linear codes. For each integer "r" ≥ 2 there is a code with block length "n" = 2"r"−1 and message length "k" = 2"r"−"r"−1. Hence the rate of Hamming codes is "R" = "k"/"n" = 1 − "r"/(2"r"−1), which is the highest possible for codes with minimum distance 3 (i.e., the minimal number of bit changes needed to go from any code word to any other code word is 3) and block length 2"r"−1. The parity-check matrix of a Hamming code is constructed by listing all columns of length "r" that are non-zero, which means that the dual code of the Hamming code is the punctured Hadamard code. The parity-check matrix has the property that any two columns are pairwise linearly independent.
Due to the limited redundancy that Hamming codes add to the data, they can only detect and correct errors when the error rate is low. This is the case in computer memory (ECC memory), where bit errors are extremely rare and Hamming codes are widely used. In this context, an extended Hamming code having one extra parity bit is often used. Extended Hamming codes achieve a Hamming distance of 4, which allows the decoder to distinguish between when at most one 1-bit error occurs and when any 2-bit errors occur. In this sense, extended Hamming codes are single-error correcting and double-error detecting, abbreviated as SECDED.
History.
Hamming worked at Bell Labs in the 1940s on the Bell Model V computer, an electromechanical relay-based machine with cycle times in seconds. Input was fed in on punched cards, which would invariably have read errors. During weekdays, special code would find errors and flash lights so the operators could correct the problem. During after-hours periods and on weekends, when there were no operators, the machine simply moved on to the next job.
Hamming worked on weekends, and grew increasingly frustrated with having to restart his programs from scratch due to the unreliability of the card reader. Over the next few years, he worked on the problem of error-correction, developing an increasingly powerful array of algorithms. In 1950, he published what is now known as Hamming Code, which remains in use today in applications such as ECC memory.
Codes predating Hamming.
A number of simple error-detecting codes were used before Hamming codes, but none were as effective as Hamming codes in the same overhead of space.
Parity.
Parity adds a single bit that indicates whether the number of 1 bits in the preceding data was even or odd. If an odd number of bits is changed in transmission, the message will change parity and the error can be detected at this point. (Note that the bit that changed may have been the parity bit itself!) The most common convention is that a parity value of 1 indicates that there is an odd number of ones in the data, and a parity value of 0 indicates that there is an even number of ones. If the number of bits changed is even, the check bit will be valid and the error will not be detected. Moreover, parity does not indicate which bit contained the error, even when it can detect it. The data must be discarded entirely and re-transmitted from scratch. On a noisy transmission medium, a successful transmission could take a long time or may never occur. However, while the quality of parity checking is poor, since it uses only a single bit, this method results in the least overhead.
Two-out-of-five code.
A two-out-of-five code is an encoding scheme which uses five bits consisting of exactly three 0s and two 1s. This provides ten possible combinations, enough to represent the digits 0–9. This scheme can detect all single bit-errors, all odd numbered bit-errors and some even numbered bit-errors (for example the flipping of both 1-bits). However it still cannot correct for any of these errors.
Repetition.
Another code in use at the time repeated every data bit multiple times in order to ensure that it was sent correctly. For instance, if the data bit to be sent is a 1, an "n" = 3 "repetition code" will send 111. If the three bits received are not identical, an error occurred during transmission. If the channel is clean enough, most of the time only one bit will change in each triple. Therefore, 001, 010, and 100 each correspond to a 0 bit, while 110, 101, and 011 correspond to a 1 bit, as though the bits count as "votes" towards what the intended bit is. A code with this ability to reconstruct the original message in the presence of errors is known as an "error-correcting" code. This triple repetition code is a Hamming code with "m" = 2, since there are two parity bits, and 22 − 2 − 1 = 1 data bit.
Such codes cannot correctly repair all errors, however. In our example, if the channel flips two bits and the receiver gets 001, the system will detect the error, but conclude that the original bit is 0, which is incorrect. If we increase the number of times we duplicate each bit to four, we can detect all two-bit errors but cannot correct them (the votes "tie"); at five repetitions, we can correct all two-bit errors, but not all three-bit errors.
Moreover, the repetition code is extremely inefficient, reducing throughput by three times in our original case, and the efficiency drops drastically as we increase the number of times each bit is duplicated in order to detect and correct more errors.
Hamming codes.
If more error-correcting bits are included with a message, and if those bits can be arranged such that different incorrect bits produce different error results, then bad bits could be identified. In a 7-bit message, there are seven possible single bit errors, so three error control bits could potentially specify not only that an error occurred but also which bit caused the error.
Hamming studied the existing coding schemes, including two-of-five, and generalized their concepts. To start with, he developed a nomenclature to describe the system, including the number of data bits and error-correction bits in a block. For instance, parity includes a single bit for any data word, so assuming ASCII words with 7-bits, Hamming described this as an "(8,7)" code, with eight bits in total, of which 7 are data. The repetition example would be "(3,1)", following the same logic. The code rate is the second number divided by the first, for our repetition example, 1/3.
Hamming also noticed the problems with flipping two or more bits, and described this as the "distance" (it is now called the "Hamming distance", after him). Parity has a distance of 2, so one bit flip can be detected, but not corrected and any two bit flips will be invisible. The (3,1) repetition has a distance of 3, as three bits need to be flipped in the same triple to obtain another code word with no visible errors. It can correct one-bit errors or detect but not correct two-bit errors. A (4,1) repetition (each bit is repeated four times) has a distance of 4, so flipping three bits can be detected, but not corrected. When three bits flip in the same group there can be situations where attempting to correct will produce the wrong code word. In general, a code with distance "k" can detect but not correct "k" − 1 errors.
Hamming was interested in two problems at once; increasing the distance as much as possible, while at the same time increasing the code rate as much as possible. During the 1940s he developed several encoding schemes that were dramatic improvements on existing codes. The key to all of his systems was to have the parity bits overlap, such that they managed to check each other as well as the data.
General algorithm.
The following general algorithm generates a single-error correcting (SEC) code for any number of bits.
The form of the parity is irrelevant. Even parity is simpler from the perspective of theoretical mathematics, but there is no difference in practice.
This general rule can be shown visually:
Shown are only 20 encoded bits (5 parity, 15 data) but the pattern continues indefinitely. The key thing about Hamming Codes that can be seen from visual inspection is that any given bit is included in a unique set of parity bits. To check for errors, check all of the parity bits. The pattern of errors, called the error syndrome, identifies the bit in error. If all parity bits are correct, there is no error. Otherwise, the sum of the positions of the erroneous parity bits identifies the erroneous bit. For example, if the parity bits in positions 1, 2 and 8 indicate an error, then bit 1+2+8=11 is in error. If only one parity bit indicates an error, the parity bit itself is in error.
As you can see, if you have formula_1 parity bits, it can cover bits from 1 up to formula_2. If we subtract out the parity bits, we are left with formula_3 bits we can use for the data. As formula_1 varies, we get all the possible Hamming codes:
If, in addition, an overall parity bit (bit 0) is included, the code can detect (but not correct) any two-bit error, making a SECDED code. The overall parity indicates whether the total number of errors is even or odd. If the basic Hamming code detects an error, but the overall parity says that there are an even number of errors, an uncorrectable 2-bit error has occurred.
Hamming codes with additional parity (SECDED).
Hamming codes have a minimum distance of 3, which means that the decoder can detect and correct a single error, but it cannot distinguish a double bit error of some codeword from a single bit error of a different codeword. Thus, they can detect double-bit errors only if correction is not attempted.
To remedy this shortcoming, Hamming codes can be extended by an extra parity bit. This way, it is possible to increase the minimum distance of the Hamming code to 4, which allows the decoder to distinguish between single bit errors and two-bit errors. Thus the decoder can detect and correct a single error and at the same time detect (but not correct) a double error. If the decoder does not attempt to correct errors, it can detect up to 3 errors.
This extended Hamming code is popular in computer memory systems, where it is known as "SECDED" (abbreviated from "single error correction, double error detection"). Particularly popular is the (72,64) code, a truncated (127,120) Hamming code plus an additional parity bit, which has the same space overhead as a (9,8) parity code.
[7,4] Hamming code.
In 1950, Hamming introduced the [7,4] Hamming code. It encodes 4 data bits into 7 bits by adding three parity bits. It can detect and correct single-bit errors. With the addition of an overall parity bit, it can also detect (but not correct) double-bit errors.
Construction of G and H.
The matrix 
formula_5 is called a (Canonical) generator matrix of a linear (n,k) code,
and formula_6 is called a parity-check matrix.
This is the construction of G and H in standard (or systematic) form. Regardless of form, G and H for linear block codes must satisfy
formula_7, an all-zeros matrix.
Since [7,4,3]=[n,k,d]=[2m − 1, 2m−1-m, m]. The parity-check matrix H of a Hamming code is constructed by listing all columns of length m that are pair-wise independent.
Thus H is a matrix whose left side is all of the nonzero n-tuples where order of the n-tuples in the columns of matrix does not matter. The right hand side is just the (n-k)-identity matrix. 
So G can be obtained from H by taking the transpose of the left hand side of H with the identity k-identity matrix on the left hand side of G.
The code generator matrix formula_8 and the parity-check matrix formula_9 are:
formula_10
and
formula_11
Finally, these matrices can be mutated into equivalent non-systematic codes by the following operations:
Encoding.
Example
From the above matrix we have 2k=24=16 codewords.
The codewords formula_12 of this binary code can be obtained from formula_13. With formula_14 with formula_15 exist in formula_16 ( A field with two elements namely 0 and 1).
Thus the codewords are all the 4-tuples (k-tuples).
Therefore,
(1,0,1,1) gets encoded as (1,0,1,1,0,1,0).
[7,4] Hamming code with an additional parity bit.
The [7,4] Hamming code can easily be extended to an [8,4] code by adding an extra parity bit on top of the (7,4) encoded word ("see Hamming(7,4)").
This can be summed up with the revised matrices:
and
Note that H is not in standard form. To obtain G, elementary row operations can be used to obtain an equivalent matrix to H in systematic form:
For example, the first row in this matrix is the sum of the second and third rows of H in non-systematic form. Using the systematic construction for Hamming codes from above, the matrix A is apparent and the systematic form of G is written as
The non-systematic form of G can be row reduced (using elementary row operations) to match this matrix.
The addition of the fourth row effectively computes the sum of all the codeword bits (data and parity) as the fourth parity bit.
For example, 1011 is encoded into 01100110 where blue digits are data; red digits are parity from the [7,4] Hamming code; and the green digit is the parity added by [8,4] code.
The green digit makes the parity of the [7,4] code even.
Finally, it can be shown that the minimum distance has increased from 3, as with the [7,4] code, to 4 with the [8,4] code. Therefore, the code can be defined as [8,4] Hamming code. 

</doc>
<doc id="41227" url="http://en.wikipedia.org/wiki?curid=41227" title="Hamming distance">
Hamming distance

In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of "substitutions" required to change one string into the other, or the minimum number of "errors" that could have transformed one string into the other.
A major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field.
Examples.
The Hamming distance between:
On a two-dimensional grid such as a chessboard, the Hamming distance is the minimum number of moves it would take a rook to move from one cell to the other.
Special properties.
For a fixed length "n", the Hamming distance is a metric on the vector space of the words of length n (also known as a Hamming space), as it fulfills the conditions of non-negativity, identity of indiscernibles and symmetry, and it can be shown by complete induction that it satisfies the triangle inequality as well. The Hamming distance between two words "a" and "b" can also be seen as the Hamming weight of "a"−"b" for an appropriate choice of the − operator.
For binary strings "a" and "b" the Hamming distance is equal to the number of ones (population count) in "a" XOR "b". The metric space of length-"n" binary strings, with the Hamming distance, is known as the "Hamming cube"; it is equivalent as a metric space to the set of distances between vertices in a hypercube graph. One can also view a binary string of length "n" as a vector in formula_1 by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an "n"-dimensional hypercube, and the Hamming distance of the strings is equivalent to the Manhattan distance between the vertices.
Error detection and error correction.
The Hamming distance is used to define some essential notions in coding theory, such as error detecting and error correcting codes. In particular, a code "C" is said to be "k-errors detecting" if any two codewords "c"1 and "c"2 from "C" that have a Hamming distance less than "k" coincide; Otherwise put it, a code is "k"-errors detecting if and only if the minimum Hamming distance between any two of its codewords is at least "k"+1.
A code "C" is said to be "k-errors correcting" if for every word "w" in the underlying Hamming space "H" there exists at most one codeword "c" (from "C") such that the Hamming distance between "w" and "c" is less than "k". In other words, a code is "k"-errors correcting if and only if the minimum Hamming distance between any two of its codewords is at least 2"k"+1. This is more easily understood geometrically as any closed balls of radius "k" centered on distinct codewords being disjoint. These balls are also called Hamming spheres in this context.
Thus a code with minimum Hamming distance "d" between its codewords can detect at most "d"-1 errors and can correct ⌊(d-1)/2⌋ errors. The latter number is also called the "packing radius" or the "error-correcting capability" of the code.
History and applications.
The Hamming distance is named after Richard Hamming, who introduced it in his fundamental paper on Hamming codes "Error detecting and error correcting codes" in 1950. Hamming weight analysis of bits is used in several disciplines including information theory, coding theory, and cryptography.
It is used in telecommunication to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the signal distance. For "q"-ary strings over an alphabet of size "q" ≥ 2 the Hamming distance is applied in case of the q-ary symmetric channel, while the Lee distance is used for phase-shift keying or more generally channels susceptible to synchronization errors because the Lee distance accounts for errors of ±1. If "q" = 2 or "q" = 3 both distances coincide because Z/2Z and Z/3Z are also fields, but Z/4Z is not a field but only a ring.
The Hamming distance is also used in systematics as a measure of genetic distance.
However, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric like the Levenshtein distance is more appropriate.
Algorithm example.
The Python function codice_1 computes the Hamming distance between
two strings (or other iterable objects) of equal length, by creating a sequence of Boolean values indicating mismatches and matches between corresponding positions in the two inputs, and then summing the sequence with False and True values being interpreted as zero and one.
def hamming_distance(s1, s2):
 """Return the Hamming distance between equal-length sequences"""
 if len(s1) != len(s2):
 raise ValueError("Undefined for sequences of unequal length")
 return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))
The following C function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the bitwise exclusive or of the two inputs, and then finds the Hamming weight of the result (the number of nonzero bits) using an algorithm of that repeatedly finds and clears the lowest-order nonzero bit.
int hamming_distance(unsigned x, unsigned y)
 int dist;
 unsigned val;
 dist = 0;
 val = x ^ y; // XOR
 // Count the number of bits set
 while (val != 0)
 // A bit is set, so increment the count and clear the bit
 dist++;
 val &= val - 1;
 // Return the number of differing bits
 return dist;

</doc>
<doc id="41229" url="http://en.wikipedia.org/wiki?curid=41229" title="Handshaking">
Handshaking

In information technology, telecommunications, and related fields, handshaking is an automated process of negotiation that dynamically sets parameters of a communications channel established between two entities before normal communication over the channel begins. It follows the physical establishment of the channel and precedes normal information transfer.
It is usually a process that takes place when a computer is about to communicate with a foreign device to establish rules for communication. When a computer communicates with another device like a modem, printer, or network server, it needs to handshake with it to establish a connection.
Handshaking can be used to negotiate parameters that are acceptable to equipment and systems at both ends of the communication channel, including, but not limited to, information transfer rate, coding alphabet, parity, interrupt procedure, and other protocol or hardware features.
Handshaking is a technique of communication between two entities. However, within TCP/IP RFCs, the term "handshake" is most commonly used to reference the TCP three-way handshake. For example, the term "handshake" is not present in RFCs covering FTP or SMTP. One exception is Transport Layer Security, TLS, setup, FTP RFC 4217. In place of the term "handshake," FTP RFC 3659 substitutes the term "conversation" for the passing of commands. 
A simple handshaking protocol might only involve the receiver sending a message meaning "I received your last message and I am ready for you to send me another one." A more complex handshaking protocol might allow the sender to ask the receiver if he is ready to receive or for the receiver to reply with a negative acknowledgement meaning "I did not receive your last message correctly, please resend it" (e.g. if the data was corrupted en route). 
Handshaking makes it possible to connect relatively heterogeneous systems or equipment over a communication channel without the need for human intervention to set parameters. One classic example of handshaking is that of modems, which typically negotiate communication parameters for a brief period when a connection is first established, and thereafter use those parameters to provide optimal information transfer over the channel as a function of its quality and capacity. The "squealing" (which is actually a sound that changes in pitch 100 times every second) noises made by some modems with speaker output immediately after a connection is established are in fact the sounds of modems at both ends engaging in a handshaking procedure; once the procedure is completed, the speaker might be silenced, depending on the settings of operating system or the application controlling the modem.
Examples.
The TLS Handshake Protocol is used to negotiate the secure attributes of a session. (RFC 5246, p.37)
Common types of handshake.
Establishing a normal TCP connection requires three separate steps:
One of the most important factors of three-way handshake is that, in order to exchange the starting sequence number the two sides plan to use, the client first sends a segment with its own initial sequence number formula_1, then the server responds by sending a segment with its own sequence number formula_2 and the acknowledgement number formula_3, and finally the client responds by sending a segment with acknowledgement number formula_4.
The reason for the client and server not using the default sequence number such as 0 for establishing connection is to protect against two incarnations of the same connection reusing the same sequence number too soon, which means a segment from an earlier incarnation of a connection might interfere with a later incarnation of the connection.

</doc>
<doc id="41230" url="http://en.wikipedia.org/wiki?curid=41230" title="Hard copy">
Hard copy

In information handling, a hard copy is a permanent reproduction, or copy, in the form of a physical object, of any media suitable for direct use by a person (in particular paper), of displayed or transmitted data. Examples of hard copy include teleprinter pages, continuous printed tapes, computer printouts, and radio photo prints.
Hard Copy Records, such as printed forms, tab cards, and OCR forms, are best designed using a record layout.[]
Magnetic tapes, diskettes, and non-printed punched paper tapes are not hard copies.
The term "hard copy" predates the age of the digital computer. In the process of producing printed books and newspapers, hard copy refers to a manuscript or typewritten document that has been edited and proofread, and is ready for typesetting, or being read on-air in a radio or television broadcast. This traditional meaning has been all but forgotten in the wake of the information revolution.
"Dead-tree edition".
"Dead-tree edition" refers to a printed paper version of a written work, as opposed to digital alternatives such as a web page. It is a dysphemism for "hard copy". Variations include "dead-tree format" and "dead-tree-ware". "Dead-tree" refers to trees being cut down for raw material for producing paper. Newspapers are, sometimes pejoratively, referred to as "the dead-tree-press". "The Guardian" website on 29 November 2006 wrote:Maybe this is more a multimedia victory for Jeff Randall himself: he did manage a "dead-tree" front page, web scoop, vodcast and major plug on the 10 O'clock news.
A related saying among computer fans is "You can't grep dead trees", from the Unix command grep meaning to search the contents of text files. This means that an advantage of keeping documents in digital form rather than on paper is that they can be more easily searched for specific contents. An exception are texts stored as digital images (digital facsimile), as they cannot be easily searched, except by sophisticated means such as optical character recognition or examining the image metadata. On the other hand, paper copies have tremendous data integrity in proper conditions.
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41231" url="http://en.wikipedia.org/wiki?curid=41231" title="Hard sectoring">
Hard sectoring

Hard sectoring in a magnetic or optical data storage device is a form of sectoring which uses a physical mark or hole in the recording medium to reference sector locations.
In older 8- and 51⁄4-inch floppy disks, hard sectoring was implemented by punching sector holes in the disk to mark the start
of each sector. These were equally spaced holes, at a common radius. This was in addition to the index hole, situated between two sector holes, to mark the start of the entire track of sectors. When the index or sector hole was recognized by an optical sensor, a sector signal was generated. Timing electronics or software would use the faster timing of the index hole between sector holes, to generate an index signal. Data read and write is faster in this technique than soft sectoring as no operations are to be performed regarding the starting and ending points of tracks.

</doc>
<doc id="41232" url="http://en.wikipedia.org/wiki?curid=41232" title="Harmonic">
Harmonic

A harmonic of a wave is a component frequency of the signal that is an integer multiple of the fundamental frequency, i.e. if the fundamental frequency is "f", the harmonics have frequencies 2"f", 3"f", 4"f", . . . etc. The harmonics have the property that they are all periodic at the fundamental frequency, therefore the sum of harmonics is also periodic at that frequency. As multiples of the fundamental frequency, successive harmonics can be found by repeatedly adding the fundamental frequency. For example, if the fundamental frequency (first harmonic) is 25 Hz, the frequencies of the next harmonics are: 50 Hz (2nd harmonic), 75 Hz (3rd harmonic), 100 Hz (4th harmonic) etc.
Characteristics.
Many oscillators, including the human voice, a bowed violin string, or a Cepheid variable star, are more or less periodic, and thus composed of harmonics, also known as "harmonic partials".
Most passive oscillators, such as a plucked guitar string, a struck drum head, or struck bell, naturally oscillate at not one, but several frequencies known as partials. When the oscillator is long and thin, such as a guitar string, or the column of air in a trumpet, many of the partials are integer multiples of the fundamental frequency; these are called harmonics. Sounds made by long, thin oscillators are for the most part arranged harmonically, and these sounds are generally considered to be musically pleasing. Partials whose frequencies are not integer multiples of the fundamental are referred to as "inharmonic partials". Instruments such as cymbals, pianos, and strings plucked pizzicato create noticeable inharmonic sounds in addition to harmonic sounds.
The untrained human ear typically does not perceive harmonics as separate notes. Rather, a musical note composed of many harmonically related frequencies is perceived as one sound, the quality, or timbre of that sound being a result of the relative strengths of the individual harmonic frequencies. Bells have more clearly perceptible inharmonics than most instruments. Antique singing bowls are well known for their unique quality of producing multiple harmonic partials or multiphonics.
Harmonics and overtones.
An overtone is any frequency higher than the fundamental. The tight relation between overtones and harmonics in music often leads to their being used synonymously in a strictly musical context, but they are counted differently leading to some possible confusion. This chart demonstrates how they are counted:
In many musical instruments, it is possible to play the upper harmonics without the fundamental note being present. In a simple case (e.g., recorder) this has the effect of making the note go up in pitch by an octave, but in more complex cases many other pitch variations are obtained. In some cases it also changes the timbre of the note. This is part of the normal method of obtaining higher notes in wind instruments, where it is called "overblowing". The extended technique of playing multiphonics also produces harmonics. On string instruments it is possible to produce very pure sounding notes, called harmonics or "flageolets" by string players, which have an eerie quality, as well as being high in pitch. Harmonics may be used to check at a unison the tuning of strings that are not tuned to the unison. For example, lightly fingering the node found halfway down the highest string of a cello produces the same pitch as lightly fingering the node 1/3 of the way down the second highest string. For the human voice see Overtone singing, which uses harmonics. 
While it is true that electronically produced periodic tones (e.g. square waves or other non-sinusoidal waves) have "harmonics" that are whole number multiples of the fundamental frequency, practical instruments do not all have this characteristic. For example higher "harmonics"' of piano notes are not true harmonics but are "overtones" and can be very sharp, i.e. a higher frequency than given by a pure harmonic series. This is especially true of instruments other than stringed or brass/woodwind ones, e.g., xylophone, drums, bells etc., where not all the overtones have a simple whole number ratio with the fundamental frequency.
The fundamental frequency is the reciprocal of the period of the periodic phenomenon. 
 This article incorporates public domain material from websites or documents of the .
Harmonics on stringed instruments.
The following table displays the stop points on a stringed instrument, such as the guitar (guitar harmonics), at which gentle touching of a string will force it into a harmonic mode when vibrated. String harmonics (flageolet tones) are described as having a "flutelike, silvery quality that can be highly effective as a special color" when used and heard in orchestration. It is unusual to encounter natural harmonics higher than the fifth partial on any stringed instrument except the double bass, on account of its much longer strings.
Artificial harmonics.
Although harmonics are most often used on open strings, occasionally a score will call for an artificial harmonic, produced by playing an overtone on a stopped string. As a performance technique, it is accomplished by using two fingers on the fingerboard, the first to shorten the string to the desired fundamental, with the second touching the node corresponding to the appropriate harmonic.
Other information.
Harmonics may be either used or considered as the basis of just intonation systems. Composer Arnold Dreyblatt is able to bring out different harmonics on the single string of his modified double bass by slightly altering his unique bowing technique halfway between hitting and bowing the strings. Composer Lawrence Ball uses harmonics to generate music electronically.

</doc>
<doc id="41233" url="http://en.wikipedia.org/wiki?curid=41233" title="H channel">
H channel

In the Integrated Services Digital Network (ISDN), a high-speed channel comprising multiple aggregated low-speed channels to accommodate bandwidth-intensive applications such as file transfer, videoconferencing, and high-quality audio. An H channel is formed of multiple bearer B channels bonded together in a primary rate access (PRA) or primary rate interface (PRI) frame in support of applications with bandwidth requirements that exceed the B channel rate of 64 kbit/s. The channels, once bonded, remain so end-to-end, from transmitter to receiver, through the ISDN network. The feature is known variously as multirate ISDN, Nx64, channel aggregation, and bonding.
H channels are implemented as:

</doc>
