<doc id="26751" url="http://en.wikipedia.org/wiki?curid=26751" title="Sun">
Sun

The Sun is the star at the center of the Solar System and is by far the most important source of energy for life on Earth. It is a nearly perfect spherical ball of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process.<ref name="doi10.1146/annurev-astro-081913-040012">Error: Bad DOI specified: 10.1146/annurev-astro-081913-040012</ref> Its diameter is about 109 times that of Earth, and it has a mass about 330,000 times that of Earth, accounting for about 99.86% of the total mass of the Solar System.
Chemically, about three quarters of the Sun's mass consists of hydrogen, whereas the rest is mostly helium, and much smaller quantities of heavier elements, including oxygen, carbon, neon and iron.
The Sun is a G-type main-sequence star (G2V) based on spectral class and it is informally referred to as a yellow dwarf. It formed approximately 4.567 billion years ago from the gravitational collapse of matter within a region of a large molecular cloud. Most of this matter gathered in the center, whereas the rest flattened into an orbiting disk that became the Solar System. The central mass became increasingly hot and dense, eventually initiating thermonuclear fusion in its core. It is thought that almost all stars form by this process. The Sun is roughly middle age and has not changed dramatically for four billion years, and will remain fairly stable for four billion more. However, after hydrogen fusion in its core has stopped, the Sun will undergo severe changes and become a red giant. It is calculated that the Sun will become sufficiently large to engulf the current orbits of Mercury, Venus, and possibly Earth.
The enormous effect of the Sun on the Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity. Earth's movement around the Sun is the basis of the solar calendar, which is the predominant calendar in use today.
Name and etymology.
The English proper noun "Sun" developed from Old English "sunne" and may be related to "south". Cognates to English "sun" appear in other Germanic languages, including Old Frisian "sunne", "sonne", Old Saxon "sunna", Middle Dutch "sonne", modern Dutch "zon", Old High German "sunna", modern German "Sonne", Old Norse "sunna", and Gothic "sunnō". All Germanic terms for the Sun stem from Proto-Germanic *"sunnōn".
The Sun is viewed as a goddess in Germanic paganism, Sól/Sunna. Scholars theorize that the Sun, as a Germanic goddess, may represent an extension of an earlier Proto-Indo-European Sun deity due to Indo-European linguistic connections between Old Norse "Sól", Sanskrit "Surya", Gaulish "Sulis", Lithuanian "Saulė", and Slavic "Solntse".
The English weekday name "Sunday" stems from Old English ("Sunnandæg"; "Sun's day", from before 700) and is ultimately a result of a Germanic interpretation of Latin "dies solis", itself a translation of the Greek ἡμέρα ἡλίου ("hēméra hēlíou"). The Latin name for the Sun, "Sol", is widely known but is not common in general English language use; the adjectival form is the related word "solar". The term "sol" is also used by planetary astronomers to refer to the duration of a solar day on another planet, such as Mars. A mean Earth solar day is approximately 24 hours, whereas a mean Martian 'sol' is 24 hours, 39 minutes, and 35.244 seconds.
Characteristics.
The Sun is a G-type main-sequence star that comprises about 99.86% of the mass of the Solar System. Once regarded by astronomers as a small and relatively insignificant star, the Sun has an absolute magnitude of +4.83. This is now estimated to be brighter than about 85% of the stars in the Milky Way, most of which are red dwarfs.
The Sun is a Population I, or heavy-element-rich, star. The formation of the Sun may have been triggered by shockwaves from one or more nearby supernovae. This is suggested by a high abundance of heavy elements in the Solar System, such as gold and uranium, relative to the abundances of these elements in so-called Population II, heavy-element-poor, stars. These elements could most plausibly have been produced by endothermic nuclear reactions during a supernova, or by transmutation through neutron absorption within a massive second-generation star.
The Sun is by far the brightest object in the sky, with an apparent magnitude of −26.74. This is about 13 billion times brighter than the next brightest star, Sirius, which has an apparent magnitude of −1.46. The mean distance of the Sun to Earth is approximately 1 AU, though the distance varies as Earth moves from perihelion in January to aphelion in July. At this average distance, light travels from the Sun to Earth in about 8 minutes and 19 seconds. The energy of this sunlight supports almost all life on Earth by photosynthesis, and drives Earth's climate and weather.
The Sun's radius can be measured from its center to the edge of the photosphere, the apparent visible surface of the Sun. The Sun is a near-perfect sphere with an oblateness estimated at about 9 millionths, which means that its polar diameter differs from its equatorial diameter by only 10 km.
The tidal effect of the planets is weak and does not significantly affect the shape of the Sun. The Sun rotates faster at its equator than at its poles. This differential rotation is caused by convective motion due to heat transport and the Coriolis force due to the Sun's rotation. In a frame of reference defined by the stars, the rotational period is approximately 25.6 days at the equator and 33.5 days at the poles. Viewed from Earth as it orbits the Sun, the "apparent rotational period" of the Sun at its equator is about 28 days.
The Sun does not have a definite boundary, and in its outer parts its density decreases exponentially with increasing distance from its center. The solar interior is not directly observable, and the Sun itself is opaque to electromagnetic radiation. However, just as seismology uses waves generated by earthquakes to reveal the interior structure of Earth, the discipline of helioseismology makes use of pressure waves (infrasound) traversing the Sun's interior to measure and visualize its inner structure. Computer modeling of the Sun is also used as a theoretical tool to investigate its deeper layers.
During a total solar eclipse, when the disk of the Sun is covered by that of the Moon, the Sun's surrounding atmosphere, the corona, can be seen. As the corona expands outward into space, creating the solar wind, a stream of charged particles. The spatial extent of the influence of the solar wind defines the heliosphere, a "bubble" in the interstellar medium that is roughly 100 astronomical units in radius, the largest continuous structure in the Solar System.
The outer boundary of the heliosphere is the heliopause.
Sunlight.
The Sun's color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when high in the sky; when low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta. Despite its typical whiteness, most people mentally picture the Sun as yellow; the reasons for this are the subject of debate.
The Sun is a G2V star, with "G2" indicating its surface temperature of approximately 5,778 K (5,505 °C, 9,941 °F), and "V" that it, like most stars, is a main-sequence star. The luminance of the Sun is about 1.88 gigacandela per square metre, but as viewed through Earth's atmosphere, this is lowered to about 1.44 Gcd/m2.
Sunlight is Earth's primary source of energy. The other significant source of Earth's energy is the store of fissionable materials generated by the cataclysmic death of other stars. These fissionable materials trapped in Earth's crust give rise to geothermal energy, which drives the volcanism on Earth and also makes it possible for humans to fuel nuclear reactors. The solar constant is the amount of power that the Sun deposits per unit area that is directly exposed to sunlight. The solar constant is equal to approximately (watts per square meter) at a distance of one astronomical unit (AU) from the Sun (that is, on or near Earth). Sunlight on the surface of Earth is attenuated by Earth's atmosphere so that less power arrives at the surface—closer to in clear conditions when the Sun is near the zenith. Sunlight at the top of Earth's atmosphere is composed (by total energy) of about 50% infrared light, 40% visible light, and 10% ultraviolet light. The atmosphere in particular filters out over 70% of solar ultraviolet, especially at the shorter wavelengths. Solar ultraviolet radiation ionizes the Earth's dayside upper atmosphere, creating the electrically conducting ionosphere.
Solar energy can be harnessed by a variety of natural and synthetic processes—photosynthesis by plants captures the energy of sunlight and converts it to chemical form (oxygen and reduced carbon compounds), whereas direct heating or electrical conversion by solar cells are used by solar power equipment to generate electricity or to do other useful work, sometimes employing concentrating solar power (that it is measured in suns). The energy stored in petroleum and other fossil fuels was originally converted from sunlight by photosynthesis in the distant past.
Composition.
The Sun is composed primarily of the chemical elements hydrogen and helium; they account for 74.9% and 23.8% of the mass of the Sun in the photosphere, respectively. All heavier elements, called "metals" in astronomy, account for less than 2% of the mass. The most abundant metals are oxygen (roughly 1% of the Sun's mass), carbon (0.3%), neon (0.2%), and iron (0.2%).
The Sun inherited its chemical composition from the interstellar medium out of which it formed. The hydrogen and helium in the Sun were produced by Big Bang nucleosynthesis, and the metals were produced by stellar nucleosynthesis in generations of stars that completed their stellar evolution and returned their material to the interstellar medium before the formation of the Sun. The chemical composition of the photosphere is normally considered representative of the composition of the primordial Solar System. However, since the Sun formed, some of the helium and heavy elements have gravitationally settled from the photosphere. Therefore, in today's photosphere the helium fraction is reduced and the metallicity is only 84% of that in the protostellar phase (before nuclear fusion in the core started). The protostellar Sun's composition was reconstructed as 71.1% hydrogen, 27.4% helium, and 1.5% metals.
In the inner portions of the Sun, nuclear fusion has modified the composition by converting hydrogen into helium, so the innermost portion of the Sun is now roughly 60% helium, with the metal abundance unchanged. Because the interior of the Sun is radiative, not convective (see Radiative zone below), none of the fusion products from the core have risen to the photosphere.
The reactive core zone of "hydrogen burning", where hydrogen is converted into helium, is starting to surround the core of "helium ash". This development will continue and will eventually cause the Sun to leave the main sequence, to become a red giant.
The solar heavy-element abundances described above are typically measured both using spectroscopy of the Sun's photosphere and by measuring abundances in meteorites that have never been heated to melting temperatures. These meteorites are thought to retain the composition of the protostellar Sun and are thus not affected by settling of heavy elements. The two methods generally agree well.
Singly ionized iron-group elements.
In the 1970s, much research focused on the abundances of iron-group elements in the Sun. Although significant research was done, up until 1978 it was difficult to determine the abundance of some iron-group elements (e.g. cobalt and manganese) via spectrography because of their hyperfine structures.
The first largely complete set of oscillator strengths of singly ionized iron-group elements were made available in the 1960s, and these were subsequently improved. In 1978 the abundances of 'singly Ionized' elements of the iron group were derived.
Solar and planetary mass fractionation relationship.
Fractionation is a separation process in which the composition of a mixture varies according to a gradient. Various authors have considered the existence of a mass fractionation relationship between the isotopic compositions of solar and planetary noble gases. For example, correlations between isotopic compositions of neon and xenon in the Sun and on the planets.
Prior to 1983, the belief that the whole Sun has the same composition as the solar atmosphere was widespread. In 1983, it was claimed that it was the fractionation in the Sun itself that caused the fractionation relationship between the isotopic compositions of planetary and solar-wind-implanted noble gases.
Structure.
Core.
The core of the Sun extends from the center to about 20–25% of the solar radius. It has a density of up to (about 150 times the density of water) and a temperature of close to 15.7 million kelvin (K). By contrast, the Sun's surface temperature is approximately 5,800 K. Recent analysis of SOHO mission data favors a faster rotation rate in the core than in the rest of the radiative zone. Through most of the Sun's life, energy is produced by nuclear fusion in the core region through a series of steps called the p–p (proton–proton) chain; this process converts hydrogen into helium. Only 0.8% of the energy generated in the Sun comes from the CNO cycle, though this proportion is expected to increase as the Sun gets older.
The core is the only region in the Sun that produces an appreciable amount of thermal energy through fusion; 99% of the power is generated within 24% of the Sun's radius, and by 30% of the radius, fusion has stopped nearly entirely. The rest of the Sun is heated by this energy that is transferred outwards through many successive layers to the solar photosphere before it escapes into space as sunlight or the kinetic energy of particles.
The proton–proton chain occurs around times each second in the core, converting about 3.7×1038 protons into alpha particles (helium nuclei) every second (out of a total of ~8.9×1056 free protons in the Sun), or about 6.2×1011 kg/s. Fusing four free protons (hydrogen nuclei) into a single alpha particle (helium nuclei) releases around 0.7% of the fused mass as energy, so the Sun releases energy at the mass–energy conversion rate of 4.26 million metric tons per second, for 384.6 yotta watts (), or 9.192×1010 megatons of TNT per second. Theoretical models of the Sun's interior indicate a power density of approximately 276.5 W/m3, a value that more nearly approximates reptile metabolism than a thermonuclear bomb. Peak power production in the Sun has been compared to the volumetric heat generated in an active compost heap. The tremendous power output of the Sun is not due to its high power per volume, but instead due to its large size.
The fusion rate in the core is in a self-correcting equilibrium: a slightly higher rate of fusion would cause the core to heat up more and expand slightly against the weight of the outer layers, reducing the fusion rate and correcting the perturbation; and a slightly lower rate would cause the core to cool and shrink slightly, increasing the fusion rate and again reverting it to its present level.
Radiative zone.
From the core out to about 0.7 solar radii, thermal radiation is the primary means of energy transfer. This zone is not regulated by thermal convection; however the temperature drops from approximately 7 to 2 million kelvin with increasing distance from the core. This temperature gradient is less than the value of the adiabatic lapse rate and hence cannot drive convection. Energy is transferred by radiation—ions of hydrogen and helium emit photons, which travel only a brief distance before being reabsorbed by other ions. The density drops a hundredfold (from 20 g/cm3 to only 0.2 g/cm3) from 0.25 solar radii to the top of the radiative zone.
Tachocline.
The radiative zone and the convective zone are separated by a transition layer, the tachocline. This is a region where the sharp regime change between the uniform rotation of the radiative zone and the differential rotation of the convection zone results in a large shear—a condition where successive horizontal layers slide past one another. The fluid motions found in the convection zone above, slowly disappear from the top of this layer to its bottom, matching the calm characteristics of the radiative zone on the bottom. Presently, it is hypothesized (see Solar dynamo) that a magnetic dynamo within this layer generates the Sun's magnetic field.
Convective zone.
In the Sun's outer layer, from its surface to approximately 200,000 km below (70% of the solar radius from the center), the temperature is lower than in the radiative zone and heavier atoms are not fully ionized. As a result, radiative heat transport is less effective. The density of the plasma is low enough to allow convective currents to develop. Material heated at the tachocline picks up heat and expands, thereby reducing its density and allowing it to rise. As a result, thermal convection develops as thermal cells carry the majority of the heat outward to the Sun's photosphere. Once the material diffusively and radiatively cools just beneath the photospheric surface, its density increases, and it sinks to the base of the convection zone, where it picks up more heat from the top of the radiative zone and the convective cycle continues. At the photosphere, the temperature has dropped to 5,700 K and the density to only 0.2 g/m3 (about 1/6,000th the density of air at sea level).
The thermal columns in the convection zone form an imprint on the surface of the Sun as the solar granulation and supergranulation. The turbulent convection of this outer part of the solar interior sustains "small-scale" dynamo action over the near-surface volume of the Sun. The Sun's thermal columns are Bénard cells and take the shape of hexagonal prisms.
Photosphere.
The visible surface of the Sun, the photosphere, is the layer below which the Sun becomes opaque to visible light. Above the photosphere visible sunlight is free to propagate into space, and its energy escapes the Sun entirely. The change in opacity is due to the decreasing amount of H− ions, which absorb visible light easily. Conversely, the visible light we see is produced as electrons react with hydrogen atoms to produce H− ions.
The photosphere is tens to hundreds of kilometers thick, being slightly less opaque than air on Earth. Because the upper part of the photosphere is cooler than the lower part, an image of the Sun appears brighter in the center than on the edge or "limb" of the solar disk, in a phenomenon known as limb darkening. The spectrum of sunlight has approximately the spectrum of a black-body radiating at about 6,000 K, interspersed with atomic absorption lines from the tenuous layers above the photosphere. The photosphere has a particle density of ~1023 m−3 (about 0.37% of the particle number per volume of Earth's atmosphere at sea level). The photosphere is not fully ionized—the extent of ionization is about 3%, leaving almost all of the hydrogen in atomic form.
During early studies of the optical spectrum of the photosphere, some absorption lines were found that did not correspond to any chemical elements then known on Earth. In 1868, Norman Lockyer hypothesized that these absorption lines were caused by a new element that he dubbed "helium", after the Greek Sun god Helios. Twenty-five years later, helium was isolated on Earth.
Atmosphere.
The parts of the Sun above the photosphere are referred to collectively as the "solar atmosphere". They can be viewed with telescopes operating across the electromagnetic spectrum, from radio through visible light to gamma rays, and comprise five principal zones: the "temperature minimum", the chromosphere, the transition region, the corona, and the heliosphere.
The coolest layer of the Sun is a temperature minimum region about above the photosphere, with a temperature of about . This part of the Sun is cool enough to allow the existence of simple molecules such as carbon monoxide and water, which can be detected via their absorption spectra.
The chromosphere, transition region, and corona are much hotter than the surface of the Sun. The reason is not well understood, but evidence suggests that Alfvén waves may have enough energy to heat the corona.
Above the temperature minimum layer is a layer about thick, dominated by a spectrum of emission and absorption lines. It is called the "chromosphere" from the Greek root "chroma", meaning color, because the chromosphere is visible as a colored flash at the beginning and end of total solar eclipses. The temperature in the chromosphere increases gradually with altitude, ranging up to around near the top. In the upper part of the chromosphere helium becomes partially ionized.
Above the chromosphere, in a thin (about 200 km) transition region, the temperature rises rapidly from around 20,000 K in the upper chromosphere to coronal temperatures closer to 1,000,000 K. The temperature increase is facilitated by the full ionization of helium in the transition region, which significantly reduces radiative cooling of the plasma. The transition region does not occur at a well-defined altitude. Rather, it forms a kind of nimbus around chromospheric features such as spicules and filaments, and is in constant, chaotic motion. The transition region is not easily visible from Earth's surface, but is readily observable from space by instruments sensitive to the extreme ultraviolet portion of the spectrum.
The corona is the next layer of the Sun. The low corona, near the surface of the Sun, has a particle density around 1015–1016 m−3. The average temperature of the corona and solar wind is about 1,000,000–2,000,000 K; however, in the hottest regions it is 8,000,000–20,000,000 K. Although no complete theory yet exists to account for the temperature of the corona, at least some of its heat is known to be from magnetic reconnection.
The corona is the extended atmosphere of the Sun, which has a volume much larger than the volume enclosed by the Sun's photosphere. Waves at the outer surface of the corona that randomly blow even further from the Sun is called the solar wind, and is one of the ways the Sun influences the whole Solar System.
The heliosphere, the tenuous outermost atmosphere of the Sun, is filled with the solar wind plasma. This outermost layer of the Sun is defined to begin at the distance where the flow of the solar wind becomes "superalfvénic"—that is, where the flow becomes faster than the speed of Alfvén waves, at approximately 20 solar radii (0.1 AU).
Turbulence and dynamic forces in the heliosphere cannot affect the shape of the solar corona within, because the information can only travel at the speed of Alfvén waves. The solar wind travels outward continuously through the heliosphere, forming the solar magnetic field into a spiral shape, until it impacts the heliopause more than 50 AU from the Sun. In December 2004, the Voyager 1 probe passed through a shock front that is thought to be part of the heliopause. Both of the Voyager probes have recorded higher levels of energetic particles as they approach the boundary.
The heliosphere extends to the outer fringe of the Solar System, farther than the orbit of Pluto, is defined to end at the heliopause, which is the end of influence from the Sun, and is the boundary with the interstellar medium.
Photons and neutrinos.
High-energy gamma-ray photons initially released with fusion reactions in the core are almost immediately absorbed by the solar plasma of the radiative zone, usually after traveling only a few millimeters. Re-emission happens in a random direction and usually at a slightly lower energy. With this sequence of emissions and absorptions, it takes a long time for radiation to reach the Sun's surface. Estimates of the photon travel time range between 10,000 and 170,000 years. In contrast, it takes only 2.3 seconds for the neutrinos, which account for about 2% of the total energy production of the Sun, to reach the surface. Because energy transport in the Sun is a process that involves photons in thermodynamic equilibrium with matter, the time scale of energy transport in the Sun is longer, on the order of 30,000,000 years. This is the time it would take the Sun to return to a stable state, if the rate of energy generation in its core were suddenly changed.
Neutrinos are also released by the fusion reactions in the core, but, unlike photons, they rarely interact with matter, so almost all are able to escape the Sun immediately. For many years measurements of the number of neutrinos produced in the Sun were lower than theories predicted by a factor of 3. This discrepancy was resolved in 2001 through the discovery of the effects of neutrino oscillation: the Sun emits the number of neutrinos predicted by the theory, but neutrino detectors were missing 2⁄3 of them because the neutrinos had changed flavor by the time they were detected.
Magnetism and activity.
Magnetic field.
The Sun has a magnetic field that varies across a wide range of timescales. The most prominent of such variation is related to the quasi-periodic 11-year solar cycle waxing and waning in the number and size of sunspots.
Sunspots are visible as dark patches on the Sun's photosphere and correspond to concentrations of magnetic field where the convective transport of heat is inhibited from the solar interior to the surface. As a result, sunspots are slightly cooler than the surrounding photosphere, and, so, they appear dark. At a typical solar minimum, few sunspots are visible, and occasionally none can be seen at all. Those that do appear are at high solar latitudes. As the solar cycle progresses towards its maximum, sunspots tend form closer to the solar equator, a phenomenon known as Spörer's law. The largest sunspots can be tens of thousands of kilometers across.
An 11-year sunspot cycle is half of a 22-year Babcock–Leighton dynamo cycle, which corresponds to an oscillatory exchange of energy between toroidal and poloidal solar magnetic fields. At solar-cycle maximum, the external poloidal dipolar magnetic field is near its dynamo-cycle minimum strength, but an internal toroidal quadrupolar field, generated through differential rotation within the tachocline, is near its maximum strength. At this point in the dynamo cycle, buoyant upwelling within the convective zone forces emergence of toroidal magnetic field through the photosphere, giving rise to pairs of sunspots, roughly aligned east–west and having footprints with opposite magnetic polarities. The magnetic polarity of sunspot pairs alternates every solar cycle, a phenomenon known as the Hale cycle.
During the solar cycle’s declining phase, energy shifts from the internal toroidal magnetic field to the external poloidal field, and sunspots diminish in number. At solar-cycle minimum, the toroidal field is, correspondingly, at minimum strength, sunspots are relatively rare, and the poloidal field is at its maximum strength. With the rise of the next 11-year sunspot cycle, differential rotation shifts magnetic energy back from the poloidal to the toroidal field, but with a polarity that is opposite to the previous cycle. The process carries on continuously, and in an idealized, simplified scenario, each 11-year sunspot cycle corresponds to a change, then, in the overall polarity of the Sun's large-scale magnetic field.
The solar magnetic field extends well beyond the Sun itself. The electrically conducting solar wind plasma carries the Sun's magnetic field into space, forming what is called the interplanetary magnetic field. In an approximation known as ideal magnetohydrodynamics, plasma particles only move along the magnetic field lines. As a result, the outward-flowing solar wind stretches the interplanetary magnetic field outward, forcing it into a roughly radial structure. For a simple dipolar solar magnetic field, with opposite hemispherical polarities on either side of the solar magnetic equator, a thin current sheet is formed in the solar wind. At great distances, the rotation of the Sun twists the dipolar magnetic field and corresponding current sheet into an Archimedean spiral structure called the Parker spiral. The interplanetary magnetic field is much stronger than the dipole component of the solar magnetic field. The Sun's dipole magnetic field of 50–400 μT (at the photosphere) reduces with the inverse-cube of the distance to about 0.1 nT at the distance of Earth. However, according to spacecraft observations the interplanetary field at Earth's location is around 5 nT, about a hundred times greater. The difference is due to magnetic fields generated by electrical currents in the plasma surrounding the Sun.
Variation in activity.
The Sun's magnetic field leads to many effects that are collectively called solar activity. Solar flares and coronal-mass ejections tend to occur at sunspot groups. Slowly changing high-speed streams of solar wind are emitted from coronal holes at the photospheric surface. Both coronal-mass ejections and high-speed streams of solar wind carry plasma and interplanetary magnetic field outward into the Solar System. The effects of solar activity on Earth include auroras at moderate to high latitudes and the disruption of radio communications and electric power. Solar activity is thought to have played a large role in the formation and evolution of the Solar System.
With solar-cycle modulation of sunspot number comes a corresponding modulation of space weather conditions, including those surrounding Earth where technological systems can be affected.
Long-term change.
Long-term secular change in sunspot number is thought, by some scientists, to be correlated with long-term change in solar irradiance, which, in turn, might influence Earth's long-term climate.
For example, in the 17th century, the solar cycle appeared to have stopped entirely for several decades; few sunspots were observed during a period known as the Maunder minimum. This coincided in time with the era of the Little Ice Age, when Europe experienced unusually cold temperatures. Earlier extended minima have been discovered through analysis of tree rings and appear to have coincided with lower-than-average global temperatures.
A recent theory claims that there are magnetic instabilities in the core of the Sun that cause fluctuations with periods of either 41,000 or 100,000 years. These could provide a better explanation of the ice ages than the Milankovitch cycles.
Life phases.
The Sun today is roughly halfway through the most stable part of its life. It has not changed dramatically for four billion years, and will remain fairly stable for four billion more. However after hydrogen fusion in its core has stopped, the Sun will undergo severe changes, both internally and externally.
Formation.
The Sun was formed about 4.57 billion years ago from the collapse of part of a giant molecular cloud that consisted mostly of hydrogen and helium and that probably gave birth to many other stars. This age is estimated using computer models of stellar evolution and through nucleocosmochronology. The result is consistent with the radiometric date of the oldest Solar System material, at 4.567 billion years ago. Studies of ancient meteorites reveal traces of stable daughter nuclei of short-lived isotopes, such as iron-60, that form only in exploding, short-lived stars. This indicates that one or more supernovae must have occurred near the location where the Sun formed. A shock wave from a nearby supernova would have triggered the formation of the Sun by compressing the matter within the molecular cloud and causing certain regions to collapse under their own gravity. As one fragment of the cloud collapsed it also began to rotate due to conservation of angular momentum and heat up with the increasing pressure. Much of the mass became concentrated in the center, whereas the rest flattened out into a disk that would become the planets and other Solar System bodies. Gravity and pressure within the core of the cloud generated a lot of heat as it accreted more matter from the surrounding disk, eventually triggering nuclear fusion. Thus, the Sun was born.
Main sequence.
The Sun is about halfway through its main-sequence stage, during which nuclear fusion reactions in its core fuse hydrogen into helium. Each second, more than four million tonnes of matter are converted into energy within the Sun's core, producing neutrinos and solar radiation. At this rate, the Sun has so far converted around 100 times the mass of Earth into energy, about 0.03% of the total mass of the Sun. The Sun will spend a total of approximately 10 billion years as a main-sequence star.
After core hydrogen exhaustion.
The Sun does not have enough mass to explode as a supernova. Instead it will exit the main sequence in approximately 5.4 billion years and start to turn into a red giant. It is calculated that the Sun will become sufficiently large to engulf the current orbits of the Solar System's inner planets, possibly including Earth.
Even before it becomes a red giant, the luminosity of the Sun will have nearly doubled, and Earth will be hotter than Venus is today. Once the core hydrogen is exhausted in 5.4 billion years, the Sun will expand into a subgiant phase and slowly double in size over about half a billion years. It will then expand more rapidly over about half a billion years until it is over two hundred times larger than today and a couple of thousand times more luminous. This then starts the red-giant-branch (RGB) phase where the Sun will spend around a billion years and lose around a third of its mass.
After RGB the Sun now has only about 120 million years of active life left, but they are highly eventful. First the core ignites violently in the helium flash, and the Sun shrinks back to around 10 times its current size with 50 times the luminosity, with a temperature a little lower than today. It will then have reached the red clump or horizontal branch (HB), but a star of the Sun's mass does not evolve blueward along the HB. Instead it just becomes mildly larger and more luminous over about 100 million years as it continues to burn helium in the core.
When the helium is exhausted, the Sun will repeat the expansion it followed when the hydrogen in the core was exhausted, except that this time it all happens faster, and the Sun becomes larger and more luminous. This is the asymptotic-giant-branch (AGB) phase, and the Sun is alternately burning hydrogen in a shell or helium in a deeper shell. After about 20 million years on the early AGB, the Sun becomes increasingly unstable, with rapid mass loss and thermal pulses that increase the size and luminosity for a few hundred years every 100,000 years or so. The thermal pulses become larger each time, with the later pulses pushing the luminosity to as much as 5,000 times the current level and the radius to over 1 AU. Models vary depending on the rate and timing of mass loss. Models that have higher mass loss on the RGB produce smaller, less luminous stars at the tip of the AGB, perhaps only 2,000 times the luminosity and less than 200 times the radius. For the Sun, four thermal pulses are predicted before it completely loses its outer envelope and starts to make a planetary nebula. By the end of that phase – lasting approximately 500,000 years – the Sun will only have about half of its current mass.
The post AGB evolution is even faster. The luminosity stays approximately constant as the temperature increases, with the ejected half of the Sun's mass becoming ionised into a planetary nebula as the exposed core reaches 30,000 K. The final naked core temperature will be over 100,000 K, after which the remnant will cool towards a white dwarf. The planetary nebula will disperse in about 10,000 years, but the white dwarf will survive for trillions of years before fading to black.
Earth's fate.
During the Sun's life in the main sequence, the Sun is becoming more luminous (about 10% every 1 billion years, at the present time). The surface temperature of the Sun is almost constant. The increase of luminosity is essentially due to a slow increase in the solar radius. The increase in solar luminosity is such that in about another billion years Earth's water will evaporate and escape into space, rendering it inhospitable to all known terrestrial life.
Earth is not expected to survive the Sun's transition into a red giant. At its largest, the Sun will have a maximum radius beyond Earth's current orbit, 1 AU (1.5×1011 m), 250 times the present radius of the Sun. By the time the Sun has entered the asymptotic red giant branch, the orbits of the planets will have drifted outwards due to a loss of roughly 30% of the Sun's present mass. Most of this mass will be lost as the solar wind increases. Also, tidal acceleration will help boost Earth to a higher orbit (similar to what Earth does to the Moon). If it were only for this, Earth would probably remain outside the Sun. However, current research suggests that after the Sun becomes a red giant, Earth will be pulled in owing to tidal deceleration.
Motion and location.
Orbit in Milky Way.
The Sun lies close to the inner rim of the Milky Way's Orion Arm, in the Local Interstellar Cloud or the Gould Belt, at a distance of 7.5–8.5 kpc (25,000–28,000 light-years) from the Galactic Center.
The Sun is contained within the Local Bubble, a space of rarefied hot gas, possibly produced by the supernova remnant Geminga. The distance between the local arm and the next arm out, the Perseus Arm, is about 6,500 light-years. The Sun, and thus the Solar System, is found in what scientists call the galactic habitable zone.
The "Apex of the Sun's Way", or the solar apex, is the direction that the Sun travels through space in the Milky Way, relative to other nearby stars. The general direction of the Sun's galactic motion is towards the star Vega in the constellation of Lyra at an angle of roughly 60 sky degrees to the direction of the Galactic Center. Of the 50 nearest stellar systems within 17 light-years from Earth (the closest being the red dwarf Proxima Centauri at approximately 4.2 light-years), the Sun ranks fourth in mass.
The Sun's orbit around the Milky Way is expected to be roughly elliptical with the addition of perturbations due to the galactic spiral arms and non-uniform mass distributions. In addition the Sun oscillates up and down relative to the galactic plane approximately 2.7 times per orbit. It has been argued that the Sun's passage through the higher density spiral arms often coincides with mass extinctions on Earth, perhaps due to increased impact events. It takes the Solar System about 225–250 million years to complete one orbit through the Milky Way (a "galactic year"), so it is thought to have completed 20–25 orbits during the lifetime of the Sun. The orbital speed of the Solar System about the center of the Milky Way is approximately 251 km/s (156 mi/s). At this speed, it takes around 1,190 years for the Solar System to travel a distance of 1 light-year, or 7 days to travel 1 AU.
The Sun's motion about the center of mass of the Solar System is complicated by perturbations from the planets. The barycenter is just outside the volume of the Sun when Jupiter and Saturn (the two planets with the greatest masses) are roughly in the same direction, as seen from the Sun. When they are in opposite directions, and the other planets are aligned appropriately, the barycenter can be very close to the center of the Sun. Every few hundred years this motion switches between prograde and retrograde.
Theoretical problems.
Coronal heating problem.
The temperature of the photosphere is approximately 6,000 K, whereas the temperature of the corona reaches 1,000,000–2,000,000 K. The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.
It is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating. The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone. These waves travel upward and dissipate in the corona, depositing their energy in the ambient matter in the form of heat. The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller events—nanoflares.
Currently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfvén waves have been found to dissipate or refract before reaching the corona. In addition, Alfvén waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.
Faint young Sun problem.
Theoretical models of the Sun's development suggest that 3.8 to 2.5 billion years ago, during the Archean period, the Sun was only about 75% as bright as it is today. Such a weak star would not have been able to sustain liquid water on Earth's surface, and thus life should not have been able to develop. However, the geological record demonstrates that Earth has remained at a fairly constant temperature throughout its history, and that the young Earth was somewhat warmer than it is today. The consensus among scientists is that the atmosphere of the young Earth contained much larger quantities of greenhouse gases (such as carbon dioxide, methane and/or ammonia) than are present today, which trapped enough heat to compensate for the smaller amount of solar energy reaching it.
History of observation.
The enormous effect of the Sun on the Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity.
Early understanding.
The Sun has been an object of veneration in many cultures throughout human history. Humanity's most fundamental understanding of the Sun is as the luminous disk in the sky, whose presence above the horizon creates day and whose absence causes night. In many prehistoric and ancient cultures, the Sun was thought to be a solar deity or other supernatural entity. Worship of the Sun was central to civilizations such as the ancient Egyptians, the Inca of South America and the Aztecs of what is now Mexico. In religions such as Hinduism, the Sun is still considered a God. Many ancient monuments were constructed with solar phenomena in mind; for example, stone megaliths accurately mark the summer or winter solstice (some of the most prominent megaliths are located in Nabta Playa, Egypt; Mnajdra, Malta and at Stonehenge, England); Newgrange, a prehistoric human-built mount in Ireland, was designed to detect the winter solstice; the pyramid of El Castillo at Chichén Itzá in Mexico is designed to cast shadows in the shape of serpents climbing the pyramid at the vernal and autumn equinoxes.
The Egyptians portrayed the god Ra as being carried across the sky in a solar barque, accompanied by lesser gods, and to the Greeks, he was Helios, carried by a chariot drawn by fiery horses. From the reign of Elagabalus in the late Roman Empire the Sun's birthday was a holiday celebrated as Sol Invictus (literally "Unconquered Sun") soon after the winter solstice, which may have been an antecedent to Christmas. Regarding the fixed stars, the Sun appears from Earth to revolve once a year along the ecliptic through the zodiac, and so Greek astronomers considered it to be one of the seven planets (Greek "planetes", “wanderer”), after which the seven days of the week are named in some languages.
Development of scientific understanding.
In the early first millennium BC, Babylonian astronomers observed that the Sun's motion along the ecliptic is not uniform, though they did not know why; it is today known that this is due to the movement of Earth in an elliptic orbit around the Sun, with Earth moving faster when it is nearer to the Sun at perihelion and moving slower when it is farther away at aphelion.
One of the first people to offer a scientific or philosophical explanation for the Sun was the Greek philosopher Anaxagoras, who reasoned that it is a giant flaming ball of metal even larger than the Peloponnesus rather than the chariot of Helios, and that the Moon reflected the light of the Sun. For teaching this heresy, he was imprisoned by the authorities and sentenced to death, though he was later released through the intervention of Pericles. Eratosthenes estimated the distance between Earth and the Sun in the 3rd century BC as "of stadia myriads 400 and 80000", the translation of which is ambiguous, implying either 4,080,000 stadia (755,000 km) or 804,000,000 stadia (148 to 153 million kilometers or 0.99 to 1.02 AU); the latter value is correct to within a few percent. In the 1st century AD, Ptolemy estimated the distance as 1,210 times the radius of Earth, approximately e6km.
The theory that the Sun is the center around which the planets orbit was first proposed by the ancient Greek Aristarchus of Samos in the 3rd century BC, and later adopted by Seleucus of Seleucia (see Heliocentrism). This largely philosophical view was developed into fully predictive mathematical model of a heliocentric system in the 16th century by Nicolaus Copernicus.
Observations of sunspots were recorded during the Han Dynasty (206 BC–AD 220) by Chinese astronomers, who maintained records of these observations for centuries. Averroes also provided a description of sunspots in the 12th century. The invention of the telescope in the early 17th century permitted detailed observations of sunspots by Thomas Harriot, Galileo Galilei and other astronomers. Galileo posited that sunspots were on the surface of the Sun rather than small objects passing between Earth and the Sun.
Arabic astronomical contributions include Albatenius' discovery that the direction of the Sun's apogee (the place in the Sun's orbit against the fixed stars where it seems to be moving slowest) is changing. (In modern heliocentric terms, this is caused by a gradual motion of the aphelion of the "Earth's" orbit). Ibn Yunus observed more than 10,000 entries for the Sun's position for many years using a large astrolabe.
The transit of Venus was first observed in 1032 by Persian astronomer and polymath Avicenna, who concluded that Venus is closer to Earth than the Sun. In 1672 Giovanni Cassini and Jean Richer determined the distance to Mars and were thereby able to calculate the distance to the Sun.
In 1666, Isaac Newton observed the Sun's light using a prism, and showed that it is made up of light of many colors. In 1800, William Herschel discovered infrared radiation beyond the red part of the solar spectrum. The 19th century saw advancement in spectroscopic studies of the Sun; Joseph von Fraunhofer recorded more than 600 absorption lines in the spectrum, the strongest of which are still often referred to as Fraunhofer lines. In the early years of the modern scientific era, the source of the Sun's energy was a significant puzzle. Lord Kelvin suggested that the Sun is a gradually cooling liquid body that is radiating an internal store of heat. Kelvin and Hermann von Helmholtz then proposed a gravitational contraction mechanism to explain the energy output, but the resulting age estimate was only 20 million years, well short of the time span of at least 300 million years suggested by some geological discoveries of that time. In 1890 Joseph Lockyer, who discovered helium in the solar spectrum, proposed a meteoritic hypothesis for the formation and evolution of the Sun.
Not until 1904 was a documented solution offered. Ernest Rutherford suggested that the Sun's output could be maintained by an internal source of heat, and suggested radioactive decay as the source. However, it would be Albert Einstein who would provide the essential clue to the source of the Sun's energy output with his mass-energy equivalence relation . In 1920, Sir Arthur Eddington proposed that the pressures and temperatures at the core of the Sun could produce a nuclear fusion reaction that merged hydrogen (protons) into helium nuclei, resulting in a production of energy from the net change in mass. The preponderance of hydrogen in the Sun was confirmed in 1925 by Cecilia Payne using the ionization theory developed by Meghnad Saha, an Indian physicist. The theoretical concept of fusion was developed in the 1930s by the astrophysicists Subrahmanyan Chandrasekhar and Hans Bethe. Hans Bethe calculated the details of the two main energy-producing nuclear reactions that power the Sun. In 1957, Margaret Burbidge, Geoffrey Burbidge, William Fowler and Fred Hoyle showed that most of the elements in the universe have been synthesized by nuclear reactions inside stars, some like the Sun.
Solar space missions.
The first satellites designed to observe the Sun were NASA's Pioneers 5, 6, 7, 8 and 9, which were launched between 1959 and 1968. These probes orbited the Sun at a distance similar to that of Earth, and made the first detailed measurements of the solar wind and the solar magnetic field. Pioneer 9 operated for a particularly long time, transmitting data until May 1983.
In the 1970s, two Helios spacecraft and the Skylab Apollo Telescope Mount provided scientists with significant new data on solar wind and the solar corona. The Helios 1 and 2 probes were U.S.–German collaborations that studied the solar wind from an orbit carrying the spacecraft inside Mercury's orbit at perihelion. The Skylab space station, launched by NASA in 1973, included a solar observatory module called the Apollo Telescope Mount that was operated by astronauts resident on the station. Skylab made the first time-resolved observations of the solar transition region and of ultraviolet emissions from the solar corona. Discoveries included the first observations of coronal mass ejections, then called "coronal transients", and of coronal holes, now known to be intimately associated with the solar wind.
In 1980, the Solar Maximum Mission was launched by NASA. This spacecraft was designed to observe gamma rays, X-rays and UV radiation from solar flares during a time of high solar activity and solar luminosity. Just a few months after launch, however, an electronics failure caused the probe to go into standby mode, and it spent the next three years in this inactive state. In 1984 Space Shuttle Challenger mission STS-41C retrieved the satellite and repaired its electronics before re-releasing it into orbit. The Solar Maximum Mission subsequently acquired thousands of images of the solar corona before re-entering Earth's atmosphere in June 1989.
Launched in 1991, Japan's Yohkoh ("Sunbeam") satellite observed solar flares at X-ray wavelengths. Mission data allowed scientists to identify several different types of flares, and demonstrated that the corona away from regions of peak activity was much more dynamic and active than had previously been supposed. Yohkoh observed an entire solar cycle but went into standby mode when an annular eclipse in 2001 caused it to lose its lock on the Sun. It was destroyed by atmospheric re-entry in 2005.
One of the most important solar missions to date has been the Solar and Heliospheric Observatory, jointly built by the European Space Agency and NASA and launched on 2 December 1995. Originally intended to serve a two-year mission, a mission extension through 2012 was approved in October 2009. It has proven so useful that a follow-on mission, the Solar Dynamics Observatory (SDO), was launched in February 2010. Situated at the Lagrangian point between Earth and the Sun (at which the gravitational pull from both is equal), SOHO has provided a constant view of the Sun at many wavelengths since its launch. Besides its direct solar observation, SOHO has enabled the discovery of a large number of comets, mostly tiny sungrazing comets that incinerate as they pass the Sun.
All these satellites have observed the Sun from the plane of the ecliptic, and so have only observed its equatorial regions in detail. The Ulysses probe was launched in 1990 to study the Sun's polar regions. It first travelled to Jupiter, to "slingshot" into an orbit that would take it far above the plane of the ecliptic. Serendipitously, it was well-placed to observe the collision of Comet Shoemaker–Levy 9 with Jupiter in 1994. Once Ulysses was in its scheduled orbit, it began observing the solar wind and magnetic field strength at high solar latitudes, finding that the solar wind from high latitudes was moving at about 750 km/s, which was slower than expected, and that there were large magnetic waves emerging from high latitudes that scattered galactic cosmic rays.
Elemental abundances in the photosphere are well known from spectroscopic studies, but the composition of the interior of the Sun is more poorly understood. A solar wind sample return mission, Genesis, was designed to allow astronomers to directly measure the composition of solar material. Genesis returned to Earth in 2004 but was damaged by a crash landing after its parachute failed to deploy on re-entry into Earth's atmosphere. Despite severe damage, some usable samples have been recovered from the spacecraft's sample return module and are undergoing analysis.
The Solar Terrestrial Relations Observatory (STEREO) mission was launched in October 2006. Two identical spacecraft were launched into orbits that cause them to (respectively) pull further ahead of and fall gradually behind Earth. This enables stereoscopic imaging of the Sun and solar phenomena, such as coronal mass ejections.
The Indian Space Research Organisation has scheduled the launch of a 100 kg satellite named Aditya for 2015–16. Its main instrument will be a coronagraph for studying the dynamics of the Solar corona.
Observation and effects.
The brightness of the Sun can cause pain from looking at it with the naked eye; however, doing so for brief periods is not hazardous for normal non-dilated eyes. Looking directly at the Sun causes phosphene visual artifacts and temporary partial blindness. It also delivers about 4 milliwatts of sunlight to the retina, slightly heating it and potentially causing damage in eyes that cannot respond properly to the brightness. UV exposure gradually yellows the lens of the eye over a period of years, and is thought to contribute to the formation of cataracts, but this depends on general exposure to solar UV, and not whether one looks directly at the Sun. Long-duration viewing of the direct Sun with the naked eye can begin to cause UV-induced, sunburn-like lesions on the retina after about 100 seconds, particularly under conditions where the UV light from the Sun is intense and well focused; conditions are worsened by young eyes or new lens implants (which admit more UV than aging natural eyes), Sun angles near the zenith, and observing locations at high altitude.
Viewing the Sun through light-concentrating optics such as binoculars may result in permanent damage to the retina without an appropriate filter that blocks UV and substantially dims the sunlight. When using an attenuating filter to view the Sun, the viewer is cautioned to use a filter specifically designed for that use. Some improvised filters that pass UV or IR rays, can actually harm the eye at high brightness levels.
Herschel wedges, also called Solar Diagonals, are effective and inexpensive for small telescopes. The sunlight that is destined for the eyepiece is reflected from an unsilvered surface of a piece of glass. Only a very small fraction of the incident light is reflected. The rest passes through the glass and leaves the instrument. If the glass breaks because of the heat, no light at all is reflected, making the device fail-safe. Simple filters made of darkened glass allow the full intensity of sunlight to pass through if they break, endangering the observer's eyesight. Unfiltered binoculars can deliver hundreds of times as much energy as using the naked eye, possibly causing immediate damage. It is claimed that even brief glances at the midday Sun through an unfiltered telescope can cause permanent damage.
Partial solar eclipses are hazardous to view because the eye's pupil is not adapted to the unusually high visual contrast: the pupil dilates according to the total amount of light in the field of view, "not" by the brightest object in the field. During partial eclipses most sunlight is blocked by the Moon passing in front of the Sun, but the uncovered parts of the photosphere have the same surface brightness as during a normal day. In the overall gloom, the pupil expands from ~2 mm to ~6 mm, and each retinal cell exposed to the solar image receives up to ten times more light than it would looking at the non-eclipsed Sun. This can damage or kill those cells, resulting in small permanent blind spots for the viewer. The hazard is insidious for inexperienced observers and for children, because there is no perception of pain: it is not immediately obvious that one's vision is being destroyed.
During sunrise and sunset, sunlight is attenuated due to Rayleigh scattering and Mie scattering from a particularly long passage through Earth's atmosphere, and the Sun is sometimes faint enough to be viewed comfortably with the naked eye or safely with optics (provided there is no risk of bright sunlight suddenly appearing through a break between clouds). Hazy conditions, atmospheric dust, and high humidity contribute to this atmospheric attenuation.
An optical phenomenon, known as a green flash, can sometimes be seen shortly after sunset or before sunrise. The flash is caused by light from the Sun just below the horizon being bent (usually through a temperature inversion) towards the observer. Light of shorter wavelengths (violet, blue, green) is bent more than that of longer wavelengths (yellow, orange, red) but the violet and blue light is scattered more, leaving light that is perceived as green.
Ultraviolet light from the Sun has antiseptic properties and can be used to sanitize tools and water. It also causes sunburn, and has other medical effects such as the production of vitamin D. Ultraviolet light is strongly attenuated by Earth's ozone layer, so that the amount of UV varies greatly with latitude and has been partially responsible for many biological adaptations, including variations in human skin color in different regions of the globe.

</doc>
<doc id="26752" url="http://en.wikipedia.org/wiki?curid=26752" title="Smiley">
Smiley

A smiley (sometimes simply called a happy or smiling face) is a stylized representation of a smiling humanoid face, an important part of popular culture. The classic form designed in 1963 comprises a yellow circle with two black dots representing eyes and a black arc representing the mouth (codice_1). On the Internet and in other plain text communication channels, the emoticon form (sometimes also called the smiley-face emoticon) has traditionally been most popular, typically employing a colon and a right parenthesis to form sequences like codice_2, codice_3, or codice_4 that resemble a smiling face when viewed
sideways. "Smiley" is also sometimes used as a generic term for any emoticon. The smiley has been referenced in nearly all areas of Western culture including music, movies, and art.
The variant spelling "smilie" is not as common, but the plural form "smilies" is commonly used.
In 1992 a smiley face museum was started by American University alumni Mark Sachs in Silver Spring, Maryland.
History.
The poet and author Johannes V. Jensen was amongst other things famous for experimenting with the form of his writting. In a letter sent to publisher Ernst Bojesen in December 1900 he includes both a happy face and a sad face, resembling the modern smiley. 
Ingmar Bergman's 1948 film "Port of Call" includes a scene where the unhappy Berit draws a "sad" face – closely resembling the modern "frowny", but including a dot for the nose – in lipstick on her mirror, before being interrupted. In 1953 and 1958, similar happy faces were used in promotional campaigns for the films "Lili" and "Gigi".
The smiley was first introduced to popular culture as part of a promotion by New York radio station WMCA beginning in 1962. Listeners who answered their phone "WMCA Good Guys!" were rewarded with a "WMCA good guys" sweatshirt that incorporated a happy face into its design. Thousands of these sweatshirts were given away. The WMCA smiley was yellow with black dots as eyes, but it had a slightly crooked smile instead of a full smile, and no creases in the mouth.
In 1963, Harvey Ball, an American commercial artist, was employed by State Mutual Life Assurance Company of Worcester, Massachusetts (now known as Hanover Insurance) to create a happy face to raise the morale of the employees. Ball created the design in ten minutes and was paid $45 (equivalent to $330 USD in 2012 currency). His rendition, with bright yellow background, dark oval eyes, full smile and creases at the sides of the mouth, was imprinted on more than fifty million buttons and was familiar around the world. The design is so simple that it is certain that similar versions were produced before 1963, including those cited above. However, Ball’s rendition, as described here, has become the most iconic version. In 1967, Seattle graphic artist, George Tenagi, drew his own version at the request of advertising agent, David Stern. Tenagi's design was used in an advertising campaign for Seattle-based University Federal Savings & Loan. The ad campaign was inspired by Charles Strouse' lyrics in "Put on a Happy Face" from the musical "Bye Bye Birdie". Stern, the man behind this campaign, incorporated the Happy Face in his run for Seattle Mayor in 1993.
The graphic was further popularized in the early 1970s by Philadelphia brothers Bernard and Murray Spain, who seized upon it in September 1970 in a campaign to sell novelty items. The two produced buttons as well as coffee mugs, t-shirts, bumper stickers and many other items emblazoned with the symbol and the phrase "Have a happy day" (devised by Gyula Bogar), which mutated into "have a nice day". Working with New York button manufacturer NG Slater, some 50 million happy face badges were produced by 1972.
In 1972 Frenchman Franklin Loufrani became the first person to legally trademark the smiley face. He used it to highlight the good news parts of the newspaper France Soir. He simply called the design "Smiley" and launched the Smiley Company. In 1996 Loufrani's son Nicolas took over the family business and transformed it into a huge multinational corporation. Nicolas Loufrani was outwardly skeptical of Harvey Ball's claim to creating the first smiley face. After all, the design that his father came up with and Ball's design were nearly identical. Loufrani argued that the design is so simple that no one person can lay claim to having created it. As evidence for this, Loufrani's website points to early cave paintings found in France (2500 BC) that he claims are the first depictions of a smiley face. Loufrani also points to a 1960 radio ad campaign that reportedly made use of a similar design.
In the UK, the happy face has been associated with psychedelic culture since Ubi Dwyer and the Windsor Free Festival in the 1970s and the electronic dance music culture, particularly with acid house, that emerged during the Second Summer of Love in the late 1980s. The association was cemented when the band Bomb the Bass used an extracted smiley from "Watchmen" on the centre of its "Beat Dis" hit single.
In text.
On the Internet, the smiley has become a visual means of conveyance that uses images. On September 19, 1982, Scott Fahlman from Carnegie Mellon University first proposed using the emoticon codice_2 to mark jokes from serious posts in online message boards. There is no history of the smiley/emoticons occurring prior to this on what would become the Internet. Fahlman stated “I propose that [sic] the following character sequence for joke markers: :-) . Read it sideways. Actually, it is probably more economical to mark things that are NOT jokes, given current trends. For this, use: :-(.” This suggestion took a symbol used predominantly marketing and it “became an integral part of online communication, if not always a welcome one. These "smileys," as they came to be known, were effectively the first online irony marks.” As the digital age evolved the need for smileys that were easily understood across all cultures gave birth to the emoji.
One of the first uses of the smiley in text may have been in Robert Herrick's poem "To Fortune" (1648), which contains the line "Upon my ruines (smiling yet :)". Journalist Levi Stahl has suggested that this may have been an intentional "orthographic joke", but this interpretation of the punctuation is disputed, and there are citations of similar punctuation in a non-humorous context, even within Herrick's own work. It is likely that the parenthesis was added later by modern editors.
The smiley is the printable version of characters 1 and 2 of (black-and-white versions of) codepage 437 (1981) of the first IBM PC and all subsequent PC compatible computers. For modern computers, all versions of Microsoft Windows after Windows 95 can use the smiley as part of Windows Glyph List 4, although some computer fonts miss some characters, and some characters cannot be reproduced by programs not compatible with Unicode. It also appears in Unicode's Basic Multilingual Plane.
Licensing and legal issues.
The rights to the Smiley trademark in one hundred countries are owned by the Smiley Company. Its subsidiary SmileyWorld Ltd, in London, headed by Nicolas Loufrani, creates or approves all the Smiley products sold throughout the world. The Smiley brand and logo have significant exposure through licensees in sectors such as clothing, home decoration, perfumery, plush, stationery, publishing, and through promotional campaigns. The Smiley Company is one of the 100 biggest licensing companies in the world, with a turnover of US$167 million in 2012. The first Smiley shop opened in London in the Boxpark shopping centre in December 2011.
In 1997, Franklin Loufrani and Smiley World attempted to acquire trademark rights to the symbol (and even to the word "smiley" itself) in the United States. This brought Loufrani into conflict with Wal-Mart, which had begun prominently featuring a happy face in its "Rolling Back Prices" campaign over a year earlier. Wal-Mart responded first by trying to block Loufrani's application, then later by trying to register the smiley face itself; Loufrani in turn sued to stop Wal-Mart's application, and in 2002 the issue went to court, where it would languish for seven years before a decision.
Wal-Mart began phasing out the smiley face on its vests and its website in 2006. Despite that, Wal-Mart sued an online parodist for alleged "trademark infringement" after he used the symbol (as well as various portmanteaus of "Wal-", such as "Walocaust"). The District Court found in favor of the parodist when in March 2008, the judge concluded that [Wal-Mart's] smiley face [logo] was not shown to be "inherently distinctive" and that it "has failed to establish that the smiley face has acquired secondary meaning or that it is otherwise a protectible trademark" under U.S. law.
In June 2010, Wal-Mart and the Smiley Company founded by Loufrani settled their 10-year-old dispute in front of the Chicago federal court. The terms remain confidential.

</doc>
<doc id="26753" url="http://en.wikipedia.org/wiki?curid=26753" title="Signature">
Signature

A signature (from Latin: "signare", "to sign") is a handwritten (and often stylized) depiction of someone's name, nickname, or even a simple "X" or other mark that a person writes on documents as a proof of identity and intent. The writer of a signature is a signatory or signer. Similar to a handwritten signature, a signature work describes the work as readily identifying its creator. A signature may be confused with an autograph, which is chiefly an artistic signature. This can lead to confusion when people have both an autograph and signature and as such some people in the public eye keep their signatures private whilst fully publishing their autograph.
Function and types.
The traditional function of a signature is evidential: it is to give evidence of:
For example, the role of a signature in many consumer contracts is not solely to provide evidence of the identity of the contracting party, but also to provide evidence of deliberation and informed consent.
In many countries, signatures may be witnessed and recorded in the presence of a notary public to carry additional legal force. On legal documents, an illiterate signatory can make a "mark" (often an "X" but occasionally a personalized symbol), so long as the document is countersigned by a literate witness. In some countries, illiterate people place a thumbprint on legal documents in lieu of a written signature.
There are many other terms which are synonymous with 'signature'. In the United States, one is John Hancock, named after the first of the signatories of the United States Declaration of Independence.
The signature of a famous person is sometimes known as an autograph, and is then typically written on its own or with a brief note to the recipient. Rather than providing authentication for a document, the autograph is given as a souvenir which acknowledges the recipient's access to the autographer.
In the United States, signatures encompass marks and actions of all sorts that are indicative of identity and intent. The legal rule is that unless a statute specifically prescribes a particular method of making a signature it may be made in any number of ways. These include by a mechanical or rubber stamp facsimile. A signature may be made by the purported signatory; alternatively someone else duly authorized by the signatory, acting in the signer's presence and at the signatory's direction, may make the signature.
Many individuals have much more fanciful signatures than their normal cursive writing, including elaborate ascenders, descenders and exotic flourishes, much as one would find in calligraphic writing. As an example, the final "k" in John Hancock's famous signature on the US Declaration of Independence loops back to underline his name. This kind of flourish is also known as a "paraph".
Several cultures whose languages use writing systems other than alphabets do not share the Western notion of signatures per se: the "signing" of one's name results in a written product no different from the result of "writing" one's name in the standard way. For these languages, to write or to sign involves the same written characters. Also see Calligraphy.
Mechanically produced signatures.
Special signature machines, called autopens, are capable of automatically reproducing an individual's signature. These are typically used by people required to 
sign a lot of printed matter, such as celebrities, heads of state or CEOs.
More recently, Members of Congress in the United States have begun having their signature made into a TrueType font file. This allows staff members in the Congressman's office to easily reproduce it on correspondence, legislation, and official documents. 
In the East Asian languages of Chinese, Japanese, and Korean, people typically use "name-seals" with the name written in "tensho" script ("seal script") in lieu of a handwritten signature. 
Wet signatures.
Some government agencies require that professional persons or official reviewers sign originals and all copies of originals to authenticate that they personally viewed the content. In the United States this is prevalent with architectural and construction plans. Its intent is to prevent mistakes or fraud but the practice is not known to be effective.
Online usage.
In e-mail and newsgroup usage, another type of signature exists which is independent of one's language. Users can set one or more lines of custom text known as a signature block to be automatically appended to their messages. This text usually includes a name, contact information, and sometimes quotations and ASCII art. A shortened form of a signature block, only including one's name, often with some distinguishing prefix, can be used to simply indicate the end of a post or response. Some web sites also allow graphics to be used. Note, however, that this type of signature is not related to electronic signatures or digital signatures, which are more technical in nature and not directly understandable by humans.
Other uses.
The signature on a painting or other work of art has always been an important item in the assessment of art. Fake signatures are sometimes added to enhance the value of a painting, or are added to a fake painting to support its authenticity. A notorious case was the signature of Johannes Vermeer on the fake "Supper at Emmaus" made by the art-forger Han van Meegeren.
However, the fact that often painters' signatures vary over time (particularly in the modern and contemporary periods) might complicate the issue. The signatures of some painters take on an artistic form that may be of less value in determining forgeries. For example, Daniel C. Boyer's gouaches are known for their often large, elaborate to the point of near-illegibility, and multicoloured signatures.
The term "signature" is also used to mean the characteristics that give an object, or a piece of information, its identity—for example, the shape of a Coca-Cola bottle.
By analogy, the word "signature" may be used to refer to the characteristic expression of a process or thing. For example, the climate phenomenon known as ENSO or El Niño has characteristic modes in different ocean basins which are often referred to as the "signature" of ENSO.
Copyright.
Under British law, the appearance of signatures (not the names themselves) may be protected under copyright law.
Under United States Copyright Law, "titles, names [...]; mere variations of typographic ornamentation, lettering, or coloring" are not eligible for copyright; however, the appearance of signatures (not the names themselves) may be protected under copyright law.
Uniform Commercial Code.
Uniform Commercial Code §1-201(37) of the United States generally defines signed as "using any symbol executed or adopted with present intention to adopt or accept a writing."
Uniform Commercial Code §3-401(b) for negotiable instruments states "A signature may be made (i) manually or by means of a device or machine, and (ii) by the use of any name, including a trade or assumed name, or by a word, mark, or symbol executed or adopted by a person with present intention to authenticate a writing."
See also.
Listen to this article ()
This audio file was created from a revision of the "Signature" article dated 2006-05-21, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="26754" url="http://en.wikipedia.org/wiki?curid=26754" title="Seal">
Seal

Seal commonly refers to: 
Seal may also refer to:

</doc>
<doc id="26756" url="http://en.wikipedia.org/wiki?curid=26756" title="Sino-Tibetan languages">
Sino-Tibetan languages

The Sino-Tibetan languages are a family of more than 400 languages spoken in East Asia, Southeast Asia and parts of South Asia. The family is second only to the Indo-European languages in terms of the number of native speakers. The Sino-Tibetan languages with the most native speakers are the Chinese languages (1.2 billion speakers), Burmese (33 million) and the Tibetic languages (8 million). Many Sino-Tibetan languages are spoken by small communities in remote mountain areas and are poorly documented.
Several low-level groupings are well established, but the higher-level structure of the family remains unclear. Although the family is often presented as divided into Sinitic and Tibeto-Burman branches, a common origin of the non-Sinitic languages has never been demonstrated, and is rejected by an increasing number of researchers. A minority of researchers call the whole family "Tibeto-Burman", and the name "Trans-Himalayan" has also been proposed.
History.
A genetic relationship between Chinese, Tibetan, Burmese and other languages was first proposed in the early 19th century, and is now broadly accepted. The initial focus on languages of civilizations with long literary traditions has been broadened to include less widely spoken languages, some of which have only recently, or never, been written. However, the reconstruction of the family is much less developed than for families such as Indo-European or Austroasiatic. Difficulties have included the great diversity of the languages, the lack of inflection in many of them, and the effects of language contact. In addition, many of the smaller languages are spoken in mountainous areas that are difficult to access, and are often also sensitive border zones.
Early work.
During the 18th century, several scholars had noticed parallels between Tibetan and Burmese, both languages with extensive literary traditions.
Early in the following century, Brian Houghton Hodgson and others noted that many non-literary languages of the highlands of northeast India and Southeast Asia were also related to these.
The name "Tibeto-Burman" was first applied to this group in 1856 by James Richardson Logan, who added Karen in 1858.
The third volume of the "Linguistic Survey of India", edited by Sten Konow, was devoted to the Tibeto-Burman languages of British India.
Studies of the "Indo-Chinese" languages of Southeast Asia from the mid-19th century by Logan and others revealed that they comprised four families: Tibeto-Burman, Tai, Mon–Khmer and Malayo-Polynesian.
Julius Klaproth had noted in 1823 that Burmese, Tibetan and Chinese all shared common basic vocabulary but that Thai, Mon, and Vietnamese were quite different.
Ernst Kuhn envisaged a group with two branches, Chinese-Siamese and Tibeto-Burman.
August Conrady called this group Indo-Chinese in his influential 1896 classification, though he had doubts about Karen. Conrady's terminology was widely used, but there was uncertainty regarding his exclusion of Vietnamese. Franz Nikolaus Finck in 1909 placed Karen as a third branch of Chinese-Siamese.
Jean Przyluski introduced the term "sino-tibétain" as the title of his chapter on the group in Meillet and Cohen's "Les langues du monde" in 1924.
He retained Conrady's two branches of Tibeto-Burman and "Sino-Daic", with Miao–Yao included within Daic (Tai–Kadai).
The English translation "Sino-Tibetan" first appeared in a short note by Przyluski and Luce in 1931.
Shafer and Benedict.
In 1935, the anthropologist Alfred Kroeber started the Sino-Tibetan Philology Project, funded by the Works Project Administration and based at the University of California, Berkeley.
The project was supervised by Robert Shafer until late 1938, and then by Paul K. Benedict.
Under their direction, the staff of 30 non-linguists collated all the available documentation of Sino-Tibetan languages.
The result was 8 copies of a 15-volume typescript entitled "Sino-Tibetan Linguistics".
This work was never published, but furnished the data for a series of papers by Shafer, as well as Shafer's five-volume "Introduction to Sino-Tibetan" and Benedict's "Sino-Tibetan, a Conspectus".
Benedict completed the manuscript of his work in 1941, but it was not published until 1972. Instead of building the entire family tree, he set out to reconstruct a Proto-Tibeto-Burman language by comparing five major languages, with occasional comparisons with other languages. He reconstructed a two-way distinction on initial consonants based on voicing, with aspiration conditioned by pre-initial consonants that had been retained in Tibetic but lost in many other languages. Thus, Benedict reconstructed the following initials:
Although the initial consonants of cognates tend to have the same place and manner of articulation, voicing and aspiration is often unpredictable.
This irregularity was attacked by Roy Andrew Miller, though Benedict's supporters attribute it to the effects of prefixes that have been lost and are often unrecoverable.
The issue remains unsolved today.
It was cited together with the lack of reconstructable shared morphology, and evidence that much shared lexical material has been borrowed from Chinese into Tibeto-Burman, by Christopher Beckwith, one of the few scholars still arguing that Chinese is not related to Tibeto-Burman.
Study of literary languages.
Old Chinese is by far the oldest recorded Sino-Tibetan language, with inscriptions dating from 1200 BC and a huge body of literature from the first millennium BC, but the Chinese script is not alphabetic. Scholars have sought to reconstruct the phonology of Old Chinese by comparing the obscure descriptions of the sounds of Middle Chinese in medieval dictionaries with phonetic elements in Chinese characters and the rhyming patterns of early poetry. The first complete reconstruction, the "Grammata Serica Recensa" of Bernard Karlgren, was used by Benedict and Shafer.
It was somewhat unwieldy, with many sounds with a highly non-uniform distribution. Later scholars have refined Karlgren's work by drawing on a range of other sources. Some proposals were based on cognates in other Sino-Tibetan languages, though workers have also found solely Chinese evidence for them. For example, recent reconstructions of Old Chinese have reduced Karlgren's 15 vowels to a six-vowel system originally suggested by Nicholas Bodman on the basis of comparisons with Tibetic. Similarly, Karlgren's *l has been recast as *r, with a different initial interpreted as *l, matching Tibeto-Burman cognates, but also supported by Chinese transcriptions of foreign names. A growing number of scholars believe that Old Chinese was an atonal language, and that the tones of Middle Chinese developed from final consonants. One of these, *-s, is believed to be a suffix, with cognates in other Sino-Tibetan languages.
Tibetic has extensive written records from the adoption of writing by the Tibetan Empire in the mid-7th century. The earliest records of Burmese (such as the 12th-century Myazedi inscription) are more limited, but later an extensive literature developed. Both languages are recorded in alphabetic scripts ultimately derived from the Brahmi script of Ancient India. Most comparative work has used the conservative written forms of these languages, following the dictionaries of Jäschke (Tibetan) and Judson (Burmese), though both contain entries from a wide range of periods.
There are also extensive records in Tangut, the language of the Western Xia (1038–1227). Tangut is recorded in a Chinese-inspired logographic script, whose interpretation presents many difficulties, even though multilingual dictionaries have been found.
Gong Hwang-cherng has compared Old Chinese, Tibetic, Burmese and Tangut in an effort to establish sound correspondences between those languages. He found that Tibetic and Burmese /a/ correspond to two vowels, *a and *ə, in Old Chinese. While this has been considered evidence for a separate Tibeto-Burman subgroup, Hill (2014) finds that Burmese still distinguishes the specific rhymes *-aj (> "-ay") and *-əj (> "-i"), and hence the development *ə > *a should be considered to have occurred independently in Tibetan and Burmese. 
Fieldwork.
The descriptions of non-literary languages used by Shafer and Benedict were often produced by missionaries and colonial administrators of varying linguistic skill.
Most of the smaller Sino-Tibetan languages are spoken in inaccessible mountainous areas, many of which are politically or militarily sensitive and thus closed to investigators.
Until the 1980s, the best-studied areas were Nepal and northern Thailand.
In the 1980s and 1990s, new surveys were published from the Himilayas and southwestern China.
Of particular interest was the discovery of a new branch of the family, the Qiangic languages of western Sichuan and adjacent areas.
Classification.
Several low-level branches of the family, particularly Lolo-Burmese, have been securely reconstructed, but in the absence of a secure reconstruction of proto-Sino-Tibetan, the higher-level structure of the family remains unclear.
Thus, a conservative classification of Sino-Tibetan/Tibeto-Burman would posit several dozen small coordinate families and isolates; attempts at subgrouping are either geographic conveniences or hypotheses for further research.
Li (1937).
In a survey in the 1937 "Chinese Yearbook", Li Fang-Kuei described the family as consisting of four branches:
Tai and Miao–Yao were included because they shared isolating typology, tone systems and some vocabulary with Chinese. At the time, tone was considered so fundamental to language that tonal typology could be used as the basis for classification. In the Western scholarly community, these languages are no longer included in Sino-Tibetan, with the similarities attributed to diffusion across the Mainland Southeast Asia linguistic area, especially since .
The exclusions of Vietnamese by Kuhn and of Tai and Miao–Yao by Benedict were vindicated in 1954 when André-Georges Haudricourt demonstrated that the tones of Vietnamese were reflexes of final consonants from Proto-Mon–Khmer.
Many Chinese linguists continue to follow Li's classification. However, this arrangement remains problematic. For example, there is disagreement over whether to include the entire Tai–Kadai family or just Kam–Tai (Zhuang–Dong excludes the Kra languages), because the Chinese cognates that form the basis of the putative relationship are not found in all branches of the family and have not been reconstructed for the family as a whole. In addition, Kam–Tai itself no longer appears to be a valid node within Tai–Kadai.
Benedict (1942).
Benedict overtly excluded Vietnamese (placing it in Mon–Khmer) as well as Hmong–Mien and Tai–Kadai (placing them in Austro-Tai).
He otherwise retained the outlines of Conrady's Indo-Chinese classification, though putting Karen in an intermediate position:
Shafer (1955).
Shafer criticized the division of the family into Tibeto-Burman and Sino-Daic branches, which he attributed to the different groups of languages studied by Konow and other scholars in British India on the one hand and by Henri Maspero and other French linguists on the other.
He proposed a detailed classification, with six top-level divisions:
Shafer was skeptical of the inclusion of Daic, but after meeting Maspero in Paris decided to retain it pending a definitive resolution of the question.
Matisoff (1978).
James Matisoff abandoned Benedict's Tibeto-Karen hypothesis:
Some more-recent Western scholars, such as Bradley (1997) and La Polla (2003), have retained Matisoff's two primary branches, though differing in the details of Tibeto-Burman. However, Jacques (2006) notes, "comparative work has never been able to put forth evidence for common innovations to all the Tibeto-Burman languages (the Sino-Tibetan languages to the exclusion of Chinese)" and that "it no longer seems justified to treat Chinese as the first branching of the Sino-Tibetan family," because the morphological divide between Chinese and Tibeto-Burman has been bridged by recent reconstructions of Old Chinese.
Starostin (1996).
Sergei Starostin proposed that both the Kiranti languages and Chinese are divergent from a "core" Tibeto-Burman of at least Bodish, Lolo-Burmese, Tamangic, Jinghpaw, Kukish, and Karen (other families were not analysed) in a hypothesis called "Sino-Kiranti". The proposal takes two forms: that Sinitic and Kiranti are themselves a valid node or that the two are not demonstrably close, so that Sino-Tibetan has three primary branches:
Van Driem (1997, 2001).
Van Driem, like Shafer, rejects a primary split between Chinese and the rest, suggesting that Chinese owes its traditional privileged place in Sino-Tibetan to historical, typological, and cultural rather than linguistic criteria. He calls the entire family "Tibeto-Burman", a name he says has historical primacy, but other linguists who reject a privileged position for Chinese continue to call the resulting family "Sino-Tibetan".
Like Matisoff, van Driem acknowledges that the relationships of the "Kuki–Naga" languages (Kuki, Mizo, Meithei, etc.), both amongst each other and to the other languages of the family, remain unclear. However, rather than placing them in a geographic grouping, as Matisoff does, van Driem leaves them unclassified.
He has proposed several hypotheses, including the reclassification of Chinese to a Sino-Bodic subgroup:
Van Driem points to two main pieces of evidence establishing a special relationship between Sinitic and Bodic and thus placing Chinese within the Tibeto-Burman family. First, there are a number of parallels between the morphology of Old Chinese and the modern Bodic languages. Second, there is an impressive body of lexical cognates between the Chinese and Bodic languages, represented by the Kirantic language Limbu.
In response, Matisoff notes that the existence of shared lexical material only serves to establish an absolute relationship between two language families, not their relative relationship to one another. Although some cognate sets presented by van Driem are confined to Chinese and Bodic, many others are found in Sino-Tibetan languages generally and thus do not serve as evidence for a special relationship between Chinese and Bodic.
Van Driem (2011).
Van Driem has also proposed a "fallen leaves" model that lists 40 well-established low-level groups while remaining agnostic about intermediate grouping of these:
Van Driem considers the recently discovered 'Ole, Gongduk, and Lhokpu languages to be independent top-level subgroups and notes that there are probably even more than those listed above. He also suggests that the Sino-Tibetan language family be renamed "Trans-Himalayan", which he considers to be more neutral. Subgroups yet to be incorporated in the above are Ersuish and Naic.
Blench and Post (2013).
Roger Blench and Mark W. Post have criticized the applicability of conventional Sino-Tibetan classification schemes to minor languages lacking an extensive written history (unlike Chinese, Tibetic, and Burmese). They find that the evidence for the subclassification or even ST affiliation at all of several minor languages of northeastern India, in particular, is either poor or absent altogether.
While relatively little has been known about the languages of this region up to and including the present time, this has not stopped scholars from proposing that these languages either constitute or fall within some other Tibeto-Burman subgroup. However, in absence of any sort of systematic comparison – whether the data are thought reliable or not – such "subgroupings" are essentially vacuous. The use of pseudo-genetic labels such as "Himalayish" and "Kamarupan" inevitably give an impression of coherence which is at best misleading.—, p. 3
In their view, many such languages would for now be best considered unclassified, or "internal isolates" within the family. They propose a provisional classification of the remaining languages:
Because they propose that the three best-known branches may actually be much closer related to each other than they are to "minor" Sino-Tibetan languages, Blench and Post argue that "Sino-Tibetan" or "Tibeto-Burman" would be inappropriate names for a family whose earliest divergences lead to different languages altogether. They support the proposed name "Trans-Himalayan".
Typology.
Word order.
Except for the Chinese, Karen, and Bai languages, the usual word order in Sino-Tibetan languages is object–verb. Most scholars believe this to be the original order, with Chinese, Karen and Bai having acquired subject–verb–object order due to the influence of neighbouring languages in the Mainland Southeast Asia linguistic area.
However, Chinese and Bai differ from almost all other VO languages in the world in placing relative clauses before the nouns they modify.
Morphology.
Hodgson had in 1849 noted a dichotomy between "pronominalized" (inflecting) languages, stretching across the Himalayas from Himachal Pradesh to eastern Nepal, and "non-pronominalized" (isolating) languages. Konow (1909) explained the pronominalized languages as due to a Munda substratum, with the idea that Indo-Chinese languages were essentially isolating as well as tonal. Maspero later attributed the putative substratum to Indo-Aryan. It was not until Benedict that the inflectional systems of these languages were recognized as (partially) native to the family.
Scholars disagree over the extent to which the agreement system in the various languages can be reconstructed for the proto-language.
In morphosyntactic alignment, many Tibeto-Burman languages have ergative and/or anti-ergative (an argument that is not an actor) case marking. However, the anti-ergative case markings can not be reconstructed at higher levels in the family and are thought to be innovations.
External classification.
Beyond the traditionally recognized families of Southeast Asia, a number of possible broader relationships have been suggested. One of these is the "Sino-Caucasian" hypothesis of Sergei Starostin, which posits that the Yeniseian languages and North Caucasian languages form a clade with Sino-Tibetan. The Sino-Caucasian hypothesis has been expanded by others to "Dené–Caucasian" to include the Na-Dené languages of North America, Burushaski, Basque and, occasionally, Etruscan. Edward Sapir had commented on a connection between Na-Dené and Sino-Tibetan. A narrower binary Dené–Yeniseian family has recently been well-received, though not conclusively demonstrated. In contrast, Laurent Sagart proposes a Sino-Austronesian family relating Sino-Tibetan to the Austronesian and Tai–Kadai languages.
Peoples and languages.
Proportion of first-language speakers of larger branches of Sino-Tibetan
   Chinese
 (94.28%)   Lolo-Burmese
 (3.39%)   Tibetic
 (0.44%)   Karen
 (0.30%)  Other (1.59%)
There is no ethnic unity among the many peoples who speak Sino-Tibetan languages. 
The most numerous are the Han Chinese, numbering 1.3 billion. The Hui (10 million) also speak Chinese but are considered ethnically distinct by the PRC regime. The more numerous peoples speaking other Sino-Tibetan languages are the Burmese (42 million), Yi (Lolo) (7 million), Tibetans (6 million), Karen (5 million), Meitheis (1.5 million), Naga (1.2 million), Tamang (1.1 million), Chin (1.1 million), Newar (1 million), Bodo (1.5 million), and Kachin (1 million). The Burmese live in Burma (Myanmar). Kachin, Karen, Red Karen, and Chin peoples live in the Rakhine, Kachin, Kayin, Kayah, and Chin states of Burma. Tibetans live in the Tibet Autonomous Region, Qinghai, western Sichuan, Gansu, and northern Yunnan provinces in China and in Ladakh in the Kashmir region of Pakistan and India, whereas Manipuris, Mizo, Naga, Tripuri, Idu Mishmis, and Garo live in Manipur, Mizoram, Nagaland, Tripura, and Meghalaya states of India. Bodo and Karbi live in Assam, India, whereas Adi, Nishi, Apa Tani, and Galo live in Arunachal Pradesh, India. The Newar and Tamang live in Nepal and Sikkim, India.
J. A. Matisoff proposed that the urheimat of the Sino-Tibetan languages was located around the upper reaches of the Yangtze, Brahmaputra, Salween, and Mekong. This view is in accordance with the hypothesis that bubonic plague, cholera, and other diseases made the easternmost foothills of the Himalayas between China and India difficult for people outside to migrate in but relatively easily for the indigenous people, who had been adapted to the environment, to migrate out.

</doc>
<doc id="26757" url="http://en.wikipedia.org/wiki?curid=26757" title="Slavic languages">
Slavic languages

The Slavic languages (also called Slavonic languages), a group of closely related languages of the Slavic peoples and a subgroup of Indo-European languages, have speakers in most of Eastern Europe, much of the Balkans, parts of Central Europe, and the northern part of Asia. The Slavic languages are spoken by some 315 million people (see Table on the right), and are the second largest native spoken language group among the Languages of Europe, after the Germanic languages and ahead of the Romance languages.
Branches.
Scholars traditionally divide Slavic languages on the basis of geographical and genealogical principle into three main branches, some of which feature subbranches:
Some linguists speculate that a North Slavic branch has existed as well. The Old Novgorod dialect may have reflected some idiosyncrasies of this group. On the other hand, the term "North Slavic" is also used sometimes to combine the West and East Slavic languages into one group, in opposition to the South Slavic languages, due to traits the West and East Slavic branches share with each other that they do not with the South Slavic languages.
The most obvious differences between the West and East Slavic branches are in the orthography of the standard languages: West Slavic languages are written in the Latin script, and have had more western European influence due to their speakers being historically Roman Catholic, whereas the East Slavic languages are written in Cyrillic and, with Eastern Orthodox or Uniate faithful, have had more Greek influence. East Slavic languages such as Russian have, however, during and after Peter the Great's Europeanization campaign, absorbed many words of Latin, French, German, and Italian origin, somewhat reducing this difference in influence. Although the South Slavic group has traits that distinguish it from the West or East Slavic branches, within itself it displays much the same variations: Bulgarian, for example, has some East Slavic traits (Cyrillic alphabet, Russian loanwords, and Greek influence) and Croatian many West Slavic ones (Latin alphabet, overall central European influence like Czech), despite both being South Slavic.
The tripartite division of the Slavic languages does not take into account the spoken dialects of each language. Of these, certain so-called transitional dialects and hybrid dialects often bridge the gaps between different languages, showing similarities that do not stand out when comparing Slavic literary (i.e. standard) languages. For example, Slovak (West Slavic) and Ukrainian (East Slavic) are bridged by the Rusyn of Eastern Slovakia and western Ukraine. Similarly, Polish shares transitional features with both western Ukrainian and Belarusian dialects. The Croatian Kajkavian dialect is more similar to Slovene than to the standard Croatian language.
Although the Slavic languages diverged from a common proto-language later than any other group of the Indo-European language family, enough differences exist between the various Slavic dialects and languages to make communication between speakers of different Slavic languages difficult. Within the individual Slavic languages, dialects may vary to a lesser degree, as those of Russian, or to a much greater degree, as those of Slovene.
History.
Common roots and ancestry.
Slavic languages descend from Proto-Slavic, their immediate parent language, ultimately deriving from Proto-Indo-European, the ancestor language of all Indo-European languages, via a Proto-Balto-Slavic stage. During the Proto-Balto-Slavic period a number of exclusive isoglosses in phonology, morphology, lexis, and syntax developed, which makes Slavic and Baltic the closest related of all the Indo-European branches. The secession of the Balto-Slavic dialect ancestral to Proto-Slavic is estimated on archaeological and glottochronological criteria to have occurred sometime in the period 1500–1000 BCE.
A minority of Baltists maintain the view that the Slavic group of languages differs so radically from the neighboring Baltic group (Lithuanian, Latvian, and the now-extinct Old Prussian), that they could not have shared a parent language after the breakup of the Proto-Indo-European continuum about five millennia ago. Substantial advances in Balto-Slavic accentology that occurred in the last three decades, however, make this view very hard to maintain nowadays, especially when one considers that there was most likely no "Proto-Baltic" language and that West Baltic and East Baltic differ from each other as much as each of them does from Proto-Slavic.
Evolution.
The imposition of Church Slavonic on Orthodox Slavs was often at the expense of the vernacular. Says WB Lockwood, a prominent Indo-European linguist, "It [O.C.S] remained in use to modern times but was more and more influenced by the living, evolving languages, so that one distinguishes Bulgarian, Serbian, and Russian varieties. The use of such media hampered the development of the local languages for literary purposes, and when they do appear the first attempts are usually in an artificially mixed style." (148)
Lockwood also notes that these languages have "enriched" themselves by drawing on Church Slavonic for the vocabulary of abstract concepts. The situation in the Catholic countries, where Latin was more important, was different. The Polish Renaissance poet Jan Kochanowski and the Croatian Baroque writers of the 16th century all wrote in their respective vernaculars (though Polish itself had drawn amply on Latin in the same way Russian would eventually draw on Church Slavonic).
Although Church Slavonic hampered vernacular literatures, it fostered Slavonic literary activity and abetted linguistic independence from external influences. Only the Croatian vernacular literary tradition nearly matches Church Slavonic in age. It began with the Vinodol Codex and continued through the Renaissance until the codifications of Croatian in 1830, though much of the literature between 1300 and 1500 was written in much the same mixture of the vernacular and Church Slavonic as prevailed in Russia and elsewhere.
The most important early monument of Croatian literacy is the Baška tablet from the late 11th century. It is a large stone tablet found in the small Church of St. Lucy, Jurandvor on the Croatian island of Krk, containing text written mostly in Čakavian dialect in angular Croatian Glagolitic script. The independence of Dubrovnik facilitated the continuity of the tradition.
More recent foreign influences follow the same general pattern in Slavic languages as elsewhere and are governed by the political relationships of the Slavs. In the 17th century, bourgeois Russian ("delovoi jazyk") absorbed German words through direct contacts between Russians and communities of German settlers in Russia. In the era of Peter the Great, close contacts with France invited countless loan words and calques from French, a significant fraction of which not only survived but also replaced older Slavonic loans. In the 19th century, Russian influenced most literary Slavic languages by one means or another.
Differentiation.
The Proto-Slavic language existed until around 500 AD. By the 7th century, it had broken apart into large dialectal zones.
There are no reliable hypotheses about the nature of the subsequent breakups of West and South Slavic. East Slavic is generally thought to converge to one Old Russian or Old East Slavonic language, which existed until at least the 12th century.
Linguistic differentiation was accelerated by the dispersion of the Slavic peoples over a large territory, which in Central Europe exceeded the current extent of Slavic-speaking majorities. Written documents of the 9th, 10th, and 11th centuries already display some local linguistic features. For example the Freising manuscripts show a language that contains some phonetic and lexical elements peculiar to Slovene dialects (e.g. rhotacism, the word "krilatec"). The Freising manuscripts are the first Latin-script continuous text in a Slavic language.
The migration of Slavic speakers into the Balkans in the declining centuries of the Byzantine Empire expanded the area of Slavic speech, but the pre-existing writing (notably Greek) survived in this area. The arrival of the Hungarians in Pannonia in the 9th century interposed non-Slavic speakers between South and West Slavs. Frankish conquests completed the geographical separation between these two groups, also severing the connection between Slavs in Moravia and Lower Austria (Moravians) and those in present-day Styria, Carinthia, East Tyrol in Austria, and in the provinces of modern Slovenia, where the ancestors of the Slovenes settled during first colonisation.
Linguistic history.
The following is a summary of the main changes from Proto-Indo-European (PIE) leading up to the Common Slavic (CS) period immediately following the Proto-Slavic language (PS).
Common features.
The Slavic languages are a relatively homogeneous family, compared with other families of Indo-European languages (e.g. Germanic, Romance, and Indo-Iranian). As late as the 10th century AD, the entire Slavic-speaking area still functioned as a single, dialectally differentiated language, termed "Common Slavic". Compared with most other Indo-European languages, the Slavic languages are quite conservative, particularly in terms of morphology (the means of inflecting nouns and verbs to indicate grammatical differences). Most Slavic languages have a rich, fusional morphology that conserves much of the inflectional morphology of Proto-Indo-European.
Consonants.
The following table shows the inventory of consonants of Late Common Slavic:
1The sound /sʲ/ did not occur in West Slavic, where it had developed to /ʃ/.
This inventory of sounds is quite similar to what is found in most modern Slavic languages. The extensive series of palatal consonants, along with the affricates *ts and *dz, developed through a series of palatalizations that happened during the Proto-Slavic period, from earlier sequences either of velar consonants followed by front vowels (e.g. *ke, *ki, *ge, *gi, *xe, and *xi), or of various consonants followed by *j (e.g. *tj, *dj, *sj, *zj, *rj, *lj, *kj, and *gj, where *j indicates the sound of the English consonant "y" as in "yes" or "you").
The biggest change in this inventory results from a further general palatalization occurring near the end of the Common Slavic period, where "all" consonants became palatalized before front vowels. This produced a large number of new palatalized (or "soft") sounds, which formed pairs with the corresponding non-palatalized (or "hard") consonants and absorbed the existing palatalized sounds *lʲ *rʲ *nʲ *sʲ. These sounds were best preserved in Russian but were lost to varying degrees in other languages (particularly Czech and Slovak). The following table shows the inventory of modern Russian:
This general process of palatalization did not occur in Serbo-Croatian and Slovenian. As a result, the modern consonant inventory of these languages is nearly identical to the Late Common Slavic inventory.
Late Common Slavic tolerated relatively few consonant clusters. However, as a result of the loss of certain formerly present vowels (the weak yers), the modern Slavic languages allow quite complex clusters, as in the Russian word взблеск ] ("flash"). Also present in many Slavic languages are clusters rarely found cross-linguistically, as in Russian ртуть ] ("mercury") or Polish mchu ] ("moss", gen. sg.).
Vowels.
A typical vowel inventory is as follows:
The sound [ɨ] occurs only in some languages (Russian, Belarusian, Polish), and even in these languages, it is unclear whether it is its own phoneme or an allophone of /i/. Nonetheless, it is a quite prominent and noticeable characteristic of the languages in which it is present.
Common Slavic also had two nasal vowels: *ę [ẽ] and *ǫ [õ]. However, these are preserved only in modern Polish (along with a few lesser-known dialects and microlanguages).
Other phonemic vowels are found in certain languages (e.g. the schwa /ǝ/ in Bulgarian and Slovenian, distinct high-mid and low-mid vowels in Slovenian, and the lax front vowel /ɪ/ in Ukrainian).
Length, accent, and tone.
An area of great difference among Slavic languages is that of prosody (i.e. syllabic distinctions such as vowel length, accent, and tone). Common Slavic had a complex system of prosody, inherited with little change from Proto-Indo-European. This consisted of phonemic vowel length and a free, mobile pitch accent:
The modern languages vary greatly in the extent to which they preserve this system. On one extreme, Serbo-Croatian preserves the system nearly unchanged (even more so in the conservative Chakavian dialect); on the other, Macedonian has basically lost the system in its entirety. Between them are found numerous variations:
Grammar.
Similarly, Slavic languages have extensive morphophonemic alternations in their derivational and inflectional morphology, including between velar and postalveolar consonants, front and back vowels, and between a vowel and no vowel.
Selected cognates.
The following is a very brief selection of cognates in basic vocabulary across the Slavic language family, which may serve to give an idea of the sound changes involved. This is not a list of translations: cognates have a common origin, but their meaning may be shifted and loanwords may have replaced them.
Influence on neighboring languages.
Most languages of the former Soviet Union and of some neighbouring countries (for example, Mongolian) are significantly influenced by Russian, especially in vocabulary. In the south, the Romanian, Albanian, and Hungarian languages show the influence of the neighboring Slavic nations, especially in vocabulary pertaining to urban life, agriculture, and crafts and trade—the major cultural innovations at times of limited long-range cultural contact. In each one of these languages, Slavic lexical borrowings represent at least 20% of the total vocabulary. However, Romanian has much lower influence from Slavic than Albanian or Hungarian. This is because Slavic tribes crossed and partially settled the territories inhabited by ancient Illyrians and Vlachs on their way to the Balkans.
Although also spoken in neighbouring lands, the Germanic languages show less significant Slavic influence, partly because Slavic migrations were mostly headed south rather than west. Slavic tribes did push westwards into Germanic territory, but borrowing for the most part seems to have been from Germanic to Slavic rather than the other way: for instance, the now-extinct Polabian language was heavily influenced by German, far more than any living Slavic language today. The Slavic contributions to Germanic languages remains a moot question, though Max Vasmer, a specialist in Slavic etymology, has claimed that there were no Slavic loans into Proto-Germanic. The only Germanic languages that shows significant Slavic influence are Yiddish and the historical colonial dialects of German that were spoken East of the Oder–Neisse line, such as Silesian German (formerly spoken in Silesia and South of East Prussia) and the Eastern varieties of East Low German, with the exception of Low Prussian, which had a strong Baltic substratum. Modern Dutch slang, especially the Amsterdam dialect, borrowed much from Yiddish in turn. However, there are isolated Slavic loans (mostly recent) into other Germanic languages. For example, the word for "border" (in modern German "Grenze", Dutch "grens") was borrowed from the Common Slavic "granica". English derives "quark" (a kind of cheese, not the subatomic particle) from the German "Quark", which in turn is derived from the Slavic "tvarog", which means "curd". Many German surnames, particularly in Eastern Germany and Austria, are Slavic in origin. Swedish also has "torg" (market place) from Old Russian "tъrgъ" or Polish "targ", "tolk" (interpreter) from Old Slavic "tlŭkŭ", and "pråm" (barge) from West Slavonic "pramŭ".
The Czech word is now found in most languages worldwide, and the word , probably also from Czech, is found in many Indo-European languages, including Greek (, pistóli).
A well-known Slavic word in almost all European languages is vodka, a borrowing from Russian "водка" ("vodka") – which itself was borrowed from Polish "wódka" (lit. "little water"), from common Slavic "voda" ("water", cognate to the English word) with the diminutive ending "-ka". Owing to the medieval fur trade with Northern Russia, Pan-European loans from Russian include such familiar words as "sable". The English word "vampire" was borrowed (perhaps via French "vampire") from German "Vampir", in turn derived from Serbian "vampir", continuing Proto-Slavic "*ǫpyrь", although Polish scholar K. Stachowski has argued that the origin of the word is early Slavic "*vąpěrь", going back to Turkic "oobyr". Several European languages, including English, have borrowed the word "polje" (meaning "large, flat plain") directly from the former Yugoslav languages (i.e. Slovene, Croatian, and Serbian). During the heyday of the USSR in the 20th century, many more Russian words became known worldwide: "da", "Soviet", "sputnik", "perestroika", "glasnost", "kolkhoz", etc. Also in the English language borrowed from Russian is "samovar" (lit. "self-boiling") to refer to the specific Russian tea urn.
Detailed list.
The following tree for the Slavic languages derives from the Ethnologue report for Slavic languages. It includes the ISO 639-1 and ISO 639-3 codes where available.
East Slavic languages:
West Slavic languages:
South Slavic languages:
Para- and supranational languages
References.
</dl>
External links.
</dl>

</doc>
<doc id="26758" url="http://en.wikipedia.org/wiki?curid=26758" title="SGI">
SGI

SGI may refer to:

</doc>
<doc id="26764" url="http://en.wikipedia.org/wiki?curid=26764" title="International System of Units">
International System of Units

The International System of Units (French: "Système International d'Unités", SI) is the modern form of the metric system and is the world's most widely used system of measurement, used in both commerce and science. It comprises a coherent system of units of measurement built on seven base units. It defines twenty-two named units, and includes many more unnamed coherent derived units. The system also establishes a set of twenty prefixes to the unit names and unit symbols that may be used when specifying multiples and fractions of the units. 
The system was published in 1960 as the result of an initiative that started in 1948. It is based on the metre-kilogram-second system of units (MKS) rather than any variant of the centimetre–gram–second system (CGS). SI is intended to be an evolving system, so prefixes and units are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves. The 25th General Conference on Weights and Measures (CGPM) in 2014, for example, discussed a proposal to change the definition of the kilogram.
The motivation for the development of the SI was the diversity of units that had sprung up within the CGS systems and the lack of coordination between the various disciplines that used them. The CGPM, which was established by the Metre Convention of 1875, brought together many international organizations to not only agree on the definitions and standards of the new system but also agree rules on writing and presenting measurements in a standardised manner around the world.
The International System of Units has been adopted by most developed countries, however, the adoption has not been universal in all English-speaking countries. While metrication in the United States is consistent in science, medicine, government, and various fields of technology and engineering, common measurements are mostly performed in customary units, although these have officially been defined in terms of SI units. The United Kingdom has officially adopted a policy of partial metrication, with no intention of replacing imperial units entirely. Canada has adopted the SI for most governmental, medical and scientific purposes and for such varied uses as grocery weights, weather reports, traffic signs and gasoline sales, but imperial units are still legally permitted and remain in common use throughout many sectors of Canadian society, particularly in the building trade and the railway sector.
History.
The metric system was first implemented during the French Revolution (1790s) with just the metre and kilogram as standards of length and mass respectively. In the 1830s Carl Friedrich Gauss laid the foundations for a coherent system based on length, mass and time. In the 1860s a group working under the auspices of the British Association for the Advancement of Science formulated the requirement for a coherent system of units with base units and derived units. The inclusion of electrical units into the system was hampered by the customary use of more than one set of units, until 1900 when Giovanni Giorgi identified the need to define one single electrical quantity as a fourth base quantity alongside the original three base quantities.
Meanwhile, in 1875, the Treaty of the Metre passed responsibility for verification of the kilogram and metre against agreed prototypes from French to international control. In 1921 the Treaty was extended to include all physical quantities including electrical units originally defined in 1893.
In 1948 an overhaul of the metric system was set in motion which resulted in the development of the "Practical system of units" which, on its publication in 1960, was given the name "The International System of Units". In 1954 the 10th General Conference on Weights and Measures (CGPM) identified electric current as the fourth base quantity in the practical system of units and added two more base quantities—temperature and luminous intensity—making six base quantities in all. The units associated with these quantities were the metre, kilogram, second, ampere, kelvin and candela. In 1971 a seventh base quantity, amount of substance represented by the mole, was added to the definition of SI.
Early development.
The metric system was developed from 1791 onwards by a committee of the French Academy of Sciences, commissioned by the National Assembly and Louis XVI to create a unified and rational system of measures. The group, which included Antoine Lavoisier (the "father of modern chemistry") and the mathematicians Pierre-Simon Laplace and Adrien-Marie Legendre,:89 used the same principles for relating length, volume and mass that had been proposed by the English clergyman John Wilkins in 1668 and the concept of using the Earth's meridian as the basis of the definition of length, originally proposed in 1670 by the French abbot Mouton.
On 30 March 1791, the Assembly adopted the committee's proposed principles for the new decimal system of measure and authorized a survey between Dunkirk and Barcelona to establish the length of the meridian. On 11 July 1792, the committee proposed the names "metre", "are", "litre" and "grave" for the units of length, area, capacity and mass, respectively. The committee also proposed that multiples and submultiples of these units were to be denoted by decimal-based prefixes such as "centi" for a hundredth and "kilo" for a thousand.:82
The law of 7 April 1795 ("Loi du ") defined the terms "gramme" and "kilogramme", which replaced the former terms "gravet" (correctly "milligrave") and "grave", and on 22 June 1799 (after Pierre Méchain and Jean-Baptiste Delambre had completed the meridian survey) the definitive standard "mètre des Archives" and "kilogramme des Archives" were deposited in the "Archives nationales". On 10 December 1799 (a month after Napoleon's coup d'état), the law by which the metric system was to be definitively adopted in France ("Loi du ") 
was passed.
During the first half of the nineteenth century there was little consistency in the choice of preferred multiples of the base units – typically the myriametre ( metres) was in widespread use in both France and parts of Germany, while the kilogram (1000 grams) rather than the myriagram was used for mass.
In 1832, the German mathematician Carl Friedrich Gauss, assisted by Wilhelm Weber, implicitly defined the second as a base unit when he quoted the earth's magnetic field in terms of millimetres, grams, and seconds. Prior to this, the strength of the earth’s magnetic field had only been described in relative terms. The technique used by Gauss was to equate the torque induced on a suspended magnet of known mass by the earth’s magnetic field with the torque induced on an equivalent system under gravity. The resultant calculations enabled him to assign dimensions based on mass, length and time to the magnetic field.
In the 1860s James Clerk Maxwell, William Thomson (later Lord Kelvin) and others working under the auspices of the British Association for the Advancement of Science, built on Gauss' work and formalised the concept of a coherent system of units with base units and derived units. The principle of coherence was successfully used to define a number of units of measure based on the centimetre–gram–second (CGS) system of units (CGS), including the erg for energy, the dyne for force, the barye for pressure, the poise for dynamic viscosity and the stokes for kinematic viscosity.
Metre Convention.
A French-inspired initiative for international cooperation in metrology led to the signing in 1875 of the Metre Convention.:353–354 Initially the convention only covered standards for the metre and the kilogram. A set of 30 prototypes of the metre and 40 prototypes of the kilogram, in each case made of a 90% platinum-10% iridium alloy, were manufactured by the British firm Johnson, Matthey & Co and accepted by the CGPM in 1889. One of each was selected at random to become the International prototype metre and International prototype kilogram that replaced the "mètre des Archives" and "kilogramme des Archives" respectively. Each member state was entitled to one of each of the remaining prototypes to serve as the national prototype for that country.
The treaty established three international organisations to oversee the keeping of international standards of measurement:
In 1921 the Metre Convention was extended to include all physical units, including the ampere and others defined by the Fourth International Conference of Electricians in Chicago in 1893, thereby enabling the CGPM to address inconsistencies in the way that the metric system had been used.:96
The official language of the Metre Convention is French and the definitive version of all official documents published by or on behalf of the CGPM is the French-language version.:94
Towards the SI.
At the close of the 19th century three different systems of units of measure existed for electrical measurements: a CGS-based system for electrostatic units, also known as the Gaussian or ESU system, a CGS-based system for electromechanical units (EMU) and an MKS-based system ("international system") for electrical distribution systems. 
Attempts to resolve the electrical units in terms of length, mass and time using dimensional analysis was beset with difficulties—the dimensions depended on whether one used the ESU or EMU systems. This anomaly was resolved in 1900 when Giovanni Giorgi published a paper in which he advocated using a fourth base unit alongside the existing three base units. The fourth unit could be chosen to be electric current , voltage, or electrical resistance.
In the late 19th and early 20th centuries a number of non-coherent units of measure based on the gram/kilogram, the centimetre/metre and the second, such as the "Pferdestärke" (metric horsepower) for power, the darcy for permeability and the use of "millimetres of mercury" for the measurement of both barometric and blood pressure were developed or propagated, some of which incorporated standard gravity in their definitions.
At the end of the Second World War, a number of different systems of measurement were in use throughout the world. Some of these systems were metric system variations, whereas others were based on customary systems of measure. In 1948, after representations by the International Union of Pure and Applied Physics (IUPAP) and by the French Government, the 9th General Conference on Weights and Measures (CGPM) asked the International Committee for Weights and Measures (CIPM) to conduct an international study of the measurement needs of the scientific, technical, and educational communities and "to make recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention".
On the basis of the findings of this study, the 10th CGPM in 1954 decided that an international system should be derived from six base units to provide for the measurement of temperature and optical radiation in addition to mechanical and electromagnetic quantities. Six base units were recommended: the metre, kilogram, second, ampere, degree Kelvin (later renamed kelvin), and candela. In 1960, the 11th CGPM named the system the "International System of Units", abbreviated SI from the French name, "Le Système International d'Unités".:110 The BIPM has also described SI as "the modern metric system".:95 The seventh base unit, the mole, was added in 1971 by the 14th CGPM.
International System of Quantities.
The International System of Quantities is a system based on seven base quantities: length, mass, time, electric current, thermodynamic temperature, amount of substance and luminous intensity. Other quantities such as area, pressure and electrical resistance are derived from these base quantities by clear non-contradictory equations. The International System of Units (SI) is based on the International System of Quantities (ISQ) in that the ISQ defines the quantities which are measured with the SI units. The ISQ is defined in the international standard ISO/IEC 80000.
SI Brochure and conversion factors.
The CGPM publishes a brochure which defines and presents SI. Its official version is in French, in line with the Metre Convention.:102 It leaves some scope for local interpretation, particularly regarding names and terms in different languages, so for example the United States' National Institute of Standards and Technology (NIST) has produced a version of the CGPM document (NIST SP 330) which clarifies local interpretation for English-language publications that use American English and another document (NIST SP 811) that gives general guidance for the use of SI in the United States and conversion factors between SI and customary units.
The writing and maintenance of the CGPM brochure is carried out by one of the committees of the International Committee for Weights and Measures (CIPM), the Consultative Committee for Units (CCU). The CIPM nominates the chairman of this committee, but the committee includes representatives of various other international bodies rather than CIPM or CGPM nominees. This committee thus provides a forum for the bodies concerned to provide input to the CIPM in respect of ongoing enhancements to SI.
The definitions of the terms "quantity", "unit", "dimension" etc. that are used in the "SI Brochure" are those given in the International vocabulary of metrology, a publication produced by the Joint Committee for Guides in Metrology (JCGM), a working group consisting of eight international standards organisations under the chairmanship of the director of the BIPM. The quantities and equations that define the SI units are now referred to as the "International System of Quantities" (ISQ), and are set out in the International Standard "ISO/IEC 80000 Quantities and Units".
Units and prefixes.
The International System of Units consists of a set of base units, a set of derived units with special names, and a set of decimal-based multipliers that are used as prefixes. The term "SI Units" covers all three categories, but the term "coherent SI units" includes only base units and coherent derived units.:103–106
Base units.
The SI base units are the building blocks of the system and all other units are derived from them. When Maxwell first introduced the concept of a coherent system, he identified three quantities that could be used as base units: mass, length and time. Giorgi later identified the need for an electrical base unit. Theoretically any one of electrical current, potential difference, electrical resistance, electrical charge or a number of other quantities could have provided the base unit, with the remaining units then being defined by the laws of physics. In the event, the unit of electric current was chosen for SI. Another three base units (for temperature, substance and luminous intensity) were added later.
Derived units.
The derived units in the SI are formed by powers, products or quotients of the base units and are unlimited in number.:103:3 Derived units are associated with derived quantities, for example velocity is a quantity that is derived from the base quantities of time and length, so in SI the derived unit is metres per second (symbol m/s). The dimensions of derived units can be expressed in terms of the dimensions of the base units.
Coherent units are derived units that contain no numerical factor other than 1—quantities such as standard gravity and density of water are absent from their definitions. In the example above, "one" newton is the force required to accelerate a mass of "one" kilogram by "one" metre per second squared. Since the SI units of mass and acceleration are kg and m⋅s−2 respectively and "F" ∝ "m" × "a", the units of force (and hence of newtons) is formed by multiplication to give kg⋅m⋅s−2. Since the newton is part of a coherent set of units, the constant of proportionality is 1.
For the sake of convenience, some derived units have special names and symbols. Such units may themselves be used in combination with the names and symbols for base units and for other derived units to express the units of other derived quantities. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa)—and the pascal can be defined as "newtons per square metre" (N/m2).
Prefixes.
Prefixes are added to unit names to produce multiple and sub-multiples of the original unit. All multiples are integer powers of ten, and above a hundred or below a hundredth all are integer powers of a thousand. For example, "kilo-" denotes a multiple of a thousand and "milli-" denotes a multiple of a thousandth, so there are one thousand millimetres to the metre and one thousand metres to the kilometre. The prefixes are never combined, so for example a millionth of a metre is a "micrometre", not a millimillimetre. Multiples of the kilogram are named as if the gram were the base unit, so a millionth of a kilogram is a "milligram", not a microkilogram.:122:14
Non-SI units accepted for use with SI.
Although, in theory, SI can be used for any physical measurement, the CIPM has recognized that some non-SI units still appear in the scientific, technical, and commercial literature, and will continue to be used for many years to come. In addition, certain other units are so deeply embedded in the history and culture of the human race that they will continue to be used for the foreseeable future. The CIPM has catalogued several such units and published them in the SI Brochure so that their use may be consistent around the world. These units have been grouped as follows::123–129:7–11 
Writing unit symbols and the values of quantities.
Before 1948, the writing of metric quantities was haphazard. In 1879, the CIPM published recommendations for writing the symbols for length, area, volume and mass, but it was outside its domain to publish recommendations for other quantities. Beginning in about 1900, physicists who had been using the symbol "μ" for "micrometre" (or "micron"), "λ" for "microlitre", and "γ" for "microgram" started to use the symbols "μm", "μL" and "μg", but it was only in 1935, a decade after the revision of the Metre Convention that the CIPM formally adopted this proposal and recommended that the symbol "μ" be used universally as a prefix for .
In 1948, the ninth CGPM approved the first formal recommendation for the writing of symbols in the metric system when the basis of the rules as they are now known was laid down. These rules were subsequently extended by International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) and now cover unit symbols and names, prefix symbols and names, how quantity symbols should be written and used and how the values of quantities should be expressed.:104,130 Both ISO and the IEC have published rules for the presentation of SI units that are generally compatible with those published in the SI Brochure. s of 2013[ [update]] ISO and IEC were in the process of merging their standards for quantities and units into a single set of compatible documents identified as the ISO/IEC 80000 Standard. The rules covering printing of quantities and units are part of ISO 80000-1:2009.
Unit names.
Names of units follow the grammatical rules associated with common nouns: in English and in French they start with a lowercase letter (e.g., newton, hertz, pascal), even when the symbol for the unit begins with a capital letter. This also applies to "degrees Celsius", since "degree" is the unit. In German, however, the names of units, as with all German nouns, start with capital letters. The spelling of unit names is a matter for the guardians of the language concerned – the official British and American spellings for certain SI units differ – British English, as well as Australian, Canadian and New Zealand English, uses the spelling "deca-", "metre", and "litre" whereas American English uses the spelling "deka-", "meter", and "liter", respectively.
Likewise, the plural forms of units follow the grammar of the language concerned: in English, the normal rules of English grammar are used, e.g. "henries" is the plural of "henry".:31 However, the units lux, hertz, and siemens have irregular plurals in that they remain the same in both their singular and plural form.
In English, when unit names are combined to denote multiplication of the units concerned, they are separated with a hyphen or a space (e.g. newton-metre or newton metre). The plural is formed by converting the last unit name to the plural form (e.g. ten newton-metres).
Unit names as adjectives.
In English, a space is recommended between the number and the unit symbol when used as an adjective, e.g. "a 25 kg sphere".
The normal rules of English apply to unit names, where a hyphen is incorporated into the adjectival sense, e.g. "a 25-kilogram sphere".
Chinese and Japanese.
Chinese uses traditional logograms for writing the unit names, while in Japanese unit names are written in the phonetic katakana script; in both cases symbols are written using the internationally recognised Latin and Greek characters.
A set of characters representing various metric units was created in Japan in the late 19th century. Characters exist for three base units: the metre (米), litre (升) and gram (克). These were combined with a set of six prefix characters – "kilo-" (千), "hecto-" (百), "deca-" (十), "deci-" (分), "centi-" (厘) and "milli-" (毛) – to form an additional 18 single-character units. The seven length units (kilometre to millimetre), for example, are 粁, 粨, 籵, 米, 粉, 糎 and 粍. These characters, however, are not in common use today; instead, units are written out in katakana, the Japanese syllabary used for foreign borrowings, such as "キロメートル" ("kiromētoru") for "kilometre". A few Sino-Japanese words for these units remain in use in Japanese, most significantly "平米" ("heibei") for "square metre", but otherwise borrowed pronunciations are used.
These characters are examples of the rare phenomenon of single-character loan words – a foreign word represented by a single Japanese character – and form the plurality of such words. Similar characters were also coined for other units, such as British units, though these also have fallen out of use; see Single character gairaigo: Metric units and Single character gairaigo: Other units for a full list.
The basic units are metre (米 "mǐ"), litre (升 "shēng"), gram (克 "kè"), and second (秒 "miǎo"), while others include watt (瓦 "wǎ"). Prefixes include "deci-" (分 "fēn"), "centi-" (厘 "lí"), "milli-" (毫 "háo"), "micro-" (微 "wēi"), "kilo-" (千 "qiān"), and "mega-" (兆 "zhào"). These are combined to form disyllabic characters, such as 厘米 "límǐ" "centimetre" or 千瓦 "qiānwǎ" "kilowatt". In the 19th century various compound characters were also used, similar to Japanese, either imported or formed on the same principles, such as 瓩 for 千瓦 "qiānwǎ" (kilowatt) or 糎 for 厘米. These are generally not used today – for example centimetres is usually written 厘米 "límǐ" – but are occasionally found in older or technical writing.
Unit symbols and the values of quantities.
Although the writing of unit names is language-specific, the writing of unit symbols and the values of quantities is consistent across all languages and therefore the SI Brochure has specific rules in respect of writing them.:130–135 The guideline produced by the National Institute of Standards and Technology (NIST) clarifies language-specific areas in respect of American English that were left open by the SI Brochure, but is otherwise identical to the SI Brochure.
General rules.
General rules for writing SI units and quantities apply to text that is either handwritten or produced using an automated process:
Printing SI symbols.
Further rules are specified in respect of production of text using printing presses, word processors, typewriters and the like.
Realisation of units.
Metrologists carefully distinguish between the definition of a unit and its realisation. The definition of each base unit of the SI is drawn up so that it is unique and provides a sound theoretical basis on which the most accurate and reproducible measurements can be made. The realisation of the definition of a unit is the procedure by which the definition may be used to establish the value and associated uncertainty of a quantity of the same kind as the unit. A description of the "mise en pratique" of the base units is given in an electronic appendix to the SI Brochure.:168–169
The published "mise en pratique" is not the only way in which a base unit can be determined: the SI Brochure states that "any method consistent with the laws of physics could be used to realise any SI unit.":111 In the current (2012) exercise to overhaul the definitions of the base units, various consultative committees of the CIPM have required that more than one "mise en pratique" shall be developed for determining the value of each unit. In particular:
Post-1960 changes.
The preamble to the Metre Convention read "Desiring the international uniformity and precision in standards of weight and measure, have resolved to conclude a convention ...". Changing technology has led to an evolution of the definitions and standards that has followed two principal strands – changes to SI itself and clarification of how to use units of measure that are not part of SI, but are still nevertheless used on a worldwide basis.
Changes to the SI.
Since 1960 the CGPM has made a number of changes to SI. These include:
In addition, advantage was taken of developments in technology to redefine many of the base units enabling the use of higher precision techniques.
Retention of non-SI units.
Although, in theory, SI can be used for any physical measurement, it is recognised that some non-SI units still appear in the scientific, technical and commercial literature, and will continue to be used for many years to come. In addition, certain other units are so deeply embedded in the history and culture of the human race that they will continue to be used for the foreseeable future. The CIPM has catalogued such units and included them in the SI Brochure so that they can be used consistently.
The first such group comprises the units of time and of angles and certain legacy non-SI metric units. Most of mankind has used the day and its subdivisions as a basis of time with the result that the second, minute, hour and day, unlike the foot or the pound, were the same regardless of where it was being measured. The second has been catalogued as an SI unit, its multiples as units of measure that may be used alongside the SI. The measurement of angles has likewise had a long history of consistent use – the radian, being 1/2π of a revolution, has mathematical niceties, but it is cumbersome for navigation, hence the retention of the degree, minute and second of arc. The tonne, litre and hectare were adopted by the CGPM in 1879 and have been retained as units that may be used alongside SI units, having been given unique symbols.
Physicists often use units of measure that are based on natural phenomena such as the speed of light, the mass of a proton (approximately one dalton), the charge of an electron and the like. These too have been catalogued in the SI Brochure with consistent symbols, but with the caveat that their physical values need to be measured.
In the interests of standardising health-related units of measure used in the nuclear industry, the 12th CGPM (1964) accepted the continued use of the curie (symbol Ci) as a non-SI unit of activity for radionuclides;:152 the becquerel, sievert and gray were adopted in later years. Similarly, the millimetre of mercury (symbol mmHg) was retained for measuring blood pressure.:127
Global adoption.
SI has become the world's most widely used system of measurement, used in both everyday commerce and science. The change to SI had little effect on everyday life in countries that used the metric system – the metre, kilogram, litre and second remained unchanged as did the way in which they were used – most of the changes only affected measurements in the workplace. The CGPM has a role of recommending changes, but no formal role in the enforcement of such changes—another inter-governmental organisation, the International Organization of Legal Metrology (OIML) provides a forum for harmonisation of national standards and legislation in respect of metrology.
Both the degree and rate of adoption of SI varied from country to country—countries that had not adopted the metric system by 1960 and subsequently adopted SI did so directly as part of their metrication programs while others migrated from the CGS system of units to SI. In 1960, the world's largest economy was that of the United States, followed by the United Kingdom, West Germany, France, Japan, China and India. The United States and the United Kingdom were non-metric, France and Germany had been using the metric system for about a century, and China had been using the metric system for 35 years, while India and Japan had adopted the metric system within the preceding five years. Other non-metric countries were those where the United Kingdom or the United States had considerable influence. These differences are brought out in the examples below:
United Kingdom and the former British Empire.
Even though the use of metric units was legalised for trade in the UK in 1864, the UK had signed the Metre Convention in 1884 and the UK Parliament had defined the yard and the pound in terms of the metre and the kilogram in 1897, the UK continued to use the imperial system of measure 
and to export the imperial system of units to the Empire. In 1932, the system of Imperial Preference was set up at the Ottawa Conference. Although Ireland left the Commonwealth in 1948 and South Africa in 1961, both continued their close economic ties with the Commonwealth.
When the SI standard was published in 1960, the only major Commonwealth country to have adopted the metric system was India. In 1863, the first reading of a bill that would have made the metric system compulsory passed its first reading in the House of Commons by 110 votes to 75. The bill, however, failed to make the statute book because of lack of parliamentary time.:136 In 1965, after this and similar false starts the then Federation of British Industry informed the British Government that its members favoured the adoption of the metric system. The rationale behind the request was that 80% of British exports were to countries that used the metric system or that were considering changing to the metric system. The Board of Trade, on behalf of the Government, agreed to support a ten-year metrication programme. The government agreed to a voluntary policy requiring minimal legislation and costs to be borne where they fell. SI would be used from the outset. The rest of the Commonwealth, South Africa and Ireland followed within a few years; in some countries such as South Africa and Australia metrication was mandatory rather than voluntary.
By 1980 all apart from the United Kingdom, Canada and Ireland had effectively completed their programs. In the United Kingdom the breakdown of voluntary metrication in the mid-1970s:§1.8 coincided with the United Kingdom's obligations as part of the EEC to adopt the metric system, resulting in legislation to force metrication in certain areas and the Eurosceptic movement adopting an anti-metrication stance and the United Kingdom seeking a number of derogations from the relevant EEC directives. Once the metrication of most consumer goods was completed in 2000, aspects of British life, especially in government, commerce and industry used SI.:§1.6 & §1.10 Although SI or units approved for use alongside SI are used in most areas where units of measure are regulated imperial units are widely encountered in unregulated areas such as the press and everyday speech. Canada has adopted it for most purposes, but imperial units are still legally permitted and remain in common use throughout a few sectors of Canadian society, particularly in the buildings, trades and railways sectors. The situation in Ireland, apart from road signs which were metricated in the early 2000s, is similar to that in the United Kingdom.
United States.
Even though Congress set up a framework for the use of the metric system in the nineteenth century, the United States continues to use US customary units, based on English measure passed by parliament under the reign of Queen Anne in 1706, for most purposes apart from science and medicine, though as a result of their Spanish heritage, metric units are used widely in Puerto Rico.
On 10 February 1964, the National Bureau of Standards (now the National Institute of Standards and Technology) issued a statement that it was to use SI except where this would have an obvious detrimental effect. In 1968 Congress authorised the U.S. Metric Study the emphasis of which was to examine the feasibility of adopting SI. The first volume was delivered in 1970. The study recommended that the United States adopt the International System of units, and in 1975 Congress passed the Metric Conversion Act of 1975 which established a national policy of coordinating and planning for the increased use of the metric measurement system in the United States. Metrication was voluntary and to be coordinated by the United States Metric Board (USMB).
Efforts during the Ford and Carter administrations to force metrication were seized on by many newspaper editorialists as being dictatorial.:365 Public response included resistance, apathy, and sometimes ridicule. The underlying reasons for this response include a relative uniformity of weights and measures (though, notably, US liquid measure differed by about 20% from British Imperial measure, which was adopted throughout the British Empire in 1824) inherited from the United Kingdom in 1776, a homogeneous economy and the influence of business groups and populists in Congress caused the country to look at the short-term costs associated with the change-over, particularly those that would be borne by the consumer rather than long-term benefits of efficiency and international trade. The Metrication Board was disbanded under President Ronald Reagan's direction in 1982.:362–365
The 1988 Omnibus Foreign Trade and Competitiveness Act removed international trade barriers and amended the Metric Conversion Act of 1975, designating the metric system as "the Preferred system of weights and measures for United States trade and commerce". The legislation stated that the federal government has a responsibility to assist industry, especially small business, as it voluntarily converts to the metric system of measurement. Exceptions were made for the highway and construction industries; the Department of Transportation planned to require metric units by 2000, but this plan was cancelled by the 1998 highway bill TEA21. However, the U.S. military uses the metric system widely, partly because of the need to work with armed services from other nations.
Although overall responsibility for labelling requirements of consumer goods lies with Congress and is therefore covered by federal law, details of labelling requirements for certain commodities are controlled by state law or by other authorities such as the Food and Drug Administration, Environmental Protection Agency and Alcohol and Tobacco Tax and Trade Bureau.
The federal Fair Packaging and Labeling Act (FPLA), originally passed in 1964, was amended in 1992 to require consumer goods directly under its jurisdiction to be labelled in both customary and metric units. Some industries are engaged in efforts to amend this law to allow manufacturers to use only metric labelling. The National Conference on Weights and Measures has developed the Uniform Packaging and Labeling Regulations (UPLR) which provides a standard approach to those sections of packaging law that are under state control. Acceptance of the UPLR varies from state to state – fourteen states accept it by merely citing it in their legislation. 
During the first decade of the 21st century, the EU directive 80/181/EEC had required that dual unit labelling of goods sold within the EU cease by the end of 2009. This was backed up by requests from other nations including Japan and New Zealand to permit metric-only labelling as an aid to trade with those countries. Opinion in the United States was split – a bill to permit metric-only labelling at the federal level was to have been introduced in 2005 but significant opposition from the Food Marketing Institute, representing U.S. grocers, has delayed the introduction of the bill. During a routine decennial review of the directive in 2008, the EU postponed the sunset clause for dual units indefinitely.
Meanwhile, in 1999 the UPLR was amended to permit metric-only labelling and automatically became law in those states that accept UPLR "as is". By 1 January 2009, 48 out of 50 states permit metric-only labelling, either through UPLR or through their own legislation. s of February 2013[ [update]] the use of metric (and therefore SI) units in the United States does not follow any pattern. Dual-unit labelling on consumer goods is mandatory. Some consumer goods such as soft drinks are sold in metric quantities, others such as milk are sold in customary units. The engineering industry is equally split. The automotive industry is largely metric, but aircraft such as the Boeing 787 Dreamliner were designed using customary units.
European Union.
In 1960, all the largest industrialised nations that had an established history of using the metric system were members of the European Economic Community (EEC).
In 1972, in order to harmonise units of measure as part of a programme to facilitate trade between member states, the EEC issued directive 71/354/EEC. This directive catalogued units of measure that could be used for "economic, public health, public safety and administrative purposes" and also provided instructions for a transition from the existing units of measure that were in use. The directive replicated the CGPM SI recommendations and in addition pre-empted some of the additions whose use had been recommended by the CIPM in 1969, but had not been ratified by the CGPM. The directive also catalogued units of measure whose status would be reviewed by the end of 1977 (mainly coherent CGS units of measure) and also catalogued units of measure that were to be phased out by the end of 1977, including the use of obsolete names for the sale of timber such as the stere, the use of units of force and pressure that made use of the acceleration due to gravity, the use of non-coherent units of power such as the Pferdestärke (PS), the use of the calorie as a measure of energy and the stilb as a measure of luminance. The directive was silent in respect of units that were specific to one or two countries including the "pond", "pfund", "livre" (Dutch, German and French synonyms for 500 g), thereby effectively prohibiting their use as well.
When the directive was revisited during 1977, some of the older units that were being reviewed (such as millimetre of mercury for blood pressure) were retained but others were phased out, thereby broadly aligning the allowable units with SI. The directive was however overhauled to accommodate British and Irish interests in retaining the imperial system in certain circumstances. It was reissued as directive 80/181/EEC. During subsequent revisions, the directive has reflected changes in the definition of SI. The directive also formalised the use of "supplementary units", which in 1979 were permitted for a period of ten years. The cut-off date for the use of supplementary units was extended a number of times and in 2009 was extended indefinitely.
India.
India was one of the last countries to start a metrication programme before the advent of SI. When it became independent in 1947, both imperial and native units of measure were in use. Its metrication programme started in 1956 with the passing of the Standards of Weights and Measures Act. Part of the act fixed the value of the seer (a legacy unit of mass) to 0.9331 kg exactly; elsewhere the Act declared that from 1960 all non-metric units of measure were to be illegal.
Four years after the Indian Government announced its metrication programme, SI was published. The result was that the initial metrication programme was a conversion to the CGS system of units and the subsequent adoption of SI has been haphazard. Fifty years later, many of the country's schoolbooks still use CGS or imperial units. Originally the Indian Government had planned to replace all units of measure with metric units by 1960. In 1976 a new Weights and Measures Act replaced the 1956 Act which, amongst other things, required that all weighing devices be approved before being released onto the market place. However, in 2012, it was reported that traditional units were still encountered in small manufacturing establishments and in the marketplace alongside CGS, SI and imperial measures, particularly in the poorer areas.
The use of the Indian numbering system of crores (10,000,000) and lakhs (100,000), which do not map onto the SI system of prefixes, is widespread and is often found alongside or in place of the western numbering system.
Redefinition of units.
After the metre was redefined in 1960, the kilogram remained the only SI base unit that relied on a specific physical artifact, the international prototype of the kilogram (IPK), for its definition and thus the only unit that was still subject to periodic comparisons of national standard kilograms with the IPK. After the 1996–1998 recalibration, a clear divergence between the various prototype kilograms was observed.
At its 23rd meeting, held in 2007, the CGPM recommended that the CIPM should continue to investigate methods to provide exact fixed values for physical constants of nature that could then be used in the definitions of units of measure in place of the IPK, thus enabling the transition from explicit unit definitions to explicit constant definitions.
At a meeting of the CCU held in Reading, United Kingdom, in September 2010, a resolution and draft changes to the SI Brochure that were to be presented to the next meeting of the CIPM in October 2010 were agreed to in principle. The proposals that the CCU put forward were:
The CIPM meeting of October 2010 reviewed progress towards establishing fixed values for the constants but found that "the conditions set by the General Conference at its 23rd meeting have not yet been fully met. For this reason the CIPM does not propose a revision of the SI at the present time".
At the 24th CGPM meeting, held in October 2011, the CIPM sponsored a resolution in which the requisite definition changes were agreed to in principle and in which the conditions required to be met before the redefinitions could be implemented were restated.
By November 2014 the conditions set out at the 23rd meeting of the CGPM for the unit redefinitions had still not been met, and the 25th meeting of the CGPM, held in November 2014, adopted a similar resolution encouraging further work towards establishing fixed values for the fundamental constants.
See also.
Organisations
Standards and conventions

</doc>
<doc id="26766" url="http://en.wikipedia.org/wiki?curid=26766" title="Sapiens">
Sapiens

Sapiens, a Latin word meaning wise, may refer to :

</doc>
<doc id="26768" url="http://en.wikipedia.org/wiki?curid=26768" title="Sirenia">
Sirenia

Sirenia (commonly referred to as sea cows) are an order of fully aquatic, herbivorous mammals that inhabit swamps, rivers, estuaries, marine wetlands, and coastal marine waters. Four species are living, in two families and genera. These are the dugong (one species) and manatees (three species). Sirenia also include Steller's sea cow, extinct since the 18th century, and a number of taxa known only from fossils. The order evolved during the Eocene, more than 50 million years ago.
Sirenia, commonly sirenians, are also referred to by the common name sirens, deriving from the sirens of Greek mythology. This comes from a legend about their discovery, involving lonely sailors mistaking them for mermaids.
"Sea cow" ("seekoei") is also the name for a hippopotamus in Afrikaans. In Germanic languages, the word "Sea" can mean either a body of fresh or salt water, so this follows from the species inhabiting lakes in southern Africa rather than the sea itself.
Description.
Sirenians have major aquatic adaptations: the forelimbs have modified into arms used for steering, the tail has modified into a paddle used for propulsion, and the hindlimbs (legs) are but two small remnant bones floating deep in the muscle. They appear fat, but are fusiform, hydrodynamic, and highly muscular. Their skulls are highly modified for taking breaths of air at the water's surface, and dentition is greatly reduced. The skeletal bones of both the manatees and dugong are very dense, which helps to neutralize the buoyancy of their blubber. The manatee appears to have an almost unlimited ability to produce new teeth as the anterior teeth wear down. They have only two teats, located under their forelimbs, similar to elephants. The elephants are thought to be the closest living relatives of the sirenians.
The lungs of sirenians are unlobed. In sirenians, the lungs and diaphragm extend the entire length of the vertebral column. These adaptations help sirenians control their buoyancy and maintain their horizontal position in the water.
Living sirenians grow between 2.5 and 4.0 meters long and can weigh up to 1,500 kg. "Hydrodamalis gigas", Steller's sea cow, could reach lengths of 8 m.
The three manatee species (family Trichechidae) and the dugong (family Dugongidae) are endangered species. All four are vulnerable to extinction from habitat loss and other negative impacts related to human population growth and coastal development. Steller's sea cow, extinct since 1786, was hunted to extinction by humans. Manatees and dugongs are the only marine mammals classified as herbivores. Unlike the other marine mammals (dolphins, whales, seals, sea lions, sea otters, and walruses), sirenians eat primarily sea grasses and other aquatic vegetation, and have an extremely low metabolism and poor tolerance for especially cold water. Sirenians have been observed eating dead animals (sea gulls), but their diets are made up primarily of vegetation. Like dolphins and whales, manatees and dugongs are completely aquatic mammals that never leave the water—not even to give birth. These animals have been observed eating grass clippings from homes adjacent to waterways, but in this rare occurrence, only the top portion of the sirenian is lifted out of the water. The combination of these factors means sirenians are restricted to warm, shallow, coastal waters, estuaries, and rivers with healthy ecosystems that support large amounts of seagrass or other vegetation.
The Trichechidae species differ from the Dugongidae in the shape of their skull and tails.
Classification.
The order Sirenia has been placed in the clade Paenungulata, within Afrotheria, grouping it with two other orders of living mammals: Proboscidea, the elephant families, and Hyracoidea, the hyraxes, and two extinct orders, Embrithopoda and Desmostylia.
Subdivision.
After Voss, 2014.
† extinct

</doc>
<doc id="26769" url="http://en.wikipedia.org/wiki?curid=26769" title="South America">
South America

South America is a continent located in the Western Hemisphere, mostly in the Southern Hemisphere, with a relatively small portion in the Northern Hemisphere. It can also be considered as a subcontinent of the Americas.
It is bordered on the west by the Pacific Ocean and on the north and east by the Atlantic Ocean; North America and the Caribbean Sea lie to the northwest. It includes twelve sovereign states – Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, Guyana, Paraguay, Peru, Suriname, Uruguay, and Venezuela – and two non-sovereign areas – French Guiana, an overseas department of France, and the Falkland Islands, a British Overseas Territory (though disputed by Argentina). In addition to this, the ABC islands of the Netherlands and Trinidad and Tobago may also be considered part of South America.
South America has an area of 17,840,000 square kilometers (6,890,000 sq mi). Its population as of 2005 has been estimated at more than 371,090,000. South America ranks fourth in area (after Asia, Africa, and North America) and fifth in population (after Asia, Africa, Europe, and North America).
Most of the population lives near the continent's western or eastern coasts while the interior and the far south are sparsely populated. The geography of western South America is dominated by the Andes mountains; in contrast, the eastern part contains both highland regions and large lowlands where rivers such as the Amazon, Paraná and Orinoco flow. Most of the continent lies in the tropics.
The continent's cultural and ethnic outlook has its origin with the interaction of indigenous peoples with European conquerors and immigrants and, more locally, with African slaves. Given a long history of colonialism, the overwhelming majority of South Americans speak Portuguese or Spanish, and societies and states commonly reflect Western traditions.
Geography.
South America occupies the southern portion of the Americas. The continent is generally delimited on the northwest by the Darién watershed along the Colombia–Panama border, although some may consider the border instead to be the Panama Canal. Geopolitically and geographically all of Panama – including the segment east of the Panama Canal in the isthmus – is typically included in North America alone and among the countries of Central America. Almost all of mainland South America sits on the South American Plate.
South America is home to the world's highest uninterrupted waterfall, Angel Falls in Venezuela; the highest single drop waterfall Kaieteur Falls in Guyana; the largest river (by volume), the Amazon River; the longest mountain range, the Andes (whose highest mountain is Aconcagua at 6,962 m); the driest non-polar place on earth, the Atacama Desert; the largest rainforest, the Amazon Rainforest; the highest capital city, La Paz, Bolivia; the highest commercially navigable lake in the world, Lake Titicaca; and, excluding research stations in Antarctica, the world's southernmost permanently inhabited community, Puerto Toro, Chile.
South America's major mineral resources are gold, silver, copper, iron ore, tin, and petroleum. These resources found in South America have brought high income to its countries especially in times of war or of rapid economic growth by industrialized countries elsewhere. However, the concentration in producing one major export commodity often has hindered the development of diversified economies. The fluctuation in the price of commodities in the international markets has led historically to major highs and lows in the economies of South American states, often causing extreme political instability. This is leading to efforts to diversify production to drive away from staying as economies dedicated to one major export.
South America is one of the most biodiverse continents on earth. South America is home to many interesting and unique species of animals including the llama, anaconda, piranha, jaguar, vicuña, and tapir. The Amazon rainforests possess high biodiversity, containing a major proportion of the Earth's species.
Brazil is the largest country in South America, encompassing around half of the continent's land area and population. The remaining countries and territories are divided among three regions: The Andean States, the Guianas and the Southern Cone.
Outlying islands.
Traditionally, South America also includes some of the nearby islands. Aruba, Bonaire, Curaçao, Trinidad, Tobago, and the federal dependencies of Venezuela sit on the northerly South American continental shelf and are often considered part of the continent. Geo-politically, the island states and overseas territories of the Caribbean are generally grouped as a part or subregion of North America, since they are more distant on the Caribbean Plate, even though San Andres and Providencia are politically part of Colombia and Aves Island is controlled by Venezuela.
Other islands that are included with South America are the Galápagos Islands that belong to Ecuador and Easter Island (in Oceania but belonging to Chile), Robinson Crusoe Island, Chiloé (both Chilean) and Tierra del Fuego (split between Chile and Argentina). In the Atlantic, Brazil owns Fernando de Noronha, Trindade and Martim Vaz, and the Saint Peter and Saint Paul Archipelago, while the Falkland Islands are governed by the United Kingdom, whose sovereignty over the islands is disputed by Argentina. South Georgia and the South Sandwich Islands may be associated with either South America or Antarctica.
History.
Prehistory.
South America is believed to have been joined with Africa from the late Paleozoic Era to the early Mesozoic Era, until the supercontinent Pangaea began to rift and break apart about 225 million years ago. Therefore, South America and Africa share similar fossils and rock layers.
South America is thought to have been first inhabited by humans when people were crossing the Bering Land Bridge (now the Bering Strait) at least 15,000 years ago from the territory that is present-day Russia. They migrated south through North America, and eventually reached South America through the Isthmus of Panama.
The first evidence for the existence of the human race in South America dates back to about 9000 BC, when squashes, chili peppers and beans began to be cultivated for food in the highlands of the Amazon Basin. Pottery evidence further suggests that manioc, which remains a staple food today, was being cultivated as early as 2000 BC.
By 2000 BC, many agrarian communities had been settled throughout the Andes and the surrounding regions. Fishing became a widespread practice along the coast, helping establish fish as a primary source of food. Irrigation systems were also developed at this time, which aided in the rise of an agrarian society.
South American cultures began domesticating llamas, vicuñas, guanacos, and alpacas in the highlands of the Andes circa 3500 BC. Besides their use as sources of meat and wool, these animals were used for transportation of goods.
Pre-Columbian civilizations.
The rise of plant growing and the subsequent appearance of permanent human settlements allowed for the multiple and overlapping beginnings of civilizations in South America.
One of the earliest known South American civilizations was at Norte Chico, on the central Peruvian coast. Though a pre-ceramic culture, the monumental architecture of Norte Chico is contemporaneous with the pyramids of Ancient Egypt. Norte Chico governing class established a trade network and developed agriculture then followed by Chavín by 900 BC, according to some estimates and archaeological finds. Artifacts were found at a site called Chavín de Huantar in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned 900 BC to 300 BC.
In the central coast of Peru, around the beginning of the 1st millennium AD, Moche (100 BC – 700 AD, at the northern coast of Peru), Paracas and Nazca (400 BC – 800 AD, Peru) cultures flourished with centralized states with permanent militia improving agriculture through irrigation and new styles of ceramic art. At the Altiplano, Tiahuanaco or Tiwanaku (100 BC – 1200 AD, Bolivia) managed a large commercial network based on religion.
Around 7th century, both Tiahuanaco and Wari or Huari Empire (600–1200, Central and northern Peru) expanded its influence to all the Andean region, imposing the Huari urbanism and tiahuanaco religious iconography.
The Muisca were the main indigenous civilization in what is now modern Colombia. They established a confederation of many clans, or cacicazgos, that had a free trade network among themselves. They were goldsmiths and farmers.
Other important Pre-Columbian cultures include: the Cañaris (in south central Ecuador), Chimu Empire (1300–1470, Peruvian northern coast), Chachapoyas, and the Aymaran kingdoms (1000–1450, Bolivia and southern Peru).
Holding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533. Known as "Tawantin suyu", and "the land of the four regions," in Quechua, the Inca civilization was highly distinct and developed. Inca rule extended to nearly a hundred linguistic or ethnic communities, some 9 to 14 million people connected by a 25,000 kilometer road system. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture.
The Mapuche in Central and Southern Chile resisted the European and Chilean settlers, waging the Arauco War for more than 300 years.
European colonization.
In 1494, Portugal and Spain, the two great maritime European powers of that time, on the expectation of new lands being discovered in the west, signed the Treaty of Tordesillas, by which they agreed, with the support of the Pope, that all the land outside Europe should be an exclusive duopoly between the two countries.
The Treaty established an imaginary line along a north-south meridian 370 leagues west of Cape Verde Islands, roughly 46° 37' W. In terms of the treaty, all land to the west of the line (known to comprise most of the South American soil) would belong to Spain, and all land to the east, to Portugal. As accurate measurements of longitude were impossible at that time, the line was not strictly enforced, resulting in a Portuguese expansion of Brazil across the meridian.
Beginning in the 1530s, the people and natural resources of South America were repeatedly exploited by foreign conquistadors, first from Spain and later from Portugal. These competing colonial nations claimed the land and resources as their own and divided it in colonies.
European infectious diseases (smallpox, influenza, measles, and typhus) – to which the native populations had no immune resistance – and systems of forced labor, such as the haciendas and mining industry's mita, decimated the native population under Spanish control. After this, African slaves, who had developed immunities to these diseases, were quickly brought in to replace them.
The Spaniards were committed to convert their native subjects to Christianity and were quick to purge any native cultural practices that hindered this end; however, many initial attempts at this were only partially successful, as native groups simply blended Catholicism with their established beliefs and practices. Furthermore, the Spaniards brought their language to the degree they did with their religion, although the Roman Catholic Church's evangelization in Quechua, Aymara, and Guaraní actually contributed to the continuous use of these native languages albeit only in the oral form.
Eventually, the natives and the Spaniards interbred, forming a mestizo class. At the beginning, many mestizos of the Andean region were offspring of Amerindian mothers and Spanish fathers. After independence, most mestizos had native fathers and white or mestizo mothers.
Many native artworks were considered pagan idols and destroyed by Spanish explorers; this included many gold and silver sculptures and other artifacts found in South America, which were melted down before their transport to Spain or Portugal. Spaniards and Portuguese brought the western European architectural style to the continent, and helped to improve infrastructures like bridges, roads, and the sewer system of the cities they discovered or conquered. They also significantly increased economic and trade relations, not just between the old and new world but between the different South American regions and peoples. Finally, with the expansion of the Portuguese and Spanish languages, many cultures that were previously separated became united through that of Latin American.
Guyana was first a Dutch, and then a British colony, though there was a brief period during the Napoleonic Wars when it was colonized by the French. The country was once partitioned into three parts, each being controlled by one of the colonial powers until the country was finally taken over fully by the British.
Independence from Spain and Portugal.
The European Peninsular War (1807–1814), a theater of the Napoleonic Wars, changed the political situation of both the Spanish and Portuguese colonies. First, Napoleon invaded Portugal, but the House of Braganza avoided capture by escaping to Brazil. Napoleon also captured King Ferdinand VII of Spain, and appointed his own brother instead. This appointment provoked severe popular resistance, which created Juntas to rule in the name of the captured king.
Many cities in the Spanish colonies, however, considered themselves equally authorized to appoint local Juntas like those of Spain. This began the Spanish American wars of independence between the patriots, who promoted such autonomy, and the royalists, who supported Spanish authority over the Americas. The Juntas, in both Spain and the Americas, promoted the ideas of the Enlightenment. Five years after the beginning of the war, Ferdinand VII returned to the throne and began the Absolutist Restoration as the royalists got the upper hand in the conflict.
The independence of South America was secured by Simón Bolívar (Venezuela) and José de San Martín (Argentina), the two most important "Libertadores". Bolívar led a great uprising in the north, then led his army southward towards Lima, the capital of the Viceroyalty of Peru. Meanwhile, San Martín led an army across the Andes Mountains, along with Chilean expatriates, and liberated Chile. He organized a fleet to reach Peru by sea, and sought the military support of various rebels from the Viceroyalty of Peru. The two armies finally met in Guayaquil, Ecuador, where they cornered the Royal Army of the Spanish Crown and forced its surrender.
In the Portuguese kingdom of Brazil and Algarve, Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese King Dom João VI, proclaimed the independent Kingdom of Brazil in 1822, which later became the Empire of Brazil. Despite the Portuguese loyalties of garrisons in Bahia and Pará, independence was diplomatically accepted by the crown in Portugal, on condition of a high compensation paid by Brazil.
Nation-building and Balkanization.
The newly independent nations began a process of Balkanization, with several civil and international wars. However, it was not as strong as in Central America. Some countries created from provinces of larger countries stayed as such up to modern day (such as Paraguay or Uruguay), while others were reconquered and reincorporated into their former countries (such as the Republic of Entre Ríos and the Riograndense Republic).
Attempted merge of Peru and Bolivia were blocked by Chile in the War of the Confederation (1836–1839) and again during the War of the Pacific (1879–1883). Paraguay was largely dismembered by Argentina and Brazil after the Paraguayan War. 
Rise and fall of military dictatorships.
Wars became less frequent in the 20th century. With Bolivia-Paraguay and Peru-Ecuador fighting the last inter-state wars.
Early in the 20th century, the three wealthiest South American countries engaged in a vastly expensive naval arms race which was catalyzed by the introduction of a new warship type, the "dreadnought". At one point, the Argentine government was spending a fifth of its entire yearly budget for just two dreadnoughts, a price that did not include later in-service costs, which for the Brazilian dreadnoughts was sixty percent of the initial purchase.
The continent became a battlefield of the Cold War in the late 20th century. Some democratically elected governments of Argentina, Brazil, Chile, Uruguay and Paraguay were overthrown or displaced by military dictatorships in the 1960s and 1970s. To curtail opposition, their governments detained tens of thousands of political prisoners, many of whom were tortured and/or killed on inter-state collaboration. Economically, they began a transition to neoliberal economic policies. They placed their own actions within the US Cold War doctrine of "National Security" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict.
Argentina and Britain fought the Falklands War in 1982.
Colombia has had an ongoing, though diminished internal conflict, which started in 1964 with the creation of Marxist guerrillas (FARC-EP) and then involved several illegal armed groups of leftist-leaning ideology as well as the private armies of powerful drug lords. Many of these are now defunct, and only a small portion of the ELN remains, along with the stronger, though also greatly reduced FARC. These leftist groups smuggle narcotics out of Colombia to fund their operations, while also using kidnapping, bombings, land mines and assassinations as weapons against both elected and non-elected citizens.
Revolutionary movements and right-wing military dictatorships became common after World War II, but since the 1980s, a wave of democratization came through the continent, and democratic rule is widespread now. Nonetheless, allegations of corruption are still very common, and several countries have developed crises which have forced the resignation of their governments, although, in most occasions, regular civilian succession has continued.
International indebtedness turned into a severe problem in late 1980s, and some countries, despite having strong democracies, have not yet developed political institutions capable of handling such crises without recurring to unorthodox economic policies, as most recently illustrated by Argentina's default in the early 21st century. The last twenty years have seen an increased push towards regional integration, with the creation of uniquely South American institutions such as the Andean Community, Mercosur and Unasur. Notably, starting with the election of Hugo Chávez in Venezuela in 1998, the region experienced what has been termed a pink tide – the election of several leftist and center-left administrations to most countries of the area, except for the Guianas and Colombia.
Politics.
During the first decade of the 21st century, South American governments have drifted to the political left, with socialist leaders being elected in Chile, Uruguay, Brazil, Argentina, Ecuador, Bolivia, Paraguay, Peru and Venezuela. Most South American countries are making an increasing use of protectionist policies, undermining a greater global integration but helping local development.
Recently, an intergovernmental entity has been formed which aims to merge the two existing customs unions: Mercosur and the Andean Community, thus forming the third-largest trade bloc in the world.
This new political organization known as Union of South American Nations seeks to establish free movement of people, economic development, a common defense policy and the elimination of tariffs.
Ethnic demographics.
Descendants of indigenous peoples, such as the Quechua and Aymara, or the Urarina of Amazonia make up the majority of the population in Bolivia (56%) and, per some sources, in Peru (44%). In Ecuador, Amerindians are a large minority that comprises two-fifths of the population. The white/European population is also a significant element in most other former Portuguese colonies.
South America is also home to one of the largest populations of Africans. This group is also significantly present in Guyana, Brazil, Colombia, Suriname, French Guiana, and Ecuador. Mestizos (mixed white and Amerindian) are the largest ethnic group in Paraguay, Venezuela, Colombia (49%) and Ecuador and the second group in Peru. East Indians form the largest ethnic group in Guyana and Suriname. Brazil followed by Peru also have the largest Japanese, Korean and Chinese communities in South America.
The demographics of Colombia include approximately 37% white and European descendants, while in Peru, European descendants are the third group in importance (15%). Compared to other South American countries, the people who identify as of primarily or totally European descent, or identify their phenotype as corresponding to such group, are more of a majority in Argentina, Chile and Uruguay, and are about half of the population of Brazil. In Venezuela, according to the national census 42% of the population is primarily white Spanish, Italian and Portuguese descendants.
Indigenous people.
In many places indigenous people still practice a traditional lifestyle based on subsistence agriculture or as hunter-gatherers. There are still some uncontacted tribes residing in the Amazon Rainforest.
Economy.
South America relies less on the export of both manufactured goods and natural resources than the world average; merchandise exports from the continent were 16% of GDP on an exchange rate basis, compared to 25% for the world as a whole. Brazil (the seventh largest economy in the world and the largest in South America) leads in terms of merchandise exports at $251 billion, followed by Venezuela at $93 billion, Chile at $86 billion, and Argentina at $84 billion.
The economic gap between the rich and poor in most South American nations is larger than in most other continents. The richest 10% receive over 40% of the nation's income in Bolivia, Brazil, Chile, Colombia, and Paraguay, while the poorest 20% receive 3% or less in Bolivia, Brazil, and Colombia. This wide gap can be seen in many large South American cities where makeshift shacks and slums lie in the vicinity of skyscrapers and upper-class luxury apartments; nearly one in nine in South America live on less than $2 per day (on a purchasing power parity basis).
Tourism.
Tourism has increasingly become a significant source of income for many South American countries. Historical relics, architectural and natural wonders, a diverse range of foods and culture, vibrant and colorful cities, and stunning landscapes attract millions of tourists every year to South America. Some of the most visited places in the region are Recife, Olinda, Machu Picchu, the Amazon Rainforest, Rio de Janeiro, Salvador, Fortaleza, Maceió, Bogota, Lima, Florianópolis, Isla Margarita, Natal, Buenos Aires, São Paulo, Angel Falls, Nazca Lines, Cuzco, Lake Titicaca, Los Roques archipelago, Medellín, Patagonia, Gran Sabana, Cartagena and the Galápagos Islands.
Culture.
South Americans are culturally influenced by their indigenous peoples, the historic connection with the Iberian Peninsula and Africa, and waves of immigrants from around the globe.
South American nations have a rich variety of music. Some of the most famous genres include vallenato and cumbia from Colombia, pasillo from Ecuador, samba and bossa nova from Brazil, and tango from Argentina and Uruguay. Also well known is the non-commercial folk genre Nueva Canción movement which was founded in Argentina and Chile and quickly spread to the rest of the Latin America. People on the Peruvian coast created the fine guitar and cajon duos or trios in the most mestizo (mixed) of South American rhythms such as the Marinera (from Lima), the Tondero (from Piura), the 19th century popular Creole Valse or Peruvian Valse, the soulful Arequipan Yaravi, and the early 20th century Paraguayan Guarania. In the late 20th century, Spanish rock emerged by young hipsters influenced by British pop and American rock. Brazil has a Portuguese-language pop rock industry as well a great variety of other music genres.
The literature of South America has attracted considerable critical and popular acclaim, especially with the Latin American Boom of the 1960s and 1970s, and the rise of authors such as Mario Vargas Llosa, Gabriel García Márquez in novels, and Pablo Neruda and Jorge Luis Borges in other genres. The Brazilian Machado de Assis, a 19th-century realist writer, is widely regarded as the greatest Brazilian writer. His admirers include José Saramago, Carlos Fuentes, Susan Sontag and Harold Bloom.
Nowadays Paulo Coelho is one of the most read and translated authors. His bestseller "The Alchemist" has been translated into 73 languages and is one of the most read books in the world.
Because of South America's broad ethnic mix, South American cuisine has African, South American Indian, Asian, and European influences. Bahia, Brazil, is especially well known for its West African–influenced cuisine. Argentines, Chileans, Uruguayans, Brazilians, Bolivians, and Venezuelans regularly consume wine. Argentina, Paraguay, Uruguay, and people in southern Chile, Bolivia and Brazil drink mate, a herb which is brewed. The Paraguayan version, terere, differs from other forms of mate in that it is served cold. Pisco is a liquor distilled from grapes in Peru and Chile. Peruvian cuisine mixes elements from Chinese, Japanese, Spanish, African, Andean, and Amazonic food.
Language.
Spanish and Portuguese are the most spoken languages in South America, with approximately 200 million speakers each. Spanish is the official language of most countries, along with other native languages in some countries. Portuguese is the official language of Brazil. Dutch is the official language of Suriname; English is the official language of Guyana, although there are at least twelve other languages spoken in the country, including Hindi and Arabic. English is also spoken in the Falkland Islands. French is the official language of French Guiana and the second language in Amapá, Brazil.
Indigenous languages of South America include Quechua in Ecuador, Peru, Chile, Colombia, and Bolivia; Wayuunaiki in northern Colombia (La Guajira) and northwestern Venezuela (Zulia); Guaraní in Paraguay and, to a much lesser extent, in Bolivia; Aymara in Bolivia, Peru, and less often in Chile; and Mapudungun is spoken in certain pockets of southern Chile and, more rarely, Argentina. At least three South American indigenous languages (Quechua, Aymara, and Guarani) are recognized along with Spanish as national languages.
Other languages found in South America include, Hindi and Javanese in Suriname; Italian in Argentina, Brazil, Uruguay, Venezuela and Chile; and German in certain pockets of Argentina, Brazil, and Chile. German is also spoken in many regions of the southern states of Brazil, Riograndenser Hunsrückisch being the most widely spoken German dialect in the country; among other Germanic dialects, a Brazilian form of Pomeranian is also well represented and is experiencing a revival. Welsh remains spoken and written in the historic towns of Trelew and Rawson in the Argentine Patagonia. There are also small clusters of Japanese-speakers in Brazil, Colombia and Peru. Arabic speakers, often of Lebanese, Syrian, or Palestinian descent, can be found in Arab communities in Argentina, Colombia, Brazil, Venezuela, Peru, Chile, and in Paraguay.
Sport.
A wide range of sports are played in the continent of South America, with football being the most popular overall, while baseball is the most popular in Venezuela and northern Colombia.
Other sports include futsal, basketball, volleyball, beach volleyball, motorsports, rugby (mostly in Argentina and Uruguay), handball, tennis, golf, field hockey and boxing.
South America will hold its first Olympic Games in Rio de Janeiro, Brazil, in 2016. 
South America shares with Europe the supremacy over the sport of football as all winners in FIFA World Cup history and all winning teams in the FIFA Club World Cup have come from these two continents. Brazil holds the record at the FIFA World Cup with five titles in total. Argentina and Uruguay have two titles each. So far four South American nations have hosted the tournament including the first edition in Uruguay (1930). The other three were Brazil (1950, 2014), Chile (1962), and Argentina (1978).
South America is home to the longest running international football tournament; the Copa América, which has been regularly contested since 1916. Uruguay have won the Copa America a record 15 times, surpassing hosts Argentina in 2011 to reach 15 titles (they were previously equal on 14 titles each during the 2011 Copa America). The continent has produced many of the most famous and most talented players including Diego Maradona, Pelé, Alfredo Di Stéfano, Ronaldo, Ronaldinho, Kaká, Rivaldo, Teófilo Cubillas, Mario Kempes, Gabriel Batistuta, Ángel Di María, César Cueto, Enzo Francescoli, Arsenio Erico, Alberto Spencer, Carlos Valderrama, Ivan Zamorano, Elias Figueroa, Marcelo Salas, Antonio Valencia, Juan Arango, Neymar, Radamel Falcao, Luis Suárez, Edinson Cavani, and Lionel Messi.
Also, in South America, a multi-sport event, the South American Games, are held every four years. The first edition was held in La Paz in 1978 and the most recent took place in Santiago in 2014.
References.
Content notes.
^ Continent model: In some parts of the world South America is viewed as a subcontinent of the Americas (a single continent in these areas), for example Latin America, Latin Europe, and Iran. In most of the countries with English as an official language, however, it is considered a continent;
see Americas (terminology).

</doc>
<doc id="26771" url="http://en.wikipedia.org/wiki?curid=26771" title="Spindletop">
Spindletop

Spindletop is a salt dome oil field located in the southern portion of Beaumont, Texas in the United States. The Spindletop dome was derived from the Louann Salt evaporite layer of the Jurassic geologic period. On January 10, 1901, a well at Spindletop struck oil ("came in"). The new oil field soon produced more than 100000 oilbbl of oil per day. Gulf Oil and Texaco, now part of Chevron Corporation, were formed to develop production at Spindletop.
The strike at Spindletop represented a turning point for Texas and the United States; no oil field in the world had ever been so productive. The frenzy of oil exploration and the economic development it generated in the state became known as the Texas Oil Boom. The United States soon became the world's leading oil producer.
History.
There had long been suspicions that oil might be under "Spindletop Hill." The area was known for its vast sulfur springs and bubbling gas seepages that would ignite if lit. In August 1892, George W. O'Brien, George W. Carroll, Pattillo Higgins and others formed the Gladys City Oil, Gas, and Manufacturing Company to do exploratory drilling on Spindletop Hill. The company drilled many dry holes and ran into trouble, as investors began to balk at pouring more money into drilling with no oil to show for it.
Pattillo Higgins left the company and teamed with Captain Anthony F. Lucas, the leading expert in the U.S. on salt dome formations. Lucas made a lease agreement in 1899 with the Gladys City Company and a subsequent agreement with Higgins. Lucas drilled to 575 ft before running out of money. He secured additional funding from John H. Galey and James M. Guffey of Pittsburgh, but the deal left Lucas with only a small share of the lease and Higgins with nothing.
Lucas continued drilling and on January 10, 1901, at a depth of 1,139 ft (347 m), what is known as the Lucas Gusher or the Lucas Geyser blew oil over 150 ft in the air at a rate of 100000 oilbbl/d(4,200,000 gallons). It took nine days before the well was brought under control. Spindletop was the largest gusher the world had seen and catapulted Beaumont into an oil-fueled boomtown. Beaumont's population of 10,000 tripled in three months and eventually rose to 50,000. Speculation led land prices to increase rapidly. By the end of 1902, more than 500 companies had been formed and 285 wells were in operation.
Spindletop was the first oil field found on the US Gulf Coast, and prompted further drilling, and further oil field discoveries. Oil drillers looking for another Spindletop particularly sought out other salt domes, and were often successful. The Gulf Coast turned into a major oil region.
Standard Oil, which then had a monopoly or near-monopoly on the petroleum industry in the eastern states, was prevented from moving aggressively into the new oil field by state antitrust laws. Populist sentiment against Standard Oil was particularly strong at the time of the Spindletop discovery. In 1900, an oil products marketing company affiliated with Standard Oil had been banned from the state for its cutthroat business practices. Although Standard built refineries in the area, Standard was unable to dominate the new Gulf Coast oil fields the way it had in the eastern states. As a result, a number of startup oil companies at Spindletop, such as Texaco and Gulf Oil, grew into formidable competitors to Standard Oil.
Among those drilling at Spindletop was W. Scott Heywood, a native of Cleveland, Ohio, who in 1901 made the first oil discovery in nearby Jeff Davis Parish in southwestern Louisiana. In 1932, Heywood was elected to a single term in the Louisiana State Senate.
Production at Spindletop began to decline rapidly after 1902, and the wells produced only 10000 oilbbl/d by 1904. On November 14, 1925, the Yount-Lee Oil Company brought in its McFaddin No. 2 at a depth of about 2500 ft, sparking a second boom, which culminated in the field's peak production year of 1927, during which 21 millions barrels (3.3 GL) were produced. Over the ten years following the McFaddin discovery, more than 72 million barrels (11.4 GL) of oil were produced, mostly from the newer areas of the field. Spindletop continued as a productive source of oil until about 1936. It was then mined for sulfur from the 1950s to about 1975.
Spindletop-Gladys City Boomtown Museum.
In 1976 Lamar University dedicated the Spindletop-Gladys City Boomtown Museum to preserve the history of the Spindletop oil gusher era in Beaumont. The museum features an oil derrick and many reconstructed Gladys City building interiors furnished with authentic artifacts from the Spindletop boomtown period.
The Lucas Gusher Monument is located at the museum. The Monument, erected at the wellhead in July, 1941, was moved to the Spindletop-Gladys City Museum after it became unstable due to ground subsidence. According to an article by Nedra Foster, LS in the July/August, 2000 issue of the Professional Surveyor Magazine, the Monument was originally located within four feet of the actual site of the Spindletop well.
Today a flag pole flying a Texas flag marks the location of the wellhead, at Spindletop Park, about 1.5 miles southwest of the museum, off West Port Arthur Road/Spur 93. There is a viewing platform with information placards there, about a quarter mile from the flagpole, which is in the middle of swampland on private land and not accessible. Directions to the site are available at the museum.
On December 4, 1955, the Spindletop story was dramatized in "Spindletop - The First Great Texas Oil Strike (January 10, 1901)" on the CBS history series, "You Are There". Robert Bray was cast as Pattillo Higgins; Mike Ragan as Marion Fletcher; Parley Baer as Captain Lucas, Jean Byron as Caroline Lucas, DeForest Kelley as Al Hammill, Tyler McVey as Mayor Wheat, and William Fawcett as a farmer.

</doc>
<doc id="26773" url="http://en.wikipedia.org/wiki?curid=26773" title="Stendhal">
Stendhal

Marie-Henri Beyle (]; 23 January 1783 – 23 March 1842), better known by his pen name Stendhal (] or ]; in English ], , or ), was a 19th-century French writer. Known for his acute analysis of his characters' psychology, he is considered one of the earliest and foremost practitioners of realism, as is evident in the novels "Le Rouge et le Noir" ("The Red and the Black", 1830) and "La Chartreuse de Parme" ("The Charterhouse of Parma", 1839).
Life.
Born in Grenoble, Isère, he was an unhappy child, disliking his "unimaginative" father and mourning his mother, whom he passionately loved, and who died when he was seven. He spent "the happiest years of his life" at the Beyle country house in Claix near Grenoble. His closest friend was his younger sister, Pauline, with whom he maintained a steady correspondence throughout the first decade of the 19th century.
The military and theatrical worlds of the First French Empire were a revelation to Beyle. He was named an auditor with the Conseil d'État on 3 August 1810, and thereafter took part in the French administration and in the Napoleonic wars in Italy. He travelled extensively in Germany and was part of Napoleon's army in the 1812 invasion of Russia.
After the 1814 Treaty of Fontainebleau, he left for Italy, where he settled in Milan. He formed a particular attachment to Italy, where he spent much of the remainder of his career, serving as French consul at Trieste and Civitavecchia. His novel "The Charterhouse of Parma", written in 52 days, is set in Italy, which he considered a more sincere and passionate country than Restoration France. An aside in that novel, referring to a character who contemplates suicide after being jilted, speaks about his attitude towards his home country: "To make this course of action clear to my French readers, I must explain that in Italy, a country very far away from us, people are still driven to despair by love."
Besides many others, Beyle used the pseudonym "Stendhal". Scholars in general believe he borrowed this "nom de plume" from the German city of Stendal in homage to Johann Joachim Winckelmann.
Stendhal was a dandy and wit about town in Paris, as well as an inveterate womaniser who was obsessed with his sexual conquests. His genuine empathy towards women is evident in his books; Simone de Beauvoir spoke highly of him in "The Second Sex". One of his early works is "On Love," a rational analysis of romantic passion that was based on his unrequited love for Mathilde, Countess Dembowska, whom he met while living at Milan. This fusion of, and tension between, clear-headed analysis and romantic feeling is typical of Stendhal's great novels; he could be considered a Romantic realist.
Stendhal suffered miserable physical disabilities in his final years as he continued to produce some of his most famous work. As he noted in his journal, he was taking iodide of potassium and quicksilver to treat his syphilis, resulting in swollen armpits, difficulty swallowing, pains in his shrunken testicles, sleeplessness, giddiness, roaring in the ears, racing pulse and "tremors so bad he could scarcely hold a fork or a pen". Indeed, he dictated "Charterhouse" in this pitiable state. Modern medicine has shown that his health problems were more attributable to his treatment than to his syphilis.
Stendhal died on 23 March 1842, a few hours after collapsing with a seizure on the streets of Paris. He is interred in the Cimetière de Montmartre.
Works.
Contemporary readers did not fully appreciate Stendhal's realistic style during the Romantic period in which he lived; he was not fully appreciated until the beginning of the 20th century. He dedicated his writing to "the Happy Few" (in English in the original). This is often interpreted as a sly dedication to the few who understood or appreciated his writing. This can be interpreted as a reference to Canto 11 of Byron's "Don Juan", which refers to "the thousand happy few" who enjoy high society, or to the "we few, we happy few, we band of brothers" line of Shakespeare's "Henry V", but Stendhal's use more likely refers to "The Vicar of Wakefield" by Oliver Goldsmith, parts of which he had memorized in the course of teaching himself English. In "The Vicar of Wakefield", "the happy few" refers ironically to the small number of people who read the title character's obscure and pedantic treatise on monogamy. As a literary critic, such as in "Racine and Shakespeare", Stendhal championed the Romantic aesthetic by unfavorably comparing the rules and strictures of Racine's classicism to the freer verse and settings of Shakespeare, and supporting the writing of plays in prose. The German philosopher Friedrich Nietzsche refers to Stendhal as "France's last great psychologist" in his 1886 work, "Beyond Good and Evil" and acknowledged his personal debt to the French writer in the "Twilight of the Idols" (when referring to Dostoevsky as the only psychologist from whom he had something to learn), stating that encountering him (Dostoevsky) was "the most beautiful accident of my life, more so than even my discovery of Stendhal".
Today, Stendhal's works attract attention for their irony and psychological and historical dimensions. Stendhal was an avid fan of music, particularly the works of the composers Cimarosa, Mozart and Rossini. He wrote a biography of Rossini, "Vie de Rossini" (1824), now more valued for its wide-ranging musical criticism than for its historical content.
In his works, Stendhal reprised excerpts appropriated from Giuseppe Carpani, Théophile Frédéric Winckler, Sismondi and others.
Autobiography.
Stendhal's brief memoir, "Souvenirs d'Égotisme" ("Memoirs of an Egotist") was published posthumously in 1892. Also published was a more extended autobiographical work, thinly disguised as the "Life of Henry Brulard".
Non-fiction.
His other works include short stories, journalism, travel books ("Promenades dans Rome"), a famous collection of essays on Italian painting, and biographies of several prominent figures of his time, including Napoleon, Haydn, Mozart, Rossini and Metastasio.
Crystallization.
In Stendhal's 1822 classic "On Love" he describes or compares the “birth of love”, in which the love object is 'crystallized' in the mind, as being a process similar or analogous to a trip to Rome. In the analogy, the city of Bologna represents "indifference" and Rome represents "perfect love":
When we are in Bologna, we are entirely indifferent; we are not concerned to admire in any particular way the person with whom we shall perhaps one day be madly in love; even less is our imagination inclined to overrate their worth. In a word, in Bologna “crystallization” has not yet begun. When the journey begins, love departs. One leaves Bologna, climbs the Apennines, and takes the road to Rome. The departure, according to Stendhal, has nothing to do with one’s will; it is an instinctive moment. This transformative process actuates in terms of four steps along a journey:
This journey or crystallization process (shown above) was detailed by Stendhal on the back of a playing card while speaking to Madame Gherardi, during his trip to the Salzburg salt mine.
Stendhal syndrome.
In 1817 Stendhal reportedly was overcome by the cultural richness of Florence he encountered when he first visited the Tuscan city. As he described in his book "Naples and Florence: A Journey from Milan to Reggio":
As I emerged from the porch of Santa Croce, I was seized with a fierce palpitation of the heart (that same symptom which, in Berlin, is referred to as an attack of the nerves); the well-spring of life was dried up within me, and I walked in constant fear of falling to the ground.
The condition was diagnosed and named in 1979 by Italian psychiatrist Dr. Graziella Magherini, who had noticed similar psychosomatic conditions (racing heart beat, nausea and dizziness) amongst first-time visitors to the city.
In homage to Stendhal, Trenitalia named their overnight train service from Paris to Venice the Stendhal Express.

</doc>
<doc id="26775" url="http://en.wikipedia.org/wiki?curid=26775" title="Syndicalism">
Syndicalism

Syndicalism is a type of proposed economic system, a form of socialism, considered a replacement for capitalism. It suggests that industries be organized into confederations or syndicates. It is "a system of economic organization in which industries are owned and managed by the workers."
Its theory and practice is the advocation of multiple cooperative productive units composed of specialists and representatives of workers in each respective field to negotiate and manage the economy. Syndicalism also refers to the political movement (praxis) and tactics used to bring about this type of system.
For adherents, labour unions and labour training (see below) are the potential means of both overcoming economic aristocracy and running society fairly and in the interest of informed and skilled majorities, through union democracy. Industry in a syndicalist system would be run through co-operative confederations and mutual aid. Local syndicates would communicate with other syndicates through the Bourse du Travail (labour exchange) which would cooperatively determine distributions of commodities.
Syndicalism is also used to refer to the tactic of bringing about this social arrangement, typically expounded by anarcho-syndicalism and De Leonism. It aims to achieve a general strike, a workers' outward refusal of their current modes of production, followed by organisation into federations of trade unions, such as the CNT. Throughout its history, the reformist section of syndicalism has been overshadowed by its revolutionary section, typified by the Federación Anarquista Ibérica section of the CNT.
Theory.
Syndicalism is one of the three most common currents of socialist economics, together with market socialism and socialist planned economies. It holds, on an ethical basis, that all participants in an organised trade internally share ownership of its production. Socialism, in contrast, concerns solely the methods of distribution among trades as is centrally required of each trade; it does not necessarily consider how trades organise internally. Communism (the theory) rejects government-sanctioned private ownership of the means of production in favour of ownership by the social class who actually produce such property, i.e., the workers or proletariat. Under most variants of communism, in the period of socialist transition to a Communist social formation, this social class would also have control of the state; for syndicalists, however, this muddles the distinction between state and proletarian ownership. In syndicalism, unions exist independent of a state, and do not operate under state micromanagement and central planning. As with businesses in capitalism, labour unions in syndicalism would likely share a complicated relationship of co-operation and opposition with the state (with the obvious exception of anarcho-syndicalism, under which there would be no state).
Syndicalists state that society ought to be organised bottom-up based on direct democracy, confederation, workplace democracy and decentralised socialism. In order to achieve such a society they may either initiate a general strike through direct action and workplace occupation or—in the case of reformist syndicalists—develop the syndicalist economics "alongside" the state, in competition to it. Syndicalists state that delegation—the use of direct representation—will facilitate direct democracy and that each commune/region would be independent in the confederation.
Syndicalism and anarcho-syndicalism.
Syndicalism can be accurately divided into the purely economic focused camp, exemplified by the Italian USI (Unione Sindacale Italiana, the largest Italian syndicalist union in 1920, taking part in Biennio rosso ) and the anarcho syndicalism of the CNT (national confederation of labour), taking both political and economic action, wishing to take control of both workplace and political life, while syndicalism has traditionally focused on the economic sector alone.
Although the terms anarcho-syndicalism and "revolutionary" syndicalism are often used interchangeably, the anarcho-syndicalist label was not widely used until the early 1920s (some credit Sam Mainwaring with coining the term). “The term ‘anarcho-syndicalist’ only came into wide use in 1921-1922 when it was applied polemically as a pejorative term by communists to any syndicalists…who opposed increased control of syndicalism by the communist parties”.
Traditionally the revolutionary political syndicalism of figures such as Rudolph Rocker (widely credited as the father of anarcho-syndicalism) has overshadowed the more reformist or economically focused syndicalism.
Related theories include anarchism, socialism, Marxism, Leninism, and communism.
History.
"Syndicalisme"/"Sindicalismo" is a French/Spanish word meaning "trade unionism". More moderate versions of syndicalism were overshadowed in the early 20th century by revolutionary anarcho-syndicalism, which advocated, in addition to the abolition of capitalism, the abolition of the state, which was expected to be made obsolete by syndicalist economics. Anarcho-syndicalism was most powerful in Spain in and around the time of the Spanish Civil War, but also appeared in other parts of the world, such as in the US-based Industrial Workers of the World and the Unione Sindacale Italiana - the Italian Syndicalist Union.
The earliest expressions of syndicalist structure and methods were formulated in the International Workingmen's Association or First International, particularly in the Jura federation. In 1895, the Confédération Générale du Travail (CGT) in France expressed fully the organisational structure and methods of revolutionary syndicalism influencing labour movements the world over. The CGT was modelled on the development of the Bourse de Travail (labour exchange), a workers' central organisation which would encourage self-education and mutual aid, and facilitate communication with local workers' syndicates. Through a general strike, workers would take control of industry and services and self-manage society and facilitate production and consumption through the labour exchanges. The Charter of Amiens, adopted by the CGT in 1906, represents a key text in the development of revolutionary syndicalism rejecting parliamentarianism and political action in favour of revolutionary class struggle. The Central Organisation of the Workers of Sweden (SAC) (in Swedish the Sveriges Arbetares Centralorganisation), formed in 1910, are a notable example of an anarcho-syndicalist union influenced by the CGT. Today, the SAC is one of the largest anarcho-syndicalist unions in the world in proportion to the population, with some strongholds in the public sector.
The International Workers Association, formed in 1922, is an international syndicalist federation of various labour unions from different countries. At its peak, the International Workers Association represented millions of workers and competed directly for the hearts and minds of the working class with social democratic unions and parties. The Spanish Confederación Nacional del Trabajo played a major role in the Spanish labour movement. It was also a decisive force in the Spanish Civil War, organising worker militias and facilitating the collectivisation of vast sections of the industrial, logistical, and communications infrastructure, principally in Catalonia. Another Spanish anarcho-syndicalist union, the Confederacion General del Trabajo de España, is now the fourth largest union in Spain and the largest anarchist union with tens of thousands of members.
The Industrial Workers of the World (IWW), although explicitly "not" syndicalist, were informed by developments in the broader revolutionary syndicalist milieu at the turn of the twentieth-century. At its founding congress in 1905, influential members with strong anarchist or anarcho-syndicalist sympathies like Thomas J. Hagerty, William Trautmann, and Lucy Parsons contributed to the union's overall revolutionary syndicalist orientation. Lucy Parsons, in particular, was a veteran anarchist union organiser in Chicago from a previous generation, having participated in the struggle for the 8-hour day in Chicago and subsequent series of events which came to be known as the Haymarket Affair in 1886.
An emphasis on industrial organisation was a distinguishing feature of syndicalism when it began to be identified as a distinct current at the beginning of the 20th century. Due to a still-tangible faith in the viability of the state socialist system, most socialist groups of that period emphasised the importance of political action through party organisations as a means of bringing about socialism; in syndicalism, trade unions are thus seen as simply a stepping stone to common ownership. Although all syndicalists emphasise industrial organisation, not all reject political action altogether. For example, De Leonists and some other Industrial Unionists advocate parallel organisation both politically and industrially, while recognising that trade unions are at a comparable disadvantage due to the lobby of business groups on political leaders. Syndicalism would historically gain most of its support in Italy, France and particularly Spain, where the anarcho-syndicalist revolution during the Spanish civil war resulted in the widespread implementation of anarchist and more broadly socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia, Aragon, Andalusia, and parts of the Levante. Much of Spain's economy was put under worker control; in anarchist strongholds like Catalonia, the figure was as high as 75%. Their eventual defeat and World War II led to the formerly prominent theory being repressed, as the three nations where it had the most power were now under fascist control. Support for Syndicalism never fully recovered to the height it enjoyed in the early 20th century.

</doc>
<doc id="26779" url="http://en.wikipedia.org/wiki?curid=26779" title="Soviet Union">
Soviet Union

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1922–1952
 |- class="mergedrow"
 |style="width:1.0em; padding:0 0 0 0.6em;"| - ||style="padding-left:0;text-align:left;"|1922–1938|| 
 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1991 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |style="padding-left:0;text-align:left;"| 1991 est.
 |- class="mergedbottomrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
 | .su1
 | +7
The Union of Soviet Socialist Republics (Russian: Сою́з Сове́тских Социалисти́ческих Респу́блик, "Soyuz Sovetskikh Sotsialisticheskikh Respublik"; ]) abbreviated to USSR (Russian: СССР, "SSSR")) or shortened to the Soviet Union (Russian: Сове́тский Сою́з, "Sovetskij Soyuz"; ]), was a Marxist–Leninist state on the Eurasian continent that existed between 1922 and 1991. It was governed as a single-party state by the Communist Party with Moscow as its capital. A union of multiple subnational Soviet republics, its government and economy were highly centralized.
The Soviet Union had its roots in the Russian Revolution of 1917, which overthrew the Russian Empire. The Bolsheviks, the majority faction of the Social Democratic Labour Party, led by Vladimir Lenin, then led a second revolution which overthrew the provisional government and established the Russian Socialist Federative Soviet Republic (renamed Russian Soviet Federative Socialist Republic in 1936), beginning a civil war between pro-revolution Reds and counter-revolution Whites. The Red Army entered several territories of the former Russian Empire, and helped local Communists take power through soviets that nominally acted on behalf of workers and peasants. In 1922, the Communists were victorious, forming the Soviet Union with the unification of the Russian, Transcaucasian, Ukrainian, and Byelorussian republics. Following Lenin's death in 1924, a troika collective leadership and a brief power struggle, Joseph Stalin came to power in the mid-1920s. Stalin suppressed political opposition to him, committed the state ideology to Marxism–Leninism (which he created) and initiated a centrally planned economy. As a result, the country underwent a period of rapid industrialisation and collectivisation which laid the basis for its later war effort and dominance after World War II. However, Stalin established political paranoia, and introduced arbitrary arrests on a massive scale after which the authorities transferred many people (military leaders, Communist Party members, ordinary citizens alike) to correctional labour camps or sentenced them to execution.
In the beginning of World War II, after the United Kingdom and France rejected an alliance with the Soviet Union against Nazi Germany, the USSR signed a non-aggression pact with Germany; the treaty delayed confrontation between the two countries, but was disregarded in 1941 when the Nazis invaded, opening the largest and bloodiest theatre of combat in history. Soviet war casualties accounted for the highest proportion of the conflict in the cost of acquiring the upper hand over Axis forces at intense battles such as Stalingrad. Soviet forces eventually drove through Eastern Europe and captured Berlin in 1945, inflicting the vast majority of German losses. Soviet occupied territory conquered from Axis forces in Central and Eastern Europe became satellite states of the Eastern Bloc. Ideological and political differences with Western Bloc counterparts directed by the United States led to the forming of economic and military pacts, culminating in the prolonged Cold War.
Following Stalin's death in 1953, a period of moderate social and economic liberalization (known as "de-Stalinization") occurred under the administration of Nikita Khrushchev. The Soviet Union then went on to initiate significant technological achievements of the 20th century, including launching the first ever satellite and world's first human spaceflight, which led it into the Space Race. The 1962 Cuban Missile Crisis marked a period of extreme tension between the two superpowers, considered the closest to a mutual nuclear confrontation. In the 1970s, a relaxation of relations followed, but tensions resumed when the Soviet Union began providing military assistance in Afghanistan at the request of its new socialist government in 1979. The campaign drained economic resources and dragged on without achieving meaningful political results.
In the late 1980s the last Soviet leader, Mikhail Gorbachev, sought to reform the Union and move it in the direction of Nordic-style social democracy, introducing the policies of "glasnost" and "perestroika" in an attempt to end the period of economic stagnation and democratize the government. However, this led to the rise of strong nationalist and separatist movements. Central authorities initiated a referendum, boycotted by the Baltic republics, Armenia, Georgia, and Moldova, which resulted in the majority of participating citizens voting in favour of preserving the Union as a renewed federation. In August 1991, a coup d'état was attempted by hardliners against Gorbachev, with the intention of reversing his policies. The coup failed, with Russian President Boris Yeltsin playing a high-profile role in facing down the coup, resulting in the banning of the Communist Party. On 25 December 1991, Gorbachev resigned and the remaining twelve constituent republics emerged from the dissolution of the Soviet Union as independent post-Soviet states. The Russian Federation (formerly the Russian SFSR) assumed the Soviet Union's rights and obligations and is recognised as its continued legal personality.
Geography, climate and environment.
With an area of 22402200 km2, the Soviet Union was the world's largest state, a status that is retained by the Russian Federation. Covering a sixth of the Earth's land surface, its size was comparable to that of North America. The European portion accounted for a quarter of the country's area, and was the cultural and economic center. The eastern part in Asia extended to the Pacific Ocean to the east and Afghanistan to the south, and, except some areas in Central Asia, was much less populous. It spanned over 10000 km east to west across 11 time zones, and over 7200 km north to south. It had five climate zones: tundra, taiga, steppes, desert, and mountains.
The Soviet Union had the world's longest boundary, like Russia, measuring over 60000 km, or 1 1/2 circumferences of the Earth. Two-thirds of it were a coastline. Across the Bering Strait was the United States. The Soviet Union bordered Afghanistan, China, Czechoslovakia, Finland, Hungary, Iran, Mongolia, North Korea, Norway, Poland, Romania, and Turkey from 1945 to 1991.
The Soviet Union's highest mountain was Communism Peak (now Ismoil Somoni Peak) in Tajikistan, at 7495 m. The Soviet Union also included most of the world's largest lake, the Caspian Sea (shared with Iran), and also Lake Baikal, the world's largest freshwater and deepest lake, an internal body of water in Russia.
History.
The last Russian Tsar, Nicholas II, ruled the Russian Empire until his abdication in March 1917 in the aftermath of the February Revolution, due in part to the strain of fighting in World War I, which lacked public support. A short-lived Russian Provisional Government took power, to be overthrown in the October Revolution (N.S. 7 November 1917) by revolutionaries led by the Bolshevik leader Vladimir Lenin.
The Soviet Union was officially established in December 1922 with the union of the Russian, Ukrainian, Byelorussian, and Transcaucasian Soviet republics, each ruled by local Bolshevik parties. Despite the foundation of the Soviet state as a federative entity of many constituent republics, each with its own political and administrative entities, the term "Soviet Russia" – strictly applicable only to the Russian Federative Socialist Republic – was often applied to the entire country by non-Soviet writers and politicians.
Revolution and foundation.
Modern revolutionary activity in the Russian Empire began with the Decembrist Revolt of 1825. Although serfdom was abolished in 1861, it was done on terms unfavourable to the peasants and served to encourage revolutionaries. A parliament—the State Duma—was established in 1906 after the Russian Revolution of 1905, but Tsar Nicholas II resisted attempts to move from absolute to constitutional monarchy. Social unrest continued and was aggravated during World War I by military defeat and food shortages in major Soviet cities.
A spontaneous popular uprising in Petrograd, in response to the wartime decay of Russia's economy and morale, culminated in the February Revolution and the toppling of the imperial government in March 1917. The tsarist autocracy was replaced by the Russian Provisional Government, which intended to conduct elections to the Russian Constituent Assembly and to continue fighting on the side of the Entente in World War I.
At the same time, workers' councils, known in Russian as "Soviets", sprang up across the country. The Bolsheviks, led by Vladimir Lenin, pushed for socialist revolution in the Soviets and on the streets. On 7 November 1917, the Red Guards stormed the Winter Palace in Petrograd, ending the rule of the Provisional Government and leaving all political power to the Soviets. This event would later be known as the Great October Socialist Revolution. In December, the Bolsheviks signed an armistice with the Central Powers, though by February 1918, fighting had resumed. In March, the Soviets ended involvement in the war for good and signed the Treaty of Brest-Litovsk.
A long and bloody Civil War ensued between the Reds and the Whites, starting in 1917 and ending in 1923 with the Reds' victory. It included foreign intervention, the execution of the former tsar and his family, and the famine of 1921, which killed about five million. In March 1921, during a related conflict with Poland, the Peace of Riga was signed, splitting disputed territories in Belarus and Ukraine between the Republic of Poland and Soviet Russia. Soviet Russia had to resolve similar conflicts with the newly established Republic of Finland, the Republic of Estonia, the Republic of Latvia, and the Republic of Lithuania.
Unification of republics.
On 28 December 1922, a conference of plenipotentiary delegations from the Russian SFSR, the Transcaucasian SFSR, the Ukrainian SSR and the Byelorussian SSR approved the Treaty of Creation of the USSR and the Declaration of the Creation of the USSR, forming the Union of Soviet Socialist Republics. These two documents were confirmed by the 1st Congress of Soviets of the USSR and signed by the heads of the delegations, Mikhail Kalinin, Mikhail Tskhakaya, Mikhail Frunze, Grigory Petrovsky, and Aleksandr Chervyakov, on 30 December 1922. The formal proclamation was made from the stage of the Bolshoi Theatre.
On 1 February 1924, the USSR was recognized by the British Empire. The same year, a Soviet Constitution was approved, legitimizing the December 1922 union.
An intensive restructuring of the economy, industry and politics of the country began in the early days of Soviet power in 1917. A large part of this was done according to the Bolshevik Initial Decrees, government documents signed by Vladimir Lenin. One of the most prominent breakthroughs was the GOELRO plan, which envisioned a major restructuring of the Soviet economy based on total electrification of the country. The plan was developed in 1920 and covered a 10 to 15-year period. It included construction of a network of 30 regional power plants, including ten large hydroelectric power plants, and numerous electric-powered large industrial enterprises. The plan became the prototype for subsequent Five-Year Plans and was fulfilled by 1931.
Stalin era.
From its creation, the government in the Soviet Union was based on the one-party rule of the Communist Party (Bolsheviks). After the economic policy of "War Communism" during the Russian Civil War, as a prelude to fully developing socialism in the country, the Soviet government permitted some private enterprise to coexist alongside nationalized industry in the 1920s and total food requisition in the countryside was replaced by a food tax (see New Economic Policy).
The stated purpose of the one-party state was to ensure that capitalist exploitation would not return to the Soviet Union and that the principles of Democratic Centralism would be most effective in representing the people's will in a practical manner. Debate over the future of the economy provided the background for a power struggle in the years after Lenin's death in 1924. Initially, Lenin was to be replaced by a "troika" consisting of Grigory Zinoviev of Ukraine, Lev Kamenev of Moscow, and Joseph Stalin of Georgia.
On 3 April 1922, Stalin was named the General Secretary of the Communist Party of the Soviet Union. Lenin had appointed Stalin the head of the Workers' and Peasants' Inspectorate, which gave Stalin considerable power. By gradually consolidating his influence and isolating and outmaneuvering his rivals within the party, Stalin became the undisputed leader of the Soviet Union and, by the end of the 1920s, established totalitarian rule. In October 1927, Grigory Zinoviev and Leon Trotsky were expelled from the Central Committee and forced into exile.
In 1928, Stalin introduced the First Five-Year Plan for building a socialist economy. In place of the internationalism expressed by Lenin throughout the Revolution, it aimed to build socialism in one country. In industry, the state assumed control over all existing enterprises and undertook an intensive program of industrialization. In agriculture, rather than adhering to the "lead by example" policy advocated by Lenin, forced collectivisation of farms was implemented all over the country.
Famines ensued, causing millions of deaths; surviving kulaks were persecuted and many sent to Gulags to do forced labour. Social upheaval continued in the mid-1930s. Stalin's Great Purge resulted in the execution or detainment of many "Old Bolsheviks" who had participated in the October Revolution with Lenin. According to declassified Soviet archives, in 1937 and 1938, the NKVD arrested more than one and a half million people, of whom 681,692 were shot. Over those two years that averages to over one thousand executions a day. According to historian Geoffrey Hosking, "...excess deaths during the 1930s as a whole were in the range of 10–11 million." Yet despite the turmoil of the mid-to-late 1930s, the Soviet Union developed a powerful industrial economy in the years before World War II.
1930s.
The early 1930s saw closer cooperation between the West and the USSR. From 1932 to 1934, the Soviet Union participated in the World Disarmament Conference. In 1933, diplomatic relations between the United States and the USSR were established when in November, the newly elected President of the United States, Franklin D. Roosevelt chose to formally recognize Stalin's Communist government and negotiated a new trade agreement between the two nations. In September 1934, the Soviet Union joined the League of Nations. After the Spanish Civil War broke out in 1936, the USSR actively supported the Republican forces against the Nationalists, who were supported by Fascist Italy and Nazi Germany.
In December 1936, Stalin unveiled a new Soviet Constitution. The constitution was seen as a personal triumph for Stalin, who By contrast, Western historians and historians from former Soviet occupied countries have viewed the constitution as a meaningless propaganda document.
The late 1930s saw a shift towards the Axis powers. In 1939, almost a year after the United Kingdom and France had concluded the Munich Agreement with Germany, the USSR dealt with the Nazis as well, both militarily and economically during extensive talks. The two countries concluded the German–Soviet Nonaggression Pact and the German–Soviet Commercial Agreement in August 1939. The nonaggression pact made possible Soviet occupation of Lithuania, Latvia, Estonia, Bessarabia, northern Bukovina, and eastern Poland. In late November of the same year, unable to coerce the Republic of Finland by diplomatic means into moving its border 25 km back from Leningrad, Joseph Stalin ordered the invasion of Finland.
In the east, the Soviet military won several decisive victories during border clashes with the Japanese Empire in 1938 and 1939. However, in April 1941, USSR signed the Soviet–Japanese Neutrality Pact with the Empire of Japan, recognizing the territorial integrity of Manchukuo, a Japanese puppet state.
World War II.
Although it has been debated whether the Soviet Union intended to invade Germany once it was strong enough, Germany itself broke the treaty and invaded the Soviet Union on 22 June 1941, starting what was known in the USSR as the "Great Patriotic War". The Red Army stopped the seemingly invincible German Army at the Battle of Moscow, aided by an unusually harsh winter. The Battle of Stalingrad, which lasted from late 1942 to early 1943, dealt a severe blow to the Germans from which they never fully recovered and became a turning point in the war. After Stalingrad, Soviet forces drove through Eastern Europe to Berlin before Germany surrendered in 1945. The German Army suffered 80% of its military deaths in the Eastern Front.
The same year, the USSR, in fulfillment of its agreement with the Allies at the Yalta Conference, denounced the Soviet–Japanese Neutrality Pact in April 1945 and invaded Manchukuo and other Japan-controlled territories on 9 August 1945. This conflict ended with a decisive Soviet victory, contributing to the unconditional surrender of Japan and the end of World War II.
The Soviet Union suffered greatly in the war, losing around 27 million people. Despite this, it emerged as a superpower in the post-war period. Once denied diplomatic recognition by the Western world, the Soviet Union had official relations with practically every nation by the late 1940s. A member of the United Nations at its foundation in 1945, the Soviet Union became one of the five permanent members of the UN Security Council, which gave it the right to veto any of its resolutions (see Soviet Union and the United Nations).
The Soviet Union maintained its status as one of the world's two superpowers for four decades through its hegemony in Eastern Europe, military strength, economic strength, aid to developing countries, and scientific research, especially in space technology and weaponry.
Cold War.
During the immediate postwar period, the Soviet Union rebuilt and expanded its economy, while maintaining its strictly centralized control. It aided post-war reconstruction in the countries of Eastern Europe, while turning them into satellite states, binding them in a military alliance (the Warsaw Pact) in 1955, and an economic organization (The Council for Mutual Economic Assistance or Comecon) from 1949 to 1991, the latter a counterpart to the European Economic Community. Later, the Comecon supplied aid to the eventually victorious Chinese Communist Party, and saw its influence grow elsewhere in the world. Fearing its ambitions, the Soviet Union's wartime allies, the United Kingdom and the United States, became its enemies. In the ensuing Cold War, the two sides clashed indirectly using mostly proxies.
Khrushchev era.
Stalin died on 5 March 1953. Without a mutually agreeable successor, the highest Communist Party officials opted to rule the Soviet Union jointly. Nikita Khrushchev, who had won the power struggle by the mid-1950s, denounced Stalin's use of repression in 1956 and eased repressive controls over party and society. This was known as de-Stalinization.
Moscow considered Eastern Europe to be a buffer zone for the forward defense of its western borders, and ensured its control of the region by transforming the Eastern European countries into satellite states. Soviet military force was used to suppress anti-Stalinist uprisings in Hungary and Poland in 1956.
In the late 1950s, a confrontation with China regarding the USSR's rapprochement with the West and what Mao Zedong perceived as Khrushchev's revisionism led to the Sino–Soviet split. This resulted in a break throughout the global Marxist–Leninist movement, with the governments in Albania, Cambodia and Somalia choosing to ally with China in place of the USSR.
During this period of the late 1950s and early 1960s, the Soviet Union continued to realize scientific and technological exploits in the space race, rivaling the United States: launching the first artificial satellite, Sputnik 1 in 1957; a living dog named Laika in 1957; the first human being, Yuri Gagarin in 1961; the first woman in space, Valentina Tereshkova in 1963; Alexey Leonov, the first person to walk in space in 1965; the first soft landing on the moon by spacecraft Luna 9 in 1966 and the first moon rovers, Lunokhod 1 and Lunokhod 2.
Khrushchev initiated "The Thaw" (better known as Khrushchev's Thaw), a complex shift in political, cultural and economic life in the Soviet Union. This included some openness and contact with other nations and new social and economic policies with more emphasis on commodity goods, allowing living standards to rise dramatically while maintaining high levels of economic growth. Censorship was relaxed as well.
Khrushchev's reforms in agriculture and administration, however, were generally unproductive. In 1962, he precipitated a crisis with the United States over the Soviet deployment of nuclear missiles in Cuba. An agreement was made between the Soviet Union and the United States to remove enemy nuclear missiles from both Cuba and Turkey, concluding the crisis. This event caused Khrushchev much embarrassment and loss of prestige, resulting in his removal from power in 1964.
Brezhnev era.
Following the ousting of Khrushchev, another period of collective leadership ensued, consisting of Leonid Brezhnev as General Secretary, Alexei Kosygin as Premier and Nikolai Podgorny as Chairman of the Presidium, lasting until Brezhnev established himself in the early 1970s as the preeminent Soviet leader. In 1968, the Soviet Union and Warsaw Pact allies invaded Czechoslovakia to halt the Prague Spring reforms.
Brezhnev presided over a period of "détente" with the West (see SALT I, SALT II, Anti-Ballistic Missile Treaty) while at the same time building up Soviet military might.
In October 1977, the third Soviet Constitution was unanimously adopted. The prevailing mood of the Soviet leadership at the time of Brezhnev's death in 1982 was one of aversion to change. The long period of Brezhnev's rule had come to be dubbed one of "standstill", with an aging and ossified top political leadership.
Gorbachev era.
Two developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. Kenneth S. Deffeyes argued in "Beyond Oil" that the Reagan administration encouraged Saudi Arabia to lower the price of oil to the point where the Soviets could not make a profit selling their oil, so that the USSR's hard currency reserves became depleted.
Brezhnev's next two successors, transitional figures with deep roots in his tradition, did not last long. Yuri Andropov was 68 years old and Konstantin Chernenko 72 when they assumed power; both died in less than two years. In an attempt to avoid a third short-lived leader, in 1985, the Soviets turned to the next generation and selected Mikhail Gorbachev.
Gorbachev made significant changes in the economy and party leadership, called "perestroika". His policy of "glasnost" freed public access to information after decades of heavy government censorship.
Gorbachev also moved to end the Cold War. In 1988, the Soviet Union abandoned its nine-year war in Afghanistan and began to withdraw its forces. In the late 1980s, , which favored the Revolutions of 1989. With the tearing down of the Berlin Wall and with East Germany and West Germany pursuing unification, the Iron Curtain came down.
In the late 1980s, the constituent republics of the Soviet Union started legal moves towards potentially declaring sovereignty over their territories, citing Article 72 of the USSR constitution, which stated that any constituent republic was free to secede. On 7 April 1990, a law was passed allowing a republic to secede if more than two-thirds of its residents voted for it in a referendum. Many held their first free elections in the Soviet era for their own national legislatures in 1990. Many of these legislatures proceeded to produce legislation contradicting the Union laws in what was known as the "War of Laws".
In 1989, the Russian SFSR, which was then the largest constituent republic (with about half of the population) convened a newly elected Congress of People's Deputies. Boris Yeltsin was elected its chairman. On 12 June 1990, the Congress declared Russia's sovereignty over its territory and proceeded to pass laws that attempted to supersede some of the USSR's laws. After a landslide victory of Sąjūdis in Lithuania, that country declared its independence restored on 11 March 1990.
A referendum for the preservation of the USSR was held on 17 March 1991 in nine republics (the remainder having boycotted the vote), with the majority of the population in those nine republics voting for preservation of the Union. The referendum gave Gorbachev a minor boost. In the summer of 1991, the New Union Treaty, which would have turned the Soviet Union into a much looser Union, was agreed upon by eight republics.
The signing of the treaty, however, was interrupted by the August Coup—an attempted coup d'état by hardline members of the government and the KGB who sought to reverse Gorbachev's reforms and reassert the central government's control over the republics. After the coup collapsed, Yeltsin was seen as a hero for his decisive actions, while Gorbachev's power was effectively ended. The balance of power tipped significantly towards the republics. In August 1991, Latvia and Estonia immediately declared the restoration of their full independence (following Lithuania's 1990 example). Gorbachev resigned as general secretary in late August, and soon afterward the Party's activities were indefinitely suspended—effectively ending its rule. By the fall, Gorbachev could no longer influence events outside of Moscow, and he was being challenged even there by Yeltsin, who had been elected President of Russia in July 1991.
Dissolution.
The remaining 12 republics continued discussing new, increasingly looser, models of the Union. However, by December, all except Russia and Kazakhstan had formally declared independence. During this time, Yeltsin took over what remained of the Soviet government, including the Kremlin. The final blow was struck on 1 December, when Ukraine, the second most powerful republic, voted overwhelmingly for independence. Ukraine's secession ended any realistic chance of the Soviet Union staying together even on a limited scale.
On 8 December 1991, the presidents of Russia, Ukraine and Belarus (formerly Byelorussia), signed the Belavezha Accords, which declared the Soviet Union dissolved and established the Commonwealth of Independent States (CIS) in its place. While doubts remained over the authority of the accords to do this, on 21 December 1991, the representatives of all Soviet republics except Georgia signed the Alma-Ata Protocol, which confirmed the accords. On 25 December 1991, Gorbachev resigned as the President of the USSR, declaring the office extinct. He turned the powers that had been vested in the presidency over to Yeltsin. That night, the Soviet flag was lowered for the last time, and the Russian tricolor was raised in its place.
The following day, the Supreme Soviet, the highest governmental body of the Soviet Union, voted both itself and the Soviet Union out of existence. This is generally recognized as marking the official, final dissolution of the Soviet Union as a functioning state. The Soviet Army originally remained under overall CIS command, but was soon absorbed into the different military forces of the newly independent states. The few remaining Soviet institutions that had not been taken over by Russia ceased to function by the end of 1991.
Following the dissolution of the Soviet Union on 26 December 1991, Russia was internationally recognized as its legal successor on the international stage. To that end, Russia voluntarily accepted all Soviet foreign debt and claimed overseas Soviet properties as its own. Under the 1992 Lisbon Protocol, Russia also agreed to receive all nuclear weapons remaining in the territory of other former Soviet republics. Since then, the Russian Federation has assumed the Soviet Union's rights and obligations.
Post-Soviet states.
The analysis of the succession of states with respect to the 15 post-Soviet states is complex. The Russian Federation is seen as the legal "continuator" state and is for most purposes the heir to the Soviet Union. It retained ownership of all former Soviet embassy properties, as well as the old Soviet UN membership and permanent membership on the Security Council. The Baltic states are not successor states to the Soviet Union; they are instead considered to have de jure continuity with their pre-World War II governments through the non-recognition of the original Soviet incorporation in 1940. The other 11 post-Soviet states are considered newly-independent successor states to the Soviet Union.
There are additionally four states that claim independence from the other internationally recognized post-Soviet states, but possess limited international recognition: Abkhazia, Nagorno-Karabakh, South Ossetia, and Transnistria. The Chechen separatist movement of the Chechen Republic of Ichkeria lacks any international recognition.
Politics.
There were three power hierarchies in the Soviet Union: the legislative branch represented by the Supreme Soviet of the Soviet Union, the government represented by the Council of Ministers, and the Communist Party of the Soviet Union (CPSU), the only legal party and the ultimate policymaker in the country.
Communist Party.
At the top of the Communist Party was the Central Committee, elected at Party Congresses and Conferences. The Central Committee in turn voted for a Politburo (called the Presidium between 1952–1966), Secretariat and the General Secretary (First Secretary from 1953 to 1966), the de facto highest office in the USSR. Depending on the degree of power consolidation, it was either the Politburo as a collective body or the General Secretary, who always was one of the Politburo members, that effectively led the party and the country (except for the period of the highly personalized authority of Stalin, exercised directly through his position in the Council of Ministers rather than the Politburo after 1941). They were not controlled by the general party membership, as the key principle of the party organization was democratic centralism, demanding strict subordination to higher bodies, and elections went uncontested, endorsing the candidates proposed from above.
The Communist Party maintained its dominance over the state largely through its control over the system of appointments. All senior government officials and most deputies of the Supreme Soviet were members of the CPSU. Of the party heads themselves, Stalin in 1941–1953 and Khrushchev in 1958–1964 were Premiers. Upon the forced retirement of Khrushchev, the party leader was prohibited from this kind of double membership, but the later General Secretaries for at least some part of their tenure occupied the largely ceremonial position of Chairman of the Presidium of the Supreme Soviet, the nominal head of state. The institutions at lower levels were overseen and at times supplanted by primary party organizations.
In practice, however, the degree of control the party was able to exercise over the state bureaucracy, particularly after the death of Stalin, was far from total, with the bureaucracy pursuing different interests that were at times in conflict with the party. Nor was the party itself monolithic from top to bottom, although factions were officially banned.
Government.
The Supreme Soviet (successor of the Congress of Soviets and Central Executive Committee) was nominally the highest state body for most of the Soviet history, at first acting as a rubber stamp institution, approving and implementing all decisions made by the party. However, the powers and functions of the Supreme Soviet were extended in the late 1950s, 1960s and 1970s, including the creation of new state commissions and committees. It gained additional powers when it came to the approval of the Five-Year Plans and the Soviet state budget. The Supreme Soviet elected a Presidium to wield its power between plenary sessions, ordinarily held twice a year, and appointed the Supreme Court, the Procurator General and the Council of Ministers (known before 1946 as the Council of People's Commissars), headed by the Chairman (Premier) and managing an enormous bureaucracy responsible for the administration of the economy and society. State and party structures of the constituent republics largely emulated the structure of the central institutions, although the Russian SFSR, unlike the other constituent republics, for most of its history had no republican branch of the CPSU, being ruled directly by the union-wide party until 1990. Local authorities were organized likewise into party committees, local Soviets and executive committees. While the state system was nominally federal, the party was unitary.
The state security police (the KGB and its predecessor agencies) played an important role in Soviet politics. It was instrumental in the Stalinist terror, but after the death of Stalin, the state security police was brought under strict party control. Under Yuri Andropov, KGB chairman in 1967–1982 and General Secretary from 1982 to 1983, the KGB engaged in the suppression of political dissent and maintained an extensive network of informers, reasserting itself as a political actor to some extent independent of the party-state structure, culminating in the anti-corruption campaign targeting high party officials in the late 1970s and early 1980s.
Separation of power and reform.
The Union constitutions, which were promulgated in 1918, 1924, 1936 and 1977, did not limit state power. No formal separation of powers existed between the Party, Supreme Soviet and Council of Ministers that represented executive and legislative branches of the government. The system was governed less by statute than by informal conventions, and no settled mechanism of leadership succession existed. Bitter and at times deadly power struggles took place in the Politburo after the deaths of Lenin and Joseph Stalin, as well as after Khrushchev's dismissal, itself due to a decision by both the Politburo and the Central Committee. All leaders of the Communist Party before Gorbachev died in office, except Georgy Malenkov and Khrushchev, both dismissed from the party leadership amid internal struggle within the party.
Between 1988 and 1990, facing considerable opposition, Mikhail Gorbachev enacted reforms shifting power away from the highest bodies of the party and making the Supreme Soviet less dependent on them. The Congress of People's Deputies was established, the majority of whose members were directly elected in competitive elections held in March 1989. The Congress now elected the Supreme Soviet, which became a full-time parliament, much stronger than before. For the first time since the 1920s, it refused to rubber stamp proposals from the party and Council of Ministers. In 1990, Gorbachev introduced and assumed the position of the President of the Soviet Union, concentrated power in his executive office, independent of the party, and subordinated the government, now renamed the Cabinet of Ministers of the USSR, to himself.
Tensions grew between the union-wide authorities under Gorbachev, reformists led in Russia by Boris Yeltsin and controlling the newly elected Supreme Soviet of the Russian SFSR, and Communist Party hardliners. On 19–21 August 1991, a group of hardliners staged an abortive coup attempt. Following the failed coup, the State Council of the Soviet Union became the highest organ of state power "in the period of transition". Gorbachev resigned as General Secretary, only remaining President for the final months of the existence of the USSR.
Judicial system.
The judiciary was not independent of the other branches of government. The Supreme Court supervised the lower courts (People's Court) and applied the law as established by the Constitution or as interpreted by the Supreme Soviet. The Constitutional Oversight Committee reviewed the constitutionality of laws and acts. The Soviet Union used the inquisitorial system of Roman law, where the judge, procurator, and defense attorney collaborate to establish the truth.
Administrative divisions.
Constitutionally, the USSR was a federation of constituent Union Republics, which were either unitary states, such as Ukraine or Belarus (SSRs), or federal states, such as Russia or Transcaucasia (SFSRs), all four being the founding republics who signed the Treaty on the Creation of the USSR in December 1922. In 1924, during the national delimitation in Central Asia, the Uzbek and Turkmen SSRs were formed from parts of the Russia's Turkestan ASSR and two Soviet dependencies, the Khorezm and Bukharan SSRs. In 1929, the Tajik SSR was split off from the Uzbek SSR. With the constitution of 1936, the Transcaucasian SFSR was dissolved, resulting in its constituent Georgian, Armenian and Azerbaijan SSRs being elevated to Union Republics, while the Kazakh and Kirghiz SSRs were split off from Russian SFSR, resulting in the same status. In August 1940, the Moldavian SSR was formed from parts of the Ukrainian SSR and Bessarabia and Northern Bukovina. The Estonian, Latvian and Lithuanian SSRs were also admitted into the union. The Karelo-Finnish SSR was split off from Russia as a Union Republic in March 1940 and was reabsorbed in 1956. Between July 1956 and September 1991, there were 15 union republics (see map below). The RSFSR was by far the largest republic, in both population and geography, as well as the strongest and most developed economically due to its vast natural resources. Additionally, it was the most powerful politically; Russians dominated the state and party apparatuses, and all but one undisputed leader of the Soviet Union was a Russian (the only one who wasn't, Stalin, was a Russified Georgian). For these reasons, until the 1980s the Soviet Union was commonly—but incorrectly—referred to as "Russia."
Economy.
The Soviet Union became the first country to adopt a planned economy, whereby production and distribution of goods were centralised and directed by the government. The first Bolshevik experience with a command economy was the policy of War Communism, which involved the nationalization of industry, centralized distribution of output, coercive requisition of agricultural production, and attempts to eliminate the circulation of money, as well as private enterprises and free trade. After the severe economic collapse caused by the war, Lenin replaced War Communism with the New Economic Policy (NEP) in 1921, legalising free trade and private ownership of smaller businesses. The economy quickly recovered.
Following a lengthy debate among the members of Politburo over the course of economic development, by 1928–1929, upon gaining control of the country, Joseph Stalin abandoned the NEP and pushed for full central planning, starting forced collectivisation of agriculture and enacting draconian labor legislation. Resources were mobilised for rapid industrialisation, which greatly expanded Soviet capacity in heavy industry and capital goods during the 1930s. Preparation for war was one of the main driving forces behind industrialisation, mostly due to distrust of the outside capitalistic world. As a result, the USSR was transformed from a largely agrarian economy into a great industrial power, leading the way for its emergence as a superpower after World War II. During the war, the Soviet economy and infrastructure suffered massive devastation and required extensive reconstruction.
By the early 1940s, the Soviet economy had become relatively self-sufficient; for most of the period until the creation of Comecon, only a very small share of domestic products was traded internationally. After the creation of the Eastern Bloc, external trade rose rapidly. Still the influence of the world economy on the USSR was limited by fixed domestic prices and a state monopoly on foreign trade. Grain and sophisticated consumer manufactures became major import articles from around the 1960s. During the arms race of the Cold War, the Soviet economy was burdened by military expenditures, heavily lobbied for by a powerful bureaucracy dependent on the arms industry. At the same time, the Soviet Union became the largest arms exporter to the Third World. Significant amounts of Soviet resources during the Cold War were allocated in aid to the other socialist states.
From the 1930s until its collapse in the late 1980s, the way the Soviet economy operated remained essentially unchanged. The economy was formally directed by central planning, carried out by Gosplan and organized in five-year plans. In practice, however, the plans were highly aggregated and provisional, subject to "ad hoc" intervention by superiors. All key economic decisions were taken by the political leadership. Allocated resources and plan targets were normally denominated in rubles rather than in physical goods. Credit was discouraged, but widespread. Final allocation of output was achieved through relatively decentralized, unplanned contracting. Although in theory prices were legally set from above, in practice the actual prices were often negotiated, and informal horizontal links (between producer factories etc.) were widespread.
A number of basic services were state-funded, such as education and healthcare. In the manufacturing sector, heavy industry and defense were assigned higher priority than the production of consumer goods. Consumer goods, particularly outside large cities, were often scarce, of poor quality and limited choice. Under command economy, consumers had almost no influence over production, so the changing demands of a population with growing incomes could not be satisfied by supplies at rigidly fixed prices. A massive unplanned second economy grew up alongside the planned one at low levels, providing some of the goods and services that the planners could not. Legalisation of some elements of the decentralised economy was attempted with the reform of 1965.
Although statistics of the Soviet economy are notoriously unreliable and its economic growth difficult to estimate precisely, by most accounts, the economy continued to expand until the mid-1980s. During the 1950s and 1960s, the Soviet economy experienced comparatively high growth and was catching up to the West. However, after 1970, the growth, while still positive, steadily declined much more quickly and consistently than in other countries despite a rapid increase in the capital stock (the rate of increase in capital was only surpassed by Japan).
Overall, between 1960 and 1989, the growth rate of per capita income in the Soviet Union was slightly above the world average (based on 102 countries). According to Stanley Fischer and William Easterly, growth could have been faster. By their calculation, per capita income of Soviet Union in 1989 should have been twice as high as it was considering the amount of investment, education and population. The authors attribute this poor performance to low productivity of capital in the Soviet Union. Steven Rosenfielde states that the standard of living actually declined as a result of Stalin's despotism, and while there was a brief improvement following his death, lapsed into stagnation.
In 1987, Mikhail Gorbachev tried to reform and revitalize the economy with his program of "perestroika". His policies relaxed state control over enterprises, but did not yet allow it to be replaced by market incentives, ultimately resulting in a sharp decline in production output. The economy, already suffering from reduced petroleum export revenues, started to collapse. Prices were still fixed, and property was still largely state-owned until after the dissolution of the Soviet Union. For most of the period after World War II up to its collapse, the Soviet economy was the second largest in the world by GDP (PPP), and was 3rd in the world during the middle of the 1980s to 1989, though in per capita terms the Soviet GDP was behind that of the First World countries.
Energy.
The need for fuel declined in the Soviet Union from the 1970s to the 1980s, both per ruble of gross social product and per ruble of industrial product. At the start, this decline grew very rapidly but gradually slowed down between 1970 and 1975. From 1975 and 1980, it grew even slower, only 2.6 percent. David Wilson, a historian, believed that the gas industry would account for 40 percent of Soviet fuel production by the end of the century. His theory did not come to fruition because of the USSR's collapse. The USSR, in theory, would have continued to have an economic growth rate of 2–2.5 percent during the 1990s because of Soviet energy fields. However, the energy sector faced many difficulties, among them the country's high military expenditure and hostile relations with the First World (pre-Gorbachev era).
In 1991, the Soviet Union had a pipeline network of 82000 km for crude oil and another 206500 km for natural gas. Petroleum and petroleum-based products, natural gas, metals, wood, agricultural products, and a variety of manufactured goods, primarily machinery, arms and military equipment, were exported. In the 1970s and 1980s, the Soviet Union heavily relied on fossil fuel exports to earn hard currency. At its peak in 1988, it was the largest producer and second largest exporter of crude oil, surpassed only by Saudi Arabia.
Science and technology.
The Soviet Union placed great emphasis on science and technology within its economy, however, the most remarkable Soviet successes in technology, such as producing the world's first space satellite, typically were the responsibility of the military. Lenin believed that the USSR would never overtake the developed world if it remained as technologically backward as it was upon its founding. Soviet authorities proved their commitment to Lenin's belief by developing massive networks, research and development organizations. In the early 1960s, the Soviets awarded 40% of chemistry PhD's to women, compared to only 5% who received such a degree in the United States. By 1989, Soviet scientists were among the world's best-trained specialists in several areas, such as energy physics, selected areas of medicine, mathematics, welding and military technologies. Due to rigid state planning and bureaucracy, the Soviets remained far behind technologically in chemistry, biology, and computers when compared to the First World.
Project Socrates, under the Reagan administration, determined that the Soviet Union addressed the acquisition of science and technology in a manner that was radically different from what the US was using. In the case of the US, economic prioritization was being used for indigenous research and development as the means to acquire science and technology in both the private and public sectors. In contrast, the Soviet Union was offensively and defensively maneuvering in the acquisition and utilization of the worldwide technology, to increase the competitive advantage that they acquired from the technology, while preventing the US from acquiring a competitive advantage. However, in addition, the Soviet Union's technology-based planning was executed in a centralized, government-centric manner that greatly hindered its flexibility. It was this significant lack of flexibility that was exploited by the US to undermine the strength of the Soviet Union and thus foster its reform.
Transport.
Transport was a key component of the nation's economy. The economic centralization of the late 1920s and 1930s led to the development of infrastructure on a massive scale, most notably the establishment of Aeroflot, an aviation enterprise. The country had a wide variety of modes of transport by land, water and air. However, due to bad maintenance, much of the road, water and Soviet civil aviation transport were outdated and technologically backward compared to the First World.
Soviet rail transport was the largest and most intensively used in the world; it was also better developed than most of its Western counterparts. By the late 1970s and early 1980s, Soviet economists were calling for the construction of more roads to alleviate some of the burden from the railways and to improve the Soviet state budget. The road network and automobile industry remained underdeveloped, and dirt roads were common outside major cities. Soviet maintenance projects proved unable to take care of even the few roads the country had. By the early-to-mid-1980s, the Soviet authorities tried to solve the road problem by ordering the construction of new ones. Meanwhile, the automobile industry was growing at a faster rate than road construction. The underdeveloped road network led to a growing demand for public transport.
Despite improvements, several aspects of the transport sector were still riddled with problems due to outdated infrastructure, lack of investment, corruption and bad decision-making. Soviet authorities were unable to meet the growing demand for transport infrastructure and services.
The Soviet merchant fleet was one of the largest in the world.
Demographics.
Excess deaths over the course of World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million, some 10 million in the 1930s, and more than 26 million in 1941–5. The postwar Soviet population was 45 to 50 million smaller than it would have been if pre-war demographic growth had continued. According to Catherine Merridale, "... reasonable estimate would place the total number of excess deaths for the whole period somewhere around 60 million."
The crude birth rate of the USSR decreased from 44.0 per thousand in 1926 to 18.0 in 1974, largely due to increasing urbanization and the rising average age of marriages. The crude death rate demonstrated a gradual decrease as well – from 23.7 per thousand in 1926 to 8.7 in 1974. In general, the birth rates of the southern republics in Transcaucasia and Central Asia were considerably higher than those in the northern parts of the Soviet Union, and in some cases even increased in the post–World War II period, a phenomenon partly attributed to slower rates of urbanization and traditionally earlier marriages in the southern republics. Soviet Europe moved towards sub-replacement fertility, while Soviet Central Asia continued to exhibit population growth well above replacement-level fertility.
The late 1960s and the 1970s witnessed a reversal of the declining trajectory of the rate of mortality in the USSR, and was especially notable among men of working age, but was also prevalent in Russia and other predominantly Slavic areas of the country. An analysis of the official data from the late 1980s showed that after worsening in the late-1970s and the early 1980s, adult mortality began to improve again. The infant mortality rate increased from 24.7 in 1970 to 27.9 in 1974. Some researchers regarded the rise as largely real, a consequence of worsening health conditions and services. The rises in both adult and infant mortality were not explained or defended by Soviet officials, and the Soviet government simply stopped publishing all mortality statistics for ten years. Soviet demographers and health specialists remained silent about the mortality increases until the late-1980s, when the publication of mortality data resumed and researchers could delve into the real causes.
Education.
Before 1917, education was not free in the Russian Empire and was therefore either inaccessible or barely accessible for many children from lower-class working and peasant families. Estimates from 1917 recorded that 75–85 percent of the Russian population was illiterate.
Anatoly Lunacharsky became the first People's Commissar for Education of Soviet Russia. At the beginning, the Soviet authorities placed great emphasis on the elimination of illiteracy. People who were literate were automatically hired as teachers. For a short period, quality was sacrificed for quantity. By 1940, Joseph Stalin could announce that illiteracy had been eliminated. Throughout the 1930s social mobility rose sharply, which has been attributed to Soviet reforms in education. In the aftermath of the Great Patriotic War, the country's educational system expanded dramatically. This expansion had a tremendous effect. In the 1960s, nearly all Soviet children had access to education, the only exception being those living in remote areas. Nikita Khrushchev tried to make education more accessible, making it clear to children that education was closely linked to the needs of society. Education also became important in giving rise to the New Man.
The country's system of education was highly centralized and universally accessible to all citizens, with affirmative action for applicants from nations associated with cultural backwardness. Citizens directly entering the work force had the constitutional right to a job and to free vocational training. The Brezhnev administration introduced a rule that required all university applicants to present a reference from the local Komsomol party secretary. According to statistics from 1986, the number of higher education students per the population of 10,000 was 181 for the USSR, compared to 517 for the U.S.
Ethnic groups.
The Soviet Union was a very ethnically diverse country, with more than 100 distinct ethnic groups. The total population was estimated at 293 million in 1991. According to a 1990 estimate, the majority were Russians (50.78%), followed by Ukrainians (15.45%) and Uzbeks (5.84%).
All citizens of the USSR had their own ethnic affiliation. The ethnicity of a person was chosen at the age of sixteen by the child's parents. If the parents did not agree, the child was automatically assigned the ethnicity of the father. Partly due to Soviet policies, some of the smaller minority ethnic groups were considered part of larger ones, such as the Mingrelians of the Georgian SSR, who were classified with the linguistically related Georgians. Some ethnic groups voluntarily assimilated, while others were brought in by force. Russians, Belarusians, and Ukrainians shared close cultural ties, while other groups did not. With multiple nationalities living in the same territory, ethnic antagonisms developed over the years.
Health.
In 1917, before the revolution, health conditions were significantly behind the developed countries. As Lenin later noted, "Either the lice will defeat socialism, or socialism will defeat the lice". The Soviet principle of health care was conceived by the People's Commissariat for Health in 1918. Health care was to be controlled by the state and would be provided to its citizens free of charge, this at the time being a revolutionary concept. Article 42 of the 1977 Soviet Constitution gave all citizens the right to health protection and free access to any health institutions in the USSR. Before Leonid Brezhnev became head of state, the healthcare system of the Soviet Union was held in high esteem by many foreign specialists. This changed however, from Brezhnev's accession and Mikhail Gorbachev's tenure as leader, the Soviet health care system was heavily criticised for many basic faults, such as the quality of service and the unevenness in its provision. Minister of Health Yevgeniy Chazov, during the 19th Congress of the Communist Party of the Soviet Union, while highlighting such Soviet successes as having the most doctors and hospitals in the world, recognised the system's areas for improvement and felt that billions of Soviet rubles were squandered. 
After the socialist revolution, the life expectancy for all age groups went up. This statistic in itself was seen by some that the socialist system was superior to the capitalist system. These improvements continued into the 1960s, when the life expectancy in the Soviet Union surpassed that of the United States. It remained stable during most years, although in the 1970s, it went down slightly, possibly because of alcohol abuse. At the same time, infant mortality began to rise. After 1974, the government stopped publishing statistics on this. This trend can be partly explained by the number of pregnancies rising drastically in the Asian part of the country where infant mortality was highest, while declining markedly in the more developed European part of the Soviet Union. The USSR had several centers of excellence, such as the Fyodorov Eye Microsurgery Complex, founded in 1988 by Russian eye surgeon Svyatoslav Fyodorov.
Language.
The Soviet government headed by Vladimir Lenin gave small language groups their own writing systems. The development of these writing systems was very successful, even though some flaws were detected. During the later days of the USSR, countries with the same multilingual situation implemented similar policies. A serious problem when creating these writing systems was that the languages differed dialectally greatly from each other. When a language had been given a writing system and appeared in a notable publication, that language would attain "official language" status. There were many minority languages which never received their own writing system; therefore their speakers were forced to have a second language. There are examples where the Soviet government retreated from this policy, most notable under Stalin's regime, where education was discontinued in languages which were not widespread enough. These languages were then assimilated into another language, mostly Russian. During the Great Patriotic War (World War II), some minority languages were banned, and their speakers accused of collaborating with the enemy.
As the most widely spoken of the Soviet Union's many languages, Russian "de facto" functioned as an official language, as the "language of interethnic communication" (Russian: язык межнационального общения), but only assumed the "de jure" status as the official national language in 1990.
Religion.
The religious made up a significant minority of the Soviet Union prior to break up. In 1990, the religious makeup was 20% Russian Orthodox, 10% Muslim, 7% Protestant, Georgian Orthodox, Armenian Orthodox, and Roman Catholic, less than 1% Jewish and 60% atheist.
Christianity and Islam had the greatest number of adherents among the Soviet state's religious citizens. Eastern Christianity predominated among Christians, with Russia's traditional Russian Orthodox Church being the Soviet Union's largest Christian denomination. About 90 percent of the Soviet Union's Muslims were Sunnis, with Shiites concentrated in the Azerbaijani Soviet Socialist Republic. Smaller groups included Roman Catholics, Jews, Buddhists, and a variety of Protestant sects.
Religious influence had been strong in the Russian Empire. The Russian Orthodox Church enjoyed a privileged status as the church of the monarchy and took part in carrying out official state functions. The immediate period following the establishment of the Soviet state included a struggle against the Orthodox Church, which the revolutionaries considered an ally of the former ruling classes.
In Soviet law, the "freedom to hold religious services" was constitutionally guaranteed, although the ruling Communist Party regarded religion as incompatible with the Marxist spirit of scientific materialism. In practice, the Soviet system subscribed to a narrow interpretation of this right, and in fact utilized a range of official measures to discourage religion and curb the activities of religious groups.
The 1918 Council of People's Commissars decree establishing the Russian Soviet Federative Socialist Republic (RSFSR) as a secular state also decreed that "the teaching of religion in all [places] where subjects of general instruction are taught, is forbidden. Citizens may teach and may be taught religion privately." Among further restrictions, those adopted in 1929, a half-decade into Stalin's rule, included express prohibitions on a range of church activities, including meetings for organized Bible study. Both Christian and non-Christian establishments were shut down by the thousands in the 1920s and 1930s. By 1940, as many as 90 percent of the churches, synagogues, and mosques that had been operating in 1917 were closed.
Convinced that religious anti-Sovietism had become a thing of the past, the Stalin regime began shifting to a more moderate religion policy in the late 1930s. Soviet religious establishments overwhelmingly rallied to support the war effort during the Soviet war with Nazi Germany. Amid other accommodations to religious faith, churches were reopened, Radio Moscow began broadcasting a religious hour, and a historic meeting between Stalin and Orthodox Church leader Patriarch Sergius I of Moscow was held in 1943. The general tendency of this period was an increase in religious activity among believers of all faiths. The Ukrainian Greek Catholic Church in the USSR was persecuted.
The Soviet establishment again clashed with the churches under General Secretary Nikita Khrushchev's leadership in 1958–1964, a period when atheism was emphasized in the educational curriculum, and numerous state publications promoted atheistic views. During this period, the number of churches fell from 20,000 to 10,000 from 1959 to 1965, and the number of synagogues dropped from 500 to 97. The number of working mosques also declined, falling from 1,500 to 500 within a decade.
Religious institutions remained monitored by the Soviet government, but churches, synagogues, temples, and mosques were all given more leeway in the Brezhnev era. Official relations between the Orthodox Church and the Soviet government again warmed to the point that the Brezhnev government twice honored Orthodox Patriarch Alexy I with the Order of the Red Banner of Labour. A poll conducted by Soviet authorities in 1982 recorded 20 percent of the Soviet population as "active religious believers."
Women.
Soviet efforts to expand social, political and economic opportunities for women constitute "the earliest and perhaps most far-reaching attempt ever undertaken to transform the status and role of women."
Culture.
The culture of the Soviet Union passed through several stages during the USSR's 70-year existence. During the first eleven years following the Revolution (1918–1929), there was relative freedom and artists experimented with several different styles to find a distinctive Soviet style of art. Lenin wanted art to be accessible to the Russian people. On the other hand, hundreds of intellectuals, writers, and artists were exiled or executed, and their work banned, for example Nikolay Gumilev (shot for alleged conspiring against the Bolshevik regime) and Yevgeny Zamyatin (banned).
The government encouraged a variety of trends. In art and literature, numerous schools, some traditional and others radically experimental, proliferated. Communist writers Maksim Gorky and Vladimir Mayakovsky were active during this time. Film, as a means of influencing a largely illiterate society, received encouragement from the state; much of director Sergei Eisenstein's best work dates from this period.
Later, during Stalin's rule, Soviet culture was characterised by the rise and domination of the government-imposed style of socialist realism, with all other trends being severely repressed, with rare exceptions, for example Mikhail Bulgakov's works. Many writers were imprisoned and killed.
Following the Khrushchev Thaw of the late 1950s and early 1960s, censorship was diminished. During this time, a distinctive period of Soviet culture developed characterized by conformist public life and intense focus on personal life. Greater experimentation in art forms were again permissible, with the result that more sophisticated and subtly critical work began to be produced. The regime loosened its emphasis on socialist realism; thus, for instance, many protagonists of the novels of author Yury Trifonov concerned themselves with problems of daily life rather than with building socialism. An underground dissident literature, known as "samizdat", developed during this late period. In architecture the Khrushchev era mostly focused on functional design as opposed to the highly decorated style of Stalin's epoch.
In the second half of the 1980s, Gorbachev's policies of "perestroika" and "glasnost" significantly expanded freedom of expression in the media and press.
Attempt to challenge the dissolution of the Soviet Union in Court.
In 2014, on the initiative of the citizen of the city of Tolyatti, Dmitry Tretyakov, born in 1981, took judicial attempts to challenge the alleged unconstitutional dissolution of the Soviet Union in court. In his claim to the government of Russia, the applicant referred to the legislation of the Soviet Union, "On the order of issues related to the secession of Union republics from the USSR".
On 10 January 2014 the Supreme Court of Russia issued a ruling, which refused to consider the claim, stating that "acts do not affect the rights and freedoms or legitimate interests of the applicant". On 8 April, the appellate court upheld the first instance decision.
On 29 May, the Constitutional Court of Russia with 18 judges, chaired by Valery Zorkin, dismissed the complaint in a final unappealable decision.
On 27 November 2014, the European court of human rights in Strasbourg, under the chairmanship of judge Elisabeth Steiner decided to reject the complaint, additionally stating that the decision cannot be appealed to the Grand chamber.
Further reading.
Specialty studies.
</dl>
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="26781" url="http://en.wikipedia.org/wiki?curid=26781" title="Social science">
Social science

Social science is a major branch of science, and a major category of academic disciplines, concerned with society and the relationships among individuals within a society. It in turn has many branches, each of which is considered a "social science". The main social sciences include economics, political science, human geography, demography and sociology. In a wider sense, social science also includes among its branches some fields in the humanities such as anthropology, archaeology, history, law and linguistics. The term is also sometimes used to refer specifically to the field of sociology, the original 'science of society', established in the 19th century.
Positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining the quantitative and qualitative techniques). The term social research has also acquired a degree of autonomy as practitioners from various disciplines share in its aims and methods.
History.
The history of the social sciences begins in the Age of Enlightenment after 1650, which saw a revolution within natural philosophy, changing the basic framework by which individuals understood what was "scientific". Social sciences came forth from the moral philosophy of the time and was influenced by the Age of Revolutions, such as the Industrial Revolution and the French Revolution. The "social sciences" developed from the sciences (experimental and applied), or the systematic knowledge-bases or prescriptive practices, relating to the social improvement of a group of interacting entities.
The beginnings of the social sciences in the 18th century are reflected in the grand encyclopedia of Diderot, with articles from Rousseau and other pioneers. The growth of the social sciences is also reflected in other specialized encyclopedias. The modern period saw "social science" first used as a distinct conceptual field. Social science was influenced by positivism, focusing on knowledge based on actual positive sense experience and avoiding the negative; metaphysical speculation was avoided. Auguste Comte used the term "science sociale" to describe the field, taken from the ideas of Charles Fourier; Comte also referred to the field as "social physics".
Following this period, there were five paths of development that sprang forth in the social sciences, influenced by Comte on other fields. One route that was taken was the rise of social research. Large statistical surveys were undertaken in various parts of the United States and Europe. Another route undertaken was initiated by Émile Durkheim, studying "social facts", and Vilfredo Pareto, opening metatheoretical ideas and individual theories. A third means developed, arising from the methodological dichotomy present, in which social phenomena were identified with and understood; this was championed by figures such as Max Weber. The fourth route taken, based in economics, was developed and furthered economic knowledge as a hard science. The last path was the correlation of knowledge and social values; the antipositivism and verstehen sociology of Max Weber firmly demanded this distinction. In this route, theory (description) and prescription were non-overlapping formal discussions of a subject.
Around the start of the 20th century, Enlightenment philosophy was challenged in various quarters. After the use of classical theories since the end of the scientific revolution, various fields substituted mathematics studies for experimental studies and examining equations to build a theoretical structure. The development of social science subfields became very quantitative in methodology. The interdisciplinary and cross-disciplinary nature of scientific inquiry into human behavior, social and environmental factors affecting it, made many of the natural sciences interested in some aspects of social science methodology. Examples of boundary blurring include emerging disciplines like social research of medicine, sociobiology, neuropsychology, bioeconomics and the history and sociology of science. Increasingly, quantitative research and qualitative methods are being integrated in the study of human action and its implications and consequences. In the first half of the 20th century, statistics became a free-standing discipline of applied mathematics. Statistical methods were used confidently.
In the contemporary period, Karl Popper and Talcott Parsons influenced the furtherance of the social sciences. Researchers continue to search for a unified consensus on what methodology might have the power and refinement to connect a proposed "grand theory" with the various midrange theories that, with considerable success, continue to provide usable frameworks for massive, growing data banks; for more, see consilience. The social sciences will for the foreseeable future be composed of different zones in the research of, and sometime distinct in approach toward, the field.
The term "social science" may refer either to the specific "sciences of society" established by thinkers such as Comte, Durkheim, Marx, and Weber, or more generally to all disciplines outside of "noble science" and arts. By the late 19th century, the academic social sciences were constituted of five fields: jurisprudence and amendment of the law, education, health, economy and trade, and art.
Around the start of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism.
Branches.
The social science disciplines are branches of knowledge taught and researched at the college or university level. Social science disciplines are defined and recognized by the academic journals in which research is published, and the learned social science societies and academic departments or faculties to which their practitioners belong. Social science fields of study usually have several sub-disciplines or branches, and the distinguishing lines between these are often both arbitrary and ambiguous.
Anthropology.
Anthropology is the holistic "science of man", a science of the totality of human existence. The discipline deals with the integration of different aspects of the social sciences, humanities, and human biology. In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The natural "sciences" seek to derive general laws through reproducible and verifiable experiments. The "humanities" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. The "social sciences" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences.
The anthropological social sciences often develop nuanced descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains. Within the United States, anthropology is divided into four sub-fields: archaeology, physical or biological anthropology, anthropological linguistics, and cultural anthropology. It is an area that is offered at most undergraduate institutions. The word "anthropos" (άνθρωπος) is from the Greek for "human being" or "person." Eric Wolf described sociocultural anthropology as "the most scientific of the humanities, and the most humanistic of the sciences."
The goal of anthropology is to provide a holistic account of humans and human nature. This means that, though anthropologists generally specialize in only one sub-field, they always keep in mind the biological, linguistic, historic and cultural aspects of any problem. Since anthropology arose as a science in Western societies that were complex and industrial, a major trend within anthropology has been a methodological drive to study peoples in societies with more simple social organization, sometimes called "primitive" in anthropological literature, but without any connotation of "inferior." Today, anthropologists use terms such as "less complex" societies or refer to specific modes of subsistence or production, such as "pastoralist" or "forager" or "horticulturalist" to refer to humans living in non-industrial, non-Western cultures, such people or folk ("ethnos") remaining of great interest within anthropology.
The quest for holism leads most anthropologists to study a people in detail, using biogenetic, archaeological, and linguistic data alongside direct observation of contemporary customs. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.
Communication studies.
Communication studies deals with processes of human communication, commonly defined as the sharing of symbols to create meaning. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examines how messages are interpreted through the political, cultural, economic, and social dimensions of their contexts. Communication is institutionalized under many different names at different universities, including "communication", "communication studies", "speech communication", "rhetorical studies", "communication science", "media studies", "communication arts", "mass communication", "media ecology," and "communication and media science."
Communication studies integrates aspects of both social sciences and the humanities. As a social science, the discipline often overlaps with sociology, psychology, anthropology, biology, political science, economics, and public policy, among others. From a humanities perspective, communication is concerned with rhetoric and persuasion (traditional graduate programs in communication studies trace their history to the rhetoricians of Ancient Greece). The field applies to outside disciplines as well, including engineering, architecture, mathematics, and information science.
Economics.
Economics is a social science that seeks to analyze and describe the production, distribution, and consumption of wealth. The word "economics" is from the Greek οἶκος ["oikos"], "family, household, estate," and νόμος ["nomos"], "custom, law," and hence means "household management" or "management of the state." An economist is a person using economic concepts and data in the course of employment, or someone who has earned a degree in the subject. The classic brief definition of economics, set out by Lionel Robbins in 1932, is "the science which studies human behavior as a relation between scarce means having alternative uses." Without scarcity and alternative uses, there is no economic problem. Briefer yet is "the study of how people seek to satisfy needs and wants" and "the study of the financial aspects of human behavior."
Economics has two broad branches: microeconomics, where the unit of analysis is the individual agent, such as a household or firm, and macroeconomics, where the unit of analysis is an economy as a whole. Another division of the subject distinguishes positive economics, which seeks to predict and explain economic phenomena, from normative economics, which orders choices and actions by some criterion; such orderings necessarily involve subjective value judgments. Since the early part of the 20th century, economics has focused largely on measurable quantities, employing both theoretical models and empirical analysis. Quantitative models, however, can be traced as far back as the physiocratic school. Economic reasoning has been increasingly applied in recent decades to other social situations such as politics, law, psychology, history, religion, marriage and family life, and other social interactions.
This paradigm crucially assumes (1) that resources are scarce because they are not sufficient to satisfy all wants, and (2) that "economic value" is willingness to pay as revealed for instance by market (arms' length) transactions. Rival heterodox schools of thought, such as institutional economics, green economics, Marxist economics, and economic sociology, make other grounding assumptions. For example, Marxist economics assumes that economics primarily deals with the investigation of exchange value, of which human labor is the source.
The expanding domain of economics in the social sciences has been described as economic imperialism.
Education.
Education encompasses teaching and learning specific skills, and also something less tangible but more profound: the imparting of knowledge, positive judgement and well-developed wisdom. Education has as one of its fundamental aspects the imparting of culture from generation to generation (see socialization). To educate means 'to draw out', from the Latin "educare", or to facilitate the realization of an individual's potential and talents. It is an application of pedagogy, a body of theoretical and applied research relating to teaching and learning and draws on many disciplines such as psychology, philosophy, computer science, linguistics, neuroscience, sociology and anthropology.
The education of an individual human begins at birth and continues throughout life. (Some believe that education begins even before birth, as evidenced by some parents' playing music or reading to the baby in the womb in the hope it will influence the child's development.) For some, the struggles and triumphs of daily life provide far more instruction than does formal schooling (thus Mark Twain's admonition to "never let school interfere with your education"). Family members may have a profound educational effect — often more profound than they realize — though family teaching may function very informally
Geography.
Geography as a discipline can be split broadly into two main sub fields: human geography and physical geography. The former focuses largely on the built environment and how space is created, viewed and managed by humans as well as the influence humans have on the space they occupy. This may involve cultural geography, transportation, health, military operations, and cities. The latter examines the natural environment and how the climate, vegetation and life, soil, oceans, water and landforms are produced and interact. Physical geography examines phenomena related to the measurement of earth. As a result of the two subfields using different approaches a third field has emerged, which is environmental geography. Environmental geography combines physical and human geography and looks at the interactions between the environment and humans. Other branches of geography include social geography, regional geography, and geomatics.
Geographers attempt to understand the earth in terms of physical and spatial relationships. The first geographers focused on the science of mapmaking and finding ways to precisely project the surface of the earth. In this sense, geography bridges some gaps between the natural sciences and social sciences. Historical geography is often taught in a college in a unified Department of Geography.
Modern geography is an all-encompassing discipline, closely related to GISc, that seeks to understand humanity and its natural environment. The fields of urban planning, regional science, and planetology are closely related to geography. Practitioners of geography use many technologies and methods to collect data such as GIS, remote sensing, aerial photography, statistics, and global positioning systems (GPS).
History.
History is the continuous, systematic narrative and research into past human events as interpreted through historiographical paradigms or theories.
History has a base in both the social sciences and the humanities. In the United States the National Endowment for the Humanities includes history in its definition of humanities (as it does for applied linguistics). However, the National Research Council classifies history as a social science. The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. The Social Science History Association, formed in 1976, brings together scholars from numerous disciplines interested in social history.
Law.
Law in common parlance, means a rule that (unlike a rule of ethics) is capable of enforcement through institutions. However, many laws are based on norms accepted by a community and thus have an ethical foundation. The study of law crosses the boundaries between the social sciences and humanities, depending on one's view of research into its objectives and effects. Law is not always enforceable, especially in the international relations context. It has been defined as a "system of rules", as an "interpretive concept" to achieve justice, as an "authority" to mediate people's interests, and even as "the command of a sovereign, backed by the threat of a sanction". However one likes to think of law, it is a completely central social institution. Legal policy incorporates the practical manifestation of thinking from almost every social science and the humanities. Laws are politics, because politicians create them. Law is philosophy, because moral and ethical persuasions shape their ideas. Law tells many of history's stories, because statutes, case law and codifications build up over time. And law is economics, because any rule about contract, tort, property law, labour law, company law and many more can have long-lasting effects on the distribution of wealth. The noun "law" derives from the late Old English "lagu", meaning something laid down or fixed and the adjective "legal" comes from the Latin word "lex".
Linguistics.
Linguistics investigates the cognitive and social aspects of human language. The field is divided into areas that focus on aspects of the linguistic signal, such as syntax (the study of the rules that govern the structure of sentences), semantics (the study of meaning), morphology (the study of the structure of words), phonetics (the study of speech sounds) and phonology (the study of the abstract sound system of a particular language); however, work in areas like evolutionary linguistics (the study of the origins and evolution of language) and psycholinguistics (the study of psychological factors in human language) cut across these divisions.
The overwhelming majority of modern research in linguistics takes a predominantly synchronic perspective (focusing on language at a particular point in time), and a great deal of it—partly owing to the influence of Noam Chomsky—aims at formulating theories of the cognitive processing of language. However, language does not exist in a vacuum, or only in the brain, and approaches like contact linguistics, creole studies, discourse analysis, social interactional linguistics, and sociolinguistics explore language in its social context. Sociolinguistics often makes use of traditional quantitative analysis and statistics in investigating the frequency of features, while some disciplines, like contact linguistics, focus on qualitative analysis. While certain areas of linguistics can thus be understood as clearly falling within the social sciences, other areas, like acoustic phonetics and neurolinguistics, draw on the natural sciences. Linguistics draws only secondarily on the humanities, which played a rather greater role in linguistic inquiry in the 19th and early 20th centuries. Ferdinand Saussure is considered the father of modern linguistics.
Political science.
Political science is an academic and research discipline that deals with the theory and practice of politics and the description and analysis of political systems and political behavior. Fields and subfields of political science include political economy, political theory and philosophy, civics and comparative politics, theory of direct democracy, apolitical governance, participatory direct democracy, national systems, cross-national political analysis, political development, international relations, foreign policy, international law, politics, public administration, administrative behavior, public law, judicial behavior, and public policy. Political science also studies power in international relations and the theory of great powers and superpowers.
Political science is methodologically diverse, although recent years have witnessed an upsurge in the use of the scientific method, that is, the proliferation of formal-deductive model building and quantitative hypothesis testing. Approaches to the discipline include rational choice, classical political philosophy, interpretivism, structuralism, and behavioralism, realism, pluralism, and institutionalism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents, interviews, and official records, as well as secondary sources such as scholarly articles are used in building and testing theories. Empirical methods include survey research, statistical analysis/econometrics, case studies, experiments, and model building. Herbert Baxter Adams is credited with coining the phrase "political science" while teaching history at Johns Hopkins University.
Psychology.
Psychology is an academic and applied field involving the study of behavior and mental processes. Psychology also refers to the application of such knowledge to various spheres of human activity, including problems of individuals' daily lives and the treatment of mental illness. The word "psychology" comes from the ancient Greek ψυχή, "psyche" ("soul", "mind") and "logy" ("study").
Psychology differs from anthropology, economics, political science, and sociology in seeking to capture explanatory generalizations about the mental function and overt behavior of individuals, while the other disciplines focus on creating descriptive generalizations about the functioning of social groups or situation-specific human behavior. In practice, however, there is quite a lot of cross-fertilization that takes place among the various fields. Psychology differs from biology and neuroscience in that it is primarily concerned with the interaction of mental processes and behavior, and of the overall processes of a system, and not simply the biological or neural processes themselves, though the subfield of neuropsychology combines the study of the actual neural processes with the study of the mental effects they have subjectively produced.
Many people associate psychology with clinical psychology, which focuses on assessment and treatment of problems in living and psychopathology. In reality, psychology has myriad specialties including social psychology, developmental psychology, cognitive psychology, educational psychology, industrial-organizational psychology, mathematical psychology, neuropsychology, and quantitative analysis of behavior.
Psychology is a very broad science that is rarely tackled as a whole, major block. Although some subfields encompass a natural science base and a social science application, others can be clearly distinguished as having little to do with the social sciences or having a lot to do with the social sciences. For example, biological psychology is considered a natural science with a social scientific application (as is clinical medicine), social and occupational psychology are, generally speaking, purely social sciences, whereas neuropsychology is a natural science that lacks application out of the scientific tradition entirely. In British universities, emphasis on what tenet of psychology a student has studied and/or concentrated is communicated through the degree conferred: B.Psy. indicates a balance between natural and social sciences, B.Sc. indicates a strong (or entire) scientific concentration, whereas a B.A. underlines a majority of social science credits. This is not always necessarily the case however, and in many UK institutions students studying the B.Psy, B.Sc, and B.A. follow the same curriculum as outlined by The British Psychological Society and have the same options of specialism open to them regardless of whether they choose a balance, a heavy science basis, or heavy social science basis to their degree. If they applied to read the B.A. for example, but specialised in heavily science-based modules, then they will still generally be awarded the B.A.
Sociology.
Sociology is the systematic study of society and human social action. The meaning of the word comes from the suffix "-ology", which means "study of", derived from Greek, and the stem "soci-", which is from the Latin word socius, meaning "companion", or society in general.
Sociology was originally established by Auguste Comte (1798–1857) in 1838. Comte endeavoured to unify history, psychology and economics through the descriptive understanding of the social realm. He proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in "The Course in Positive Philosophy" [1830–1842] and "A General View of Positivism" (1844). Though Comte is generally regarded as the "Father of Sociology", the discipline was formally established by another French thinker, Émile Durkheim (1858–1917), who developed positivism as a foundation to practical social research. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his "Rules of the Sociological Method". In 1896, he established the journal "L'Année Sociologique". Durkheim's seminal monograph, "Suicide" (1897), a case study of suicide rates among Catholic and Protestant populations, distinguished sociological analysis from psychology or philosophy.
Karl Marx rejected Comte's positivism but nevertheless aimed to establish a "science of society" based on historical materialism, becoming recognised as a founding figure of sociology posthumously as the term gained broader meaning. Around the start of the 20th century, the first wave of German sociologists, including Max Weber and Georg Simmel, developed sociological antipositivism. The field may be broadly recognised as an amalgam of three modes of social thought in particular: Durkheimian positivism and structural functionalism; Marxist historical materialism and conflict theory; and Weberian antipositivism and verstehen analysis. American sociology broadly arose on a separate trajectory, with little Marxist influence, an emphasis on rigorous experimental methodology, and a closer association with pragmatism and social psychology. In the 1920s, the Chicago school developed symbolic interactionism. Meanwhile in the 1930s, the Frankfurt School pioneered the idea of critical theory, an interdisciplinary form of Marxist sociology drawing upon thinkers as diverse as Sigmund Freud and Friedrich Nietzsche. Critical theory would take on something of a life of its own after World War II, influencing literary criticism and the Birmingham School establishment of cultural studies.
Sociology evolved as an academic response to the challenges of modernity, such as industrialization, urbanization, secularization, and a perceived process of enveloping rationalization. Because sociology is such a broad discipline, it can be difficult to define, even for professional sociologists. The field generally concerns the social rules and processes that bind and separate people not only as individuals, but as members of associations, groups, communities and institutions, and includes the examination of the organization and development of human social life. The sociological field of interest ranges from the analysis of short contacts between anonymous individuals on the street to the study of global social processes. In the terms of sociologists Peter L. Berger and Thomas Luckmann, social scientists seek an understanding of the "Social Construction of Reality". Most sociologists work in one or more subfields. One useful way to describe the discipline is as a cluster of sub-fields that examine different dimensions of society. For example, social stratification studies inequality and class structure; demography studies changes in a population size or type; criminology examines criminal behavior and deviance; and political sociology studies the interaction between society and state.
Since its inception, sociological epistemologies, methods, and frames of enquiry, have significantly expanded and diverged. Sociologists use a diversity of research methods, drawing upon either empirical techniques or critical theory. Common modern methods include case studies, historical research, interviewing, participant observation, social network analysis, survey research, statistical analysis, and model building, among other approaches. Since the late 1970s, many sociologists have tried to make the discipline useful for non-academic purposes. The results of sociological research aid educators, lawmakers, administrators, developers, and others interested in resolving social problems and formulating public policy, through subdisciplinary areas such as evaluation research, methodological assessment, and public sociology.
New sociological sub-fields continue to appear — such as community studies, computational sociology, environmental sociology, network analysis, actor-network theory and a growing list, many of which are cross-disciplinary in nature.
Additional fields of study.
Additional applied or interdisciplinary fields related to the social sciences include:
Methodology.
Social research.
The origin of the survey can be traced back at least early as the Domesday Book in 1086, while some scholars pinpoint the origin of demography to 1663 with the publication of John Graunt's "Natural and Political Observations upon the Bills of Mortality". Social research began most intentionally, however, with the positivist philosophy of science in the 19th century.
In contemporary usage, "social research" is a relatively autonomous term, encompassing the work of practitioners from various disciplines that share in its aims and methods. Social scientists employ a range of methods in order to analyse a vast breadth of social phenomena; from census survey data derived from millions of individuals, to the in-depth analysis of a single agent's social experiences; from monitoring what is happening on contemporary streets, to the investigation of ancient historical documents. The methods originally rooted in classical sociology and statistical mathematics have formed the basis for research in other disciplines, such as political science, media studies, and marketing and market research.
Social research methods may be divided into two broad schools:
Social scientists will commonly combine quantitative and qualitative approaches as part of a multi-strategy design. Questionnaires, field-based data collection, archival database information and laboratory-based data collections are some of the measurement techniques used. It is noted the importance of measurement and analysis, focusing on the (difficult to achieve) goal of objective research or statistical hypothesis testing. A mathematical model uses mathematical language to describe a system. The process of developing a mathematical model is termed 'mathematical modelling' (also modeling). Eykhoff (1974) defined a "mathematical model" as 'a representation of the essential aspects of an existing system (or a system to be constructed) that presents knowledge of that system in usable form'. Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models.
These and other types of models can overlap, with a given model involving a variety of abstract structures. The system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. The concept of an "integrated whole" can also be stated in terms of a system embodying a set of relationships that are differentiated from relationships of the set to other elements, and from relationships between an element of the set and elements not a part of the relational regime. A dynamical system modeled as a mathematical formalization has a fixed "rule" that describes the time dependence of a point's position in its ambient space. Small changes in the state of the system correspond to small changes in the numbers. The "evolution rule" of the dynamical system is a fixed rule that describes what future states follow from the current state. The rule is deterministic: for a given time interval only one future state follows from the current state.
Theory.
Other social scientists emphasize the subjective nature of research. These writers share social theory perspectives that include various types of the following:
Other fringe social scientists delve in alternative nature of research. These writers share social theory perspectives that include various types of the following:
Education and degrees.
Most universities offer degrees in social science fields. The Bachelor of Social Science is a degree targeted at the social sciences in particular. It is often more flexible and in-depth than other degrees that include social science subjects.
In the United States, a university may offer a student who studies a social sciences field a Bachelor of Arts degree, particularly if the field is within one of the traditional liberal arts such as history, or a BSc: Bachelor of Science degree such as those given by the London School of Economics, as the social sciences constitute one of the two main branches of science (the other being the natural sciences). In addition, some institutions have degrees for a particular social science, such as the Bachelor of Economics degree, though such specialized degrees are relatively rare in the United States.

</doc>
<doc id="26783" url="http://en.wikipedia.org/wiki?curid=26783" title="Statute">
Statute

A statute is a formal written enactment of a legislative authority that governs a state, city or country. Typically, statutes command or prohibit something, or declare policy. Statutes are laws made by legislative bodies and distinguished from case law which is decided by courts, and regulations issued by government agencies. Statutes are sometimes referred to as legislation or "black letter law." As a source of law, statutes are considered primary authority (as opposed to secondary authority).
Ideally all statutes must be in harmony with constitutional law or the fundamental law of the land.
This word is used in contradistinction to the common law. Statutes acquire their force from the time of their passage, unless otherwise provided. Statutes are of several kinds; namely, Public or private. Declaratory or remedial. Temporary or perpetual. A temporary statute is one which is limited in its duration at the time of its enactment. It continues in force until the time of its limitation has expired, unless sooner repealed. A perpetual statute is one for the continuance of which there is no limited time, although it may not be expressly declared to be so. If, however, a statute which did not itself contain any limitation is to be governed by another which is temporary only, the former will also be temporary and dependent upon the existence of the latter.
Before a statute becomes law in some countries, it must be agreed upon by the highest executive in the government, and finally published as part of a code. In many countries, statutes are organized in topical arrangements (or "codified") within publications called codes, such as the United States Code. In many nations statutory law is distinguished from and subordinate to constitutional law.
Alternative meanings.
International law.
The term statute is also used to refer to an International treaty that establishes an institution, such as the Statute of the European Central Bank, a protocol to the international courts as well, such as the Statute of the International Court of Justice and the Rome Statute of the International Criminal Court. Statute is also another word for law. The term was adapted from England in about the 18th century.
Autonomy Statute.
In the Autonomous Communities of Spain, the autonomy statute is a legal document similar to a state constitution in a federated state. The autonomies statutes in Spain have the rank of "Ley Organica", a category of special laws reserved only for the main institutions and issues and mentioned in the Constitution (the highest ranking legal instrument in Spain). Leyes Organicas rank between the Constitution and ordinary laws. The name was chosen, among others, to avoid confusion with the term Constitution (i.e. the Spanish Constitution of 1978).
Religious statutes.
Biblical terminology.
In biblical terminology, statute (Hebrew "chok") refers to a law given without any reason or justification. The classic example is the statute regarding the Red Heifer.
The opposite of a chok is a "mishpat", a law given for a specified reason, e.g. the Sabbath laws, which were given because "God created the world in six days, but on the seventh day He rested". (Genesis 2:2-3)
Dharma.
"That which upholds, supports or maintains the regulatory order of the universe" meaning the "Law" or "Natural Law". This is a concept of central importance in Indian philosophy and religion.

</doc>
<doc id="26784" url="http://en.wikipedia.org/wiki?curid=26784" title="Statutory law">
Statutory law

Statutory law or statute law is written law (as opposed to oral or customary law) set down by a legislature (as opposed to regulatory law promulgated by the executive or common law of the judiciary) or by a legislator (in the case of an absolute monarchy). 
Statutes may originate with national, state legislatures or local municipalities. Statutory laws are subordinate to the higher constitutional laws of the land and other sources of law.
Codified law.
The term codified law refers to statutes that have been organized ("codified") by subject matter; in this narrower sense, some but not all statutes are considered "codified." The entire body of codified statute is referred to as a "code," such as the United States Code, the Ohio Revised Code or the Code of Canon Law. The substantive provisions of the Act could be codified (arranged by subject matter) in one or more titles of the United States Code while the provisions of the law that have not reached their "effective date" (remaining uncodified) would be available by reference to the United States Statutes at Large. Another meaning of "codified law" is a statute that takes the common law in a certain area of the law and puts it in statute or code form.
Private law (particular law).
Another example of statutes that are not typically codified is a "private law" that may originate as a private bill, a law affecting only one person or a small group of persons. An example was divorce in Canada prior to the passage of the Divorce Act of 1968. It was possible to obtain a legislative divorce in Canada by application to the Canadian Senate, which reviewed and investigated petitions for divorce, which would then be voted upon by the Senate and subsequently made into law. In the United Kingdom Parliament, private bills were used in the nineteenth century to create corporations, grant monopolies and give individuals attention to be more fully considered by the parliament. The government may also seek to have a bill introduced "unofficially" by a backbencher so as not to create a public scandal; such bills may also be introduced by the loyal opposition — members of the opposition party or parties. Sometimes a private member's bill may also have private bill aspects, in such case the proposed legislation is called a hybrid bill.
In Canon Law, private law is called "particular law."

</doc>
<doc id="26785" url="http://en.wikipedia.org/wiki?curid=26785" title="Sanction">
Sanction

A sanction may be either a permission or a restriction, depending on context, as the word is an auto-antonym.
Examples of sanctions include:
Involving countries:
In other uses:

</doc>
<doc id="26786" url="http://en.wikipedia.org/wiki?curid=26786" title="Sarajevo">
Sarajevo

Sarajevo (]) is the capital and largest city of Bosnia and Herzegovina, with an estimated population of 369,534. 
The Sarajevo metropolitan area, including Sarajevo, East Sarajevo and surrounding municipalities, is home to 608,354 inhabitants.
Moreover, it is also the capital of the Federation of Bosnia and Herzegovina entity, the capital of the Republika Srpska entity, and the center of the Sarajevo Canton. Nestled within the greater Sarajevo valley of Bosnia, it is surrounded by the Dinaric Alps and situated along the Miljacka River in the heart of Southeastern Europe and the Balkans.
Sarajevo is the leading political, social and cultural center of Bosnia and Herzegovina, a prominent center of culture in the Balkans, with its region-wide influence in entertainment, media, fashion, and the arts.
Until recently, the city was famous for its traditional cultural and religious diversity, with adherents of Islam, Orthodoxy, Judaism and Catholicism coexisting there for centuries. Due to its long and rich history of religious and cultural variety, Sarajevo was sometimes called the "Jerusalem of Europe" or "Jerusalem of the Balkans". It was, until late in the 20th century, the only major European city to have a mosque, Catholic church, Orthodox church and synagogue within the same neighborhood. A regional center in education, the city is also home to the Balkans' first institution of tertiary education in the form of an Islamic polytechnic called the Saraybosna Osmanlı Medrese, today part of the University of Sarajevo.
Although settlement in the area stretches back to prehistoric times, the modern city arose as an Ottoman stronghold in the 15th century. Sarajevo has attracted international attention several times throughout its history. In 1885, Sarajevo was the first city in Europe and the second city in the world to have a full-time electric tram network running through the city, following San Francisco. In 1914, it was the site of the assassination of the Archduke of Austria that sparked World War I. Seventy years later, it hosted the 1984 Winter Olympics. For nearly four years, from 1992 to 1996, the city suffered the longest siege of a city in the history of modern warfare (1,425 days long) during the Bosnian War.
Sarajevo has been undergoing post-war reconstruction, and is the fastest growing city in Bosnia and Herzegovina. The travel guide series, "Lonely Planet", has named Sarajevo as the 43rd best city in the world, and in December 2009 listed Sarajevo as one of the top ten cities to visit in 2010. In 2011, Sarajevo was nominated to be the European Capital of Culture in 2014 and will be hosting the European Youth Olympic Festival in 2017.
Sarajevo is also a metropolis due to being the most important and influential city in the whole country.
Etymology.
The earliest known name for the large central Bosnian region of today's Sarajevo is Vrhbosna.
Sarajevo is a slavicized word based on "saray", the Turkish word for "palace". The letter Y does not exist in the Bosnian language, so it has been changed to J which does exist, with the same pronunciation as Y. The "evo" portion may come from the term "saray ovası" first recorded in 1455, meaning "the plains around the palace" or simply "palace plains".
However, in his Dictionary of Turkish loanwords, Abdulah Škaljić maintains that the ""evo" ending is more likely to have come from the widespread Slavic suffix "evo" used to indicate place names, than from the Turkish ending "ov"a", as proposed by some. The first mention of name Sarajevo was in 1507 letter written by Feriz Beg.
Sarajevo has had many nicknames. The earliest is "Šeher", which is the term Isa-Beg Ishaković used to describe the town he was going to build. It is a Turkish word meaning an advanced city of key importance ("şehir") which in turn comes from Persian شهر "shahr" (city). As Sarajevo developed, numerous nicknames came from comparisons to other cities in the Islamic world, i.e. "Damascus of the North". The most popular of these was "European Jerusalem".
Some argue that a more correct translation of "saray" is government office or house. "Saray" is a common word in Turkish for a "palace" or "mansion" (from Persian word سرای "sarāy", means "house, palace").
History.
Ancient times.
One of the earliest findings of settlement in the Sarajevo area is that of the Neolithic Butmir culture. The discoveries at Butmir were made on the grounds of the modern-day Sarajevo suburb Ilidža in 1893 by Austro-Hungarian authorities during the construction of an agricultural school. The area's richness in flint was no doubt attractive to Neolithic man, and the settlement appears to have flourished. The settlement developed unique ceramics and pottery designs, which characterize the Butmir people as a unique culture. This was largely responsible for the International congress of archaeologists and anthropologists meeting in Sarajevo in 1894.
The next prominent culture in Sarajevo were the Illyrians. The ancient people, who considered most of the West Balkans as their homeland, had several key settlements in the region, mostly around the river Miljacka and Sarajevo valley. The Illyrians in the Sarajevo region belonged to the "Daesitiates", a war-like people who were probably the last Illyrian people in Bosnia and Herzegovina to resist Roman occupation. Their defeat by the Roman emperor Tiberius in 9 A.D. marks the start of Roman rule in the region. The Romans never built up the region of modern-day Bosnia very much, but the Roman colony of Aquae Sulphurae was located near the top of present-day Ilidža, and was the most important settlement of the time. After the Romans, the Goths settled the area, followed by the Slavs in the 7th century.
Middle Ages.
During the Middle Ages Sarajevo was part of the Bosnian province of Vrhbosna near the traditional center of the Kingdom of Bosnia. Though a city called "Vrhbosna" existed, the exact settlement of Sarajevo at this time is debated. Various documents of the high Middle Ages note a place called "Tornik" in the region. By all indications, Tornik was a very small marketplace surrounded by a proportionally small village, and was not considered very important by Ragusan merchants.
Other scholars say that "Vrhbosna" was a major city located at the site of modern-day Sarajevo. Today, that place still exists, but it's name for small part of Sarajevo, at the north-east. Papal documents say that in 1238, a cathedral dedicated to Saint Paul was built in the city. Disciples of the notable saints Cyril and Methodius stopped by the region, founding a church at "Vrelobosna". Whether or not the city was located at modern-day Sarajevo, the documents attest to its and the region's importance. Vrhbosna was a Slavic citadel from 1263 until it was occupied by the Ottoman Empire in 1429.
Ottoman era.
Sarajevo was founded by the Ottoman Empire in the 1450s upon its conquest of the region, with 1461 used as the city's founding date. The first Ottoman governor of Bosnia, Isa-Beg Ishaković, transformed the cluster of villages into a city and state capitol by building a number of key structures, including a mosque, a closed marketplace, a public bath, a hostel, and of course the governor's castle ("Saray") which gave the city its present name. The mosque was named "Careva Džamija" (the Tsar's Mosque) in honor of the Sultan Mehmed II. With the improvements Sarajevo quickly grew into the largest city in the region. Many Christians converted to Islam at this time. The settlement was established as a city, named "Bosna-Saraj", around the citadel in 1461. The name Sarajevo is derived from Turkish "saray ovası", meaning "the field around saray".
Under leaders such as the second governor Gazi Husrev-beg, Sarajevo grew at a rapid rate. Husrev-beg greatly shaped the physical city, as most of what is now the Old Town was built during his reign. Sarajevo became known for its large marketplace and numerous mosques, which by the middle of the 16th century numbered more than 100. At the peak of the empire, Sarajevo was the biggest and most important Ottoman city in the Balkans after Istanbul. By 1660, the population of Sarajevo was estimated to be over 80,000. By contrast, Belgrade in 1838 had 12,963 inhabitants, and Zagreb as late as 1851 had 14,000 people. As political conditions changed, Sarajevo became the site of warfare.
In 1697, during the Great Turkish War, a raid was led by Prince Eugene of Savoy of the Habsburg Monarchy against the Ottoman Empire, which conquered Sarajevo and left it plague-infected and burned to the ground. 
After his men had looted thoroughly, they set the city on fire and destroyed nearly all of it in one day. Only a handful of neighborhoods, some mosques, and an Orthodox church, were left standing.
Numerous other fires weakened the city, as well. The city was later rebuilt, but never fully recovered from the destruction. By 1807, it had only some 60,000 residents.
In the 1830s, several battles of the Bosnian uprising had taken place around the city. These had been led by Husein Gradaščević. Today, a major city street is named "Zmaj od Bosne" (Dragon of Bosnia) in his honor. The rebellion failed and, for several more decades, the crumbling Ottoman state remained in control of Bosnia.
The Ottoman Empire made Sarajevo an important administrative centre by 1850.
Austria-Hungary.
Austria-Hungary's occupation of Bosnia and Herzegovina came in 1878 as part of the Treaty of Berlin, and complete annexation followed in 1908, angering the Serbs. Sarajevo was industrialized by Austria-Hungary, who used the city as a testing area for new inventions, such as tramways, established in 1885, before installing them in Vienna. Architects and engineers wanting to help rebuild Sarajevo as a modern European capital rushed to the city. A fire that burned down a large part of the central city area ("čaršija") left more room for redevelopment. The city has a unique blend of the remaining Ottoman city market and contemporary western architecture. Sarajevo has some examples of Secession- and Pseudo-Moorish styles that date from this period.
The Austro-Hungarian period was one of great development for the city, as the Western power brought its new acquisition up to the standards of the Victorian age. Various factories and other buildings were built at this time, and a large number of institutions were both Westernized and modernized. For the first time in history, Sarajevo's population began writing in Latin script.
In the event that triggered World War I, Archduke Franz Ferdinand of Austria was assassinated, along with his wife Sophie, Duchess of Hohenberg in Sarajevo on 28 June 1914 by a self-declared Yugoslav, Gavrilo Princip, a member of Young Bosnia. In response, many residents of Sarajevo (mostly ethnic Croats and Bosniaks) organized riots against the Serbs, killing two and destroying their properties. In the ensuing war, however, most of the Balkan offensives occurred near Belgrade, and Sarajevo largely escaped damage and destruction.
Following the war, after the Balkans were unified under the Kingdom of Yugoslavia, Sarajevo became the capital of Drina Province.
Yugoslavia.
After World War I and contributions from the Serbian army alongside rebelling Slavic nations in Austria-Hungary, Sarajevo became part of the Kingdom of Yugoslavia. Though it held some political importance, as the center of first the Bosnian region and then the Drinska Banovina, it was not treated with the same attention or considered as significant as it was in the past. Outside of today's national bank of Bosnia and Herzegovina, virtually no significant contributions to the city were made during this period.
During World War II the Kingdom of Yugoslavia's army was overrun by superior German and Italian forces. Following a German bombing campaign, Sarajevo was captured on 15 April 1941 by the 16th Motorized infantry Division. The Axis powers created the Independent State of Croatia and included Sarajevo in its territory. On 12 October, a group of 108 notable Bosniak citizens of Sarajevo signed the Resolution of Sarajevo Muslims by which they condemned the persecution of Serbs organized by the Ustaše, made a distinction between the Bosniaks who participated in such persecutions and the rest of the Bosniak population, presented information about the persecutions of Bosniaks by Serbs, and requested security for all citizens of the country, regardless of their identity. By mid-summer 1942, around 20,000 Serbs found refuge in Sarajevo from Ustaše terror.
The city was bombed by the Allies from 1943 to 1944. The Yugoslav Partisan movement was represented in the city. Resistance was led by a NLA Partisan named Vladimir "Walter" Perić. He died while leading the final liberation of the city on 6 April 1945. Many of the WWII shell casings that were used during the attacks have been carved and polished in Sarajevo tradition and are sold as art.
Following the liberation, Sarajevo was the capital of the Socialist Republic of Bosnia and Herzegovina within the Socialist Federal Republic of Yugoslavia. The Republic Government invested heavily in Sarajevo, building many new residential blocks in Novi Grad Municipality and Novo Sarajevo Municipality, while simultaneously developing the city's industry and transforming Sarajevo into one of the modern cities, in SFRYugoslavia and SR Bosnia. From a post-war population of 115,000, by the end of Yugoslavia, Sarajevo had 600,000 people. Sarajevo grew rapidly as it became an important regional industrial center in Yugoslavia. The Vraca Memorial Park, a monument for victims of World War II, was dedicated on 25 November, the "Day of Statehood of Bosnia and Herzegovina" when the ZAVNOBIH held their first meeting in 1943.
The crowning moment of Sarajevo's time in Socialist Yugoslavia was the 1984 Winter Olympics. Sarajevo beat Sapporo, Japan; and Falun/Göteborg, Sweden for the privilege of hosting the games. They were followed by an immense boom in tourism, making the 1980s one of the city's best decades in a long time.
Bosnian War for independence.
The Bosnian War for independence resulted in large-scale destruction and dramatic population shifts during the Siege of Sarajevo between 1992 and 1995. Thousands of Sarajevans lost their lives under the constant bombardment and sniper shooting at civilians by the Serb forces during the siege. It is the longest siege of a capital city in the history of modern warfare. Serb forces of the Republika Srpska and the Yugoslav People's Army besieged Sarajevo, the largest city of Bosnia and Herzegovina, from 5 April 1992 to 29 February 1996 during the Bosnian War.
When Bosnia and Herzegovina declared independence from Yugoslavia and achieved United Nations recognition, the Serbian leaders and army whose goal was to create a "greater Serbia", declared a new Serbian national state Republika Srpska (RS) which was carved from the territory of Bosnia and Herzegovina, encircled Sarajevo with a siege force of 18,000 stationed in the surrounding hills, from which they assaulted the city with weapons that included artillery, mortars, tanks, anti-aircraft guns, heavy machine-guns, multiple rocket launchers, rocket-launched aircraft bombs, and sniper rifles. From 2 May 1992, the Serbs blockaded the city. The Bosnian government defence forces inside the besieged city were poorly equipped and unable to break the siege.
During the siege, 11,541 people lost their lives, including over 1,500 children. An additional 56,000 people were wounded, including nearly 15,000 children. The 1991 census indicates that before the siege the city and its surrounding areas had a population of 525,980.
When the siege ended, the concrete scars caused by mortar shell explosions left a mark that was filled with red resin. After the red resin was placed, it left a floral pattern which led to it being dubbed a Sarajevo Rose.
Present.
Various new modern buildings have been built, most significantly the Bosmal City Center, BBI Centar and the Avaz Twist Tower, which is the tallest skyscraper in the Balkans. A new highway was completed in the late 2000s between Sarajevo and the city of Kakanj. Due to growth in population, tourism and airport traffic the service sector in the city is developing fast and welcoming new investors from various businesses.
The business enclave Sarajevo City Center is one of the largest and most modern shopping and business centers in the region. It was completed in early 2014. Airport Center Sarajevo which will be connected directly to the new airport terminal will offer a great variety of brands, products and services.
Most recently, in 2014 the city saw anti-government protests and riots and record rainfall that caused historic flooding.
Geography.
Sarajevo is located near the geometric center of the triangular-shaped Bosnia-Herzegovina and within the historical region of Bosnia proper. It is situated 518 m above sea level and lies in the Sarajevo valley, in the middle of the Dinaric Alps. The valley itself once formed a vast expanse of greenery, but gave way to urban expansion and development in the post-World War II era. The city is surrounded by heavily forested hills and five major mountains. The highest of the surrounding peaks is Treskavica at 2088 m, then Bjelašnica mountain at 2067 m, Jahorina at 1913 m, Trebević at 1627 m, with 1502 m Igman being the shortest. The last four are also known as the Olympic Mountains of Sarajevo (see also 1984 Winter Olympics). The city itself has its fair share of hilly terrain, as evidenced by the many steeply inclined streets and residences seemingly perched on the hillsides.
The Miljacka river is one of the city's chief geographic features. It flows through the city from east through the center of Sarajevo to west part of city where eventually meets up with the Bosna river. Miljacka river is "The Sarajevo River", with its source in the town of Pale, several kilometers to the east of Sarajevo. The Bosna's source, Vrelo Bosne near Ilidža (west Sarajevo), is another notable natural landmark and a popular destination for Sarajevans and other tourists. Several smaller rivers and streams also run through the city and its vicinity.
Cityscape.
Sarajevo is located close to the center of the triangular shape of Bosnia and Herzegovina in southeastern Europe. Sarajevo city proper consists of four municipalities (or "in Bosnian and Serbian: opština, in Croatian: općina"): Centar (Center), Novi Grad (New City), Novo Sarajevo (New Sarajevo), and Stari Grad (Old City), while Metropolitan area of Sarajevo (Greater Sarajevo area) includes these and the neighbouring municipalities of Ilidža, Hadžići and Vogošća "(before the war and new (Deyton) administrative division, Metro of Sarajevo consisted also, beside above mentioned, three municipalities today's divided between Federacija Bosne i Hercegovine and Republika Srpska - Trnovo, Federacija Bosne i Hercegovine / Trnovo, Republika Srpska, Lukavica and Pale)". The city has an urban area of 1041.5 km2.
Climate.
Sarajevo's climate exhibits influences of oceanic zones, with four seasons and uniformly spread precipitation. The proximity of the Adriatic Sea moderates Sarajevo's climate somewhat, although the mountains to the south of the city greatly reduce this maritime influence. The average yearly temperature is 10 °C, with January (-0.5 °C avg.) being the coldest month of the year and July (19.7 °C avg.) the warmest.
The highest recorded temperature was 40.7 °C on 19 August 1946, and on 23 August 2008 (41.0) while the lowest recorded temperature was -26.2 °C on 25 January 1942. On average, Sarajevo has 6 days where the temperature exceeds 32 °C and 4 days where the temperature drops below -15 C per year. The city typically experiences mildly cloudy skies, with an average yearly cloud cover of 45%.
The cloudiest month is December (75% average cloud cover) while the clearest is August (37%). Moderate precipitation occurs fairly consistently throughout the year, with an average 75 days of rainfall. Suitable climatic conditions have allowed winter sports to flourish in the region, as exemplified by the Winter Olympics in 1984 that were celebrated in Sarajevo. Average winds are 28 - and the city has 1,769 hours of sunshine.
Administration.
Largest city of Bosnia and Herzegovina.
Sarajevo is the capital of the country of Bosnia and Herzegovina and its sub-entity, the Federation of Bosnia and Herzegovina, as well as of the Sarajevo Canton. It is also the "de jure" capital of another entity, Republika Srpska. Each of these levels of government has their parliament or council, as well as judicial courts, in the city. In addition many foreign embassies are located in Sarajevo.
Sarajevo is home to the Council of Ministers of Bosnia and Herzegovina, Parliamentary Assembly of Bosnia and Herzegovina, Presidency of Bosnia and Herzegovina, the Constitutional Court of Bosnia and Herzegovina and the operational command of the Armed Forces of Bosnia and Herzegovina.
Bosnia and Herzegovina's Parliament office in Sarajevo was damaged heavily in the Bosnian War. Due to damage the staff and documents were moved to a nearby ground level office to resume the work. In late 2006 reconstruction work started on the Parliament and was finished in 2007. The cost of reconstruction is supported 80% by the Greek Government
through the Hellenic Program of Balkans Reconstruction (ESOAV) and 20% by Bosnia-Herzegovina.
Municipalities and city government.
The city comprises four municipalities Centar, Novi Grad, Novo Sarajevo, and Stari Grad. Each operate their own municipal government, united they form one city government with its own constitution. The executive branch (Bosnian: "Gradska Uprava") consists of a mayor, with two deputies and a cabinet. The legislative branch consists of the City Council, or "Gradsko Vijeće". The council has 28 members, including a council speaker, two deputies, and a secretary. Councilors are elected by the municipality in numbers roughly proportional to their population. The city government also has a judicial branch based on the post-transitional judicial system as outlined by the High Representative's "High Judicial and Prosecutorial Councils".
Sarajevo's Municipalities are further split into "local communities" (Bosnian, "Mjesne zajednice"). Local communities have a small role in city government and are intended as a way for ordinary citizens to get involved in city government. They are based on key neighborhoods in the city.
Panoramic view of Sarajevo
International relations.
Twin towns – Sister cities.
Sarajevo is twinned with:
Fraternity cities.
Sarajevo's fraternity cities include:
Economy.
Sarajevo is Bosnia and Herzegovina's economic focal point, generating a significant portion of the country's GDP. After the years of war, Sarajevo's economy was subject to reconstruction and rehabilitation programs. Amongst economic landmarks, the Central Bank of Bosnia and Herzegovina opened in Sarajevo in 1997 and the Sarajevo Stock Exchange began trading in 2002. The city's large manufacturing, administration, tourism sector, combined with a large informal market, makes it the strongest economic regions of Bosnia and Herzegovina.
While Sarajevo had a large industrial base during its communist period, only a few pre-existing businesses have successfully adapted to the market economy. Sarajevo industries now include tobacco products, furniture, hosiery, automobiles, and communication equipment. Companies based in Sarajevo include B&H Airlines, BH Telecom, Bosnalijek, Energopetrol, Sarajevo Tobacco Factory, and Sarajevska Pivara (Sarajevo Brewery).
Tourism and economy.
Sarajevo has a wide tourist industry and a fast expanding service sector thanks to the strong annual growth in tourist arrivals. Sarajevo also benefits from being both a summer and winter destination with continuity in its tourism throughout the year. The travel guide series, "Lonely Planet", has named Sarajevo as the 43rd best city in the world, and in December 2009 listed Sarajevo as one of the top ten cities to visit in 2010.
In 2013 302.570 tourists visited Sarajevo, up 17.9% compared to 2012, giving 595.637 overnight stays, which is 18% more than in 2012.
Sports-related tourism uses the legacy facilities of the 1984 Winter Olympics, especially the skiing facilities on the nearby mountains of Bjelašnica, Igman, Jahorina, Trebević, and Treskavica. Sarajevo's 600 years of history, influenced by both Western and Eastern empires, makes it a tourist attraction with splendid variations. 
Sarajevo has hosted travellers for centuries, because it was an important trading center during the Ottoman and Austria-Hungarian empires. Examples of popular destinations in Sarajevo include the Vrelo Bosne park, the Sarajevo cathedral, and the Gazi Husrev-beg's Mosque. Tourism in Sarajevo is chiefly focused on historical, religious, cultural aspects and winter sports.
GDP.
In 1981 Sarajevo's GDP per capita was 133% of the Yugoslav average.
In 2011 Sarajevo's GDP is estimated to be 6.30 billion US$ by the Central Bank of Bosnia, which comprises 37% of the total GDP of Bosnia and Herzegovina.
Demographics.
The last official census in Bosnia and Herzegovina took place 1991 and recorded 527,049 people living in city of Sarajevo (ten municipalities). In the settlement of Sarajevo proper, there were 416,497 inhabitants. The war displaced hundreds of thousands of people, a large majority of whom have not returned.
Today, Sarajevo's population is not known clearly and is based on estimates contributed by the United Nations Statistics Division and the Federal Office of Statistics of the Federation of Bosnia and Herzegovina, among other national and international non-profit organizations. s of June 2011[ [update]], the population of the city's four municipalities is estimated to be 411,161, whereas the Sarajevo Canton population is estimated at 578,757. With an area of 1280 km2, Sarajevo has a population density of about 2173 PD/km2. The Novo Sarajevo municipality is the most densely populated part of Sarajevo with about 7524 PD/km2, while the least densely populated is the Stari Grad, with 2742 PD/km2.
The war changed the ethnic and religious profile of the city. It had long been a multicultural city, and often went by the nickname of "Europe's Jerusalem". At the time of the 1991 census, 49.2 per cent of the city's population of 527,049 were Bosniaks, 29.8 percent Serbs, 10.7 percent Yugoslavs, 6.6 percent Croats and 3.6 percent other ethnicities (Jews, Romas, etc.). By 2002, 79.6 per cent of the canton's population of 401,118 were Bosniak, 11.2 percent Serb, 6.7 percent Croat and 2.5 percent others (Jews, Romas, etc.). Since its formalisation following the Dayton Agreement, the Bosniak category has been endorsed by the Federation of Bosnia and Herzegovina as the ethnic or national designation of choice to replace the former "Muslim by nationality" employed by the Yugoslav government. According to academic Fran Markowitz, it is not clear, however, whether the shift in identification from Muslim to Bosniak by such large numbers of Sarajevo's residents can be primarily attributed to a successful endorsement by the state among Muslims (and maybe also those who previously identified as Yugoslavs or "Ostali" (others)), or if its citizens have made that switch intrinsically. Her analysis of marriage registration data shows that 67 per cent of people marrying in 2003 identified as Muslim or Bosniak, which is significantly lower than the 79.6 per cent census figure (unlike the census, where people respond to an interviewer, applicants to the marriage registry fill in the form themselves). To explain this discrepancy, Markowitz points to a number of "administrative apparatuses and public pressures that push people who might prefer to identify as flexible, multiply constituted hybrids or with one of the now unnamed minority groups into one of the three Bosniac-Croat-Serb constituent nations". These include respondents being encouraged by census interviewers to identity as belonging to one of the three constituent peoples.
Many Serbs left urban areas including Sarajevo during the conflict, but the falling number of Serbs is also partly due to the redrawing of municipal boundaries as part of the Dayton Agreement.
Due to Census 2013, Sarajevo has 291,422 inhabitants while 438,443 live in Sarajevo Canton. Sarajevo Metropolitan Area is home to 608,354.
In comparison to Census 1991, the population decreased by 63,745 inhabitants.
Transportation.
Roads and highways.
Sarajevo's location in a valley between mountains makes it a compact city. Narrow city streets and a lack of parking areas restrict automobile traffic but allow better pedestrian and cyclist mobility. The two main roads are Titova Ulica (Street of Marshal Tito) and the east-west Zmaj od Bosne (Dragon of Bosnia) highway (E761).
Sarajevo is Bosnia's main intersection and the most passable city in Bosnia and Herzegowina and the third in region. The city is connected to all the other major cities by highway or national road like Zenica, Banja Luka, Tuzla, Mostar, Goražde and Foča.
Tourists from Central Europe and elsewhere visiting Dalmatia driving via Budapest thru Sarajevo also contribute to the traffic congestion in and around Sarajevo.
The trans-European highway, Corridor 5C, runs through Sarajevo connecting it to Budapest in the north, and Ploče at the Adriatic sea in the south. The highway is built by the government and should cost 3.5 billion Euros. Up until March 2012, the Federation of Bosnia and Herzegovina invested around 600 million Euros in the A1. In 2014 the sections Sarajevo-Zenica and Sarajevo-Tarcin were completed including the Sarajevo Beltway ring road.
Tram, bus and trolleybus.
Sarajevo's electric tramways, in operation since 1885, are the oldest form of public transportation in the city.
Sarajevo had the first full-time (dawn to dusk) tram line in Europe, and the second in the world. Opened on New Year's Day in 1885, it was the testing line for the tram in Vienna and the Austro-Hungarian Empire, and operated by horses. Originally built to , the present system in 1960 was upgraded to . The trams played a pivotal role in the growth of the city in the 20th century.
There are seven tramway lines supplemented by five trolleybus lines and numerous bus routes. The main railroad station in Sarajevo is located in the north-central area of the city. From there, the tracks head west before branching off in different directions, including to industrial zones in the city. Sarajevo is currently undergoing a major infrastructure renewal; many highways and streets are being repaved, the tram system is undergoing modernization, and new bridges and roads are under construction.
Future metro plans.
Sarajevo-based architect, Muzafer Osmanagić, in order to solve traffic congestion in Sarajevo, has proposed a study called "Eco Energy 2010–2015", idealizing a subway system underneath the bed of the river Miljacka. The first line of Metro Sarajevo should connect Basčarsija with Otoka. This line should cost some 150 million KM and be financed by the European Bank for Reconstruction and Development.
Airport.
Sarajevo International Airport (IATA: SJJ), also called Butmir, is located just a few kilometers southwest of the city and was voted Best European Airport With Under 1,000,000 Passengers at the 15th Annual ACI-Europe in Munich in 2005. During the war the airport was used for UN flights and humanitarian relief. Since the Dayton Accord in 1996, the airport has welcomed a thriving commercial flight business.
In 2011 Sarajevo International Airport had 599,996 passengers which is more than all of the airports in Bosnia-Herzegovina had together and 6,5% more than in 2010. The growth rate in 2012 is expected to be around 10%.
Plans for extension of the passenger terminal, together with upgrading and expanding the taxiway and apron, are planned to start in Fall 2012. The existing terminal will be expanded by approximately 7,000 square metres. The upgraded airport will also be directly linked to the commercial retail center Sarajevo Airport Center, making it easier for tourists and travellers to spend their time before flight boarding shopping and enjoying the many amenities that will be offered.
As of November 2014 Sarajevo is directly connected with Istanbul-Atatürk, Istanbul-Sabiha Gökçen, Munich, Vienna, Zagreb, Ljubljana, Belgrade, Zürich, Stuttgart, Cologne, Copenhagen, and Stockholm-Arlanda.
Railway.
Sarajevo has only two daily international connections to Zagreb and Ploče. There are also connections between Sarajevo and all major cities within Bosnia and Herzegovina. Once, the East Bosnian railway connected Sarajevo to Beograd.
Communications and media.
As the largest city of Bosnia and Herzegovina, Sarajevo is the main center of the country's media. Most of the communications and media infrastructure was destroyed during the war but reconstruction monitored by the Office of the High Representative has helped to modernize the industry as a whole. For example, internet was first made available to the city in 1995.
"Oslobođenje" (Liberation), founded in 1943, is Sarajevo's longest running continuously circulating newspaper and the only one to survive the war. However, this long running and trusted newspaper has fallen behind "Dnevni Avaz" (Daily Voice), founded in 1995, and "Jutarnje Novine" (Morning News) in circulation in Sarajevo. Other local periodicals include the Croatian newspaper Hrvatska riječ and the Bosnian magazine Start, as well as weekly newspapers "Slobodna Bosna" ("Free Bosnia") and "BH Dani" ("BH Days"). "Novi Plamen", a monthly magazine, is the most left-wing publication currently.
The Radiotelevision of Bosnia-Herzegovina is Sarajevo's public television station, one of three in Bosnia and Herzegovina. Other stations based in the city include NRTV "Studio 99", NTV Hayat, TV 1, Open Broadcast Network, TV Kantona Sarajevo and Televizija Alfa.
The headquarters of Al Jazeera Balkans are also located in Sarajevo, with a broadcasting studio at the top of the BBI Center. The news channel covers Bosnia and Herzegovina, Serbia, Croatia and Montenegro and the surrounding Balkan states.
Many small independent radio stations exist, including established stations such as Radio M, Radio Stari Grad (Radio Old Town), Studentski eFM Radio, Radio 202, Radio BIR, and RSG. Radio Free Europe, as well as several American and Western European stations are available.
Education.
Higher education has a long and rich tradition in Sarajevo. The first institution that can be classified as a tertiary educational institution was a school of Sufi philosophy established by Gazi Husrev-beg in 1531; numerous other religious schools have been established over time. In 1887, under the Austro-Hungarian Empire, a Sharia Law School began a five-year program. In the 1940s the University of Sarajevo became the city's first secular higher education institute, effectively building upon the foundations established by the Saraybosna Hanıka in 1531. In the 1950s, post-bachelor graduate degrees became available. Severely damaged during the war, it was recently rebuilt in partnership with more than 40 other universities.
There are also several international and private universities located in Sarajevo:
The University of Sarajevo is the most important institution of higher education in Bosnia-Herzegovina, having been established originally in 1531 as an Ottoman Law School, and in its modern incarnation in 1949. With 23 faculties and around 55,000 enrolled students, it ranks among the largest universities in Europe in terms of enrollment. Since the university opened its doors, 122,000 students received bachelor's degrees, 3,891 received master's degrees and 2,284 doctorate degrees in 43 different fields.
s of 2005[ [update]], in Sarajevo there are 46 elementary schools (Grades 1–9) and 33 high schools (Grades 10–13), including three schools for children with special needs,
'Druga gimnazija' provides the MYP and International Baccalaureate diploma. 'Prva bošnjačka gimnazija' provides the IGCSE and GCE Advanced Level.
There are also several international schools in Sarajevo, catering to the expatriate community; some of which are Sarajevo International School and the French International School of Sarajevo, established in 1998.
Culture.
Sarajevo has been home to many different religions for centuries, giving the city a range of diverse cultures. In the time of Ottoman occupation of Bosnia, Muslims, Serbian Orthodox, Roman Catholics, and Sephardi Jews all shared the city while maintaining distinctive identities. They were joined during the brief occupation by Austria-Hungary by a smaller number of Germans, Hungarians, Slovaks, Czechs and Ashkenazi Jews.
Historically, Sarajevo has been home to several famous Bosnian poets, scholars, philosophers, and writers during the Ottoman Empire. To list only a very few; Nobel Prize-winner Vladimir Prelog is from the city, as is Academy Award-winning director Danis Tanović and multiple award-winning writer Aleksander Hemon. One of the region's most prolific and prominent poets, writers and screenwriters, Abdulah Sidran is also a Sarajevo native. Nobel Prize-winner Ivo Andrić attended high school in Sarajevo for two years. Sarajevo is also the home of the East West Theatre Company, the only independent theater company in Bosnia and Herzegovina.
The Sarajevo National Theatre is the oldest professional theater in Bosnia and Herzegovina, having been established in 1921.
Museums.
The city is rich in museums, including the Museum of Sarajevo, the Ars Aevi Museum of Contemporary Art, Historical Museum of Bosnia and Herzegovina, The Museum of Literature and Theatre Arts of Bosnia and Herzegovina, and the National Museum of Bosnia and Herzegovina (established in 1888) home to the Sarajevo Haggadah, an illuminated manuscript and the oldest Sephardic Jewish document in the world issued in Barcelona around 1350, containing the traditional Jewish Haggadah, is on permanent display at the museum. It is the only remaining illustrated Sephardic Haggadah in the world. The National Museum also hosts year-round exhibitions pertaining to local, regional and international culture and history, and exhibits over 5,000 artefacts from Bosnia's history.
The Alija Izetbegović Museum was opened on 19 October 2007 and is located in the old town fort, more specifically in the Vratnik Kapija towers Ploča and Širokac. The museum is a commemoration to the influence and body of work of Alija Izetbegović, the first president of the Republic of Bosnia and Herzegovina.
The city also hosts the Sarajevo National Theater, established in 1919, as well as East West Theatre Company and the Sarajevo Youth Theatre. Some other cultural institutions include the Center for Sarajevo Culture, Sarajevo City Library, Art Gallery of Bosnia and Herzegovina, and the Bosniak Institute, a privately owned library and art collection focusing on Bosniak history.
Demolitions associated with the war, as well as reconstruction, destroyed several institutions and cultural or religious symbols including the Gazi Husrev-beg library, the national library, the Sarajevo Oriental Institute, and a museum dedicated to the 1984 Olympic games. Consequently, the different levels of government established strong cultural protection laws and institutions. Bodies charged with cultural preservation in Sarajevo include the Institute for the Protection of the Cultural, Historical and Natural Heritage of Bosnia and Herzegovina (and their Sarajevo Canton counterpart), and the Bosnia and Herzegovina Commission to Preserve National Monuments.
Music.
Sarajevo is and has historically been one of the most important musical enclaves in the region. The Sarajevo school of pop rock developed in the city between 1961 and 1991. This type of music began with bands like Indexi, and singer/song writer Kemal Monteno. It continued into the 1980s, with bands such as Plavi Orkestar, Crvena Jabuka, and Divlje Jagode, by most accounts, pioneering the regional rock and roll movement. Sarajevo was also the home and birthplace of arguably the most popular and influential Yugoslav rock band of all time, Bijelo Dugme, somewhat of a Bosnian parallel to the Rolling Stones, in both popularity and influence. Sarajevo was also the home of a very notable post-punk urban subculture known as the New Primitives, which began during the early 1980s with the Baglama Band which was banned shortly after first LP and was brought into the mainstream through bands such as Zabranjeno Pušenje and Elvis J. Kurtović & His Meteors, as well as the Top Lista Nadrealista radio, and later television show. Other notable bands considered to be part of this subculture are Bombaj Štampa. Besides and separately from the "New Primitives", Sarajevo is the hometown to one of the most significant ex-Yugoslavian alternative industrial-noise bands, SCH (1983–current).
Perhaps more importantly, Sarajevo in the late 19th and throughout the 20th century was home to a burgeoning and large center of Sevdalinka record-making and contributed greatly to bringing this historical genre of music to the mainstream, which had for many centuries been a staple of Bosnian culture. Songwriters and musicians such as Himzo Polovina, Safet Isović, Zaim Imamović, Zehra Deović, Halid Bešlić, Hanka Paldum, Nada Mamula, Meho Puzić and many more composed and wrote some of their most important pieces in the city.
Sarajevo also greatly influenced the pop scene of Yugoslavia with musicians like Dino Merlin, Hari Mata Hari, Tifa, Kemal Monteno, Željko Bebek, and many more.
Many newer Sarajevo-baseed bands have also found a name and established themselves in Sarajevo, such as Regina who also had two albums out in Yugoslavia and Letu Štuke, who actually formed their band in Yugoslavia with the famous Bosnian-American writer Aleksandar Hemon and got their real breakthrough later in the 2000s. Sarajevo is now home to an important and eclectic mix of new bands and independent musicians, which continue to thrive with the ever-increasing number of festivals, creative showcases and concerts around the country. The city is also home to the region's largest jazz festival, the Sarajevo Jazz Festival (see "Festival" section below this).
Festivals.
Sarajevo is internationally renowned for its eclectic and diverse selection of festivals. The Sarajevo Film Festival was established in 1995 during the Bosnian War and has become the premier and largest film festival in the Balkans and South-East Europe. The Sarajevo Winter Festival, Sarajevo Jazz Festival and Sarajevo International Music Festival are well-known, as is the Baščaršija Nights festival, a month-long showcase of local culture, music, and dance.
The Sarajevo Film Festival has been hosted at the National Theater, with screenings at the Open-air theater Metalac and the Bosnian Cultural Center, all located in downtown Sarajevo and has been attended by celebrities such as Angelina Jolie, Brad Pitt, Emile Hirsch, Orlando Bloom, Daniel Craig, Danny Glover, John Malkovich, Morgan Freeman, Steve Buscemi, Bono Vox (Bono holds dual Bosnian and Irish citizenship and is an honorary citizen of Sarajevo), Nick Cave, Coolio, Stephen Frears, Mickey Rourke, Michael Moore, Gérard Depardieu, Darren Aronofsky, Sophie Okonedo, Gillian Anderson, Kevin Spacey Willem Dafoe, Eric Cantona and many more.
In the past sixteen years, the festival has entertained people and celebrities alike, elevating it to a recognized international level. The first incarnation of the Sarajevo Film Festival was hosted in still-warring Sarajevo in 1995, and has now progressed into being the biggest and most significant festival in south-eastern Europe. A talent campus is also held during the duration of the festival, with numerous world-renowned lecturers speaking on behalf of world cinematography and holding workshops for film students from across South-Eastern Europe.
The Sarajevo Jazz Festival is the region's largest and most diverse of its kind and has been entertaining jazz connoisseurs for over ten years and has hosted such artists as Richard Bona, Biréli Lagrène, Cristina Branco, Dhafer Youssef, Bugge Wesseltoft, Dennis Chambers, Joseph Tawadros and many more. The festival takes place at the Bosnian Cultural Center (aka "Main Stage"), just down the street from the SFF, at the Sarajevo Youth Stage Theater (aka "Strange Fruits Stage"), at the Dom Vojske Federacije (aka "Solo Stage"), and at the CDA (aka "Groove Stage").
Sports.
The city was the location of the 1984 Winter Olympics. Yugoslavia won one medal, a silver in men's giant slalom awarded to Jure Franko. Many of the Olympic facilities survived the war or were reconstructed, including Olympic Hall Zetra and Asim Ferhatović Stadion. After co-hosting the Southeast Europe Friendship games, Sarajevo was awarded the 2009 Special Olympic winter games, but cancelled these plans. The ice arena for the 1984 Olympics, Zetra Stadium, was used during the war as a temporary hospital and, later, for housing NATO troops of the IFOR.
In 2011 Sarajevo was the host city of the 51st World Military Skiing Championship with over 350 participants from 23 different nations. This was the first international event of such standing since the 1984 Olympics.
Football (soccer) is popular in Sarajevo; the city hosts "FK Sarajevo" and "FK Željezničar", which both compete in European and international cups and tournaments and are have a very large trophy cabinet in the former Yugoslavia as well as independent Bosnia and Herzegovina. Other notable soccer clubs are "FK Olimpik" and "SAŠK". Another popular sport is basketball; the basketball club KK Bosna Sarajevo won the European Championship in 1979 as well as many Yugoslav and Bosnian national championships making it one of the greatest basketball clubs in the former Yugoslavia. The chess club, "Bosna" Sarajevo, has been a championship team since the 1980s and is the third ranked chess club in Europe, having won four consecutive European championships in the nineties. RK Bosna also competes in the European Champions League and is considered one of the most well organised handball clubs in South-Eastern Europe with a very large fan base and excellent national, as well as international results.
Sarajevo often holds international events and competitions in sports such as tennis and kickboxing.
The popularity of tennis has been picking up in recent years. Since 2003, BH Telecom Indoors is an annual tennis tournament in Sarajevo.
Since 2007, the Sarajevo Marathon is being organized in late September.
In 2017, Sarajevo and East Sarajevo will host the European Youth Olympic Winter Festival (EYOWF).
External links.
Listen to this article ()
This audio file was created from a revision of the "Sarajevo" article dated 2006-11-14, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="26787" url="http://en.wikipedia.org/wiki?curid=26787" title="Science fiction">
Science fiction

Science fiction is a genre of fiction dealing with imaginative content such as futuristic settings, futuristic science and technology, space travel, time travel, faster than light travel, parallel universes and extraterrestrial life. It usually eschews the supernatural, and unlike the related genre of fantasy, its imaginary elements are largely plausible within the scientifically established context of the story. Science fiction often explores the potential consequences of scientific and other innovations, and has been called a "literature of ideas."
Definition.
Science fiction is difficult to define, as it includes a wide range of subgenres and themes. Author and editor Damon Knight summed up the difficulty, saying "science fiction is what we point to when we say it", a definition echoed by author Mark C. Glassy, who argues that the definition of science fiction is like the definition of pornography: you do not know what it is, but you know it when you see it. Vladimir Nabokov argued that if we were rigorous with our definitions, Shakespeare's play "The Tempest" would have to be termed science fiction.
According to science fiction writer Robert A. Heinlein, "a handy short definition of almost all science fiction might read: realistic speculation about possible future events, based solidly on adequate knowledge of the real world, past and present, and on a thorough understanding of the nature and significance of the scientific method." Rod Serling's definition is "fantasy is the impossible made probable. Science fiction is the improbable made possible." Lester del Rey wrote, "Even the devoted aficionado—or fan—has a hard time trying to explain what science fiction is", and that the reason for there not being a "full satisfactory definition" is that "there are no easily delineated limits to science fiction."
Science fiction is largely based on writing rationally about alternative possible worlds or futures. It is similar to, but differs from fantasy in that, within the context of the story, its imaginary elements are largely possible within scientifically established or scientifically postulated physical laws (though some elements in a story might still be pure imaginative speculation).
The settings for science fiction are often contrary to those of consensus reality, but most science fiction relies on a considerable degree of suspension of disbelief, which is facilitated in the reader's mind by potential scientific explanations or solutions to various fictional elements. Science fiction elements include:
History.
As a means of understanding the world through speculation and storytelling, science fiction has antecedents which go back to an era when the dividing line separating the mythological from the historical tends to become somewhat blurred, though precursors to science fiction as literature can be seen in Lucian's "True History" in the 2nd century, some of the "Arabian Nights" tales, "The Tale of the Bamboo Cutter" in the 10th century and Ibn al-Nafis' "Theologus Autodidactus" in the 13th century.
A product of the budding Age of Reason and the development of modern science itself, Margaret Cavendish's "The Blazing World" (1666) and Jonathan Swift's "Gulliver's Travels" (1726) are some of of the first true science fantasy works,which both feature the adventures of the protagonist in fictional and fantastical places. together with Voltaire's "Micromégas" (1752) and Johannes Kepler's "Somnium" (1620–1630). Isaac Asimov and Carl Sagan considered the latter work the first science fiction story. It depicts a journey to the Moon and how the Earth's motion is seen from there. "The Blazing World" (1666), by English noblewoman Margaret Cavendish, has also been described as an early forerunner of science fiction. Another example is Ludvig Holberg's novel "Nicolai Klimii Iter Subterraneum" (1741).
Following the 18th-century development of the novel as a literary form, in the early 19th century, Mary Shelley's books "Frankenstein" (1818) and "The Last Man" helped define the form of the science fiction novel, and Brian Aldiss has argued that "Frankenstein" was the first work of science fiction. Later, Edgar Allan Poe wrote a story about a flight to the moon. More examples appeared throughout the 19th century.
Then with the dawn of new technologies such as electricity, the telegraph, and new forms of powered transportation, writers including H. G. Wells and Jules Verne created a body of work that became popular across broad cross-sections of society. Wells' "The War of the Worlds" (1898) describes an invasion of late Victorian England by Martians using tripod fighting machines equipped with advanced weaponry. It is a seminal depiction of an alien invasion of Earth.
In the late 19th century, the term "scientific romance" was used in Britain to describe much of this fiction. This produced additional offshoots, such as the 1884 novella "Flatland: A Romance of Many Dimensions" by Edwin Abbott Abbott. The term would continue to be used into the early 20th century for writers such as Olaf Stapledon.
In the early 20th century, pulp magazines helped develop a new generation of mainly American SF writers, influenced by Hugo Gernsback, the founder of "Amazing Stories" magazine. In 1912 Edgar Rice Burroughs published "A Princess of Mars", the first of his three-decade-long series of Barsoom novels, situated on Mars and featuring John Carter as the hero. The 1928 publication of Philip Nolan's original Buck Rogers story, "Armageddon 2419", in "Amazing Stories" was a landmark event. This story led to comic strips featuring Buck Rogers (1929), "Brick Bradford" (1933), and "Flash Gordon" (1934). The comic strips and derivative movie serials greatly popularized science fiction.
In the late 1930s, John W. Campbell became editor of "Astounding Science Fiction", and a critical mass of new writers emerged in New York City in a group called the Futurians, including Isaac Asimov, Damon Knight, Donald A. Wollheim, Frederik Pohl, James Blish, Judith Merril, and others. Other important writers during this period include E.E. (Doc) Smith, Robert A. Heinlein, Arthur C. Clarke, Olaf Stapledon, and A. E. van Vogt. Working outside the Campbell influence were Ray Bradbury and Stanisław Lem. Campbell's tenure at "Astounding" is considered to be the beginning of the Golden Age of science fiction, characterized by hard SF stories celebrating scientific achievement and progress. This lasted until post-war technological advances, new magazines such as "Galaxy", edited by H. L. Gold, and a new generation of writers began writing stories with less emphasis on the hard sciences and more on the social sciences.
In the 1950s, the Beat generation included speculative writers such as William S. Burroughs. In the 1960s and early 1970s, writers like Frank Herbert, Samuel R. Delany, Roger Zelazny, and Harlan Ellison explored new trends, ideas, and writing styles, while a group of writers, mainly in Britain, became known as the New Wave for their embrace of a high degree of experimentation, both in form and in content, and a highbrow and self-consciously "literary" or artistic sensibility. In the 1970s, writers like Larry Niven brought new life to hard science fiction. Ursula K. Le Guin and others pioneered soft science fiction.
In the 1980s, cyberpunk authors like William Gibson turned away from the optimism and support for progress of traditional science fiction. This dystopian vision of the near future is described in the work of Philip K. Dick, such as "Do Androids Dream of Electric Sheep?" and "We Can Remember It for You Wholesale", which resulted in the films "Blade Runner" and "Total Recall". The "Star Wars" franchise helped spark a new interest in space opera, focusing more on story and character than on scientific accuracy. C. J. Cherryh's detailed explorations of alien life and complex scientific challenges influenced a generation of writers.
Emerging themes in the 1990s included environmental issues, the implications of the global Internet and the expanding information universe, questions about biotechnology and nanotechnology, as well as a post-Cold War interest in post-scarcity societies; Neal Stephenson's "The Diamond Age" comprehensively explores these themes. Lois McMaster Bujold's "Vorkosigan" novels brought the character-driven story back into prominence. The television series ' (1987) began a torrent of new SF shows, including three further "Star Trek" spin-off shows (, , and ) and "Babylon 5". "Stargate", a movie about an ancient portal to other gates across the galaxy, was released in 1994. "Stargate SG-1", a TV series, premiered on July 27, 1997 and lasted 10 seasons with 214 episodes. Spin-offs include the animated television series "Stargate Infinity", the TV series "Stargate Atlantis" and "Stargate Universe", and the direct-to-DVD films ' and "". "Stargate SG-1" surpassed "The X-Files" as the longest-running North American science fiction television series, a record later broken by "Smallville".
Concern about the rapid pace of technological change crystallized around the concept of the technological singularity, popularized by Vernor Vinge's novel "Marooned in Realtime" and then taken up by other authors.
The term "sci-fi".
Forrest J Ackerman used the term sci-fi (analogous to the then-trendy "hi-fi") at UCLA in 1954. As science fiction entered popular culture, writers and fans active in the field came to associate the term with low-budget, low-tech "B-movies" and with low-quality pulp science fiction. By the 1970s, critics within the field such as Terry Carr and Damon Knight were using "sci-fi" to distinguish hack-work from serious science fiction, and around 1978, Susan Wood and others introduced the pronunciation "skiffy." Peter Nicholls writes that "SF" (or "sf") is "the preferred abbreviation within the community of sf writers and readers." David Langford's monthly fanzine "Ansible" includes a regular section "As Others See Us" which offers numerous examples of "sci-fi" being used in a pejorative sense by people outside the genre.
Innovation.
Science fiction has criticized developing and future technologies, but also initiates innovation and new technology. This topic has been more often discussed in literary and sociological than in scientific forums. Cinema and media theorist Vivian Sobchack examines the dialogue between science fiction films and the technological imagination. Technology impacts artists and how they portray their fictionalized subjects, but the fictional world gives back to science by broadening imagination. "How William Shatner Changed the World" is a documentary that gave a number of real-world examples of actualized technological imaginations. While more prevalent in the early years of science fiction with writers like Arthur C. Clarke, new authors still find ways to make currently impossible technologies seem closer to being realized.
Doctor Who is currently the longest running science fiction television series, running from 1963 to now.
Categories.
Hard SF.
Hard science fiction, or "hard SF", is characterized by rigorous attention to accurate detail in the natural sciences, especially physics, astrophysics, and chemistry, or on accurately depicting worlds that more advanced technology may make possible. Some accurate predictions of the future come from the hard science fiction subgenre, but numerous inaccurate predictions have emerged as well. Some hard SF authors have distinguished themselves as working scientists, including Gregory Benford, Geoffrey A. Landis, David Brin, and Robert L. Forward, while mathematician authors include Rudy Rucker and Vernor Vinge. Other noteworthy hard SF authors include Isaac Asimov, Arthur C. Clarke, Hal Clement, Greg Bear, Larry Niven, Robert J. Sawyer, Stephen Baxter, Alastair Reynolds, Charles Sheffield, Ben Bova, Kim Stanley Robinson, Anne McCaffery and Greg Egan.
Soft SF.
The description "soft" science fiction may describe works based on social sciences such as psychology, economics, political science, sociology, and anthropology. Noteworthy writers in this category include Ursula K. Le Guin and Philip K. Dick. The term can describe stories focused primarily on character and emotion; SFWA Grand Master Ray Bradbury was an acknowledged master of this art. The Eastern Bloc produced a large quantity of social science fiction, including works by Polish authors Stanislaw Lem and Janusz Zajdel, as well as Soviet authors such as the Strugatsky brothers, Kir Bulychov, Yevgeny Zamyatin and Ivan Yefremov. Some writers blur the boundary between hard and soft science fiction.
Related to social SF and soft SF are utopian and dystopian stories; George Orwell's "Nineteen Eighty-Four", Aldous Huxley's "Brave New World", and Margaret Atwood's "The Handmaid's Tale" are examples. Satirical novels with fantastic settings such as "Gulliver's Travels" by Jonathan Swift may also be considered science fiction or speculative fiction.
Subgenres.
Cyberpunk.
The cyberpunk genre emerged in the early 1980s; combining cybernetics and punk,
the term was coined by author Bruce Bethke for his 1980 short story "Cyberpunk".
The time frame is usually near-future and the settings are often dystopian in nature and characterized by misery. Common themes in cyberpunk include advances in information technology and especially the Internet, visually abstracted as cyberspace, artificial intelligence, and prosthetics and post-democratic societal control where corporations have more influence than governments. Nihilism, post-modernism, and film noir techniques are common elements, and the protagonists may be disaffected or reluctant anti-heroes. Noteworthy authors in this genre are William Gibson, Bruce Sterling, Neal Stephenson, and Pat Cadigan. James O'Ehley has called the 1982 film "Blade Runner" a definitive example of the "cyberpunk" visual style.
Time travel.
Time travel stories have antecedents in the 18th and 19th centuries. The first major time travel novel was Mark Twain's "A Connecticut Yankee in King Arthur's Court". The most famous is H. G. Wells' 1895 novel "The Time Machine", which uses a vehicle that allows an operator to travel purposefully and selectively, while Twain's time traveler is struck in the head. The term "time machine", coined by Wells, is now universally used to refer to such a vehicle. "Back to the Future" is one of the most popular franchises of this category. Stories of this type are complicated by logical problems such as the grandfather paradox. Time travel continues to be a popular subject in modern science fiction, in print, movies, and television episodes of "Stargate SG1" and the BBC television series "Doctor Who".
Alternate history.
Alternative history stories are based on the premise that historical events might have turned out differently. These stories may use time travel to change the past, or may simply set a story in a universe with a different history from our own. Classics in the genre include "Bring the Jubilee" by Ward Moore, in which the South wins the American Civil War, and "The Man in the High Castle" by Philip K. Dick, in which Germany and Japan win World War II. The Sidewise Award acknowledges the best works in this subgenre; the name is taken from Murray Leinster's 1934 story "Sidewise in Time". Harry Turtledove is one of the most prominent authors in the subgenre and is sometimes called the "master of alternate history."
Military SF.
Military science fiction is set in the context of conflict between national, interplanetary, or interstellar armed forces; the primary viewpoint characters are usually soldiers. Stories include detail about military technology, procedure, ritual, and history; military stories may use parallels with historical conflicts. Heinlein's "Starship Troopers" is an early example, along with the Dorsai novels of Gordon Dickson. Joe Haldeman's "The Forever War" is a critique of the genre, a Vietnam-era response to the World War II–style stories of earlier authors. Prominent military SF authors include John Scalzi, John Ringo, David Drake, David Weber, Tom Kratman, Michael Z. Williamson, S. M. Stirling, John Carr, and Don Hawthorne. The publishing company Baen Books is known for cultivating several of these military science fiction authors.
Superhuman.
Superhuman stories deal with the emergence of humans who have abilities beyond the norm. This can stem either from natural causes such as in Olaf Stapledon's novel "Odd John", Theodore Sturgeon's "More Than Human", and Philip Wylie's "Gladiator", or be the result of scientific advances, such as the intentional augmentation in A. E. van Vogt's novel "Slan". These stories usually focus on the alienation that these beings feel as well as society's reaction to them. These stories have played a role in the real life discussion of human enhancement. Frederik Pohl's "Man Plus" also belongs to this category.
Apocalyptic and post-apocalyptic.
Apocalyptic fiction is concerned with the end of civilization through war ("On the Beach"), pandemic ("The Last Man"), astronomic impact ("When Worlds Collide"), ecological disaster ("The Wind from Nowhere"), or some other general disaster or with a world or civilization after such a disaster. Typical of the genre are George R. Stewart's novel "Earth Abides" and Pat Frank's novel "Alas, Babylon". Apocalyptic fiction generally concerns the disaster itself and the direct aftermath, while post-apocalyptic fiction can deal with anything from the near aftermath (as in Cormac McCarthy's "The Road") to 375 years in the future (as in "By The Waters of Babylon") to hundreds or thousands of years in the future, as in Russell Hoban's novel "Riddley Walker" and Walter M. Miller, Jr.'s "A Canticle for Leibowitz". Apocalyptic science-fiction is a popular genre in video games. The critically acclaimed role-playing action adventure video game series, "Fallout", is set on a post-apocalyptic Earth, where civilization is recovering from a nuclear war as survivors struggle to survive and seek to rebuild society.
Space opera.
Space opera is adventure science fiction set mainly or entirely in outer space or on multiple (sometimes distant) planets. The conflict is heroic, and typically on a large scale.
The term "space opera" is sometimes used pejoratively, to describe improbable plots, absurd science, and cardboard characters. But it is also used nostalgically, and modern space opera may be an attempt to recapture the sense of wonder of the golden age of science fiction. The pioneer of this subgenre is generally recognized to be Edward E. (Doc) Smith, with his "Skylark" and "Lensman" series. Gene Roddenberry's "Star Trek" and George Lucas's "Star Wars" series are among the most popular and famous franchises in cinematic space opera. It covers epic battles between good and evil throughout an entire galaxy. Alastair Reynolds's "Revelation Space" series, Peter F. Hamilton's "Void", "Night's Dawn", "Pandora's Star" series, Vernor Vinge's "A Fire Upon the Deep", "A Deepness in the Sky" are newer examples of this genre. A prime example of the space opera genre seen in video games is the "Mass Effect" series.
Space Western.
Space Western transposes themes of the American Western books and film to a backdrop of futuristic space frontiers. These stories typically involve colony worlds that have only recently been terraformed and/or settled serving as stand-ins for the backdrop of lawlessness and economic expansion that were predominant in the American west. Examples include the Sean Connery film "Outland", Heinlein's "Farmer in the Sky", the "Firefly" television series, and the film sequel "Serenity" by Joss Whedon, as well as the manga and anime series "Trigun", "Outlaw Star", and "Cowboy Bebop".
Social science fiction.
Social science fiction is a science fiction subgenre that focuses on themes of human society and human nature in a science fiction setting. Since it usually focuses more on the speculation of humanity and less on scientific accuracy, it's usually placed within soft science fiction.
Related genres.
Other speculative fiction, fantasy, and horror.
The broader category of speculative fiction includes science fiction, fantasy, alternate histories (which may have no particular scientific or futuristic component), and even literary stories that contain fantastic elements, such as the work of Jorge Luis Borges or John Barth. For some editors, magic realism is considered to be within the broad definition of speculative fiction.
Fantasy.
Fantasy is commonly associated with science fiction, and a number of writers have worked in both genres, while writers such as Anne McCaffrey, Ursula K. Le Guin, and Marion Zimmer Bradley have written works that appear to blur the boundary between the two related genres. The authors' professional organization is called the Science Fiction and Fantasy Writers of America (SFWA). SF conventions routinely have programming on fantasy topics, and fantasy authors such as J. K. Rowling have won the highest honor within the science fiction field, the Hugo Award.
In general, science fiction differs from fantasy in that the former concerns things that might someday be possible or that at least embody the pretense of realism. Supernaturalism, usually absent in science fiction, is the distinctive characteristic of fantasy literature. A dictionary definition referring to fantasy literature is "fiction characterized by highly fanciful or supernatural elements." Examples of fantasy supernaturalism include magic (spells, harm to opponents), magical places (Narnia, Oz, Middle Earth, Hogwarts), supernatural creatures (witches, vampires, orcs, trolls), supernatural transportation (flying broomsticks, ruby slippers, windows between worlds), and shapeshifting (beast into man, man into wolf or bear, lion into sheep). Such things are basic themes in fantasy.
Literary critic Fredric Jameson has characterized the difference between the two genres by describing science fiction as turning "on a formal framework determined by concepts of the mode of production rather than those of religion" – that is, science fiction texts are bound by an inner logic based more on historical materialism than on magic or the forces of good and evil. Some narratives are described as being essentially science fiction but "with fantasy elements." The term "science fantasy" is sometimes used to describe such material.
Supernatural fiction.
Supernatural fiction is a genre that features supernatural and other paranormal phenomenon in stories and settings.
Science fantasy.
Science fantasy is a genre where elements of science fiction and fantasy co-exist or combine. Stories and franchises that display fictional science as well as supernatural elements, sorcery or/and "magical technology" are considered science fantasy.
Climate fiction.
A relatively new term used to describe works based around themes of reaction to major climate change. It takes its name as a shortening of climate fiction, much as science fiction is often shortened to "sci-fi." Cli-fi novels and films are often set in either the present or the near or distant future, but they can also be set in the past. Many cli-fi works raise awareness about the major threats that global warming and climate change present to life on Earth.
Horror fiction.
Horror fiction is the literature of the unnatural and supernatural, with the aim of unsettling or frightening the reader, sometimes with graphic violence. Historically it has also been known as weird fiction. Although horror is not "per se" a branch of science fiction, some works of horror literature incorporates science fictional elements. One of the defining classical works of horror, Mary Shelley's novel "Frankenstein", is the first fully realized work of science fiction, where the manufacture of the monster is given a rigorous science-fictional grounding. The works of Edgar Allan Poe also helped define both the science fiction and the horror genres. Today horror is one of the most popular categories of films. Horror is often mistakenly categorized as science fiction at the point of distribution by libraries, video rental outlets, etc.
Mystery fiction.
Works in which science and technology are a dominant theme, but based on current reality, may be considered mainstream fiction. Much of the thriller genre would be included, such as the novels of Tom Clancy or Michael Crichton, or the James Bond films. Modernist works from writers like Kurt Vonnegut, Philip K. Dick, and Stanisław Lem have focused on speculative or existential perspectives on contemporary reality and are on the borderline between SF and the mainstream. According to Robert J. Sawyer, "Science fiction and mystery have a great deal in common. Both prize the intellectual process of puzzle solving, and both require stories to be plausible and hinge on the way things really do work." Isaac Asimov, Walter Mosley, and other writers incorporate mystery elements in their science fiction, and vice versa.
Distinct from the above, a full-fledged Science Fiction Mystery is one which is set in a completely different world from ours, in which the circumstances and motives of the crime committed and the identity of the detective(s) seeking to solve it are of an essentially science fictional character. A prime example is Isaac Asimov's "The Caves of Steel" and its sequels, set in a world thousands of years in the future and presenting the Robot detective R. Daneel Olivaw. An allied genre is the Fantasy Mystery, a detective mystery set in a world of fantasy - such as the Lord Darcy mysteries taking place in a world where magic works, or "The Idylls of the Queen" set in the mythical King Arthur's court.
Superhero fiction.
Superhero fiction is a genre characterized by beings with much higher than usual capability and prowess, generally with a desire or need to help the citizens of their chosen country or world by using his or her powers to defeat natural or superpowered threats. A number of superhero fiction characters involve themselves (either intentionally or accidentally) with science fiction and fact, including advanced technologies, alien worlds, time travel, and interdimensional travel; but the standards of scientific plausibility are lower than with actual science fiction. Authors of this genre include Stan Lee (co-creator of "Spider-Man", the "Fantastic Four", the "X-Men", and the "Hulk"); Marv Wolfman, the creator of "Blade" for Marvel Comics, and "The New Teen Titans" for DC Comics; Dean Wesley Smith ("Smallville", "Spider-Man", and "X-Men" novels) and "Superman" writers Roger Stern and Elliot S! Maggin.
Fandom and community.
Science fiction fandom is the "community of the literature of ideas... the culture in which new ideas emerge and grow before being released into society at large." Members of this community, "fans", are in contact with each other at conventions or clubs, through print or online fanzines, or on the Internet using web sites, mailing lists, and other resources.
SF fandom emerged from the letters column in "Amazing Stories" magazine. Soon fans began writing letters to each other, and then grouping their comments together in informal publications that became known as fanzines. Once they were in regular contact, fans wanted to meet each other, and they organized local clubs. In the 1930s, the first science fiction conventions gathered fans from a wider area. Conventions, clubs, and fanzines were the dominant form of fan activity, or "fanac", for decades, until the Internet facilitated communication among a much larger population of interested people.
Authors.
Science fiction is being written worldwide by a diverse population of authors. According to 2013 statistics by the science fiction publisher Tor Books, men outnumber women by 78% to 22% among submissions to the publisher. A controversy about voting slates in the 2015 Hugo Awards highlighted tensions in the science fiction community between a trend of increasingly diverse works and authors being honored by awards, and a backlash by groups of authors and fans who preferred what they considered more traditional science fiction.
Awards.
Among the most respected awards for science fiction are the Hugo Award, presented by the World Science Fiction Society at Worldcon; the Nebula Award, presented by SFWA and voted on by the community of authors; and the John W. Campbell Memorial Award for Best Science Fiction Novel and Theodore Sturgeon Memorial Award for short fiction. One notable award for science fiction films is the Saturn Award. It is presented annually by The Academy of Science Fiction, Fantasy, and Horror Films.
There are national awards, like Canada's Prix Aurora Awards, regional awards, like the Endeavour Award presented at Orycon for works from the Pacific Northwest, special interest or subgenre awards like the Chesley Award for art or the World Fantasy Award for fantasy. Magazines may organize reader polls, notably the Locus Award.
Conventions, clubs, and organizations.
Conventions (in fandom, shortened as "cons"), are held in cities around the world, catering to a local, regional, national, or international membership. General-interest conventions cover all aspects of science fiction, while others focus on a particular interest like media fandom, filking, etc. Most are organized by volunteers in non-profit groups, though most media-oriented events are organized by commercial promoters. The convention's activities are called the "program", which may include panel discussions, readings, autograph sessions, costume masquerades, and other events. Activities that occur throughout the convention are not part of the program; these commonly include a dealer's room, art show, and hospitality lounge (or "con suites").
Conventions may host award ceremonies; Worldcons present the Hugo Awards each year. SF societies, referred to as "clubs" except in formal contexts, form a year-round base of activities for science fiction fans. They may be associated with an ongoing science fiction convention, or have regular club meetings, or both. Most groups meet in libraries, schools and universities, community centers, pubs or restaurants, or the homes of individual members. Long-established groups like the New England Science Fiction Association and the Los Angeles Science Fantasy Society have clubhouses for meetings and storage of convention supplies and research materials. The Science Fiction and Fantasy Writers of America (SFWA) was founded by Damon Knight in 1965 as a non-profit organization to serve the community of professional science fiction authors, 24 years after his essay "Unite or Fie!" had led to the organization of the National Fantasy Fan Federation. Fandom has helped incubate related groups, including media fandom, the Society for Creative Anachronism, gaming, filking, and furry fandom.
Fanzines and online fandom.
The first science fiction fanzine, "The Comet", was published in 1930. Fanzine printing methods have changed over the decades, from the hectograph, the mimeograph, and the ditto machine, to modern photocopying. Distribution volumes rarely justify the cost of commercial printing. Modern fanzines are printed on computer printers or at local copy shops, or they may only be sent as email. The best known fanzine (or "'zine") today is "Ansible", edited by David Langford, winner of numerous Hugo awards. Other fanzines to win awards in recent years include "File 770", "Mimosa", and "Plokta". Artists working for fanzines have risen to prominence in the field, including Brad W. Foster, Teddy Harvia, and Joe Mayhew; the Hugos include a category for Best Fan Artists. The earliest organized fandom online was the community, originally a mailing list in the late 1970s with a text archive file that was updated regularly. In the 1980s, Usenet groups greatly expanded the circle of fans online. In the 1990s, the development of the World-Wide Web exploded the community of online fandom by orders of magnitude, with thousands and then literally millions of web sites devoted to science fiction and related genres for all media. Most such sites are small, ephemeral, and/or very narrowly focused, though sites like SF Site and SFcrowsnest offer a broad range of references and reviews about science fiction.
Fan fiction.
Fan fiction, known to aficionados as "fanfic", is non-commercial fiction created by fans in the setting of an established book, film, video game, or television series. This modern meaning of the term should not be confused with the traditional (pre-1970s) meaning of "fan fiction" within the community of fandom, where the term meant original or parody fiction written by fans and published in fanzines, often with members of fandom as characters therein. Examples of this would include the Goon Defective Agency stories, written starting in 1956 by Irish fan John Berry and published in his and Arthur Thomson's fanzine "Retribution". In the last few years, sites have appeared such as Orion's Arm and Galaxiki, which encourage collaborative development of science fiction universes. In some cases, the copyright owners of the books, films, or television series have instructed their lawyers to issue "cease and desist" letters to fans.
Science fiction studies.
" style="
 width:180px; 
 padding: 6px; 
 border: 1px solid #aaa; 
 font-size: 88%; 
 background-color: #F9F9F9; 
[Science fiction] is the one real international literary form we have today, and as such has branched out to visual media, interactive media and on to whatever new media the world will invent in the 21st century... crossover issues between the sciences and the humanities are crucial for the century to come.
”
 George Edgar Slusser
The study of science fiction, or science fiction studies, is the critical assessment, interpretation, and discussion of science fiction literature, film, new media, fandom, and fan fiction. Science fiction scholars take science fiction as an object of study in order to better understand it and its relationship to science, technology, politics, and culture-at-large. Science fiction studies has a long history dating back to the turn of the 20th century, but it was not until later that science fiction studies solidified as a discipline with the publication of the academic journals Extrapolation (1959), Foundation - The International Review of Science Fiction (1972), and Science Fiction Studies (1973), and the establishment of the oldest organizations devoted to the study of science fiction, the Science Fiction Research Association and the Science Fiction Foundation, in 1970. The field has grown considerably since the 1970s with the establishment of more journals, organizations, and conferences with ties to the science fiction scholarship community, and science fiction degree-granting programs such as those offered by the University of Liverpool and Kansas University.
The National Science Foundation has conducted surveys of "Public Attitudes and Public Understanding" of "Science Fiction and Pseudoscience." They write that "Interest in science fiction may affect the way people think about or relate to science...one study found a strong relationship between preference for science fiction novels and support for the space program...The same study also found that students who read science fiction are much more likely than other students to believe that contacting extraterrestrial civilizations is both possible and desirable (Bainbridge 1982).
As serious literature.
Mary Shelley wrote a number of science fiction novels including "Frankenstein", and is treated as a major Romantic writer. A number of science fiction works have received critical acclaim including "Childhood's End" and "Do Androids Dream of Electric Sheep?" (the inspiration for the movie "Blade Runner"). A number of respected writers of mainstream literature have written science fiction, including Aldous Huxley's "Brave New World", George Orwell's "Nineteen Eighty-Four", Anthony Burgess' "A Clockwork Orange" and Margaret Atwood's "The Handmaid's Tale". Nobel Laureate Doris Lessing wrote a series of SF novels, "Canopus in Argos", and nearly all of Kurt Vonnegut's works contain science fiction premises or themes.
The scholar Tom Shippey asks a perennial question of science fiction: "What is its relationship to fantasy fiction, is its readership still dominated by male adolescents, is it a taste which will appeal to the mature but non-eccentric literary mind?" In her much reprinted essay "Science Fiction and Mrs Brown," the science fiction writer Ursula K. Le Guin has approached an answer by first citing the essay written by the English author Virginia Woolf entitled "Mr Bennett and Mrs Brown" in which she states:
I believe that all novels, ... deal with character, and that it is to express character – not to preach doctrines, sing songs, or celebrate the glories of the British Empire, that the form of the novel, so clumsy, verbose, and undramatic, so rich, elastic, and alive, has been evolved ... The great novelists have brought us to see whatever they wish us to see through some character. Otherwise they would not be novelists, but poets, historians, or pamphleteers.
Le Guin argues that these criteria may be successfully applied to works of science fiction and so answers in the affirmative her rhetorical question posed at the beginning of her essay: "Can a science fiction writer write a novel?"
Tom Shippey in his essay does not dispute this answer but identifies and discusses the essential differences that exists between a science fiction novel and one written outside the field. To this end, he compares George Orwell's "Coming Up for Air" with Frederik Pohl and C. M. Kornbluth's "The Space Merchants" and concludes that the basic building block and distinguishing feature of a science fiction novel is the presence of the "novum", a term Darko Suvin adapts from Ernst Bloch and defines as "a discrete piece of information recognizable as not-true, but also as not-unlike-true, not-flatly- (and in the current state of knowledge) impossible."
In science fiction the style of writing is often relatively clear and straightforward compared to classical literature. Orson Scott Card, an author of both science fiction and non-SF fiction, has postulated that in science fiction the message and intellectual significance of the work is contained within the story itself and, therefore, there need not be stylistic gimmicks or literary games; but that some writers and critics confuse clarity of language with lack of artistic merit. In Card's words:
...a great many writers and critics have based their entire careers on the premise that anything that the general public can understand without mediation is worthless drivel. [...] If everybody came to agree that stories should be told this clearly, the professors of literature would be out of job, and the writers of obscure, encoded fiction would be, not honored, but pitied for their impenetrability.
Science fiction author and physicist Gregory Benford has declared that: "SF is perhaps the defining genre of the twentieth century, although its conquering armies are still camped outside the Rome of the literary citadels." This sense of exclusion was articulated by Jonathan Lethem in an essay published in the "Village Voice" entitled "Close Encounters: The Squandered Promise of Science Fiction." Lethem suggests that the point in 1973 when Thomas Pynchon's "Gravity's Rainbow" was nominated for the Nebula Award, and was passed over in favor of Arthur C. Clarke's "Rendezvous with Rama", stands as "a hidden tombstone marking the death of the hope that SF was about to merge with the mainstream." Among the responses to Lethem was one from the editor of the "Magazine of Fantasy and Science Fiction" who asked: "When is it [the SF genre] ever going to realize it can't win the game of trying to impress the mainstream?" On this point the journalist and author David Barnett has remarked:
The ongoing, endless war between "literary" fiction and "genre" fiction has well-defined lines in the sand. Genre's foot soldiers think that literary fiction is a collection of meaningless but prettily drawn pictures of the human condition. The literary guard consider genre fiction to be crass, commercial, whizz-bang potboilers. Or so it goes.
Barnett, in an earlier essay had pointed to a new development in this "endless war":
 What do novels about a journey across post-apocalyptic America, a clone waitress rebelling against a future society, a world-girdling pipe of special gas keeping mutant creatures at bay, a plan to rid a colonizable new world of dinosaurs, and genetic engineering in a collapsed civilization have in common? 
They are all most definitely not science fiction. 
Literary readers will probably recognize "The Road" by Cormac McCarthy, one of the sections of "Cloud Atlas" by David Mitchell, "The Gone-Away World" by Nick Harkaway, "The Stone Gods" by Jeanette Winterson and "Oryx and Crake" by Margaret Atwood from their descriptions above. All of these novels use the tropes of what most people recognize as science fiction, but their authors or publishers have taken great pains to ensure that they are not categorized as such. 
World-wide examples.
Although perhaps most developed as a genre and community in the United States, Canada, and the United Kingdom, science fiction is a worldwide phenomenon. Organisations devoted to promotion and even translation in particular countries are commonplace, as are country- or language-specific genre awards.
Africa.
Mohammed Dib, an Algerian writer, wrote a science fiction allegory about his nation's politics, "Qui se souvient de la mer" ("Who Remembers the Sea?") in 1962.
Masimba Musodza, a Zimbabwean author, published "MunaHacha Maive Nei?" the first science-fiction novel in the Shona language, which also holds the distinction of being the first novel in the Shona language to appear as an ebook first before it came out in print. In South Africa, a movie titled "District 9" came out in 2009, an apartheid allegory featuring extraterrestrial life forms, produced by Peter Jackson.
Science fiction examines society through shifting power structures (such as the shift of power from humanity to alien overlords). African science fiction often uses this genre norm to situate slavery and the slave trade as an alien abduction. Commonalities in experiences with unknown languages, customs, and culture lend themselves well to this comparison. The subgenre also commonly employs the mechanism of time travel to examine the effects of slavery and forced emigration on the individual and the family.
Asia.
Indian science fiction, defined loosely as science fiction by writers of Indian descent, began with the English-language publication of Kylas Chundar Dutt's "A Journal of Forty-Eight Hours of the Year 1945" in the "Calcutta Literary Gazette" (June 6, 1835). Since this story was intended as a political polemic, credit for the first science fiction story is often given to later Bengali authors such as Jagadananda Roy, Hemlal Dutta and the polymath Jagadish Chandra Bose. Eminent film maker and writer Satyajit Ray also enriched Bengali science fiction by writing many short stories as well as science fiction series, Professor Shonku. (see Bengali science fiction). Similar traditions exist in Hindi, Marathi, Tamil and English. In English, the modern era of Indian speculative fiction began with the works of authors such as Samit Basu, Payal Dhar, Vandana Singh and Anil Menon. Works such as Amitav Ghosh's "The Calcutta Chromosome" and Salman Rushdie's "Grimus" and Boman Desai's "The Memory of Elephants" are generally classified as magic realist works but make essential use of SF tropes and techniques. In recent years some other Indian languages have begun telling stories in this genre, for example in Punjabi IP Singh and Roop Dhillon have written stories that can clearly be defined as Punjabi Science Fiction. The latter has coined the term Vachitarvaad to describe such literature.
Modern science fiction in China mainly depends on the magazine "Science Fiction World". A number of works were published in installments in it originally, including the most successful fiction "Three Body", written by Liu Cixin.
Until recently, there has been little domestic science fiction literature in Korea. Within the small field, the author and critic writing under the nom de plume Djuna has been credited with being the major force. The upswing that began in 2009 has been attributed by Shin Junebong to a combination of factors. Shin goes on to quote the Korean science-fiction writer and editor as saying that, "'It looks like the various literary awards established by one newspaper after another, with hefty sums of prize money, had a big impact.'" Another factor cited was the active use of Web bulletin boards among the then-young writers brought up on translations of Western SF. In spite of the increase, at the time, there were still no more than sixty or so authors writing in the field at that time.
"Chalomot Be'aspamia" is an Israeli magazine of short science fiction and fantasy stories. "The Prophecies Of Karma", published in 2011, is advertised as the first work of science fiction by an Arabic author, the Lebanese writer Nael Gharzeddine.
Europe.
France and Belgium.
Jules Verne, a 19th-century French novelist known for his pioneering science fiction works ("Twenty Thousand Leagues Under the Sea", "Journey to the Center of the Earth", "From the Earth to the Moon") is the prime representative of the French legacy of science fiction. French science fiction of the 19th century was also represented with such artists as Albert Robida and Isidore Grandville. In the 20th century, traditions of French science fiction were carried on by writers like Pierre Boulle (best known for his "Planet of the Apes") Serge Brussolo, Bernard Werber, René Barjavel and Robert Merle, among others.
In Franco-Belgian comics, the bande dessinée ("BD") science-fiction is a well established genre. Among the notable French science fiction comics, there is "Valerian et Laureline" by Pierre Christin and Jean-Claude Mézières, a space opera franchise lasting since 1967. "Metal Hurlant" magazine (known in US as "Heavy Metal") was one of the largest contributors to Francophone science-fiction comics. Its major authors include Jean "Moebius" Giraud, creator of "Arzach", Chilean Alejandro Jodorowsky, who created a series of comics, including "L'Incal" and "Les Metabarons", set in Jodoverse, and Enki Bilal with "The Nikopol Trilogy". Giraud also contributed to French SF animation, collaborating with René Laloux on several animated features. A number of artists from neighboring countries, such as Spain and Italy, create science fiction and fantasy comics in French aimed at a Franco-Belgian market.
In French cinema, science fiction was started with silent film director and visual effects pioneer George Méliès, whose most famous film was "Voyage to the Moon", loosely based on books by Verne and Wells. In the 20th and 21st centuries, French science fiction films were represented by René Laloux animated features, as well as Enki Bilal's adaptation of Nikopol trilogy, "Immortal". Also, Luc Besson filmed "The Fifth Element" as a joint Franco-American production.
In the French-speaking world, the colloquial use of the term sci-fi is an accepted Anglicism for the word science fiction. This probably stems from the fact that science fiction writing never expanded there to the extent it did in the English-speaking world, particularly with the dominance of the United States. Nevertheless, France has made a tremendous contribution to science fiction in its seminal stages of development. Although the term "science fiction" is understood in France their penchant for the "weird and wacky" has a long tradition and is sometimes called "le culte du merveilleux." This uniquely French tradition certainly encompasses what the Anglophone world would call French science fiction but also ranges across fairies, Dada-ism and Surrealisme. 
Italy.
Italy has a vivid history in science fiction, though almost unknown outside her borders. 
The history of Italian science fiction recognizes a varied roadmap of this genre which spread to a popular level after World War Two, and in particular in the second half of the 1950s, on the wave of American and British literature.
The earliest pioneers may be found in the literature of the fantastic voyage and of the Renaissance Utopia, even in previous masterpieces such as "The Million" of Marco Polo. 
In the second half of the 19th century stories and short novels of "scientific fantasies" (also known as "incredible stories" or "fantastic" or "adventuristic", "novels of the future times" or "utopic", "of the tomorrow") appeared in Sunday newspaper supplements, in literary magazines, and as booklets published in installments. Added to these, at the beginning of the 20th century, were the most futuristic masterpieces of the great Emilio Salgari, considered by most the father of Italian science fiction, and Yambo and Luigi Motta, the most renowned authors of popular novels of the time, with extraordinary adventures in remote and exotic places, and even works of authors representing known figures of the "top" literature, among them Massimo Bontempelli, Luigi Capuana, Guido Gozzano, Ercole Luigi Morselli.
The true birth of Italian science fiction is placed in 1952, with the publishing of the first specialized magazines, "Scienza Fantastica" (Fantastic Science) and "Urania," and with the appearance of the term "fantascienza" which has become the usual translation of the English term "science fiction." The "Golden Years" span the period 1957-1960.
From the end of the 1950s science fiction became in Italy one of the most popular genres, although its popular success was not followed by critical success. In spite of an active and organized fandom there hasn't been an authentic sustained interest on the part of the Italian cultural élite towards science fiction.
Popular Italian science fiction writers include Gianluigi Zuddas, Giampietro Stocco, Lino Aldani, as well as comic artists, such as Milo Manara. Valerio Evangelisti is the best known modern author of Italian science fiction and fantasy.
Also, popular Italian children's writer Gianni Rodari often turned to science fiction aimed at children, most notably, in "Gip in the Television".
Germany.
The main German science fiction writer in the 19th century was Kurd Laßwitz. According to Austrian SF critic Franz Rottensteiner, though significant German novels of a science-fiction nature were published in the first half of the 20th century, SF did not exist as a genre in the country until after World War II and the heavy importing and translation of American works. In the 20th century, during the years of divided Germany, both East and West spawned a number of successful writers. Top East German writers included Angela and Karlheinz Steinmüller, as well as Günther Krupkat. West German authors included Carl Amery, Gudrun Pausewang, Wolfgang Jeschke and Frank Schätzing, among others. A well known science fiction book series in the German language is "Perry Rhodan", which started in 1961. Having sold over one billion copies (in pulp format), it claims to be the most successful science fiction book series ever written, worldwide. Current well-known SF authors from Germany are five-time "Kurd-Laßwitz-Award" winner Andreas Eschbach, whose books "The Carpet Makers" and "Eine Billion Dollar" are big successes, and Frank Schätzing, who in his book "The Swarm" mixes elements of the science thriller with SF elements to an apocalyptic scenario. The most prominent German-speaking author, according to "Die Zeit", is Austrian Herbert W. Franke.
In 1920's Germany produced a number of critically acclaimed high-budget science fiction and horror films. "Metropolis" by director Fritz Lang is credited as one of the most influential science fiction films ever made. Other films of the era included "Woman in the Moon", "Alraune", "Algol", "Gold", "Master of the World", among others. In the second half of the 20th century, East Germany also became a major science fiction film producer, often in a collaboration with fellow Eastern Bloc countries. Films of this era include "Eolomea", "First Spaceship on Venus" and "Hard to Be a God".
Russia and ex-Soviet countries.
Russians made their first steps to science fiction in the mid-19th century, with utopias by Faddei Bulgarin and Vladimir Odoevsky.
However, it was the Soviet era that became the genre's golden age. Soviet writers were prolific,
despite limitations set up by state censorship. Early Soviet writers, such as Alexander Belayev, Alexey N. Tolstoy and Vladimir Obruchev, employed Vernian/Wellsian hard science fiction based on scientific predictions.
The most notable books of the era include Belayev's "Amphibian Man", "The Air Seller" and "Professor Dowell's Head"; Tolstoy's "Aelita" and "Engineer Garin's Death Ray". Early Soviet science fiction was influenced by communist ideology and often featured a leftist agenda or anti-capitalist satire.
Those few early Soviet books that challenged the communist worldview and satirized the Soviets, such as Yevgeny Zamyatin's dystopia "We" or Mikhail Bulgakov's "Heart of a Dog" and "Fatal Eggs", were banned from publishing until the 1980s, although they still circulated in fan-made copies.
In the second half of the 20th century, a new generation of writers developed a more complex approach. Social science fiction, concerned with philosophy, ethics, utopian and dystopian ideas, became the prevalent subgenre.
The breakthrough was started by Ivan Yefremov's utopian novel "Andromeda Nebula" (1957). He was soon followed by brothers Arkady and Boris Strugatsky, who explored darker themes and social satire in their Noon Universe novels, such as "Hard to be a God" (1964) and "Prisoners of Power" (1969), as well as in their science fantasy trilogy "Monday Begins on Saturday" (1964). A good share of Soviet science fiction was aimed at children. Probably the best known
was Kir Bulychov, who created "Alisa Selezneva" (1965-2003), a children's space adventure series about a teenage girl from the future.
Soviet film industry also contributed to the genre, starting from the 1924 film "Aelita". Some of early Soviet films, namely "Planet of the Storms" (1962) and "Battle Beyond the Sun" (1959), were pirated, re-edited and released in the West under new titles.
Late Soviet science fiction films include "Mystery of the Third Planet" (1981), "" (1973) and "Kin-dza-dza!" (1986), as well as Andrey Tarkovsky's "Solaris" and "Stalker", among others.
After the fall of the Soviet Union, science fiction in the former Soviet republics is still written mostly in the Russian language, which allows an appeal to a broader audience. Aside from Russians themselves, especially notable are Ukrainian writers, who greatly contributed to science fiction and fantasy in Russian language.
Among the most notable post-Soviet authors are H. L. Oldie, Sergey Lukyanenko, Alexander Zorich and Vadim Panov. Russia's film industry, however, was less successful recently and produced only a few science fiction films, most of them are adaptations of books by Strugatskies ("The Inhabited Island, The Ugly Swans") or Bulychov ("Alice's Birthday"). Science fiction media in Russia is represented with such magazines as "Mir Fantastiki" and "Esli".
Other European countries.
Poland is a traditional producer of science fiction and fantasy. The country's most influential science fiction writer of all time is Stanisław Lem, who is probably best known for his social science fiction books, such as "Solaris" and the stories involving "Ijon Tichy", but who also wrote very successful hard sci-fi such as "The Invincible" and the stories involving "Pirx the Pilot". A number of Lem's books were adapted for screen, both in Poland and abroad. Other notable Polish writers of the genre include Jerzy Żuławski, Janusz A. Zajdel, Konrad Fiałkowski, Jacek Dukaj and Rafał A. Ziemkiewicz.
Czech writer and playwright Karel Čapek in his play "R.U.R." (1920) introduced the word "robot" into science fiction. Čapek is also known for his satirical science fiction novels and plays, such as "War with the Newts" and "The Absolute at Large". Traditions of Czech science fiction were carried on by writers like Ludvík Souček, Josef Nesvadba and Ondřej Neff.
Oceania.
Australia: American David G. Hartwell noted there is "nothing essentially Australian about Australian science-fiction." A number of Australian science-fiction (and fantasy and horror) writers are in fact international English language writers, and their work is published worldwide. This is further explainable by the fact that the Australian inner market is small (with Australian population being around 21 million), and sales abroad are crucial to most Australian writers.
North America.
In Canadian Francophone province Québec, Élisabeth Vonarburg and other authors developed a tradition of French-Canadian SF, related to the European French literature. The Prix Boreal was established in 1979 to honor Canadian science fiction works in French. The Prix Aurora Awards (briefly preceded by the Casper Award) were founded in 1980 to recognize and promote the best works of Canadian science fiction in both French and English. Also, due to Canada's bilingualism and the US publishing almost exclusively in English, translation of science fiction prose into French thrives and runs nearly parallel upon a book's publishing in the original English. A sizeable market also exists within Québec for European-written Francophone science fiction literature.
Latin America.
Although there is still some controversy as to when science fiction began in Latin America, the earliest works date from the late 19th century. All published in 1875, "O Doutor Benignus" by the Brazilian Augusto Emílio Zaluar, "El Maravilloso Viaje del Sr. Nic-Nac" by the Argentinian Eduardo Holmberg, and "Historia de un Muerto" by the Cuban Francisco Calcagno are three of the earliest novels which appeared in the continent.
Up to the 1960s, science fiction was the work of isolated writers who did not identify themselves with the genre, but rather used its elements to criticize society, promote their own agendas or tap into the public's interest in pseudo-sciences. It received a boost of respectability after authors such as Horacio Quiroga and Jorge Luis Borges used its elements in their writings. This, in turn, led to the permanent emergence of science fiction in the 1960s and mid-1970s, notably in Argentina, Brazil, Chile, Mexico, and Cuba. Magic realism enjoyed parallel growth in Latin America, with a strong regional emphasis on using the form to comment on social issues, similar to social science fiction and speculative fiction in the English world.
Economic turmoil and the suspicious eye of the dictatorial regimes in place reduced the genre's dynamism for the following decade. In the mid-1980s, it became increasingly popular once more. Although led by Argentina, Brazil and Mexico, Latin America now hosts dedicated communities and writers with an increasing use of regional elements to set them apart from English-language science-fiction.

</doc>
<doc id="26788" url="http://en.wikipedia.org/wiki?curid=26788" title="Spirotrich">
Spirotrich

The spirotrichs are a large and distinctive group of ciliate protozoa. They typically have prominent oral cilia in the form of a series of polykinetids, called the adoral zone of membranelles, beginning anterior to the oral cavity and running down to the left side of the mouth. There may also be one or two paroral membranes on its right side. The body cilia are fused to form polykinetids called cirri in some, and are sparse to absent in others.
Forms with cirri are common throughout soil, freshwater, and marine environments. Individuals tend to be flattened, with cirri confined to the ventral surface. These are variously used for crawling over objects, acting as feet, swimming, or assisting in food capture. They are generally divided into hypotrichs and stichotrichs, but were originally all considered hypotrichs. 
Forms with sparse or absent body cilia tend to be smaller and are mostly marine, but a few are common in freshwater. Again, they are generally divided into oligotrichs and choreotrichs, but were originally all considered oligotrichs. The latter group includes the tintinnids, which produce loricae or shells and are the predominant fossil ciliates.
As first defined by Bütschli in 1889 the spirotrichs were one of two orders, together with the now-abandoned holotrichs, and included all ciliates with prominent oral cilia: heterotrichs, hypotrichs, oligotrichs, and peritrichs, although the last were soon separated. The heterotrichs have an adoral zone of membranelles, but molecular and ultrastructure studies have shown they are a separate group that diverged from most other ciliates early on. A few of the smaller groups included with them may be genuine spirotrichs, however, such as the Protocruziida.
The remaining spirotrichs form a monophyletic group, but their relationships are uncertain. For the most part the oligotrichs and choreotrichs appear to form closely related, natural groups. However "Halteria" and its close relatives, originally considered oligotrichs, form a separate group and may even be modified stichotrichs. Studies also suggest the hypotrichs are paraphyletic to the stichotrichs, and possibly to the oligotrichs and choreotrichs as well. This stands in contrast to the earlier belief that they were the most advanced of all protozoa.

</doc>
<doc id="26789" url="http://en.wikipedia.org/wiki?curid=26789" title="Sexual selection">
Sexual selection

Sexual selection is a mode of natural selection in which some individuals out-reproduce others of a population because they are better at securing mates. In 1858, Darwin described sexual selection as an important process driving species evolution and as a significant element of his theory of natural selection, but this concept was only named in his 1859 book "On the Origin of Species". The sexual form of selection
... depends, not on a struggle for existence, but on a struggle between the males for possession of the females; the result is not death to the unsuccessful competitor, but few or no offspring.
... when the males and females of any animal have the same general habits ... but differ in structure, colour, or ornament, such differences have been mainly caused by sexual selection.
His sexual selection examples include ornate peacock feathers, birds of paradise, the antlers of stags (male deer), and the manes of lions.
Darwin greatly expanded his initial three-page treatment of sexual selection in the 1871 book "The Descent of Man and Selection in Relation to Sex". This 900-page, two-volume work includes 70 pages on sexual selection in human evolution, and 500 pages on sexual selection in other animals. In summary, while natural selection results from the struggle to survive, sexual selection emerges from the struggle to reproduce.
The sexual struggle is of two kinds; in the one it is between individuals of the same sex, generally the males, in order to drive away or kill their rivals, the females remaining passive; whilst in the other, the struggle is likewise between the individuals of the same sex, in order to excite or charm those of the opposite sex, generally the females, which no longer remain passive, but select the more agreeable partners.
Concept.
The concept of sexual selection arose from the observation that many animals had evolved features whose function was not to help those individuals survive (and indeed, which might be deleterious to their individual survival), but to help them to maximize their reproductive success. This can be realized in two different ways:
Thus, sexual selection takes two major forms: "intersexual selection" (also known as 'mate choice' or 'female choice') in which males compete with each other to be chosen by females; and "intrasexual selection" (also known as 'male–male competition') in which members of the less limited sex (typically males) compete aggressively among themselves for access to the limiting sex. The "limiting sex" is the sex which has the higher parental investment, which therefore faces the most pressure to make a good mate decision.
For "intersexual selection" to work, one sex must evolve a feature alluring to the opposite sex, sometimes resulting in a "fashion fad" of intense selection in an arbitrary direction. Or, in the second case, while natural selection can help animals develop ways of killing or escaping from other species, "intrasexual selection" drives the selection of attributes that allow alpha males to dominate their own breeding partners and rivals.
Sexual selection sometimes generates monstrously absurd features that, in harder times, may help cause a species' extinction, as has been suggested for the giant antlers of the Irish elk ("Megaloceros giganteus") that became extinct in Pleistocene Europe. However, sexual selection can also do the opposite, driving species divergence - sometimes through elaborate changes in genitalia - such that new species emerge.
Although the driving force for both sexes is reproductive success, the two genders have different ways to maximize it: males benefit from frequent mating, including by monopolizing access to a group of fertile females, because new matings make them increase the number of eggs they fertilize. Females have a limited number of offspring they can have which is not increased by mating more frequently. They maximize the return on the energy they invest in reproduction by seeing their offspring grow into healthy adults, such as sons with well-developed, sexually attractive features, which sire many descendants, or fecund daughters. In addition, males often invest less of their energy in each individual offspring whereas female energy expenditures on gestation and parental care being much higher. Females have much more reason to be "picky". They need a way to choose males that are most likely to produce high-quality offspring.
Modern Interpretation.
Today, biologists would say that certain evolutionary traits can be explained by intraspecific competition - competition between members of the same species - distinguishing between competition "before" or "after" sexual intercourse.
Finally, sexual conflict is said to occur between breeding partners, sometimes leading to an evolutionary arms race between males and females.
Female mating preferences are widely recognized as being responsible for the rapid and divergent evolution of male secondary sexual traits. Females of many animal species prefer to mate with males with external ornaments - exaggerated features of morphology such as elaborate sex organs. These preferences may arise when an arbitrary female preference for some aspect of male morphology — initially, perhaps, a result of genetic drift — creates, in due course, selection for males with the appropriate ornament. One interpretation of this is known as the sexy son hypothesis. Alternatively, genes that enable males to develop impressive ornaments or fighting ability may simply show off greater disease resistance or a more efficient metabolism, features that also benefit females. This idea is known as the good genes hypothesis.
Criteria for reproductive success.
The success of an organism is not only measured by the number of offspring left behind, but by the quality or probable fitness of the offspring: their reproductive fitness.
Sexual selection increases the ability of organisms to differentiate one another at the "species" level: inter"species" selection.
The grossest blunder in sexual preference, which we can conceive of an animal making, would be to mate with a species different from its own, and with which hybrids are either infertile, or, through the mixture of instincts and other attributes appropriate to different courses of life, at so serious a disadvantage as to leave no descendants. ... it is no conjecture that a discriminative mechanism exists, variations in which will be capable of giving rise to a similar discrimination within its own species, should such a discrimination become at any time advantageous.
Ronald Fisher, 1930
Individuals in each region most readily attracted to, or excited by, mates of the type there favored, in contrast to possible mates of the opposite type, will, in fact, be the better represented in future generations, and both the discrimination and the preference will thereby be enhanced. It appears certainly possible that an evolution of sexual preference due to this cause would establish an effective isolation between two differentiated parts of a species, even when geographical and other factors were least favorable to such separation.
Ronald Fisher, 1930
The expansion of interspecies selection and intraspecies selection is a driving force behind species fission: the separation of a single contiguous species into multiple non-contiguous variants. Sexual preference creates a tendency towards assortative mating or homogamy, providing a system by which a group otherwise invaded by diverse genes is able to suppress their effects and diverge genetically.
The general conditions of sexual discrimination appear to be (1) the acceptance of one mate precludes the effective acceptance of alternative mates, and (2) the rejection of an offer will be followed by other offers, either certainly, or at such high chance that the risk of non-occurrence will be smaller than the chance advantage to be gained by selecting a mate.
Example: intersexual selection.
The conditions determining which sex becomes the more limited resource in intersexual selection can be best understood by way of Bateman's principle which states that "the sex which invests the most in producing offspring becomes a limiting resource over which the other sex will compete." This can be most easily illustrated by the contrast in nutritional investment into a zygote between egg and sperm, and the limited reproductive capacity of females compared to males. Thus, 'sexual selection' typically refers to the process of choice (the limiting factor, which is typically females) over members of the opposite sex (the non-limited factor, typically males).
The peacock provides a particularly well known example of intersexual selection, where ornate males compete to be chosen by females. The result is a stunning feathered display, which is large and unwieldy enough to pose a significant survival disadvantage. Biologists have suggested that the layers of the ornate plumage of males provide a means of demonstrating body symmetry, such that peahens are "trying" to discover the health of the male or the quality of his genes. Diseases, injuries, and genetic disorders may impair the body's symmetry.
Bird species often demonstrate intersexual selection, perhaps because - due to their lightweight body structures - fights between males may be ineffective or impractical. Therefore, male birds commonly use the following methods to try to seduce the females:
As a propagandist, the cock behaves as though he knew that it was as advantageous to impress the males as the females of his species, and a sprightly bearing with fine feathers and triumphant song are quite as well adapted for war-propaganda as for courtship. —"Ronald Fisher, 1930"
In some bird species, both the male and the female contribute a great deal to offspring-care. In these cases, the male and female will be continuously assessing each other based on sexual characteristics. In the blue-footed booby, the females tend to choose males with brighter blue feet, because birds with brighter feet are younger, and thus have greater fertility and ability to provide paternal care. When researchers put make-up on the males' feet to make them look duller after the laying of the first eggs, their mates consequently laid smaller second eggs, which shows that female boobies continuously evaluate their mates' reproductive value. Males also vary their behavior based on the females' foot color. Males mated to females with brighter feet are more willing to incubate their eggs.
Exponential growth in female preference.
In species where intersexual selection is active, as in many polygamous birds, sexual selection acts by accelerating the preference that specific "fashion" ornaments attract, causing the preferred trait "and" female preference for it to increase together, explosively. While Darwin had been criticised for simply accepting female whims as given, Ronald Fisher grasped the underlying mechanism in "The Genetical Theory of Natural Selection", in a remark that was not widely understood for another 50 years:
... plumage development in the male, and sexual preference for such developments in the female, must thus advance together, and so long as the process is unchecked by severe counterselection, will advance with ever-increasing speed. In the total absence of such checks, it is easy to see that the speed of development will be proportional to the development already attained, which will therefore increase with time exponentially, or in geometric progression. —"Ronald Fisher, 1930"
Fisher's runaway process causes a dramatic increase in both the male's conspicuous feature and in female preference for it, until practical, physical constraints halt further exaggeration. A positive feedback loop is created, producing extravagant physical structures in the non-limiting sex. A classic example of female choice and potential runaway selection is the long-tailed widowbird (left). While males have long tails that are selected for by female choice, female tastes in tail length are still more extreme with females being attracted to tails longer than those that naturally occur. Fisher understood that female preference for long tails may be passed on genetically, in conjunction with genes for the long tail itself. Long-tailed widowbird offspring of both sexes will inherit both sets of genes, with females expressing their genetic preference for long tails, and males showing off the coveted long tail itself.
Richard Dawkins presents a non-mathematical explanation of the runaway sexual selection process in his book "The Blind Watchmaker". Females who prefer long tailed males tend to have mothers that chose long-tailed fathers. As a result, they carry both sets of genes in their bodies. That is, genes for long tails and for preferring long tails become linked. The taste for long tails and tail length itself may therefore become correlated, tending to increase together. The more tails lengthen, the more long tails are desired. Any slight initial imbalance between taste and tails may set off an explosion in tail lengths. Fisher corresponded that:
The exponential element, which is the kernel of the thing, arises from the rate of change in hen taste being proportional to the absolute average degree of taste. —"Ronald Fisher, 1932"
The female widow bird will desire to mate with the most attractive long-tailed male so that her progeny, if male, will themselves be attractive to females of the next generation - thereby fathering many offspring who will carry the female's genes. Since the rate of change in preference is proportional to the average taste amongst females, and as females desire to secure the services of the most sexually attractive males, an additive effect is created that, if unchecked, can yield exponential increases in a given taste and in the corresponding desired sexual attribute.
It is important to notice that the conditions of relative stability brought about by these or other means, will be far longer duration than the process in which the ornaments are evolved. In most existing species the runaway process must have been already checked, and we should expect that the more extraordinary developments of sexual plumage are not due like most characters to a long and even course of evolutionary progress, but to sudden spurts of change. —"Ronald Fisher, 1930"
Since Fisher's initial conceptual model of the 'runaway' process, Russell Lande and Peter O'Donald have provided detailed mathematical proofs that define the circumstances under which runaway sexual selection can take place.
Example: intrasexual selection.
A good example of intrasexual selection, in which males fight for dominance over a harem of females, is the elephant seal - large, oceangoing mammals of the genus "Mirounga". There are two species: the northern ("M. angustirostris") and southern elephant seal ("M. leonina") - the largest carnivore living today. Both species show extreme sexual dimorphism, possibly the largest of any mammal, with southern males typically five to six times heavier than the females. While the females average 400 to and 2.6 to long, the bulls average 2200 to and 4.2 to long.
The record-sized bull, shot in Possession Bay, South Georgia, on February 28, 1913, measured 6.85 m long and was estimated to weigh 5000 kg. The maximum weight of a female is 1000 kg with a length of 3.7 m.
Males arrive in the colonies before the females and fight for control of harems. Large body size confers advantages in fighting. The agonistic behaviour of the bulls gives rise to a dominance hierarchy, with access to harems and breeding activity being determined by rank. The dominant bulls or "harem masters" establish harems of several dozen females. The least successful males have no harems, but may try to copulate with a harem male's females when the dominant male is not looking. A dominant male must stay in his territory to defend it, which can mean months without eating, living on his store of blubber. Some males have stayed ashore for more than three months without food. Two fighting males use their weight and canine teeth against each other. The outcome is rarely fatal, and the defeated bull will flee; however, bulls suffer severe tears and cuts. Males commonly vocalize with a coughing roar that serves in both individual recognition and size assessment. Conflicts between high-ranking males are more often resolved with posturing and vocalizing than with physical contact.
In the case of "intrasexual selection", adorned males may gain a reproductive advantage without the intervention of female preference. This advantage will be conferred by weapons used in the process of resolving disputes, such as those over territorial rights. The use of sexual ornamentation as a signaling device to create a dominance hierarchy among males, also known as a pecking order, allows struggle to proceed without excessive injury or fatality. It is predominantly when two opposing males are so closely matched, as would be found in males not having established themselves in a dominance hierarchy, that asymmetries cannot be found and the confrontation escalates to a point where the asymmetries must be proved by aggressive use of ornamentation.
How often males will physically engage each other, and in what manner, can best be understood by applying game theory developed for biology, most notably by John Maynard Smith.
Sexual dimorphism.
Sex differences directly related to reproduction and serving no direct purpose in courtship are called primary sexual characteristics. Traits amenable to sexual selection, which give an organism an advantage over its rivals (such as in courtship) without being directly involved in reproduction, are called secondary sex characteristics.
In most sexual species the males and females have different equilibrium strategies, due to a difference in relative investment in producing offspring. As formulated in Bateman's principle, females have a greater initial investment in producing offspring (pregnancy in mammals or the production of the egg in birds and reptiles), and this difference in initial investment creates differences in variance in expected reproductive success and bootstraps the sexual selection processes. Classic examples of reversed sex-role species include the pipefish, and Wilson's phalarope. Also, unlike a female, a male (except in monogamous species) has some uncertainty about whether or not he is the true parent of a child, and so will be less interested in spending his energy helping to raise offspring that may or may not be related to him. As a result of these factors, males are typically more willing to mate than females, and so females are typically the ones doing the choosing (except in cases of forced copulations, which can occur in certain species of primates, ducks, and others). The effects of sexual selection are thus held to typically be more pronounced in males than in females.
Differences in secondary sexual characteristics between males and females of a species are referred to as sexual dimorphisms. These can be as subtle as a size difference (sexual size dimorphism, often abbreviated as SSD) or as extreme as horns and color patterns. Sexual dimorphisms abound in nature. Examples include the possession of antlers by only male deer, the brighter coloration of many male birds in comparison with females of the same species, or even more distinct differences in basic morphology, such as the drastically increased eye-span of the male stalk-eyed fly. The peacock, with its elaborate and colorful tail feathers, which the peahen lacks, is often referred to as perhaps the most extraordinary example of a dimorphism. Male and female black-throated blue warblers and Guianan cock-of-the-rocks also differ radically in their plumage. Early naturalists even believed the females to be a separate species. The largest sexual size dimorphism in vertebrates is the shell dwelling cichlid fish "Neolamprologus callipterus" in which males are up to 30 times the size of females. Many other fish such as guppies also exhibit sexual dimorphism. Extreme sexual size dimorphism, with females larger than males, is quite common in spiders.
Sexual selection as a toolkit of natural selection.
Sexual selection may explain how certain characteristics (such as feathers) had distinct survival value at an early stage in their evolution. One recent theory sees evolution as an "adventure quest" in which species develop complexity and novelty by acquiring modular capabilities through chance encounters in an evolutionary game. But this still leaves open the question of how natural selection initiated each module.
Geoffrey Miller proposes that sexual selection might have contributed by creating evolutionary modules such as "Archaeopteryx" feathers as sexual ornaments, at first. The earliest proto-birds such as China's "Protarchaeopteryx", discovered in the early 1990s, had well-developed feathers but no sign of the top/bottom asymmetry that gives wings lift. Some have suggested that the feathers served as insulation, helping females incubate their eggs. But perhaps the feathers served as the kinds of sexual ornaments still common in most bird species, and especially in birds such as peacocks and birds-of-paradise today. If proto-bird courtship displays combined displays of forelimb feathers with energetic jumps, then the transition from display to aerodynamic functions could have been relatively smooth.
Viability and variations of the theory.
Due to their sometimes greatly exaggerated nature, secondary sexual characteristics can prove to be a hindrance to an animal, thereby lowering its chances of survival. For example, the large antlers of a moose are bulky and heavy and slow the creature's flight from predators; they also can become entangled in low-hanging tree branches and shrubs, and undoubtedly have led to the demise of many individuals. Bright colorations and showy ornamenations, such as those seen in many male birds, in addition to capturing the eyes of females, also attract the attention of predators. Some of these traits also represent energetically costly investments for the animals that bear them. Because traits held to be due to sexual selection often conflict with the survival fitness of the individual, the question then arises as to why, in nature, in which survival of the fittest is considered the rule of thumb, such apparent liabilities are allowed to persist.
An often-cited theory published by R.A. Fisher in 1930 that attempts to resolve the paradox posits that such traits are the results of explosive positive feedback loops that have as their starting points particular sexual preferences for features that confer a survival advantage and thus "become established in the species." Fisher argued that such features advance in the direction of the preference even beyond the optimal level for survival, until the selection pressure of female choice is precisely counterbalanced by the resultant disadvantage for survival. Fisher further argued that the strength of the female preference tends to grow exponentially (leading to 'explosive' evolution of the characteristic) until finally checked by ecological selection, since the offspring of those females with the strongest preference typically fare better in reproducing than the offspring of females with weaker preferences. Any mutations for the preference opposite to the given characteristic, though tending to promote survival against ecological selection, nevertheless tend not to survive in the gene pool because male offspring that result from matings based on the preference are less sexually attractive to the majority of the females in the population, and thus infrequently chosen as mates. An equivalent way of expressing this is that if most females are looking, for example, for long-tailed males, then each female individually does better to select a long-tailed male, since then her male children are more likely to succeed. (The females do not actually have this thought process; this kind of "decision" is an evolutionarily stable strategy.)
Other theories highlight intrinsically useful qualities of such traits. Antlers, horns and the like can be used in physical defense from a predator, and also in competition among males in a tournament species. The winner, which typically becomes the dominant animal in the population, is granted access to females, and therefore increases his reproductive output. Antlers are not the only mechanism that can be used to counteract predation. Predators typically look for the eyes of their prey so they can attack that end of the creature. The conspicuousness of eyespots on many species of butterflies and fishes confuses predators and helps to prevent the prey from suffering serious damage.
Another, more recently developed, theory, the handicap principle of Amotz Zahavi, Russell Lande and W. D. Hamilton, holds that the fact that the male of the species is able to survive until and through the age of reproduction with such a seemingly maladaptive trait is effectively considered by the female to be a testament to his overall fitness. Such handicaps might prove he is either free of or resistant to disease, or it might demonstrate that this animal possesses more speed or a greater physical strength that is used to combat the troubles brought on by the exaggerated trait.
Zahavi's work spurred a re-examination of the field, which has produced an ever-accelerating number of theories. In 1984, Hamilton and Marlene Zuk introduced the "Bright Male" hypothesis, suggesting that male elaborations might serve as a marker of health, by exaggerating the effects of disease and deficiency. In 1990, Michael Ryan and A.S. Rand, working with the túngara frog, proposed the hypothesis of "Sensory Exploitation", where exaggerated male traits may provide a sensory stimulation that females find hard to resist. Subsequently the theories of the "Gravity Hypothesis" by Jordi Moya-Larano et al. and "Chase Away" by Brett Holland and William R. Rice have also been added. In addition, in the late 1970s Janzen and Mary Willson, noting that male flowers are often larger than female flowers, expanded the field of sexual selection into plants.
In the past few years, the field has exploded to include many additional observations and areas of study, not all of which are clearly included under Darwin's definition of sexual selection. These include cuckoldry, nuptial gifts, sperm competition, infanticide, physical beauty, mating by subterfuge, species isolation mechanisms, male parental care, ambiparental care, mate location, polygamy, and mechanisms that can only be called bizarre, including homosexual rape in certain male animals, cementing of females' vaginal pores by males in some lepidopteran insects, and insect penises specialized to remove any sperm packets from females which may have been deposited by previous suitors. These theories are not mutually exclusive; combinations of them may also be considered.
Focusing on the effect of sexual conflict, as hypothezised by William Rice, Locke Rowe et Göran Arnvist, Thierry Lodé underlines that the divergence of interest constitutes a key for evolutionary process. Sexual conflict leads to an antagonistic co-evolution in which one sex tends to control the other, resulting in a tug of war. Besides, "the sexual propaganda theory" only argued that mate were opportunistically lead, on the basis of various factors determining the choice such as phenotypic characteristics, apparent vigor of individual, strength of mate signals, trophic resources, territoriality etc. and could explain the maintenance of genetic diversity within populations.
Several workers have brought attention to the fact that elaborated characters that ought to be costly in one way or another for their bearers (e.g., the tails of some species of Xiphophorus fish) do not always appear to have a cost in terms of energetics, performance or even survival. One possible explanation for the apparent lack of costs is that "compensatory traits" have evolved in concert with the sexually selected traits.
In humans.
Charles Darwin conjectured that the male beard, as well as the relative hairlessness of humans compared to nearly all other mammals, are results of sexual selection. He reasoned that since, compared to males, the bodies of females are more nearly hairless, hairlessness is one of the atypical cases due to its selection by males at a remote prehistoric time, when males had overwhelming selective power, and that it nonetheless affected males due to genetic correlation between the sexes. He also hypothesized that contrasts in sexual selection acting along with natural selection were significant in the geographical differentiation in human appearance of some isolated groups as he did not believe that natural selection alone provided a satisfactory answer. As an example for this, he implicitly mentions Steatopygia in Khoisan women.
Geoffrey Miller, drawing on some of Darwin's largely neglected ideas about human behavior, has hypothesized that many human behaviors not clearly tied to survival benefits, such as humor, music, visual art, verbal creativity, and some forms of altruism, are courtship adaptations that have been favored through sexual selection.
In that view, many human artefacts could be considered subject to sexual selection as part of the extended phenotype, for instance clothing that enhances sexually selected traits. The German anthropologist Ferdinand Fellman argues that the emergence of human self-consciousness is due to an extended sexual selection, termed "emotional selection", bridging the gap between animal sexual behaviour and human erotic love.
Some hypotheses about the evolution of the human brain argue that it is a sexually selected trait, as it would not confer enough fitness in itself relative to its high maintenance costs (a quarter to a fifth of the energy and oxygen consumed by a human). Related to this is vocabulary, where humans, on average, know far more words than are necessary for communication. Miller (2000) has proposed that this apparent redundancy is due to individuals using vocabulary to demonstrate their intelligence, and consequently their "fitness", to potential mates. This has been tested experimentally and it appears that males do make greater use of lower frequency (more unusual) words when in a romantic mindset compared to a non-romantic mindset, meaning that vocabulary is likely to be used as a sexual display. 
An uncertain example: the giraffe.
The evolutionary origins of the giraffe's ("Giraffa camelopardalis") long neck are controversial. The long-accepted "competing browser's hypothesis" originally put forth by Charles Darwin has been put into question. Originally, scientists believed that the elongation of the giraffe's neck had been a result of natural selection acting in relation to foraging behavior, where it was supposed that longer necks enabled favored individuals to gather food inaccessible to other animals. But even though the giraffe’s overall height is about 6 meters, it still typically feeds at about 2 meters above the ground. Moreover, the giraffe's kudu, impala, and steenbok competitors do not feed above 2 meters and prefer feeding at shoulder level as well, rather than at the maximum height they could reach.
An alternative explanation for the origin of long necks in giraffe is sexual selection. Male giraffe often neck with other males to exhibit dominance. There are six criteria that need to be satisfied for the exaggerated neck to be classified as a result of sexual selection. The characteristic should be more exaggerated in one of the sexes; it must be used to indicate dominance; have no direct survival benefits; cost the organism in terms of survival or other factors (e.g., energetics); positive allometry should be observed. But evolutionary history shows that increased neck length is not correlated to increases in other parts of the body, which would be expected from foraging selection, so sexual selection may be a more satisfactory explanation. Studies have failed to resolve the causes involved: perhaps the neck was a result of both or other forces.
History and application of the theory.
The theory of sexual selection was first proposed by Charles Darwin in his book "The Origin of Species", though it was primarily devoted to natural selection. A later work, "The Descent of Man and Selection in Relation to Sex" dealt with the subject of sexual selection exhaustively, in part because Darwin felt that natural selection alone was unable to account for certain types of apparently non-competitive adaptations, such as the tail of a male peacock. He once wrote to a colleague that "The sight of a feather in a peacock's tail, whenever I gaze at it, makes me sick!" His work divided sexual selection into two primary categories: male-male competition (which would produce adaptations such as a bighorn sheep's horns, which are used primarily in sparring with other males over females), and cases of female choice (which would produce adaptations like beautiful plumage, elaborate songs, and other things related to impressing and attracting).
Darwin's views on sexual selection were opposed strongly by his "co-discoverer" of natural selection, Alfred Russel Wallace, though much of his "debate" with Darwin took place after Darwin's death. Wallace argued that the aspects of it which were male-male competition, while real, were simply forms of natural selection, and that the notion of "female choice" was attributing the ability to judge standards of beauty to animals far too cognitively undeveloped to be capable of aesthetic feeling (such as beetles).
Wallace also argued that Darwin too much favored the bright colors of the male peacock as adaptive without realizing that the "drab" peahen's coloration is itself adaptive, as camouflage. Wallace more speculatively argued that the bright colors and long tails of the peacock were "not" adaptive in any way, and that bright coloration could result from non-adaptive physiological development (for example, the internal organs of animals, not being subject to a visual form of natural selection, come in a wide variety of bright colors). This has been questioned by later scholars as quite a stretch for Wallace, who in this particular instance abandoned his normally strict "adaptationist" agenda in asserting that the highly intricate and developed forms such as a peacock's tail resulted by sheer "physiological processes" that were somehow not at all subjected to adaptation.
Though Darwin considered sexual and natural selection to be two separate processes of equal importance, most of his contemporaries were not convinced, and sexual selection is usually de-emphasized as being a lesser force than, or simply a part of, natural selection.
The sciences of evolutionary psychology, human behavioral ecology, and sociobiology study the influence of sexual selection in humans, though these are often controversial fields.

</doc>
<doc id="26790" url="http://en.wikipedia.org/wiki?curid=26790" title="Stanisław Lem">
Stanisław Lem

Stanisław Lem (]; 12 September 1921 – 27 March 2006) was a Polish writer of science fiction, philosophy and satire. His books have been translated into 41 languages and have sold over 45 million copies. From the 1950s to 2000s he published many books, both science fiction and philosophical/futurological. He is best known as the author of the 1961 novel "Solaris", which has been made into a feature film three times. In 1976 Theodore Sturgeon wrote that Lem was the most widely read science-fiction writer in the world.
His works explore philosophical themes through speculation on technology, the nature of intelligence, the impossibility of mutual communication and understanding, despair about human limitations and humanity's place in the universe. They are sometimes presented as fiction, but others are in the form of essays or philosophical books.
Translations of his works are difficult due to passages with elaborate word formation, alien or robotic poetry, and puns.
Life.
Early life.
Lem was born in 1921 in Lwów, Poland (now Ukraine). He was the son of Sabina Woller (1892–1979) and Samuel Lem (1879–1954), a wealthy laryngologist and former physician in the Austro-Hungarian Army. Though raised a Roman Catholic, he later became an atheist "for moral reasons ... the world appears to me to be put together in such a painful way that I prefer to believe that it was not created ... intentionally". Lem called himself an "agnostic" later in life. After the Soviet invasion and occupation of Eastern Poland, he was not allowed to study at the Polytechnic as he wished because of his "bourgeois origin" and only due to his father's connections was accepted to study medicine at Lwów University in 1940. During the subsequent Nazi occupation (1941–1944), Lem's family, which had Jewish roots, avoided imprisonment in a ghetto, surviving with false papers. During that time Lem earned a living as a car mechanic and welder.
 "During that period, I learned in a very personal, practical way that I was no “Aryan”. I knew that my ancestors were Jews, but I knew nothing of the Mosaic faith and, regrettably, nothing at all of Jewish culture. So it was, strictly speaking, only the Nazi legislation that brought home to me the realization that I had Jewish blood in my veins. "
In 1945, Polish eastern Kresy was annexed into Soviet Ukraine and the family, like many other Poles, was resettled to Kraków where Lem, at his father's insistence, took up medical studies at the Jagiellonian University. He refused to tailor his answers to the prevailing Lysenkoism and failed his final examinations on purpose so as not to be obliged to become a military doctor. Earlier he had started working as an assistant in a hospital and writing stories in his spare time.
Rise to fame.
Lem made his literary debut in 1946 with a number of works of different genres, including poetry as well as a science fiction novel "The Man from Mars" ("Człowiek z Marsa") serialized in "Nowy Świat Przygód" ("New World of Adventures"). Between 1948 and 1950 Lem was working as a scientific research assistant at the Jagiellonian University, and published a number of short stories, poems, reviews and similar works, particularly at "Tygodnik Powszechny". In 1951 he published his first book, "The Astronauts" ("Astronauci"). In 1954 he published a short story anthology, "Sesame and other stories" ("Sezam i inne opowiadania"). That year he also married Barbara Leśniak. Following, year, 1955, saw the publication of another sci-fi novel, "The Magellanic Cloud" ("Obłok Magellana").
During the era of Stalinism, which in Poland begun in the late 40s, all published works had to be directly approved by the communist regime. Thus "Astronauci" was not, in fact, the first novel Lem finished, just the first that made it past the censors. Going by the date of finished manuscript, Lem's first book would be a partly autobiographical novella "Hospital of the Transfiguration" ("Szpital Przemienienia"), finished in 1948. It would be published seven years later, in 1955, as a trilogy under a title "Czas nieutracony" ("Time Not Lost"). The experience of trying to push "Czas.." through the censors were one of the major reasons Lem decided to focus on the less censored genre of science fiction. Nonetheless, most of Lem's works published in the 1950s also contain, forced upon him by the censors and editors, various references to socialist realism as well as "glorious future of communism". Lem later criticized several of his early pieces as compromised by the ideological pressure.
Lem became truly productive after 1956, when the de-Stalinization period in the Soviet Union led to the "Polish October", when Poland experienced an increase in freedom of speech. Between 1956 and 1968, Lem authored 17 books. His writing over the next three decades or so was split between science fiction (primarily, prose) and essays about science and culture.
In 1957 he published his first non-fiction, philosophical book, "Dialogues" (Dialogi), as well as a science-fiction anthology "The Star Diaries" ("Dzienniki gwiazdowe"), collecting short stories about one of his most popular character, Ijon Tichy. 1959 saw the publication of three books: "Eden", "Śledztwo" and the short story anthology, "Inwazja z Aldebarana". 1961 saw two more books, the first seen as among his top works: "Pamiętnik znaleziony w wannie", "Solaris" as well as "Powrót z gwiazd". This was followed by a collections of his essays and non-fiction prose, "Wejście na orbitę" (1962), and a short story anthology "Noc księżycowa" (1963). In 1964 Lem published a large work on the border of philosophy and sociology of science and futurology, "Summa Technologiae", as well as a novel "The Invincible" ("Niezwyciężony").
1965 saw the publication of "The Cyberiad" ("Cyberiada"). That year also saw the publication of a short story anthology, "The Hunt" (Polowanie). 1966 is the year of "Wysoki Zamek", and 1968, "Głos Pana" and "Opowieści o pilocie Pirxie". "Wysoki Zamek" was another of Lem's autobiographical works, and touched upon a theme that usually was not favored by the censors - Lem's youth in the pre-war, then-Polish Lviv. 1967 and 1970 saw two more non-fiction treaties, "Filozofia przypadku" and "Fantastyka i futurologia". Ijon Tichy returns in 1971 "The Futurological Congress" "Kongres futurologiczny", the year of a genre-mixing experient, "Doskonała próżnia" (a collection of reviews of non-existing books). 1973 sees a similar work, "Wielkość urojona". In 1976 Lem publishes two novels: "Maska" and "Katar". In 1980 he published another set of reviews of non-existing works, "Prowokacja". Next year sees the another Tichy's novel, "Wizja lokalna". Later this decade he publishes "Pokój na Ziemi" (1984) and "Fiasko" (1986); Lem's final science-fiction novel.
In the late 70s and early 80s Lem cautiously supported Polish dissidents movement, and started publishing essays in Paris-based "Kultura". In 1982, with martial law in Poland declared, Lem moved to West Berlin, where he became a fellow of the Institute for Advanced Study, Berlin ("Wissenschaftskolleg zu Berlin"). After that, he settled in Vienna. He returned to Poland in 1988.
Final years.
From the late 1980s onwards he tended to concentrate on philosophical texts and essays, published in a number of Polish magazines ("Tygodnik Powszechny", "Odra", "Przegląd" and others). They were later collected in a number of anthologies.
In the early 1990s Lem met with a literary scholar and critic Peter Swirski for a series of extensive interviews, published together with other critical materials and translations as "A Stanislaw Lem Reader" (1997). In the book Lem speaks about a range of issues rarely touched on before in any interview. Moreover, the book includes Swirski's translation of Lem's retrospective essay "Thirty Years Later", devoted to Lem's legendary nonfictional treatise "Summa Technologiae". During later interviews in 2005, Lem expressed his disappointment with the genre of science fiction and his general pessimism regarding technical progress. He viewed the human body as unsuitable for space travel, held that information technology drowns people in a glut of low-quality information, and considered truly intelligent robots as both undesirable and impossible to construct. Since then Peter Swirski published a series of in-depth studies of Lem as a writer, philosopher, and futurologist. Notable among them are the recent "From Literature to Biterature: Lem, Turing, Darwin" (2013), "Stanislaw Lem: Selected Letters to Michael Kandel" (2014), "Lemography" (2014), and "Stanislaw Lem: Philosopher of the Future" (2015).
In 1996, he received the prestigious Polish award, the Order of the White Eagle.
Lem died in Kraków on 27 March 2006 at the age of 84 due to heart disease.
Lem's cousin was Polish poet Marian Hemar (Lem's father and Hemar's mother were brother and sister).
Controversies.
SFWA.
Lem was awarded an honorary membership in the Science Fiction Writers of America (SFWA) in 1973. SFWA Honorary membership is given to people who do not meet the publishing criteria for joining the regular membership but who would be welcomed as members had their work appeared in the qualifying English language publications. Lem, however, never had a high opinion of American science fiction, describing it as ill thought-out, poorly written, and interested more in making money than in ideas or new literary forms. After his eventual American publication, when he became eligible for regular membership, his honorary membership was rescinded, an action that some of the SFWA members apparently intended as a rebuke, and it seems that Lem interpreted it as such. Lem was invited to stay on with the organization with a regular membership, but declined. After many members (including Ursula K. Le Guin) protested Lem's treatment by the SFWA, a member offered to pay his dues. Lem never accepted the offer.
Philip K. Dick.
Lem singled out only one American SF writer for praise, Philip K. Dick—see the 1986 English-language anthology of his critical essays, "Microworlds". Dick thought that Stanisław Lem was probably a false name used by a composite committee operating on orders of the Communist party to gain control over public opinion, and wrote a letter to the FBI to that effect. Stanislaw Lem was also responsible for Polish translation of Dick's work, and when Dick felt monetarily short-changed by the publisher, he held Lem personally responsible (see "Microworlds").
Significance.
Lem is the most internationally famous Polish writer. He has become one of the most highly acclaimed science-fiction writers, hailed by critics as equal to such classic authors as H. G. Wells and Olaf Stapledon. In 1976, Theodore Sturgeon wrote that Lem was the most widely read science-fiction writer in the world.
In Poland, in the 60s and 70s, Lem remained under the radar of mainstream critics, who dismissed him as a "mass market", low-brow, youth-oriented writer, and, in this way, such a dismissal might have given him a form of invisibility from censorship.
The total volume of his published works is over 28 million volumes. His works were widely translated abroad, appearing in over 40 languages, though the bulk of them were in the Eastern Bloc countries (Poland, Germany and Soviet Union). Franz Rottensteiner, Lem's former agent abroad, had this to say about Lem's reception on international markets: With [number of translations and copies sold], Lem is the most successful author in modern Polish fiction; nevertheless his commercial success in the world is limited, and the bulk of his large editions was due to the special publishing conditions in the Communist countries: Poland, the Soviet Union, and the German Democratic Republic). Only in West Germany was Lem really a critical and a commercial success [... and everywhere... ] in recent years interest in him has waned.
But he is the only writer of European [science fiction, most of whose] books have been translated into English, and [...] kept in print in the USA. Lem's critical success in English is due mostly to the excellent translations of Michael Kandel.
His best-known novels include "Solaris" (1961), "His Master's Voice" ("Głos pana", 1968), and the late "Fiasco" ("Fiasko", 1987). "Solaris" was made into a film in 1968 by Russian director Boris Nirenburg, a film in 1972 by Russian director Andrei Tarkovsky, which won a Special Jury Prize at the Cannes Film Festival in 1972, and an American readaptation in 2002 by American director Steven Soderbergh, and starring George Clooney.
"Solaris" is not the only work of Lem's to be made into a movie. Over ten movie, film and television adaptations of his work exist, such as adaptations of "The Astronauts" ("First Spaceship on Venus", 1960) and "The Magellan Nebula" ("Ikarie XB-I" (1963)). Lem himself was however critical of most of the screen adaptations, with the sole exception of "Przekładaniec" in 1968 by Andrzej Wajda. More recently, in 2013, the Israeli Polish co-production "The Congress" was released, inspired by Lem's novel "The Futurological Congress".
Lem's works have been used in education, for example as teaching texts for philosophy students.
Lem's works have influenced not only the realm of literature, but that of science as well. For example, "Return from the Stars" includes the "opton", which is often cited as the first published appearance of the idea of electronic paper. In 1981 the philosophers Douglas R. Hofstadter and Daniel C. Dennett included three extracts from Lem's fiction in their important annotated anthology "The Mind's I". ... Hofstadter commented that Lem's "literary and intuitive approach... does a better job of convincing readers of his views than any hard-nosed scientific article... might do".
 Other influences exerted by Lem's works include Will Wright's popular city planning game "SimCity" which was partly inspired by Lem's short story "The Seventh Sally".
Writings.
Science fiction.
Stanisław Lem works were influenced by such masters of Polish literature as Cyprian Norwid and Stanisław Witkiewicz. His prose show a mastery of numerous genres and themes.
One of Lem's major recurring themes, beginning from his very first novel, "The Man from Mars", was the impossibility of communication between profoundly alien beings and humans, which may have no common ground with human intelligence. The best known example is the living planetary ocean in Lem's novel "Solaris". Other examples include swarms of mechanical insects (in "The Invincible") or strangely ordered societies of more human-like beings in "Fiasco" and "Eden", describing the failure of the first contact. In "His Master's Voice" Lem describes the failure of humanity's intelligence in deciphering and truly comprehending an apparent message from space.
Two overlapping arcs of short stories, "Fables for Robots" ("Bajki Robotów"), translated in the collection "Mortal Engines"), "The Cyberiad" ("Cyberiada") provide a commentary on humanity in the form of a series of grotesque, humorous, fairy tale-like short stories from a mechanical universe inhabited by robots (who had occasional contacts with biological "slimies" and human "palefaces"). 
"Śledztwo" and "Katar" are crime novels (the latter, without a murderer); "Pamiętnik..." in turn is a psychological drama inspired by Kafka. "Doskonała próżnia" and "Wielkość urojona" are, in turn, collections of reviews of non-existent books, and introductions to them. Similarly, "Prowokacja" purports to review a Holocaust-themed work.
Essays.
His criticism of most science fiction surfaced in literary and philosophical essays "Science Fiction and Futurology" and interviews. In the 1990s Lem forswore science fiction and returned to futurological prognostications, most notably those expressed in "Blink of an Eye" ("Okamgnienie"). He became increasingly critical of modern technology in his later life, criticizing inventions such as the Internet.
"Dialogi" and "Summa Technologiae" (1964) are his two most famous philosophical texts. The "Summa" is notable for being a unique analysis of prospective social, cybernetic, and biological advances. In this work, Lem discusses philosophical implications of technologies that were completely in the realm of science fiction then, but are gaining importance today—for instance, virtual reality and nanotechnology.

</doc>
<doc id="26791" url="http://en.wikipedia.org/wiki?curid=26791" title="Satire">
Satire

Satire is a genre of literature, and sometimes graphic and performing arts, in which vices, follies, abuses, and shortcomings are held up to ridicule, ideally with the intent of shaming individuals, corporations, government or society itself, into improvement. Although satire is usually meant to be humorous, its greater purpose is often constructive social criticism, using wit to draw attention to both particular and wider issues in society.
A feature of satire is strong irony or sarcasm—"in satire, irony is militant"—but parody, burlesque, exaggeration, juxtaposition, comparison, analogy, and double entendre are all frequently used in satirical speech and writing. This "militant" irony or sarcasm often professes to approve of (or at least accept as natural) the very things the satirist wishes to attack.
Satire is nowadays found in many artistic forms of expression, including literature, plays, commentary, television shows, and media such as lyrics.
Etymology and roots.
The word satire comes from the Latin word "satur" and the subsequent phrase "." "Satur" meant "full" but the juxtaposition with "lanx" shifted the meaning to "miscellany or medley": the expression "lanx satura" literally means "a full dish of various kinds of fruits."
The word "satura" as used by Quintilian, however, was used to denote only Roman verse satire, a strict genre that imposed hexameter form, a narrower genre than what would be later intended as "satire". Quintilian famously said that "satura," that is a satire in hexameter verses, was a literary genre of wholly Roman origin ("satura tota nostra est"). He was aware of and commented on Greek satire, but at the time did not label it as such, although today the origin of satire is considered to be Aristophanes' Old Comedy. The first critic to use satire in the modern broader sense was Apuleius.
To Quintilian, the satire was a strict literary form, but the term soon escaped from the original narrow definition. Robert Elliott writes:
As soon as a noun enters the domain of metaphor, as one modern scholar has pointed out, it clamours for extension; and satura (which had had no verbal, adverbial, or adjectival forms) was immediately broadened by appropriation from the Greek word for “satyr” (satyros) and its derivatives. The odd result is that the English “satire” comes from the Latin satura; but "satirize", "satiric", etc., are of Greek origin. By about the 4th century AD the writer of satires came to be known as satyricus; St. Jerome, for example, was called by one of his enemies 'a satirist in prose' ('satyricus scriptor in prosa'). Subsequent orthographic modifications obscured the Latin origin of the word satire: satura becomes satyra, and in England, by the 16th century, it was written 'satyre.'
The word "satire" derives from "satura", and its origin was not influenced by the Greek mythological figure of the "satyr". In the 17th century, philologist Isaac Casaubon was the first to dispute the etymology of satire from satyr, contrary to the belief up to that time.
Satire and humor.
Laughter is not an essential component of satire; in fact there are types of satire that are not meant to be "funny" at all. Conversely, not all humour, even on such topics as politics, religion or art is necessarily "satirical", even when it uses the satirical tools of irony, parody, and burlesque.
Even light-hearted satire has a serious "after-taste": the organizers of the Ig Nobel Prize describe this as "first make people laugh, and then make them think".
Social and psychological functions.
Satire and irony in some cases have been regarded as the most effective source to understand a society, the oldest form of social study. They provide the keenest insights into a group's collective psyche, reveal its deepest values and tastes, and the society's structures of power. Some authors have regarded satire as superior to non-comic and non-artistic disciplines like history or anthropology. In a prominent example from Ancient Greece, philosopher Plato, when asked by a friend for a book to understand Athenian society, referred him to the plays of Aristophanes.
Historically, satire has satisfied the popular need to debunk and ridicule the leading figures in politics, economy, religion and other prominent realms of power. Satires confronts public discourse and the collective imaginary, playing as a public opinion counterweight to power (being political, economic, religious, symbolic, or otherwise), by challenging leaders and authorities. For instance, it forces administrations to clarify, amend or establish their policies. Satire's job is to expose problems and contradictions, and it's not obligated to solve them. Karl Kraus set in the history of satire a prominent example of a satirist role as cofronting public discourse.
For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. The satiric impulse, and its ritualized expressions, carry out the function of resolving social tension. Institutions like the ritual clowns, by giving expression to the antisocial tendencies, represent a safety valve which reestablishes equilibrium and health in the collective imaginary, which are jeopardized by the repressive aspects of society.
The state of political satire in a given society reflects the intolerance or intolerance that characterizes it, and the state of civil liberties and human rights. Under totalitarian regimes any criticism of a political system, and especially satire, is suppressed. A typical example is the Soviet Union where the dissidents, such as Aleksandr Solzhenitsyn and Andrei Sakharov were under strong pressure from the government. While satire of everyday life in the USSR was allowed, the most prominent satirist being Arkady Raikin, political satire existed in the form of anecdotes. that made fun of Soviet political leaders, especially Brezhnev, famous for his narrow-mindness and love for awards and decorations.
Classifications of satire.
Satire is a diverse genre which is complex to classify and define, with a wide range of satiric "modes".
Horatian, Juvenalian, Menippean.
Satirical literature can commonly be categorized as either Horatian, Juvenalian, or Menippean.
Horatian.
Horatian satire, named for the Roman satirist Horace (65–8 BCE), playfully criticizes some social vice through gentle, mild, and light-hearted humour. Horace (Quintus Horatius Flaccus) wrote Satires to gently ridicule the dominant opinions and “philosophical beliefs of ancient Rome and Greece” (Rankin). Rather than writing in harsh or accusing tones, he addressed issues with humor and clever mockery. Horatian satire follows this same pattern of “gently [ridiculing] the absurdities and follies of human beings” (Drury). 
<br>It directs wit, exaggeration, and self-deprecating humour toward what it identifies as folly, rather than evil. Horatian satire's sympathetic tone is common in modern society.
<br> A Horatian satirist’s goal is to heal the situation with smiles, rather than by anger. A Horatian satirist makes fun of general human folly rather than pointing to any specific follier. Shamekia Thomas suggests, “In a work using Horatian satire, readers often laugh at the characters in the story who are the subject of mockery as well as themselves and society for behaving in those ways.” Alexander Pope has been established as an author whose satire “heals with morals what it hurts with wit” (Green). Alexander Pope—and Horatian satire—attempt to teach.
Examples:
Juvenalian.
Juvenalian satire, named after the Roman satirist Juvenal (late 1st century – early 2nd century AD), is more contemptuous and abrasive than the Horatian. Juvenal disagreed with the opinions of the public figures and institutions of the Republic and actively attacked them through his literature. “He utilized the satirical tools of exaggeration and parody to make his targets appear monstrous and incompetent” (Podzemny). Juvenal satire follows this same pattern of abrasively ridiculing societal structures.
<br> Juvenalian satire addresses social evil through scorn, outrage, and savage ridicule. This form is often pessimistic, characterized by irony, sarcasm, moral indignation and personal invective, with less emphasis on humor. Strongly polarized political satire is often Juvenalian. Also see: "Satires" of Juvenal. 
<br> A Juvenal satirist’s goal is to provoke some sort of change because he sees his opponent as evil or harmful. A Juvenal satirist mocks “societal structure, power, and civilization” (Thomas). He will do this by exaggerating the words or position of his opponent in order to jeopardize his opponent’s reputation and/or power. Jonathan Swift has been established as an author who “borrowed heavily from Juvenal’s techniques in [his critique] of contemporary English society” (Podzemny). Jonathan Swift—and Juvenalian satire—attempt to punish.
Examples:
Satire versus teasing.
In the history of theatre there has always been a conflict between engagement and disengagement on politics and relevant issue, between satire and grotesque on one side, and jest with teasing on the other. Max Eastman defined the spectrum of satire in terms of "degrees of biting", as ranging from satire proper at the hot-end, and "kidding" at the violet-end; Eastman adopted the term kidding to denote what is just satirical in form, but is not really firing at the target. Nobel laureate satirical playwright Dario Fo pointed out the difference between satire and teasing ("sfottò"). Teasing is the reactionary side of the comic, it limits itself to a shallow parody of physical appearance; the side-effect of teasing is that it humanizes and draws sympathy for the powerful individual towards which it is directed. Satire instead uses the comic to go against power and its oppressions, has a subversive character, and a moral dimension which draws judgement against its targets. Fo formulated an operational criteria to tell real satire from "sfottò", saying that real satire arouses an outraged and violent reaction, and that the more they try to stop you, the better is the job you are doing. Fo contends that, historically, people in positions of power have welcomed and encouraged good-humoured buffoonery, while modern day people in positions of power have tried to censor, ostracize and repress satire.
Teasing ("sfottò") is an ancient form of simple buffoonery, a form of comedy without satire's subversive edge. Teasing includes light and affectionate parody, good-humoured mockery, simple one-dimensional poking fun, benign spoofs. Teasing typically consists in an impersonation of someone monkeying around with his exterior attributes, tics, physical blemishes, voice and mannerisms, quirks, way of dressing and walking, the phrases he typically repeats. By contrast, teasing never touches on the core issue, never makes a serious criticism judging the target with irony; it never harms the target's conduct, ideology and position of power; it never undermines the perception of his morality and cultural dimension. "Sfottò" directed towards a powerful individual, makes him appear more human and draws sympathy towards him. Hermann Göring propagated jests and jokes against himself, with the aim of humanizing his image.
Classifications by topics.
Types of satire can also be classified according to the topics it deals with. From the earliest times, at least since the plays of Aristophanes, the primary topics of literary satire have been politics, religion and sex. This is partly because these are the most pressing problems that affect anybody living in a society, and partly because these topics are usually taboo. Among these, politics in the broader sense is considered the pre-eminent topic of satire. Satire which targets the clergy is a type of political satire, while religious satire is that which targets religious beliefs. Satire on sex may overlap with blue comedy, off-color humor and dick jokes.
Scatology has a long literary association with satire, as it is a classical mode of the grotesque, the grotesque body and the satiric grotesque. Shit plays a fundamental role in satire because it symbolizes death, the turd being "the ultimate dead object." The satirical comparison of individuals or institutions with human excrement, exposes their "inherent inertness, corruption and dead-likeness." The ritual clowns of clown societies, like among the Pueblo Indians, have ceremonies with filth-eating. In other cultures, sin-eating is an apotropaic rite in which the sin-eater (also called filth-eater), by ingesting the food provided, takes "upon himself the sins of the departed." Satire about death overlaps with black humor, gallows humor.
Another classification by topics, is the distinction between political satire, religious satire and satire of manners. Political satire is sometimes called topical satire, satire of manners is sometimes called satire of everyday life, and religious satire is sometimes called philosophical satire. Comedy of manners, sometimes also called satire of manners, criticizes mode of life of common people; political satire aims at behavior, manners of politicians, and vices of political systems; Historically, Comedy of manners, which first appeared in British theater in 1620, has uncritically accepted the social code of the upper classes. Comedy in general accepts the rules of the social game, while satire subverts them.
Another analysis of satire is the spectrum of his possible tones: wit, ridicule, irony, sarcasm, cynicism, the sardonic and invective.
Classifications by medium.
Satire is found not only in written literary forms. In preliterate cultures it manifests itselfs in ritual and folk forms, as well as in trickster tales and oral poetry.
It appears also in graphic arts, music, sculpture, dance, cartoon strips, graffiti. Examples are Dada sculptures, Pop Art works, music of Gilbert and Sullivan and Erik Satie, punk and rock music. In modern media culture, stand-up comedy is an enclave in which satire can be introduced into mass media, challenging mainstream discourse. Comedy roasts, mock festivals, and stand-up comedians in nightclubs and concerts, are the modern forms of ancient satiric rituals.
Development.
Ancient Egypt.
One of the earliest examples of what we might call satire, The Satire of the Trades, is in Egyptian writing from the beginning of the 2nd millennium BC. The text's apparent readers are students, tired of studying. It argues that their lot as scribes is useful, and their lot far superior to that of the ordinary man. Scholars such as Helck think that the context was meant to be serious.
The Papyrus Anastasi I (late 2nd millennium BC) contains a satirical letter which first praises the virtues of its recipient, but then mocks the reader's meagre knowledge and achievements.
Ancient Greece.
The Greeks had no word for what later would be called "satire", although the terms cynicism and parody were used. Modern critics call the Greek playwright Aristophanes one of the best known early satirists: his plays are known for their critical political and societal commentary, particularly for the political satire by which he criticized the powerful Cleon (as in "The Knights"). He is also notable for the persecution he underwent. Aristophanes' plays turned upon images of filth and disease. His bawdy style was adopted by Greek dramatist-comedian Menander. His early play "Drunkenness" contains an attack on the politician Callimedon.
The oldest form of satire still in use is the Menippean satire by Menippus of Gadara. His own writings are lost. Examples from his admirers and imitators mix seriousness and mockery in dialogues and present parodies before a background of diatribe. The reader is meant to question approved truths to reform knowledge. As in the case of Aristophanes plays, menippean satire turned upon images of filth and disease.
Roman world.
The first Roman to discuss satire critically was Quintilian, who invented the term to describe the writings of Lucilius. The two most prominent and influential ancient Roman satirists are Horace and Juvenal, who wrote during the early days of the Roman Empire. Other important satirists in ancient Latin are Lucilius and Persius. "Satire" in their work is much wider than in the modern sense of the word, including fantastic and highly coloured humorous writing with little or no real mocking intent. When Horace criticized Augustus, he used veiled ironic terms. In contrast, Pliny reports that the 6th century BC poet Hipponax wrote "satirae" that were so cruel that the offended hanged themselves.
Medieval Islamic world.
Medieval Arabic poetry included the satiric genre "hija". Satire was introduced into Arabic prose literature by the Afro-Arab author Al-Jahiz in the 9th century. While dealing with serious topics in what are now known as anthropology, sociology and psychology, he introduced a satirical approach, "based on the premise that, however serious the subject under review, it could be made more interesting and thus achieve greater effect, if only one leavened the lump of solemnity by the insertion of a few amusing anecdotes or by the throwing out of some witty or paradoxical observations. He was well aware that, in treating of new themes in his prose works, he would have to employ a vocabulary of a nature more familiar in "hija", satirical poetry." For example, in one of his zoological works, he satirized the preference for longer human penis size, writing: "If the length of the penis were a sign of honor, then the mule would belong to the (honorable tribe of) Quraysh". Another satirical story based on this preference was an "Arabian Nights" tale called "Ali with the Large Member".
In the 10th century, the writer Tha'alibi recorded satirical poetry written by the Arabic poets As-Salami and Abu Dulaf, with As-Salami praising Abu Dulaf's wide breadth of knowledge and then mocking his ability in all these subjects, and with Abu Dulaf responding back and satirizing As-Salami in return. An example of Arabic political satire included another 10th-century poet Jarir satirizing Farazdaq as "a transgressor of the Sharia" and later Arabic poets in turn using the term "Farazdaq-like" as a form of political satire.
The terms "comedy" and "satire" became synonymous after Aristotle's "Poetics" was translated into Arabic in the medieval Islamic world, where it was elaborated upon by Islamic philosophers and writers, such as Abu Bischr, his pupil Al-Farabi, Avicenna, and Averroes. Due to cultural differences, they disassociated comedy from Greek dramatic representation and instead identified it with Arabic poetic themes and forms, such as "hija" (satirical poetry). They viewed comedy as simply the "art of reprehension", and made no reference to light and cheerful events, or troubled beginnings and happy endings, associated with classical Greek comedy. After the Latin translations of the 12th century, the term "comedy" thus gained a new semantic meaning in Medieval literature.
Ubayd Zakani introduced satire in Persian literature during the 14th century. His work is noted for its satire and obscene verses, often political or bawdy, and often cited in debates involving homosexual practices. He wrote the "Resaleh-ye Delgosha", as well as "Akhlaq al-Ashraf" ("Ethics of the Aristocracy") and the famous humorous fable "Masnavi Mush-O-Gorbeh" (Mouse and Cat), which was a political satire. His non-satirical serious classical verses have also been regarded as very well written, in league with the other great works of Persian literature. Between 1905 and 1911, Bibi Khatoon Astarabadi and other Iranian writers wrote notable satires.
Medieval Europe.
In the Early Middle Ages, examples of satire were the songs by Goliards or vagants now best known as an anthology called Carmina Burana and made famous as texts of a composition by the 20th-century composer Carl Orff. Satirical poetry is believed to have been popular, although little has survived. With the advent of the High Middle Ages and the birth of modern vernacular literature in the 12th century, it began to be used again, most notably by Chaucer. The disrespectful manner was considered "Unchristian" and ignored but for the moral satire, which mocked misbehaviour in Christian terms. Examples are "Livre des Manières" by Étienne de Fougères (~1178), and some of Chaucer's "Canterbury Tales". The epos was mocked, and even the feudal society, but there was hardly a general interest in the genre.
Two major satirists of Europe in the Renaissance were Giovanni Boccaccio and François Rabelais. Other examples of Renaissance satire include "Till Eulenspiegel", "Reynard the Fox", Sebastian Brant's "Narrenschiff" (1494), Erasmus' "Moriae Encomium" (1509) and Thomas More's "Utopia" (1516).
Early modern western satire.
Direct social commentary via satire returned with a vengeance in the 16th century, when farcical texts such as the works of François Rabelais tackled more serious issues (and incurred the wrath of the crown as a result).
The Elizabethan (i.e. 16th-century English) writers thought of satire as related to the notoriously rude, coarse and sharp satyr play. Elizabethan "satire" (typically in pamphlet form) therefore contains more straightforward abuse than subtle irony. The French Huguenot Isaac Casaubon pointed out in 1605 that satire in the Roman fashion was something altogether more civilised. Casaubon discovered and published Quintilian's writing and presented the original meaning of the term (satira, not satyr), and the sense of wittiness (reflecting the "dishfull of fruits") became more important again. 17th-century English satire once again aimed at the "amendment of vices" (Dryden).
In the 1590s a new wave of verse satire broke with the publication of Hall's "Virgidemiarum", six books of verse satires targeting everything from literary fads to corrupt noblemen. Although Donne had already circulated satires in manuscript, Hall's was the first real attempt in English at verse satire on the Juvenalian model. The success of his work combined with a national mood of disillusion in the last years of Elizabeth's reign triggered an avalanche of satire – much of it less conscious of classical models than Hall's — until the fashion was brought to an abrupt stop by censorship.
Age of Enlightenment.
The Age of Enlightenment, an intellectual movement in the 17th and 18th century advocating rationality, produced a great revival of satire in Britain. This was fuelled by the rise of partisan politics, with the formalisation of the Tory and Whig parties — and also, in 1714, by the formation of the Scriblerus Club, which included Alexander Pope, Jonathan Swift, John Gay, John Arbuthnot, Robert Harley, Thomas Parnell, and Henry St John, 1st Viscount Bolingbroke. This club included several of the notable satirists of early 18th century Britain. They focused their attention on Martinus Scriblerus, "an invented learned fool... whose work they attributed all that was tedious, narrow-minded, and pedantic in contemporary scholarship". In their hands astute and biting satire of institutions and individuals became a popular weapon. The turn to the 18th century was characterized by a switch from Horatian, soft, pseudo-satire, to biting "juvenal" satire.
Jonathan Swift was one of the greatest of Anglo-Irish satirists, and one of the first to practise modern journalistic satire. For instance, In his "A Modest Proposal" Swift suggests that Irish peasants be encouraged to sell their own children as food for the rich, as a solution to the "problem" of poverty. His purpose is of course to attack indifference to the plight of the desperately poor. In his book "Gulliver's Travels" he writes about the flaws in human society in general and English society in particular. John Dryden wrote an influential essay entitled "A Discourse Concerning the Original and Progress of Satire" that helped fix the definition of satire in the literary world. His satirical "Mac Flecknoe" was written in response to a rivalry with Thomas Shadwell and eventually inspired Alexander Pope to write his satirical "The Rape of the Lock". Other satirical works by Pope include the "Epistle to Dr Arbuthnot".
Daniel Defoe pursued a more journalistic type of satire, being famous for his "The True-Born Englishman" which mocks xenophobic patriotism, and "The Shortest-Way with the Dissenters" - advocating religious toleration by means of an ironical exaggeration of the highly intolerant attitudes of his time.
Ebenezer Cooke (1665-1732), author of "The Sot-Weed Factor" (1708), was among the first American colonialists to write literary satire. Benjamin Franklin (1706-1790) and others followed, using satire to shape an emerging nation's culture through its sense of the ridiculous.
Satire in Victorian England.
Several satiric papers competed for the public's attention in the Victorian era (1837-1901) and Edwardian period, such as "Punch" (1841) and "Fun" (1861).
Perhaps the most enduring examples of Victorian satire, however, are to be found in the Savoy Operas of Gilbert and Sullivan. In fact, in "The Yeomen of the Guard", a jester is given lines that paint a very neat picture of the method and purpose of the satirist, and might almost be taken as a statement of Gilbert's own intent:
Novelists such as Charles Dickens often used passages of satiric writing in their treatment of social issues.
In the same period, in the United States, Mark Twain (1835-1910) was a great American satirist: his novel "Huckleberry Finn" (1884) is set in the antebellum South, where the moral values Twain wishes to promote are completely turned on their heads. His hero, Huck, is a rather simple but goodhearted lad who is ashamed of the "sinful temptation" that leads him to help a runaway slave. In fact his conscience, warped by the distorted moral world he has grown up in, often bothers him most when he is at his best. Ironically, he is prepared to do good, believing it to be wrong.
Twain's younger contemporary Ambrose Bierce (1842-1913) gained notoriety as a cynic, pessimist and black humorist with his dark, bitterly ironic stories, many set during the American Civil War, which satirized the limitations of human perception and reason. Bierce's most famous work of satire is probably "The Devil's Dictionary" (1906), in which the definitions mock cant, hypocrisy and received wisdom.
20th century satire.
Karl Kraus is considered the first major European satirist since Jonathan Swift. In 20th century literature, satire was used by authors such as Aldous Huxley (1930s) and George Orwell (1940s), which under the inspiration of Zamyatin's Russian 1921 novel "We", made serious and even frightening commentaries on the dangers of the sweeping social changes taking place throughout Europe. Many social critics of this same time in the United States, such as Dorothy Parker and H. L. Mencken, used satire as their main weapon, and Mencken in particular is noted for having said that "one horse-laugh is worth ten thousand syllogisms" in the persuasion of the public to accept a criticism. Novelist Sinclair Lewis was known for his satirical stories such as "Main Street" (1920), "Babbitt" (1922), "Elmer Gantry" (1927; dedicated by Lewis to H.L. Menchen), and "It Can't Happen Here" (1935), and his books often explored and satirized contemporary American values. The film "The Great Dictator" (1940) by Charlie Chaplin is itself a parody of Adolf Hitler; Chaplin later declared that he would have not made the film if he had known about the concentration camps.
In the United States 1950s, satire was introduced into American stand-up comedy most prominently by Lenny Bruce and Mort Sahl. As they challenged the taboos and conventional wisdom of the time, were ostracized by the mass media establishment as "sick comedians". In the same period, Paul Krassner's magazine "The Realist" began publication, to become immensely popular during the 1960s and early 1970s among people in the counterculture; it had articles and cartoons that were savage, biting satires of politicians such as Lyndon Johnson and Richard Nixon, the Vietnam War, the Cold War and the War on Drugs. Prominent satiric stand-up comedian George Carlin acknowledged the influence "The Realist" had in his 1970s conversion to a satiric comedian.
A more humorous brand of satire enjoyed a renaissance in the UK in the early 1960s with the satire boom, led by such luminaries as Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore, whose stage show "Beyond the Fringe" was a hit not only in Britain, but also in the United States. Other significant influences in 1960s British satire include David Frost, Eleanor Bron and the television program "That Was The Week That Was".
Joseph Heller's most famous work, "Catch-22" (1961), satirizes bureaucracy and the military, and is frequently cited as one of the greatest literary works of the twentieth century. The film "Dr. Strangelove" from 1964 was a popular satire on the Cold War.
Contemporary satire.
Contemporary popular usage of the term "satire" is often very imprecise. While satire often uses caricature and parody, by no means are all uses of these or other humorous devices, satiric. Refer to the careful definition of satire that heads this article.
Satire is used on many UK television programmes, particularly popular panel shows and quiz shows such as "Mock the Week" (2005) and "Have I Got News for You" (1990-ongoing). Similarly it is found on radio quiz shows such as "The News Quiz" (1977-ongoing) and "The Now Show" (1998-ongoing). One of the most-watched UK television shows of the 1980s and early 1990s, the puppet show "Spitting Image" was a satire of the royal family, politics, entertainment, sport and British culture of the era.
The television program "South Park" (1997-ongoing) relies almost exclusively on satire to address issues in American culture, with episodes addressing anti-Semitism, militant atheism, homophobia, environmentalism, corporate culture, political correctness and anti-Catholicism, among many other issues.
Australian Chris Lilley produces comedy art in the style of mockumentaries ("", "Summer Heights High", "Angry Boys") and his work is often described as complex social satire.
American culture is extremely welcoming of satire, with many citizens supporting popular television programs and social outlets. 
Stephen Colbert’s television program, "The Colbert Report" (2005-2014), is instructive in the methods of contemporary American satire. Colbert's character is an opinionated and self-righteous commentator who, in his TV interviews, interrupts people, points and wags his finger at them, and "unwittingly" uses a number of logical fallacies. In doing so, he demonstrates the principle of modern American political satire: the ridicule of the actions of politicians and other public figures by taking all their statements and purported beliefs to their furthest (supposedly) logical conclusion, thus revealing their perceived hypocrisy or absurdity. Other political satire includes various political causes in the past, including the relatively successful Polish Beer-Lovers' Party and the joke political candidates Molly the Dog and Brian Miner.
In the United Kingdom, a popular modern satirist is Sir Terry Pratchett, author of the internationally best-selling "Discworld" book series. One of the most well-known and controversial British satirists is Sir Chris Morris, co-writer and director of "Four Lions".
In Canada, satire has become an important part of the comedy scene. Stephen Leacock was one of the best known early Canadian satirists, and in the early 20th century, he achieved fame by targeting the attitudes of small town life. In more recent years, Canada has had several prominent satirical television series and radio shows. Some, including "CODCO", "The Royal Canadian Air Farce", "This Is That", and "This Hour Has 22 Minutes" deal directly with current news stories and political figures, while others, like "History Bites" present contemporary social satire in the context of events and figures in history. The Canadian organization "Canada News Network" provides commentary on contemporary news events that are primarily Canadian in nature. Canadian songwriter Nancy White uses music as the vehicle for her satire, and her comic folk songs are regularly played on CBC Radio.
Cartoonists often use satire as well as straight humour. Al Capp's satirical comic strip "Li'l Abner" was censored in September 1947. The controversy, as reported in "Time", centred on Capp's portrayal of the US Senate. Said Edward Leech of Scripps-Howard, "We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables." Walt Kelly's "Pogo" was likewise censored in 1952 over his overt satire of Senator Joe McCarthy, caricatured in his comic strip as "Simple J. Malarky". Garry Trudeau, whose comic strip "Doonesbury" focuses on satire of the political system, and provides a trademark cynical view on national events. Trudeau exemplifies humour mixed with criticism. For example, the character Mark Slackmeyer lamented that because he was not legally married to his partner, he was deprived of the "exquisite agony" of experiencing a nasty and painful divorce like heterosexuals. This, of course, satirized the claim that gay unions would denigrate the sanctity of heterosexual marriage.
Like some literary predecessors, many recent television satires contain strong elements of parody and caricature; for instance, the popular animated series "The Simpsons" and "South Park" both parody modern family and social life by taking their assumptions to the extreme; both have led to the creation of similar series. As well as the purely humorous effect of this sort of thing, they often strongly criticise various phenomena in politics, economic life, religion and many other aspects of society, and thus qualify as satirical. Due to their animated nature, these shows can easily use images of public figures and generally have greater freedom to do so than conventional shows using live actors.
Fake News is also a very popular form of contemporary satire, appearing in as wide an array of formats as the news media itself: print (e.g. "The Onion", "Canada News Network", "Private Eye"), "Not Your Homepage," radio (e.g. "On the Hour"), television (e.g. "The Day Today", "The Daily Show", "Brass Eye") and the web (e.g. Mindry.in, , Scunt News, Faking News, El Koshary Today, The Giant Napkin, Unconfirmed Sources and The "Onion"'s website). Other satires are on the list of satirists and satires. Another internet-driven form of satire is to lampoon bad internet performers. An example of this is the Internet meme character Miranda Sings.
In an interview with "Wikinews", Sean Mills, President of "The Onion", said angry letters about their news parody always carried the same message. "It’s whatever affects that person", said Mills. "So it’s like, 'I love it when you make a joke about murder or rape, but if you talk about cancer, well my brother has cancer and that’s not funny to me.' Or someone else can say, 'Cancer’s "hilarious", but don’t talk about rape because my cousin got raped.' Those are rather extreme examples, but if it affects somebody personally, they tend to be more sensitive about it."
Zhou Libo, a comedian from Shanghai, is the most popular satirist in China. His humour has interests middle-class people and has sold out shows ever since his rise to fame. Primarily a theater performer, Zhou said his work is never scripted, allowing him to improvise jokes about recent events. He often mocks political figures he supports.
Techniques.
Literary satire is usually written out of earlier satiric works, reprising previous conventions, commonplaces, stance, situations and tones of voice. Exaggeration is one of the most common satirical techniques.
Legal status.
For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. In Germany, and Italy satire is protected by the constitution.
Since satire belongs to the realm of art and artistic expression, it benefits from broader lawfulness limits than mere freedom of information of journalistic kind. In some countries a specific "right to satire" is recognized and its limits go beyond the "right to report" of journalism and even the "right to criticize." Satire benefits not only of the protection to freedom of speech, but also to that to culture, and that to scientific and artistic production.
Censorship and criticism of satire.
Descriptions of satire's biting effect on its target include 'venomous', 'cutting', 'stinging', vitriol. Because satire often combines anger and humor, as well as the fact that it addresses and calls into question many controversial issues, it can be profoundly disturbing.
Typical arguments.
Because it is essentially ironic or sarcastic, satire is often misunderstood. A typical misunderstanding is to confuse the satirist with his persona.
Bad taste.
Common uncomprehending responses to satire include revulsion (accusations of poor taste, or that "it's just not funny" for instance), to the idea that the satirist actually does support the ideas, policies, or people he is attacking. For instance, at the time of its publication, many people misunderstood Swift’s purpose in "A Modest Proposal", assuming it to be a serious recommendation of economically motivated cannibalism.
Targeting the victim.
Some critics of Mark Twain see "Huckleberry Finn" as racist and offensive, missing the point that its author clearly intended it to be satire (racism being in fact only one of a number of Mark Twain's known concerns attacked in "Huckleberry Finn"). This same misconception was suffered by the main character of the 1960s British television comedy satire "Till Death Us Do Part". The character of Alf Garnett (played by Warren Mitchell) was created to poke fun at the kind of narrow-minded, racist, little Englander that Garnett represented. Instead, his character became a sort of anti-hero to people who actually agreed with his views. The same thing happened in regard to the main character in the American TV Show "All in the Family", Archie Bunker.
The Australian satirical television comedy show "The Chaser's War on Everything" has suffered repeated attacks based on various perceived interpretations of the "target" of its attacks. The "Make a Realistic Wish Foundation" sketch (June 2009), which attacked in classical satiric fashion the heartlessness of people who are reluctant to donate to charities, was widely interpreted as an attack on the Make a Wish Foundation, or even the terminally ill children helped by that organisation. Prime Minister of the time Kevin Rudd stated that The Chaser team "should hang their heads in shame". He went on to say that "I didn't see that but it's been described to me. ...But having a go at kids with a terminal illness is really beyond the pale, absolutely beyond the pale." Television station management suspended the show for two weeks and reduced the third season to eight episodes.
Romantic prejudice.
The romantic prejudice against satire is the belief spread by the romantic movement that satire is something unworthy of serious attention; this prejudice has held considerable influence to this day. Such prejudice extends to humor and everything that arouses laughter, which are often underestimated as frivolous and unworthy of serious study. For instance, humor is generally neglected as a topic of anthropological research and teaching.
History of opposition toward notable satires.
Because satire criticises in an ironic, essentially indirect way, it frequently escapes censorship in a way more direct criticism might not. Periodically, however, it runs into serious opposition, and people in power who perceive themselves as attacked attempt to censor it or prosecute its practitioners. In a classic example, Aristophanes was persecuted by the demagogue Cleon.
1599 book ban.
In 1599, the Archbishop of Canterbury John Whitgift and the Bishop of London Richard Bancroft, whose offices had the function of licensing books for publication in England, issued a decree banning verse satire. The decree, now known as the Bishops' Ban of 1599, ordered the burning of certain volumes of satire by John Marston, Thomas Middleton, Joseph Hall, and others; it also required histories and plays to be specially approved by a member of the Queen's Privy Council, and it prohibited the future printing of satire in verse.
The motives for the ban are obscure, particularly since some of the books banned had been licensed by the same authorities less than a year earlier. Various scholars have argued that the target was obscenity, libel, or sedition. It seems likely that lingering anxiety about the Martin Marprelate controversy, in which the bishops themselves had employed satirists, played a role; both Thomas Nashe and Gabriel Harvey, two of the key figures in that controversy, suffered a complete ban on all their works. In the event, though, the ban was little enforced, even by the licensing authority itself.
21st century polemics.
In 2005, the Jyllands-Posten Muhammad cartoons controversy caused global protests by offended Muslims and violent attacks with many fatalities in the Near East. It was not the first case of Muslim protests against criticism in the form of satire, but the Western world was surprised by the hostility of the reaction: Any country's flag in which a newspaper chose to publish the parodies was being burnt in a Near East country, then embassies were attacked, killing 139 people in mainly four countries; politicians throughout Europe agreed that satire was an aspect of the freedom of speech, and therefore to be a protected means of dialogue. Iran threatened to start an International Holocaust Cartoon Competition, which was immediately responded to by Jews with an Israeli Anti-Semitic Cartoons Contest.
In 2006 British comedian Sacha Baron Cohen released "Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan", a "mockumentary" that satirized everyone, from high society to frat boys. Criticism of the film was heavy, from claims of antisemitism (despite the fact that Baron Cohen is Jewish) to the massive boycott of the film by the Kazakh government; the film itself had been a reaction to a longer quarrel between the government and the comedian.
In 2008, popular South African cartoonist and satirist Jonathan Shapiro (who is published under the pen name Zapiro) came under fire for depicting then-president of the ANC Jacob Zuma in the act of undressing in preparation for the implied rape of 'Lady Justice' which is held down by Zuma loyalists. The cartoon was drawn in response to Zuma's efforts to duck corruption charges, and the controversy was heightened by the fact that Zuma was himself acquitted of rape in May 2006. In February 2009, the South African Broadcasting Corporation, viewed by some opposition parties as the mouthpiece of the governing ANC, shelved a satirical TV show created by Shapiro, and in May 2009 the broadcaster pulled a documentary about political satire (featuring Shapiro among others) for the second time, hours before scheduled broadcast. Apartheid South Africa also had a long history of censorship.
On December 29, 2009, Samsung sued Mike Breen, and the "Korea Times" for $1 million, claiming criminal defamation over a satirical column published on Christmas Day, 2009.
Satirical prophecy.
Satire is occasionally prophetic: the jokes precede actual events. Among the eminent examples are:

</doc>
<doc id="26792" url="http://en.wikipedia.org/wiki?curid=26792" title="Samuel Butler (poet)">
Samuel Butler (poet)

Samuel Butler (baptized 14 February 1613 – 25 September 1680) was a poet and satirist. He is remembered now chiefly for a long satirical poem entitled "Hudibras".
Biography.
Samuel Butler was born in Strensham, Worcestershire, and was the son of a farmer and churchwarden, also named Samuel. His date of birth is unknown, but there is documentary evidence for the date of his baptism of 14 February. The date of Butler's baptism is given as 8 February by Treadway Russell Nash in his 1793 edition of "Hudibras". Nash had already mentioned Butler in his "Collections for a History of Worcestershire" (1781), and perhaps because the latter date seemed to be a revised account, it has been repeated by many writers and editors. However, The parish register of Strensham records under the year 1612: "Item was christened Samuell Butler the sonne of Samuell Butler the xiiijth of February anno ut supra". Lady Day, 25 March, was New Year's Day in England at the time, so the year of his baptism was 1613 according to the modern Gregorian calendar. Nash also claims in his 1793 edition of "Hudibras" that Butler's father entered his son's baptism into the register, an error that was also repeated in later publications; however, the entry was clearly written by a different hand.
He was educated at the King's School, Worcester, under Henry Bright whose teaching is recorded favourably by Thomas Fuller, a contemporary writer, in his "Worthies of England". In early youth he was a servant to the Countess of Kent. Through Lady Kent he met her steward, the jurist John Selden who influenced his later writings. He also tried his hand at painting but was reportedly not very good at it; one of his editors reporting that "his pictures served to stop windows and save the tax" (on window glass).
After the Restoration he became secretary, or steward, to Richard Vaughan, 2nd Earl of Carbery, Lord President of Wales, which entailed living at least a year in Ludlow, Shropshire, until January 1662 while he was paying craftsmen working on repairing the castle there. In late 1662 the first part of "Hudibras", which he began writing when lodging at Holborn, London, in 1658 and continued to work on while in Ludlow, was published, and the other two in 1664 and 1678 respectively. One early purchaser of the first two parts was Samuel Pepys. While the diarist acknowledged that the book was the "greatest fashion" he could not see why it was found to be so witty.
Despite the popularity of "Hudibras", Butler was not offered a place at Court. However, Butler is thought to have been in the employment of the Duke of Buckingham in the summer of 1670, and accompanied him on a diplomatic mission to France. Butler also received financial support in the form of a grant from King Charles II.
Butler was buried at St. Paul's, Covent Garden. Aubrey in "Brief Lives" describes his grave as "being in the north part next to the church at the east end.. 2 yards distant from the pillaster of the dore". Also, a monument to him was placed in Westminster Abbey in 1732 by a printer with the surname Barber, and the Lord Mayor of London. There is also a memorial plaque to him in the small village church of Strensham, Worcestershire, near the town of Upton upon Severn, his birthplace.
"Hudibras".
"Hudibras" is directed against religious sectarianism. The poem was very popular in its time, and several of its phrases have passed into the dictionary. It was sufficiently popular to spawn imitators. "Hudibras" takes some of its characterization from "Don Quixote" but unlike that work, it has many more references to personalities and events of the day. Butler was also influenced by satirists such as John Skelton and Paul Scarron's "Virgile travesti"; a satire on classical literature, particularly Virgil.
"Hudibras" was reprinted many times in the centuries following Butler's death. Two of the more noteworthy editions are those edited by Zachery Grey (1752) and Treadway Russell Nash (1793). The standard edition of the work was edited by John Wilders (1967).
Other writings.
Most of his other writings never saw print until they were collected and published by Robert Thyer in 1759. Butler wrote many short biographies, epigrams and verses the earliest surviving from 1644. Of his verses, the best known is "The Elephant on the Moon", about a mouse trapped in a telescope, a satire on Sir Paul Neale of the Royal Society. Butler's taste for the mock heroic is shown by another early poem "Cynarctomachy", or Battle between Bear and Dogs, which is both a homage to and a parody of a Greek poem ascribed to Homer, "Batrachomyomachia". His supposed lack of money later in life is strange as he had numerous unpublished works which could have offered him income including a set of Theophrastan character sketches which were not printed until 1759. Many other works are dubiously attributed to him.
References.
 #if: 
 #if: A Short Biographical Dictionary of English Literature
 |, 
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Butler, Samuel (satirist)
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Butler, Samuel (satirist)
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 
 #if: A Short Biographical Dictionary of English Literature
 |{{
 #if: Cousin
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2=" A Short Biographical Dictionary of English Literature
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: Cousin
 |{{
 #if: 1910
 |, 1910{{
 #if:
}}{{
 #if: 
 #ifeq: | 1910
 |{{
 #if: 
 #if: Cousin
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Butler, Samuel (satirist)
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |, {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Butler, Samuel (satirist)
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if: A Short Biographical Dictionary of English Literature
 |, {{
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |journal&rft.genre=article&rft.atitle={{urlencode: A Short Biographical Dictionary of English Literature
 |book{{
 #if: 
 |&rft.genre=bookitem&rft.btitle={{urlencode:}}&rft.atitle={{urlencode: A Short Biographical Dictionary of English Literature
 |&rft.genre=book&rft.btitle={{urlencode: A Short Biographical Dictionary of English Literature
 #if: Cousin |&rft.aulast={{urlencode:Cousin}}{{
 }}{{
 #if: Cousin |&rft.au={{urlencode:Cousin}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Butler, Samuel (satirist)
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |&rft.pages={{urlencode: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Butler, Samuel (satirist)
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle = 
 |IncludedWorkURL = 
 |Other = 
 |Edition = 
 |Place = London
 |PublicationPlace = London
 |Publisher = J. M. Dent & Sons. Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=harv
 |Sep = .
 |PS = 
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 

</doc>
<doc id="26794" url="http://en.wikipedia.org/wiki?curid=26794" title="List of science fiction and fantasy artists">
List of science fiction and fantasy artists

This is a list of science fiction and fantasy artists, 20th- and 21st-century artists who have created book covers or interior illustrations for books, or who have published their own books or comic books of fantastic art with science fiction or fantasy themes. Artists known exclusively for their work in comic books are not included. Many of the artists are known for their work in both the fantasy and sf fields. Artists who have won the Hugo Award, the World Fantasy Award, or the Chesley Award are noted, as are inductees into the Science Fiction Hall of Fame.
 * A

</doc>
<doc id="26795" url="http://en.wikipedia.org/wiki?curid=26795" title="Saxophone">
Saxophone

The saxophone (also referred to as the sax) is a family of woodwind instruments. Saxophones are usually made of brass and played with a single-reed mouthpiece similar to that of the clarinet. The saxophone family was invented by the Belgian instrument maker Adolphe Sax in 1840. Adolphe Sax wanted to create a group or series of instruments that would be the most powerful and vocal of the woodwinds, and the most adaptive of the brass that would fill the vacant middle ground between the two sections. He patented the saxophone on June 28, 1846, in two groups of seven instruments each. Each series consisted of instruments of various sizes in alternating transposition. The series pitched in B♭ and E♭, designed for military bands, have proved extremely popular and most saxophones encountered today are from this series. Instruments from the so-called "orchestral" series, pitched in C and F, never gained a foothold, and the B♭ and E♭ instruments have now replaced the C and F instruments when the saxophone is used in the orchestra.
The saxophone is commonly used in classical music (such as concert bands, chamber music, and solo repertoire), military bands (such as military concert bands, marching bands, etc.), marching bands, and jazz (such as big bands, jazz combos, etc.). Saxophone players are called "saxophonists".
History.
The saxophone was developed in 1846 by Adolphe Sax, a Belgian instrument maker, flautist, and clarinetist based in Brussels. Prior to his work on the saxophone, he had made several improvements to the bass clarinet by improving its keywork and acoustics and extending its lower range. Sax was also a maker of the then-popular ophicleide, a large conical brass instrument in the bass register with keys similar to a woodwind instrument. His experience with these two instruments allowed him to develop the skills and technologies needed to make the first saxophones. As an outgrowth of his work improving the bass clarinet, Sax began developing an instrument with the projection of a brass instrument and the agility of a woodwind. He wanted it to overblow at the octave, unlike the clarinet, which rises in pitch by a twelfth when overblown. An instrument that overblew at the octave, would have identical fingering for both registers.
Sax created an instrument with a single reed mouthpiece like a clarinet, conical brass body like an ophicleide, and the acoustic properties of both the French horn and the clarinet.
Having constructed saxophones in several sizes in the early 1840s, Sax applied for, and received, a 15-year patent for the instrument on June 28, 1846. The patent encompassed 14 versions of the fundamental design, split into two categories of seven instruments each, and ranging from sopranino to contrabass. Although the instruments transposed at either F or C have been considered "orchestral", there is no evidence that Sax intended this. As only 3 percent of Sax's surviving production were pitched in F and C, and as contemporary composers used the E♭ alto and B♭ bass saxophone freely in orchestral music, it is almost certain that Sax experimented to find the most suitable keys for these instruments, settling upon instruments alternating between E♭ and B♭ rather than those pitched in F or C, for reasons of tone and economy (the saxophones were the most expensive wind instruments of their day). The C soprano saxophone was the only instrument to sound at concert pitch. All the instruments were given an initial written range from the B below the treble staff to the F, one space above the three ledger lines above staff, giving each saxophone a range of two and a half octaves.
Sax's patent expired in 1866; thereafter, numerous saxophonists and instrument manufacturers implemented their own improvements to the design and keywork. The first substantial modification was by a French manufacturer who extended the bell slightly and added an extra key to extend the range downwards by one semitone to B♭. It is suspected that Sax himself may have attempted this modification. This extension is now commonplace in almost all modern designs, along with other minor changes such as added keys for alternate fingerings. Using alternate fingerings will allow the player to play easily and as fast as they can. the player may also use alternate fingerings to bend the pitch. Some of the alternate fingerings are good for trilling, scales, and big interval jumps. 
Sax's original keywork, which was based on the Triebert system 3 oboe for the left hand and the Boehm clarinet for the right, was very simplistic and made playing some legato passages and wide intervals extremely difficult to finger, so numerous developers added extra keys and alternate fingerings to make chromatic playing less difficult. While the early saxophone had two separate octave vents to assist in the playing of the upper registers just as modern instruments do, players of Sax's original design had to operate these via two separate octave keys operated by the left thumb. A substantial advancement in saxophone keywork was the development of a method by which the left thumb operates both tone holes with a single octave key, which is now universal on modern saxophones. One of the most radical, however temporary, revisions of saxophone keywork was made in the 1950s by M. Houvenaghel of Paris, who completely redeveloped the mechanics of the system to allow a number of notes (C♯, B, A, G, F and E♭) to be flattened by a semitone simply by pressing the right middle finger. This enables a chromatic scale to be played over two octaves simply by playing the diatonic scale combined with alternately raising and lowering this one digit. However, this keywork never gained much popularity, and is no longer in use.
Uses for the saxophone.
Uses in military bands and classical music.
The saxophone first gained popularity in one of the uses it was designed for: the military band. Although the instrument was studiously ignored in Germany at first, French and Belgian military bands took full advantage of the instrument that Sax had designed. Most French and Belgian military bands incorporate at least a quartet of saxophones comprising at least the E♭ baritone, B♭ tenor, E♭ alto and B♭ soprano. These four instruments have proved the most popular of all of Sax's creations, with the E♭ contrabass and B♭ bass usually considered impractically large and the E♭ sopranino insufficiently powerful. British military bands tend to include at minimum two saxophonists on the alto and tenor. Today, the saxophone is used in military bands all around the world.
The saxophone was subsequently introduced into the concert band, which generally calls for the E♭ alto saxophone, the B♭ tenor saxophone, and the E♭ baritone saxophone. The typical high-level concert band includes two altos, one tenor, and one baritone. The B♭ soprano saxophone is also occasionally used, in which case it is normally played by the first alto saxophonist. The bass saxophone in B♭ is called for in some concert band music (especially music by Percy Grainger).
The saxophone is used in chamber music, such as the saxophone quartet, reed quintet, and other chamber combinations of instruments.
The classical saxophone quartet consists of the soprano saxophone, alto saxophone, tenor saxophone, and baritone saxophone. There is a repertoire of classical compositions and arrangements for the SATB instrumentation dating back to the nineteenth century, particularly by French composers who knew Adolphe Sax. Classical saxophone quartets include Quatuor Habanera, the h2 quartet, Raschèr Saxophone Quartet, the Aurelia Saxophone Quartet, the New Century Saxophone Quartet, and others. Historically, the quartets led by Marcel Mule and Daniel Deffayet, saxophone professors at the Conservatoire de Paris, were started in 1928 and 1953, respectively, and were highly regarded. The Mule quartet is often considered the prototype for future quartets, due the level of virtuosity demonstrated by its members and its central role in the development of the quartet repertoire. However, organised quartets did exist before Mule's ensemble, the prime example being the quartet headed by Eduard Lefebre (1834–1911), former soloist with the Sousa band, in the United States "c." 1904–1911. Other ensembles most likely existed at this time as part of the saxophone sections of the many touring professional bands that existed in the late 19th and early 20th centuries.
The saxophone is a member of the reed quintet. The reed quintet consists of an oboe, a clarinet, a saxophone, a bass clarinet, and a bassoon.
In the 20th and 21st centuries, the saxophone has found increased popularity in the symphony orchestra. In one or other size, the instrument has also been found as a useful accompaniment to genres as wide-ranging as opera and choral music. Many musical theatre scores include parts for the saxophone, sometimes doubling another woodwind or brass instrument. In this way, the sax serves as a middle point between other woodwinds and the brass section, helping to blend the two sections.
Uses in jazz and popular music.
The saxophone is also commonly used in jazz music, where the saxophone is one of the signature sounds. Beginning in the early 20th century, the saxophone became popular in dance orchestras, which were not jazz ensembles but influenced the format of the big swing era bands that were soon to follow. The arrival of the saxophone as a jazz instrument is attributed to tenor saxophonist Coleman Hawkins' stint with the Fletcher Henderson Orchestra starting in 1923. The saxophone was soon embraced by Chicago style musicians who added it, along with chordal instruments such as a piano, banjo, or guitar, to the trumpet-clarinet-trombone-bass-drums ensemble format inherited from New Orleans Jazz. The Duke Ellington Orchestra of the late 1920s featured saxophone-based ensemble sounds and solos by saxophonists Otto Hardwick, Johnny Hodges, and Harry Carney. The swing bands of the 1930s utilized arrangements of saxophone and brass sections playing off each other in call-response patterns. The influence of tenor saxophonist Lester Young with the Count Basie Orchestra in the late 1930s and the tremendous popularity of Coleman Hawkins' 1939 recording of Body and Soul marked the saxophone as an influence on jazz equal to that of the trumpet, which had been the defining instrument of jazz since its beginnings in New Orleans. But the greatest influence of the saxophone on jazz was to occur just a few years later, as alto saxophonist Charlie Parker became an icon of the bebop revolution that influenced generations of jazz musicians. The small group format of bebop and post-bebop jazz ensembles, typically with one to three lead instruments(usually including a saxophone), a chordal instrument, bass, and drums, gained ascendancy in the 1940s as musicians emphasized extended exploration utilizing the new harmonic and melodic freedoms that bebop provided, thanks to Charlie Parker and a few other pioneers such as Dizzy Gillespie, Thelonious Monk, and Bud Powell.
In addition to the colossal brilliance and virtuosity of Parker, the alto sax was also popularized in the 1950s by top saxophonists such as Sonny Stitt, Cannonball Adderley, Sonny Criss and Paul Desmond (latter of the Dave Brubeck Quartet). The tenor sax, which some consider to be the more popular form of saxophone as a solo instrument in jazz, was popularized by jazz greats such as Lester Young, Coleman Hawkins, Dexter Gordon, John Coltrane, Sonny Rollins, Stan Getz and Zoot Sims. The baritone sax, featured more in big bands (notably by Harry Carney in the Duke Ellington Orchestra) and larger ensembles than as a solo instrument, was popularized in jazz as a solo instrument within small groups by musicians such as Serge Chaloff, Gerry Mulligan, Pepper Adams and Leo Parker. The soprano saxophone was popularized by Sidney Bechet in early jazz, but then largely fell out of favor on the jazz scene until John Coltrane began to feature the instrument. Popular smooth jazz/contemporary pop musician Kenny G also features the soprano sax as his principal instrument.
Saxophone players such as John Coltrane, Ornette Coleman, Sam Rivers and Pharaoh Sanders again defined the forefront of creative exploration with the avant-garde movement of the 1960s. Modal, harmolodic, and free jazz again removed boundaries and the new space was explored with every device that saxophone players could conceive of. Sheets of sound, tonal exploration, upper harmonics, and multiphonics were hallmarks of the creative possibilities that saxophones offered in the new realm. One lasting influence of the avant-garde movement has been the exploration of non-western ethnic sounds on the saxophone, for example, the Africanized sounds used by Pharaoh Sanders. The devices of the avant-garde movement have continued to be influential in music that challenges the boundaries between avant-garde and other categories of jazz, such as that of alto saxophonists Steve Coleman and Greg Osby.
The jazz saxophone quartet is usually made up of one B♭ soprano, one E♭ alto, one B♭ tenor and one E♭ baritone (SATB). On occasion, the soprano is replaced with a second alto sax (AATB); a few professional saxophone quartets have featured non-standard instrumentation, such as James Fei's Alto Quartet (four altos) and Hamiet Bluiett's Bluiett Baritone Nation (four baritones). Recently, the World Saxophone Quartet has become known as the preeminent jazz saxophone quartet.
The saxophone, as a solo instrument or as part of a horn section, may also be heard in blues, soul music, rhythm and blues, reggae, ska, funk, rock and roll and other forms of popular music. Some players of these genres include King Curtis, Maceo Parker, Bobby Keys, Clarence Clemons, the Memphis Horns, and the Phenix Horns.
The saxophone family.
The primary (military band) saxophone family alternates instruments in B♭ and E♭. The other (orchestral) family patented by Sax, alternating instruments in C and F, has always been marginal, although some manufacturers tried to popularise the soprano in C (or C soprano saxophone), the alto in F (or mezzo-soprano saxophone), and the tenor in C (or C melody saxophone) early in the twentieth century. The C melody enjoyed some success in the late 1920s and early 1930s as a parlor instrument. One company has recently revived production of the C soprano and C melody. Instruments in F are rare.
Description.
The saxophone consists of an approximately conical tube, usually of thin brass, flared at the tip to form a bell. At intervals along the tube are between 20 and 23 tone holes of varying size and two very small vent holes to assist the playing of the upper register. These holes are covered by keys (also known as pad cups), containing soft leather pads, which are closed to produce an airtight seal. At rest some of the holes stand open and others are closed. The keys are activated by keytouches pressed by the fingers, either directly on the pad cup or connected to it with levers, either directly or with joints called "linkages." The right thumb sits under a thumb rest to stabilize and balance the saxophone, while the weight of most saxophones is supported by a neckstrap attached to a strap ring on the rear of the body of the instrument. The fingering for the saxophone is a combination of that of the oboe with the Boehm system, and is very similar to the flute or the upper register of the clarinet. Instruments that play to low A have a left thumb key for that note.
The simplest design of saxophone is a straight conical tube, and the sopranino and soprano saxophones are usually of this straight design. However, as the lower-pitched instruments would be unacceptably long if straight, for ergonomic reasons, the larger instruments usually incorporate a U-bend ("bow") at, or slightly above, the third-lowest tone hole. As this would cause the bell of the instrument to point almost directly upward, the end of the instrument is either beveled or tilted slightly forward. This U-shape has become a distinctive feature of the saxophone family, to the extent that soprano and even sopranino saxes are sometimes made in the curved style, even though not strictly necessary. By contrast, tenors and even baritones have occasionally been made in the straight style. Most commonly, however, the alto and tenor saxophones incorporate a detachable, curved "neck" above the highest tone hole, directing the mouthpiece to the player's mouth while the instrument is held in a playing stance. The baritone, bass and contrabass saxophones accommodate the length of the bore with extra bows and right angle bends between the main body and the mouthpiece.
Materials.
Most saxophones, both past and present, are made from brass. Despite this, they are categorized as woodwind instruments rather than brass, as the sound waves are produced by an oscillating wood reed, not the player's lips against a mouthpiece as in a brass instrument, and because different pitches are produced by breath wind passing opening and closing keys. The screw pins that connect the rods to the posts, as well as the needle and leaf springs that cause the keys to return to their rest position after being released, are generally made of blued or stainless steel. Since 1920, most saxophones have 'key touches' (smooth replaceable pieces placed where the fingers touch the instrument) made from either plastic or mother of pearl. Recently, some saxophones are offered with abalone or stone keytouches.
Other materials have been tried with varying degrees of success, such as the 1950s Grafton plastic alto saxophone and its recent successor, the polycarbonate saxophone, . There is also the wooden Sawat saxophone created in Thailand on a small scale. Recent years have seen use higher copper alloys substituted for the "yellow brass" or "cartridge brass" that are most common, for visual and tonal effect. Yanagisawa's 902 and 992 series saxophones are made with phosphor bronze, which is claimed to offer slightly different, more "vintage" tonal qualities from the brass 901 and 991 models of identical design. Other saxophones made of high copper alloys are sold under the brands Chateau, Kessler, Saxgourmet, and Bauhaus Walstein. Yanagisawa and other manufacturers, starting with the King Super 20 around 1950, have made saxophone necks, bells, or entire instruments from sterling silver. Keilwerth and P. Mauriat have made saxes with a nickel silver body. Opinions vary on the significance of body materials to sound. With the exception of the identical brass and phosphor bronze Yanagisawa models, opportunities to isolate body materials from other variables in design and construction are lacking.
Prior to final assembly, the manufacturers usually apply a thin coating of clear or colored acrylic lacquer, or silver plate, over the bare brass. The lacquer or plating serves to protect the brass from oxidation, and maintains its shiny appearance. Several different types and colors of surface finish have been used over the years. It is also possible to plate the instrument with nickel or gold, and a number of gold-plated saxophones have been produced. Plating saxophones with gold is an expensive process because gold does not adhere directly to brass. As a result, the brass is first plated with silver, then gold.
Some players, sellers, and repair technicians argue that the type of lacquer or plating, or absence thereof, may enhance an instrument's tone quality. The possible effects of different finishes on tone are difficult to isolate from the other variables that affect an instrument's tone colors. In any case, what constitutes a pleasing tone is a matter of personal preference.
Mouthpiece and reed.
The saxophone uses a single-reed mouthpiece similar to that of the clarinet. Most saxophonists use reeds made from "Arundo donax" cane, but since the 20th century some have also been made of fiberglass and other composite materials. Saxophone reeds are proportioned slightly differently from clarinet reeds, being wider for the same length, although some soprano saxophonists use clarinet reeds on the soprano saxophone. Each size of saxophone (alto, tenor, etc.) uses a different size of reed. Reeds are commercially available in a vast array of brands, styles, and strengths. Players experiment with reeds of different strength (hardnesses) and material to find which strength and cut suits their mouthpiece, embouchure, physiology, and playing style.
The saxophone mouthpiece is larger than that of the clarinet, has a wider inner chamber, and lacks the cork-covered tenon of a clarinet mouthpiece because the saxophone neck inserts into the mouthpiece whereas the clarinet mouthpiece piece is inserted into the barrel. Saxophone and clarinet embouchures differ from each other in firmness, position of the lower lip, and range of entry angles. The "long tones" exercise is used to develop embouchure, along with airstream and breath control.
Mouthpieces come in a wide variety of materials, including vulcanized rubber (sometimes called hard rubber or ebonite), plastic, and metals such as bronze or surgical steel. Less common materials that have been used include wood, glass, crystal, porcelain, and even bone. According to Larry Teal, the mouthpiece material has little, if any, effect on the sound, and the physical dimensions give a mouthpiece its tone colour. There are examples of "dark" sounding metal pieces and "bright" sounding hard rubber pieces. Some contend that instability at the mouthpiece/neck connection moves harmonic frequencies off series with the fundamental frequency and each other, resulting in a "spread" sound, and that the weight of a metal mouthpiece counteracts that instability, increasing tonal "focus." Mouthpiece design has a profound impact on tone.
Early mouthpieces were designed to produce a warm and round sound for classical playing. Among classical mouthpieces, those with a concave ("excavated") chamber are more true to Adolphe Sax's original design; these provide a softer or less piercing tone favored by some saxophonists, including students of Sigurd Raschèr, for classical playing. Saxophonists who follow the French school of classical saxophone playing, influenced by Marcel Mule, generally use mouthpieces with smaller chambers than Rascher style mouthpieces. The use of the saxophone in dance orchestras and jazz ensembles put a premium on dynamic range, projection, and tonal richness, leading to rapid innovation in chamber shape and tip design, and metal construction. At the opposite extreme from the classical mouthpieces are those with a small chamber and a low clearance above the reed between the tip and the chamber, called high baffle. These produce a bright sound with maximum projection, suitable for having a sound stand out among amplified instruments and typical of modern pop and smooth jazz. Most saxophonists who play different styles have a mouthpiece suited for each style.
Unusual saxophone variants.
A number of saxes and saxophone-related instruments have appeared since Sax's original work, most with no significant success. These include the saxello, essentially a straight B♭ soprano, but with a slightly curved neck and tipped bell; the straight alto; and the straight B♭ tenor. Since a straight-bore tenor is approximately five feet long, the cumbersome size of such a design makes it almost impossible to either play or transport. "King" Saxellos, made by the H. N. White Company in the 1920s, now command prices up to US$4,000. A number of companies, including Keilwerth, Rampone & Cazzani ("altello" model), L.A. Sax and Sax Dakota USA, are marketing straight-bore, tipped-bell soprano saxophones as saxellos (or "saxello sopranos").
The "contralto" saxophone, similar in size to the orchestral soprano, was developed in the late 20th century by California instrument maker Jim Schmidt. This instrument has a larger bore and a new fingering system, and does not resemble the C melody instrument except for its key and register. Another new arrival to the sax scene is the soprillo sax, a piccolo-sized straight instrument with the upper speaker hole built into the mouthpiece. The instrument, which extends Sax's original family, as it is pitched a full octave higher than the B♭ soprano sax, is manufactured by Benedikt Eppelsheim, of Munich, Germany. There is a rare prototype slide tenor saxophone, but few were ever made. One company that produced a slide soprano saxophone was Reiffel & Husted, Chicago, ca. 1922 (catalog NMM 5385).
Two of these variants were championed by jazz musician Rahsaan Roland Kirk, who called his straight Buescher alto a stritch and his modified saxello a manzello; the latter featured a larger-than-usual bell and modified key work. Among some saxophonists, Kirk's terms have taken on a life of their own in that it is believed that these were "special" or "new" saxophones that might still be available. Though rare, the Buescher straight alto was a production item instrument while the manzello was indeed a saxello with a custom-made bell.
Another unusual variant of the saxophone was the "Conn-O-Sax", a straight-conical bore instrument in F (one step above the E♭ alto) with a slightly curved neck and spherical bell. The instrument, which combined a saxophone bore and keys with a bell shaped similar to that of a heckelphone, was intended to imitate the timbre of the English horn and was produced only in 1929 and 1930. The instrument had a key range from low A to high G. Fewer than 100 Conn-O-Saxes are in existence, and they are eagerly sought by collectors.
The tubax, developed in 1999 by the German instrument maker Benedikt Eppelsheim, plays the same range, and with the same fingering, as the E♭ contrabass saxophone; its bore, however, is narrower than that of a contrabass saxophone, making for a more compact instrument with a "reedier" tone (akin to the double-reed contrabass sarrusophone). It can be played with the smaller (and more commonly available) baritone saxophone mouthpiece and reeds. Eppelsheim has also produced subcontrabass tubaxes in C and B♭, the latter being the lowest saxophone ever made. Among the most recent developments is the aulochrome, a double soprano saxophone invented by Belgian instrument maker François Louis in 2001.
The fingering scheme of the saxophone, which has had only minor changes since the instrument's original invention, has presented inherent acoustic problems related to closed keys below the first open tonehole that affect response of, and slightly muffle, some notes. There is also a lack of tactile consistency moving between key centers. In other words, extra effort is required from the player to adjust modes of muscle memory when moving between key centers. Two efforts to remedy the acoustic problems and awkward aspects of the original fingering system are noteworthy.
The Leblanc Rationale and System saxophones had key mechanics designed to remedy the acoustic problems associated with closed keys below the first open tonehole. They also enabled the player to make half-step shifts of scales by depressing one key while keeping the rest of the fingering consistent with that of the fingering a half step away (which could also trip up players used to certain alternate fingerings on a regular saxophone). Some Leblanc System features were built into the Vito Model 35 saxophones of the 1950s and 1960s. The acceptance of what was arguably a superior system was impaired by the adjustment required of players switching between System and non-System horns, and the added costs associated with the added complexity of certain key mechanisms.
The chromatic, or linear fingering, saxophone is a project of instrument designer and builder Jim Schmidt, developing a horn maximizing tactile and logical consistency between every interval on the horn regardless of key, and avoiding the acoustic problems associated closed keys below the first open tone hole. Several working prototypes have been built and presented at trade shows. Production of this fascinating and expensive saxophone is on an individual order basis according to the designer's website referenced above.
Related instruments.
Although not true saxophones, inexpensive keyless folk versions of the saxophone made of bamboo (recalls a Chalumeau) were developed in the 20th century by instrument makers in Hawaii, Jamaica, Thailand, Indonesia, Ethiopia, and Argentina. The Hawaiian instrument, called a xaphoon, was invented during the 1970s and is also marketed as a "bamboo sax," although its cylindrical bore more closely resembles that of a clarinet, and its lack of any keywork makes it more akin to a recorder. Jamaica's best known exponent of a similar type of homemade bamboo "saxophone" was the mento musician and instrument maker 'Sugar Belly' (William Walker). In the Minahasa region of the Indonesian island of Sulawesi, there exist entire bands made up of bamboo "saxophones" and "brass" instruments of various sizes. These instruments are imitations of European instruments, made using local materials. Very similar instruments are produced in Thailand. In Argentina, Ángel Sampedro del Río and Mariana García have produced bamboo saxophones of various sizes since 1985, the larger of which have bamboo keys to allow for the playing of lower notes.
Composition.
The extension in C major of the military soprano, alto, tenor and baritone when playing a B♭ major scale.
Music for most saxophones is usually notated using treble clef. The standard written range extends from a B♭ below the staff to an F or F♯ three ledger lines above the staff. Most, if not all, intermediate and professional saxophones made today are built with F♯ keys, with F♯ included on even student instruments.
There are many models of soprano saxophone that have a key for high G, and most modern models of baritone saxophone have an extended bore and key to produce low A; it is also possible to play a low A on any saxophone by blocking the end of the bell, usually with the foot or inside of the left thigh. Low A keys however were not limited to just the baritone saxophone. For a short time Selmer Paris produced mark VI alto saxophones with the low A key. Notes above F are considered part of the altissimo register of any sax, and can be produced using advanced embouchure techniques and fingering combinations. Sax himself had mastered these techniques; he demonstrated the instrument as having a range of just beyond three octaves up to a (written) high B4. Modern saxophone players have extended this range to over 4 octaves on tenor and alto.
Because all saxophones use the same key arrangement and fingering to produce a given notated pitch, it is not difficult for a competent player to switch among the various sizes when the music has been suitably transposed, and many do so. Since the baritone and alto are pitched in E♭, players can read concert pitch music notated in the bass clef by reading it as if it were treble clef and adding three sharps to the key signature. This process, referred to as "clef substitution", makes it possible for the Eb instruments to play from parts written for bassoon, tuba, trombone, or string bass. This can be useful if a band or orchestra lacks one of those instruments.
References.
</dl>

</doc>
<doc id="26797" url="http://en.wikipedia.org/wiki?curid=26797" title="Sackbut">
Sackbut

A sackbut is a type of trombone from the Renaissance and Baroque eras. It is characterised by a telescopic slide used to vary the length of the tube to change pitch, allowing chromaticism easy and accurate doubling of voices. Sackbuts adjust tuning at the joint between the bell and slide. The sackbut differs from modern trombones by its smaller bore, its less-flared bell, and in the lack of a water key, slide lock, and tuning slide on the bell curve. More delicately constructed than their modern counterparts and featuring a softer, more flexible sound, they attracted a more sizeable repertoire of original chamber and vocal music than many instruments contemporary with them.
Terminological history.
The first reference to a slide instrument was probably "trompette des ménestrels", first found in Burgundy in the 1420s and later in other regions of Europe. The name distinguished the instrument from the "trompettes de guerre" (war trumpets), which were of fixed length.
The next word to appear in the 15th century that implied a slide was the "sackbut" group of words. There are two theories for the sources: it is either derived from the Middle French "sacquer" (to pull) and "bouter" (to push) or from the Spanish "sacar" (to draw or pull) and "bucha" (a tube or pipe). The term survives in numerous English spelling variations including sacbut, sackbutte, sagbut, shagbolt, sacabushe and shakbusshe.
Closely related to "sackbut" was the name used in France: "sacqueboute" and in Spain, where it was "sacabuche". These terms were used in England and France until the 18th century.
In Scotland in 1538 the slide instrument is referred to as "draucht trumpet" (drawn trumpet) as opposed to a "weir trumpet" (war trumpet), which had a fixed length.
In Germany, the original word was "Posaune", appearing about 1450 and is still used today. This (as well as "bason") derives from "busine," which is Latinate and meant straight trumpet.
In Italy it was (and remains) "trombone", which derived from trumpet in the Latin "tromba" or "drompten", used in the Low Countries. The first records of it being used are around 1440, but it is not clear whether this was just a nickname for a trumpet player. In 1487 a writer links the words "trompone" and "sacqueboute" and mentions the instrument as playing the contratenor part in a danceband.
History.
The trombone developed from the trumpet. Up until 1375 trumpets were simply a long straight tube with a bell flare.
There are various uses of "sackbut"-like words in the Bible, which has led to a faulty translation from the Latin bible that suggested the trombones date back as far as 600 BC, but there is no evidence of slides at this time.
From 1375 the iconography sees trumpets being made with bends, and some in 'S' shapes. Around 1400 we see the 'loop' shaped trumpet appear in paintings and at some point in the 15th century, a single slide was added. This slide trumpet was known as a 'trompette des ménestrels' in the alta capella bands.
The earliest clear evidence of a double slide instrument is in a fresco painting by Filippino Lippi in Rome - "The Assumption of the Virgin", dating from 1488-1493.
From the 15th to the 19th centuries, the instrument designs changed very little overall, apart from a slight widening of the bell in classical era. Since the 19th century, trombone bore sizes and bells have increased significantly.
It was one of the most important instruments in Baroque polychoral works, along with the cornetto and organ.
Instrument sizes.
Sackbuts come in several sizes. According to Michael Praetorius, these were:
The pitch of the trombones has (notionally) moved up a semi-tone since the 17th century, and this is explained in the section on Pitch.
Because the tenor instrument is described as "Gemeine" (common or ordinary), this is probably the most widely used trombone.
The basses, due to their longer slides, have a hinged handle on the slide stay, which is used to reach the long positions.
The giant Octav-Posaun / double bass trombone / contra-bass trombone in the style of the those made in 16th/17th centuries is represented by only a few existing instruments. There is an original instrument made by Georg Nicolaus Oller built in Stockholm in 1639 and housed in the Musikmuseet. In addition, Ewald Meinl has made a modern copy of this instrument, and it is currently owned and played by Wim Becu.
Construction.
The bore size of renaissance/baroque trombones is approximately 10 mm and the bell rarely more than 10.5 cm in diameter. This compares with modern tenor trombones, which commonly have bores 12.7 mm to 13.9 mm and bells 17.8 cm to 21.6 cm.
Modern reproductions of sackbuts sacrifice some authenticity to harness manufacturing techniques and inventions that make them more comfortable for modern players, while retaining much of the original character of the old instruments.
Some original instruments could be disassembled into the constituent straight tubes, bowed tubes, bell flare, and stays, with ferrules at the joints. Mersenne has a diagram. (Little imagination is needed to see how it could be reassembled - with an extra tube - into something approaching a natural trumpet.) There is a debate as to whether they used tight fittings, wax or another joining substance. Modern sackbut reproductions are usually soldered together. Some modern sackbut reproductions use glue as a compromise to give a loose fitting for high resonance without risk of falling apart.
Tuning slides came in during the very late 18th century. Early trombonists adjusted pitch with the slide, and by adding variously shaped and sized crooks. Modern reproductions often have a bell bow tuning slide or telescopic slide between the slide and bell sections. Crooks are still used, as are variously sized bell bow sections for larger changes. 
The stays on period sackbuts are flat. While the bell stay remained flat, from about 1660 the slide stays became tubular. On many modern reproductions round slide stays are much more comfortable to play and easier to make.
A loose connection between the bell stay and the bell is thought key to a resonant bell, and thus a better sackbut sound. Original instruments have a hinge joint. Modern copies with a tuning slide in the bell can need more support for operation of the slide, so either an extra stay by the tuning slide is provided or a joint without play in only one axis is employed.
The original way to make the slide tubes was to roll a flat piece of metal around a solid cylinder mandrel, and the joining edges soldered together. Modern manufacturers now draw the tubes. They also tend to have stockings, which were only invented around 1850. In addition, modern made slides are usually made of nickel silver with chrome plating, giving a smoother finish and quieter action than simply the brass that would have originally been used.
The water key was added in the 19th century, but modern reproductions often have them.
Pitch.
Until some time in the 18th century, the trombone was in A and the pitch of that A was about a half-step higher than it is today—460–480 Hz. There was a transition around the 18th century when trombones started to be thought of in Bb at around 440 Hz. This change did not require a change in the instrument, merely a new set of slide positions for each note. But it does mean that the baroque and renaissance repertoire was intended to be played at the higher pitch. There are many examples of evidence for this:
The tenor trombones that survive are pitched closest to Bb at A=440 Hz, which is the same as A at A=466 Hz. So what we now think of as a tenor trombone with Bb in first position, pitched at A=440 was actually thought of as a trombone in A (in first position), pitched at A=466. Surviving basses in D at A=466 (Eb at 440) - for example: Ehe, 1612 (Leipzig) and Hainlein, c.1630 (Nuremberg) confirm Praetorius' description. It is also worth noting that Rognoni's "Suzanne ung jour" setting descends repeatedly to BBb, which is a tone lower than the lowest note playable on a bass in F; on a bass in D, it falls in (modern) fifth position.
Many groups now perform at A=466 Hz for the sake of greater historical accuracy.
Timbre.
The llsackbut was described as suitable for playing with the 'loud' ensembles in the outdoors, as well as the 'soft' ensembles inside.
The alta capella bands are seen in drawings as entertaining outside with ensembles including shawms, trumpets and trombones. When pushed, sackbuts can easily make a loud and brassy sound.
The sackbut also responds very well to rather soft playing - more so than a modern trombone. The sound is characterized by a more delicate, vocal timbre. The flat rims and shallow cups of the older mouthpieces are instrumental in providing the player with a much wider palette of articulations and tonal colours. This flexibility lends itself to a vocal style of playing and facilitates very characterful phrasing.
Mersenne wrote in 1636, "It should be blown by a skillful musician so that it may not imitate the sounds of the trumpet, but rather assimilate itself to the sweetness of the human voice, lest it should emit a warlike rather than a peaceful sound."
The Lorenzo da Lucca was said to have had "in his playing a certain grace and lightness with a manner so pleasing".
Performance practice.
Musicians of the 16th and 17th centuries benefited from a broader base of skills than the average performer today.
They would have to improvise new music. In the Middle Ages to the Renaissance, various music treatises include in their tuition improvising at sight fast moving melody over a cantus firmus, or extra contrapuntal lines to a plainchant. In a non-liturgical setting, an alta capella group (in which a slide trumpet or trombone often featured) would involve the tenor playing the main tune in long tones while two others improvised florid counterpart tunes.
These traditions continued into the baroque with musicians expected to give expression to the written music by ornamenting with a mixture of one-note “graces” and whole passage “divisions” (also known as “diminutions”). The suggestions for producing effective ornaments without disrupting the line and harmony are discussed alongside countless examples in the 16th and early 17th century Italian division tutors. Graces such as the accento, portar della voce, tremolo, groppo, trillo, esclamationo and intonatio are all to be considered by performers of any music in this period.
“Cornetts and trombones...play divisions that are neither scrappy, nor so wild and involved that they spoil the underlying melody and the composer's design: but are introduced at such moments and with such vivacity and charm that they give the music the greatest beauty and spirit”
Bottrigari, Venice 1594
Along with the improvisation, many of these tutors discuss articulation. Francesco Rognoni in 1620 describes the tonguing as the most important part of producing “a good and beautiful effect in playing wind instruments, and principally the cornetto” (which of course had a very similar role to the trombone). The treatises discuss the various strengths of consonants from “le” through “de” to “te”. But the focus of the text is for playing rapid notes “similar to the gorgia of the human voice” with “soft and smooth” double tonguing (“lingua riversa”) using “le re le re”. This is opposed to using “te che te che,” which is described as “harsh, barbarous and displeasing”. The natural ‘pairing’ of notes these articulations provide is similar to the instructions for string players who are instructed to slur (“lireggiar”) pairs of eighth notes with one bow stroke per quarter beat.
Another integral part of the early music sound-world is the musical temperament. Music in the middle-ages favours intervals of the 4th and 5th, which is why Pythagorean tuning was used. The interval of a third was used as a clash until the Renaissance, when it became consonant in compositions, which went hand-in-hand with the widespread use of Meantone temperament. During the 17th century, Well temperament began to become more and more popular as the range of keys increased. Temperament affects the colour of a composition, and therefore modern performances, typically employing equal temperament, may not be true representations of the composers' intentions.
These old tunings can come naturally on a sackbut. As the bell is smaller than a modern trombone, the harmonic series is closer to a perfect harmonic series, which is the basis for just tuning. Without adjusting the slide, the 1st to 2nd harmonic is a perfect octave, 2nd to 3rd harmonic is a 5th slightly wider than equal temperament and 4th to 5th harmonic is a major 3rd slightly narrower than in equal temperament. These adjusted intervals make chords ring and are the basis of meantone. In fact, Speer says, “Once you have found a good C (3rd position), this is also the place you will find your F♯.” Playing C and F♯ in exactly the same position on a modern orchestra sounds out of tune, but it tunes perfectly well on a sackbut if everyone plays meantone.
Plenty of musical understanding can be gathered from reading the original music print. Publishers such as SPES and Arnaldo Forni Edition provide facsimile copies of plenty of music for trombone from this era. To read these it one needs to become familiar with the old clefs, time signatures, ligatures and notational conventions of the era. There are myriad performance indicators embedded in the quirks of the old notation that are simply lost in modern editions.
When reading sackbut music, it is important to consider Musica ficta, to help solve some of the controversial pitches. The scores are unclear and composers were embarrassed to point out accidentals they felt were ‘obvious’ to performers. For example there are occasions where a leading note should be sharpened to a major 7th as you go into a cadence. There also are often questions about which notes accidental markings apply to. There are differences of opinion between editors and performers now, just as there were between performers then.
Repertoire.
Before 1600.
The sackbut replaced the slide trumpet in the 15th century alta capella wind bands that were common in towns throughout Europe playing courtly dance music. See Waits.
Another key use of the trombone was in ceremonies, in conjunction with the trumpet. In many towns in Germany and Northern Italy, 'piffari' bands were employed by local governments throughout the 16th century to give regular concerts in public squares and would lead processions for festivals. Piffari usually contained a mix of wind, brass and percussion instruments and sometimes viols.
Venice's doge had his own piffari company and they gave an hour-long concert in the Piazza each day, as well as sometimes performing for services in St. Mark's. Each of the six confraternities in Venice also had their own independent piffari groups too, which would all play at a lavish procession on the feast of Corpus Domini. These groups are in addition to the musicians employed by St. Mark's to play in the balconies with the choir (the piffari would play on the main level).
It also was used in church music both for instrumental service music and as a doubling instrument for choral music. The treble and high alto parts were most often played by cornetts or shawms, with the violin sometimes replacing the cornett in 17th century Italian music.
The first record of trombones being used in churches was in Innsbruck 1503. Seville Cathedral's records show employment of trombonists in 1526, followed by several other Spanish cathedrals during the 16th century, used not only for ceremonial music and processionals, but also for accompaniment of the liturgical texts as well, doubling voices.
The sacred use of trombones was brought to a fine art by the Andrea Gabrieli, Giovanni Gabrieli and their contemporaries c.1570-1620 Venice and there is also evidence of trombonists being employed in churches and cathedrals in Italy at times during the second half of the 16th century in Bologna, Rome, Padua, Mantua and Modena.
Since ensembles had flexible instrumentation at this time, there is relatively little music before Giovanni Gabrieli's publication "Symphoniae sacrae" (1597) that specifically mentions trombones. The only example currently known is the music by Francesco Corteccia for the Medici wedding 1539.
1600-1700.
Solo.
The 17th century brings two pieces of real solo trombone repertoire.
Giovanni Martino Cesare wrote "La Hieronyma," (Musikverlag Max Hieber, MH6012) the earliest known piece for accompanied solo trombone. It comes from Cesare's collection "Musicali Melodie per voci et instrumenti a una, due, tre, quattro, cinque, e sei" published in Munich 1621 of 28 pieces for a mixture of violins, cornetts, trombone, vocal soloists and organ continuo. The collection also contains "La Bavara" for four trombones.
The other solo trombone piece of the 17th century, "Sonata trombone & basso" (modern edition by H Weiner, Ensemble Publications), was written around 1665. This anonymous piece is also known as the 'St. Thomas Sonata' because it was kept in the library of the Saint Thomas Augustinian Monastery in Brno, Czech Republic.
Francesco Rognoni was another composer who specified the trombone in a set of divisions (variations) on the well-known song "Suzanne ung jour" (London Pro Musica, REP15). Rognoni was a master violin and gamba player whose treatise "Selva di Varie passaggi secondo l'uso moderno" (Milan 1620 and facsimile reprint by Arnaldo Forni Editore 2001) details improvisation of diminutions and Suzanne is given as one example. Although most diminutions are written for organ, string instruments or cornett, Suzanne is "per violone over Trombone alla bastarda". With virtuosic semiquaver passages across the range of the instrument, it reflects Praetorius' comments about the large range of the tenor and bass trombones, and good players of the Quartposaune (bass trombone in F) could play fast runs and leaps like a viola bastarda or cornetto. The term "bastarda" describes a technique that made variations on all the different voices of a part song, rather than just the melody or the bass: "considered illegitimate because it was not polyphonic".
Chamber music.
In the 17th century, a considerable repertoire of chamber music using sackbut with various combinations of violins, cornetts and dulcians, often with continuo, appeared. Composers included Dario Castello, Giovanni Battista Fontana, Giovanni Paolo Cima, Andrea Cima, Johann Heinrich Schmelzer and Matthias Weckmann.
Giovanni Paolo Cima, organist of S. Celso wrote the oldest known trio sonata and solo violin sonata. Contained in his "Concerti ecclesiastici" (Milan 1610) is his brother Andrea's "Capriccio" 'for cornett and trombone or violin and violone'.
Antonio Bertali wrote several trio sonatas for 2 violins, trombone and bass continuo in the mid-17th century. One such "Sonata a 3" is freely available in facsimile form from the Düben Collection website hosted by Uppsala universitet. A "Sonata a3 in C" is published by Musica Rara and attributed to Biber, although the authorship is unclear and it is more likely to have been written by Bertali.
Dario Castello, a wind player at St. Mark's Venice in the early 17th century had two books of "Sonate Concertate" published in 1621 and 1629. The sonatas of 1-4 parts with bass continuo often specify trombones, as well as cornett, violin and bassoon. The numerous reprints during the 17th century affirm his popularity then, as perhaps now.
Giuseppe Scarani joined St. Mark's Venice in 1629 as a singer and in the following year published "Sonate concertate", a volume of works for 2 or 3 (unspecified) instruments (and b.c.). The title has been suggested was chosen to try and capture some of Castello's success.
Tiburtio Massaino wrote a Canzona for eight trombones, published in Raverio's 1608 collection.
Johann Heinrich Schmelzer wrote several sonatas that included trombones—such as his "Sonata à 7" for two cornetts, two trumpets, three trombones, and basso continuo.
Daniel Speer published a four-part sonata in "Neu-gebachene Taffel-Schnitz" (1685). In 1687, Speer published the first written instruction in sackbut (and several other instruments) playing: "Grund-richtiger/kurtz/leicht und noethiger Unterricht der Musicalischen Kunst". The second edition in 1697 provides two three part sonatas for trombones.
An English work of note from this period is Matthew Locke's "Music for His Majestys Sagbutts and Cornetts", a suite for Charles II's coronation 1661.
Light music.
Non-serious music, often based on dances for festive occasions, rarely had specified instrumentation. Often you find something like "per diversi musici". Indeed the groups that would perform them would often be full of multi-instrumentalists.
Johann Pezel wrote for Stadtpfeifer with his "Hora decima musicorum" (1670), containing sonatas, as well as "Fünff-stimmigte blasende Music" (1685) with five-part intradas and dance pieces.
Well known pieces from Germany includes Samuel Scheidt's "Ludi Musici" (1621) and Johann Hermann Schein's "Banchetto musicale" (1617).
The first English piece scored for trombone is John Adson's "Courtly Masquing Ayres" (1611). Another light collection suitable for including trombones is Anthony Holborne's "Pavans, Galliards, Allmains, and other short Aeirs both Grave and Light in Five Parts for Viols, Violins or Other Musicall Winde Instruments" (1599).
Sacred music.
Venice.
Trombonists were in the regular ensemble at St. Mark's Venice from its formation in 1568 until they left the payroll in 1732. The first two ensemble directors - "maestro di concerti" - Girolamo Dalla Casa (1568–1601) and Giovanni Bassano (1601–1617) - were cornett players and the nucleus of the group was 2 cornetts and 2 trombones, although for the larger ceremonies many extra players were hired. During a mass attended by the Doge, evidence suggests they would have played a canzona in the Gradual after the Epistle and the Agnus Dei, a sonata in the Offertory as well as reinforcing vocal parts or substituting for absent singers.
This ensemble was used extensively by Giovanni Gabrieli in pieces substantially for brass, voices and organ in Venice up until his death in 1612. He was greatly influential in Venetian composers in other churches and confraternities, and his early baroque and cori spezzati style is seen in contemporaries like Giovanni Picchi and Giovanni Battista Grillo.
It is suggested that Monteverdi wrote his Vespro della Beata Vergine (1610) as a pitch for employment at St. Mark's as successor to Giovanni Gabrieli. In addition to the Magnificat, two movements specify trombones: the opening "Deus in adiutorium" is for 6 voices, 2 violins, 2 cornetts, 3 trombones, 5 viola da braccio and basso continuo; "Sonata sopra ‘Sancta Maria, ora pro nobis’" is for soprano, 2 violins, 2 cornetts, 3 trombones (one of which can be a viola da braccio), viola da braccio and basso continuo. Monteverdi also leaves the option to use trombones as part of the "sex instrumentis" of the "Dixit Dominus" and in the instrumental "Ritornello a 5" between verses of "Ave maris stella".
From around 1617, when the "maestro de' concerti" at St. Marks changed to violinist Francesco Bonfante and correspondingly the ensemble changed from basically a brass ensemble to being more evenly mixed with brass, wind and string instruments.
Monteverdi arrived at St. Mark's in 1613 and it is unsurprising that he includes trombones and strings for several more sacred works during his time here, published in his "Selva Morale e Spirituale" 1641. Of the c.40 items in this collection, six specify three or four trombones (or viola da braccio, ad lib): SV268 Beatus vir I, SV263 Dixit Dominus I, SV263 Dixit Dominus II, SV261 Et iterum venturus est, SV258 Gloria in excelsis Deo, SV281 Magnificat I. Each is for 3-8 voices with 3 violins (apart from SV261), the trombones/violas and basso continuo. Monteverdi also specified trombones in two more sacred works: SV198 Laetatus sum (i) (1650) for 6 voices, 2 violins, 2 trombones and bassoon and SV272 Laudate Dominum omnes gentes I (1641) for 5 voices ‘concertato’, 4 voice chorus ad lib, 4 viola da braccio or trombones and basso continuo.
Germany/Austria.
A prolific composer for trombones in Germany in the 17th century was Heinrich Schütz. His "Fili me, Absalon" (SWV 269) and "Attendite, popule meus" (SWV 270), are both scored for bass voice, four trombones (of which two are optionally violins) and basso continuo, are well known. They are part of his first "Symphoniae Sacrae" collection dating from 1629 and commentators have noted that the style reflects his studies in Venice with Giovanni Gabrieli 1609-1612. Other pieces that specify trombones (according to Grove) are (grouped by the collection they were published in): Concert mit 11 Stimmen (1618): SWV 21, in "Psalmen Davids" (Psalms of David) Op. 2 (1619): SWV 38, 40-46, Symphoniae Sacrae I Op.6 (1629): SWV 259, 269-271, 274, Symphoniae Sacrae II Op.10 (1647): SWV 344, Symphoniae Sacrae III Op. 12 (1650): SWV 398a, Historia (1664): SWV 435, 448, 449, 453, 461, 452, 466-470, 473, 474-476, Schwanengesang Psalm 119 (1671): SWV 500, although many others are suitable for trombones too.
Johann Hermann Schein specified trombones in some of his sacred vocal works in the "Opella nova, ander Theil, geistlicher Concerten" collection (Leipzig, 1626). For example, "Uns ist ein Kind geboren" is scored for violino, traversa, alto trombone, tenor voice, fagotto and basso continuo. "Mach dich auf, werde licht, Zion" uses Canto 1: violino, cornetto, flauto picciolo e voce, Canto 2: voce e traversa, Alto: Trombone e Voce, Tenore: Voce e Trombone, Basso: Fagotto Trombone e Voce and Basso Continuo, during which solos for each of the trombonists are specified. Of particular interest is "Maria, gegrüsset seist du, Holdselige," which uses soprano and tenor voices, alto trombone, 2 tenor trombones and on the bass line "trombone grosso," which goes down to pedal A, and a couple of diatonic scale passages from bottom C.
German composer Johann Rudolf Ahle wrote some notable sacred pieces for voices and trombones. "Höre, Gott" uses five favoriti singers, two ripieno choirs (which double other parts at intense moments) and seven trombones, with basso continuo. And his most famous "Neu-gepflanzte Thüringische Lust-Garten.." (1657–65) contains several sacred works with 3 or 4 trombones, including "Magnificat a 8" for SATB soloists, cornett, 3 trombones and continuo and "Herr nun lässestu deinen Diener a 5" for bass, 4 trombones and continuo.
Dieterich Buxtehude specifies trombones in a few sacred concertos using style derived from polychoral Venetian works and one secular piece. For example, "Gott fähret auf mit Jauchzen" (BuxWV33 from CW v, 44) is scored for SSB voices, 2 vn, 2 va, trbn, 2 cornetts, 2 tpt, bn and bc.
There are a few vocal works involving trombones in works by Andreas Hammerschmidt. These include "Lob- und Danck Lied aus dem 84. Psalm" for 9 voices, 5 tpt, 3 trbn, 5 va and bc (Freiberg, 1652). There is also "Hochzeitsgesang für Daniel Sartorius: Es ist nicht gut, dass der Mensch allein sei" for 5 voices, 2 vn, 2 trbn, bn and bc.
Johann Schelle has numerous sacred vocal works that use trombones. For instance "Vom Himmel kam der Engel Schar" is scored for soprano, tenor, SSATB choir, 2 violins, 2 violas, 2 cornetts, 3 trombones, 2 trumpets, timpani, basso continuo, and "Lobe den Herrn, meine Seele" is for two choirs of SSATB and similar instruments to the previous work.
The lesser known Austrian composer Christoph Strauss, Kapellmeister to the Habsburg Emperor Mathias 1616-1620, wrote two important collections for trombones, cornetts and voices. His motets published in Nova ac diversimoda sacrarum cantionum composition, seu motettae (Vienna, 1613) are in a similar tradition to Gabrieli's music. Of the sixteen motets in the collection, all are titled "concerto" apart from the "sonata" "Expectans Expectavi Dominum" for 6 trombones, cantus voice and tenor voice. In 1631 he published a number of masses, which were much more baroque, with basso continuo, rhetorical word painting and obligato usage of instruments.
Later in the 17th century, Heinrich Ignaz Franz Biber composed sacred works for voices and orchestra featuring trombones. His "Requiem" mass (1692) uses an orchestra of strings, 3 trombones and basso continuo. A similar ensemble accompanies 8 vocal lines in his "Lux perpetua" (c1673), and three more similar works in the 1690s.
Theatre.
Monteverdi ushers sackbuts into the first great opera - 'L'Orfeo' 1607. The orchestra at the first performance, as shown in the first publication, the list of "stromenti" at the front of the score specifies four trombones, but at one point in Act 3, however, the score calls for five trombones.
1700-1750.
There is relatively little repertoire for the trombone in the late baroque.
But Johann Sebastian Bach uses trombones in fourteen of his church cantatas - BWV 2, 3, 4, 21, 23, 25, 28, 38, 64, 68, 96, 101, 121, 135 as well as motet BWV 118. He uses the trombone sound to reflect the (by now) archaic sounds of the Renaissance trombones doubling voices (with cornett playing the soprano line), yet he also uses them independently, which John Eliot Gardiner says prepares the way for their use in Beethoven's "Symphony No. 5". The cantatas were either composed in Leipzig during 1723-1725, or (for BWV 4, 21 & 23) the trombone parts were added to the existing cantata during the same period. The cornett and trombone parts would have been played by the Stadtpfeifer.
In England, George Frideric Handel includes trombones in three of his oratorios: "Saul" (1738), "Israel in Egypt" (1738) and "Samson" (1741). There are no other documented groups or performances with trombone players in England at this time, and it has been suggested that the premiers took place with a visiting group from Germany, as was the custom in Paris at this time.
Vienna's Imperial court used trombones in church music:
Johann Joseph Fux was Hofkapellmeister in Vienna from 1715 until 1741. Many of his masses use the choir strengthened by strings, cornetts and trombones, often with independent moments for the instrumentalists and sometimes. "Missa SS Trinitatis" uses two choirs, which again points to the traditions going back to Gabrieli. His highly successful Requiem is for five vocal parts, two cornetts, two trombones, strings and continuo. He also uses the trombone in smaller motets and antiphons, such as his setting of "Alma Redemptoris mater" for soprano, alto trombone, strings and continuo. Some of his chamber music involves trombones, as do many of his operas, used as an obbligato instrument.
Also in the Vienna court was Antonio Caldara, vice-kapellmeister 1717-1736. Among his output are two Holy Week settings as Da Capo arias: "Deh sciogliete, o mesti lumi" for soprano, unison violins, bassoon, two trombones and organ and "Dio, qual sia" for soprano, trombone, bassoon and basso continuo.
1750-1800.
Again this period suffers from a lack of trombone players. Most of these works derive from Vienna and Salzburg.
Joseph Haydn uses trombones in "Il rotorno di Tobia", "Die Sieben Letzten Worte", The Creation, Die Jahreszeiten, "Der Sturm", "Orfeo de Euridice" and secular cantata choruses.
Wolfgang Amadeus Mozart uses trombones in connection with death or the supernatural. This includes the Requiem (K626, 1791), Great Mass in C minor (K423, 1783), "Coronation Mass (C major)" (K317, 1779), several other masses, "Vesperae Solennes de Confessore" (K339, 1780), "Vesperae de Dominica", his arrangement of Handel's "Messiah" plus two of his three great operas: Don Giovanni (K527, 1787) and Die Zauberflöte (K620, 1791). Mozart's first use of the trombone was an obligato line in the oratorio "Die Schuldigkeit des ersten Gebots" (K35, 1767)
Christoph Willibald Gluck includes trombones in five of his operas: "Iphigénie en Aulide" (1774), Orfeo ed Euridice (1774), "Alceste" (1776), Iphigénie en Tauride (1779) and "Echo et Narcisse" (1779), as well as ballet "Don Juan" (1761).
Some chamber music in this period includes trombone in an obligato role with voice, and also as a concerto instrument with string orchestra. Composers include the likes of Leopold Mozart, Georg Christoph Wagenseil, Johann Albrechtsberger, Michael Haydn and Johann Ernst Eberlin.
For works for trombone post-1800, please see trombone.
Modern performance.
Many groups specializing in period music make frequent and prominent use of the sackbut.
External links:
Recordings.
Plenty of recordings of the authentic sackbut are now available from the groups such as Concerto Palatino, HMSC, Gabrieli Consort and the Toulouse Sacqueboutiers. For a closer examination of the instrument, here are some recommended recordings where the sackbut is heavily featured in a 'solo' capacity.
Early surviving instruments.
The earliest instruments:
Other notable sackbuts:
For more information, see Herbert (2006).

</doc>
<doc id="26798" url="http://en.wikipedia.org/wiki?curid=26798" title="Saxhorn">
Saxhorn

The saxhorn is a valved brass instrument with a conical bore and deep cup-shaped mouthpiece. The sound has a characteristic mellow quality, and blends well with other brass.
The saxhorn family.
The saxhorns form a family of seven brass instruments (although at one point ten different sizes seem to have existed). Designed for band use, they are pitched alternately in E-flat and B-flat, like the saxophone group.
Historically much confusion exists as to the nomenclature of the various instruments in different languages. During the 19th century, the now-pointless debate as to whether the saxhorn family was truly new, or rather a development of members of the previously existing cornet and tuba families, or copied directly from the flügelhorn was the subject of bitter and prolonged lawsuits.
The following table lists the members of the saxhorn family as described in the orchestration texts of Hector Berlioz and Cecil Forsyth, the J. Howard Foote catalog of 1893, and modern names. The modern instrument names continue to exhibit inconsistency, denoted by a "/" between the two names in use. All of the "modern" instrument names represent exceedingly rare instruments with the exception of the E♭ Tenor/Alto (unless one counts, controversially, the baritone horn as the B♭ Tenor/Baritone member of the family). In the table "Pitch" means the concert pitch of notational Middle C on each instrument (2nd partial, no valves depressed) in scientific pitch notation.
This list is not exhaustive of historic nomenclature for the saxhorns, for which there may exist no comprehensive and authoritative source.
Ranges of individual members.
The saxhorn is based on the same three-valve system as most other valved brass instruments. Each member of the family is named after the root note produced by the second partial with no valves actuated. Each member nominally possesses or possessed the typical three-valve brass range from the note one tritone below that root note (second partial, all valves actuated) to the note produced by eighth partial with no valves actuated, i.e., the note two octaves above the root note.
All the modern members of the family are transposing instruments written in the treble clef with the root note produced by the second partial with no valves actuated being written as middle C, though baritone horn (sometimes viewed as being a saxhorn family member despite its being a predominately cylindrical, rather than conical, instrument) often plays bass clef parts, especially those written for the trombone.
History.
Developed during the mid-to-late 1830s, the saxhorn family was patented in Paris in 1845 by Adolphe Sax. Sax's claim to have invented the instrument was hotly contested by other brass instrument makers during his lifetime, leading to various lawsuits. Throughout the mid-1850s, he continued to experiment with the instrument's valve pattern.
The Trojan March ("Marche Troyenne") of the Berlioz opera Les Troyens (1856–58) features an on-stage band which includes a family of saxhorns.
Saxhorns were popularized by the distinguished Distin Quintet, who toured Europe during the mid-19th century. This family of musicians, publishers and instrument manufacturers had a significant impact on the growth of the brass band movement in Britain during the mid- to late-19th century.
The saxhorn was the most common brass instrument in American Civil War bands. The over-the-shoulder variety of the instrument was used, as the backward-pointing bell of the instrument allowed troops marching behind the band to hear the music.
Contemporary works featuring this instrument are Désiré Dondeyne's "Tubissimo" for bass tuba or saxhorn and piano (1983) and Olivier Messiaen's "Et exspecto resurrectionem mortuorum" (1964).

</doc>
<doc id="26799" url="http://en.wikipedia.org/wiki?curid=26799" title="Scanner">
Scanner

Scanner may refer to:
Other.
Barbara Sher uses the word "scanner" for someone who scans the surface of things, as opposed to "divers" or experts.
Other words for scanner includes polymath, renaissance soul, multitalent, generalist and multipotentialite (as in Multipotentiality)

</doc>
<doc id="26800" url="http://en.wikipedia.org/wiki?curid=26800" title="Sonic Team">
Sonic Team

Sonic Team Corp. (SONICTEAM/ソニックチーム, Sonikku Chīmu) is a Japanese computer and video game developer established in Ōta, Tokyo, Japan in 1990. The Japan-based division is also known as CS (Consumer) No. 2 Research and Development division. Sonic Team are best known for the "Sonic the Hedgehog" series.
History.
In 1990, Sega asked to create a game with a character that was popular enough to rival Nintendo's Super Mario, resulting in the creation of Sonic the Hedgehog. In 1991 AM8 took its name from the Sonic the Hedgehog series and became Sonic Team.
Following the release of "Sonic the Hedgehog", Yuji Naka grew dissatisfied with Sega of Japan's policies and so moved to Sega of America to work in the offices of the newly established Sega Technical Institute, headed by Mark Cerny. Due to most of Sonic Team's key members moving to the Western branch, Sega Technical Institute got the job of handling Sonic's Mega Drive sequels. The American developers collaborated with Sonic Team in the development of "Sonic the Hedgehog 2". Most of the STI staff worked on the zone art and special stages, while most of Sonic Team worked on the level designs and the programming. However, due to most of the Sonic Team staff lacking the ability to speak English, there was trouble with language barriers and differences in approach to game design. Sonic Team members worked very late nights and sometimes slept under their desks in order to perfectly achieve Yuji Naka's guidelines.
The launch of the Sega Saturn, and the cancellation of the beleaguered Sonic Xtreme title saw Sonic Team focus on other titles, including "Nights into Dreams..." and "Burning Rangers". Although Sega did release a modified version of Sonic 3D Blast for the Sega Saturn, and releasing the first Sonic compilation, Sonic Jam which featured a limited bonus area showing off 3D gameplay. The Sega Dreamcast followed in 1998, and launched with the return of main series Sonic the Hedgehog in "Sonic Adventure". Other Sonic Team Dreamcast titles included "ChuChu Rocket!", "Samba de Amigo", and "Phantasy Star Online".
During the transitional phase of Sega dropping out of the console race to concentrate on software and game development, all of its main departments were separated from the main company and established on semi-autonomous subsidiaries. In 2000, Sonic Team officially became Sonic Team Ltd. In 2002, the other creator of Sonic, Hirokazu Yasuhara, left Sonic Team when he moved to Naughty Dog.
Also during this phase, United Game Artists (formerly Sega AM9) merged with Sonic Team Japan in 2003 to start the "Sonic Riders" series. In 2004, following Sega's merger with Sammy, Sonic Team once more became an internal division of Sega after being spun off as a second-party developer in 2000. The company name of Sonic Team USA was also changed to Sega Studio USA. Unlike most of the other divisions, Sonic Team still retains its internal structure and name. On May 8, 2006, Naka left the group with ten other members of Sonic Team to establish an independent game developer, Prope.
In 2006, Sonic Team developed the "Sonic the Hedgehog" for PlayStation 3 and Xbox 360.
"", the sequel to Sega Saturn title "Nights into Dreams..." was released for the Wii in 2007. A new game engine was designed for the game, which utilises PhysX. The team also decided to not include American influence in the game like some of the modern 3D Sonic games, instead choosing a more British based influence. Following the game's release, Eitaro Toyoda moved back to Sonic Team Japan to work on "Sonic and the Secret Rings" as a game designer.
In 2009, the director of "Sonic Unleashed" and creator of the Hedgehog Engine, Yoshihisa Hashimoto, left Sonic Team to move to Square Enix.
In 2010, Sonic Team released ', a sequel to the Genesis-era Sonic entries. The game was co-developed with Dimps, developers of the Sonic Advance and Sonic Rush titles. A sequel, ', was released in 2012.
2011 saw the release of "Sonic Generations", a 2D/3D platformer that would blend classic levels from the original series up to the modern generation as a milestone celebration title of the series's 20th anniversary.
Sega Studio USA.
Sega Studio USA (formerly Sonic Team USA) was the United States division of Sonic Team located in San Francisco, California. The division was formed in 1999. All of the team members that worked at Sega Studio USA were Japanese, with the exception of Brad Wagner, who was American and hired at the studio during the development of "Shadow the Hedgehog". The division first worked on ChuChu Rocket! and the international release of "Sonic Adventure".
The US branch's first game was the 2001 Dreamcast game "Sonic Adventure 2". The newly established Sonic Team USA was so influenced by their new San Francisco location, that the level designers of the game, Takashi Iizuka and Eitaro Toyoda, designed some of the levels, such as the City Escape, Mission Street, Radical Highway, Route 101, and Route 280 levels as references to major San Francisco locations. The City Escape level resembles the steep, downhill roads of the city.
In 2002, Sonic Team USA ported "Sonic Adventure 2" to the Nintendo GameCube, the first home console Sonic title to appear on a major non-SEGA system following the cancellation of the Dreamcast. They had 6 months to polish and refine the game for the GameCube. The port was renamed "Sonic Adventure 2 Battle", after the added multiplayer mode enhancements. The developers made minor graphical improvements, included additional Chao Garden extras, and added in extra level geometry such as trees. "Sonic Adventure 2 Battle" was the only third party game on the GameCube to sell over one million copies in USA alone.
Their next project was "Sonic Heroes", released in 2003. The team wanted to port this game on all platforms to achieve better sales and broaden the fanbase, so they decided to use RenderWare as a game engine to make programming the game on multiple consoles a lot easier, due to the studio's lack of experience developing for the Xbox. The game was released on the GameCube, PlayStation 2, Xbox, and PC platforms.
After "Sonic Heroes", Sonic Team USA decided to change their official name to Sega Studio USA. Sega Studio USA went on to make their next multiplatform game, "Shadow the Hedgehog", using the studio's own game engine instead of RenderWare. The game was released on the GameCube, PlayStation 2, and Xbox. Despite a largely negative critical reaction, the game sold in excess of a million copies. This helped increase Sega's profits in 2006. Members of Sega Studio USA also supervised and designed the concept for "Sonic Rivals" and "Sonic Rivals 2" on the PlayStation Portable although the actual development of the games was done by Backbone Entertainment.
In 2008, Sega Studio USA was absorbed back into Sonic Team Japan.

</doc>
<doc id="26805" url="http://en.wikipedia.org/wiki?curid=26805" title="Sex">
Sex

Organisms of many species are specialized into male and female varieties, each known as a sex. Sexual reproduction involves the combining and mixing of genetic traits: specialized cells known as gametes combine to form offspring that inherit traits from each parent. Gametes can be identical in form and function (known as isogamy), but in many cases an asymmetry has evolved such that two sex-specific types of gametes (heterogametes) exist (known as anisogamy). By definition, male gametes are small, motile, and optimized to transport their genetic information over a distance, while female gametes are large, non-motile and contain the nutrients necessary for the early development of the young organism. Among humans and other mammals, males typically carry XY chromosomes, whereas females typically carry XX chromosomes, which are a part of the XY sex-determination system.
The gametes produced by an organism determine its sex: males produce male gametes (spermatozoa, or sperm, in animals; pollen in plants) while females produce female gametes (ova, or egg cells); individual organisms which produce both male and female gametes are termed hermaphroditic. Frequently, physical differences are associated with the different sexes of an organism; these sexual dimorphisms can reflect the different reproductive pressures the sexes experience.
Evolution.
It is considered that sexual reproduction first appeared about a billion years ago, evolved within ancestral single-celled eukaryotes. The reason for the initial evolution of sex, and the reason(s) it has survived to the present, are still matters of debate. Some of the many plausible theories include: that sex creates variation among offspring, sex helps in the spread of advantageous traits, and that sex helps in the removal of disadvantageous traits.
Sexual reproduction is a process specific to eukaryotes, organisms whose cells contain a nucleus and mitochondria. In addition to animals, plants, and fungi, other eukaryotes (e.g. the malaria parasite) also engage in sexual reproduction. Some bacteria use conjugation to transfer genetic material between cells; while not the same as sexual reproduction, this also results in the mixture of genetic traits.
What is considered defining of sexual reproduction in eukaryotes is the difference between the gametes and the binary nature of fertilization. Multiplicity of gamete types within a species would still be considered a form of sexual reproduction. However, no third gamete is known in multicellular animals.
While the evolution of sex itself dates to the prokaryote or early eukaryote stage, the origin of chromosomal sex determination may have been fairly early in eukaryotes. The ZW sex-determination system is shared by birds, some fish and some crustaceans. Most mammals, but also some insects ("Drosophila") and plants ("Ginkgo") use XY sex-determination.
X0 sex-determination is found in certain insects.
No genes are shared between the avian ZW and mammal XY chromosomes, and from a comparison between chicken and human, the Z chromosome appeared similar to the autosomal chromosome 9 in human, rather than X or Y, suggesting that the ZW and XY sex-determination systems do not share an origin, but that the sex chromosomes are derived from autosomal chromosomes of the common ancestor of birds and mammals.
A paper from 2004 compared the chicken Z chromosome with platypus X chromosomes and suggested that the two systems are related.
Sexual reproduction.
Sexual reproduction in eukaryotes is a process whereby organisms form offspring that combine genetic traits from both parents. Chromosomes are passed on from one generation to the next in this process. Each cell in the offspring has half the chromosomes of the mother and half of the father.
Genetic traits are contained within the deoxyribonucleic acid (DNA) of chromosomes—by combining one of each type of chromosomes from each parent, an organism is formed containing a doubled set of chromosomes. This double-chromosome stage is called "diploid", while the single-chromosome stage is "haploid". Diploid organisms can, in turn, form haploid cells (gametes) that randomly contain one of each of the chromosome pairs, via meiosis. Meiosis also involves a stage of chromosomal crossover, in which regions of DNA are exchanged between matched types of chromosomes, to form a new pair of mixed chromosomes. Crossing over and fertilization (the recombining of single sets of chromosomes to make a new diploid) result in the new organism containing a different set of genetic traits from either parent.
In many organisms, the haploid stage has been reduced to just gametes specialized to recombine and form a new diploid organism; in others, the gametes are capable of undergoing cell division to produce multicellular haploid organisms. In either case, gametes may be externally similar, particularly in size (isogamy), or may have evolved an asymmetry such that the gametes are different in size and other aspects (anisogamy).
By convention, the larger gamete (called an ovum, or egg cell) is considered female, while the smaller gamete (called a spermatozoon, or sperm cell) is considered male. An individual that produces exclusively large gametes is female, and one that produces exclusively small gametes is male. An individual that produces both types of gametes is a hermaphrodite; in some cases hermaphrodites are able to self-fertilize and produce offspring on their own, without a second organism.
Animals.
Most sexually reproducing animals spend their lives as diploid organisms, with the haploid stage reduced to single cell gametes. The gametes of animals have male and female forms—spermatozoa and egg cells. These gametes combine to form embryos which develop into a new organism.
The male gamete, a spermatozoon (produced within a testicle), is a small cell containing a single long flagellum which propels it.
Spermatozoa are extremely reduced cells, lacking many cellular components that would be necessary for embryonic development. They are specialized for motility, seeking out an egg cell and fusing with it in a process called fertilization.
Female gametes are egg cells (produced within ovaries), large immobile cells that contain the nutrients and cellular components necessary for a developing embryo.
Egg cells are often associated with other cells which support the development of the embryo, forming an egg. In mammals, the fertilized embryo instead develops within the female, receiving nutrition directly from its mother.
Animals are usually mobile and seek out a partner of the opposite sex for mating. Animals which live in the water can mate using external fertilization, where the eggs and sperm are released into and combine within the surrounding water. Most animals that live outside of water, however, must transfer sperm from male to female to achieve internal fertilization.
In most birds, both excretion and reproduction is done through a single posterior opening, called the cloaca—male and female birds touch cloaca to transfer sperm, a process called "cloacal kissing". In many other terrestrial animals, males use specialized sex organs to assist the transport of sperm—these male sex organs are called intromittent organs. In humans and other mammals this male organ is the penis, which enters the female reproductive tract (called the vagina) to achieve insemination—a process called sexual intercourse. The penis contains a tube through which semen (a fluid containing sperm) travels. In female mammals the vagina connects with the uterus, an organ which directly supports the development of a fertilized embryo within (a process called gestation).
Because of their motility, animal sexual behavior can involve coercive sex. Traumatic insemination, for example, is used by some insect species to inseminate females through a wound in the abdominal cavity—a process detrimental to the female's health.
Plants.
Like animals, plants have developed specialized male and female gametes. Within most familiar plants, male gametes are contained within hard coats, forming pollen. The female gametes of plants are contained within ovules; once fertilized by pollen these form seeds which, like eggs, contain the nutrients necessary for the development of the embryonic plant.
Female (left) and male (right) cones are the sex organs of pines and other conifers.
Many plants have flowers and these are the sexual organs of those plants. Flowers are usually hermaphroditic, producing both male and female gametes. The female parts, in the center of a flower, are the carpels—one or more of these may be merged to form a single pistil. Within carpels are ovules which develop into seeds after fertilization. The male parts of the flower are the stamens: these long filamentous organs are arranged between the pistil and the petals and produce pollen at their tips. When a pollen grain lands upon the top of a carpel, the tissues of the plant react to transport the grain down into the carpel to merge with an ovule, eventually forming seeds.
In pines and other conifers the sex organs are conifer cones and have male and female forms. The more familiar female cones are typically more durable, containing ovules within them. Male cones are smaller and produce pollen which is transported by wind to land in female cones. As with flowers, seeds form within the female cone after pollination.
Because plants are immobile, they depend upon passive methods for transporting pollen grains to other plants. Many plants, including conifers and grasses, produce lightweight pollen which is carried by wind to neighboring plants. Other plants have heavier, sticky pollen that is specialized for transportation by insects. The plants attract these insects with nectar-containing flowers. Insects transport the pollen as they move to other flowers, which also contain female reproductive organs, resulting in pollination.
Fungi.
Most fungi reproduce sexually, having both a haploid and diploid stage in their life cycles. These fungi are typically isogamous, lacking male and female specialization: haploid fungi grow into contact with each other and then fuse their cells. In some of these cases the fusion is asymmetric, and the cell which donates only a nucleus (and not accompanying cellular material) could arguably be considered "male".
Some fungi, including baker's yeast, have mating types that create a duality similar to male and female roles. Yeast with the same mating type will not fuse with each other to form diploid cells, only with yeast carrying the other mating type.
Fungi produce mushrooms as part of their sexual reproduction. Within the mushroom diploid cells are formed, later dividing into haploid spores—the height of the mushroom aids the dispersal of these sexually produced offspring.
Sex determination.
The most basic sexual system is one in which all organisms are hermaphrodites, producing both male and female gametes— this is true of some animals (e.g. snails) and the majority of flowering plants. In many cases, however, specialization of sex has evolved such that some organisms produce only male or only female gametes. The biological cause for an organism developing into one sex or the other is called sex determination.
In the majority of species with sex specialization, organisms are either male (producing only male gametes) or female (producing only female gametes). Exceptions are common—for example, in the roundworm "C. elegans" the two sexes are hermaphrodite and male (a system called androdioecy).
Sometimes an organism's development is intermediate between male and female, a condition called intersex. Sometimes intersex individuals are called "hermaphrodite"; but, unlike biological hermaphrodites, intersex individuals are unusual cases and are not typically fertile in both male and female aspects.
Genetic.
In genetic sex-determination systems, an organism's sex is determined by the genome it inherits. Genetic sex-determination usually depends on asymmetrically inherited sex chromosomes which carry genetic features that influence development; sex may be determined either by the presence of a sex chromosome or by how many the organism has. Genetic sex-determination, because it is determined by chromosome assortment, usually results in a 1:1 ratio of male and female offspring.
Humans and other mammals have an XY sex-determination system: the Y chromosome carries factors responsible for triggering male development. The default sex, in the absence of a Y chromosome, is female. Thus, XX mammals are female and XY are male. XY sex determination is found in other organisms, including the common fruit fly and some plants. In some cases, including in the fruit fly, it is the number of X chromosomes that determines sex rather than the presence of a Y chromosome (see below).
In birds, which have a ZW sex-determination system, the opposite is true: the W chromosome carries factors responsible for female development, and default development is male. In this case ZZ individuals are male and ZW are female. The majority of butterflies and moths also have a ZW sex-determination system. In both XY and ZW sex determination systems, the sex chromosome carrying the critical factors is often significantly smaller, carrying little more than the genes necessary for triggering the development of a given sex.
Many insects use a sex determination system based on the number of sex chromosomes. This is called X0 sex-determination—the 0 indicates the absence of the sex chromosome. All other chromosomes in these organisms are diploid, but organisms may inherit one or two X chromosomes. In field crickets, for example, insects with a single X chromosome develop as male, while those with two develop as female. In the nematode "C. elegans" most worms are self-fertilizing XX hermaphrodites, but occasionally abnormalities in chromosome inheritance regularly give rise to individuals with only one X chromosome—these X0 individuals are fertile males (and half their offspring are male).
Other insects, including honey bees and ants, use a haplodiploid sex-determination system. In this case diploid individuals are generally female, and haploid individuals (which develop from unfertilized eggs) are male. This sex-determination system results in highly biased sex ratios, as the sex of offspring is determined by fertilization rather than the assortment of chromosomes during meiosis.
Nongenetic.
For many species, sex is not determined by inherited traits, but instead by environmental factors experienced during development or later in life. Many reptiles have temperature-dependent sex determination: the temperature embryos experience during their development determines the sex of the organism. In some turtles, for example, males are produced at lower incubation temperatures than females; this difference in critical temperatures can be as little as 1–2 °C.
Many fish change sex over the course of their lifespan, a phenomenon called sequential hermaphroditism. In clownfish, smaller fish are male, and the dominant and largest fish in a group becomes female. In many wrasses the opposite is true—most fish are initially female and become male when they reach a certain size. Sequential hermaphrodites may produce both types of gametes over the course of their lifetime, but at any given point they are either female or male.
In some ferns the default sex is hermaphrodite, but ferns which grow in soil that has previously supported hermaphrodites are influenced by residual hormones to instead develop as male.
Sexual dimorphism.
Many animals and some plants have differences between the male and female sexes in size and appearance, a phenomenon called sexual dimorphism. Sex differences in humans include, generally, a larger size and more body hair in men; women have breasts, wider hips, and a higher body fat percentage. In other species, the differences may be more extreme, such as differences in coloration or bodyweight. In humans, biological sex is determined by five factors present at birth: the presence or absence of a Y chromosome, the type of gonads, the sex hormones, the internal reproductive anatomy (such as the uterus in females), and the external genitalia.
Sexual dimorphisms in animals are often associated with sexual selection – the competition between individuals of one sex to mate with the opposite sex. Antlers in male deer, for example, are used in combat between males to win reproductive access to female deer. In many cases the male of a species is larger than the female. Mammal species with extreme sexual size dimorphism tend to have highly polygynous mating systems—presumably due to selection for success in competition with other males—such as the elephant seals. Other examples demonstrate that it is the preference of females that drive sexual dimorphism, such as in the case of the stalk-eyed fly.
Other animals, including most insects and many fish, have larger females. This may be associated with the cost of producing egg cells, which requires more nutrition than producing sperm—larger females are able to produce more eggs. For example, female southern black widow spiders are typically twice as long as the males. Occasionally this dimorphism is extreme, with males reduced to living as parasites dependent on the female, such as in the anglerfish. Some plant species also exhibit dimorphism in which the females are significantly larger than the males, such as in the moss "Dicranum" and the liverwort "Sphaerocarpos". There is some evidence that, in these genera, the dimorphism may be tied to a sex chromosome, or to chemical signalling from females.
In birds, males often have a more colourful appearance and may have features (like the long tail of male peacocks) that would seem to put the organism at a disadvantage (e.g. bright colors would seem to make a bird more visible to predators). One proposed explanation for this is the handicap principle. This hypothesis says that, by demonstrating he can survive with such handicaps, the male is advertising his genetic fitness to females—traits that will benefit daughters as well, who will not be encumbered with such handicaps.
Further reading.
</dl>

</doc>
<doc id="26808" url="http://en.wikipedia.org/wiki?curid=26808" title="Star">
Star

A star is a luminous sphere of plasma held together by its own gravity. The nearest star to Earth is the Sun. Other stars, mostly in the Milky Way, are visible from Earth during the night, appearing as a multitude of fixed luminous points in the sky due to their immense distance from Earth. Historically, the most prominent stars were grouped into constellations and asterisms, and the brightest stars gained proper names. Extensive catalogues of stars have been assembled by astronomers, which provide standardized star designations.
For at least a portion of its life, a star shines due to thermonuclear fusion of hydrogen into helium in its core, releasing energy that traverses the star's interior and then radiates into outer space. Once the hydrogen in the core of a star is nearly exhausted, almost all naturally occurring elements heavier than helium are created by stellar nucleosynthesis during the star's lifetime and, for some stars, by supernova nucleosynthesis when it explodes. Near the end of its life, a star can also contain degenerate matter. Astronomers can determine the mass, age, metallicity (chemical composition), and many other properties of a star by observing its motion through space, luminosity, and spectrum respectively. The total mass of a star is the principal determinant of its evolution and eventual fate. Other characteristics of a star, including diameter and temperature, change over its life, while the star's environment affects its rotation and movement. A plot of the temperature of many stars against their luminosities, known as a Hertzsprung–Russell diagram (H–R diagram), allows the age and evolutionary state of a star to be determined.
A star's life begins with the gravitational collapse of a gaseous nebula of material composed primarily of hydrogen, along with helium and trace amounts of heavier elements. Once the stellar core is sufficiently dense, hydrogen becomes steadily converted into helium through nuclear fusion, releasing energy in the process. The remainder of the star's interior carries energy away from the core through a combination of radiative and convective processes. The star's internal pressure prevents it from collapsing further under its own gravity. Once the hydrogen fuel at the core is exhausted, a star with at least 0.4 times the mass of the Sun expands to become a red giant, in some cases fusing heavier elements at the core or in shells around the core. The star then evolves into a degenerate form, recycling a portion of its matter into the interstellar environment, where it will contribute to the formation of a new generation of stars with a higher proportion of heavy elements. Meanwhile, the core becomes a stellar remnant: a white dwarf, a neutron star, or (if it is sufficiently massive) a black hole.
Binary and multi-star systems consist of two or more stars that are gravitationally bound, and generally move around each other in stable orbits. When two such stars have a relatively close orbit, their gravitational interaction can have a significant impact on their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy.
Observation history.
Historically, stars have been important to civilizations throughout the world. They have been part of religious practices and used for celestial navigation and orientation. Many ancient astronomers believed that stars were permanently affixed to a heavenly sphere, and that they were immutable. By convention, astronomers grouped stars into constellations and used them to track the motions of the planets and the inferred position of the Sun. The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices. The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth's rotational axis relative to its local star, the Sun.
The oldest accurately dated star chart appeared in ancient Egyptian astronomy in 1534 BC. The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the late 2nd millennium BC, during the Kassite Period ("ca." 1531–1155 BC).
The first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis. The star catalog of Hipparchus (2nd century BC) included 1020 stars and was used to assemble Ptolemy's star catalogue. Hipparchus is known for the discovery of the first recorded "nova" (new star). Many of the constellations and star names in use today derive from Greek astronomy.
In spite of the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear. In 185 AD, they were the first to observe and write about a supernova, now known as the SN 185. The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers. The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers.
Medieval Islamic astronomers gave Arabic names to many stars that are still used today, and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory research institutes, mainly for the purpose of producing "Zij" star catalogues. Among these, the "Book of Fixed Stars" (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi's Clusters) and galaxies (including the Andromeda Galaxy). According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and also gave the latitudes of various stars during a lunar eclipse in 1019.
According to Josep Puig, the Andalusian astronomer Ibn Bajjah proposed that the Milky Way was made up of many stars which almost touched one another and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars on 500 AH (1106/1107 AD) as evidence. 
Early European astronomers such as Tycho Brahe identified new stars in the night sky (later termed "novae"), suggesting that the heavens were not immutable. In 1584 Giordano Bruno suggested that the stars were like the Sun, and may have other planets, possibly even Earth-like, in orbit around them, an idea that had been suggested earlier by the ancient Greek philosophers, Democritus and Epicurus, and by medieval Islamic cosmologists such as Fakhr al-Din al-Razi. By the following century, the idea of the stars being the same as the Sun was reaching a consensus among astronomers. To explain why these stars exerted no net gravitational pull on the Solar System, Isaac Newton suggested that the stars were equally distributed in every direction, an idea prompted by the theologian Richard Bentley.
The Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby "fixed" stars, demonstrating that they had changed positions from the time of the ancient Greek astronomers Ptolemy and Hipparchus.
William Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he performed a series of gauges in 600 directions, and counted the stars observed along each line of sight. From this he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is also noted for his discovery that some stars do not merely lie along the same line of sight, but are also physical companions that form binary star systems.
The science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in a stellar spectra due to the absorption of specific frequencies by the atmosphere. In 1865 Secchi began classifying stars into spectral types. However, the modern version of the stellar classification scheme was developed by Annie J. Cannon during the 1900s.
The first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius, and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as William Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of the orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827.
The twentieth century saw increasingly rapid advances in the scientific study of stars. The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star, and hence its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed very precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope.
Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.
With the exception of supernovae, individual stars have primarily been observed in our Local Group of galaxies, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for our
galaxy). But some stars have been observed in the M100 galaxy of the Virgo Cluster, about 100 million light years from the Earth. In the Local Supercluster it is possible to see star clusters, and current telescopes could in principle observe faint individual stars in the Local Cluster (see Cepheids). However, outside the Local Supercluster of galaxies, neither individual stars nor clusters of stars have been observed. The only exception is a faint image of a large star cluster containing hundreds of thousands of stars located at a distance of one billion light years—ten times further than the most distant star cluster previously observed.
Designations.
The concept of the constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were also given names, particularly with Arabic or Latin designations.
As well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were also Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)
Circa 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star's right ascension was invented and added to John Flamsteed's star catalogue in his book "Historia coelestis Britannica" (the 1712 edition), whereby this numbering system came to be called "Flamsteed designation" or "Flamsteed numbering".
The only internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). A number of private companies sell names of stars, which the British Library calls an unregulated commercial enterprise. However, the IAU has disassociated itself from this commercial practice, and these names are neither recognized by the IAU nor used by them. One such star naming company is the International Star Registry, which, during the 1980s, was accused of deceptive practice for making it appear that the assigned name was official. This now-discontinued ISR practice was informally labeled a scam and a fraud, and the New York City Department of Consumer Affairs issued a violation against ISR for engaging in a deceptive trade practice.
Units of measurement.
Although stellar parameters can be expressed in SI units or CGS units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun:
Large lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit (AU)—approximately the mean distance between the Earth and the Sun (150 million km or 93 million miles).
Formation and evolution.
Stars form within extended regions of higher density in the interstellar medium, although the density is still lower than the inside of a vacuum chamber. These regions - known as "molecular clouds" - consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. As massive stars form from molecular clouds, they powerfully illuminate those clouds. They also ionize the hydrogen, creating an H II region.
All stars spend the majority of their existence as "main sequence stars", fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosity and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:
Protostar formation.
The formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density - often triggered by shock-waves from nearby supernovae (massive stellar explosions), the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). Once a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.
As the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre–main sequence stars are often surrounded by a protoplanetary disk and powered mainly by the release of gravitational energy. The period of gravitational contraction lasts about 10 to 15 million years.
Early stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.
These jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.
Early in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.
Main sequence.
Stars spend about 90% of their existence fusing hydrogen into helium in high-temperature and high-pressure reactions near the core. Such stars are said to be on the main sequence and are called dwarf stars. Starting at zero-age main sequence, the proportion of helium in a star's core will steadily increase, the rate of nuclear fusion at the core will slowly increase, as will the star's temperature and luminosity.
The Sun, for example, is estimated to have increased in luminosity by about 40% since it reached the main sequence 4.6 billion (4.6 × 109) years ago.
Every star generates a stellar wind of particles that causes a continual outflow of gas into space. For most stars, the mass lost is negligible. The Sun loses 10−14 M☉ every year, or about 0.01% of its total mass over its entire lifespan. However, very massive stars can lose 10−7 to 10−5 M☉ each year, significantly affecting their evolution. Stars that begin with more than 50 M☉ can lose over half their total mass while on the main sequence.
The duration that a star spends on the main sequence depends primarily on the amount of fuel it has to fuse and the rate at which it fuses that fuel, i.e. its initial mass and its luminosity. For the Sun, its life is estimated to be about 10 billion (1010) years. Massive stars consume their fuel very rapidly and are short-lived. Low mass stars consume their fuel very slowly. Stars less massive than 0.25 M☉, called red dwarfs, are able to fuse nearly all of their mass as fuel while stars of about 1 M☉ can only use about 10% of their mass as fuel. The combination of their slow fuel-consumption and relatively large usable fuel supply allows about 0.25 M☉ stars to last for about one trillion (1012) years according to stellar-evolution calculations, while the least-massive hydrogen-fusing stars (0.08 M☉) will last for about 12 trillion years. At the end of their lives, red dwarfs simply become dimmer and dimmer. However, since the lifespan of such stars is greater than the current age of the universe (13.8 billion years), no stars under about 0.85 M☉ are expected to have moved off the main sequence.
Besides mass, the elements heavier than helium can play a significant role in the evolution of stars. Astronomers consider all elements heavier than helium "metals", and call the chemical concentration of these elements the metallicity. The metallicity can influence the duration that a star will burn its fuel, control the formation of magnetic fields and modify the strength of the stellar wind. Older, population II stars have substantially less metallicity than the younger, population I stars due to the composition of the molecular clouds from which they formed. Over time these clouds become increasingly enriched in heavier elements as older stars die and shed portions of their atmospheres.
Post–main sequence.
As stars of at least 0.4 M☉ exhaust their supply of hydrogen at their core, their outer layers expand greatly and cool to form a red giant. In about 5 billion years, when the Sun enters this phase, it will expand to a maximum radius of roughly 1 AU, 250 times its present size. As a giant, the Sun will lose roughly 30% of its current mass.
In a red giant of up to 2.25 M☉, hydrogen fusion proceeds in a shell surrounding the core. Eventually the core is compressed enough to start helium fusion, and the star now gradually shrinks in radius and its surface temperature increases. For larger stars, the core region transitions directly from fusing hydrogen to fusing helium.
After the star has consumed the helium at the core, fusion continues in a shell around a hot core of carbon and oxygen. The star then follows an evolutionary path that parallels the original red giant phase, but at a higher surface-temperature.
Massive stars.
During their helium-burning phase, very high-mass stars with more than nine solar masses expand to form red supergiants. Once this fuel is exhausted at the core, they continue to fuse elements heavier than helium.
The core contracts until the temperature and pressure suffice to fuse carbon (see carbon burning process). This process continues, with the successive stages being fueled by neon (see neon burning process), oxygen (see oxygen burning process), and silicon (see silicon burning process). Near the end of the star's life, fusion continues along a series of onion-layer shells within the star. Each shell fuses a different element, with the outermost shell fusing hydrogen; the next shell fusing helium, and so forth.
The final stage occurs when a massive star begins producing iron. Since iron nuclei are more tightly bound than any heavier nuclei, any fusion beyond iron does not produce a net release of energy—the process would, on the contrary, consume energy. Likewise, since they are more tightly bound than all lighter nuclei, energy cannot be released by fission. In relatively old, very massive stars, a large core of inert iron will accumulate in the center of the star. The heavier elements in these stars can work their way to the surface, forming evolved objects known as Wolf-Rayet stars that have a dense stellar wind which sheds the outer atmosphere.
Collapse.
As a star's core shrinks, the intensity of radiation from that surface increases, creating such radiation pressure on the outer shell of gas that it will push those layers away, forming a planetary nebula. If what remains after the outer atmosphere has been shed is less than 1.4 M☉, it shrinks to a relatively tiny object about the size of Earth, known as a white dwarf. White dwarfs lack the mass for further gravitational compression to take place. The electron-degenerate matter inside a white dwarf is no longer a plasma, even though stars are generally referred to as being spheres of plasma. Eventually, white dwarfs fade into black dwarfs over a very long period of time.
In larger stars, fusion continues until the iron core has grown so large (more than 1.4 M☉) that it can no longer support its own mass. This core will suddenly collapse as its electrons are driven into its protons, forming neutrons, neutrinos and gamma rays in a burst of electron capture and inverse beta decay. The shockwave formed by this sudden collapse causes the rest of the star to explode in a supernova. Supernovae become so bright that they may briefly outshine the star's entire home galaxy. When they occur within the Milky Way, supernovae have historically been observed by naked-eye observers as "new stars" where none seemingly existed before.
Supernova explosions blow away most of their stars' matter (forming nebulae such as the Crab Nebula). There remains a neutron star (which sometimes manifests itself as a pulsar or X-ray burster) or, in the case of the largest stars (large enough to leave a remnant greater than roughly 4 M☉), a black hole. In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core. Within a black hole the matter is in a state that is not currently understood.
The blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium.
Distribution.
In addition to isolated stars, a multi-star system can consist of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars are also found. For reasons of orbital stability, such multi-star systems are often organized into hierarchical sets of binary stars. Larger groups called star clusters also exist. These range from loose stellar associations with only a few stars, up to enormous globular clusters with hundreds of thousands of stars.
It has been a long-held assumption that the majority of stars occur in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, where 80% of the stars are believed to be part of multiple-star systems. However the proportion of single star systems increases for smaller stars, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, most stars in the Milky Way are likely single from birth.
Stars are not spread uniformly across the universe, but are normally grouped into galaxies along with interstellar gas and dust. A typical galaxy contains hundreds of billions of stars, and there are more than 100 billion (1011) galaxies in the observable universe. A 2010 star count estimate was 300 sextillion (3 × 1023) in the observable universe.
While it is often believed that stars only exist within galaxies, intergalactic stars have been discovered.
The nearest star to the Earth, apart from the Sun, is Proxima Centauri, which is 39.9 trillion kilometres, or 4.2 light-years away. Travelling at the orbital speed of the Space Shuttle (8 kilometres per second—almost 30,000 kilometres per hour), it would take about 150,000 years to get there. Distances like this are typical inside galactic discs, including in the vicinity of the solar system. Stars can be much closer to each other in the centres of galaxies and in globular clusters, or much farther apart in galactic halos.
Due to the relatively vast distances between stars outside the galactic nucleus, collisions between stars are thought to be rare. In denser regions such as the core of globular clusters or the galactic center, collisions can be more common. Such collisions can produce what are known as blue stragglers. These abnormal stars have a higher surface temperature than the other main sequence stars with the same luminosity in the cluster.
Characteristics.
Almost everything about a star is determined by its initial mass, including essential characteristics such as luminosity and size, as well as its evolution, lifespan, and eventual fate.
Age.
Most stars are between 1 billion and 10 billion years old. Some stars may even be close to 13.8 billion years old—the observed age of the universe. The oldest star yet discovered, HD 140283, nicknamed Methuselah star, is an estimated 14.46 ± 0.8 billion years old. (Due to the uncertainty in the value, this age for the star does not conflict with the age of the Universe, determined by the Planck satellite as 13.798 ± 0.037.)
The more massive the star, the shorter its lifespan, primarily because massive stars have greater pressure on their cores, causing them to burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last tens to hundreds of billions of years.
Chemical composition.
“From a chemist’s point of view, the surface or interior of a star…is boring—there are no molecules there.”--Roald Hoffmann
When stars form in the present Milky Way galaxy they are composed of about 71% hydrogen and 27% helium, as measured by mass, with a small fraction of heavier elements. Typically the portion of heavy elements is measured in terms of the iron content of the stellar atmosphere, as iron is a common element and its absorption lines are relatively easy to measure. The portion of heavier elements may be an indicator of the likelihood that the star has a planetary system.
The star with the lowest iron content ever measured is the dwarf HE1327-2326, with only 1/200,000th the iron content of the Sun. By contrast, the super-metal-rich star μ Leonis has nearly double the abundance of iron as the Sun, while the planet-bearing star 14 Herculis has nearly triple the iron. There also exist chemically peculiar stars that show unusual abundances of certain elements in their spectrum; especially chromium and rare earth elements.
Diameter.
Due to their great distance from the Earth, all stars except the Sun appear to the unaided eye as shining points in the night sky that twinkle because of the effect of the Earth's atmosphere. The Sun is also a star, but it is close enough to the Earth to appear as a disk instead, and to provide daylight. Other than the Sun, the star with the largest apparent size is R Doradus, with an angular diameter of only 0.057 arcseconds.
The disks of most stars are much too small in angular size to be observed with current ground-based optical telescopes, and so interferometer telescopes are required to produce images of these objects. Another technique for measuring the angular size of stars is through occultation. By precisely measuring the drop in brightness of a star as it is occulted by the Moon (or the rise in brightness when it reappears), the star's angular diameter can be computed.
Stars range in size from neutron stars, which vary anywhere from 20 to 40 km in diameter, to supergiants like Betelgeuse in the Orion constellation, which has a diameter approximately 1,070 times that of the Sun—about 1490171880 km. Betelgeuse, however, has a much lower density than the Sun.
Kinematics.
The motion of a star relative to the Sun can provide useful information about the origin and age of a star, as well as the structure and evolution of the surrounding galaxy. The components of motion of a star consist of the radial velocity toward or away from the Sun, and the traverse angular movement, which is called its proper motion.
Radial velocity is measured by the doppler shift of the star's spectral lines, and is given in units of km/s. The proper motion of a star is determined by precise astrometric measurements in units of milli-arc seconds (mas) per year. By determining the parallax of a star, the proper motion can then be converted into units of velocity. Stars with high rates of proper motion are likely to be relatively close to the Sun, making them good candidates for parallax measurements.
Once both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has also led to the identification of stellar associations. These are most likely groups of stars that share a common point of origin in giant molecular clouds.
Magnetic field.
The magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, generating magnetic fields that extend throughout the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic fields that reach out into the corona from active regions. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.
Young, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During
the Maunder minimum, for example, the Sun underwent a
70-year period with almost no sunspot activity.
Mass.
One of the most massive stars known is Eta Carinae, which, with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. A study of the Arches cluster suggests that 150 M☉ is the upper limit for stars in the current era of the universe. The reason for this limit is not precisely known, but it is partially due to the Eddington luminosity which defines the maximum amount of luminosity that can pass through the atmosphere of a star without ejecting the gases into space. However, a star named R136a1 in the Large Magellanic Cloud, RMC 136a star cluster has been measured at 256 M☉, which puts this limit into question. A study determined that stars larger than 150 M☉ in R136 were created through the collision and merger of massive stars in close binary systems, providing a way to sidestep the 150 M☉ limit.
The first stars to form after the Big Bang may have been larger, up to 300 M☉ or more, due to the complete absence of elements heavier than lithium in their composition. This generation of supermassive, population III stars is long extinct, however, and currently only theoretical.
With a mass only 80 times that of Jupiter (MJ), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For stars with similar metallicity to the Sun, the theoretical minimum mass the star can have, and still undergo fusion at the core, is estimated to be about 75 MJ. When the metallicity is very low, however, a recent study of the faintest stars found that the minimum star size seems to be about 8.3% of the solar mass, or about 87 MJ. Smaller bodies are called brown dwarfs, which occupy a poorly defined grey area between stars and gas giants.
The combination of the radius and the mass of a star determines the surface gravity. Giant stars have a much lower surface gravity than main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.
Rotation.
The rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking the rotation rate of starspots. Young stars can have a rapid rate of rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial rotation velocity of about 225 km/s or greater, causing its equator to be slung outward and giving it an equatorial diameter that is more than 50% larger than the distance between the poles. This rate of rotation is just below the critical velocity of 300 km/s where the star would break apart. By contrast, the Sun only rotates once every 25 – 35 days, with an equatorial velocity of 1.994 km/s. The star's magnetic field and the stellar wind serve to slow a main sequence star's rate of rotation by a significant amount as it evolves on the main sequence.
Degenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.
Temperature.
The surface temperature of a main sequence star is determined by the rate of energy production at the core and by its radius, and is often estimated from the star's color index. The temperature is normally given as the effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. Note that the effective temperature is only a representative value, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.
The stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).
Massive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they also have a high luminosity due to their large exterior surface area.
Radiation.
The energy produced by stars, as a product of nuclear fusion, radiates into space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which streams from the outer layers as free protons, and electrically charged alpha, and beta particles. Although almost massless there also exists a steady stream of neutrinos emanating from the star's core.
The production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.
The color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides visible light, stars also emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.
Using the stellar spectrum, astronomers can also determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is known, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can also estimate the age of the star.
Luminosity.
The luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by the radius and the surface temperature. However, many stars do not radiate a uniform flux (the amount of energy radiated per unit area) across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux at its poles than along its equator.
Surface patches with a lower temperature and luminosity than average are known as starspots. Small, "dwarf" stars such as our Sun generally have essentially featureless disks with only small starspots. Larger, "giant" stars have much larger, more obvious starspots, and they also exhibit strong stellar limb darkening. That is, the brightness decreases towards the edge of the stellar disk. Red dwarf flare stars such as UV Ceti may also possess prominent starspot features.
Magnitude.
The apparent brightness of a star is expressed in terms of its apparent magnitude, which is the brightness of a star and is a function of the star's luminosity, distance from Earth, and the altering of the star's light as it passes through Earth's atmosphere. Intrinsic or absolute magnitude is directly related to a star's luminosity and is what the apparent magnitude a star would be if the distance between the Earth and the star were 10 parsecs (32.6 light-years).
Both the apparent and absolute magnitude scales are logarithmic units: one whole number difference in magnitude is equal to a brightness variation of about 2.5 times (the 5th root of 100 or approximately 2.512). This means that a first magnitude star (+1.00) is about 2.5 times brighter than a second magnitude (+2.00) star, and approximately 100 times brighter than a sixth magnitude star (+6.00). The faintest stars visible to the naked eye under good seeing conditions are about magnitude +6.
On both apparent and absolute magnitude scales, the smaller the magnitude number, the brighter the star; the larger the magnitude number, the fainter. The brightest stars, on either scale, have negative magnitude numbers. The variation in brightness (Δ"L") between two stars is calculated by subtracting the magnitude number of the brighter star ("m"b) from the magnitude number of the fainter star ("m"f), then using the difference as an exponent for the base number 2.512; that is to say:
Relative to both luminosity and distance from Earth, a star's absolute magnitude ("M") and apparent magnitude ("m") are not equivalent; for example, the bright star Sirius has an apparent magnitude of −1.44, but it has an absolute magnitude of +1.41.
The Sun has an apparent magnitude of −26.7, but its absolute magnitude is only +4.83. Sirius, the brightest star in the night sky as seen from Earth, is approximately 23 times more luminous than the Sun, while Canopus, the second brightest star in the night sky with an absolute magnitude of −5.53, is approximately 14,000 times more luminous than the Sun. Despite Canopus being vastly more luminous than Sirius, however, Sirius appears brighter than Canopus. This is because Sirius is merely 8.6 light-years from the Earth, while Canopus is much farther away at a distance of 310 light-years.
As of 2006, the star with the highest known absolute magnitude is LBV 1806-20, with a magnitude of −14.2. This star is at least 5,000,000 times more luminous than the Sun. The least luminous stars that are currently known are located in the NGC 6397 cluster. The faintest red dwarfs in the cluster were magnitude 26, while a 28th magnitude white dwarf was also discovered. These faint stars are so dim that their light is as bright as a birthday candle on the Moon when viewed from the Earth.
Classification.
The current stellar classification system originated in the early 20th century, when stars were classified from "A" to "Q" based on the strength of the hydrogen line. It was not known at the time that the major influence on the line strength was temperature; the hydrogen line strength reaches a peak at over 9000 K, and is weaker at both hotter and cooler temperatures. When the classifications were reordered by temperature, it more closely resembled the modern scheme.
Stars are given a single-letter classification according to their spectra, ranging from type "O", which are very hot, to "M", which are so cool that molecules may form in their atmospheres. The main classifications in order of decreasing surface temperature are: "O, B, A, F, G, K", and "M". A variety of rare spectral types have special classifications. The most common of these are types "L" and "T", which classify the coldest low-mass stars and brown dwarfs. Each letter has 10 sub-divisions, numbered from 0 to 9, in order of decreasing temperature. However, this system breaks down at extreme high temperatures: class "O0" and "O1" stars may not exist.
In addition, stars may be classified by the luminosity effects found in their spectral lines, which correspond to their spatial size and is determined by the surface gravity. These range from "0" (hypergiants) through "III" (giants) to "V" (main sequence dwarfs); some authors add "VII" (white dwarfs). Most stars belong to the main sequence, which consists of ordinary hydrogen-burning stars. These fall along a narrow, diagonal band when graphed according to their absolute magnitude and spectral type. The Sun is a main sequence "G2V" yellow dwarf of intermediate temperature and ordinary size.
Additional nomenclature, in the form of lower-case letters, can follow the spectral type to indicate peculiar features of the spectrum. For example, an ""e" can indicate the presence of emission lines; "m" represents unusually strong levels of metals, and "var"" can mean variations in the spectral type.
White dwarf stars have their own class that begins with the letter "D". This is further sub-divided into the classes "DA", "DB", "DC", "DO", "DZ", and "DQ", depending on the types of prominent lines found in the spectrum. This is followed by a numerical value that indicates the temperature index.
Variable stars.
Variable stars have periodic or random changes in luminosity because of intrinsic or extrinsic properties. Of the intrinsically variable stars, the primary types can be subdivided into three principal groups.
During their stellar evolution, some stars pass through phases where they can become pulsating variables. Pulsating variable stars vary in radius and luminosity over time, expanding and contracting with periods ranging from minutes to years, depending on the size of the star. This category includes Cepheid and cepheid-like stars, and long-period variables such as Mira.
Eruptive variables are stars that experience sudden increases in luminosity because of flares or mass ejection events. This group includes protostars, Wolf-Rayet stars, and Flare stars, as well as giant and supergiant stars.
Cataclysmic or explosive variable stars are those that undergo a dramatic change in their properties. This group includes novae and supernovae. A binary star system that includes a nearby white dwarf can produce certain types of these spectacular stellar explosions, including the nova and a Type 1a supernova. The explosion is created when the white dwarf accretes hydrogen from the companion star, building up mass until the hydrogen undergoes fusion. Some novae are also recurrent, having periodic outbursts of moderate amplitude.
Stars can also vary in luminosity because of extrinsic factors, such as eclipsing binaries, as well as rotating stars that produce extreme starspots. A notable example of an eclipsing binary is Algol, which regularly varies in magnitude from 2.3 to 3.5 over a period of 2.87 days.
Structure.
The interior of a stable star is in a state of hydrostatic equilibrium: the forces on any small volume almost exactly counterbalance each other. The balanced forces are inward gravitational force and an outward force due to the pressure gradient within the star. The pressure gradient is established by the temperature gradient of the plasma; the outer part of the star is cooler than the core. The temperature at the core of a main sequence or giant star is at least on the order of 107 K. The resulting temperature and pressure at the hydrogen-burning core of a main sequence star are sufficient for nuclear fusion to occur and for sufficient energy to be produced to prevent further collapse of the star.
As atomic nuclei are fused in the core, they emit energy in the form of gamma rays. These photons interact with the surrounding plasma, adding to the thermal energy at the core. Stars on the main sequence convert hydrogen into helium, creating a slowly but steadily increasing proportion of helium in the core. Eventually the helium content becomes predominant and energy production ceases at the core. Instead, for stars of more than 0.4 M☉, fusion occurs in a slowly expanding shell around the degenerate helium core.
In addition to hydrostatic equilibrium, the interior of a stable star will also maintain an energy balance of thermal equilibrium. There is a radial temperature gradient throughout the interior that results in a flux of energy flowing toward the exterior. The outgoing flux of energy leaving any layer within the star will exactly match the incoming flux from below.
The radiation zone is the region within the stellar interior where radiative transfer is sufficiently efficient to maintain the flux of energy. In this region the plasma will not be perturbed and any mass motions will die out. If this is not the case, however, then the plasma becomes unstable and convection will occur, forming a convection zone. This can occur, for example, in regions where very high energy fluxes occur, such as near the core or in areas with high opacity as in the outer envelope.
The occurrence of convection in the outer envelope of a main sequence star depends on the mass. Stars with several times the mass of the Sun have a convection zone deep within the interior and a radiative zone in the outer layers. Smaller stars such as the Sun are just the opposite, with the convective zone located in the outer layers. Red dwarf stars with less than 0.4 M☉ are convective throughout, which prevents the accumulation of a helium core. For most stars the convective zones will also vary over time as the star ages and the constitution of the interior is modified.
The portion of a star that is visible to an observer is called the photosphere. This is the layer at which the plasma of the star becomes transparent to photons of light. From here, the energy generated at the core becomes free to propagate out into space. It is within the photosphere that sun spots, or regions of lower than average temperature, appear.
Above the level of the photosphere is the stellar atmosphere. In a main sequence star such as the Sun, the lowest level of the atmosphere is the thin chromosphere region, where spicules appear and stellar flares begin. This is surrounded by a transition region, where the temperature rapidly increases within a distance of only 100 km. Beyond this is the corona, a volume of super-heated plasma that can extend outward to several million kilometres. The existence of a corona appears to be dependent on a convective zone in the outer layers of the star. Despite its high temperature, the corona emits very little light. The corona region of the Sun is normally only visible during a solar eclipse.
From the corona, a stellar wind of plasma particles expands outward from the star, propagating until it interacts with the interstellar medium. For the Sun, the influence of its solar wind extends throughout the bubble-shaped region of the heliosphere.
Nuclear fusion reaction pathways.
A variety of different nuclear fusion reactions take place inside the cores of stars, depending upon their mass and composition, as part of stellar nucleosynthesis. The net mass of the fused atomic nuclei is smaller than the sum of the constituents. This lost mass is released as electromagnetic energy, according to the mass-energy equivalence relationship "E" = "mc"2.
The hydrogen fusion process is temperature-sensitive, so a moderate increase in the core temperature will result in a significant increase in the fusion rate. As a result the core temperature of main sequence stars only varies from 4 million kelvin for a small M-class star to 40 million kelvin for a massive O-class star.
In the Sun, with a 10-million-kelvin core, hydrogen fuses to form helium in the proton-proton chain reaction:
These reactions result in the overall reaction:
where e+ is a positron, γ is a gamma ray photon, νe is a neutrino, and H and He are isotopes of hydrogen and helium, respectively. The energy released by this reaction is in millions of electron volts, which is actually only a tiny amount of energy. However enormous numbers of these reactions occur constantly, producing all the energy necessary to sustain the star's radiation output.
In more massive stars, helium is produced in a cycle of reactions catalyzed by carbon—the carbon-nitrogen-oxygen cycle.
In evolved stars with cores at 100 million kelvin and masses between 0.5 and 10 M☉, helium can be transformed into carbon in the triple-alpha process that uses the intermediate element beryllium:
For an overall reaction of:
In massive stars, heavier elements can also be burned in a contracting core through the neon burning process and oxygen burning process. The final stage in the stellar nucleosynthesis process is the silicon burning process that results in the production of the stable isotope iron-56. Fusion can not proceed any further except through an endothermic process, and so further energy can only be produced through gravitational collapse.
The example below shows the amount of time required for a star of 20 M☉ to consume all of its nuclear fuel. As an O-class main sequence star, it would be 8 times the solar radius and 62,000 times the Sun's luminosity.

</doc>
<doc id="26809" url="http://en.wikipedia.org/wiki?curid=26809" title="StarCraft">
StarCraft

StarCraft is a military science fiction real-time strategy video game developed and published by Blizzard Entertainment and released for Microsoft Windows on March 31, 1998. The game later spawned a franchise, and is the first game of the "StarCraft" series. A Mac OS version was released in 1999, and a Nintendo 64 adaptation co-developed with Mass Media was released on June 13, 2000. Work on the game started shortly after ""‍ '​s release in 1995. "StarCraft" debuted at the 1996 E3, where it was unfavorably compared to "Warcraft II". As a result, the project was entirely overhauled and then showcased to public in early 1997, receiving a far more positive response.
Set in a fictitious timeline during the Earth's 25th century, the game revolves around three species fighting for dominance in a distant part of the Milky Way galaxy known as the Koprulu Sector: the Terrans, humans exiled from Earth skilled at adapting to any situation; the Zerg, a race of insectoid aliens in pursuit of genetic perfection, obsessed with assimilating other races; and the Protoss, a humanoid species with advanced technology and psionic abilities, attempting to preserve their civilization and strict philosophical way of living from the Zerg.
Many of the industry's journalists have praised "StarCraft" as one of the best and most important video games of all time, and for having raised the bar for developing real-time strategy games. With more than 11 million copies sold worldwide as of February 2009, "StarCraft" is one of the best-selling games for the personal computer. The game has been praised for pioneering the use of unique factions in real-time strategy gameplay and for a compelling story. "StarCraft"‍ '​s multiplayer is particularly popular in South Korea, where players and teams participate in , earn sponsorships, and compete in televised tournaments. "StarCraft" has had its storyline adapted and expanded through a series of novels, the expansion pack ' and two authorized add-ons. Over 12 years later, a sequel, ', was released in July 2010.
Gameplay.
Blizzard Entertainment's use of three distinct races in "StarCraft" is widely credited with revolutionizing the real-time strategy genre. All units are unique to their respective races and while rough comparisons can be drawn between certain types of units in the technology tree, every unit performs differently and requires different tactics for a player to succeed.
The enigmatic Protoss have access to powerful units and machinery and advanced technologies such as energy shields and localized warp capabilities, powered by their psionic traits. However, their forces have lengthy and expensive manufacturing processes, encouraging players to follow a strategy of the quality of their units over the quantity. The insectoid Zerg possess entirely organic units and structures, which can be produced quickly and at a far cheaper cost to resources, but are accordingly weaker, relying on sheer numbers and speed to overwhelm enemies. The Terrans provide a middle ground between the other two races, providing units that are versatile and flexible. They have access to a range of more ballistic military technologies and machinery, such as tanks and nuclear weapons.
Although each race is unique in its composition, no race has an innate advantage over the other. Each species is balanced out so that while they have different strengths, powers, and abilities their overall strength is the same. The balance stays complete via infrequent patches (game updates) provided by Blizzard.
"StarCraft" features artificial intelligence which scales in difficulty, although the player cannot change the difficulty level in the single-player campaigns. Each campaign starts with enemy factions running easy AI modes, scaling through the course of the campaign to the hardest AI modes. In the level editor provided with the game, a designer has access to four levels of AI difficulties: "easy", "medium", "hard" and "insane", each setting differing in the units and technologies allowed to an AI faction and the extent of the AI's tactical and strategic planning. The single-player campaign consists of thirty missions, split into ten for each race.
Resource management.
Each race relies on two resources to sustain their game economies and to build their forces: minerals and vespene gas. Minerals are needed for all units and structures, and are obtained by using a worker unit to harvest the resource directly from mineral nodes scattered around the battlefield. Players require vespene gas to construct advanced units and buildings, and acquire it by constructing a gas extraction building on top of a geyser and using worker units to extract the gas from it. In addition, players need to regulate the supplies for their forces to ensure that they can construct the number of units they need. Although the nature of the supply differs between the races—Terrans use physical supplies held in depots, Protoss use a psionic power nexus and Zerg are regulated by the number of controlling overlord units present—the supply mechanic works in exactly the same way for each race (with different side effects for each race), allowing players to create new units when there are sufficient resources to sustain them.
Base construction.
Protoss and Zerg building construction is limited to specific locations: Protoss buildings need to be linked to a power grid while almost every Zerg structure must be placed on a carpet of biomass, called "creep", that is produced by certain structures. Terran buildings are far less limited, with certain primary base structures possessing the ability to take off and fly slowly to a new location. Terran buildings, however, require the worker unit to continue construction on the building until it is completed. Also, once a Terran building has taken a certain amount of damage, it will catch fire and eventually burn to the ground without further enemy action, though this can be prevented by repairs performed by a worker unit. The Protoss, by contrast, only require a worker unit to begin the process of transporting a building to the theater of operations via warp, and their buildings' shields (but not their structure) are regenerative. The Zerg worker unit physically transforms into the structure created, which is capable of slowly healing itself.
Multiplayer.
Multiplayer on "StarCraft" is powered through Blizzard Entertainment's Battle.net Internet service. Through this, a maximum of eight players can compete in a variety of game modes, including simply destroying all other players on a level (which may be competitive, as in Ladder play, or non-ranked, as in melee play), to king of the hill and capture the flag objective-based games. In addition, the game incorporates a variety of specialized scenarios for different types of game, such as simulating a football game, using the Terran hoverbike unit to conduct a bike race, or hosting a Zerg hunting competition. "StarCraft" is also one of the few games that include a spawn installation, which allows for limited multiplayer. It must be installed from a disc, and requires a product key to work just as the full version does. However, one product key can support up to eight spawned installations with access to Battle.net. Limitations of a spawned installation include the inability to play single-player missions, create multiplayer games or use the campaign editor. Newer releases of the game available through Battle.net or discs that include Windows Vista label don't support the spawn installation.
Synopsis.
Setting.
"StarCraft" takes place in a science fiction universe created by Chris Metzen and James Phinney for Blizzard Entertainment. According to the story presented in the game's manual, the overpopulation of Earth in the early 21st century has caused the international government to exile certain members of the human race, such as criminals, the cybernetically enhanced and genetic mutants to colonize the far reaches of the galaxy. An attempt to colonize a nearby solar system goes wrong, resulting in humanity's arrival in the Koprulu Sector. In the distant Koprulu Sector of the galaxy, the exiles form several governments, but quickly fall into conflict with each other. One government, the Confederacy of Man, eventually emerges as the strongest faction, but its oppressive nature and brutal methods of suppressing dissidents stir up major rebel opposition in the form of a terrorist group called the Sons of Korhal. Just prior to the beginning of the game, in December 2499, an alien race possessing advanced technology and psionic power, the Protoss, makes first contact with humanity by destroying a Confederate colony world without any prior warning. Soon after this, the Terrans discover that a second alien race, the insectoid Zerg, has been stealthily infesting the surface of several of the Terran colonies, and that the Protoss are destroying the planets to prevent the Zerg from spreading. With the Confederacy threatened by two alien races and internal rebellion, it begins to crumble.
Characters.
The player assumes the role of three nameless characters over the course of the game. In the first act, the player acts as the Confederate magistrate of an outlying colony world of Mar Sara, threatened by both the Zerg and the Protoss, and is forced through events to join the rebel Sons of Korhal under its leader Arcturus Mengsk. Mengsk's campaign is accompanied by Jim Raynor, a morally conscious law enforcement officer from Mar Sara, and Sarah Kerrigan, a psychic assassin and Mengsk's second-in-command. The second episode of the game sees the player as a cerebrate, a commander within the Zerg Swarm. The player is ruled over by the Zerg Overmind — the manifestation of the collective consciousness of the Swarm and the game's primary antagonist — and is given advice from other cerebrates of higher rank and status while accomplishing the objectives of the Swarm. In the final part of "StarCraft", the player is a newly appointed Executor within the Protoss military reporting to Aldaris, a representative of the Protoss government. Aldaris is at odds with Tassadar — the former occupant of the player's position — over his association with Zeratul, a member of a heretical group known as dark templar.
Plot.
The story of "StarCraft" is presented through its instruction manual, the briefings to each mission and conversations within the missions themselves, along with the use of cinematic cutscenes at key points. The game itself is split into three episodes, one for the player to command each race. In the first segment of the game, the player and Jim Raynor are attempting to control the colony of Mar Sara in the wake of the Zerg attacks on other Terran worlds. After the Confederacy arrests Raynor for destroying Confederate property, despite the fact that it had been infested by the Zerg, the player joins Arcturus Mengsk and the Sons of Korhal. Raynor, who is freed by Mengsk's troops, also joins and frequently accompanies the player on missions. Mengsk then begins to use Confederate technology captured on Mar Sara to lure the Zerg to Confederate installations and further his own goals. After forcing Confederate general Edmund Duke to join him, Mengsk sacrifices his own second-in-command, Sarah Kerrigan, to ensure the destruction of the Confederacy by luring the Zerg to the Confederate capital Tarsonis. Raynor is outraged by Mengsk's true aims of obtaining power at any cost and deserts, taking with him a small army of the former colonial militia of Mar Sara. Mengsk reorganizes what remains of the Terran population into the Terran Dominion, crowning himself as emperor.
The second campaign reveals that Kerrigan was not killed by the Zerg, but rather is captured and infested in an effort to incorporate her psionic traits into the Zerg gene pool. She emerges with far more psionic powers and physical strength, her DNA completely altered. Meanwhile, the Protoss commander Tassadar discovers that the Zerg's cerebrates cannot be killed by conventional means, but that they can be harmed by the powers wielded by the heretical dark templar. Tassadar allies himself with the dark templar prelate Zeratul, who assassinates Zasz, one of the Zerg's cerebrates in their hive clusters on Char. The cerebrate's death results in its forces running amok through the Zerg hives, but briefly links the minds of Zeratul and the Zerg Overmind, allowing the Overmind to finally learn the location of the Protoss homeworld Aiur, which the Overmind has been seeking for millennia. The main Zerg swarm promptly invades Aiur while Kerrigan is dispatched to deal with Tassadar and despite heavy Protoss resistance, the Overmind is able to embed itself into the crust of the planet.
The final episode of the game sees Aldaris and the Protoss government branding Tassadar a traitor and a heretic for conspiring with the dark templar. The player (later hinted to be Artanis) initially serves Aldaris in defending Aiur from the Zerg invasion, but while on a mission to arrest Tassadar, the player joins him instead. A Protoss civil war erupts, pitting Tassadar, Zeratul, and their allies against the Protoss establishment. The dark templar prove their worth when they use their energies to slay two more of the Zerg cerebrates on Aiur, and the Conclave reconciles with them. Aided by Raynor's forces—who sided with Tassadar back on Char—the Protoss break through the Overmind's weakened defenses and destroy the Overmind's outer shell, but take heavy casualties in the process. Tassadar channels his own psionic energies in combination with those of the dark templar through the hull of his command ship and crashes it into the Overmind, sacrificing himself in order to destroy it.
Development.
Blizzard Entertainment began development on "StarCraft" in 1995, shortly after the release of highly successful '. Using the "Tides of Darknesss game engine as a base, "StarCraft" made its debut at E3 1996. The version of the game displayed, assembled by the team's lead programmer Bob Fitch, received a rather weak response from the convention and was criticized by many for being ""Warcraft" in space." As a consequence the entire project was overhauled, bringing the focus onto creating three distinct species. Bill Roper, one of the game's producers, stated this would be a major departure from the "Warcraft" approach, comparing its two equal sides to those of chess and stating that "StarCraft" would allow players to "develop very unique strategies based on which species is being played, and will require players to think of different strategies to combat the other two species." In early 1997, the new version of "StarCraft" was unveiled, receiving a far more positive response.
However, the game was still marred by technical difficulties, so Bob Fitch completely redesigned the "Warcraft II" engine within two months to ensure that many of the features desired by the designers, such as the abilities for units to burrow and cloak, could be implemented. Later improvements to the game included pre-rendered sprites and backgrounds, constructed using 3D Studio Max. An isometric in-game view was also adopted, in contrast to "Warcraft II"‍ '​s 3/4s birdseye perspective. In addition, the game utilized high quality music, composed by Blizzard's resident composers, and professional voice actors were hired.
Despite the progress, "StarCraft" was slow to emerge. The continual delays inspired a group of "StarCraft" fans on the official forums who labeled themselves "Operation: Can't Wait Any Longer" to write a series of fictional stories in which the members of Operation CWAL attempted to retrieve the beta version of "StarCraft" from Blizzard's headquarters in Irvine, California. To pay homage to their presence on the forums and enthusiasm for the game, Blizzard Entertainment later incorporated the group's name into "StarCraft" as a cheat code to speed up the production of units and gave the group thanks in the game's credits. The game was released for Windows on March 31, 1998, with the Mac OS version following a year later in 1999. Development on a Nintendo 64 version, "StarCraft 64", began in 1999, converted from PC by Mass Media Interactive Entertainment—a subsidiary of THQ—and published by Nintendo. "StarCraft 64" was released on June 13, 2000 in the USA and Europe. It was also released in Australia on May 25, 2001.
Audio.
The musical score to "StarCraft" was composed by Blizzard Entertainment's in-house composers. Derek Duke and Glenn Stafford composed the tracks in the menus and the in-game music, while Jason Hayes composed the music used in the cinematic cut scenes. Tracy W. Bush provided additional support in composing. The musical score of the game was received well by reviewers, who have described it as "appropriately melodic and dark" and "impressive", with one reviewer noting that some of the music owed much of its inspiration to Jerry Goldsmith's score for the film "Alien". The first official game soundtrack, "StarCraft: Game Music Vol. 1", was released in 2000, comprising tracks from both "StarCraft" and "", as well as a sizable portion of remix tracks and music inspired by "StarCraft", created by several South Korean disc jockeys. The soundtrack was distributed by Net Vision Entertainment. In September 2008, Blizzard Entertainment announced that a second soundtrack, "StarCraft Original Soundtrack", had been released on iTunes. This soundtrack consisted entirely of the original music from "StarCraft" and "Brood War", both from in-game themes to music used in the cinematic cut scenes. Of special interest, the tracks were remastered at a 44.1 kHz sampling frequency. This is notable as the actual .wav files in the game released in 1998 were only of 22 kHz sampling frequency.
Expansions and versions.
Computer expansions.
Shortly before the release of "StarCraft", Blizzard Entertainment developed a shareware game demo campaign, entitled "Loomings". Comprising three missions and a tutorial, the campaign acts as a prequel to the events of "StarCraft", taking place on a Confederate colony in the process of being overrun by the Zerg. In October 1999, Blizzard Entertainment made the prequel available for the full game as a custom map campaign, adding two extra missions and hosting it on Battle.net. In addition, the full release of "StarCraft" included a secondary campaign entitled "Enslavers". Consisting of five missions played as both the Terrans and the Protoss, "Enslavers" is set in the second campaign in "StarCraft" and follows the story of a Terran smuggler who manages to take control of a Zerg cerebrate and is pursued by both the Protoss and Terran Dominion. "Enslavers" acts as an exemplar single-player campaign for the game's level editor, highlighting how to use the features of the program.
"StarCraft"‍ '​s first expansion, Insurrection, was released for Windows on July 31, 1998. The expansion was developed by Aztech New Media and authorized by Blizzard Entertainment. Its story focused on a separate Confederate colony alluded to in the manual to "StarCraft", following a group of Terran colonists and a Protoss fleet in their fight against the Zerg and a rising local insurgency. "Insurrection" was not received well, being criticized by reviewers for lacking the quality of the original game. "Insurrection" was followed within a few months by a second expansion, Retribution. Developed by Stardock, published by WizardWorks Software and authorized by Blizzard Entertainment, "Retribution" follows all three races attempting to seize control of a powerful crystal on a Terran Dominion colony. The expansion was not received with critical support, instead being regarded as average but at least challenging. After the release of "Retribution", Blizzard Entertainment announced a new official expansion pack that would continue on the story of "StarCraft". "" was consequently created, developed jointly by Blizzard Entertainment and Saffire. "Brood War" continues the story of "StarCraft" from days after its conclusion, and was released for both Windows and Mac OS to critical praise on November 30, 1998 in the US and in March 1999 in Europe.
Before Insurrection, an unauthorized expansion pack, called Stellar Forces, was published by Micro Star but was recalled weeks later when Blizzard won the court case against it. It consisted of 22 single player maps and 32 multi-player maps which are considered to be rather plain.
Nintendo 64 version.
In 2000, "StarCraft 64" was released for the Nintendo 64, co-developed by Blizzard Entertainment and Mass Media Inc. and published by Nintendo. The game featured all of the missions from both "StarCraft" and the expansion "Brood War", as well as some exclusive missions, such as two different tutorials and a new secret mission, "Resurrection IV". "Resurrection IV" is set after the conclusion of "Brood War", and follows Jim Raynor embarking on a mission to rescue the "Brood War" character Alexei Stukov, a vice admiral from Earth who has been captured by the Zerg. The "Brood War" missions required the use of a Nintendo 64 memory Expansion Pak to run. In addition, "StarCraft 64" features a split screen cooperative mode, also requiring the expansion pak, allowing two players to control one force in-game. "StarCraft 64" was not as popular as the PC version, and lacked the online multiplayer capabilities and speech in mission briefings. In addition, cut scenes were shortened. Blizzard Entertainment had previously considered a PlayStation port of the game, but it was decided that the game would instead be released on the Nintendo 64.
"StarCraft" 64 was planned to be released in Europe in fall 2000 but was finally cancelled. The PAL version was released only in Australia.
Cultural impact.
Reception.
"StarCraft" was released internationally on March 31, 1998 and became the best-selling PC game for that year, selling over 1.5 million copies worldwide. In the next decade, "StarCraft" sold over 9.5 million copies across the globe, with 4.5 million of these being sold in South Korea. Since the initial release of "StarCraft", Blizzard Entertainment reported that its Battle.net online multiplayer service grew by 800 percent. "StarCraft" remains one of the most popular online games in the world.
Generally, "StarCraft" was received positively by critics, with many contemporary reviewers noting that while the game may not have deviated significantly from the status quo of most real-time strategy games, it was one of the best to have applied the formula. In addition, "StarCraft"‍ '​s pioneering use of three distinct, unique and balanced races over two equal sides was praised by critics, with GameSpot commenting that this helped the game to "avoid the problem that has plagued every other game in the genre". Many critics also praised the strength of the story accompanying the game, with some reviewers being impressed by how well the story was folded into the gameplay. The game's voice acting in particular was praised; GameSpot later hailed the voice work in the game as one of the ten best in the industry at the time. Equally, the multiplayer aspects of the game were positively received. "StarCraft" has received multiple awards, including being named as one of the best games of all time by GameSpot, IGN, and "Game Informer". According to Blizzard Entertainment "StarCraft" has won 37 awards, and has received a star on the floor of the Metreon as part of the Walk of Game in San Francisco in early 2006.
Although at the time "StarCraft"‍ '​s graphics and audio were praised by critics, later reviews have noted that the graphics do not compare to more modern games. The capacity for the game's artificial intelligence to navigate units to waypoints also faced some heavy criticism, with "PC Zone" stating that the inability for developers to make an effective pathfinding system was "the single most infuriating element of the real-time strategy genre". In addition, several reviewers expressed concern over some familiarities between the unit structures of each race, as well as over the potential imbalance of players using rushing tactics early in multiplayer games. Blizzard Entertainment has strived to balance rush tactics in later updates. The Nintendo 64 version of the game was not received as positively by reviewers, and was criticized for poor graphics in comparison to the PC version. However, critics did praise the game and Mass Media for using effective controls on the gamepad and maintaining the high quality audio.
Legacy.
GameSpot described "StarCraft" as "The defining game of its genre. It is the standard by which all real-time strategy games are judged." IGN stated that "StarCraft" "is hands down one of the best, if not the best, real-time strategy games ever created." "StarCraft" is frequently included in the industry's best games rankings, for example it ranked 37 in "Edge"‍ '​s top 100 games of all time. "StarCraft" has even been taken into space, as Daniel Barry took a copy of the game with him on the Space Shuttle mission STS-96 in 1999. "StarCraft"‍ '​s popularity resulted in "Guinness World Records" awarding the game four world records, including "Best Selling PC Strategy Game," "Largest Income in Professional Gaming," and "Largest Audience for a Game Competition" when 120,000 fans turned out to watch the final of the SKY proleague season 2005 in Busan, South Korea. Researchers have shown that the audience for watching "StarCraft" games is diverse and that "StarCraft" uses instances of information asymmetry to make the game more entertaining for spectators. In addition, "StarCraft" has been the subject of an academic course; the University of California, Berkeley offered a student-run introductory course on theory and strategy in spring 2009. The verb "to zerg" has entered general usage as a gaming term to refer to the tactic of zerg rushing an opponent with a very large force of weak units.
After its release, "StarCraft" rapidly grew in popularity in South Korea, eventually making its way to become the country's national e-sport after establishing a successful pro-gaming scene. Professional gamers in South Korea are media celebrities, and "StarCraft" games are broadcast over three television channels dedicated to the professional gaming scene. Professional gamers in South Korea have gained television contracts, sponsorships, and tournament prizes, allowing one of the most famous players, Lim "BoxeR" Yo-hwan, to gain a fan club of over half a million people. One player, Lee Yun-yeol, reported earnings in 2005 of US$.
"StarCraft" was part of the United States Air Force's Air and Space Basic Course, used to teach newly active officers about crisis planning under stress and joint service teamwork. Other efforts to make more 'realistic' current-day battle software led to distractions when simulated hardware didn't align with the real hardware active duty officers knew about. The science fiction setting allowed students to focus on the battle tactics.
In 2014 a unofficial version for the Pandora handheld and the ARM architecture became available by static recompilation and reverse engineering of the original x86 version.
Merchandise.
The storyline of "StarCraft" has been adapted into several novels. The first novel, "Uprising", which was written by Blizzard employee Micky Neilson and published in December 2000, acts as a prequel to the events of "StarCraft". Other novels—"Liberty's Crusade" by Jeff Grubb and Aaron Rosenberg's "Queen of Blades"—retell the story of the game from different perspectives. At BlizzCon 2007, "StarCraft" creator Chris Metzen stated that he hoped to novelize the entirety of "StarCraft" and its expansion "Brood War" into a definitive text-based story. Later novels, such as Gabriel Mesta's "Shadow of the Xel'Naga" and Christie Golden's "The Dark Templar Saga", further expand the storyline, creating the setting for "".
A number of action figures and collectable statues based upon the characters and units in "StarCraft" have been produced by ToyCom. A number of model kits, made by Academy Hobby Model Kits, were also produced, displaying 1/30 scale versions of the marine and the hydralisk. In addition, Blizzard Entertainment teamed up with Fantasy Flight Games to create a board game with detailed sculptures of game characters. Blizzard Entertainment also licensed Wizards of the Coast to produce an Alternity based game entitled "StarCraft Adventures".

</doc>
<doc id="26810" url="http://en.wikipedia.org/wiki?curid=26810" title="Skepticism">
Skepticism

Skepticism or scepticism (see spelling differences) is generally any questioning attitude towards knowledge, facts, or opinions/beliefs stated as facts, or doubt regarding claims that are taken for granted elsewhere.
Philosophical skepticism is an overall approach that requires all information to be well supported by evidence. Classical philosophical skepticism derives from the 'Skeptikoi', a school who "asserted nothing". Adherents of Pyrrhonism (and more recently, partially synonymous with Fallibilism), for instance, suspend judgment in investigations. Skeptics may even doubt the reliability of their own senses. Religious skepticism, on the other hand, is "doubt concerning basic religious principles (such as immortality, providence, and revelation)". Scientific skepticism is about testing scientific beliefs for reliability, by subjecting them to systematic investigation using the scientific method, to discover empirical evidence for them.
Definition.
In ordinary usage, skepticism (US) or scepticism (UK) (Greek: 'σκέπτομαι' "skeptomai", to think, to look about, to consider; see also spelling differences) refers to:
In philosophy, skepticism refers more specifically to any one of several propositions. These include propositions about:
Philosophical skepticism.
In philosophical skepticism, pyrrhonism is a position that refrains from making truth claims. A philosophical skeptic does not claim that truth is impossible (which itself would be a truth claim), instead it recommends "suspending belief". The label is commonly used to describe philosophies which appear similar to philosophical skepticism, such as academic skepticism, an ancient variant of Platonism that claimed knowledge of truth was impossible. Empiricism is a closely related, but not identical, position to philosophical skepticism. Empiricists see empiricism as a pragmatic compromise between philosophical skepticism and nomothetic science; philosophical skepticism is in turn sometimes referred to as "radical empiricism."
Western Philosophical skepticism originated in ancient Greek philosophy. The Greek Sophists of the 5th century BC were partially skeptics.
Pyrrho of Elis (365-275 BC) is usually credited with founding the "school" of skepticism. He traveled to India and studied with the "gymnosophists" (naked lovers of wisdom), which could have been any number of Indian sects. From there, he brought back the idea that nothing can be known for certain. The senses are easily fooled, and reason follows too easily our desires. Pyrrhonism was a school of skepticism founded by his follower Aenesidemus in the first century BC and recorded by Sextus Empiricus in the late 2nd century or early 3rd century AD. Subsequently, in the "New Academy" Arcesilaus (c. 315-241 BC) and Carneades (c. 213-129 BC) developed more theoretical perspectives by which conceptions of absolute truth and falsity were refuted as uncertain. Carneades criticized the views of the Dogmatists, especially supporters of Stoicism, asserting that absolute certainty of knowledge is impossible. Sextus Empiricus (c. AD 200), the main authority for Greek skepticism, developed the position further, incorporating aspects of empiricism into the basis for asserting knowledge.
Greek skeptics criticized the Stoics, accusing them of dogmatism. For the skeptics, the logical mode of argument was untenable, as it relied on propositions which could not be said to be either true or false without relying on further propositions. This was the regress argument, whereby every proposition must rely on other propositions in order to maintain its validity (see the five tropes of Agrippa the Sceptic). In addition, the skeptics argued that two propositions could not rely on each other, as this would create a circular argument (as p implies q and q implies p). For the skeptics, such logic was thus an inadequate measure of truth and could create as many problems as it claimed to have solved. Truth was not, however, necessarily unobtainable, but rather an idea which did not yet exist in a pure form. Although skepticism was accused of denying the possibility of truth, in fact it appears to have mainly been a critical school which merely claimed that logicians had not discovered truth.
In Islamic philosophy, skepticism was established by Al-Ghazali (1058–1111), known in the West as "Algazel", as part of the Ash'ari school of Islamic theology, whose method of skepticism shares many similarities with Descartes' method.
In an effort to avoid skepticism, René Descartes begins his "Meditations on First Philosophy" attempting to find indubitable truth on which to base his knowledge. He later recognizes this truth as "I think, therefore I am," but before he finds this truth, he briefly entertains the skeptical arguments from dreaming and radical deception.
David Hume has also been described as a global skeptic.
Pierre Le Morvan (2011) has distinguished between three broad philosophical approaches to skepticism. The first he calls the "Foil Approach." According to this approach, skepticism is treated as a problem to be solved, or challenge to be met, or threat to be parried; skepticism's value on this view, insofar as it is deemed to have one, accrues from its role as a foil contrastively illuminating what is required for knowledge and justified belief. The second he calls the "Bypass Approach" according to which skepticism is bypassed as a central concern of epistemology. Le Morvan advocates a third approach—he dubs it the "Health Approach"—that explores when skepticism is healthy and when it is not, or when it is virtuous and when it is vicious.
Scientific skepticism.
A scientific (or empirical) skeptic is one who questions beliefs on the basis of scientific understanding. Most scientists, being scientific skeptics, test the reliability of certain kinds of claims by subjecting them to a systematic investigation using some form of the scientific method. As a result, a number of claims are considered "pseudoscience" if they are found to improperly apply or ignore the fundamental aspects of the scientific method. Scientific skepticism may discard beliefs pertaining to things outside perceivable observation and thus outside the realm of systematic, empirical falsifiability/testability.
Religious skepticism.
Religious skepticism generally refers to doubting given religious beliefs or claims. Historically, religious skepticism can be traced back to Socrates, who doubted many religious claims of the time. Modern religious skepticism typically places more emphasis on scientific and historical methods or evidence, with Michael Shermer writing that it is a process for discovering the truth rather than blanket non-acceptance. For this reason a religious skeptic might believe that Jesus existed while questioning claims that he was the messiah or performed miracles (see historicity of Jesus). Religious skepticism is not the same as atheism or agnosticism, though these often do involve skeptical attitudes toward religion and philosophical theology (for example, towards divine omnipotence). Religious people are generally skeptical about claims of other religions, at least when the two denominations conflict in some stated belief. In addition, they may also be skeptical of the claims made by atheists. The historian Will Durant writes that Plato was "as skeptical of atheism as of any other dogma."

</doc>
<doc id="26818" url="http://en.wikipedia.org/wiki?curid=26818" title="Stagflation">
Stagflation

In economics, stagflation, a portmanteau of "stagnation" and "inflation", is a situation where the inflation rate is high, the economic growth rate slows down, and unemployment remains steadily high. It raises a dilemma for economic policy since actions designed to lower inflation may exacerbate unemployment, and vice versa.
The term is generally attributed to a British Conservative Party politician who became chancellor of the exchequer in 1970, Iain Macleod, who coined the phrase in his speech to Parliament in 1965. 
Keynes didn't use the term, but some of his work refers to the conditions most would recognise as stagflation. In the version of Keynesian macroeconomic theory which was dominant between the end of WWII and the late-1970s, inflation and recession were regarded as mutually exclusive, the relationship between the two being described by the Phillips curve. Stagflation is very costly and difficult to eradicate once it starts, in human terms as well as in budget deficits.
One economic indicator, the misery index, is derived by the simple addition of the inflation rate to the unemployment rate.
The Great Inflation.
The term stagflation was first coined during a period of inflation and unemployment in the United Kingdom. The United Kingdom experienced an outbreak of inflation in the 1960s and 1970s. As early as 17 November 1965, Iain Macleod, the spokesman on economic issues for the United Kingdom’s Conservative Party, warned of the gravity of the UK economic situation in the House of Commons: "We now have the worst of both worlds—not just inflation on the one side or stagnation on the other, but both of them together. We have a sort of
“stagflation” situation. And history, in modern terms, is indeed being made."
With these words, Macleod coined the term ‘stagflation’. In a Bank of England working papers series article authors, Edward Nelson and Kalin Nikolov, (2002) examined causes and policy errors related to the Great Inflation in the United Kingdom in the 1970s, arguing that as inflation rose in the 1960s and 1970s, UK policy makers failed to recognize the primary role of monetary policy in controlling inflation. Instead, they attempted to use non-monetary policies and devices to respond to the economic crisis. Policy makers also made "inaccurate estimates of the degree of excess demand in the economy, contributed significantly to the outbreak of inflation in the United Kingdom in the 1960s and 1970s.
Stagflation was not limited to the UK, however. Economists have shown that stagflation was prevalent among seven major economies from 1973 to 1982. After inflation rates began to fall in 1982, economists' focus shifted from the causes of stagflation to the "determinants of productivity growth and the effects of real wages on the demand for labor".
Causes.
Economists offer two principal explanations for why stagflation occurs. First, stagflation can result when the productive capacity of an economy is reduced by an unfavorable supply shock that causes an increase in the price of oil for an oil importing country. Such an unfavorable supply shock tends to raise prices at the same time that it slows the economy by making production more costly and less profitable.
Milton Friedman famously described this situation as "too much money chasing too few goods".
Second, both stagnation and inflation can result from inappropriate macroeconomic policies. For example, central banks can cause inflation by permitting excessive growth of the money supply, and the government can cause stagnation by excessive regulation of goods markets and labour markets. Either of these factors can cause stagflation. Excessive growth of the money supply taken to such an extreme that it must be reversed abruptly can clearly be a cause. Both types of explanations are offered in analyses of the global stagflation of the 1970s: it began with a huge rise in oil prices, but then continued as central banks used excessively stimulative monetary policy to counteract the resulting recession, causing a runaway price/wage spiral.
Postwar Keynesian and monetarist views.
Early Keynesianism and monetarism.
Up to the 1960s many Keynesian economists ignored the possibility of stagflation, because historical experience suggested that high unemployment was typically associated with low inflation, and vice versa (this relationship is called the "Phillips curve"). The idea was that high demand for goods drives up prices, and also encourages firms to hire more; and likewise high employment raises demand. However, in the 1970s and 1980s, when stagflation occurred, it became obvious that the relationship between inflation and employment levels was not necessarily stable: that is, the Phillips relationship could shift. Macroeconomists became more skeptical of Keynesian theories, and the Keynesians themselves reconsidered their ideas in search of an explanation of stagflation.
The explanation for the shift of the Phillips curve was initially provided by the monetarist economist Milton Friedman, and also by Edmund Phelps. Both argued that when workers and firms begin to expect more inflation, the Phillips curve shifts up (meaning that more inflation occurs at any given level of unemployment). In particular, they suggested that if inflation lasted for several years, workers and firms would start to take it into account during wage negotiations, causing workers' wages and firms' costs to rise more quickly, thus further increasing inflation. While this idea was a severe criticism of early Keynesian theories, it was gradually accepted by most Keynesians, and has been incorporated into New Keynesian economic models.
Neo-Keynesianism.
Neo-Keynesian theory distinguished two distinct kinds of inflation: demand-pull (caused by shifts of the aggregate demand curve) and cost-push (caused by shifts of the aggregate supply curve). Stagflation, in this view, is caused by cost-push inflation. Cost-push inflation occurs when some force or condition increases the costs of production. This could be caused by government policies (such as taxes), or from purely external factors such as a shortage of natural resources or an act of war.
Contemporary Keynesian analyses argue that stagflation can be understood by distinguishing factors that affect aggregate demand from those that affect aggregate supply. While monetary and fiscal policy can be used to stabilise the economy in the face of aggregate demand fluctuations, they are not very useful in confronting aggregate supply fluctuations. In particular, an adverse shock to aggregate supply, such as an increase in oil prices, can give rise to stagflation.
Supply theory.
Fundamentals.
Supply theories are based on the neo-Keynesian cost-push model and attribute stagflation to significant disruptions to the supply side of the supply-demand market equation, for example, when there is a sudden real or relative scarcity of key commodities, natural resources, or natural capital needed to produce goods and services. Other factors may also cause supply problems, for example, social and political conditions such as policy changes, acts of war, extremely restrictive government control of production (For example monopolising Dictatorships). In this view, stagflation is thought to occur when there is an adverse supply shock (for example, a sudden increase in the price of oil or a new tax) that causes a subsequent jump in the "cost" of goods and services (often at the wholesale level). In technical terms, this results in contraction or negative shift in an economy's aggregate supply curve.
In the resource scarcity scenario (Zinam 1982), stagflation results when economic growth is inhibited by a restricted supply of raw materials. That is, when the actual or relative supply of basic materials (fossil fuels (energy), minerals, agricultural land in production, timber, etc.) decreases and/or cannot be increased fast enough in response to rising or continuing demand. The resource shortage may be a real physical shortage or a relative scarcity due to factors such as taxes or bad monetary policy which have affected the "cost" or availability of raw materials. This is consistent with the cost-push inflation factors in neo-Keynesian theory (above). The way this plays out is that after supply shock occurs, the economy will first try to maintain momentum – that is, consumers and businesses will begin paying higher prices in order to maintain their level of demand. The central bank may exacerbate this by increasing the money supply, by lowering interest rates for example, in an effort to combat a recession. The increased money supply props up the demand for goods and services, though demand would normally drop during a recession.
In the Keynesian model, higher prices will prompt increases in the supply of goods and services. However, during a supply shock (i.e. scarcity, "bottleneck" in resources, etc.), supplies don't respond as they normally would to these price pressures. So, inflation jumps and output drops, producing stagflation.
Explaining the 1970s stagflation.
Following Richard Nixon's imposition of wage and price controls on 15 August 1971, an initial wave of cost-push shocks in commodities were blamed for causing spiraling prices. Perhaps the most notorious factor cited at that time was the failure of the Peruvian anchovy fishery in 1972, a major source of livestock feed. The second major shock was the 1973 oil crisis, when the Organization of Petroleum Exporting Countries (OPEC) constrained the worldwide supply of oil. Both events, combined with the overall energy shortage that characterized the 1970s, resulted in actual or relative scarcity of raw materials. The price controls resulted in shortages at the point of purchase, causing, for example, queues of consumers at fuelling stations and increased production costs for industry.
Theoretical responses.
Under this set of theories, the solution to stagflation is to restore the supply of materials. In the case of a physical scarcity, stagflation is mitigated either by finding a replacement for the missing resources or by developing ways to increase economic productivity and energy efficiency so that more output is produced with less input. For example, in the late 1970s, the scarcity of oil was relieved by increases in both energy efficiency and global oil production. This factor, along with adjustments in monetary policies, helped end stagflation.
Recent views.
Through the mid-1970s, none of the major macroeconomic models (Keynesian, New Classical, and monetarist) were able to explain stagflation.
After several years of research, a convincing explanation was provided based on the effects of adverse supply shocks on both prices and output. According to Blanchard (2009), these adverse events were one of two components of stagflation; the other was "ideas", which Robert Lucas (famous for the Lucas Supply Curve), Thomas Sargent, and Robert Barro were cited as expressing as "wildly incorrect" and "fundamentally flawed" predictions [of Keynesian economics] which, they said, left stagflation to be explained by "contemporary students of the business cycle". In this discussion, Blanchard hypothesizes that the recent oil price increases could trigger another period of stagflation, although this has not yet happened (pg. 152).
Neoclassical views.
A purely neoclassical view of the macroeconomy rejects the idea that monetary policy can have real effects. Neoclassical macroeconomists argue that real economic quantities, like real output, employment, and unemployment, are determined by real factors only. Nominal factors like changes in the money supply only affect nominal variables like inflation. The neoclassical idea that nominal factors cannot have real effects is often called "monetary neutrality" or also the "classical dichotomy".
Since the neoclassical viewpoint says that real phenomena like unemployment are essentially unrelated to nominal phenomena like inflation, a neoclassical economist would offer two separate explanations for 'stagnation' and 'inflation'. Neoclassical explanations of stagnation (low growth and high unemployment) include inefficient government regulations or high benefits for the unemployed that give people less incentive to look for jobs. Another neoclassical explanation of stagnation is given by real business cycle theory, in which any decrease in labour productivity makes it efficient to work less. The main neoclassical explanation of inflation is very simple: it happens when the monetary authorities increase the money supply too much.
In the neoclassical viewpoint, the real factors that determine output and unemployment affect the aggregate supply curve only. The nominal factors that determine inflation affect the aggregate demand curve only. When some adverse changes in real factors are shifting the aggregate supply curve left at the same time that unwise monetary policies are shifting the aggregate demand curve right, the result is stagflation.
Thus the main explanation for stagflation under a classical view of the economy is simply policy errors that affect both inflation and the labour market. Ironically, a very clear argument in favour of the classical explanation of stagflation was provided by Keynes himself. In 1919, John Maynard Keynes described the inflation and economic stagnation gripping Europe in his book The Economic Consequences of the Peace. Keynes wrote:
Keynes explicitly pointed out the relationship between governments printing money and inflation.
Keynes also pointed out how government price controls discourage production.
Keynes detailed the relationship between German government deficits and inflation.
Keynesian in the short run, classical in the long run.
While most economists believe that changes in money supply can have some real effects in the short run, neoclassical and neo-Keynesian economists tend to agree that there are no long-run effects from changing the money supply. Therefore, even economists who consider themselves neo-Keynesians usually believe that in the long run, money is neutral. In other words, while neoclassical and neo-Keynesian models are often seen as competing points of view, they can also be seen as two descriptions appropriate for different time horizons. Many mainstream textbooks today treat the neo-Keynesian model as a more appropriate description of the economy in the short run, when prices are 'sticky', and treat the neoclassical model as a more appropriate description of the economy in the long run, when prices have sufficient time to adjust fully.
Therefore, while mainstream economists today might often attribute short periods of stagflation (not more than a few years) to adverse changes in supply, they would not accept this as an explanation of very prolonged stagflation. More prolonged stagflation would be explained as the effect of inappropriate government policies: excessive regulation of product markets and labor markets leading to long-run stagnation, and excessive growth of the money supply leading to long-run inflation.
Alternative views.
As differential accumulation.
Political economists Jonathan Nitzan and Shimshon Bichler have proposed an explanation of stagflation as part of a theory they call differential accumulation, which says firms seek to beat the average profit and capitalisation rather than maximise. According to this theory, periods of mergers and acquisitions oscillate with periods of stagflation. When mergers and acquisitions are no longer politically feasible (governments clamp down with anti-monopoly rules), stagflation is used as an alternative to have higher relative profit than the competition. With increasing mergers and acquisitions, the power to implement stagflation increases.
Stagflation appears as a societal crisis, such as during the period of the oil crisis in the 70s and in 2007 to 2010. Inflation in stagflation, however, doesn't affect all firms equally. Dominant firms are able to increase their own prices at a faster rate than competitors. While in the aggregate no one appears to be profiting, differentially dominant firms improve their positions with higher relative profits and higher relative capitalisation. Stagflation is not due to any actual supply shock, but because of the societal crisis that hints at a supply crisis. It is mostly a 20th and 21st century phenomenon that has been mainly used by the "weapondollar-petrodollar coalition" creating or using Middle East crises for the benefit of pecuniary interests.
Demand-pull stagflation theory.
Demand-pull stagflation theory explores the idea that stagflation can result exclusively from monetary shocks without any concurrent supply shocks or negative shifts in economic output potential. Demand-pull theory describes a scenario where stagflation can occur following a period of monetary policy implementations that cause inflation. This theory was first proposed in 1999 by Eduardo Loyo of Harvard University's John F. Kennedy School of Government.
Supply-side theory.
Supply-side economics emerged as a response to US stagflation in the 1970s. It largely attributed inflation to the ending of the Bretton Woods system in 1971 and the lack of a specific price reference in the subsequent monetary policies (Keynesian and Monetarism). Supply-side economists asserted that the contraction component of stagflation resulted from an inflation-induced rise in real tax rates (see bracket creep)
Austrian School of economics.
Adherents to the Austrian School maintain that creation of new money ex nihilo benefits the creators and early recipients of the new money relative to late recipients. Money creation is not wealth creation; it merely allows early money recipients to outbid late recipients for resources, goods, and services.
Since the actual producers of wealth are typically late recipients, increases in the money supply weakens wealth formation and undermines the rate of economic growth. Says Austrian economist Frank Shostak:
"The increase in the money supply rate of growth coupled with the slowdown in the rate of growth of goods produced is what the increase in the rate of price inflation is all about. (Note that a price is the amount of money paid for a unit of a good.) What we have here is a faster increase in price inflation and a decline in the rate of growth in the production of goods. But this is exactly what stagflation is all about, i.e., an increase in price inflation and a fall in real economic growth. Popular opinion is that stagflation is totally made up. It seems therefore that the phenomenon of stagflation is the normal outcome of loose monetary policy. This is in agreement with [Phelps and Friedman (PF)]. Contrary to PF, however, we maintain that stagflation is not caused by the fact that in the short run people are fooled by the central bank. Stagflation is the natural result of monetary pumping which weakens the pace of economic growth and at the same time raises the rate of increase of the prices of goods and services."
Jane Jacobs and the influence of cities on stagflation.
In 1984, journalist and activist Jane Jacobs proposed the failure of major macroeconomic theories to explain stagflation was due to their focus on the nation as the salient unit of economic analysis, rather than the city. She proposed the key to avoiding stagflation was for a nation to focus on the development of "import-replacing cities", which would experience economic ups and downs at different times, providing overall national stability and avoiding widespread stagflation. According to Jacobs, import-replacing cities are those which have developed economies balancing their own production with domestic imports, meaning they can respond with flexibility as economic supply and demand cycles change over time. While lauding her originality, clarity, and consistency, urban planning scholars have criticized Jacobs for not comparing her own ideas to those of major theorists (e.g., Adam Smith, Karl Marx) with the same depth and breadth they developed, as well as a lack of scholarly documentation. Despite these issues, Jacobs' work is notable for having widespread public readership and influence on decision-makers.
Responses.
Stagflation undermined support for Keynesian consensus. The rise of conservative theories of economics, including monetarism, can be traced to the failure of Keynesian policies to combat stagflation or explain it to the satisfaction of economists and policy-makers.
Federal Reserve chairman Paul Volcker very sharply increased interest rates from 1979–1983 in what was called a "disinflationary scenario." After U.S. prime interest rates had soared into the double-digits, inflation did come down; these interest rates were the highest long-term prime interest rates that had ever existed in modern capital markets. Volcker is often credited with having stopped at least the inflationary side of stagflation, although the American economy also dipped into recession. Starting in approximately 1983, growth began a recovery. Both fiscal stimulus and money supply growth were policy at this time. A five- to six-year jump in unemployment during the Volcker disinflation suggests Volcker may have trusted unemployment to self-correct and return to its natural rate within a reasonable period.
Further reading.
</dl>

</doc>
<doc id="26819" url="http://en.wikipedia.org/wiki?curid=26819" title="Soundness">
Soundness

In mathematical logic, a logical system has the soundness property if and only if its inference rules prove only formulas that are valid with respect to its semantics. In most cases, this comes down to its rules having the property of preserving "truth", but this is not the case in general.
Of arguments.
An argument is sound if and only if
For instance,
The argument is valid (because the conclusion is true based on the premises, that is, that the conclusion follows the premises) and since the premises are in fact true, the argument is sound.
The following argument is valid but not sound:
Since the first premise is actually false, the argument, though valid, is not sound.
Logical systems.
Soundness is among the most fundamental properties of mathematical logic. The soundness property provides the initial reason for counting a logical system as desirable. The completeness property means that every validity (truth) is provable. Together they imply that all and only validities are provable.
Most proofs of soundness are trivial. For example, in an axiomatic system, proof of soundness amounts to verifying the validity of the axioms and that the rules of inference preserve validity (or the weaker property, truth). Most axiomatic systems have only the rule of modus ponens (and sometimes substitution), so it requires only verifying the validity of the axioms and one rule of inference.
Soundness properties come in two main varieties: weak and strong soundness, of which the former is a restricted form of the latter.
Soundness.
Soundness of a deductive system is the property that any sentence that is provable in that deductive system is also true on all interpretations or structures of the semantic theory for the language upon which that theory is based. In symbols, where "S" is the deductive system, "L" the language together with its semantic theory, and "P" a sentence of "L": if ⊢"S" "P", then also ⊨"L" "P".
Strong soundness.
Strong soundness of a deductive system is the property that any sentence "P" of the language upon which the deductive system is based that is derivable from a set Γ of sentences of that language is also a logical consequence of that set, in the sense that any model that makes all members of Γ true will also make "P" true. In symbols where Γ is a set of sentences of "L": if Γ ⊢"S" "P", then also Γ ⊨"L" "P". Notice that in the statement of strong soundness, when Γ is empty, we have the statement of weak soundness.
Arithmetic soundness.
If "T" is a theory whose objects of discourse can be interpreted as natural numbers, we say "T" is "arithmetically sound" if all theorems of "T" are actually true about the standard mathematical integers. For further information, see ω-consistent theory.
Relation to completeness.
The converse of the soundness property is the semantic completeness property. A deductive system with a semantic theory is strongly complete if every sentence "P" that is a semantic consequence of a set of sentences Γ can be derived in the deduction system from that set. In symbols: whenever Γ ⊨ "P", then also Γ ⊢ "P". Completeness of first-order logic was first explicitly established by Gödel, though some of the main results were contained in earlier work of Skolem.
Informally, a soundness theorem for a deductive system expresses that all provable sentences are true. Completeness states that all true sentences are provable.
Gödel's first incompleteness theorem shows that for languages sufficient for doing a certain amount of arithmetic, there can be no effective deductive system that is complete with respect to the intended interpretation of the symbolism of that language. Thus, not all sound deductive systems are complete in this special sense of completeness, in which the class of models (up to isomorphism) is restricted to the intended one. The original completeness proof applies to "all" classical models, not some special proper subclass of intended ones.

</doc>
<doc id="26820" url="http://en.wikipedia.org/wiki?curid=26820" title="Syllabary">
Syllabary

A syllabary is a set of written symbols that represent the syllables or (more frequently) moras which make up words. 
A symbol in a syllabary, called a syllabogram, typically represents an (optional) consonant sound (simple onset) followed by a vowel sound (nucleus)—that is, a CV or V syllable—but other phonographic mappings such as CVC and CV-tone are also found in syllabaries.
Types.
A writing system using a syllabary is "complete" when it covers all syllables in the corresponding spoken language without requiring complex orthographic / graphemic rules, like implicit codas (⟨C1V⟩ ⇒ /C1VC2/) silent vowels (⟨C1V1+C2V2⟩ ⇒ /C1V1C2/) or echo vowels (⟨C1V1+C2V1⟩ ⇒ /C1V1C2/). This loosely corresponds to "shallow" orthographies in alphabetic writing systems.
"True" syllabograms are those that encompass all parts of a syllable, i.e. initial onset, medial nucleus and final coda, but since onset and coda are optional in at least some languages, there are "middle" (nucleus), "start" (onset-nucleus), "end" (nucleus-coda) and "full" (onset-nucleus-coda) true syllabograms. Most syllabaries only feature one or two kinds of syllabograms and form other syllables by graphemic rules.
Syllabograms, hence syllabaries, are "pure", "analytic" or "arbitrary" if they do not share graphic similarities that correspond to phonic similarities, i.e. the symbol for "ka" does not resemble in any predictable way the symbol for "ki", nor the symbol for "a".
Otherwise they are "synthetic", if they vary by onset, rime, nucleus "or" coda, or "systematic", if they vary by all of them.
Some scholars, e.g. Daniels, reserve the general term for analytic syllabaries and invent other terms (abugida, abjad) as necessary.
Languages using syllabaries.
Languages that use syllabic writing include Mycenaean Greek (Linear B), the North American language Cherokee, the African language Vai, the English-based creole Ndyuka written with the Afaka script, Yi language and formerly Nü Shu for the language of the Yao people in China. In addition, the undecoded Cretan Linear A is also believed by some to be a syllabic script, though this is not proven. 
The Chinese, Sumerian and Akkadian cuneiform, and Maya scripts are largely syllabic in nature, although based on logograms. They are therefore sometimes referred to as "logosyllabic". 
The contemporary Japanese language uses two syllabaries together called kana, namely hiragana and katakana (developed around AD 700). They are mainly used to write some native words and grammatical elements, as well as foreign words, e.g. "hotel" is written with three kana, ホテル ("ho-te-ru"). Because Japanese uses many CV (consonant + vowel) syllables, a syllabary is well suited to write the language. As in many syllabaries, however, vowel sequences and final consonants are written with separate glyphs, so that both "atta" and "kaita" are written with three kana: あった ("a-t-ta") and かいた ("ka-i-ta"). It is therefore sometimes called a "moraic" writing system.
Languages that use syllabaries today tend to have simple phonotactics, with a predominance of monomoraic (CV) syllables. 
For example, the modern Yi script is used to write a language that has no diphthongs or syllable codas; unusually among syllabaries, there is a separate glyph for every consonant-vowel-tone combination (CVT) in the language (apart from one tone which is indicated with a diacritic). 
Few syllabaries have glyphs for syllables that are not monomoraic, and those that once did have simplified over time to eliminate that complexity. 
For example, the Vai syllabary originally had separate glyphs for syllables ending in a coda "(doŋ)," a long vowel "(soo)," or a diphthong "(bai)," though not enough glyphs to distinguish all CV combinations (some distinctions were ignored). The modern script has been expanded to cover all moras, but at the same time reduced to exclude all other syllables. Bimoraic syllables are now written with two letters, as in Japanese: diphthongs are written with the help of V or "h"V glyphs, and the nasal coda is written with the glyph for "ŋ", which can form a syllable of its own in Vai. 
In Linear B, which was used to transcribe Greek, a language with complex syllables, complex consonant onsets were either written with two glyphs or simplified to one, while codas were generally ignored, e.g. "ko-no-so" for Κνωσός "Knōsos", "pe-ma" for σπέρμα "sperma." 
The Cherokee syllabary generally uses dummy vowels for coda consonants, but also has a segmental grapheme for /s/, which can be used both as a coda and in an initial /sC/ consonant cluster.
Difference from abugidas.
The languages of South Asia and Southeast Asia, as well as the Ethiopian languages, have a type of alphabet called an "abugida" or "alphasyllabary". In these scripts, unlike in pure syllabaries, syllables starting with the same consonant are generally expressed with characters that are based on the same sign in a regular way, and usually each character representing a syllable consists of several elements which designate the individual sounds of that syllable. In the 19th century these systems were called "syllabics", a term which has survived in the name of Canadian Aboriginal syllabics (also an abugida). In a true syllabary there may be graphic similarity between characters that share a common consonant or vowel sound, but it is not systematic or close to regular. For example, the characters for 'ke', 'ka', and 'ko' in Japanese hiragana have no similarity to indicate their common "k" sound (these being: け, か and こ). Compare abugida, where each grapheme typically represents a syllable but where characters representing related sounds are all similar graphically (typically, a common consonantal base is annotated in a more or less consistent manner to represent the vowel in the syllable). For example, in "Devanagari", an abugida, the same characters for 'ke', 'ka' and 'ko' are के, का and को respectively, with क indicating their common "k" sound.
Comparison to Latin alphabet.
Latin allows complex syllable structures, making it cumbersome to write English words with a syllabary. A "pure" syllabary would require a separate glyph for every syllable in English. Thus one would need separate symbols for "bag", "beg", "big", "bog", "bug"; "bad", "bed", "bid", "bod", "bud", "book", "bay", "bead", "bide", "bode", "boom", "bird", "Boyd", "bow", etc. However, such pure systems are rare. A work-around to this problem, common to several syllabaries around the world (including English loanwords in Japanese), is to write an echo vowel, as if the syllable coda were a second syllable:　"ba-gu" for "bag", etc. Another common approach is to simply ignore the coda, so that "bag" would be written "ba". This obviously would not work well for English, but was done in Mycenean Greek when the root word was two or three syllables long and the syllable coda was a weak consonant such as "n" or "s" (example: χρυσος "chrysos" written as "ku-ru-so").

</doc>
<doc id="26822" url="http://en.wikipedia.org/wiki?curid=26822" title="Steve Reich">
Steve Reich

Stephen Michael Reich (; born October 3, 1936) is an American composer who, along with La Monte Young, Terry Riley, and Philip Glass, pioneered minimal music in the mid to late 1960s.
His innovations include using tape loops to create phasing patterns (for example, his early compositions "It's Gonna Rain" and "Come Out"), and the use of simple, audible processes to explore musical concepts (for instance, "Pendulum Music" and "Four Organs"). These compositions, marked by their use of repetitive figures, slow harmonic rhythm and canons, have significantly influenced contemporary music, especially in the US. Reich's work took on a darker character in the 1980s with the introduction of historical themes as well as themes from his Jewish heritage, notably the Grammy Award-winning "Different Trains".
Reich's style of composition influenced many composers and groups. Writing in "The Guardian", music critic Andrew Clements described Reich as one of "a handful of living composers who can legitimately claim to have altered the direction of musical history". The American composer and critic Kyle Gann has said Reich "may...be considered, by general acclamation, America's greatest living composer".
Early life.
Reich was born in New York City to the Broadway lyricist June Sillman and Leonard Reich. When he was one year old, his parents divorced, and Reich divided his time between New York and California. He was given piano lessons as a child and describes growing up with the "middle-class favorites", having no exposure to music written before 1750 or after 1900. At the age of 14 he began to study music in earnest, after hearing music from the Baroque period and earlier, as well as music of the 20th century. Reich studied drums with Roland Kohloff in order to play jazz. While attending Cornell University, he minored in music and graduated in 1957 with a B.A. in Philosophy. Reich's B.A. thesis was on Ludwig Wittgenstein; later he would set texts by that philosopher to music in "Proverb" (1995) and "You Are (variations)" (2006).
For a year following graduation, Reich studied composition privately with Hall Overton before he enrolled at Juilliard to work with William Bergsma and Vincent Persichetti (1958–1961). Subsequently he attended Mills College in Oakland, California, where he studied with Luciano Berio and Darius Milhaud (1961–1963) and earned a master's degree in composition. At Mills, Reich composed "Melodica" for melodica and tape, which appeared in 1986 on the three-LP release "Music from Mills".
Reich worked with the San Francisco Tape Music Center along with Pauline Oliveros, Ramon Sender, Morton Subotnick, and Terry Riley. He was involved with the premiere of Riley's "In C" and suggested the use of the eighth note pulse, which is now standard in performance of the piece.
Career.
1960s.
Reich's early forays into composition involved experimentation with twelve-tone composition, but he found the rhythmic aspects of the number twelve more interesting than the pitch aspects. Reich also composed film soundtracks for "Plastic Haircut", "Oh Dem Watermelons", and "Thick Pucker", three films by Robert Nelson. The soundtrack of "Plastic Haircut", composed in 1963, was a short tape collage, possibly Reich's first. The "Watermelons" soundtrack used two old Stephen Foster minstrel tunes as its basis, and used repeated phrasing together in a large five-part canon. The music for "Thick Pucker" arose from street recordings Reich made walking around San Francisco with Nelson, who filmed in black and white 16mm. This film no longer survives. A fourth film from 1965, about 25 minutes long and tentatively entitled "Thick Pucker II", was assembled by Nelson from outtakes of that shoot and more of the raw audio Reich had recorded. Nelson was not happy with the resulting film and never showed it.
Reich was influenced by fellow minimalist Terry Riley, whose work "In C" combines simple musical patterns, offset in time, to create a slowly shifting, cohesive whole. Reich adopted this approach to compose his first major work, "It's Gonna Rain". Composed in 1965, the piece used a fragment of a sermon about the end of the world given by a black Pentecostal street-preacher known as Brother Walter. Reich built on his early tape work, transferring the last three words of the fragment, "it's gonna rain!", to multiple tape loops which gradually move out of phase with one another.
The 13-minute "Come Out" (1966) uses similarly manipulated recordings of a single spoken line given by Daniel Hamm, one of the falsely accused Harlem Six, who was severely injured by police. The survivor, who had been beaten, punctured a bruise on his own body to convince police about his beating. The spoken line includes the phrase "to let the bruise’s blood come out to show them." Reich rerecorded the fragment "come out to show them" on two channels, which are initially played in unison. They quickly slip out of sync; gradually the discrepancy widens and becomes a reverberation. The two voices then split into four, looped continuously, then eight, and continues splitting until the actual words are unintelligible, leaving the listener with only the speech's rhythmic and tonal patterns.
Reich's first attempt at translating this phasing technique from recorded tape to live performance was the 1967 "Piano Phase", for two pianos. In "Piano Phase" the performers repeat a rapid twelve-note melodic figure, initially in unison. As one player keeps tempo with robotic precision, the other speeds up very slightly until the two parts line up again, but one sixteenth note apart. The second player then resumes the previous tempo. This cycle of speeding up and then locking in continues throughout the piece; the cycle comes full circle three times, the second and third cycles using shorter versions of the initial figure. "Violin Phase", also written in 1967, is built on these same lines. "Piano Phase" and "Violin Phase" both premiered in a series of concerts given in New York art galleries.
A similar, lesser known example of this so-called process music is "Pendulum Music" (1968), which consists of the sound of several microphones swinging over the loudspeakers to which they are attached, producing feedback as they do so. "Pendulum Music" has never been recorded by Reich himself, but was introduced to rock audiences by Sonic Youth in the late 1990s.
Reich also tried to create the phasing effect in a piece "that would need no instrument beyond the human body". He found that the idea of phasing was inappropriate for the simple ways he was experimenting to make sound. Instead, he composed "Clapping Music" (1972), in which the players do not phase in and out with each other, but instead one performer keeps one line of a 12-quaver-long (12-eighth-note-long) phrase and the other performer shifts by one quaver beat every 12 bars, until both performers are back in unison 144 bars later.
The 1967 prototype piece "Slow Motion Sound" was not performed although Chris Hughes performed it 27 years later as "Slow Motion Blackbird" on his Reich-influenced 1994 album "Shift". It introduced the idea of slowing down a recorded sound until many times its original length without changing pitch or timbre, which Reich applied to "Four Organs" (1970), which deals specifically with augmentation. The piece has maracas playing a fast eighth note pulse, while the four organs stress certain eighth notes using an 11th chord. This work therefore dealt with repetition and subtle rhythmic change. It is unique in the context of Reich's other pieces in being linear as opposed to cyclic like his earlier works— the superficially similar "Phase Patterns", also for four organs but without maracas, is (as the name suggests) a phase piece similar to others composed during the period. "Four Organs" was performed as part of a Boston Symphony Orchestra program, and was Reich's first composition to be performed in a large traditional setting.
1970s.
In 1971, Reich embarked on a five-week trip to study music in Ghana, during which he learned from the master drummer Gideon Alorwoyie. Reich also studied Balinese gamelan in Seattle. From his African experience, as well as A. M. Jones's "Studies in African Music" about the music of the Ewe people, Reich drew inspiration for his 90-minute piece "Drumming", which he composed shortly after his return. Composed for a nine-piece percussion ensemble with female voices and piccolo, "Drumming" marked the beginning of a new stage in his career, for around this time he formed his ensemble, Steve Reich and Musicians, and increasingly concentrated on composition and performance with them. Steve Reich and Musicians, which was to be the sole ensemble to interpret his works for many years, still remains active with many of its original members.
After "Drumming", Reich moved on from the "phase shifting" technique that he had pioneered, and began writing more elaborate pieces. He investigated other musical processes such as augmentation (the temporal lengthening of phrases and melodic fragments). It was during this period that he wrote works such as "Music for Mallet Instruments, Voices and Organ" (1973) and "Six Pianos" (1973).
In 1974, Reich began writing "Music for 18 Musicians". This piece involved many new ideas, although it also hearkened back to earlier pieces. It is based on a cycle of eleven chords introduced at the beginning (called "Pulses"), followed by a small section of music based on each chord ("Sections I-XI"), and finally a return to the original cycle ("Pulses"). This was Reich's first attempt at writing for larger ensembles. The increased number of performers resulted in more scope for psychoacoustic effects, which fascinated Reich, and he noted that he would like to "explore this idea further". Reich remarked that this one work contained more harmonic movement in the first five minutes than any other work he had written. Steve Reich and Musicians made the premier recording of this work on ECM Records.
Reich explored these ideas further in his frequently recorded pieces "Music for a Large Ensemble" (1978) and "Octet" (1979). In these two works, Reich experimented with "the human breath as the measure of musical duration ... the chords played by the trumpets are written to take one comfortable breath to perform". Human voices are part of the musical palette in "Music for a Large Ensemble" but the wordless vocal parts simply form part of the texture (as they do in "Drumming"). With "Octet" and his first orchestral piece "Variations for Winds, Strings and Keyboards" (also 1979), Reich's music showed the influence of Biblical cantillation, which he had studied in Israel since the summer of 1977. After this, the human voice singing a text would play an increasingly important role in Reich's music.
The technique [...] consists of taking pre-existing melodic patterns and stringing them together to form a longer melody in the service of a holy text. If you take away the text, you're left with the idea of putting together small motives to make longer melodies – a technique I had not encountered before.
In 1974 Reich published the book "Writings About Music", containing essays on his philosophy, aesthetics, and musical projects written between 1963 and 1974. An updated and much more extensive collection, "Writings On Music (1965–2000)", was published in 2002.
1980s.
Reich's work took on a darker character in the 1980s with the introduction of historical themes as well as themes from his Jewish heritage. "Tehillim" (1981), Hebrew for "psalms", is the first of Reich's works to draw explicitly on his Jewish background. The work is in four parts, and is scored for an ensemble of four women's voices (one high soprano, two lyric sopranos and one alto), piccolo, flute, oboe, English horn, two clarinets, six percussion (playing small tuned tambourines without jingles, clapping, maracas, marimba, vibraphone and crotales), two electronic organs, two violins, viola, cello and double bass, with amplified voices, strings, and winds. A setting of texts from psalms 19:2–5 (19:1–4 in Christian translations), 34:13–15 (34:12–14), 18:26–27 (18:25–26), and 150:4–6, "Tehillim" is a departure from Reich's other work in its formal structure; the setting of texts several lines long rather than the fragments used in previous works makes melody a substantive element. Use of formal counterpoint and functional harmony also contrasts with the loosely structured minimalist works written previously.
"Different Trains" (1988), for string quartet and tape, uses recorded speech, as in his earlier works, but this time as a melodic rather than a rhythmic element. In "Different Trains" Reich compares and contrasts his childhood memories of his train journeys between New York and California in 1939–1941 with the very different trains being used to transport contemporaneous European children to their deaths under Nazi rule. The Kronos Quartet recording of "Different Trains" was awarded the Grammy Award for Best Classical Contemporary Composition in 1990. The composition was described by Richard Taruskin as "the only adequate musical response—one of the few adequate artistic responses in any medium—to the Holocaust", and he credited the piece with earning Reich a place among the great composers of the 20th century.
1990s.
In 1993, Reich collaborated with his wife, the video artist Beryl Korot, on an opera, "The Cave", which explores the roots of Judaism, Christianity and Islam through the words of Israelis, Palestinians, and Americans, echoed musically by the ensemble. The work, for percussion, voices, and strings, is a musical documentary, named for the Cave of Machpelah in Hebron, where a mosque now stands and Abraham is said to have been buried.
Reich and Korot collaborated on the opera "Three Tales", which concerns the "Hindenburg" disaster, the testing of nuclear weapons on Bikini Atoll, and other more modern concerns, specifically Dolly the sheep, cloning, and the technological singularity.
Reich used sampling techniques for pieces like "Three Tales" and "City Life" from 1994. Reich returned to composing purely instrumental works for the concert hall, starting with "Triple Quartet" in 1998 written for the Kronos Quartet that can either be performed by string quartet and tape, three string quartets or 36-piece string orchestra. According to Reich, the piece is influenced by Bartók's and Alfred Schnittke's string quartets, and Michael Gordon's "Yo Shakespeare".
2000s.
The instrumental series for the concert hall continued with "Dance Patterns" (2002), "Cello Counterpoint" (2003), and sequence of works centered around Variations: "You Are (Variations)" (2004), a work which looks back to the vocal writing of works like "Tehillim" or "The Desert Music", "Variations for Vibes, Pianos, and Strings" in 2005, for the London Sinfonietta and "Daniel Variations" (2006).
in 2002 Reich was invited by Walter Fink to the annual Komponistenporträt of the Rheingau Musik Festival, as the 12th composer featured.
In an interview with "The Guardian", Reich stated that he continued to follow this direction with his piece "Double Sextet" (2007), which was commissioned by eighth blackbird, an American ensemble consisting of the instrumental quintet (flute, clarinet, violin or viola, cello and piano) of Schoenberg's piece "Pierrot Lunaire" (1912) plus percussion. Reich states that he was thinking about Stravinsky's "Agon" (1957) as a model for the instrumental writing.
December 2010 Nonesuch Records and Indaba Music held a community remix contest in which over 250 submissions were received, and Steve Reich and Christian Carey judged the finals. Reich spoke in a related BBC interview that once he composed a piece he would not alter it again himself; "When it's done, it's done," he said. On the other hand he acknowledged that remixes have an old tradition e.g. famous religious music pieces where melodies were further developed into new songs.
2010s.
Reich has the world premiere of a piece, "WTC 9/11", written for String Quartet and Tape, a similar instrumentation to that of "Different Trains". It was premiered in March 2011 by the Kronos Quartet, at Duke University, North Carolina, US.
On March 5, 2013 the London Sinfonietta, conducted by Brad Lubman, at the Royal Festival Hall in London gave the world premiere of "Radio Rewrite" for ensemble with 11 players, inspired by the music of Radiohead. The programme also included "Double Sextet" for ensemble with 12 players, "Clapping Music", for two people and four hands featuring Reich himself alongside percussionist Colin Currie, "Electric Counterpoint", with electric guitar by Mats Bergstrom accompanied by a layered soundtrack, as well as two of Reich's small ensemble pieces, one for acoustic instruments, the other for electric instruments and tape.
Awards.
On January 25, 2007, Reich was named 2007 recipient of the Polar Music Prize with jazz saxophonist Sonny Rollins.
On April 20, 2009, Reich was awarded the 2009 Pulitzer Prize for Music, recognizing "Double Sextet", first performed in Richmond March 26, 2008. The citation called it "a major work that displays an ability to channel an initial burst of energy into a large-scale musical event, built with masterful control and consistently intriguing to the ear".
In May 2011 Steve Reich received an honorary doctorate from the New England Conservatory of Music.
In 2013 Reich received the US$400,000 BBVA Foundation Frontiers of Knowledge Award in contemporary music for bringing a new conception of music, based on the use of realist elements from the realm of daily life and others drawn from the traditional music of Africa and Asia.
In September 2014, Reich was awarded the "Leone d'Oro" (Golden Lion for Lifetime Achievement in Music) from the Venice Biennale.
Influence.
Reich's style of composition has influenced many other composers and musical groups, including John Adams, the progressive rock band King Crimson, the new-age guitarist Michael Hedges, the art-pop and electronic musician Brian Eno, the experimental art/music group The Residents, the composers associated with the Bang on a Can festival (including David Lang, Michael Gordon, and Julia Wolfe), and numerous indie rock musicians including songwriter Sufjan Stevens and instrumental ensembles Tortoise, The Mercury Program (themselves influenced by Tortoise), and Godspeed You! Black Emperor (who titled an unreleased song "Steve Reich").
John Adams commented, "He didn't reinvent the wheel so much as he showed us a new way to ride." He has also influenced visual artists such as Bruce Nauman, and many notable choreographers have made dances to his music, Eliot Feld, Jiří Kylián, Douglas Lee and Jerome Robbins among others; he has expressed particular admiration of Anne Teresa De Keersmaeker's work set to his pieces.
In featuring a sample of Reich's "Electric Counterpoint" (1987) the British ambient techno act the Orb exposed a new generation of listeners to the composer's music with its 1990 production "Little Fluffy Clouds". In 1999 the album "Reich Remixed" featured "re-mixes" of a number of Reich's works by various electronic dance-music producers, such as DJ Spooky, Kurtis Mantronik, Ken Ishii, and Coldcut amongst others.
Reich often cites Pérotin, J.S. Bach, Debussy, Bartók, and Stravinsky as composers whom he admires and who greatly influenced him when he was young. Jazz is a major part of the formation of Reich's musical style, and two of the earliest influences on his work were vocalists Ella Fitzgerald and Alfred Deller, whose emphasis on the artistic capabilities of the voice alone with little vibrato or other alteration was an inspiration to his earliest works. John Coltrane's style, which Reich has described as "playing a lot of notes to very few harmonies", also had an impact; of particular interest was the album "Africa/Brass", which "was basically a half-an-hour in F." Reich's influence from jazz includes its roots, also, from the West African music he studied in his readings and visit to Ghana. Other important influences are Kenny Clarke and Miles Davis, and visual artist friends such as Sol LeWitt and Richard Serra. Reich has also stated that he admires the music of the band Radiohead, which led to his composition "Radio Rewrite". Reich recently contributed the introduction to "Sound Unbound: Sampling Digital Music and Culture" (The MIT Press, 2008) edited by Paul D. Miller, a.k.a. DJ Spooky.
Notes.
</dl>
External links.
</dl>
</dl>
</dl>
</dl>

</doc>
<doc id="26823" url="http://en.wikipedia.org/wiki?curid=26823" title="Simon &amp; Garfunkel">
Simon &amp; Garfunkel

Simon & Garfunkel were an American folk rock duo consisting of singer-songwriter Paul Simon and singer Art Garfunkel. They were one of the most popular groups of the 1960s, and were viewed as counterculture icons of the decade's social revolution, alongside artists such as the Beatles and Bob Dylan. Their biggest hits—including "Mrs. Robinson" (1968), "The Boxer" (1969), and "Bridge over Troubled Water" (1969)—reached number one on several charts worldwide. Their often rocky relationship led to artistic disagreements, resulting in their 1970 breakup. Their final studio record, "Bridge over Troubled Water", was subsequently their most successful, becoming one of the world's best-selling albums. They have reunited several times since their split, most famously for 1981's "The Concert in Central Park", which attracted more than 500,000 people, the seventh-largest concert attendance in history.
The duo met as children in Queens, New York in 1953, where they learned to harmonize together and began writing original material. By 1957, under the name Tom & Jerry, the teenagers had their first minor success with "Hey Schoolgirl", a song imitating their idols the Everly Brothers. Afterwards, the duo went their separate ways, with Simon making unsuccessful solo records. In 1963, aware of a growing public interest in folk music, they regrouped and were signed to Columbia Records as Simon & Garfunkel. Their début, "Wednesday Morning, 3 A.M.", sold poorly, and they once again disbanded; Simon returning to a solo career, this time in England. A remix of their song "The Sound of Silence" gained airplay on U.S. radio in 1965, reaching number one on the "Billboard" Hot 100. Simon & Garfunkel reunited, releasing their second studio album "Sounds of Silence" and touring colleges nationwide. On their third release, "Parsley, Sage, Rosemary and Thyme" (1966), the duo assumed more creative control. Their music was featured in the 1967 film "The Graduate", propelling the duo to further exposure. "Bookends" (1968), their next album, benefited from this promotion, and increased their profile. After their 1970 breakup they both continued recording, Simon releasing a number of highly acclaimed albums, including 1986's "Graceland"; Garfunkel also carried on acting, including the lead in Nicolas Roeg's 1980 film "Bad Timing".
Simon & Garfunkel are considered the most popular folk rock act from the 1960s, and one of the most popular artists from the decade in general. Their music made a "deep impression" on the baby boomer generation, and they are considered alongside artists such as Bob Dylan and the Beatles to be representative of the era's cultural movements. They have been given 10 Grammy Awards, were inducted into the Rock and Roll Hall of Fame in 1990, and their "Bridge Over Troubled Water" album was nominated at the 1977 Brit Awards for Best International Album, and ranked at #51 on Rolling Stone's 500 Greatest Albums of All Time.
History.
Early years (1953–1956).
Paul Simon and Art Garfunkel grew up in the 1940s and 1950s in the predominantly Jewish neighborhood of Forest Hills in Queens, New York, just three blocks away from one another, and attended the same schools, Public School 164 in Flushing, Parsons Junior High School, and Forest Hills High School. Individually, when still young, they developed a fascination with music; both listened to the radio and were taken with rock and roll as it emerged, particularly the Everly Brothers. When Simon first noticed Garfunkel, he was singing in a fourth grade talent show, and Simon thought that was a good way to attract girls; he hoped for a friendship which eventually started in 1953 when they were in the sixth grade and appeared on stage together in a school play adaptation of "Alice in Wonderland". That first stage appearance was followed by the duo forming a street-corner doo-wop group, the Peptones, with three other friends, and learning to harmonize together. They began performing for the first time as a duo at school dances.
They moved to Forest Hills High School in 1955, where, in 1956, they wrote their first song, "The Girl for Me"; Simon's father sending a handwritten copy to the Library of Congress to register a copyright. While trying to remember the lyrics to the Everly's song "Hey Doll Baby", they created their own song, "Hey Schoolgirl", which they recorded themselves for $25 at Sanders Recording Studio in Manhattan. While recording they were overheard by a promoter, Sid Prosen, who – after speaking to their parents – signed them to his independent label Big Records.
From Tom & Jerry to Simon & Garfunkel (1957–1964).
While still aged 15, Simon & Garfunkel now had a recording contract with Sid Prosen's independent label Big Records. Using the name Tom & Jerry; Garfunkel naming himself Tom Graph, a reference to his interest in mathematics; and Simon naming himself Jerry Landis, after the surname of Sue Landis, a girl he had dated, the single "Hey Schoolgirl" was released, with the B-side "Dancin' Wild", in 1957. Prosen, using the payola system, bribed Alan Freed $200 to get the single played on his radio show, where it became a nightly staple. "Hey Schoolgirl" attracted regular rotation on nationwide AM pop stations, leading it to sell over 100,000 copies and to land on "Billboard"‍‍ '​‍s charts at number 49. Prosen promoted the group heavily, getting them a spot on Dick Clark's "American Bandstand" (headlining alongside Jerry Lee Lewis). The duo shared approximately $4,000 from the song – earning two percent each from royalties, the rest staying with Prosen. They released three more singles on Big Records: "Our Song", "That's My Story", and "Don't Say Goodbye", none of them successful.
After graduating from Forest Hills High School in 1959, they were still exploring the possibilities of a music career, though continued their education as a back up; Simon studying English at Queens College, City University of New York, Garfunkel studying first architecture, then switching to art history at Columbia College, Columbia University. While still with Big Records as a duo, Simon released a solo single, "True or False", under the name "True Taylor". This recording upset Garfunkel, who regarded it as a betrayal; the emotional tension from that incident occasionally surfacing throughout their relationship. Their last recording with Big Records was a cover of a Jan and Dean single, "Baby Talk", but the company became bankrupt soon after release; the track was reissued on Bell Records, but failed to sell, so Tom & Jerry was dissolved. Both, however, continued recording, albeit as solo artists: Garfunkel composing and recording "Private World" for Octavia Records, and - under the name Artie Garr - "Beat Love" for Warwick; Simon recorded with The Mystics, and Tico & The Triumphs, and wrote and recorded under the names Jerry Landis and Paul Kane. Simon also wrote and performed demos for other artists, working for a while with Carole King and Gerry Goffin.
After graduating in 1963, Simon met up with Garfunkel, who was still at Columbia, and they occasionally performed together again as a duo; this time with a shared interest in folk music. Simon enrolled part-time in Brooklyn Law School, By late 1963, billing themselves as "Kane & Garr", they performed at Gerde's Folk City, a Greenwich club that hosted Monday night performances. The duo performed three new songs — "Sparrow", "He Was My Brother", and "The Sound of Silence" — and got the attention of Columbia producer Tom Wilson, who worked with Bob Dylan. As a "star producer" for the label, he wanted to record "He Was My Brother" with a new British act named the Pilgrims. Simon convinced Wilson to let him and his partner have a studio audition, and they performed "The Sound of Silence". House engineer Roy Halee recorded the audition, and at Wilson's urging, Columbia signed the duo.
Their debut studio, "Wednesday Morning, 3 A.M.", was recorded over three daytime sessions in March 1964, and released in October. The album contains five original Simon compositions, with the remainder consisting of traditional folk songs, not unlike Dylan's first album. Simon was adamant that they would no longer use fake names, and they adopted the name Simon & Garfunkel. Columbia set up a promotional showcase at Folk City on March 31, 1964, the duo's first public concert as Simon & Garfunkel. The showcase, as well as other scheduled performances, did not go well.
Simon in England (1964–1965).
"Wednesday Morning, 3 A.M." sold only 3,000 copies upon its October release, and its poor sales led Simon to move to England where he had previously visited and played some gigs.
He toured the small folk clubs, appearing on the same bill and befriending British folk artists such as Bert Jansch, Martin Carthy, Al Stewart, and Sandy Denny. He met Kathy Chitty, who became the object of his affection and is the Kathy in "Kathy's Song" and "America".
A small music publishing company, Lorna Music, licensed "Carlos Dominguez", a single Simon had cut two years prior as "Paul Kane", for a cover by Val Doonican that sold very well. Simon visited Lorna to thank them, and the meeting resulted in a publishing and recording contract. He signed to the Oriole label and released "He Was My Brother" as a single. Simon invited Garfunkel to stay for the summer of 1964. Near the end of the season, Garfunkel returned to Columbia for class, and Simon surprised his friends by saying that he would be returning to the States as well. He would resume his studies at Brooklyn Law School for one semester, partially at his parents' insistence. He returned to England in January 1965, now certain that music was his calling. In the meantime, his landlord, Judith Piepe, had compiled a tape from his work at Lorna and sent it to the BBC in hopes they would play it. The demos aired on the "Five to Ten" morning show, and were instantly successful. Oriole had folded into CBS by that point, and hoped to record a new Paul Simon album. "The Paul Simon Songbook" was recorded in June 1965 and featured multiple future Simon & Garfunkel staples, among them "I Am a Rock" and "April Come She Will". CBS flew over Wilson to produce the record, and he stayed at Simon's flat. The album saw release in August, and although sales were poor, Simon felt content with his future in England.
Meanwhile, in the United States, a late-night disc jockey at WBZ-FM in Boston played "The Sound of Silence", where it found a college demographic. It was picked up the next day along the East Coast of the United States, down to Cocoa Beach, Florida. Wilson, inspired by the folk rock sound of the Byrds' cover of "Turn! Turn! Turn!" and Dylan's "Like a Rolling Stone", created a rock remix of the song with the same musicians who overdubbed the Dylan song. The remix of "The Sound of Silence" was issued in September 1965, where it reached the "Billboard" Hot 100. Wilson had not informed the duo of his intention to remix the track; as such, Simon was "horrified" when he first heard it. Garfunkel graduated in 1965, returning to Columbia University to do a master's degree in mathematics.
Mainstream breakthrough and success (1965–66).
By January 1966, "The Sound of Silence" topped the Hot 100, selling over one million copies. Simon reunited with Garfunkel that winter in New York, leaving Chitty and his friends in England behind. CBS demanded a new album from the duo, to be called "Sounds of Silence" to ride the wave of the hit. Recorded in three weeks, and mainly consisting of re-recorded songs from "The Paul Simon Songbook", plus four new tracks, "Sounds of Silence" was rush-released onto the market in mid-January 1966, peaking at number 21 "Billboard" Top LPs chart. A week later, "Homeward Bound" was released as a single, entering the USA top ten, followed by "I Am a Rock" peaking at number three. The duo supported the recordings with a nationwide tour of America, while CBS continued their promotion by re-releasing "Wednesday Morning, 3 A.M.", which promptly charted at number 30. Despite the commercial and popular success, the duo received critical derision, as many considered them a manufactured imitation of folk.
As they considered their previous effort a "rush job" to capitalize on their sudden success, the duo spent more time crafting the follow-up. It was the first time Simon insisted on total control in aspects of recording. Work began in 1966 and took nine months. Garfunkel considered the recording of "Scarborough Fair" the moment they stepped into the role as producer, because they were constantly beside engineer Roy Halee mixing the track. "Parsley, Sage, Rosemary and Thyme" was issued in October 1966, following the release of several singles and receiving sold-out college campus shows. The duo resumed their trek on the college circuit eleven days following the release, crafting an image that was described as "alienated", "weird", and "poetic". Manager Mort Lewis also was responsible for this public perception, as he withheld them from television appearances (unless they were allowed to play an uninterrupted set or choose the setlist). Simon, then 27, felt he had finally "made it" into an upper echelon of rock and roll, while most importantly retaining artistic integrity ("making him spiritually closer to Bob Dylan than to, say, Bobby Darin", wrote biographer Marc Eliot). The duo chose William Morris as their booking agency after a recommendation from Wally Amos, a mutual friend through their producer, Tom Wilson.
During the sessions for "Parsley", the duo cut "A Hazy Shade of Winter"; it was released as a single, peaking at number 13 on the national charts. Similarly, they recorded "At the Zoo" for single release in early 1967 (it charted lower, at number 16). Simon began work for their next album around this time, noting to a writer at "High Fidelity" that "I'm not interested in singles anymore". He had hit a dry spell in his writing, which led to no Simon & Garfunkel album on the horizon for 1967. Artists at the time were expected to release two, perhaps three albums each year and the lack of productivity from the duo worried executives at Columbia Records. Amid concerns for Simon's idleness, Columbia Records chairman Clive Davis arranged for up-and-coming record producer John Simon to kick-start the recording. Simon was distrustful of "suits" at the label; on one occasion, he and Garfunkel brought a tape recorder into a meeting with Davis, who was giving a "fatherly talk" on speeding up production, in order to laugh at it later.
Meanwhile, director Mike Nichols, then filming "The Graduate", had become fascinated with the duo's past two efforts, listening to them nonstop before and after filming. After two weeks of this obsession, he met with Clive Davis to ask for permission to license Simon & Garfunkel music for his film. Davis viewed it as a perfect fit and envisioned a best-selling soundtrack album. Simon was not as immediately receptive, viewing movies akin to "selling out", creating a damper on his artistic integrity. However, after meeting Nichols and becoming impressed by his wit and the script, he agreed to write at least one or two new songs for the film. Leonard Hirshan, a powerful agent at William Morris, negotiated a deal that paid Simon $25,000 to submit three songs to Nichols and producer Lawrence Turman. Several weeks later, Simon re-emerged with two new tracks, "Punky's Dilemma" and "Overs", neither of which Nichols was particularly taken with. The duo offered another new song, which later became "Mrs. Robinson", that was not as developed. Nichols loved it.
Studio time and low profile (1967–68).
The duo's fourth studio album, "Bookends", was recorded in fits and starts over various periods from late 1966 to early 1968. The duo were signed under an older contract that specified the label pay for sessions, and Simon & Garfunkel took advantage of this indulgence, hiring viola and brass players, as well as percussionists. The record's brevity reflects its concise and perfectionist production. The team spent over 50 studio hours recording "Punky's Dilemma", for example, and re-recorded vocal parts, sometimes note by note, until they were satisfied. While Garfunkel's songs and voice took a lead role on some songs, the harmonies the band were known for gradually disappeared. For Simon, "Bookends" represented the end of the duo and became an early indicator of his intentions to go solo. Although the album had been planned long in advance, work did not begin in earnest until the late months of 1967.
Prior to release, the band helped put together and performed at the Monterey Pop Festival, which signaled the beginning of the Summer of Love on the West Coast. "Fakin' It" was issued as a single that summer and found only modest success on AM radio; the duo were much more focused on the rising FM format, which played album cuts and treated their music with respect. In January 1968, the duo appeared on a Kraft Music Hall special, "Three for Tonight", performing ten songs largely culled from their third album. "Bookends" was released by Columbia Records in April 1968. In a historical context, this was just 24 hours before the assassination of civil rights activist Martin Luther King, Jr., which spurred nationwide outrage and riots. The album debuted on the "Billboard" Top LPs in the issue dated April 27, 1968, climbing to number one and staying at that position for seven non-consecutive weeks; it remained on the chart as a whole for 66 weeks. "Bookends" received such heavy orders weeks in advance of its release that Columbia was able to apply for award certification before copies left the warehouse, a fact it touted in magazine ads. At one point in 1968, the duo held down the top three positions on the chart, with "Bookends", "The Graduate", and "Parsley". The record became the duo's best-selling album to date: it fed off the buzz created by the release of "The Graduate" soundtrack album ten weeks earlier, creating an initial combined sales figure of over five million units.
Davis had predicted this fact, and suggested raising the list price of "Bookends" by one dollar to $5.79, above the then standard retail price, to compensate for including a large poster included in vinyl copies. Simon instead scoffed and viewed it as charging a premium on "what was sure to be that year's best-selling Columbia album". According to biographer Marc Eliot, Davis was "offended by what he perceived as their lack of gratitude for what he believed was his role in turning them into superstars". Rather than implement Davis' price increase plan, Simon & Garfunkel signed a contract extension with Columbia that guaranteed them a higher royalty rate. Lead single "Mrs. Robinson" became, at the 1969 Grammy Awards the first rock and roll song to receive Record of the Year; it was also awarded Best Contemporary Pop Performance by a Duo or Group.
Growing apart and final years (1969–70).
"Bookends", alongside "The Graduate" soundtrack, propelled Simon & Garfunkel to become the biggest rock duo in the world.
 Simon was approached by producers to write music for films or license songs; he turned down Franco Zeffirelli, who was preparing to film "Brother Sun, Sister Moon", and John Schlesinger, who likewise was readying to shoot "Midnight Cowboy". In addition to Hollywood proposals, producers from the Broadway show "Jimmy Shine" (starring Simon's friend Dustin Hoffman, also the lead in "Midnight Cowboy") asked for two original songs and Simon declined.
 He collaborated briefly with Leonard Bernstein on a sacred mass before withdrawing from the project due to "finding it perhaps too far afield from his comfort zone". Garfunkel took the role of Captain Nately in the Nichols film, "Catch-22", based on the Catch-22 novel. Initially Simon was to play the character of Dunbar, but screenwriter Buck Henry felt the film was already crowded with characters and subsequently wrote Simon's part out.
The filming of "Catch-22" began in January 1969 and lasted about eight months. The unexpectedly long film production endangered the relationship between the duo; Simon had not completed any new songs at this point, and the duo planned to collaborate when the filming would be finished. Following the end of filming of "Catch-22" in October, the first performance of what was, for a time, their last tour, took place in Ames, Iowa. The US leg of the tour ended in the sold-out Carnegie Hall on November 27. After breaking for Christmas, the duo continued working on the album in early 1970 and finished it in late January. Meanwhile, the duo, working with director Charles Grodin, produced an hourlong CBS special, "Songs of America", which is a mixture of scenes featuring notable political events and leaders concerning the USA, such as the Vietnam War, Martin Luther King, John F. Kennedy's funeral procession, Cesar Chavez and the Poor People's March. It was broadcast only once, due to internal tension at the network regarding its content.
"Bridge over Troubled Water", their final studio album, was released in January 1970 and charted in over 11 countries, topping the charts in 10, including the "Billboard" Top LP's chart in the US and the UK Albums Chart. It was the best-selling album in 1970, 1971 and 1972 and was at that time the best-selling album of all time, a position it kept until outsold by Michael Jackson's "Thriller" (1982). The album topped the "Billboard" charts for 10 weeks and stayed in the charts for 85 weeks. In the United Kingdom, the album topped the charts for 35 weeks, and spent 285 weeks in the top 100, from 1970 to 1975. It has since sold over 25 million copies worldwide. "Bridge over Troubled Water", the album's lead single, hit number one in five countries and became their biggest seller. The song has been covered by over 50 artists since then, including Elvis Presley and Johnny Cash.
 "Cecilia", the follow-up, hit number four in the US, and "El Condor Pasa" hit number 18.
The recording process was tough for both musicians, and their breakup was almost certain considering the deterioration of their relationship. "At that point, I just wanted out," Simon later said.
 Their breakup was not intended to be semi-permanent: Garfunkel hoped for a two-year break from Simon & Garfunkel and did not intend to pursue a film-career. Likewise, Simon did not intend to begin a solo career. A brief British tour followed the album release, and the duo's last concert as Simon & Garfunkel occurred at Forest Hills Stadium.
 In 1971, the album took home six awards at the 13th Annual Grammy Awards, including Album of the Year. Simon's wife, Peggy Harper, pushed for him to make the split official, and he placed a call to Davis to confirm the duo's breakup: "I want you to know I’ve decided to split with Artie. I don’t think we’ll be recording together again."
 For the next several years, the duo would only speak "two or three" times a year.
Breakup, rifts, and reunions (1971–2003).
In the 1970s, the duo reunited several times. Their first reunion was a benefit concert for presidential candidate George McGovern at New York's Madison Square Garden in June 1972. In 1975, they reconciled once more when they visited a recording session with John Lennon and Harry Nilsson.
 For the rest of the year, they attempted to make the reunion work, but their collaboration only yielded one song, "My Little Town," that was featured on Simon's "Still Crazy After All These Years" and Garfunkel's "Breakaway". It peaked at number nine on the Hot 100. In 1975, Garfunkel joined Simon for a medley of three songs on the television series "Saturday Night Live" which Simon was guest hosting. In 1977, Garfunkel joined Simon for a brief performance of their old songs on Simon's television special "The Paul Simon Special", and later that year they recorded a cover of Sam Cooke's "(What a) Wonderful World" along with James Taylor. Old tensions finally appeared to dissipate upon Garfunkel's return to New York in 1978, when the duo began interacting more often. On May 1, 1978, Simon joined Garfunkel for a concert held at Carnegie Hall to benefit the hearing disabled.
By 1980, the duo's respective solo efforts were not doing well. To help alleviate New York's economic decline, concert promoter Ron Delsener came up with the idea to throw a free concert in Central Park.
 Delsener contacted Simon with the idea of a Simon & Garfunkel reunion, and once Garfunkel agreed, plans were made.
 The Concert in Central Park, performed September 19, 1981, attracted more than 500,000 people, at that time the largest-ever concert attendance. Warner Bros. Records released a live album of the show that went double platinum in the US. A 90-minute recording of the concert was sold to the Home Box Office (HBO) for over one million.
 The concert created a renewed interest in the duo's work.
 They had several "heart-to-heart talks," attempting to put past issues behind them. The duo planned a world tour, kicking off in May 1982, but their relationship grew contentious: for the majority of the tour, they didn't speak to one another.
 Warner Bros. pushed for them to extend the tour and release an all-new Simon & Garfunkel studio album.
After recording several vocal tracks for a possible new Simon & Garfunkel album, Simon decided to adopt it as his own solo album. Garfunkel had refused to learn the songs in the studio, and would not give up cannabis and cigarettes, despite Simon's requests.
 An official spokesperson remarked, "Paul simply felt the material he wrote is so close to his own life that it had to be his own record. Art was hoping to be on the album, but I'm sure there will be other projects that they will work on together. They are still friends." The material was later released on Simon's 1983 effort "Hearts and Bones". Another rift opened between the duo when the lengthy recording of Simon's 1986 album "Graceland" prevented Garfunkel from working with Roy Halee on a Christmas album.
 In 1990, the duo were inducted into the Rock and Roll Hall of Fame. Garfunkel thanked his partner, calling him "the person who most enriched my life by putting those songs through me," to which Simon responded, "Arthur and I agree about almost nothing. But it's true, I have enriched his life quite a bit." After three songs, the duo left without speaking.
We are indescribable. You'll never capture it. It's an ingrown, deep friendship. Yes, there is deep love in there. But there's also shit.
”
Garfunkel describing his six-decade-long friendship with Simon
By 1993, their relationship had thawed again, and Simon invited Garfunkel on an international tour with him.
 Following a 21-date, sold-out run at the Paramount Theater in New York and an appearance at that year's Bridge School Benefit in California, the duo toured the Far East. The duo had a falling out over the course of the rest of the decade, the details of which have never been disclosed. Simon thanked Garfunkel at his 2001 induction into the Rock and Roll Hall of Fame as a solo artist: "I regret the ending of our friendship. I hope that some day before we die we will make peace with each other," resuming after a pause, "No rush." They were awarded a Lifetime Achievement Award at the 45th Annual Grammy Awards in 2003, for which the promoters convinced them to reconcile and open the show with a performance of "The Sound of Silence." The performance was satisfying for both musicians, and they planned out a full-scale reunion tour over the summer. The Old Friends tour began in October 2003 and played to sold-out audiences across the United States for 30 dates until mid-December. The tour earned an estimated $123 million. Following a twelve-city run in Europe in 2004, they ended their nine-month tour with a free concert at the Colosseum in Rome. It attracted 600,000 fans, more than their The Concert in Central Park.
Recent years (2009–present).
In 2009, the duo reunited again for three songs during Simon's two-night arrangement at New York's Beacon Theatre. This led to a reunion tour of Asia and Australia in June 2009. Their headlining set at the 2010 New Orleans Jazz and Heritage Festival was very difficult for Garfunkel, who was experiencing serious vocal problems. "I was terrible, and crazy nervous. I leaned on Paul Simon and the affection of the crowd," he told "Rolling Stone" several years later. Garfunkel was diagnosed with vocal cord paresis, and the remaining tour dates were postponed indefinitely. His manager, John Scher, informed Simon's camp that Garfunkel would be ready within a year, which did not happen, leading to poor relations between the two. He regained his vocal strength over the course of the next four years, performing shows in a Harlem theater and to underground audiences.
Despite this, the duo have not staged a full-scale tour or performed shows since 2010. Garfunkel confirmed to "Rolling Stone" in 2014 that he believes they will tour in the future, although Simon had been too "busy" in recent years. "I know that audiences all over the world like Simon and Garfunkel. I'm with them. But I don't think Paul Simon's with them," he remarked.
Musical style and legacy.
Over the course of their career, Simon & Garfunkel's music gradually moved from a very basic, folk rock sound to incorporate more experimental elements for the time, including Latin and gospel music. They are considered the most popular folk rock act from the 1960s, and one of the most popular artists from the decade in general. Their music made a "deep impression" on the baby boomer generation, and they are considered alongside artists such as Bob Dylan and the Beatles to be representative of the era's cultural movements. Many adolescents of the period found their music relevant, while adults regarded them as intelligent. Their music, according to "Rolling Stone", struck a chord among lonely, adrift young adults near the end of the 1960s.
Despite their popularity, the group were not without criticism, especially in their heyday. Upon their arrival on the popular music scene, many considered them a manufactured imitation of folk. Their rather clean sound and muted lyricism made them unpopular among hippies in some circles. Richie Unterberger of AllMusic writes that the duo "inhabited the more polished end of the folk-rock spectrum and was sometimes criticized for a certain collegiate sterility." Many critics would later regard Simon's lyricism in his work with Simon & Garfunkel to pale in comparison to his later solo material. In their later years, they became famous for their rocky relationship, which included "breaking up and making up about every dozen years."
Awards.
The Grammy Awards are awarded annually by the National Academy of Recording Arts and Sciences. Simon & Garfunkel have won 10 total awards.
References.
</dl>

</doc>
<doc id="26824" url="http://en.wikipedia.org/wiki?curid=26824" title="State Street Corporation">
State Street Corporation

State Street Corporation, known as State Street, is a US-based international financial services holding company. State Street was founded in 1792 and is the second oldest financial institution in the United States. The company’s headquarters are at One Lincoln Street in Boston and it has offices in 25 countries around the world.
State Street is organized into three main divisions. The Global Services business is a custodian bank with $28 trillion (USD) of assets under custody and administration. The Global Advisors business provides investment management services and has $2.3 trillion (USD) of assets under management. The Global Markets business offers investment research and trading services to institutional investors.
History.
State Street’s past can be dated back to the founding years of Boston’s banking industry. In 1792 the Union Bank became the third bank to be chartered in Boston and was located at the corner of State and Exchange Streets. State Street was known as the “Great Street to the Sea” as Boston became a flourishing maritime capital. The clipper in State Street’s logo today reflects this period.
In 1865 the Union Bank received a national charter and became the National Union Bank of Boston. State Street Deposit & Trust Co opened alongside National Union in 1891. It became the custodian of the first US mutual fund in 1924, the Massachusetts Investors Trust. State Street and National Union merged in 1925.
State Street’s growth during the mid-1900s was fuelled by mergers and acquisitions. It merged with the Second National Bank in 1955 and with the Rockland-Atlas National Bank in 1961. William Edgerly gained control in 1975 and shifted the company’s strategy from commercial banking to investments and securities processing.
The company began investing heavily in technologies for securities management and custodian processing. It was helped by a partial acquisition of Boston Financial Data Services in 1973. More than 100 top staff from IBM were headhunted by State Street as it set about implementing IBM mainframe systems.
State Street’s new building was completed in 1966 and became the first high-rise office tower in downtown Boston. In 1972 the company opened its first international office in Munich. For much of the 1980s and 1990s it expanded to foreign markets with offices in Montreal, Toronto, Dublin, London, Paris, Dubai, Sydney, Wellington, Hong Kong, and Tokyo.
It was the early 1990s before State Street brought its technology platform to international markets. By 1992 most of State Street’s revenue came from fees for holding securities, settling trades, keeping records, and performing accounting. It formed a new global asset management business in 1994 and in 1999 divested its retail and commercial banking businesses to Citizens Financial Group.
State Street acquired Kansas City, Missouri-based Investors Fiduciary Trust Co. in 1995 for $162 million (USD) from DST Systems, and Kemper Financial Services. In 2003 it purchased Deutsche Bank’s securities services division for $1.5 billion (USD). State Street purchased Investors Financial Services for $4.5 billion (USD) in 2007. In 2010 it acquired Mourant International Finance Administration and the securities services group of Intesa Sanpaolo.
State Street was named by the G-20 as amongst the world’s 29 systemic banks and must meet all conditions of the Basel III accord. The company now employs 29,530 people around the world. It claims to have funds under management of $2.3 trillion (USD) and assets under custody and administration of $28 trillion (USD), second to The Bank of New York Mellon.
Organization.
State Street Global Advisors.
Global Advisors is State Street’s asset management business and dates back to 1978. It provides investment management, research, and advisory services to corporations, mutual funds, insurance companies, and other institutional investors. Global Advisors develops both passive and active management strategies using both quantitative and fundamental approaches.
It created the first exchange-traded fund in 1993, the SPDR S&P 500, and is now one of the world’s largest ETF providers. Global Advisors has staff in 27 global offices and claims to have over $2.3 trillion (USD) of funds under management.
In November 2014, State Street Global Advisors sold SSARIS to senior management.
State Street Global Markets.
Global Markets is State Street’s securities business. It offers research, trading, and securities lending services for foreign exchange, equities, fixed income, and derivatives. The company claims to be a trading partner free from conflicted interests as it does not run proprietary trading books. Global Markets maintains trading desks in Boston, London, Sydney, Toronto, and Tokyo.
State Street Global Services.
Global Services is the investment servicing division of State Street, also known as the State Street Bank & Trust Co. It provides asset owners and managers with custodian (safekeeping, corporate actions), fund accounting (pricing and valuation), and administration (financial reporting, tax, compliance, and legal) services.
Global Services handles assets from many classes, including equities, derivatives, exchange-traded funds, fixed income assets, private equity, and real estate. State Street now administers 40 percent of the assets under administration in the US mutual fund market. Global Services also provides outsourcing for operations activities and handles $10.2 trillion (USD) of middle-office assets.
Regulation.
State Street is registered with the Board of Governors of the Federal Reserve System as a bank holding company pursuant to the Bank Holding Company Act of 1956. It is a member of the Federal Reserve System and its deposits are insured by the Federal Deposit Insurance Corporation. Certain aspects of State Street’s public disclosure are subject to the requirements of the Sarbanes–Oxley Act of 2002.
State Street’s broker-dealer operation, known as Global Markets, is registered with and regulated by the Securities and Exchange Commission and the New York Stock Exchange in the United States. The Prudential Regulation Authority and the London Stock Exchange regulate State Street in the United Kingdom.
Controversies.
In 2009 the State of California alleged on behalf of its pension funds CalPERS and CalSTRS that State Street had committed fraud on currency trades handled by the custodian bank. Two executives from State Street Global Markets left the company in October 2011 following enquiries over the pricing of a fixed income transaction.
State Street in December 2010 announced that it would be retrenching 5% of its workforce and effectively reducing the wages of remaining employees by 10%. In March 2011 it reversed its wage-reduction decision but declared that it would still require all employees to work a longer 40 hour week.
On 28 February 2012, State Street Global Advisors entered into a consent order with the Massachusetts Securities Division. The Division was investigating SSGA’s role as the investment manager of a $1.65 billion (USD) hybrid collateralized debt obligation. The investigation resulted in a fine of $5 million (USD) for the non-disclosure of certain initial investors taking a short position on portions of the CDO.
State Street Bank has been accused of "stealth outsourcing" or transferring American jobs to their outsourcing partners Syntel and HCL-India under the radar, in small increments to avoid any political backlash. State Street Bank - the second largest client of Syntel - also has a joint venture in the Indian city of Pune with them which they have the option to buy out. The controversy is compounded by the fact State Street bank received an $11.5 million tax incentive from the city of Boston to move into a new location in the South Boston Innovation District as well as 2 billion in TARP assistance all while still sending jobs overseas.
During the May 2012 annual shareholders meeting, chairman and chief executive Jay Hooley was shouted down on numerous occasions by protesters in relation to the outsourcing and other grievances.
See also.
"State Street Bank v. Signature Financial Group" is the landmark case in which the Court of Appeals for the Federal Circuit ruled (23 July 1998) that a computer algorithm can be patented to the extent that it produces "a useful, concrete and tangible result".

</doc>
<doc id="26825" url="http://en.wikipedia.org/wiki?curid=26825" title="Spanish language">
Spanish language

Spanish (, "español"), also called Castilian (,   ), is a Romance language that originated in the Castile region of Spain. More than 400 million people speak Spanish as a native language, making it second only to Mandarin in terms of its number of native speakers worldwide. There are an estimated 470 million Spanish speakers with native competence and 548 million Spanish speakers as a first or second language, including speakers with limited competence and 20 million students of Spanish as a foreign language. It is the third language by total speakers behind English and Mandarin. Spanish is one of the six official languages of the United Nations, and it is used as an official language by the European Union, the Organization of American States, and the Union of South American Nations, among many other international organizations.
Spanish is a part of the Ibero-Romance group of languages, which evolved from several dialects of common Latin in Iberia after the collapse of the Western Roman Empire in the 5th century. It was first documented in central-northern Iberia in the 9th century and gradually spread with the expansion of the Kingdom of Castile into central and southern Iberia. Beginning in the early 16th century, Spanish was taken to the colonies of the Spanish Empire, most notably to the Americas, as well as territories in Africa, Oceania and the Philippines.
From its beginnings, Spanish vocabulary was influenced by its contact with Basque, as well as by neighboring Ibero-Romance languages, and later it absorbed many Arabic words during the Muslim presence in the Iberian Peninsula. It also adopted words from non-Iberian languages, particularly the Romance languages Occitan, French, Italian and Sardinian, as well as from Nahuatl and other Indigenous languages of the Americas. 
Spanish is the official or national language of 19 countries in the Americas, totaling at least 418 million native speakers in the Hemisphere. In the European Union, Spanish is the mother tongue of 8% of the population, with an additional 7% speaking it as a second language. Spanish is the most popular second language learned in the United States.
Names of the language.
In Spain and in some other parts of the Spanish-speaking world, Spanish is called "castellano" (Castilian) as well as "español" (Spanish), that is, the language of the region of Castile, contrasting it with other languages spoken in Spain such as Galician, Basque and Catalan.
The Spanish Constitution of 1978 uses the term "castellano" to define the official language of the whole Spanish State, in contrast to "las demás lenguas españolas" (lit. "the rest of the Spanish languages"). Article III reads as follows:
"El castellano es la lengua española oficial del Estado. ... Las demás lenguas españolas serán también oficiales en las respectivas Comunidades Autónomas..."<br>
Castilian is the official Spanish language of the State. ... The rest of the Spanish languages shall also be official in their respective Autonomous Communities...
The Spanish Royal Academy, on the other hand, currently uses the term "español" in its publications but from 1713 to 1923 called the language "castellano".
The "Diccionario panhispánico de dudas" (a language guide published by the Spanish Royal Academy) states that, although the Spanish Royal Academy prefers to use the term "español" in its publications when referring to the Spanish language, both terms, "español" and "castellano", are regarded as synonymous and equally valid.
Two etymologies for "español" have been suggested. The Spanish Royal Academy Dictionary derives the term from the Provençal word "espaignol", and that in turn from the Medieval Latin word "Hispaniolus", 'from—or pertaining to—Hispania'. Other authorities attribute it to a supposed medieval Latin *"hispaniōne", with the same meaning.
History.
The Spanish language evolved from Vulgar Latin (colloquial Latin), which was brought to the Iberian Peninsula by the Romans during the Second Punic War, beginning in 210 BC. Previously, several pre-Roman languages (also called Paleohispanic languages)—unrelated to Latin, and some of them unrelated even to Indo-European—were spoken in the Iberian Peninsula. These languages included Basque (still spoken today), Iberian, Celtiberian and Celtic. Traces of Basque especially, can be found in the Spanish vocabulary today, mainly in place names.
The first documents to record what is today regarded as the precursor of modern Spanish are from the 9th century (see "Glosas Emilianenses"). Throughout the Middle Ages and into the modern era, by far the most important influence on the Spanish lexicon came from neighboring Romance languages—Navarro-Aragonese, Leonese, Aragonese, Basque, Catalan, Portuguese, Galician, Mirandese, Occitan, Gascon, and later, French and Italian—but also from Arabic, and a few words from the Germanic languages. Many words were borrowed from Latin through the influence of written language and the liturgical language of the Church.
Local sociolects of Vulgar Latin evolved into Spanish in the north of Iberia, in an area defined by Álava, Cantabria, Burgos, Soria and La Rioja. The dialect was later brought to the city of Toledo, where the written standard of Spanish was first developed, in the 13th century. In this formative stage, Spanish (Castilian) developed a strongly differing variant from its close cousin, Leonese, and, according to some authors, was distinguished by a heavy Basque influence (see Iberian Romance languages). This distinctive dialect progressively spread south with the advance of the "Reconquista", and so gathered a sizable lexical influence from the Arabic of Al-Andalus, much of it indirectly, through the Romance Mozarabic dialects (some 4,000 Arabic-derived words, make up around 8% of the language today). The written standard for this new language began to be developed in the cities of Toledo, in the 13th to 16th centuries, and Madrid, from the 1570s.
The development of the Spanish sound system from that of Vulgar Latin exhibits most of the changes that are typical of Western Romance languages, including lenition of intervocalic consonants (thus Latin vīta > Spanish "vida"). The diphthongization of Latin stressed short e and o—which occurred in open syllables in French and Italian, but not at all in Catalan or Portuguese—is found in both open and closed syllables in Spanish, as shown in the following table:
Spanish is marked by the palatalization of the Latin double consonants nn and ll (thus Latin
annum > Spanish "año", and Latin anellum > Spanish
"anillo").
The consonant written "u" or "v" in Latin and pronounced [w] in Classical Latin had probably "fortified" to a bilabial fricative /β/ in Vulgar Latin. In early Spanish (but not in Catalan or Portuguese) it merged with the consonant written "b" (a bilabial with plosive and fricative allophones). In modern Spanish, there is no difference between the pronunciation of orthographic "b" and "v", with some exceptions in Caribbean Spanish.
Peculiar to Spanish (as well as to the neighboring Gascon dialect of Occitan, and attributed to a Basque substratum) was the mutation of Latin initial "f" into "h-" whenever it was followed by a vowel that did not diphthongize. The "h-", still preserved in spelling, is now silent in most varieties of the language, although in some Andalusian and Caribbean dialects it is still aspirated in some words. This is the reason why there are modern spelling variants "Fernando" and "Hernando" (both Spanish for "Ferdinand"), "ferrero" and "herrero" (both Spanish for "smith"), "fierro" and "hierro" (both Spanish for "iron"), and "fondo" and "hondo" (both Spanish for "deep", but "fondo" means "bottom" while "hondo" means "deep"); "hacer" (Spanish for "to make") is the root word of "satisfacer" (Spanish for "to satisfy"), and "hecho" ("made") is the root word of "satisfecho" (Spanish for "satisfied").
Compare the examples in the following table:
Some consonant clusters of Latin also produced characteristically different results in these languages, as shown in the examples in the following table:
In the 15th and 16th centuries, Spanish underwent a dramatic change in the pronunciation of its sibilant consonants, known in Spanish as the "", which resulted in the distinctive velar [x] pronunciation of the letter ⟨j⟩ and—in a large part of Spain—the characteristic interdental [θ] ("th-sound") for the letter ⟨z⟩ (and for ⟨c⟩ before ⟨e⟩ or ⟨i⟩). See History of Spanish (Modern development of the Old Spanish sibilants) for details.
The "Gramática de la lengua castellana", written in Salamanca in 1492 by Elio Antonio de Nebrija, was the first grammar written for a modern European language. According to a popular anecdote, when Nebrija presented it to Queen Isabella I, she asked him what was the use of such a work, and he answered that language is the instrument of empire. In his introduction to the grammar, dated August 18, 1492, Nebrija wrote that "... language was always the companion of empire."
From the sixteenth century onwards, the language was taken to America and the Spanish East Indies via Spanish colonization of America. Miguel de Cervantes Saavedra, author of "Don Quixote", is such a well-known reference in the world that Spanish is often called "la lengua de Cervantes" ("the language of Cervantes").
In the twentieth century, Spanish was introduced to Equatorial Guinea and the Western Sahara, and to areas of the United States that had not been part of the Spanish Empire, such as Spanish Harlem in New York City. For details on borrowed words and other external influences upon Spanish, see Influences on the Spanish language.
Grammar.
Spanish is a relatively inflected language, with a two-gender noun system and about fifty conjugated forms per verb, but with inflection of nouns, adjectives, and determiners limited to number and gender. (For a detailed overview of verbs, see Spanish verbs and Spanish irregular verbs.) Spanish syntax is considered right-branching, meaning that subordinate or modifying constituents tend to be placed after their head words. The language uses prepositions (rather than postpositions or inflection of nouns for case), and usually—though not always—places adjectives after nouns, as do most other Romance languages.
Its sentence structure is generally subject–verb–object, although variations are common. It is a "pro-drop", or "null-subject" language—that is, it allows the deletion of subject pronouns when they are pragmatically unnecessary. Spanish is described as a "verb-framed" language, meaning that the "direction" of motion is expressed in the verb while the "mode" of locomotion is expressed adverbially (e.g. "subir corriendo" or "salir volando"; the respective English equivalents of these examples—'to run up' and 'to fly out'—show that English is, by contrast, "satellite-framed", with mode of locomotion expressed in the verb and direction in an adverbial modifier).
Subject/verb inversion is not required in questions, and thus the recognition of declarative or interrogative may depend entirely on intonation.
Phonology.
Segmental phonology.
The Spanish phonemic inventory consists of five vowel phonemes (/a/, /e/, /i/, /o/, /u/) and 17 to 19 consonant phonemes (the exact number depending on the dialect). The main allophonic variation among vowels is the reduction of the high vowels /i/ and /u/ to glides—[j] and [w] respectively—when unstressed and adjacent to another vowel. Some instances of the mid vowels /e/ and /o/, determined lexically, alternate with the diphthongs [je] and [we] respectively when stressed, in a process that is better described as morphophonemic rather than phonological, as it is not predictable from phonology alone.
The Spanish consonant system is characterized by (1) three nasal phonemes, and one or two (depending on the dialect) lateral phoneme(s), which in syllable-final position lose their contrast and are subject to assimilation to a following consonant; (2) three voiceless stops and the affricate /tʃ/; (3) three or four (depending on the dialect) voiceless fricatives; (4) a set of voiced obstruents—/b/, /d/, /ɡ/, and sometimes /ʝ/—which alternate between approximant and plosive allophones depending on the environment; and (5) a phonemic distinction between the "tapped" and "trilled" "r"-sounds (single ⟨r⟩ and double ⟨rr⟩ in orthography).
In the following table of consonant phonemes, /θ/ and /ʎ/ are marked with an asterisk (*) to indicate that they are preserved only in some dialects. In most dialects they have been merged, respectively, with /s/ and /ʝ/, in the mergers called, respectively, "seseo" and "yeísmo". The phoneme /ʃ/ is in parentheses () to indicate that it appears only in loanwords. Each of the voiced obstruent phonemes /b/, /d/, /ʝ/, and /ɡ/ appears to the right of a "pair" of voiceless phonemes, to indicate that, while the "voiceless" phonemes maintain a phonemic contrast between plosive (or affricate) and fricative, the "voiced" ones alternate allophonically (i.e. without phonemic contrast) between plosive and approximant pronunciations.
The letters ⟨v⟩ and ⟨b⟩ normally represent the same phoneme, /b/, which is realized as [b] after a nasal consonant or a pause, and as [β] elsewhere, as in "ambos" [ˈambos] ('both') "envío" [emˈbi.o] ('I send'), "acabar" [akaˈβaɾ] ('to finish') and "mover" [moˈβeɾ] ('to move'). The Royal Spanish Academy considers the /v/ pronunciation for the letter ⟨v⟩ to be incorrect and affected. However, some Spanish speakers maintain the pronunciation of the /v/ sound as it is in other western European languages. The sound /v/ is used for the letter ⟨v⟩, in the Spanish language, by a few second-language speakers in Spain whose native language is Catalan, in the Balearic Islands, in the Valencian Community, and in southern Catalonia. In the USA it is also common because of the proximity and influence of English phonology, and the /v/ is also occasionally used in Mexico. Some parts of Central America also use /v/, which the Royal Academy attributes to the interference of local indigenous languages.
Historically, the /v/ pronunciation was uncommon, but considered correct well into the twentieth century.
Prosody.
Spanish is classified by its rhythm as a syllable-timed language, meaning that each syllable has approximately the same duration regardless of stress.
Spanish intonation varies significantly according to dialect, but generally conforms to a pattern of falling tone for declarative sentences and wh-questions (who, what, why, etc.), and rising tone for yes/no questions. There are no syntactic markers to distinguish between questions and statements, and thus the recognition of declarative or interrogative depends entirely on intonation.
Stress most often occurs on any of the last three syllables of a word, with some rare exceptions at the fourth-last or earlier syllables. The "tendencies" of stress assignment are as follows:
In addition to the many exceptions to these tendencies, there are numerous minimal pairs which contrast solely on stress such as "sábana" ('sheet') and "sabana" ('savannah'), as well as "límite" ('boundary'), "limite" ('[that] he/she limits') and "limité" ('I limited'), or also "líquido" ('liquid'), "liquido" ('I sell off') and "liquidó" ('he/she sold off').
The spelling system unambiguously reflects where the stress occurs: in the absence of an accent mark, the stress falls on the last syllable unless the last letter is ⟨n⟩, ⟨s⟩, or a vowel, in which cases the stress falls on the next-to-last syllable; if and only if the absence of an accent mark would give the wrong stress information, an acute accent mark appears over the stressed syllable.
Geographical distribution.
Spanish is the primary language of 20 countries worldwide. It is estimated that the combined total number of Spanish speakers is between 470 and 500 million, making it the second most widely spoken language in terms of native speakers.
Spanish is the third most spoken language by total number of speakers (after Mandarin and English). Internet usage statistics for 2007 show Spanish as the third most commonly used language on the Internet, after English and Mandarin.
Europe.
In Europe, Spanish is an official language of Spain, the country after which it is named and from which it originated. It is widely spoken in Gibraltar, although English is the official, international language. It is also commonly spoken in Andorra, although Catalan is the official language.
Spanish is also spoken by small communities in other European countries, such as the United Kingdom, France, and Germany. Spanish is an official language of the European Union. In Switzerland which had a massive influx of Spanish migrants in the 20th century, Spanish is the native language of 2.2% of the population.
The Americas.
Hispanic America.
Most Spanish speakers are in Hispanic America; of all countries with a majority of Spanish speakers, only Spain and Equatorial Guinea are outside the Americas. Nationally, Spanish is the official language—either "de facto" or "de jure"—of Argentina, Bolivia (co-official with Quechua, Aymara, Guarani, and 34 other languages), Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Honduras, Mexico, Nicaragua, Panama, Paraguay (co-official with Guaraní), Peru (co-official with Quechua, Aymara, and "the other indigenous languages"), Uruguay, and Venezuela. Spanish is co-official with English in Puerto Rico.
Spanish has no official recognition in the former British colony of Belize; however, per the 2000 census, it is spoken by 43% of the population. Mainly, it is spoken by the descendants of Hispanics who have been in the region since the seventeenth century; however, English is the official language.
Due to their proximity to Spanish-speaking countries, Trinidad and Tobago and Brazil have implemented Spanish language teaching into their education systems. The Trinidad government launched the "Spanish as a First Foreign Language" (SAFFL) initiative in March 2005. In 2005, the National Congress of Brazil approved a bill, signed into law by the President, making it mandatory for schools to offer Spanish as an alternative foreign language course in both public and private secondary schools in Brazil. In many border towns and villages (especially in the Uruguayan-Brazilian and Paraguayan-Brazilian border areas), a mixed language known as Portuñol is spoken.
United States.
According to 2006 census data, 44.3 million people of the U.S. population were Hispanic or Hispanic American by origin; 38.3 million people, 13 percent, of the population over five years old speak Spanish at home. The Spanish language has a long history and presence in the United States due to historic Spanish and later, Mexican administration over territories now forming the southwestern states as well as Florida, which was Spanish territory until 1821.
Spanish is by far the most common second language spoken and taught in the country, and with over 50 million total speakers, the United States is now the second largest Spanish-speaking country in the world after Mexico. While English is the "de facto" official language of the country, Spanish is often used in public services and notices at the federal and state levels. Spanish is also used in administration in the state of New Mexico. The language also has a strong influence in major metropolitan areas such as those of Los Angeles, Miami, San Antonio, New York, San Francisco, Dallas, and Phoenix; as well as more recently, Chicago, Las Vegas, Boston, Denver, Houston, Indianapolis, Philadelphia, Cleveland, Salt Lake City, Atlanta, Nashville, Orlando, Tampa, Raleigh and Baltimore-Washington, D.C. due to 20th and 21st century immigration.
Africa.
In Africa, Spanish is official (along with Portuguese and French) in Equatorial Guinea, as well as an official language of the African Union. In Equatorial Guinea, Spanish is the predominant language when native and non-native speakers (around 500,000 people) are counted, while Fang is the most spoken language by number of native speakers. However, Equatorial Guinea has historically had closer and longer ties with Portugal.
Spanish is also spoken in the integral territories of Spain in North Africa, which include the Spanish cities of Ceuta and Melilla, the Plazas de soberanía, and the Canary Islands, archipelago located just off the northwest coast of mainland Africa. But Ceuta and Melilla are integral territories of Spain, not Africa.
Within Northern Morocco, a former Spanish protectorate that is also geographically close to Spain, approximately 20,000 people speak Spanish as a second language, while Arabic is the "de jure" official language. A small number of Moroccan Jews also speak the Sephardic Spanish dialect Haketia (related to the Ladino dialect spoken in Israel). Spanish is spoken by some small communities in Angola because of the Cuban influence from the Cold War and in South Sudan among South Sudanese natives that relocated to Cuba during the Sudanese wars and returned in time for their country's independence.
In Western Sahara, formerly Spanish Sahara, Spanish was officially spoken during the late nineteenth and twentieth centuries. Today, Spanish in this disputed territory is maintained by populations of Sahrawi nomads numbering about 500,000 people, and is de facto official alongside Arabic in the Sahrawi Arab Democratic Republic, although this entity receives limited international recognition.
Asia-Pacific.
Spanish is present on Easter Island, as it was annexed as a Chilean province in 1888.
Spanish was an official language of the Philippines from the beginning of Spanish rule in 1565 to a constitutional change in 1973. During Spanish colonization (1565–1898), it was the language of government, trade and education, and spoken as a first language by Spaniards and educated Filipinos. In the mid-nineteenth century, the colonial government set up a free public education system with Spanish as the medium of instruction. This increased use of Spanish throughout the islands led to the formation of a class of Spanish-speaking intellectuals called the "Ilustrados". However, Spanish was never spoken by the majority of the population.
Despite American administration after the defeat of Spain in the Spanish–American War in 1898, the usage of Spanish continued in Philippine literature and press during the early years of American rule. Gradually, however, the American government began increasingly promoting the use of English, and it characterized Spanish as a negative influence of the past. Eventually, by the 1920s, English became the primary language of administration and education. But despite a significant decrease in influence and speakers, Spanish remained an official language of the Philippines when it became independent in 1946, alongside English and Filipino, a standardized version of Tagalog.
Spanish was removed from official status in 1973 under the administration of Ferdinand Marcos, but regained its status as an official language two months later under Presidential Decree No. 155, dated 15 March 1973. It remained an official language until 1987, with the ratification of the present constitution, in which it was re-designated as a voluntary and optional auxiliary language. In 2010, President Gloria Macapagal-Arroyo encouraged the reintroduction of Spanish-language teaching in the Philippine education system. But by 2012, the number of secondary schools at which the language was either a compulsory subject or an elective had become very limited. Today, despite government promotions of Spanish, less than 0.5% of the population report being able to speak the language proficiently. Estimates indicate that while around 3 million people can speak Spanish with varying degrees of competency, only around 439 thousand people can speak the language at a native level. Aside from standard Spanish, a Spanish-based creole language—Chavacano—developed in the southern Philippines. The number of Chavacano-speakers was estimated at 1.2 million in 1996. However, it is not mutually intelligible with Spanish. Speakers of the Zamboangueño variety of Chavacano were numbered about 360,000 in the 2000 census. The local languages of the Philippines also retain some Spanish influence, with many words being derived from Mexican Spanish, owing to the control of the islands by Spain through Mexico City until 1821, and then directly from Madrid until 1898.
Spanish was also used by the colonial governments and educated classes in the former Spanish East Indies, consisting of modern-day Guam, Northern Mariana Islands, Palau, and Micronesia, in addition to the Philippines. Spanish loan words are present in the local languages of these territories as a legacy of colonial rule. Today, Spanish is not spoken officially in any of these former Spanish territories.
Spanish speakers by country.
The following table shows the number of Spanish speakers in some 79 countries.
Dialectal variation.
There are important variations—phonological, grammatical, and lexical—in the spoken Spanish of the various regions of Spain and throughout the Spanish-speaking areas of the Americas.
The variety with the most speakers is Mexican Spanish. It is spoken by more than twenty percent of the world's Spanish speakers (more than 112 million of the total of more than 500 million, according to the table above). One of its main features is the reduction or loss of unstressed vowels, mainly when they are in contact with the sound /s/.
In Spain, northern dialects are popularly thought of as closer to the standard, although positive attitudes toward southern dialects have increased significantly in the last 50 years. Even so, the speech of Madrid, which has typically southern features such as yeísmo and s-aspiration, is the standard variety for use on radio and television. The educated Madrid variety has most influenced the written standard for Spanish.
Phonology.
The four main phonological divisions are based respectively on (1) the sound of the spelled ⟨s⟩, (2) the debuccalization of syllable-final /s/, (3) the phoneme /θ/ ("theta"), (4) and the phoneme /ʎ/ ("turned "y""),
Grammar.
The main grammatical variations between dialects of Spanish involve differing uses of pronouns: especially those of the second person and, to a lesser extent, the object pronouns of the third person.
Voseo.
Virtually all dialects of Spanish make the distinction between a formal and a familiar register in the second-person singular, and thus have two different pronouns meaning "you": "usted" in the formal, and either "tú" or "vos" in the familiar (and each of these three pronouns has its associated verb forms), with the choice of "tú" or "vos" varying from one dialect to another. The use of "vos" (and/or its verb forms) is called "voseo". In a few dialects, all three pronouns are used—"usted", "tú", and "vos"—denoting respectively formality, familiarity, and intimacy.
In "voseo", "vos" is the subject form ("vos decís", "you say") and the form for the object of a preposition ("voy con vos", "I'm going with you"), while the direct and indirect object forms, and the possessives, are the same as those associated with "tú": "Vos sabés que tus amigos te respetan" ("You know your friends respect you"). Additional examples: ""Vos te acostaste con el tuerto" (Gené Ulf [Arg. 1988]); "Lugar que odio" [...] "como te odio a vos" (Rossi María [C. Rica 1985]); "No cerrés tus ojos"" (Flores Siguamonta [Guat. 1993]).
The verb forms of "general voseo" are the same as those used with "tú" except in the present tense (indicative and imperative) verbs. The forms for "vos" generally can be derived from those of "vosotros" (the traditional second-person familiar "plural") by deleting the glide [i̯], or /d/, where it appears in the ending: "vosotros pensáis" > "vos pensás"; "vosotros volvéis" > "vos volvés", "pensad!" ("vosotros") > "pensá!" ("vos"), "volved!" ("vosotros") > "volvé!" ("vos") .
In Chilean "voseo" on the other hand, almost all verb forms are distinct from their standard "tú"-forms.
The use of the pronoun "vos" with the verb forms of "tú" (e.g. "vos piensas") is called "pronominal "voseo"". And conversely, the use of the verb forms of "vos" with the pronoun "tú" (e.g. "tú pensás" or "tú pensái") is called "verbal "voseo"". 
In Chile, for example, "verbal voseo" is much more common than the actual use of the pronoun "vos" which is often reserved for deeply informal situations.
Distribution in Spanish-speaking regions of the Americas.
Although "vos" is not used in Spain, it occurs in many Spanish-speaking regions of the Americas as the primary spoken form of the second-person singular familiar pronoun, although with wide differences in social consideration. Generally, it can be said that there are zones of exclusive use of "tuteo" in the following areas: almost all of Mexico, the West Indies, Panama, most of Peru and Venezuela, coastal Ecuador and the Caribbean coast of Colombia.
"Tuteo" (the use of "tú") as a cultured form alternates with "voseo" as a popular or rural form in Bolivia, in the north and south of Peru, in Andean Ecuador, in small zones of the Venezuelan Andes (and most notably in the Venezuelan state of Zulia), and in a large part of Colombia. Some researchers maintain that "voseo" can be heard in some parts of eastern Cuba, and others assert that it is absent from the island.
"Tuteo" exists as the second-person usage with an intermediate degree of formality alongside the more familiar "voseo" in Chile, in the Venezuelan state of Zulia, on the Caribbean coast of Colombia (Montería, Sincelejo, Cartagena, Barranquilla, Riohacha and Valledupar), in the Azuero Peninsula in Panama, in the Mexican state of Chiapas, and in parts of Guatemala.
Areas of generalized "voseo" include Argentina, Costa Rica, eastern Bolivia, El Salvador, Guatemala, Honduras, Nicaragua, Paraguay, Uruguay and the Colombian departments of Antioquia (the second largest in population), Caldas, Risaralda, Quindio, and parts of The Valle del Cauca department.
Ustedes.
"Ustedes" functions as formal and informal second person plural in over 90% of the Spanish-speaking world, including all of Hispanic America, the Canary Islands, and some regions of Andalusia. In Seville, Cadiz, and other parts of western Andalusia, the familiar form is constructed as "ustedes vais", using the traditional second-person plural form of the verb. Most of Spain maintains the formal/familiar distinction with "ustedes" and "vosotros" respectively.
Usted.
"Usted" is the usual second-person singular pronoun in a formal context, used to convey respect toward someone who is a generation older or is of higher authority ("you, sir"/"you, ma'am"). It is also used in a "familiar" context by many speakers in Colombia and Costa Rica, and in parts of Ecuador and Panama, to the exclusion of "tú" or "vos". This usage is sometimes called "" in Spanish.
In Central America, especially in Honduras, "usted" is often used as a formal pronoun to convey respect between the members of a romantic couple. "Usted" is also used in this way, as well as between parents and children, in the Andean regions of Ecuador, Colombia and Venezuela.
Third-person object pronouns.
Most speakers use (and the "Real Academia Española" prefers) the pronouns "lo" and "la" for "direct" objects (masculine and feminine respectively, regardless of animacy, meaning "him", "her", or "it"), and "le" for "indirect" objects (regardless of gender or animacy, meaning "to him", "to her", or "to it"). This usage is sometimes called "etymological", as these direct and indirect object pronouns are a continuation, respectively, of the accusative and dative pronouns of Latin, the ancestor language of Spanish.
Deviations from this norm (more common in Spain than in the Americas) are called ""leísmo", "loísmo", or "laísmo"", according to which respective pronoun—"le", "lo", or "la"—has expanded beyond the etymological usage (i.e. "le" as a direct object, or "lo" or "la" as an indirect object).
Vocabulary.
Some words can be different, even significantly so, in different Hispanophone countries. Most Spanish speakers can recognize other Spanish forms, even in places where they are not commonly used, but Spaniards generally do not recognize specifically American usages. For example, Spanish "mantequilla", "aguacate" and "albaricoque" (respectively, 'butter', 'avocado', 'apricot') correspond to "manteca", "palta", and "damasco", respectively, in Argentina, Chile (except "manteca"), Paraguay, Peru (except "manteca" and "damasco"), and Uruguay.
The everyday Spanish words "coger" ('to take'), "pisar" ('to step on') and "concha" ('seashell') are considered extremely rude in parts of Hispanic America, where the meaning of "coger" and "pisar" is also "to have sex" and "concha" means "vagina". The Puerto Rican word for "bobby pin" ("pinche") is an obscenity in Mexico, but in Nicaragua it simply means "stingy", and in Spain refers to a chef's helper. Other examples include "taco", which means "swearword" (among other meanings) in Spain, "traffic jam" in Chile and "heels" (shoe) in Argentina and Peru but is known to the rest of the world as a Mexican dish.
"Pija" in many countries of Hispanic America and Spain itself is an obscene slang word for "penis", while in Spain the word also signifies "posh girl" or "snobby". "Coche", which means "car" in Spain, central Mexico and Argentina, for the vast majority of Spanish-speakers actually means "baby-stroller" or "pushchair", while "carro" means "car" in some Hispanic American countries and "cart" in others, as well as in Spain. "Papaya" is the slang term for "vagina" in parts of Cuba and Venezuela, where the fruit is instead called "fruta bomba" and "lechosa", respectively. Also, in Argentina and Spain, one would say "piña" when talking about punching someone else (as an alternate, slang usage), whereas in other countries, "piña" only refers to a pineapple.
Relation to other languages.
Spanish is closely related to the other West Iberian Romance languages, including Asturian, Aragonese, Galician, Ladino, Leonese, Mirandese and Portuguese.
It is generally acknowledged that Portuguese- and Spanish-speakers can communicate, although with varying degrees of difficulty.
Meanwhile, mutual intelligibility of the "written" Spanish and Portuguese languages is somewhat higher, given that the difficulties of the spoken forms are based more on phonology than on grammatical and lexical dissimilarities. "Ethnologue" gives estimates of the lexical similarity between related languages in terms of precise percentages. For Spanish and Portuguese, that figure is 89%. Italian, on the other hand—although its phonology is more similar to that of Spanish—is said to have a lexical similarity of 82%. Mutual intelligibility between Spanish and French or between Spanish and Romanian is lower still, given lexical similarity ratings of 75% and 71% respectively. And comprehension of Spanish by French speakers who have not studied the language is much lower, at an estimated 45%. In general, thanks to the common features of the writing systems of the Romance languages, interlingual comprehension of the written word is greater than that of oral communication.
The following table compares the forms of some common words in several Romance languages:
Judaeo-Spanish.
Judaeo-Spanish, also known as Ladino, is a variety of Spanish which preserves many features of medieval Spanish and Portuguese is spoken by descendants of the Sephardi Jews who were expelled from Spain in the fifteenth century. Conversely, in Portugal the vast majority of the Portuguese Jews converted and became 'New Christians'. Therefore, its relationship to Spanish is comparable with that of the Yiddish language to German. Ladino speakers today are almost exclusively Sephardi Jews, with family roots in Turkey, Greece, or the Balkans, and living mostly in Israel, Turkey, and the United States, with a few communities in Hispanic America. Judaeo-Spanish lacks the Native American vocabulary which was acquired by standard Spanish during the Spanish colonial period, and it retains many archaic features which have since been lost in standard Spanish. It contains, however, other vocabulary which is not found in standard Spanish, including vocabulary from Hebrew, French, Greek and Turkish, and other languages spoken where the Sephardim settled.
Judaeo-Spanish is in serious danger of extinction because many native speakers today are elderly as well as elderly "olim" (immigrants to Israel) who have not transmitted the language to their children or grandchildren. However, it is experiencing a minor revival among Sephardi communities, especially in music. In the case of the Latin American communities, the danger of extinction is also due to the risk of assimilation by modern Castilian.
A related dialect is Haketia, the Judaeo-Spanish of northern Morocco. This too tended to assimilate with modern Spanish, during the Spanish occupation of the region.
Writing system.
Spanish is written in the Latin script, with the addition of the character ⟨ñ⟩ ("eñe", representing the phoneme /ɲ/, a letter distinct from ⟨n⟩, although typographically composed of an ⟨n⟩ with a tilde) and the digraphs ⟨ch⟩ ("che", representing the phoneme /t͡ʃ/) and ⟨ll⟩ ("elle", representing the phoneme /ʎ/). However, the digraph ⟨rr⟩ ("erre fuerte", 'strong r', "erre doble", 'double r', or simply "erre"), which also represents a distinct phoneme /r/, is not similarly regarded as a single letter. Since 1994 ⟨ch⟩ and ⟨ll⟩ have been treated as letter pairs for collation purposes, though they remain a part of the alphabet. Words with ⟨ch⟩ are now alphabetically sorted between those with ⟨cg⟩ and ⟨ci⟩, instead of following ⟨cz⟩ as they used to. The situation is similar for ⟨ll⟩.
Thus, the Spanish alphabet has the following 27 letters and 2 digraphs:
The letters "k" and "w" are used only in words and names coming from foreign languages ("kilo, folklore, whisky, kiwi", etc.).
With the exclusion of a very small number of regional terms such as "México" (see Toponymy of Mexico), pronunciation can be entirely determined from spelling. Under the orthographic conventions, a typical Spanish word is stressed on the syllable before the last if it ends with a vowel (not including ⟨y⟩) or with a vowel followed by ⟨n⟩ or an ⟨s⟩; it is stressed on the last syllable otherwise. Exceptions to this rule are indicated by placing an acute accent on the stressed vowel.
The acute accent is used, in addition, to distinguish between certain homophones, especially when one of them is a stressed word and the other one is a clitic: compare "el" ('the', masculine singular definite article) with "él" ('he' or 'it'), or "te" ('you', object pronoun) with "té" ('tea'), "de" (preposition 'of') versus "dé" ('give' [formal imperative/third-person present subjunctive]), and "se" (reflexive pronoun) versus "sé" ('I know' or imperative 'be').
The interrogative pronouns ("qué", "cuál", "dónde", "quién", etc.) also receive accents in direct or indirect questions, and some demonstratives ("ése", "éste", "aquél", etc.) can be accented when used as pronouns. Accent marks used to be omitted in capital letters (a widespread practice in the days of typewriters and the early days of computers when only lowercase vowels were available with accents), although the "Real Academia Española" advises against this and the orthographic conventions taught at schools enforce the use of the accent.
When "u" is written between "g" and a front vowel "e" or "i", it indicates a "hard g" pronunciation. A diaeresis "ü" indicates that it is not silent as it normally would be (e.g., "cigüeña", 'stork', is pronounced [θiˈɣweɲa]; if it were written *"cigueña", it would be pronounced *[θiˈɣeɲa]).
Interrogative and exclamatory clauses are introduced with inverted question and exclamation marks ("¿" and "¡", respectively).
Organizations.
Royal Spanish Academy.
The "Real Academia Española" (Royal Spanish Academy), founded in 1713, together with the 21 other national ones (see Association of Spanish Language Academies), exercises a standardizing influence through its publication of dictionaries and widely respected grammar and style guides.
Because of influence and for other sociohistorical reasons, a standardized form of the language (Standard Spanish) is widely acknowledged for use in literature, academic contexts and the media.
Association of Spanish Language Academies.
The Association of Spanish Language Academies ("Asociación de Academias de la Lengua Española", or "ASALE") is the entity which regulates the Spanish language. It was created in Mexico in 1951 and represents the union of all the separate academies in the Spanish-speaking world. It comprises the academies of 22 countries, ordered by date of Academy foundation: Spain (1713), Colombia (1871), Ecuador (1874), Mexico (1875), El Salvador (1876), Venezuela (1883), Chile (1885), Peru (1887), Guatemala (1887), Costa Rica (1923), Philippines (1924), Panama (1926), Cuba (1926),
Paraguay (1927), Dominican Republic (1927), Bolivia (1927), Nicaragua (1928), Argentina (1931), Uruguay (1943), Honduras (1949), Puerto Rico (1955), and United States (1973).
Cervantes Institute.
The "Instituto Cervantes" (Cervantes Institute) is a worldwide non-profit organization created by the Spanish government in 1991. This organization has branched out in over 20 different countries with 54 centers devoted to the Spanish and Hispanic American culture and Spanish Language. The ultimate goals of the Institute are to promote the education, the study and the use of Spanish universally as a second language, to support the methods and activities that would help the process of Spanish language education, and to contribute to the advancement of the Spanish and Hispanic American cultures throughout non-Spanish-speaking countries.
Official use by international organizations.
Spanish is recognised as one of the official languages of the United Nations, the European Union, the World Trade Organization, the Organization of American States, the Organization of Ibero-American States, the African Union, the Union of South American Nations, the Antarctic Treaty Secretariat, the Latin Union, the Caricom and the North American Free Trade Agreement.

</doc>
<doc id="26826" url="http://en.wikipedia.org/wiki?curid=26826" title="Sodium">
Sodium

Sodium is a chemical element with symbol Na (from Latin: "natrium") and atomic number 11. It is a soft, silver-white, highly reactive metal and is a member of the alkali metals; its only stable isotope is 23Na. The free metal does not occur in nature, but instead must be prepared from its compounds. It was first isolated by Humphry Davy in 1807 by the electrolysis of sodium hydroxide. Sodium is the sixth most abundant element in the Earth's crust, and exists in numerous minerals such as feldspars, sodalite and rock salt (NaCl). Many salts of sodium are highly water-soluble. Sodium ions have been leached by the action of water so that sodium and chlorine (Cl) are the most common dissolved elements by weight in the Earth's bodies of oceanic water.
Many sodium compounds are useful, such as sodium hydroxide (lye) for soap-making, and sodium chloride for use as a de-icing agent and a nutrient (edible salt). Sodium is an essential element for all animals and some plants. In animals, sodium ions are used against potassium ions to build up charges on cell membranes, allowing transmission of nerve impulses when the charge is dissipated. The consequent need of animals for sodium causes it to be classified as a dietary inorganic macro-mineral nutrient.
Characteristics.
Physical.
Sodium at standard temperature and pressure is a soft silvery metal, that oxidizes to grayish white unless immersed in oil or inert gas. Sodium can be readily cut with a knife, and is a good conductor of electricity. These properties change dramatically at elevated pressures: at 1.5 Mbar, the color changes from silvery metallic to black; at 1.9 Mbar the material becomes transparent, with a red color; and at 3 Mbar sodium is a clear and transparent solid. All of these high-pressure allotropes are insulators and electrides.
When sodium or its compounds are introduced into a flame, they turn it yellow, because the excited 3s electrons of sodium emit a photon when they fall from 3p to 3s; the wavelength of this photon corresponds to the D line at 589.3 nm. Spin-orbit interactions involving the electron in the 3p orbital split the D line into two; hyperfine structures involving both orbitals cause many more lines.
Chemical.
When freshly cut, sodium has a bright, silvery luster. If exposed to air, the surface rapidly tarnishes, darkening at first and then forming a white coating of sodium hydroxide and sodium carbonate.
Sodium is generally less reactive than potassium and more reactive than lithium. Like all the alkali metals, it reacts exothermically with water, to the point that sufficiently large pieces melt to a sphere and may explode; this reaction produces caustic soda (sodium hydroxide) and flammable hydrogen gas. When burned in dry air, it mainly forms sodium peroxide as well as some sodium oxide. In moist air, sodium hydroxide results. Sodium metal is highly reducing, with the reduction of sodium ions requiring −2.71 volts. Hence, the extraction of sodium metal from its compounds (such as with sodium chloride) uses a significant amount of energy. However, potassium and lithium have even more negative potentials.
Isotopes.
20 isotopes of sodium are known, but only 23Na is stable. Two radioactive, cosmogenic isotopes are the byproduct of cosmic ray spallation: 22Na with a half-life of 2.6 years and 24Na with a half-life of 15 hours; all other isotopes have a half-life of less than one minute. Two nuclear isomers have been discovered, the longer-lived one being 24mNa with a half-life of around 20.2 microseconds. Acute neutron radiation, such as from a nuclear criticality accident, converts some of the stable 23Na in human blood to 24Na; by measuring the concentration of 24Na in relation to 23Na, the neutron radiation dosage of the victim can be calculated.
Occurrence.
23Na is created in the carbon-burning process in stars by fusing two carbon atoms together; this requires temperatures above 600 megakelvins and a star of at least three solar masses. The Earth's crust contains 2.6% sodium by weight, making it the sixth most abundant element on Earth. Because of its high reactivity, it is never found as a pure element. It is found in many different minerals, some very soluble, such as halite and natron, others much less soluble such as amphibole, and zeolite. The insolubility of certain sodium minerals such as cryolite and feldspar arises from their polymeric anions, which in the case of feldspar is a polysilicate. In the interstellar medium, sodium is identified by the D spectral line; though it has a high vaporization temperature, its abundance allowed it to be detected by Mariner 10 in Mercury's atmosphere.
Compounds.
See also: .
Sodium compounds are of immense commercial importance, being particularly central to industries producing glass, paper, soap, and textiles. The sodium compounds that are the most important include table salt (NaCl), soda ash (Na2CO3), baking soda (NaHCO3), caustic soda (NaOH), sodium nitrate (NaNO3), di- and tri-sodium phosphates, sodium thiosulfate (Na2S2O3·5H2O), and borax (Na2B4O7·10H2O). In its compounds, sodium is usually ionically bonded to water and anions, and is viewed as a hard Lewis acid.
Most soaps are sodium salts of fatty acids. Sodium soaps are harder (higher melting) soaps than potassium soaps. Sodium chloride is extensively used for anti-icing and de-icing and as a preservative; sodium bicarbonate is mainly used for cooking. Along with potassium, many important medicines have sodium added to improve their bioavailability; although in most cases potassium is the better ion, sodium is selected for its lower price and atomic weight.Sodium hydride is used as a base for various reactions (such as the aldol reaction) in organic chemistry, and as a reducing agent in inorganic chemistry.
Aqueous solutions.
Sodium tends to form water-soluble compounds, such as halides, sulfates, nitrates, carboxylates and carbonates. The main aqueous species are the aquo complexes [Na(H2O)"n"]+, where "n" = 4–6. The high affinity of sodium for oxygen-based ligands is the basis of crown ethers; macrolide antibiotics, which interfere with Na+ transport in the infecting organism, are functionally related and more complex.
Direct precipitation of sodium salts from aqueous solutions is rare, because sodium salts typically have a high affinity for water; an exception is sodium bismuthate (NaBiO3). Because of this, sodium salts are usually isolated as solids by evaporation or by precipitation with an organic solvent, such as ethanol; for example, only 0.35 g/L of sodium chloride will dissolve in ethanol. Crown ethers, like 15-crown-5, may be used as a phase-transfer catalyst.
Sodium content in bulk may be determined by treating with a large excess of uranyl zinc acetate; the hexahydrate (UO2)2ZnNa(CH3CO2)·6H2O precipitates and can be weighed. Caesium and rubidium do not interfere with this reaction, but potassium and lithium do. Lower concentrations of sodium may be determined by atomic absorption spectrophotometry or by potentiometry using ion-selective electrodes.
Electrides and sodides.
Like the other alkali metals, sodium dissolves in ammonia and some amines to give deeply colored solutions; evaporation of these solutions leaves a shiny film of metallic sodium. The solutions contain the coordination complex (Na(NH3)6)+, whose positive charge is counterbalanced by electrons as anions; cryptands permit the isolation of these complexes as crystalline solids. Cryptands, like crown ethers and other ionophores, have a high affinity for the sodium ion; derivatives of the alkalide Na− are obtainable by the addition of cryptands to solutions of sodium in ammonia via disproportionation.
Organosodium compounds.
Many organosodium compounds have been prepared. Because of the high polarity of the C-Na bonds, they behave like sources of carbanions (salts with organic anions). Some well known derivatives include sodium cyclopentadienide (NaC5H5) and trityl sodium ((C6H5)3CNa).
History.
Salt has been an important commodity in human activities, as shown by the English word "salary", which derives from "salarium", the wafers of salt sometimes given to Roman soldiers along with their other wages. In medieval Europe, a compound of sodium with the Latin name of "sodanum" was used as a headache remedy. The name sodium is thought to originate from the Arabic "suda" (صداع), meaning headache, as the headache-alleviating properties of sodium carbonate or soda were well known in early times. The chemical abbreviation for sodium was first published by Jöns Jakob Berzelius in his system of atomic symbols, and is a contraction of the element's New Latin name "natrium", which refers to the Egyptian "natron", a natural mineral salt primarily made of hydrated sodium carbonate. Natron historically had several important industrial and household uses, later eclipsed by other sodium compounds. Although sodium, sometimes called "soda", had long been recognised in compounds, the metal itself was not isolated until 1807 by Sir Humphry Davy through the electrolysis of sodium hydroxide.
Sodium imparts an intense yellow color to flames. As early as 1860, Kirchhoff and Bunsen noted the high sensitivity of a sodium flame test, and stated in Annalen der Physik und Chemie:
Commercial production.
Enjoying rather specialized applications, only about 100,000 tonnes of metallic sodium are produced annually. Metallic sodium was first produced commercially in 1855 by carbothermal reduction of sodium carbonate at 1100 °C, in what is known as the Deville process:
A related process based on the reduction of sodium hydroxide was developed in 1886.
Sodium is now produced commercially through the electrolysis of molten sodium chloride, based on a process patented in 1924. This is done in a Downs cell in which the NaCl is mixed with calcium chloride to lower the melting point below 700 °C. As calcium is less electropositive than sodium, no calcium will be deposited at the cathode. This method is less expensive than the previous Castner process of electrolyzing sodium hydroxide.
Reagent-grade sodium in tonne quantities sold for about US$3.30/kg in 2009; lower purity metal sells for considerably less. The market for sodium is volatile due to the difficulty in its storage and shipping; it must be stored under a dry inert gas atmosphere or anhydrous mineral oil to prevent the formation of a surface layer of sodium oxide or sodium superoxide. These oxides can react violently in the presence of organic materials. Smaller quantities of sodium cost far more, in the range of US$165/kg; the high cost is partially due to the expense of shipping hazardous material.
Applications.
Though metallic sodium has some important uses, the major applications of sodium use is in its many compounds; millions of tons of the chloride, hydroxide, and carbonate are produced annually.
Free element.
Metallic sodium is mainly used for the production of sodium borohydride, sodium azide, indigo, and triphenylphosphine. Previous uses were for the making of tetraethyllead and titanium metal; because applications for these chemicals were discontinued, the production of sodium declined after 1970. Sodium is also used as an alloying metal, an anti-scaling agent, and as a reducing agent for metals when other materials are ineffective. Note the free element is not used as a scaling agent, ions in the water are exchanged for sodium ions. Sodium vapor lamps are often used for street lighting in cities and give colours ranging from yellow-orange to peach as the pressure increases. By itself or with potassium, sodium is a desiccant; it gives an intense blue colouration with benzophenone when the desiccate is dry. In organic synthesis, sodium is used in various reactions such as the Birch reduction, and the sodium fusion test is conducted to qualitatively analyse compounds. Lasers emitting light at the D line, utilising sodium, are used to create artificial laser guide stars that assist in the adaptive optics for land-based visible light telescopes.
Heat transfer.
Liquid sodium is used as a heat transfer fluid in some fast reactors, due to its high thermal conductivity and low neutron absorption cross section, which is required to achieve a high neutron flux; the high boiling point allows the reactor to operate at ambient pressure. Drawbacks of using sodium include its opacity, which hinders visual maintenance, and its explosive properties. Radioactive sodium-24 may be formed by neutron activation during operation, posing a slight radiation hazard; the radioactivity stops within a few days after removal from the reactor. If a reactor needs to be frequently shut down, NaK is used; due to it being liquid at room temperature, cooling pipes do not freeze. In this case, the pyrophoricity of potassium means extra precautions against leaks need to be taken. Another heat transfer application is in high-performance internal combustion engines with poppet valves, where valve stems partially filled with sodium are used as a heat pipe to cool the valves.
Biological role.
In humans, sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods. The UL for sodium is 2.3 grams per day, the threshold which could lead to hypertension when exceeded, but on average people in the United States consume 3.4 grams per day. Hypertension causes 7.6 million premature deaths worldwide each year. (Note that salt contains about 39.3% sodium—the rest being chlorine and other trace chemicals; thus the UL of 2.3g sodium would be about 5.9g, or 2.7ml of salt—about half a US teaspoon)
The renin-angiotensin system regulates the amount of fluids and sodium in the body. Reduction of blood pressure and sodium concentration in the kidney result in the production of renin, which in turn produces aldosterone and angiotensin, retaining sodium in the urine. Because of the increase in sodium concentration, the production of renin decreases, and the sodium concentration returns to normal. Sodium is also important in neuron function and osmoregulation between cells and the extracellular fluid, their distribution mediated in all animals by Na+/K+-ATPase; hence, sodium is the most prominent cation in extracellular fluid.
Unusually low or high sodium levels in humans are recognized in medicine as hyponatremia and hypernatremia. These conditions may be caused by genetic factors, physical factors associated with ageing or illnesses involving vomiting or diarrhea.
In C4 plants, sodium is a micronutrient that aids in metabolism, specifically in regeneration of phosphoenolpyruvate and synthesis of chlorophyll. In others, it substitutes for potassium in several roles, such as maintaining turgor pressure and aiding in the opening and closing of stomata. Excess sodium in the soil limits the uptake of water due to decreased water potential, which may result in wilting; similar concentrations in the cytoplasm can lead to enzyme inhibition, which in turn causes necrosis and chlorosis. To avoid these problems, plants developed mechanisms that limit sodium uptake by roots, store them in cell vacuoles, and control them over long distances; excess sodium may also be stored in old plant tissue, limiting the damage to new growth.
Precautions.
Care is required in handling elemental sodium, as it generates flammable hydrogen and caustic sodium hydroxide upon contact with water; powdered sodium may spontaneously explode in the presence of an oxidizer. Excess sodium can be safely removed by hydrolysis in a ventilated cabinet; this is typically done by sequential treatment with isopropanol, ethanol and water. Isopropanol reacts very slowly, generating the corresponding alkoxide and hydrogen. Fire extinguishers based on water accelerate sodium fires; those based on carbon dioxide and bromochlorodifluoromethane lose their effectiveness when they dissipate. An effective extinguishing agent is Met-L-X, which comprises approximately 5% Saran in sodium chloride together with flow agents; it is most commonly hand-applied with a scoop. Other materials include Lith+, which has graphite powder and an organophosphate flame retardant, and dry sand.

</doc>
<doc id="26828" url="http://en.wikipedia.org/wiki?curid=26828" title="Suriname">
Suriname

Suriname (, or , also spelled Surinam), officially known as the Republic of Suriname (Dutch: "Republiek Suriname", ]), is a sovereign state on the northeastern Atlantic coast of South America. It is bordered by French Guiana to the east, Guyana to the west and Brazil to the south.
Suriname was colonized by the English and the Dutch in the 17th century. In 1667 it was captured by the Dutch, who governed Suriname as Dutch Guiana until 1954. At that time it was designated as one of the constituent countries of the Kingdom of the Netherlands, next to the Netherlands and the Netherlands Antilles (dissolved in 2010). On 25 November 1975, the country of Suriname left the Kingdom of the Netherlands to become independent. A member of CARICOM, it is considered to be a culturally Caribbean country and has extensive trade and cultural exchange with the Caribbean nations.
At just under 165000 km2, Suriname is the smallest sovereign state in South America (French Guiana, while less extensive and populous, is an overseas department of France). Suriname has a population of approximately 566,000, most of whom live on the country's north coast, where the capital Paramaribo is located. Suriname is a mostly Dutch-speaking country; Sranang, an English-based creole language, is a widely used "lingua franca". It is the only independent entity in the Americas where Dutch is spoken by a majority of the population.
Etymology.
This area was occupied by various cultures of indigenous peoples long before European contact. The name "Suriname" may derive from a Taino (Arawak-speaking) indigenous people called "Surinen," who inhabited the area at the time of European contact.
British settlers, who founded the first European colony at Marshall's Creek along the Suriname River, spelled the name as "Surinam".
When the territory was taken over by the Dutch, it became part of a group of colonies known as Dutch Guiana. The official spelling of the country's English name was changed from "Surinam" to "Suriname" in January 1978, but "Surinam" can still be found in English. A notable example is Suriname's national airline, Surinam Airways. The older English name is reflected in the English pronunciation, or . In Dutch, the official language of Suriname, the pronunciation is ], with the main stress on the third syllable and a schwa terminal vowel.
History.
Colonial period.
Beginning in the 16th century, French, Spanish, and English explorers visited the area. A century later, plantation colonies were established by the Dutch and English along the many rivers in the fertile Guiana plains. The earliest documented colony in Guiana was an English settlement named Marshall's Creek along the Suriname River. Disputes arose between the Dutch and the English. In 1667, during negotiations leading to the Treaty of Breda, the Dutch decided to keep the nascent plantation colony of Suriname they had conquered from the English. The English got to keep New Amsterdam, the main city of the former colony of New Netherland. Already a cultural and economic hub in those days, they renamed it after the Duke of York: New York.
In 1683, the Society of Suriname was founded by the city of Amsterdam, the Van Aerssen van Sommelsdijck family, and the Dutch West India Company. The society was chartered to manage and defend the colony. The planters of the colony relied heavily on African slaves to cultivate the coffee, cocoa, sugar cane and cotton plantations along the rivers. Planters' treatment of the slaves was notoriously bad, and many slaves escaped the plantations.
With the help of the native South Americans living in the adjoining rain forests, these runaway slaves established a new and unique culture that was highly successful in its own right. They were known collectively in English as the Maroons, in French as "Nèg'Marrons" (literally meaning "brown negroes", that is "pale-skinned negroes"), and in Dutch as "Bosnegers" (literally meaning "forest negroes"). The Maroons gradually developed several independent tribes through a process of ethnogenesis, as they were made up of slaves from different African ethnicities. Among them are the Saramaka, the Paramaka, the Ndyuka or Aukan, the Kwinti, the Aluku or Boni, and the Matawai.
The Maroons often raided the plantations to recruit new members from the slaves and capture women, as well as acquire weapons, food and supplies. The planters and their families were sometimes killed in the raids; colonists built defenses, which were so important they were shown on 18th-century maps, but these were not sufficient. The colonists also mounted armed campaigns against the Maroons, who generally escaped through the rainforest which they knew much better than did the colonists. To end hostilities, in the 19th century the European colonial authorities signed several peace treaties with different tribes. They granted the Maroons sovereign status and trade rights in their inland territories.
Abolition of slavery.
Slavery in Suriname was abolished by the Netherlands in 1863, but the slaves were not fully released until 1873, after a mandatory ten-year transition period during which time they were required to work on the plantations for minimal pay and without state-sanctioned discipline. As soon as they became truly free, the slaves largely abandoned the plantations where they had worked for several generations in favour of the city, Paramaribo.
As a plantation colony, Suriname was still heavily dependent on manual labour, and to make up for the shortfall, the Dutch brought in contract labourers from the Dutch East Indies (modern Indonesia) and India (through an arrangement with the British). In addition, during the late 19th and early 20th centuries, small numbers of labourers, mostly men, were brought in from China and the Middle East. Although Suriname's population remains relatively small, because of this history it is one of the most ethnically and culturally diverse countries in the world.
Decolonization.
On 23 November 1941, under an agreement with the Netherlands government-in-exile, the United States occupied Suriname to protect bauxite mines. In 1942, the Dutch government-in-exile expressed a desire to review the relations between the Netherlands and its colonies after the end of the war. In 1954, Suriname became one of the constituent countries of the Kingdom of the Netherlands, along with the Netherlands Antilles and the Netherlands. In this construction, the Netherlands retained control of defense and foreign affairs. In 1973, the local government, led by the NPS (a largely Creole, meaning ethnically African or mixed African-European, party) started negotiations with the Dutch government leading towards full independence, which was granted on 25 November 1975. The severance package was very substantial, and a large part of Suriname's economy for the first decade following independence was fueled by foreign aid provided by the Dutch government.
Independence.
The first President of the country was Johan Ferrier, the former governor, with Henck Arron (the then leader of the NPS) as Prime Minister. In the years leading up to independence, nearly one-third of the population of Suriname emigrated to the Netherlands, amidst concern that the new country would fare worse under independence than it had as a constituent country of the Kingdom of the Netherlands. Indeed, Surinamese politics soon degenerated into ethnic polarisation and corruption, with the NPS using Dutch aid money for partisan purposes. Its leaders were accused of fraud in the 1977 elections, in which Arron won a further term, and the discontent was such that a large chunk of the population fled to the Netherlands, joining the already significant Surinamese community there.
December killings.
On 25 February 1980, a military coup overthrew Arron's government. On 15 March 1981, and again on 12 March 1982, failed counter-coups were attempted. The first attempt was led by Wilfred Hawker and the second by Surendre Rambocus. Hawker escaped from prison during the second counter-coup attempt but was captured and executed. Between 2 am and 5 am on the morning of 7 December 1982, the military, under the leadership of Dési Bouterse, rounded up 13 prominent citizens who had criticized the military dictatorship in Suriname and brought them to Fort Zeelandia. They were executed over the next three days, along with Rambocus and Jiwansingh Sheombar (who was also involved in the second counter-coup attempt). Bouterse, among other defendants, was tried for the murders, but the Suriname parliament extended an amnesty law in 2012 that granted Bouterse amnesty for the alleged violations before the trial was concluded. The Dutch government stated that stopping the trial was "totally unacceptable".
Elections were held in 1987, and a new constitution was adopted that, among other things, allowed Bouterse to remain in charge of the army. Dissatisfied with the government, Bouterse summarily dismissed them in 1990, by telephone. This event became popularly known as the "Telephone Coup". His power began to wane after the 1991 elections; an ongoing brutal civil war between the Suriname army and Maroons loyal to rebel leader Ronnie Brunswijk, begun in 1986, further weakened Bouterse's position during the 1990s. In 1999, Bouterse was convicted "in absentia" in the Netherlands on drug smuggling charges.
Seeing the deterioration of the situation, Aruba, the Netherlands Antilles and the Netherlands considered inviting the country to rejoin the Netherlands as an "associated state" in 1991.
21st century.
On 19 July 2010, the former dictator Dési Bouterse returned to power when he was elected as the new President of Suriname.
Politics.
The Republic of Suriname is a constitutional, democratically representational republic based on the 1987 constitution. The legislative branch of government consists of a 51-member unicameral National Assembly, simultaneously and popularly elected for a five-year term.
In the most recent elections, held on Tuesday, 25 May 2010, the "Megacombinatie" won 23 of the National Assembly seats followed by "Nationale Front" with 20 seats. A much smaller number, important for coalition-building, went to the "A‑combinatie" and to the "Volksalliantie". Negotiations are ongoing between parties regarding the formation of coalitions.
The President of Suriname, who is elected for a five-year term by a two-thirds majority of the National Assembly or, failing that, by a majority of the People's Assembly, heads the executive branch. If at least two-thirds of the National Assembly cannot agree to vote for one presidential candidate, a People's Assembly is formed from all National Assembly delegates and regional and municipal representatives who were elected by popular vote in the most recent national election. As head of government, the president appoints a sixteen-minister cabinet. A vice president, normally elected at the same time as the president, needs a simple majority in the National Assembly or People's Assembly to be elected for a five-year term. There is no constitutional provision for removal or replacement of the president unless he resigns.
The judiciary is headed by the Court of Justice (Supreme Court). This court supervises the magistrate courts. Members are appointed for life by the president in consultation with the National Assembly, the State Advisory Council and the National Order of Private Attorneys. In April 2005, the regional Caribbean Court of Justice, based in Trinidad, was inaugurated. As the final court of appeal, it was intended to replace the London-based Privy Council.
Foreign Relations.
President Desi Bouterse was sentenced in the Netherlands to 11 years' imprisonment on a charge of drug trafficking. He is also the main suspect in the court case concerning the 'December murders', the assassination of opponents of military rule in Fort Zeelandia, Paramaribo, in 1982. These two cases place a constraint on relations between the Netherlands and Suriname. The Dutch government has stated it will only maintain necessary contact with the president.
Due to Suriname's Dutch colonial history, Suriname had a long-standing special relationship with the Netherlands. However, these were strained because of the coup, and the Netherlands no longer maintains Suriname as a member of its development program.
Since 1991, the United States has maintained positive relations with Suriname. The two countries work together through the Caribbean Basin Security Initiative (CBSI) and the U.S. President's Emergency Plan for AIDS Relief (PEPFAR). Suriname also receives military funding from the U.S. Department of Defense.
EU relations and cooperation with Suriname are carried out both on bilateral and regional basis, the latter within the framework of the ongoing EU-CELAC and EU-CARIFORUM dialogues. Suriname is party to the Cotonou Agreement, the partnership agreement between the members of the African, Caribbean and Pacific Group of States and the European Union.
On 17 February 2005, the leaders of Barbados and Suriname signed the "Agreement for the deepening of bilateral cooperation between the Government of Barbados and the Government of the Republic of Suriname."
On 23–24 April 2009, both nations formed a Joint Commission in Paramaribo, Suriname, to improve relations between both countries and to expand into various areas of cooperation. Since the first meeting, a second one was held on 3–4 March 2011, in Dover, Barbados. At the second meeting, several areas of mutual interest were reviewed including: agriculture, trade, investment, as well as international transport.
In the late 2000s, Suriname intensified development cooperation with other developing countries. China's South-South cooperation with Suriname has included a number of large-scale infrastructure projects, including port rehabilitation and road construction, and Brazil has signed agreements to cooperate with Suriname in education, health, agriculture, and energy production.
Military.
The Armed Forces of Suriname have three branches: the Army, the Air Force, and the Navy. The President of the Republic, Dési Bouterse, is the Supreme Commander-in-Chief of the Armed Forces ("Opperbevelhebber van de Strijdkrachten"). The President is assisted by the Minister of Defence. Beneath the President and Minister of Defense is the Commander of the Armed Forces ("Bevelhebber van de Strijdkrachten"). The Military Branches and regional Military Commands report to the Commander.
After the creation of the Statute of the Kingdom of the Netherlands, the Royal Netherlands Army was entrusted with the defence of Suriname, while the defence of the Netherlands Antilles was the responsibility of the Royal Netherlands Navy. The army set up a separate "Troepenmacht in Suriname" (Forces in Suriname, TRIS). Upon independence in 1975, this force was turned into the "Surinaamse Krijgsmacht" (SKM):, Surinamese Armed Forces. On 25 February 1980, a group of 15 non-commissioned officers and one junior SKM officer, under the leadership of sergeant major Dési Bouterse, overthrew the Government. Subsequently the SKM was rebranded as "Nationaal Leger" (NL), National Army.
Administrative Divisions.
The country is divided into ten administrative districts, each headed by a district commissioner appointed by the president, who also has the power of dismissal. Suriname is further subdivided into 62 resorts (ressorten).
Geography.
Suriname is the smallest independent country in South America. Situated on the Guiana Shield, it lies mostly between latitudes 1° and 6°N, and longitudes 54° and 58°W. The country can be divided into two main geographic regions. The northern, lowland coastal area (roughly above the line Albina-Paranam-Wageningen) has been cultivated, and most of the population lives here. The southern part consists of tropical rainforest and sparsely inhabited savanna along the border with Brazil, covering about 80% of Suriname's land surface.
The two main mountain ranges are the Bakhuys Mountains and the Van Asch Van Wijck Mountains. Julianatop is the highest mountain in the country at 1286 m above sea level. Other mountains include Tafelberg at 1026 m, Mount Kasikasima at 718 m, Goliathberg at 358 m and Voltzberg at 240 m.
Borders.
Suriname is situated between French Guiana to the east and Guyana to the west. The southern border is shared with Brazil and the northern border is the Atlantic coast. The southernmost borders with French Guiana and Guyana are disputed by these countries along the Marowijne and Corantijn rivers, respectively, while a part of the disputed maritime boundary with Guyana was arbitrated by a tribunal convened under the rules set out in of the United Nations Convention on the Law of the Sea on 20 September 2007.
Climate.
Lying 2 to 5 degrees north of the equator, Suriname has a very hot and wet tropical climate, and temperatures do not vary much throughout the year. Average relative humidity is between 80% and 90%. Its average temperature ranges from 29 to 34 degrees Celsius (84 to 97 degrees Fahrenheit). Due to the high humidity, actual temperatures are distorted and may therefore feel up to 6 degrees Celsius (11 degrees Fahrenheit) hotter than the recorded temperature. The year has two wet seasons, from April to August and from November to February. It also has two dry seasons, from August to November and February to April.
Nature reserves.
Located in the upper Coppename River watershed, the Central Suriname Nature Reserve has been designated a UNESCO World Heritage Site for its unspoiled forests and biodiversity. There are many national parks in the country: Galibi National Reserve, Coppename Manding National Park, and Wia Wia NR along the coast; Brownsberg NR, Raleighvallen/Voltzeberg NR, Tafelberg NR, and Eilerts de Haan NP in the centre; and the Sipaliwani NR on the Brazilian border. In all, 12.6% of the country's land area is national parks and lakes, according to the UNEP World Conservation Monitoring Centre.
Economy.
Suriname's democracy gained some strength after the turbulent 1990s, and its economy became more diversified and less dependent on Dutch financial assistance. Bauxite (aluminium ore) mining continues to be a strong revenue source, and the discovery and exploitation of oil and gold has added substantially to Suriname's economic independence. Agriculture, especially rice and bananas, remains a strong component of the economy, and ecotourism is providing new economic opportunities. More than 80% of Suriname's land-mass consists of unspoiled rain forest; with the establishment of the Central Suriname Nature Reserve in 1998, Suriname signalled its commitment to conservation of this precious resource. The Central Suriname Nature Reserve became a World Heritage Site in 2000.
The economy of Suriname is dominated by the bauxite industry, which accounts for more than 15% of GDP and 70% of export earnings. Other main export products include rice, bananas and shrimp. Suriname has recently started exploiting some of its sizeable oil and gold reserves. About a quarter of the people work in the agricultural sector. The Surinamese economy is very dependent on commerce, its main trade partners being the Netherlands, the United States, Canada, and Caribbean countries, mainly Trinidad and Tobago and the former islands of the Netherlands Antilles.
After assuming power in the fall of 1996, the Wijdenbosch government ended the structural adjustment program of the previous government, claiming it was unfair to the poorer elements of society. Tax revenues fell as old taxes lapsed and the government failed to implement new tax alternatives. By the end of 1997, the allocation of new Dutch development funds was frozen as Surinamese Government relations with the Netherlands deteriorated. Economic growth slowed in 1998, with decline in the mining, construction, and utility sectors. Rampant government expenditures, poor tax collection, a bloated civil service, and reduced foreign aid in 1999 contributed to the fiscal deficit, estimated at 11% of GDP. The government sought to cover this deficit through monetary expansion, which led to a dramatic increase in inflation. It takes longer on average to register a new business in Suriname than virtually any other country in the world (694 days or about 99 weeks).
Demographics.
According to the 2012 census, Suriname had a population of 541,638 inhabitants. It is made up of several distinct ethnic groups.
The vast majority of people (about 90%) live in Paramaribo or on the coast. There is also a significant Surinamese population in the Netherlands. In 2005 there were 328,300 Surinamese people living in the Netherlands, which is about 2% of the total population of the Netherlands, compared to 438,000 Surinamese in Suriname itself.
Religion.
The predominant religion in the country is Christianity (48.4% in 2012), both in the form of Roman Catholicism and various denominations of Protestantism, the Moravian Church being the oldest and largest; it is particularly dominant among Creoles. Many Maroons practice Winti, an Afro-American religion. Hindus form the second largest religious group in Suriname, comprising 22.3% of the population. The Indian-descended population practices predominantly Hinduism, or in minority Islam. The Javanese practice either Islam or, a small minority, Javanism. Muslims constitute 13.9% of the population of Suriname.
Languages.
Dutch is the sole official language, and is the language of education, government, business, and the media. Over 60% of the population speaks Dutch as a mother tongue, and most of the rest speak it as a second language. In 2004 Suriname became an associate member of the Dutch Language Union. It is the only Dutch-speaking country in South America as well as the only independent nation in the Americas where Dutch is spoken by a majority of the population, and one of the two non-Romance-speaking countries on the continent, the other being English-speaking Guyana.
In Paramaribo, Dutch is the main home language in two-thirds of households. The recognition of "Surinaams-Nederlands" ("Surinamese Dutch") as a national dialect equal to "Nederlands-Nederlands" ("Dutch Dutch") and "Vlaams-Nederlands" ("Flemish Dutch") was expressed in 2009 by the publication of the "Woordenboek Surinaams Nederlands" ("Surinamese–Dutch Dictionary"). Only in the interior of Suriname is Dutch seldom spoken.
Sranan, a local creole language originally spoken by the creole population group, is the most widely used language in the streets and is often used interchangeably with Dutch depending on the formality of the setting.
Surinamese Hindi or Sarnami, a dialect of Bhojpuri, is the third-most used language, spoken by the descendants of South Asian contract workers from then British India. Javanese is used by the descendants of Javanese contract workers. The Maroon languages, somewhat intelligible with Sranan Tongo, include Saramaka, Paramakan, Ndyuka (also called "Aukan"), Kwinti and Matawai. Amerindian languages, spoken by Amerindians, include Carib and Arawak. Hakka and Cantonese are spoken by the descendants of the Chinese contract workers. Mandarin is spoken by some few recent Chinese immigrants. English, Spanish and Portuguese are also used. Spanish and Portuguese are spoken by Latin American residents and their descendants and sometimes also taught in schools.
The public discourse about Suriname's languages is a part of an ongoing debate about the country's national identity. The use of the popular Sranan became associated with nationalist politics after its public use by former dictator Dési Bouterse in the 1980s, and groups descended from escaped slaves might resent it. Some propose to change the national language to English, so as to improve links to the Caribbean and North America, or to Spanish, as a nod to Suriname's location in South America, although it has no Spanish-speaking neighbours.
Culture.
Owing to the country's multicultural heritage, Suriname celebrates a variety of distinct ethnic and religious festivals.
National celebrations.
There are several Hindu and Islamic national holidays like Diwali (deepavali), Phagwa and Eid ul-Fitr and Eid-ul-adha. These holidays do not have specific dates on the Gregorian calendar, as they are based on the Hindu and Islamic calendars, respectively.
There are several holidays which are unique to Suriname. These include the Indian, Javanese and Chinese arrival days. They celebrate the arrival of the first ships with their respective immigrants.
New Year's Eve.
New Year's Eve in Suriname is called "Oud jaar", or "old year". It is during this period that the Surinamese population goes to the city's commercial district to watch "demonstrational fireworks". The bigger stores invest in these firecrackers and display them out in the streets. Every year the length of them is compared, and high praises are given for the company that has imported the largest ribbon.
These celebrations start at 10 in the morning and finish the next day. The day is usually filled with laughter, dance, music, and drinking. When the night starts, the big street parties are already at full capacity. The most popular fiesta is the one that is held at café 't Vat in the main tourist district. The parties there stop between 10 and 11 at night, after which people go home to light their pagaras (red-firecracker-ribbons) at midnight.
After 12, the parties continue and the streets fill again until daybreak.
Sports.
The Suriname Olympic Committee is the national governing body for sports in Suriname. The SOC was established in 1959 and now has 17 members: Athletics, Badminton, Basketball, Boxing, Chess, Cycling, Football, Judo, Karate, Shooting, Swimming, Table Tennis, Taekwondo, Tennis, Triathlon, Volleyball, and Wrestling.
One of the major sports in Suriname is football. Many Suriname-born players and Dutch-born players of Surinamese descent, like Gerald Vanenburg, Ruud Gullit, Frank Rijkaard, Edgar Davids, Clarence Seedorf, Andwélé Slory, Ryan Babel, Aron Winter, Patrick Kluivert, Romeo Castelen, and Jimmy Floyd Hasselbaink turned out to play for "Oranje". In 1999, Humphrey Mijnals, who played for both Suriname and the Netherlands, was elected Surinamese footballer of the century. Another famous player is André Kamperveen, who captained Suriname in the 1940s and was the first Surinamese to play professionally in the Netherlands.
The most famous international track & field athlete from Suriname is Letitia Vriesde, who won a silver medal at the 1995 World Championships behind Ana Quirot in the 800 metres, the first medal won by a South American female athlete in World Championship competition. In addition, she also won a bronze medal at the 2001 World Championships and won several medals in the 800 and 1500 metres at the Pan-American Games and Central American and Caribbean Games. Tommy Asinga also received acclaim for winning a bronze medal in the 800 metres at the 1991 Pan American Games.
Swimmer Anthony Nesty is the only Olympic medalist for Suriname. He won gold in the 100-meter butterfly at the 1988 Summer Olympics in Seoul and he won bronze in the same discipline at the 1992 Summer Olympics in Barcelona. Originally from Trinidad and Tobago, he now lives in Gainesville, Florida, and is the coach of the University of Florida, mainly coaching distance swimmers.
Cricket is popular in Suriname to some extent, influenced by its popularity in the Netherlands and in neighbouring Guyana. The Surinaamse Cricket Bond is an associate member of the International Cricket Council (ICC). Suriname and Argentina are the only ICC associates in South America, although Guyana is represented on the West Indies Cricket Board, a full member. The national cricket team was ranked 47th in the world and sixth in the ICC Americas region as of June 2014, and competes in the World Cricket League (WCL) and ICC Americas Championship. Iris Jharap, born in Paramaribo, played women's One Day International matches for the Dutch national side, the only Surinamer to do so.
In the sport of badminton the local heroes are Virgil Soeroredjo & Mitchel Wongsodikromo and also Crystal Leefmans. All winning medals for Suriname at the Carebaco Caribbean Championships, the Central American and Caribbean Games (CACSO Games) and also at the South American Games, better known as the ODESUR Games. Virgil Soeroredjo also participated for Suriname at the 2012 London Summer Olympics, only the second badminton player, after Oscar Brandon, for Suriname to achieve this.
Multiple K-1 champion and legend, Ernesto Hoost, is from Surinamese descent. Rayen Simson, another legendary multiple world champion kickboxer, was born in Suriname. Remy Bonjasky also a multiple K-1 champion was also born in Suriname. MMA and Kickboxing champions Melvin Manhoef, Gilbert Yvel were born in Suriname or from Surinamese descent. Retired female kickboxer Ilonka Elmont was also born in Suriname. Another notable up and comer kickboxer and K-1 fighter, Tyrone Spong, was born in Suriname. Ginty Vrede, a former Muay Thai Heavy Weight Champion who died in 2008 aged 22, was born in Suriname.
Involving the sport of tennis, historic national champions include Gerard van der Schroeff (men's single national champion for 10 consecutive years between the years 1931–1941, plus champion of multiple future titles). Herman Tjin-A-Djie (men's national champion 1941 and 1945, plus men's national double champion for 10 consecutive years with his brother Leo). Leo Tjin-A-Djie (between 1948–1957 he was 8 times national champion and men's national double champion for 10 consecutive years with his brother Herman).From Leo spawned the Opa Leo Tjin-A-Djie Tennis tournament. Randolf Tjin-A-Djie was national champion for 1960.
Transportation.
Suriname and neighboring Guyana are the only two countries on the mainland South American continent that drive on the left. In Guyana, this practice is inherited from United Kingdom colonial authorities. Various reasons are given to explain why Suriname drives on the left. It is thought that it is because the first cars imported were from England, but this is yet undocumented. In addition, this view does not say anything about traffic before the automobile era. Another explanation is that the Netherlands, at the time of its colonization of Suriname, used the left-hand side of the road for traffic, and yet another is that Suriname was first colonized by the English. Although the Netherlands converted to driving to the right at the end of the 18th century, Suriname did not. Writers Peter Kincaid and Ian Watson suggest that in territories such as Suriname where there are no connecting roads to neighbouring countries, there is no external pressure to change the status quo.
Air.
Airlines with departures from Suriname:
Airlines with arrivals in Suriname:
Other national companies with an air operator certification:
Health.
The fertility rate was at 2.6 births per woman. Public expenditure was at 3.6% of the GDP in 2004, whereas private expenditure was at 4.2%. There were 45 physicians per 100,000 in the early 2000s. Infant mortality was at 30 per 1,000 live births. Male life expectancy at birth was at 66.4 years, whereas female life expectancy at birth was at 73 years.
Education.
Education in Suriname is compulsory until the age of 12, and the nation had a net primary enrollment rate of 94% in 2004. Literacy is very common, particularly among males. The main university in the country is the Anton de Kom University of Suriname.
From elementary school to high school there are 13 grades. The elementary school has six grades, middle school four grades and high school three grades. Students take a test in the end of elementary school to determine whether they will go to the MULO (secondary modern school) or a middle school of lower standards like LBGO. Students from the elementary school wear a green shirt with jeans, while middle school students wear a blue shirt with jeans.
Students going from the second grade of middle school to the third grade have to choose between the business or science courses. This will determine what their major subjects will be. In order to go on to study math and physics, the student must have a total of 13 points. If the student has fewer points, he/she will go into the business courses or fail the grade.
Biodiversity.
In October 2013, 16 international scientists researching the ecosystems during a three-week expedition in Suriname's Upper Palumeu River Watershed catalogued 1,378 species and found 60—including six frogs, one snake, and 11 fish—that may be previously unknown species. According to the environmental non-profit Conservation International, which funded the expedition, Suriname's ample supply of fresh water is vital to the biodiversity and healthy ecosystems of the region.
Suriname is said to be the only region where one variety of snakewood ("Brosimum guianense"), a rare shrub-like tree, is found. The harvesting of snakewood is restricted and the wood is expensive, being sold by the ounce in some instances.
Environmental preservation.
On 21 March 2013, Suriname's REDD+ Readiness Preparation Proposal (R-PP 2013) was approved by the member countries of the Participants Committee of the Forest Carbon Partnership Facility (FCPF). Member countries include Australia, Canada, Denmark, Finland, France, Germany, Italy, Japan, Netherlands, Norway, Spain, Switzerland, UK, USA and the European Commission.
Media.
A popular newspaper is "De Ware Tijd", however "Times of Suriname" surpassed "De Ware Tijd" as most read newspaper. The most popular source for sports news in SMEsport. The only fully English online newspaper is Devsur: Development of Suriname. Suriname has twenty-four radio stations, two broadcast through the Internet (Apintie and Radio10). There are twelve television sources: TV2(Ch.2), ABC(Ch.4), RBN(Ch.5), STVS(Ch.8), Apintie(Ch.10), ATV(Ch.12), Radika(Ch.14), SCCN(Ch.17), Trishul(Ch. 20), Garuda(Ch.23), Sangeetmala(Ch.26), PL(Ch.28), Ch.30, Ch.32, Ch.38, SCTV(Ch.45), Ch.47, Mustika(Ch.50) And Ch.52. Also listened to is mArt, a broadcaster from Amsterdam founded by people from Suriname. Kondreman is one of the popular cartoons in Suriname.
In 2012, Suriname was ranked joint 22nd with Japan in the worldwide Press Freedom Index by the organization Reporters Without Borders. This was ahead of the US (47th), the UK (28th), and France (38th).
Tourism.
The hotel industry is important to Suriname's economy. The rental of apartments, or the rent-a-house phenomenon, is also popular in Suriname.
Most tourists visit Suriname for the outstanding biodiversity of the pristine Amazonian rain forests in the south of the country, which are noted for their flora and fauna. The Central Suriname Nature Reserve is the biggest and one of the most popular reserves, along with the Brownsberg Nature Park which overlooks the Brokopondo Reservoir, the latter being one of the largest man-made lakes in the world. Tonka Island in the reservoir is home to a rustic eco-tourism project run by the Saramaccaner Maroons. Pangi wraps and bowls made of calabashes are the two main products manufactured for tourists. The Maroons have learned that colorful and ornate pangis are popular with tourists. Other popular decorative souvenirs are hand-carved purple-hardwood made into bowls, plates, canes, wooden boxes, and wall decors.
There are also many waterfalls throughout the country. Raleighvallen, or Raleigh Falls, is a 56000 ha nature reserve on the Coppename River, rich in bird life. Also are the Blanche Marie Falls on the Nickerie River and the Wonotobo Falls. Tafelberg Mountain in the centre of the country is surrounded by its own reserve – the Tafelberg Nature Reserve – around the source of the Saramacca River, as is the Voltzberg Nature Reserve further north on the Coppename River at Raleighvallen. In the interior are many Maroon and Amerindian villages, many of which have their own reserves that are generally open to visitors.
Suriname is one of the few countries in the world where at least one of each biome that the state possesses has been declared a wildlife reserve. Around 30% of the total land area of Suriname is protected by law as reserves.
Other attractions include plantations such as Laarwijk, which is situated along the Suriname River. This plantation can be reached only by boat via Domburg, in the north central Wanica District of Suriname.
Landmarks.
The Jules Wijdenbosch Bridge is a bridge over the river Suriname between Paramaribo and Meerzorg in the Commewijne district. The bridge was built during the tenure of President Jules Albert Wijdenbosch (1996–2000) and was completed in 2000. The bridge is 52 m high, and 1504 m long. It connects Paramaribo with Commewijne, a connection which previously could only be made by ferry. The purpose of the bridge was to facilitate and promote the development of the eastern part of Suriname. The bridge consists of two lanes (one lane each way) and is not accessible to pedestrians.
The Cathedral of Sts. Peter and Paul is 114 years old. Before it became a cathedral it was a theatre. The theatre was built in 1809 and burned down in 1820. The construction of the Sts. Peter and Paul Cathedral started on 13 January 1883.
In between the cathedral and the Presidential Palace is the popular palm garden Palmentuin.
Suriname is one of the few countries in the world where a synagogue is located next to a mosque.
The two buildings are located next to each other in the centre of Paramaribo and have been known to share a parking facility during their respective religious rites, should they happen to coincide with one another.
A relatively new landmark is the Hindu Arya Dewaker temple in the Johan Adolf Pengelstraat in Wanica, Paramaribo, which was inaugurated in 2001. A special characteristic of the temple is that it does not have images of the Hindu divinities, as they are forbidden in the Arya Samaj, the Hindu movement to which the people who built the temple belong. Instead, the building is covered by many texts derived from the Vedas and other Hindu scriptures. The beautiful architecture makes the temple a tourist attraction.
Further reading.
</dl>

</doc>
<doc id="26829" url="http://en.wikipedia.org/wiki?curid=26829" title="Category of sets">
Category of sets

In the mathematical field of category theory, the category of sets, denoted as Set, is the category whose objects are sets. The arrows or morphisms between sets "A" and "B" are all triples ("f", "A", "B") where "f" is a function from "A" to "B".
Many other categories (such as the category of groups, with group homomorphisms as arrows) add structure to the objects of the category of sets and/or restrict the arrows to functions of a particular kind.
Properties of the category of sets.
The epimorphisms in Set are the surjective maps, the monomorphisms are the injective maps, and the isomorphisms are the bijective maps.
The empty set serves as the initial object in Set with empty functions as morphisms. Every singleton is a terminal object, with the functions mapping all elements of the source sets to the single target element as morphisms. There are thus no zero objects in Set. 
The category Set is complete and co-complete. The product in this category is given by the cartesian product of sets. The coproduct is given by the disjoint union: given sets "A""i" where "i" ranges over some index set "I", we construct the coproduct as the union of "A""i"×{"i"} (the cartesian product with "i" serves to ensure that all the components stay disjoint).
Set is the prototype of a concrete category; other categories are concrete if they "resemble" Set in some well-defined way.
Every two-element set serves as a subobject classifier in Set. The power object of a set "A" is given by its power set, and the exponential object of the sets "A" and "B" is given by the set of all functions from "A" to "B". Set is thus a topos (and in particular cartesian closed).
Set is not abelian, additive or preadditive. Its right zero morphisms are the empty functions ∅ → "X".
Every object in Set which is not initial is injective and (assuming the axiom of choice) also projective.
Foundations for the category of sets.
In Zermelo–Fraenkel set theory the collection of all sets is not a set; this follows from the axiom of foundation. One refers to collections that are not sets as proper classes. One can't handle proper classes as one handles sets; in particular, one can't write that those proper classes belong to a collection (either a set or a proper class). This is a problem: it means that the category of sets cannot be formalized straightforwardly in this setting.
One way to resolve the problem is to work in a system that gives formal status to proper classes, such as NBG set theory. In this setting, categories formed from sets are said to be "small" and those (like Set) that are formed from proper classes are said to be "large".
Another solution is to assume the existence of Grothendieck universes. Roughly speaking, a Grothendieck universe is a set which is itself a model of ZF(C) (for instance if a set belongs to a universe, its elements and its powerset will belong to the universe). The existence of Grothendieck universes (other than the empty set and the set formula_1 of all hereditarily finite sets) is not implied by the usual ZF axioms; it is an additional, independent axiom, roughly equivalent to the existence of strongly inaccessible cardinals. Assuming this extra axiom, one can limit the objects of Set to the elements of a particular universe. (There is no "set of all sets" within the model, but one can still reason about the class "U" of all inner sets, i. e., elements of "U".)
In one variation of this scheme, the class of sets is the union of the entire tower of Grothendieck universes. (This is necessarily a proper class, but each Grothendieck universe is a set because it is an element of some larger Grothendieck universe.) However, one does not work directly with the "category of all sets". Instead, theorems are expressed in terms of the category Set"U" whose objects are the elements of a sufficiently large Grothendieck universe "U", and are then shown not to depend on the particular choice of "U". As a foundation for category theory, this approach is well matched to a system like Tarski–Grothendieck set theory in which one cannot reason directly about proper classes; its principal disadvantage is that a theorem can be true of all Set"U" but not of Set.
Various other solutions, and variations on the above, have been proposed.
The same issues arise with other concrete categories, such as the category of groups or the category of topological spaces.

</doc>
<doc id="26830" url="http://en.wikipedia.org/wiki?curid=26830" title="Slovakia">
Slovakia

Slovakia (officially the Slovak Republic; Slovak: "Slovensko"  ], long form Slovenská republika ]) is a country in Central Europe.
It has a population of over five million and an area of about 49000 km2. Slovakia is bordered by the Czech Republic and Austria to the west, Poland to the north, Ukraine to the east and Hungary to the south. The largest city is the capital, Bratislava. Slovakia is a member of the European Union, Eurozone, Schengen Area, NATO, the United Nations, the OECD and the WTO. The official language is Slovak, a member of the Slavic language family.
The Slavs arrived in the territory of present-day Slovakia in the 5th and 6th centuries. In the 7th century, they played a significant role in the creation of Samo's Empire and in the 9th century established the Principality of Nitra. In the 10th century, the territory of today's Slovakia was integrated into the Kingdom of Hungary, which itself became part of the Austro-Hungarian Empire or Habsburg Empire. After World War I and the dissolution of the Austro-Hungarian Empire, the Slovaks and Czechs established Czechoslovakia. A separate Slovak Republic (1939–1945) existed in World War II as a client state of Nazi Germany. In 1945, Czechoslovakia was reestablished. Slovakia became an independent state on 1 January 1993 after the peaceful dissolution of Czechoslovakia.
Slovakia is a high-income advanced economy with one of the fastest growth rates in the European Union and the OECD. The country joined the European Union in 2004 and the Eurozone on 1 January 2009.
History.
Radiocarbon dating puts the oldest surviving archaeological artifacts from Slovakia – found near Nové Mesto nad Váhom – at 270,000 BC, in the Early Paleolithic era. These ancient tools, made by the Clactonian technique, bear witness to the ancient habitation of Slovakia.
Other stone tools from the Middle Paleolithic era (200,000 – 80,000 BC) come from the Prévôt (Prepoštská) cave near Bojnice and from other nearby sites. The most important discovery from that era is a Neanderthal cranium (c. 200,000 BC), discovered near Gánovce, a village in northern Slovakia.
Archaeologists have found prehistoric human skeletons in the region, as well as numerous objects and vestiges of the Gravettian culture, principally in the river valleys of Nitra, Hron, Ipeľ, Váh and as far as the city of Žilina, and near the foot of the Vihorlat, Inovec, and Tribeč mountains, as well as in the Myjava Mountains. The most well-known finds include the oldest female statue made of mammoth-bone (22,800 BC), the famous Venus of Moravany. The statue was found in the 1940s in Moravany nad Váhom near Piešťany. Numerous necklaces made of shells from Cypraca thermophile gastropods of the Tertiary period have come from the sites of Zákovská, Podkovice, Hubina, and Radošinare. These findings provide the most ancient evidence of commercial exchanges carried out between the Mediterranean and Central Europe.
Bronze age.
The Bronze Age in Slovakia went through three stages of development, stretching from 2000 to 800 BC. Major cultural, economic, and political development can be attributed to the significant growth in production of copper, especially in central Slovakia (for example in Špania Dolina) and northwest Slovakia. Copper became a stable source of prosperity for the local population.
After the disappearance of the Čakany and Velatice cultures, the Lusatian people expanded building of strong and complex fortifications, with the large permanent buildings and administrative centers. Excavations of Lusatian hill forts document the substantial development of trade and agriculture at that period. The richness and the diversity of tombs increased considerably. The inhabitants of the area manufactured arms, shields, jewelry, dishes, and statues.
Iron age.
Hallstatt Period.
The arrival of tribes from Thrace disrupted the people of the Kalenderberg culture, who lived in the hamlets located on the plain (Sereď) and in the hill forts like Molpír, near Smolenice, in the Little Carpathians. During Hallstatt times, monumental burial mounds were erected in western Slovakia, with princely equipment consisting of richly decorated vessels, ornaments and decorations. The burial rites consisted entirely of cremation. The common people were buried in flat urnfield cemeteries. A special role was given to weaving and the production of textiles. The local power of the "Princes" of the Hallstatt period disappeared in Slovakia during the last century before the middle of first millennium BCE, after strife between the Scytho-Thracian people and locals, resulting in abandonment of the old hill-forts. Relatively depopulated areas soon caught interest of emerging Celtic tribes, who advanced from the south towards the north, following the Slovak rivers, peacefully integrating into the remnants of the local population.
La Tène Period.
From around 500 BC, the territory of modern-day Slovakia was settled by Celts, who built powerful oppida on the sites of modern-day Bratislava and Devin. Biatecs, silver coins with inscriptions in the Latin alphabet, represent the first known use of writing in Slovakia. At the northern regions, remnants of the local population of Lusatian origin, together with Celtic and later Dacian influence, gave rise to the unique Puchov culture, with advanced crafts and iron-working, many hill-forts and fortified settlements of central type with coinage of the "Velkobysterecky" type (no inscriptions, with a horse on one side and a head on the other). This culture is often connected with the Celtic tribe mentioned in Roman sources as Cotini.
Roman Period.
From 2 AD, the expanding Roman Empire established and maintained a series of outposts around and just north of the Danube, the largest of which were known as Carnuntum (whose remains are on the main road halfway between Vienna and Bratislava) and Brigetio (present-day Szöny at the Slovak-Hungarian border). Such Roman border settlements were built on the present area of Rusovce, currently a suburb of Bratislava. The military fort was surrounded by a civilian vicus and several farms of the villa rustica type. The name of this settlement was Gerulata. The military fort had an auxiliary cavalry unit, approximately 300 horses strong, modeled after the Cananefates. The remains of Roman buildings have also survived in Devin castle (present-day downtown Bratislava), the suburbs of Dubravka and Stupava, and Bratislava Castle Hill.
Near the northernmost line of the Roman hinterlands, the Limes Romanus, there existed the winter camp of Laugaricio (modern-day Trenčín) where the Auxiliary of Legion II fought and prevailed in a decisive battle over the Germanic Quadi tribe in 179 AD during the Marcomannic Wars. The Kingdom of Vannius, a kingdom founded by the Germanic Suebian tribes of Quadi and Marcomanni, as well as several small Germanic and Celtic tribes, including the Osi and Cotini, existed in Western and Central Slovakia from 8–6 BC to 179 AD.
Great invasions from the 4th to 7th centuries.
In the 2nd and 3rd centuries AD, the Huns began to leave the Central Asian steppes. They crossed the Danube in 377 AD and occupied Pannonia, which they used for 75 years as their base for launching looting-raids into Western Europe. However, Attila's death in 453 brought about the disappearance of the Hun tribe. In 568, a Turko-Mongol tribal confederacy, the Avars, conducted its own invasion into the Middle Danube region. The Avars occupied the lowlands of the Pannonian Plain, and established an empire dominating the Carpathian Basin.
In 623, the Slavic population living in the western parts of Pannonia seceded from their empire after a revolution led by Samo, a Frankish merchant. After 626, the Avar power started a gradual decline but its reign lasted to 804.
Slavic states.
The Slavic tribes settled in the territory of present-day Slovakia in the 5th century. Western Slovakia was the centre of Samo's empire in the 7th century. A Slavic state known as the Principality of Nitra arose in the 8th century and its ruler Pribina had the first known Christian church of Slovakia consecrated by 828. Together with neighboring Moravia, the principality formed the core of the Great Moravian Empire from 833. The high point of this Slavonic empire came with the arrival of Saints Cyril and Methodius in 863, during the reign of Prince Rastislav, and the territorial expansion under King Svatopluk I.
Great Moravia (830–before 907).
Great Moravia arose around 830 when Mojmír I unified the Slavic tribes settled north of the Danube and extended the Moravian supremacy over them. When Mojmír I endeavoured to secede from the supremacy of the king of East Francia in 846, King Louis the German deposed him and assisted Moimír's nephew Rastislav (846–870) in acquiring the throne. The new monarch pursued an independent policy: after stopping a Frankish attack in 855, he also sought to weaken influence of Frankish priests preaching in his realm. Rastislav asked the Byzantine Emperor Michael III to send teachers who would interpret Christianity in the Slavic vernacular.
Upon Rastislav's request, two brothers, Byzantine officials and missionaries Saints Cyril and Methodius came in 863. Cyril developed the first Slavic alphabet and translated the Gospel into the Old Church Slavonic language. Rastislav was also preoccupied with the security and administration of his state. Numerous fortified castles built throughout the country are dated to his reign and some of them (e.g., "Dowina", sometimes identified with Devín Castle) are also mentioned in connection with Rastislav by Frankish chronicles.
During Rastislav's reign, the Principality of Nitra was given to his nephew Svatopluk as an appanage. The rebellious prince allied himself with the Franks and overthrew his uncle in 870. Similarly to his predecessor, Svatopluk I (871–894) assumed the title of the king ("rex"). During his reign, the Great Moravian Empire reached its greatest territorial extent, when not only present-day Moravia and Slovakia but also present-day northern and central Hungary, Lower Austria, Bohemia, Silesia, Lusatia, southern Poland and northern Serbia belonged to the empire, but the exact borders of his domains are still disputed by modern authors. Svatopluk also withstood attacks of the semi-nomadic Magyar tribes and the Bulgarian Empire, although sometimes it was he who hired the Magyars when waging war against East Francia.
In 880, Pope John VIII set up an independent ecclesiastical province in Great Moravia with Archbishop Methodius as its head. He also named the German cleric Wiching the Bishop of Nitra.
After the death of Prince Svatopluk in 894, his sons Mojmír II (894–906?) and Svatopluk II succeeded him as the Prince of Great Moravia and the Prince of Nitra respectively. However, they started to quarrel for domination of the whole empire. Weakened by an internal conflict as well as by constant warfare with Eastern Francia, Great Moravia lost most of its peripheral territories.
In the meantime, the semi-nomadic Magyar tribes, possibly having suffered defeat from the similarly nomadic Pechenegs, left their territories east of the Carpathian Mountains, invaded the Carpathian Basin and started to occupy the territory gradually around 896. Their armies' advance may have been promoted by continuous wars among the countries of the region whose rulers still hired them occasionally to intervene in their struggles.
We do not know what happened with both Mojmír II and Svatopluk II because they are not mentioned in written sources after 906. In three battles (4–5 July and 9 August 907) near Bratislava, the Magyars routed Bavarian armies. Some historians put this year as the date of the breakup of the Great Moravian Empire, due to the Hungarian conquest; other historians take the date a little bit earlier (to 902).
Great Moravia left behind a lasting legacy in Central and Eastern Europe. The Glagolitic script and its successor Cyrillic were disseminated to other Slavic countries, charting a new path in their sociocultural development. The administrative system of Great Moravia may have influenced the development of the administration of the Kingdom of Hungary.
Kingdom of Hungary (1000–1918).
Following the disintegration of the Great Moravian Empire at the turn of the 10th century, the Hungarians annexed the territory comprising modern Slovakia. After their defeat on the Lech River they were forced to abandon their nomadic and rapacious ways; they settled in the centre of the Carpathian valley, adopted Christianity and began to build a new state – the Hungarian kingdom.
From the 11th century, when the territory inhabited by the Slavic-speaking population of Danubian Basin was incorporated into the Kingdom of Hungary, until 1918, when the Austro-Hungarian empire collapsed, the territory of modern Slovakia was an integral part of the Hungarian state. The ethnic composition became more diverse with the arrival of the Carpathian Germans in the 13th century, and the Jews in the 14th century.
A significant decline in the population resulted from the invasion of the Mongols in 1241 and the subsequent famine. However, in medieval times the area of the present-day Slovakia was characterized rather by burgeoning towns, construction of numerous stone castles, and the cultivation of the arts. In 1465, King Matthias Corvinus founded the Hungarian Kingdom's third university, in Pressburg (Bratislava), but it was closed in 1490 after his death.
Before the Ottoman Empire's expansion into Hungary and the occupation of Buda in 1541, the capital of the Kingdom of Hungary (under the name of Royal Hungary) moved to Pressburg (in Slovak: "Prešporok" at that time, currently Bratislava). Pressburg became the capital city of the Royal Hungary in 1536. But the Ottoman wars and frequent insurrections against the Habsburg Monarchy also inflicted a great deal of devastation, especially in the rural areas. As the Turks withdrew from Hungary in the late 17th century, the importance of the territory comprising modern Slovakia decreased, although Pressburg retained its status as the capital of Hungary until 1848, when it was transferred to Buda.
During the revolution of 1848–49, the Slovaks supported the Austrian Emperor, hoping for independence from the Hungarian part of the Dual Monarchy, but they failed to achieve their aim. Thereafter relations between the nationalities deteriorated (see Magyarization), culminating in the secession of Slovakia from Hungary after World War I.
Czechoslovakia (1918–1939).
In 1918, Slovakia and the regions of Bohemia, Moravia, Czech Silesia and Carpathian Ruthenia formed a common state, Czechoslovakia, with the borders confirmed by the Treaty of Saint Germain and Treaty of Trianon. In 1919, during the chaos following the breakup of Austria-Hungary, Czechoslovakia was formed with numerous Germans and Hungarians within the newly set borders. A Slovak patriot Milan Rastislav Štefánik (1880–1919), who helped organize Czechoslovak regiments against Austria-Hungary during the First World War, died in a plane crash. In the peace following the World War, Czechoslovakia emerged as a sovereign European state. It provided what were at the time rather extensive rights to its minorities and remained the only democracy in this part of Europe in the interwar period.
During the Interwar period, democratic Czechoslovakia was allied with France, and also with Romania and Yugoslavia (Little Entente); however, the Locarno Treaties of 1925 left East European security open. Both Czechs and Slovaks enjoyed a period of relative prosperity. There was progress in not only the development of the country's economy, but also culture and educational opportunities. The minority Germans came to accept their role in the new country and relations with Austria were good. Yet the Great Depression caused a sharp economic downturn, followed by political disruption and insecurity in Europe.
Thereafter Czechoslovakia came under continuous pressure from the revisionist governments of Germany and Hungary. Eventually this led to the Munich Agreement of September 1938, which allowed Nazi Germany to partially dismember the country by occupying what was called the Sudetenland, a region with a German-speaking majority and bordering Germany and Austria. The remainder of "rump" Czechoslovakia was renamed Czecho-Slovakia and included a greater degree of Slovak political autonomy. Southern and eastern Slovakia, however, was reclaimed by Hungary at the First Vienna Award of November 1938.
World War II.
After the Munich Agreement and its Vienna Award, Nazi Germany threatened to annex part of Slovakia and allow the remaining regions to be partitioned by Hungary or Poland unless independence was declared. Thus, Slovakia seceded from Czecho-Slovakia in March 1939 and allied itself, as demanded by Germany, with Hitler's coalition. The government of the First Slovak Republic, led by Jozef Tiso and Vojtech Tuka, was strongly influenced by Germany and gradually became a puppet regime in many respects.
Most Jews were deported from the country and taken to German death camps. Thousands of Jews, however, remained to labor in Slovak work camps in Sereď, Vyhne, and Nováky. Tiso, through the granting of presidential exceptions, has been credited with saving as many as 40,000 Jews during the war, although other estimates place the figure closer to 4,000 or even 1,000.
Nevertheless, under Tiso's government, the vast majority of Slovakia's Jewish population (between 75,000-105,000 individuals) were murdered. Tiso became the only European leader to pay Nazi authorities to deport his country's Jews.
After it became clear that the Soviet Red Army was going to push the Nazis out of eastern and central Europe, an anti-Nazi resistance movement launched a fierce armed insurrection, known as the Slovak National Uprising, near the end of summer 1944. A bloody German occupation and a guerilla war followed. The territory of Slovakia was liberated by Soviet and Romanian forces by the end of April 1945.
Communist party rule (1948–1989).
After World War II, Czechoslovakia was reconstituted and Jozef Tiso was hanged in 1947 for collaboration with the Nazis. More than 80,000 Hungarians and 32,000 Germans were forced to leave Slovakia, in a series of population transfers initiated by the Allies at the Potsdam Conference. Out of about 130,000 Carpathian Germans in Slovakia in 1938, by 1947 only some 20,000 remained.
Czechoslovakia came under the influence of the Soviet Union and its Warsaw Pact after a coup in 1948. The country was occupied by the Warsaw Pact forces (with the exception of Romania) in 1968, ending a period of liberalization under the leadership of Alexander Dubček. In 1969 Czechoslovakia became a federation of the Czech Socialist Republic and the Slovak Socialist Republic.
Establishment of the Slovak Republic (1993-).
The end of Communist rule in Czechoslovakia in 1989, during the peaceful Velvet Revolution, was followed once again by the country's dissolution, this time into two successor states. In July 1992 Slovakia, led by Prime Minister Vladimír Mečiar, declared itself a sovereign state, meaning that its laws took precedence over those of the federal government. Throughout the autumn of 1992, Mečiar and Czech Prime Minister Václav Klaus negotiated the details for disbanding the federation. In November the federal parliament voted to dissolve the country officially on 31 December 1992.
The Slovak Republic and the Czech Republic went their separate ways after 1 January 1993, an event sometimes called the Velvet Divorce. Slovakia has remained a close partner with the Czech Republic. Both countries cooperate with Hungary and Poland in the Visegrád Group. Slovakia became a member of NATO on 29 March 2004 and of the European Union on 1 May 2004. On 1 January 2009, Slovakia adopted the Euro as its national currency.
Geography.
Slovakia lies between latitudes 47° and 50° N, and longitudes 16° and 23° E.
The Slovak landscape is noted primarily for its mountainous nature, with the Carpathian Mountains extending across most of the northern half of the country. Amongst these mountain ranges are the high peaks of the Fatra-Tatra Area (including Tatra Mountains, Greater Fatra and Lesser Fatra), Slovak Ore Mountains, Slovak Central Mountains or Beskids. The largest lowland is the fertile Danubian Lowland in the southwest, followed by the Eastern Slovak Lowland in the southeast.
Tatra mountains.
Tatras, with 29 peaks higher than 2500 m AMSL, are the highest mountain range in the Carpathian Mountains. Tatras occupy an area of 750 km², of which the greater part 600 km² lies in Slovakia. They are divided into several parts.
To the north, close to the Polish border, are the High Tatras which are a popular hiking and skiing destination and home to many scenic lakes and valleys as well as the highest point in Slovakia, the Gerlachovský štít at 2655 m and the country's highly symbolic mountain Kriváň. To the west are the Western Tatras with their highest peak of Rysy at 2503 m and to the east are the Belianske Tatras, smallest by area.
Separated from the Tatras proper by the valley of the Váh river are the Low Tatras, with their highest peak of Ďumbier at 2043 m.
The Tatra mountain range is represented as one of the three hills on the coat of arms of Slovakia.
National Parks.
There are 9 national parks in Slovakia:
Caves.
Slovakia has hundreds of caves and caverns under its mountains, out of which 15 are open to the public. Most of the caves have stalagmites rising from the ground and stalactites hanging from above. There are currently five Slovak caves under UNESCO's World Heritage Site status. They are Dobšinská Ice Cave, Domica, Gombasek Cave, Jasovská Cave and Ochtinská Aragonite Cave. Other caves open to public include Belianska Cave, Demänovská Cave of Liberty, Demänovská Ice Cave or Bystrianska Cave
Rivers.
Most of the rivers stem in Slovak mountains. Some are only passing through and the others make a natural border with surrounding countries (more than 620 km). For example Dunajec (17 km) to the north, Danube (172 km) to the south or Morava (119 km) to the West. The total length of the rivers on Slovak territory is 49774 km.
The longest river in Slovakia is Váh (403 km), the shortest is Čierna voda. Other important and large rivers are Myjava, Nitra (197 km), Orava, Hron (298 km), Hornád (193 km), Slaná (110 km), Ipeľ (232 km, making the border with Hungary), Bodrog, Laborec, Latorica and Ondava.
The biggest volume of discharge in Slovak rivers is during spring, when the snow is melting from the mountains. The only exception is Danube, whose discharge is the biggest during summer when the snow is melting in the Alps. Danube is the largest river that flows through Slovakia.
Lakes.
There are around 175 naturally formed tarns in High Tatras. With an area of 20 ha and its depth of 53 m, Veľké Hincovo pleso is the largest and the deepest tarn in Slovakia. Other tarns in the High Tatras include Štrbské pleso, Popradské pleso, Skalnaté pleso, Zbojnícke pleso, Velické pleso, Žabie pleso, Krivánske zelené pleso or Roháčske plesá. Other than in the High Tatras there are Vrbické pleso in Low Tatras, Morské oko and Vinné jazero in Vihorlat Mountains or Jezerské jazero in Spišská Magura.
The largest dams on the river Váh are Liptovská Mara and Sĺňava. Other well-known dams are Oravská priehrada in the north, Zemplínska Šírava and Domaša in the east, Senecké jazerá, Zlaté piesky or Zelená voda in the west.
Climate.
The Slovak climate lies between the temperate and continental climate zones with relatively warm summers and cold, cloudy and humid winters. Temperature extremes are in interval between -41 to although temperatures below -30 °C are rare. The weather differs from the mountainous North to the plain South.
The warmest region is Bratislava and Southern Slovakia where the temperatures may rise up to 30 °C in summer, occasionally to 37 °C. During night, the temperatures rise up to 20 °C. The daily temperatures in winter average in the range of -5 °C up to 10 °C. During night it may be freezing, but usually not below -10 °C.
Summer in Northern Slovakia is usually mild with temperatures around 25 °C (less in the mountains). Winters are colder in the mountains, where the snow usually lasts until March and April and the night temperatures go down to -20 °C and colder.
Biodiversity.
Slovakia signed the Rio Convention on Biological Diversity on 19 May 1993, and became a party to the convention on 25 August 1994. It has subsequently produced a National Biodiversity Strategy and Action Plan, which was received by the convention on 2 November 1998.
The biodiversity of Slovakia comprises animals (such as annellids, arthropods, molluscs, nematodes and vertebrates), fungi (Ascomycota, Basidiomycota, Chytridiomycota, Glomeromycota and Zygomycota), micro-organisms (including Mycetozoa), and plants.
Fungi.
Over 4000 species of fungi have been recorded from Slovakia. Of these, nearly 1500 are lichen-forming species Some of these fungi are undoubtedly endemic, but not enough is known to say how many. Of the lichen-forming species, about 40% have been classified as threatened in some way. About 7% are apparently extinct, 9% endangered, 17% vulnerable, and 7% rare. The conservation status of non-lichen-forming fungi in Slovakia is not well documented, but there is a red list for its larger fungi.
Politics and government.
Slovakia is a parliamentary democratic republic with a multi-party system. The last parliamentary elections were held on 10 March 2012 and two rounds of presidential elections took place on 15 and 29 March 2014.
The Slovak head of state is the president (currently Andrej Kiska), elected by direct popular vote for a five-year term. Most executive power lies with the head of government, the prime minister (currently Robert Fico), who is usually the leader of the winning party, but he/she needs to form a majority coalition in the parliament. The prime minister is appointed by the president. The remainder of the cabinet is appointed by the president on the recommendation of the prime minister.
Slovakia's highest legislative body is the 150-seat unicameral National Council of the Slovak Republic ("Národná rada Slovenskej republiky"). Delegates are elected for a four-year term on the basis of proportional representation. Slovakia's highest judicial body is the Constitutional Court of Slovakia ("Ústavný súd"), which rules on constitutional issues. The 13 members of this court are appointed by the president from a slate of candidates nominated by parliament.
Slovakia has been a member state of the European Union and NATO since 2004. As a member of the United Nations (since 1993), Slovakia was, on 10 October 2005, elected to a two-year term on the UN Security Council from 2006 to 2007. Slovakia is also a member of WTO, OECD, OSCE, and other international organizations.
The Constitution of the Slovak Republic was ratified 1 September 1992, and became effective 1 January 1993). It was amended in September 1998 to allow direct election of the president and again in February 2001 due to EU admission requirements.
The civil law system is based on Austro-Hungarian codes. The legal code was modified to comply with the obligations of Organization on Security and Cooperation in Europe (OSCE) and to expunge the Marxist–Leninist legal theory. Slovakia accepts the compulsory International Court of Justice jurisdiction with reservations.
The president is the head of state and the formal head of the executive, though with very limited powers. The president is elected by direct, popular vote under the two-round system for a five-year term.
Following National Council elections, the leader of the majority party or the leader of the majority coalition is usually appointed prime minister by the president. Cabinet appointed by the president on the recommendation of the prime minister has to receive the majority in the parliament.
Foreign relations.
Slovakia has been a member of European Union since 2004. Slovakia has been an active participant in U.S.- and NATO-led military actions. There is a joint Czech-Slovak peacekeeping force in Kosovo.
Slovakia is a member of the United Nations and participates in its specialized agencies. It is a member of the Organization for Security and Cooperation in Europe (OSCE), the World Trade Organization (WTO), and the OECD. It also is part of the Visegrad Four (Slovakia, Hungary, Czech Republic, and Poland), a forum for discussing areas of common concern.
The Slovak Republic and the Czech Republic entered into a Customs Union upon the division of Czechoslovakia in 1993, which facilitates a relatively free flow of goods and services. Slovakia maintains diplomatic relations with 134 countries, primarily through its Ministry of Foreign Affairs. There are 44 embassies and 35 honorary consulates in Bratislava.
Military.
The Armed Forces of the Slovak Republic number 14,000 uniformed personnel. Slovakia joined NATO in March 2004. From 2006 the army transformed into a fully professional organization and compulsory military service was abolished.
Slovak Ground Forces are made up of two active mechanized infantry brigades. The Air and Air Defence Forces comprise one wing of fighters, one wing of utility helicopters, and one SAM brigade. Training and support forces comprise a National Support Element (Multifunctional Battalion, Transport Battalion, Repair Battalion), a garrison force of the capital city Bratislava, as well as a training battalion, and various logistics and communication and information bases. Miscellaneous forces under the direct command of the General Staff include the 5th Special Forces Regiment.
Human rights.
The U.S. State Department in 2010 reported:
Human rights in Slovakia are guaranteed by the Constitution of Slovakia from the year 1992 and by multiple international laws signed in Slovakia between 1948 and 2006. Slovakia excludes multiple citizenships.
Administrative divisions.
As for administrative division, Slovakia is subdivided into 8 "krajov" (singular – "kraj", usually translated as "region"), each of which is named after its principal city. Regions have enjoyed a certain degree of autonomy since 2002. Their self-governing bodies are referred to as Self-governing (or autonomous) Regions (sg. "samosprávny kraj", pl. "samosprávne kraje") or Upper-Tier Territorial Units (sg. "vyšší územný celok", pl. "vyššie územné celky", abbr. VÚC).
The "kraje" are subdivided into many "okresov" (sg. "okres", usually translated as districts). Slovakia currently has 79 districts.
In terms of economics and unemployment rate, the western regions are richer than eastern regions; however the relative difference is no bigger than in most EU countries having regional differences.
Economy.
The Slovak economy is considered an advanced economy, with the country dubbed the "Tatra Tiger". Slovakia transformed from a centrally planned economy to a market-driven economy. Major privatizations are nearly complete, the banking sector is almost completely in private hands, and foreign investment has risen.
Before the financial crisis of 2007–08, Slovakia had experienced high and sustained economic growth. In 2007 (with GDP growth of 10.5%), 2008 (6%) and 2010 (with 4%), Slovakia was the fastest growing economy in the European Union. In 2011 (with the GDP growth of 3.3%) and 2012 (GDP growth of 1.8%), Slovakia was the 2nd fastest growing Eurozone member after Estonia. Slovakia's GDP growth of 0.9% in 2013 remains one of the highest in the Eurozone.
The ratio of government debt to GDP in Slovakia reached 58% by the end of 2013.
Unemployment, peaking at 19% at the end of 1999, decreased to 7.5% in October 2008 according to the Statistical Office of the Slovak Republic. In addition to economic growth, migration of workers to other EU countries also contributed to this reduction. According to Eurostat, which uses a calculation method different from that of the Statistical Office of the Slovak Republic, the unemployment rate in September 2012 is at 13.9% the third highest in the Eurozone (after Spain and Portugal).
Inflation dropped from an average annual rate of 12% in 2000 to just 3.3% in 2002, an election year, but it rose again in 2003–2004 because of rising labor costs and taxes. It reached only 1% in 2010 which is the lowest recorded rate since 1993. The rate was at 4% in 2011.
Slovakia adopted the Euro currency on 1 January 2009 as the 16th member of the Eurozone. The euro in Slovakia was approved by the European commission on 7 May 2008. The Slovak koruna was revalued on 28 May 2008 to 30.126 for 1 euro, which was also the exchange rate for the euro.
Slovakia is an attractive country for foreign investors mainly because of its low wages, low tax rates and well educated labour force. In recent years, Slovakia has been pursuing a policy of encouraging foreign investment. FDI inflow grew more than 600% from 2000 and cumulatively reached an all-time high of $17.3 billion in 2006, or around $22,000 per capita by the end of 2008.
Slovakia, along with other post-communist countries, still faces major challenges in the field of the knowledge economy. The business and public research and development expenditures are well below the EU average. The Programme for International Student Assessment, coordinated by the OECD, currently ranks Slovak secondary education the 30th in the world (placing it just below the United States and just above Spain).
In March 2008, the Ministry of Finance announced that Slovakia's economy is developed enough to stop being an aid receiver from the World Bank. Slovakia became an aid provider at the end of 2008.
Industry.
Although Slovakia's GDP comes mainly from the tertiary (services) sector, the industrial sector also plays an important role within its economy. The main industry sectors are car manufacturing and electrical engineering. Since 2007, Slovakia has been the world's largest producer of cars per capita, with a total of 571,071 cars manufactured in the country in 2007 alone. There are currently three automobile assembly plants: Volkswagen's in Bratislava, PSA Peugeot Citroën's in Trnava and Kia Motors' Žilina Plant.
From electrical engineering companies, Sony has a factory at Nitra for LCD TV manufacturing, Samsung at Galanta for computer monitors and television sets manufacturing.
ESET is an IT security company from Bratislava with more than 500 employees worldwide at present. Their branch offices are in the United States, Ireland, United Kingdom, Argentina, Czech Republic, Singapore and Poland.
Bratislava's geographical position in Central Europe has long made Bratislava a crossroads for international trade traffic. Various ancient trade routes, such as the Amber Road and the Danube waterway, have crossed territory of present-day Bratislava. Today, Bratislava is the road, railway, waterway and airway hub.
Energy.
In 2012, Slovakia produced a total of 28 393 GWh of electricity while at the same time consumed 28 786 GWh. The slightly higher level of consumption than the capacity of production (- 393 GWh) meant the country was not self-sufficient in energy sourcing. Slovakia imported electricity mainly from the Czech Republic (9 961 GWh - 73.6% of total import) and exported mainly to Hungary (10 231 GWh - 78.2% of total export).
Nuclear energy accounts for 53.8% of total electricity production in Slovakia, followed by 18.1% of thermal power energy, 15.1% by hydro power energy, 2% by solar energy, 9.6% by other sources and the rest 1.4% is imported.
The two nuclear power-plants in Slovakia are in Jaslovské Bohunice and Mochovce, each of them containing two operating reactors. Prior to the accession of Slovakia to the EU in 2004, the government agreed to turn-off the V1 block of Jaslovské Bohunice power-plant, built by Soviet Union in 1978. After deactivating the last of the two reactors of the V1 block in 2008, Slovakia instantly stopped being self-dependent in energy production. Currently there is another block (V2) with two active reactors in Jaslovské Bohunice. It is scheduled for decommissioning in 2025. The nuclear power production in Slovakia sometimes draws attention to Austrian green-energy activists who occasionally organize protests and block the borders between the two countries.
Transportation.
There are four main highways D1 to D4 and eight express ways R1 to R8. Most of them are still in the planning phase.
The D1 motorway connects Bratislava to Trnava, Nitra, Trenčín, Žilina and beyond, while the D2 motorway connects it to Prague, Brno and Budapest in the north-south direction. The D4 motorway (an outer bypass), which would ease the pressure on Bratislava's highway system, is mostly at the planning stage.
The A6 motorway to Vienna connects Slovakia directly to the Austrian motorway system and was opened on 19 November 2007.
In Bratislava there are currently five bridges standing over the Danube (ordered by the flow of the river): Lafranconi Bridge, Nový Most (The New Bridge), Starý most (The Old Bridge), Most Apollo and Prístavný most (The Harbor Bridge).
The city's inner network of roadways is made on the radial-circular shape. Nowadays, the city experiences a sharp increase in the road traffic, increasing pressure on the road network. There are about 200,000 registered cars in Bratislava, (approximately 2 inhabitants per car).
Bratislava's M. R. Štefánik Airport is the main international airport in Slovakia. It is located 9 kilometres (5.59 mi) northeast of the city centre. It serves civil and governmental, scheduled and unscheduled domestic and international flights. The current runways support the landing of all common types of aircraft currently used. The airport has enjoyed rapidly growing passenger traffic in recent years; it served 279,028 passengers in 2000, 1,937,642 in 2006 and 2,024,142 in 2007. Smaller airports served by passenger airlines include those in Košice and Poprad.
The Port of Bratislava is one of the two international river ports in Slovakia. The port connects Bratislava to international boat traffic, especially the interconnection from the North Sea to the Black Sea via the Rhine-Main-Danube Canal.
Additionally, tourist lines operate from Bratislava's passenger port, including routes to Devín, Vienna and elsewhere.
Tourism.
Slovakia features natural landscapes, mountains, caves, medieval castles and towns, folk architecture, spas and ski resorts. More than 1.6 million people visited Slovakia in 2006, and the most attractive destinations are the capital of Bratislava and the High Tatras. Most visitors come from the Czech Republic (about 26%), Poland (15%) and Germany (11%).
Typical souvenirs from Slovakia are dolls dressed in folk costumes, ceramic objects, crystal glass, carved wooden figures, črpáks (wooden pitchers), fujaras (a folk instrument on the UNESCO list) and valaškas (a decorated folk hatchet) and above all products made from corn husks and wire, notably human figures.
Souvenirs can be bought in the shops run by the state organization ÚĽUV ("Ústredie ľudovej umeleckej výroby" – Center of Folk Art Production). "Dielo" shop chain sells works of Slovak artists and craftsmen. These shops are mostly found in towns and cities.
Prices of imported products are generally the same as in the neighboring countries, whereas prices of local products and services, especially food, are usually lower.
Science.
The Slovak Academy of Sciences has been the most important scientific and research institution in the country since 1953. Slovaks have made notable scientific and technical contributions during the history. The list of important scientists and their inventions include:
Demographics.
According to the 2011 census, the majority of the inhabitants of Slovakia are Slovaks (80.7%). Hungarians are the largest ethnic minority (8.5%). Other ethnic groups include Roma (2%), Czechs (0.6%), Rusyns (0.6%) and others or unspecified (7.6%). Unofficial estimates on the number of Roma population are much higher, around 9%.
In 2007 Slovakia was estimated to have a total fertility rate of 1.33 (i.e., the average woman will have 1.33 children in her lifetime), which is significantly below the replacement level and is one of the lowest rates among EU countries.
The largest waves of Slovak emigration occurred in the 19th and early 20th centuries. In the 1990 U.S. census, 1.8 million people self-identified as having Slovak ancestry.
Languages.
The official language is Slovak, a member of the Slavic language family. Hungarian is widely spoken in the southern regions, and Rusyn is used in some parts of the Northeast. Minority languages hold co-official status in the municipalities in which the size of the minority population meets the legal threshold of 20%.
Slovakia is ranked among the top EU countries regarding the . In 2007, 68% of the population aged from 25 to 64 years claimed to speak two or more foreign languages, finishing 2nd highest in the European Union. The best known foreign language in Slovakia is Czech. Eurostat report also shows that 98.3% of Slovak students in the upper secondary education take on two foreign languages, ranking highly over the average 60.1% in the European Union.
The deaf community uses the Slovak Sign Language. Even though spoken Czech and Slovak are similar, the Slovak Sign language is not particularly close to Czech Sign Language.
Religion.
The Slovak constitution guarantees freedom of religion. In 2011, 62.0% of Slovaks identified themselves as Roman Catholics, 8.9% as Protestants, 3.8% as Greek Catholics, 0.9% as Orthodox, 13.4% identified themselves as atheists and 10.6% did not answer the question about their belief. In 2004, about one third of the then church members regularly attended church services. The pre–World War II population of the country included an estimated 90,000 Jews (1.6% of the population). After the genocidal policies of the Nazi era, only about 2,300 Jews remain today (0.04% of the population).
Education.
Education in Slovakia is compulsory from age 6 to 16.
Culture.
Folk tradition.
Folk tradition has rooted strongly in Slovakia and is reflected in literature, music, dance and architecture. The prime example is a Slovak national anthem, "Nad Tatrou sa blýska", which is based on a melody from "Kopala studienku" folk song.
Manifestation of Slovak folklore culture is the "Východná" Folklore Festival. It is the oldest and largest nationwide festival with international participation, which takes place in Východná annually. Slovakia is usually represented by many groups but mainly by SĽUK ("Slovenský ľudový umelecký kolektív - Slovak folk art collective"). SĽUK is the largest Slovak folk art group, trying to preserve the folklore tradition.
An example of wooden folk architecture in Slovakia can be seen in the well preserved village of Vlkolínec which has been the UNESCO World Heritage Site since 1993. The eastern part of Slovakia, particularly the region of Spiš, preserves the world's most remarkable folk wooden churches. Most of them are protected by Slovak law as cultural heritage, but some of them are on the UNESCO list too, in Bodružal, Hervartov, Ladomirová and Ruská Bystrá.
The best known Slovak hero, found in many folk mythologies, is Juraj Jánošík (1688–1713) (the Slovak equivalent of Robin Hood). The legend says he was taking from the rich and giving to the poor. Jánošík's life was depicted in a list of literature works and many movies throughout the 20th century. One of the most popular is a film "Jánošík" directed by Martin Frič in 1935.
Art.
Visual art in Slovakia is represented through painting, drawing, printmaking, illustration, arts and crafts, sculpture, photography or conceptual art. The supreme and central gallery institution displaying Slovak art nowadays is the Slovak National Gallery, established in 1949.
Medieval time
Well-known sculptor of the 15th century Late Gothic era in Slovakia is the "Master Paul of Levoča". Although his work can be found in many places (Banská Bystrica, Spišská Sobota or Lomnička), his most famous is a wooden altar in the Church of St. Jacob in Levoča. With its height of 18.62 m, it is the tallest Gothic altar in the world. Well-known painters of that time are the "Master from Okoličné", author of the altar in St. Elisabeth Cathedral in Košice, and "Master M.S." of the 16th century, whose statue of Madonna can be seen in the Saint Catherine Church in Banská Štiavnica. The statues of Saint Catherine and Saint Barbara are in the art gallery of the Slovak Mining Museum in Banská Štiavnica.
19th century
The 19th century in Slovakia was a turbulent period of time when Slovaks began experiencing their national revival in the kingdom of Austria-Hungary. Romanticism of Jozef B. Klemens (1817–1883) and Peter Michal Bohúň (1822–1879) was represented in the portrait paintings of Slovak national protagonists of that time (Štefan Moyses, Andrej Sládkovič, Karol Kuzmány or Ľudovít Štúr), depicting the revolutionary atmosphere of the 1840s in the background. Other important painters of the 19th century were mainly portraitists Vojtech Angyal, Dominik Skutecký (1849–1921), J. Štetka, E. Ballo, Jozef Hanula (1863 – 1944), landscapist Karol Miloslav Lehotský (1846 – 1915) and impressionists Maximilián Schurmann (1863 – 1944) and P. Kern.
Sculpture in the 19th century was dominated by a sacral sculptor Vavrinec Dunajský (1784 – 1833) and his son Ladislav Dunajský, author of Ján Hollý memorial in Dobrá Voda. Another important sculptors were Ján Koniarek (1878 – 1952), Alojz Stróbl (1856 – 1926), Ján Fadrusz (1858 – 1903) and Alojz Rigele (1879–1940).
20th century
Painters Mikuláš Galanda (1895–1938), Martin Benka (1888–1971), Janko Alexy (1894–1970), Miloš Alexander Bazovský (1899–1968), Gustáv Mallý (1879–1952) and Jan Hála (1890–1959) are considered to be the ones who laid foundations of the Slovak modern art in the first half of the 20th century. The inspiration of their work stems mainly from the lives of everyday people in Slovak rurals which they admired and idealized. The painters influenced by Art Nouveau, symbolism and expressionism are Zolo Palugyay (1898 – 1935), Anton Jasusch (1882–1965), Edmund Gwerk (1895–1956) or Július Jakoby (1903 – 1985). Important also is Blažej Baláž (1958).
Some of the most distinguished Slovak artists, whose work was closely linked to modern European art streams are Koloman Sokol (1902 – 2003), who became a professor of graphic techniques at the "Escuela de las Artes del Libro" and at the University of Mexico City from 1937 to 1941, Ľudovít Fulla (1902 – 1980) who received many international prices for his work and Imro Weiner-Kráľ (1901 – 1978). The generation 1909 represent Cyprián Majerník (1909 – 1945), Ján Želibský, Ján Mudroch (1909 – 1968), Ladislav Čemický (1909 – 1968) and Ester M. Šimerová (1909).
Slovak graphic art experienced its peak during the 20th century. The most notable print-makers are Koloman Sokol (1902 – 2003), Vincent Hložník (1919 – 1997), Albín Brunovský (1935 – 1997), Dušan Kállay (1948), Vladimír Gažovič (1939), Karol Ondreička (1898 – 1961) or the young generation of artists Katarína Vavrová, Jozef Jankovič and Matej Krén.
Andy Warhol (1928–1987), a leading figure in the 20th century visual art movement known as pop art, was born in Pittsburgh, Pennsylvania as "Andrej Varchola" to Slovak parents Ondrej Varchola (1889–1942) and Júlia (née Zavacká, 1892–1972). A museum dedicated to him is in Medzilaborce, where his parents lived.
Notable Slovak photographers in the 20th century are Martin Martinček (1913–2004) and Karol Kállay (1926–2012). Both Martinček and Kállay received the EFIAP (Excellence de la Fédération Internationale de l' Art Photographique) price in 1970.
Sculpture in the 20th century represent Ján Koniarek (1878 – 1952), Július Bártfay (1888 – 1979), Tibor Bártfay (1922) Ján Mathé (1922), Jozef Kostka (1912 – 1996), Ladislav Snopek (1919 – 2010), Rudolf Uher or Rudolf Hornák.
21st century
Notable Slovak artists of the 21st century include Cyril Blažo (1970), Petra Štefanková, Martin Vargic and Viliam Loviska (1964)
Literature.
For a list of notable Slovak writers and poets, see List of Slovak authors.
Christian topics include: poem Proglas as a foreword to the four Gospels, partial translations of the Bible into Old Church Slavonic, "Zakon sudnyj ljudem".
Medieval literature, in the period from the 11th to the 15th centuries, was written in Latin, Czech and Slovakized Czech. Lyric (prayers, songs and formulas) was still controlled by the Church, while epic was concentrated on legends. Authors from this period include Johannes de Thurocz, author of the Chronica Hungarorum and Maurus, both of them Hungarians. The worldly literature also emerged and chronicles were written in this period.
There were two leading persons who codified the Slovak language. The first was Anton Bernolák whose concept was based on the western Slovak dialect in 1787. It was the codification of the first ever literary language of Slovaks. The second was Ľudovít Štúr, whose formation of the Slovak language took principles from the central Slovak dialect in 1843.
Slovakia is also known for its polyhistors, of whom include Pavol Jozef Šafárik, Matej Bel, Ján Kollár, and its political revolutionaries and reformists, such Milan Rastislav Štefánik and Alexander Dubček.
Famous globetrotter and explorer, count Móric Benyovszky had Slovak ancestors.
Music.
Classical music
The most important Slovak composers have been Eugen Suchoň, Mikuláš Schneider-Trnavský, Ján Cikker, Ján Levoslav Bella, Alexander Moyzes and Dezider Kardoš, in the 21st century Vladimír Godár and Peter Machajdík.
Pop music
Popular music began to replace folk music beginning in the 1950s, when Slovakia was still part of Czechoslovakia; American jazz, R&B, and rock and roll were popular, alongside waltzes, polkas, and czardas, among other folk forms. By the end of the 1950s, radios were common household items, though only state stations were legal. Slovak popular music began as a mix of bossa nova, cool jazz, and rock, with propagandistic lyrics. Dissenters listened to ORF (Austrian Radio), Radio Luxembourg, or Slobodná Európa (Radio Free Europe), which played more rock.
Due to Czechoslovak isolation, the domestic market was active and many original bands evolved. Slovakia had a very strong pop culture during the 1970s and 1980s. This movement brought many original bands with their own unique interpretations of modern music. The quality of socialist music was very high. Stars such as Karel Gott, Olympic, Pražský výběr (from the Czech Republic) or Elán, Modus, Tublatanka, Team (from Slovakia) and many others were highly acclaimed and many recorded their LPs in foreign languages.
After the Velvet Revolution and the declaration of the Slovak state, domestic music dramatically diversified as free enterprise encouraged the formation of new bands and the development of new genres of music. Soon, however, major labels brought pop music to Slovakia and drove many of the small companies out of business. During the 1990s, American grunge and alternative rock, and Britpop have a wide following, as well as a newfound enthusiasm for musicals.
Jazz
Peter Lipa (born 1943) is a well-known Slovak singer, composer and promoter of modern jazz. He is one of the main organizers of the "Bratislava Jazz Days" festival, which takes place in the capital city at the end of October each year since 1975. It is the biggest jazz venue in Slovakia.
Martin Valihora (1976), having been awarded a scholarship on the Berklee College of Music in Boston, he established himself as a part of the New York's jazz scene. He has been playing with the world's famous Japanese jazz pianist Hiromi Uehara.
Other notable Slovak jazz players are Laco Déczi (1938) - composer, jazz trumpeter, Marián Varga (1947) - composer, organ player
Cuisine.
Traditional Slovak cuisine is based mainly on pork meat, poultry (chicken is the most widely eaten, followed by duck, goose, and turkey), flour, potatoes, cabbage, and milk products. It is relatively closely related to Hungarian, Czech and Austrian cuisine. On the east it is also influenced by Ukrainian and Polish cuisine. In comparison with other European countries, "game meat" is more accessible in Slovakia due to vast resources of forest and because hunting is relatively popular. Boar, rabbit, and venison, are generally available throughout the year. Lamb and goat are eaten but are not widely popular.
The traditional Slovak meals are bryndzové halušky, bryndzové pirohy and other meals with potato dough and bryndza. Bryndza is a salty cheese made of a sheep milk, characterized by a strong taste and aroma. Bryndzové halušky must be on the menu of every traditional Slovak restaurant.
A typical soup is a sauerkraut soup ("kapustnica"). A blood sausage called "jaternica", made from any and all parts of a butchered pig is also a specific slovak meal.
Wine is enjoyed throughout Slovakia. Slovak wine comes predominantly from the southern areas along the Danube and its tributaries; the northern half of the country is too cold and mountainous to grow grapevines. Traditionally, white wine was more popular than red or rosé (except in some regions), and sweet wine more popular than dry, but in recent years tastes seem to be changing. Beer (mainly of the pilsener style, though dark lagers are also consumed) is also popular.
Sport.
Sport activities are practiced widely in Slovakia, many of them on a professional level. Among the most popular are ice hockey, football, tennis, handball, basketball, volleyball, whitewater slalom or athletics.
Ice Hockey
One of the most popular collective sports in Slovakia is ice hockey. Slovakia became the member of IIHF on 2 February 1993 and ever since has won 4 medals in Ice Hockey World Championships, consisting of 1 gold, 2 silver and 1 bronze medal. The most recent success is a silver medal from 2012 IIHF World Championship in Helsinki. Slovak national hockey team made five appearances in the Olympic games too, ended up 4th in the last 2010 Winter Olympics in Vancouver. The country has 8280 registered players and is ranked 8th in the IIHF World Ranking at present. Prior to 2012, Slovak team HC Slovan Bratislava joined the Continental Hockey League, considered the strongest hockey league in Europe, and the second-best in the world.
Slovakia organized the 2011 IIHF World Championship in ice hockey in which the team of Finland won the gold medal. The venue took place in Bratislava and Košice.
The most notable Slovak hockey players who played or are still playing in the National Hockey League are Stan Mikita, Peter Šťastný, Marián Šťastný, Anton Šťastný, Peter Bondra, Žigmund Pálffy, Marián Gáborík, Marián Hossa, Pavol Demitra, Zdeno Chára, Miroslav Šatan, Ľubomír Višňovský, Tomáš Kopecký, Andrej Sekera or Jaroslav Halák.
Whitewater slalom
Whitewater slalom is the most successful Olympic sport in modern-day Slovakia. Apart from winning many World and European Championships, Slovak canoeists collected medals in each Summer Olympic Games since their first appearance in Atlanta 1996.
See also.
Lists:

</doc>
